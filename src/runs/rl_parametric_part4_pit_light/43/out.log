Using TensorFlow backend.
[2019-04-05 11:21:09,327] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=20000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=5e-06, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2000000, metric_func='part4_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_heuri_v2', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-05 11:21:09,327] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-05 11:21:09.364791: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-05 11:21:25,402] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-05 11:21:25,403] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-05 11:21:25,425] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-05 11:21:25,449] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-05 11:21:25,474] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-05 11:21:25,474] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:25,474] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-05 11:21:25,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:25,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-05 11:21:26,475] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:26,478] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-05 11:21:26,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:26,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-05 11:21:27,479] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:27,480] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-05 11:21:27,560] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:27,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-05 11:21:28,481] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:28,482] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-05 11:21:28,563] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:28,564] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-05 11:21:29,479] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-05 11:21:29,480] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:21:29,480] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:21:29,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:29,480] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:21:29,481] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:29,481] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:29,483] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:29,484] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-05 11:21:29,486] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-05 11:21:29,496] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-05 11:21:29,497] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-05 11:21:29,631] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:29,632] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-05 11:21:30,485] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:30,485] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-05 11:21:30,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:30,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-05 11:21:31,486] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:31,487] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-05 11:21:31,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:31,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-05 11:21:32,488] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:32,489] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-05 11:21:32,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:32,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-05 11:21:33,490] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:33,491] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-05 11:21:33,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:33,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-05 11:21:34,492] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:34,493] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-05 11:21:34,612] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:34,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-05 11:21:35,494] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:35,495] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-05 11:21:35,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:35,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-05 11:21:36,496] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:36,497] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-05 11:21:36,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:36,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-05 11:21:37,498] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:37,499] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-05 11:21:37,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:37,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-05 11:21:38,500] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:38,501] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-05 11:21:38,579] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:38,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-05 11:21:39,502] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:39,503] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-05 11:21:39,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:39,621] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-05 11:21:40,504] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-05 11:21:40,504] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-05 11:21:40,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:21:40,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-05 11:22:07,187] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-05 11:22:07,187] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [13.38333333333333, 79.66666666666667, 0.0, 0.0, 19.5, 24.86437821806474, 0.3625391782125416, 0.0, 1.0, 0.0]
[2019-04-05 11:22:07,187] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:22:07,188] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.08146432 0.10396799 0.20593947 0.26070896 0.11225975 0.11292049
 0.12273902], sampled 0.6640298278908148
[2019-04-05 11:23:09,424] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 3919.9493 203877562.2319 673.0869
[2019-04-05 11:23:10,321] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-05 11:23:10,321] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.833333333333334, 39.83333333333334, 0.0, 0.0, 26.0, 22.59984126553087, -0.206319892093772, 0.0, 1.0, 102325.7467334816]
[2019-04-05 11:23:10,321] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:23:10,321] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.08650319 0.1320921  0.19107199 0.19842313 0.13525283 0.1005055
 0.15615128], sampled 0.20951939898833105
[2019-04-05 11:23:24,044] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-05 11:23:24,044] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.3158556466666667, 100.0, 0.0, 0.0, 21.5, 22.86443643818518, -0.1831224037302422, 0.0, 1.0, 0.0]
[2019-04-05 11:23:24,044] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:23:24,045] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.08922417 0.11217559 0.2077542  0.22822471 0.11847995 0.11488847
 0.12925291], sampled 0.3794327189224397
[2019-04-05 11:23:27,715] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 3947.1977 229108620.2138 467.3228
[2019-04-05 11:23:31,701] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 3849.7176 241835715.1749 169.5769
[2019-04-05 11:23:32,724] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3947.1977060646873, 229108620.21376643, 467.32283367744833, 3919.9492666999263, 203877562.2319052, 673.0869362868814, 3849.717631216264, 241835715.17485008, 169.5769076389692]
[2019-04-05 11:23:34,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.10796054 0.13204385 0.17403978 0.17600246 0.11914521 0.11823227
 0.17257585], sum to 1.0000
[2019-04-05 11:23:34,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1957
[2019-04-05 11:23:34,929] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 22.0, 19.76808744456743, -0.8748451401477922, 0.0, 1.0, 49061.84949287541], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 21.0, 19.90705960788225, -0.8526390409964346, 0.0, 1.0, 46050.94279532154], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.25, 0.15892163399018747, 0.21578698633452179, 0.0, 1.0, 0.21929020378724542], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.3494374], dtype=float32), -0.12219423]. 
=============================================
[2019-04-05 11:23:34,943] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[ 0.05936124]
 [ 0.04812711]
 [-0.06379975]
 [-0.0606612 ]
 [-0.01322366]], R is [[0.79028952]
 [1.35381532]
 [1.84027719]
 [2.17901731]
 [2.80008411]].
[2019-04-05 11:23:35,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.11729462 0.15154973 0.1612578  0.22090131 0.11632321 0.10117677
 0.13149656], sum to 1.0000
[2019-04-05 11:23:35,133] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6369
[2019-04-05 11:23:35,236] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.199999999999999, 96.0, 0.0, 0.0, 20.5, 19.9840212853946, -0.849391222293498, 0.0, 1.0, 39049.95178868899], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 10200.0000, 
sim time next is 10800.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 20.5, 20.03568093591947, -0.8413779056680446, 0.0, 1.0, 18748.6228430032], 
processed observation next is [0.0, 0.13043478260869565, 0.662049861495845, 0.96, 0.0, 0.0, 0.20833333333333334, 0.1696400779932891, 0.2195406981106518, 0.0, 1.0, 0.08927915639525333], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.3455483], dtype=float32), -1.2454746]. 
=============================================
[2019-04-05 11:23:45,408] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.12808347 0.1265632  0.16321407 0.15500413 0.13651578 0.15142195
 0.13919733], sum to 1.0000
[2019-04-05 11:23:45,409] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6186
[2019-04-05 11:23:45,514] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 20.0, 19.36372181548598, -0.8990658772292944, 0.0, 1.0, 197423.5411405976], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 167400.0000, 
sim time next is 168000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 20.5, 19.37784299446253, -0.8722892799699177, 0.0, 1.0, 199015.2282150825], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.20833333333333334, 0.11482024953854413, 0.20923690667669412, 0.0, 1.0, 0.9476915629289643], 
reward next is 0.7857, 
noisyNet noise sample is [array([-1.139652], dtype=float32), 1.2458262]. 
=============================================
[2019-04-05 11:23:45,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[-0.34176737]
 [-0.29288372]
 [-0.13280034]
 [-0.17689705]
 [-0.19711721]], R is [[0.47638503]
 [1.32876408]
 [2.02976227]
 [3.00946474]
 [3.97937012]].
[2019-04-05 11:23:46,590] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.11118598 0.16717862 0.18616588 0.16873747 0.13496059 0.10283272
 0.1289388 ], sum to 1.0000
[2019-04-05 11:23:46,592] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2936
[2019-04-05 11:23:46,740] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-8.2, 66.66666666666666, 0.0, 0.0, 22.0, 21.68697879818368, -0.3901039590076021, 1.0, 1.0, 196496.6295350133], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 157200.0000, 
sim time next is 157800.0000, 
raw observation next is [-8.3, 67.33333333333334, 0.0, 0.0, 21.5, 21.66117279912627, -0.395431662197927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.23268698060941828, 0.6733333333333335, 0.0, 0.0, 0.2916666666666667, 0.3050977332605225, 0.3681894459340243, 1.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.86102563], dtype=float32), 0.09978438]. 
=============================================
[2019-04-05 11:23:47,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.09818923 0.12571557 0.18205833 0.18524021 0.13103786 0.12081208
 0.15694672], sum to 1.0000
[2019-04-05 11:23:47,225] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8923
[2019-04-05 11:23:47,332] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 21.77863340243525, -0.4275983220644344, 0.0, 1.0, 48237.36540315393], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 170400.0000, 
sim time next is 171000.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 21.77719057299774, -0.4312181458402751, 0.0, 1.0, 48013.72655555875], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.3147658810831449, 0.35626061805324166, 0.0, 1.0, 0.22863679312170834], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.0069354], dtype=float32), 0.6930843]. 
=============================================
[2019-04-05 11:23:47,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[-0.14504468]
 [-0.13255996]
 [-0.16911018]
 [-0.05259687]
 [-0.12505014]], R is [[-0.17524166]
 [-0.17348924]
 [-0.0288972 ]
 [-0.02860823]
 [ 0.04310643]].
[2019-04-05 11:23:49,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.08570889 0.12774983 0.21511693 0.18071918 0.1312753  0.0889422
 0.17048761], sum to 1.0000
[2019-04-05 11:23:49,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0339
[2019-04-05 11:23:49,392] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-8.9, 78.0, 17.0, 157.0, 20.0, 21.34493573769477, -0.5566766443801944, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 201600.0000, 
sim time next is 202200.0000, 
raw observation next is [-8.816666666666666, 78.0, 22.66666666666667, 203.3333333333333, 19.0, 21.31049011530833, -0.574364990502818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.21837488457987075, 0.78, 0.07555555555555557, 0.22467771639042353, 0.08333333333333333, 0.27587417627569416, 0.30854500316572736, 1.0, 1.0, 0.0], 
reward next is 0.2564, 
noisyNet noise sample is [array([0.44435444], dtype=float32), -1.2807359]. 
=============================================
[2019-04-05 11:23:54,448] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7612: loss 34.5219
[2019-04-05 11:23:54,493] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7612: learning rate 0.0000
[2019-04-05 11:23:54,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.12115419 0.18493469 0.1779748  0.16579935 0.13428624 0.07849208
 0.13735867], sum to 1.0000
[2019-04-05 11:23:54,580] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5483
[2019-04-05 11:23:54,585] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7647: loss 15.6862
[2019-04-05 11:23:54,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7647: learning rate 0.0000
[2019-04-05 11:23:54,611] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7654: loss 6.6359
[2019-04-05 11:23:54,612] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7655: learning rate 0.0000
[2019-04-05 11:23:54,679] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 7673: loss 2.0887
[2019-04-05 11:23:54,726] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.8, 63.66666666666666, 92.33333333333334, 426.0, 22.0, 21.895783058232, -0.5576671305879247, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 294600.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 22.0, 21.90495053379068, -0.5554958681428429, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.3333333333333333, 0.3254125444825566, 0.3148347106190524, 1.0, 1.0, 0.0], 
reward next is 0.0165, 
noisyNet noise sample is [array([0.35470635], dtype=float32), -0.8900742]. 
=============================================
[2019-04-05 11:23:54,726] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 7673: learning rate 0.0000
[2019-04-05 11:23:55,382] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7875: loss 6.3443
[2019-04-05 11:23:55,382] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7875: learning rate 0.0000
[2019-04-05 11:23:55,462] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7894: loss 0.8609
[2019-04-05 11:23:55,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7894: learning rate 0.0000
[2019-04-05 11:23:55,781] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7964: loss 28.3568
[2019-04-05 11:23:55,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7964: learning rate 0.0000
[2019-04-05 11:23:55,827] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7971: loss 7.4146
[2019-04-05 11:23:55,828] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7971: learning rate 0.0000
[2019-04-05 11:23:56,320] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8096: loss 16.0782
[2019-04-05 11:23:56,321] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8096: loss 17.0550
[2019-04-05 11:23:56,321] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8096: learning rate 0.0000
[2019-04-05 11:23:56,321] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8096: learning rate 0.0000
[2019-04-05 11:23:56,334] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8097: loss 8.9220
[2019-04-05 11:23:56,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8097: learning rate 0.0000
[2019-04-05 11:23:56,563] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8155: loss 37.9285
[2019-04-05 11:23:56,564] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8155: learning rate 0.0000
[2019-04-05 11:23:57,246] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8318: loss 8.0135
[2019-04-05 11:23:57,248] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8318: learning rate 0.0000
[2019-04-05 11:23:57,249] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8318: loss 7.8637
[2019-04-05 11:23:57,255] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8318: learning rate 0.0000
[2019-04-05 11:23:57,272] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8327: loss 1.4971
[2019-04-05 11:23:57,273] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8327: learning rate 0.0000
[2019-04-05 11:23:57,511] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8388: loss 38.0169
[2019-04-05 11:23:57,512] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8388: learning rate 0.0000
[2019-04-05 11:23:58,391] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.108755   0.20323978 0.19840863 0.13467114 0.13285543 0.07799635
 0.14407375], sum to 1.0000
[2019-04-05 11:23:58,394] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1490
[2019-04-05 11:23:58,422] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.8, 75.83333333333334, 0.0, 0.0, 20.0, 20.81469005056313, -0.648124511236757, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [-12.8, 77.0, 0.0, 0.0, 19.0, 20.88517800809689, -0.662719156817604, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.1080332409972299, 0.77, 0.0, 0.0, 0.08333333333333333, 0.24043150067474084, 0.279093614394132, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3805197], dtype=float32), 0.68756205]. 
=============================================
[2019-04-05 11:24:03,082] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.10057703 0.16448537 0.21374136 0.19087589 0.12240636 0.08892649
 0.11898753], sum to 1.0000
[2019-04-05 11:24:03,082] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3280
[2019-04-05 11:24:03,249] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-15.96666666666667, 87.0, 27.33333333333334, 520.5, 19.0, 23.1896881837412, -0.2569724168035318, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 376800.0000, 
sim time next is 377400.0000, 
raw observation next is [-15.78333333333333, 88.5, 29.66666666666666, 564.0, 19.0, 23.31368437049468, -0.2548636800497973, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.02539242843951994, 0.885, 0.09888888888888887, 0.6232044198895028, 0.08333333333333333, 0.4428070308745567, 0.41504543998340093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9810375], dtype=float32), 0.81261075]. 
=============================================
[2019-04-05 11:24:04,016] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.08334596 0.26830533 0.19545074 0.1478659  0.12053323 0.06389107
 0.12060776], sum to 1.0000
[2019-04-05 11:24:04,016] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9694
[2019-04-05 11:24:04,120] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 38.66666666666666, 0.0, 0.0, 25.5, 24.03612501324652, -0.1047955032763779, 1.0, 1.0, 202079.1493467331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 409200.0000, 
sim time next is 409800.0000, 
raw observation next is [-9.4, 39.33333333333334, 0.0, 0.0, 26.0, 23.97117024116548, -0.05842304007570087, 1.0, 1.0, 203086.5148570633], 
processed observation next is [1.0, 0.7391304347826086, 0.20221606648199447, 0.3933333333333334, 0.0, 0.0, 0.6666666666666666, 0.49759752009712344, 0.4805256533080997, 1.0, 1.0, 0.9670786421764919], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7806724], dtype=float32), -1.3677717]. 
=============================================
[2019-04-05 11:24:05,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.10172668 0.15433516 0.20545332 0.17893606 0.12847637 0.07732635
 0.15374605], sum to 1.0000
[2019-04-05 11:24:05,608] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6800
[2019-04-05 11:24:05,622] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.1, 52.0, 0.0, 0.0, 20.5, 20.87817774250308, -0.7447385428862686, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 447000.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 21.0, 20.78257248161876, -0.7632383815269695, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.25, 0.23188104013489674, 0.24558720615767682, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([1.3565243], dtype=float32), 0.3962693]. 
=============================================
[2019-04-05 11:24:10,131] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.0810174  0.18565555 0.20791642 0.21322978 0.11969541 0.08596044
 0.10652497], sum to 1.0000
[2019-04-05 11:24:10,132] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4362
[2019-04-05 11:24:10,141] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 21.0, 21.18238832278389, -0.643460496796443, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 508800.0000, 
sim time next is 509400.0000, 
raw observation next is [2.15, 94.0, 0.0, 0.0, 20.5, 21.09047705647919, -0.6606227705157658, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5221606648199446, 0.94, 0.0, 0.0, 0.20833333333333334, 0.25753975470659923, 0.27979240982807807, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.21008648], dtype=float32), 0.2612609]. 
=============================================
[2019-04-05 11:24:15,181] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15136: loss 5.2067
[2019-04-05 11:24:15,182] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15136: learning rate 0.0000
[2019-04-05 11:24:15,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.05768004 0.13942161 0.2768903  0.21068726 0.1376697  0.08091243
 0.0967387 ], sum to 1.0000
[2019-04-05 11:24:15,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8875
[2019-04-05 11:24:15,440] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-1.2, 83.0, 109.0, 204.3333333333333, 21.0, 23.06116596641602, -0.1886302210971516, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 573000.0000, 
sim time next is 573600.0000, 
raw observation next is [-1.2, 83.0, 104.5, 138.6666666666667, 22.0, 22.91885308462172, -0.2184672908508242, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.42936288088642666, 0.83, 0.34833333333333333, 0.15322283609576431, 0.3333333333333333, 0.40990442371847663, 0.4271775697163919, 0.0, 1.0, 0.0], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.679625], dtype=float32), -0.53517777]. 
=============================================
[2019-04-05 11:24:15,785] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15349: loss 8.4980
[2019-04-05 11:24:15,792] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15349: learning rate 0.0000
[2019-04-05 11:24:16,351] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 15602: loss 14.0842
[2019-04-05 11:24:16,352] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 15602: learning rate 0.0000
[2019-04-05 11:24:16,575] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15710: loss 18.8290
[2019-04-05 11:24:16,576] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15710: learning rate 0.0000
[2019-04-05 11:24:16,594] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15721: loss 0.0181
[2019-04-05 11:24:16,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15724: learning rate 0.0000
[2019-04-05 11:24:16,758] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15798: loss 36.8685
[2019-04-05 11:24:16,758] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15798: learning rate 0.0000
[2019-04-05 11:24:16,765] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15801: loss 21.9095
[2019-04-05 11:24:16,766] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15801: learning rate 0.0000
[2019-04-05 11:24:16,867] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15845: loss 12.2009
[2019-04-05 11:24:16,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15845: learning rate 0.0000
[2019-04-05 11:24:17,350] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16081: loss 24.5223
[2019-04-05 11:24:17,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16081: learning rate 0.0000
[2019-04-05 11:24:17,571] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16187: loss 25.0881
[2019-04-05 11:24:17,572] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16187: learning rate 0.0000
[2019-04-05 11:24:17,911] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16352: loss 1.5273
[2019-04-05 11:24:17,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16352: learning rate 0.0000
[2019-04-05 11:24:18,388] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16571: loss 28.0912
[2019-04-05 11:24:18,390] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16572: learning rate 0.0000
[2019-04-05 11:24:18,475] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16619: loss 3.1460
[2019-04-05 11:24:18,476] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16619: learning rate 0.0000
[2019-04-05 11:24:18,607] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16688: loss 1.0839
[2019-04-05 11:24:18,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16688: learning rate 0.0000
[2019-04-05 11:24:18,646] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16706: loss 1.2363
[2019-04-05 11:24:18,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16707: learning rate 0.0000
[2019-04-05 11:24:18,898] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16812: loss 18.6358
[2019-04-05 11:24:18,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16812: learning rate 0.0000
[2019-04-05 11:24:21,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.06461502 0.12460704 0.23199771 0.23551166 0.15449673 0.07258897
 0.11618287], sum to 1.0000
[2019-04-05 11:24:21,943] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7582
[2019-04-05 11:24:21,969] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 23.0, 23.74977169061498, -0.08446606466091273, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 23.5, 23.78790109722335, -0.0914417326500364, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.4583333333333333, 0.48232509143527924, 0.4695194224499879, 0.0, 1.0, 0.0], 
reward next is 0.3571, 
noisyNet noise sample is [array([-0.2541407], dtype=float32), 0.12386486]. 
=============================================
[2019-04-05 11:24:25,817] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.07844192 0.28247815 0.15162487 0.1772505  0.15609092 0.04800317
 0.1061104 ], sum to 1.0000
[2019-04-05 11:24:25,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1521
[2019-04-05 11:24:25,935] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 20.0, 22.27024117914768, -0.4229288692697987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 19.0, 21.97391860919279, -0.4520410450433612, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3596491228070176, 0.5566666666666668, 0.0, 0.0, 0.08333333333333333, 0.33115988409939917, 0.34931965165221296, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36183718], dtype=float32), -0.56805927]. 
=============================================
[2019-04-05 11:24:26,475] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-05 11:24:26,476] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:24:26,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:24:26,478] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:24:26,478] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:24:26,480] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:24:26,480] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:24:26,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-05 11:24:26,503] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-05 11:24:26,514] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-05 11:25:12,940] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.014930666]
[2019-04-05 11:25:12,941] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.609385659, 66.38874869, 0.0, 0.0, 21.0, 23.73705220938422, -0.1407936667214704, 0.0, 1.0, 0.0]
[2019-04-05 11:25:12,942] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:25:12,943] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.06150008 0.13436803 0.24645215 0.24491642 0.13699284 0.06967142
 0.10609896], sampled 0.5833653435295112
[2019-04-05 11:25:59,188] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 4638.4659 193148016.9454 462.8533
[2019-04-05 11:26:15,416] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4556.8993 219421013.1390 168.7301
[2019-04-05 11:26:21,286] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4385.9390 236100494.0302 -33.4802
[2019-04-05 11:26:22,309] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 20000, evaluation results [20000.0, 4556.899330392499, 219421013.13899198, 168.73006451699106, 4638.465926594076, 193148016.945372, 462.8532933955722, 4385.938969675609, 236100494.03020597, -33.48017009108373]
[2019-04-05 11:26:22,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.06782074 0.26566237 0.1455554  0.23403104 0.13367133 0.04074888
 0.11251029], sum to 1.0000
[2019-04-05 11:26:22,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0096
[2019-04-05 11:26:22,700] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-4.816666666666666, 57.16666666666667, 0.0, 0.0, 19.5, 21.42821496729701, -0.524235386178837, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 762600.0000, 
sim time next is 763200.0000, 
raw observation next is [-5.0, 58.0, 0.0, 0.0, 19.0, 21.31348856090236, -0.5493643939438796, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.58, 0.0, 0.0, 0.08333333333333333, 0.27612404674186336, 0.31687853535204014, 1.0, 1.0, 0.0], 
reward next is 0.5064, 
noisyNet noise sample is [array([-1.6002454], dtype=float32), -1.2843877]. 
=============================================
[2019-04-05 11:26:26,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06024953 0.22545414 0.17981184 0.26439893 0.1390728  0.04403338
 0.0869794 ], sum to 1.0000
[2019-04-05 11:26:26,642] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6391
[2019-04-05 11:26:26,737] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 21.0, 21.64115472320402, -0.5735848627278539, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 809400.0000, 
sim time next is 810000.0000, 
raw observation next is [-6.2, 75.0, 46.5, 0.0, 21.0, 21.6461268869386, -0.579131573501809, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.155, 0.0, 0.25, 0.3038439072448833, 0.30695614216606365, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2532523], dtype=float32), -0.6329666]. 
=============================================
[2019-04-05 11:26:26,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[2.83342  ]
 [2.764647 ]
 [2.7870119]
 [2.7514162]
 [2.79324  ]], R is [[2.83904862]
 [2.81065822]
 [3.00980854]
 [2.97971058]
 [2.9499135 ]].
[2019-04-05 11:26:28,575] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23018: loss 5.7901
[2019-04-05 11:26:28,576] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23018: learning rate 0.0000
[2019-04-05 11:26:28,658] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23047: loss 0.1321
[2019-04-05 11:26:28,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23047: learning rate 0.0000
[2019-04-05 11:26:29,683] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23524: loss 10.5213
[2019-04-05 11:26:29,683] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23524: learning rate 0.0000
[2019-04-05 11:26:29,716] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23538: loss 3.5272
[2019-04-05 11:26:29,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23538: learning rate 0.0000
[2019-04-05 11:26:29,841] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 23590: loss 2.6072
[2019-04-05 11:26:29,842] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 23590: learning rate 0.0000
[2019-04-05 11:26:30,321] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23847: loss 0.4576
[2019-04-05 11:26:30,329] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23847: learning rate 0.0000
[2019-04-05 11:26:30,404] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23892: loss 12.2447
[2019-04-05 11:26:30,405] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23892: learning rate 0.0000
[2019-04-05 11:26:30,431] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23908: loss 6.6403
[2019-04-05 11:26:30,434] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23908: learning rate 0.0000
[2019-04-05 11:26:30,613] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23997: loss 18.6677
[2019-04-05 11:26:30,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23997: learning rate 0.0000
[2019-04-05 11:26:31,095] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24216: loss 37.6648
[2019-04-05 11:26:31,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24216: learning rate 0.0000
[2019-04-05 11:26:31,377] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.04662845 0.22102964 0.1872463  0.2913525  0.1468064  0.03910042
 0.0678363 ], sum to 1.0000
[2019-04-05 11:26:31,377] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7681
[2019-04-05 11:26:31,403] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24387: loss 24.1672
[2019-04-05 11:26:31,446] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24387: learning rate 0.0000
[2019-04-05 11:26:31,489] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.8, 92.33333333333333, 15.0, 0.0, 19.0, 21.4014899914447, -0.6493545779171046, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 924000.0000, 
sim time next is 924600.0000, 
raw observation next is [4.9, 92.16666666666667, 12.0, 0.0, 19.5, 21.42204645707338, -0.7214440234580549, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5983379501385043, 0.9216666666666667, 0.04, 0.0, 0.125, 0.2851705380894485, 0.259518658847315, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3524382], dtype=float32), 2.1085224]. 
=============================================
[2019-04-05 11:26:31,723] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24508: loss 1.4027
[2019-04-05 11:26:31,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24508: learning rate 0.0000
[2019-04-05 11:26:31,785] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24539: loss 19.2574
[2019-04-05 11:26:31,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24539: learning rate 0.0000
[2019-04-05 11:26:31,916] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24611: loss 10.0648
[2019-04-05 11:26:31,918] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24611: learning rate 0.0000
[2019-04-05 11:26:32,074] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24712: loss 28.2267
[2019-04-05 11:26:32,075] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24712: learning rate 0.0000
[2019-04-05 11:26:32,287] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24840: loss 34.0978
[2019-04-05 11:26:32,288] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24841: learning rate 0.0000
[2019-04-05 11:26:34,322] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.03171498 0.18808094 0.14800763 0.3864878  0.16036607 0.01732295
 0.06801963], sum to 1.0000
[2019-04-05 11:26:34,322] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1586
[2019-04-05 11:26:34,427] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.5, 76.5, 25.0, 0.0, 21.0, 22.26349721127908, -0.07329561093646332, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1009800.0000, 
sim time next is 1010400.0000, 
raw observation next is [15.5, 77.0, 20.83333333333334, 0.0, 20.0, 23.51668195925034, 0.01404215630234147, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.77, 0.06944444444444446, 0.0, 0.16666666666666666, 0.45972349660419515, 0.5046807187674471, 1.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.3786057], dtype=float32), -0.64713335]. 
=============================================
[2019-04-05 11:26:35,795] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.03821471 0.13352062 0.17205633 0.43113375 0.09386086 0.04840964
 0.08280408], sum to 1.0000
[2019-04-05 11:26:35,800] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3360
[2019-04-05 11:26:35,810] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [13.71666666666667, 78.33333333333333, 0.0, 0.0, 25.0, 24.45108636465072, 0.2316081717742401, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1055400.0000, 
sim time next is 1056000.0000, 
raw observation next is [13.63333333333333, 78.66666666666667, 0.0, 0.0, 24.5, 24.45174855328988, 0.2373032152088752, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.840258541089566, 0.7866666666666667, 0.0, 0.0, 0.5416666666666666, 0.5376457127741568, 0.5791010717362918, 0.0, 1.0, 0.0], 
reward next is 0.2143, 
noisyNet noise sample is [array([0.40859446], dtype=float32), -0.74532026]. 
=============================================
[2019-04-05 11:26:35,830] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[5.208943 ]
 [5.2117233]
 [5.1924357]
 [5.1681   ]
 [5.142425 ]], R is [[5.37925005]
 [5.46831465]
 [5.41363144]
 [5.35949516]
 [5.3059001 ]].
[2019-04-05 11:26:37,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.02036433 0.2571762  0.10267901 0.4552764  0.09461833 0.02421245
 0.04567329], sum to 1.0000
[2019-04-05 11:26:37,107] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9357
[2019-04-05 11:26:37,142] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [10.25, 92.5, 60.0, 0.0, 23.5, 24.79961571339182, 0.1604266928114338, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 984600.0000, 
sim time next is 985200.0000, 
raw observation next is [10.33333333333333, 92.66666666666667, 66.0, 0.0, 24.5, 24.88034104651006, 0.168408083295037, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7488457987072946, 0.9266666666666667, 0.22, 0.0, 0.5416666666666666, 0.5733617538758384, 0.5561360277650124, 1.0, 1.0, 0.0], 
reward next is 0.2143, 
noisyNet noise sample is [array([-0.5394209], dtype=float32), -0.07596736]. 
=============================================
[2019-04-05 11:26:40,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.03000422 0.10572835 0.22365145 0.39017975 0.13742988 0.03839921
 0.07460714], sum to 1.0000
[2019-04-05 11:26:40,848] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2571
[2019-04-05 11:26:40,881] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 30310: loss 1.7701
[2019-04-05 11:26:40,884] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 30310: learning rate 0.0000
[2019-04-05 11:26:40,894] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [13.63333333333333, 81.0, 26.0, 0.1666666666666666, 25.0, 23.35707126985101, 0.1388688770203516, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1153200.0000, 
sim time next is 1153800.0000, 
raw observation next is [14.1, 79.5, 31.0, 0.0, 24.5, 23.860394151221, 0.210353910908002, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8531855955678671, 0.795, 0.10333333333333333, 0.0, 0.5416666666666666, 0.4883661792684168, 0.5701179703026673, 0.0, 1.0, 0.0], 
reward next is 0.2143, 
noisyNet noise sample is [array([0.48590893], dtype=float32), 0.18078703]. 
=============================================
[2019-04-05 11:26:41,637] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 30818: loss 1.0729
[2019-04-05 11:26:41,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 30818: learning rate 0.0000
[2019-04-05 11:26:41,640] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 30818: loss 12.0171
[2019-04-05 11:26:41,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 30819: learning rate 0.0000
[2019-04-05 11:26:41,794] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 30911: loss 11.3511
[2019-04-05 11:26:41,796] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 30912: learning rate 0.0000
[2019-04-05 11:26:42,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.02639991 0.10828339 0.16797781 0.51474786 0.08856557 0.0539586
 0.04006685], sum to 1.0000
[2019-04-05 11:26:42,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1475
[2019-04-05 11:26:42,158] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [16.1, 78.0, 0.0, 0.0, 22.0, 22.89374613954972, -0.03629772997569272, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1209600.0000, 
sim time next is 1210200.0000, 
raw observation next is [16.1, 78.33333333333333, 0.0, 0.0, 21.5, 22.86949271968965, -0.04041473824845469, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7833333333333333, 0.0, 0.0, 0.2916666666666667, 0.40579105997413745, 0.48652842058384843, 0.0, 0.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.77732235], dtype=float32), 1.8840549]. 
=============================================
[2019-04-05 11:26:42,436] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31267: loss 31.4792
[2019-04-05 11:26:42,437] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31268: learning rate 0.0000
[2019-04-05 11:26:42,564] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31342: loss 21.9427
[2019-04-05 11:26:42,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31342: learning rate 0.0000
[2019-04-05 11:26:43,046] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31643: loss -0.9295
[2019-04-05 11:26:43,049] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31644: learning rate 0.0000
[2019-04-05 11:26:43,179] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31719: loss 0.0629
[2019-04-05 11:26:43,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31720: learning rate 0.0000
[2019-04-05 11:26:43,725] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32052: loss 0.3098
[2019-04-05 11:26:43,726] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32052: learning rate 0.0000
[2019-04-05 11:26:43,831] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.03448153 0.10871358 0.13125741 0.5106823  0.12043263 0.0389143
 0.05551824], sum to 1.0000
[2019-04-05 11:26:43,832] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32122: loss 9.8045
[2019-04-05 11:26:43,835] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9104
[2019-04-05 11:26:43,836] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32122: learning rate 0.0000
[2019-04-05 11:26:43,935] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [11.6, 78.0, 0.0, 0.0, 24.0, 23.08405193802047, 0.0350427134452401, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1141800.0000, 
sim time next is 1142400.0000, 
raw observation next is [11.6, 79.0, 0.0, 0.0, 25.0, 23.10262366672689, 0.06036000423465832, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.79, 0.0, 0.0, 0.5833333333333334, 0.4252186388939074, 0.5201200014115528, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.1429, 
noisyNet noise sample is [array([-1.6075224], dtype=float32), -1.2326834]. 
=============================================
[2019-04-05 11:26:44,425] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32470: loss 21.5641
[2019-04-05 11:26:44,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32472: learning rate 0.0000
[2019-04-05 11:26:44,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.03846867 0.11137864 0.1923003  0.37473232 0.13822342 0.08118574
 0.06371088], sum to 1.0000
[2019-04-05 11:26:44,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-05 11:26:44,461] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [16.1, 82.5, 0.0, 0.0, 25.0, 23.56794230161896, 0.1055301647077036, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1216200.0000, 
sim time next is 1216800.0000, 
raw observation next is [16.1, 83.0, 0.0, 0.0, 24.5, 23.5510663964524, 0.1022177861096833, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9085872576177286, 0.83, 0.0, 0.0, 0.5416666666666666, 0.4625888663710332, 0.5340725953698945, 0.0, 0.0, 0.0], 
reward next is 0.2143, 
noisyNet noise sample is [array([-1.6282356], dtype=float32), -0.5183169]. 
=============================================
[2019-04-05 11:26:44,533] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32543: loss 17.2722
[2019-04-05 11:26:44,536] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32544: learning rate 0.0000
[2019-04-05 11:26:44,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02389665 0.10881643 0.13046043 0.555105   0.09470675 0.03643345
 0.05058134], sum to 1.0000
[2019-04-05 11:26:44,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1782
[2019-04-05 11:26:44,578] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.4, 96.66666666666666, 97.0, 0.0, 20.0, 22.87710429509093, 0.002248231796651826, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1252200.0000, 
sim time next is 1252800.0000, 
raw observation next is [14.4, 96.0, 98.0, 0.0, 19.5, 22.89316682751502, 0.003834201923823138, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8614958448753465, 0.96, 0.32666666666666666, 0.0, 0.125, 0.4077639022929184, 0.501278067307941, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.848757], dtype=float32), 0.17002837]. 
=============================================
[2019-04-05 11:26:45,120] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32938: loss 24.7060
[2019-04-05 11:26:45,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32939: learning rate 0.0000
[2019-04-05 11:26:45,297] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 33058: loss 2.7472
[2019-04-05 11:26:45,298] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 33058: learning rate 0.0000
[2019-04-05 11:26:45,777] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.01894879 0.18421955 0.10403448 0.46017316 0.12054759 0.04722288
 0.0648535 ], sum to 1.0000
[2019-04-05 11:26:45,781] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3307
[2019-04-05 11:26:45,787] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.38333333333333, 64.66666666666667, 19.33333333333334, 0.0, 22.5, 24.26689852731056, 0.2731542821952504, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 1183800.0000, 
sim time next is 1184400.0000, 
raw observation next is [18.3, 65.0, 14.5, 0.0, 22.5, 24.25373128423401, 0.2701682236643372, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.65, 0.04833333333333333, 0.0, 0.375, 0.5211442736861676, 0.590056074554779, 0.0, 0.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.06905597], dtype=float32), -1.6186057]. 
=============================================
[2019-04-05 11:26:45,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02662275 0.1429576  0.13880709 0.37630707 0.1594427  0.07116471
 0.08469808], sum to 1.0000
[2019-04-05 11:26:45,977] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6514
[2019-04-05 11:26:46,070] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.6, 91.33333333333333, 0.0, 0.0, 25.0, 22.27514386811719, -0.1638620134417664, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 25.0, 22.25667743123302, -0.1674869305285884, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.5833333333333334, 0.35472311926941824, 0.4441710231571372, 0.0, 0.0, 0.0], 
reward next is 0.1429, 
noisyNet noise sample is [array([0.18829934], dtype=float32), 0.98546624]. 
=============================================
[2019-04-05 11:26:46,413] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 33735: loss 0.9658
[2019-04-05 11:26:46,414] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 33736: learning rate 0.0000
[2019-04-05 11:26:46,415] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 33736: loss 29.2325
[2019-04-05 11:26:46,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 33737: learning rate 0.0000
[2019-04-05 11:26:51,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01960653 0.20283115 0.1570815  0.44328862 0.09688134 0.03499366
 0.04531713], sum to 1.0000
[2019-04-05 11:26:51,307] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6427
[2019-04-05 11:26:51,317] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.5, 19.47583995853215, -0.8486213220949596, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1401600.0000, 
sim time next is 1402200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.5, 19.49121804827504, -0.8616383854636339, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.125, 0.12426817068958673, 0.21278720484545535, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.12823269], dtype=float32), -0.7904628]. 
=============================================
[2019-04-05 11:26:52,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01457661 0.18340127 0.07964732 0.5448114  0.1318541  0.01179332
 0.03391584], sum to 1.0000
[2019-04-05 11:26:52,862] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3350
[2019-04-05 11:26:52,870] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 81.0, 0.0, 20.0, 21.57003395149181, -0.5341652454822754, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1422000.0000, 
sim time next is 1422600.0000, 
raw observation next is [0.0, 95.0, 84.0, 0.0, 20.0, 21.54656212843548, -0.539041877606426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.28, 0.0, 0.16666666666666666, 0.2955468440362899, 0.3203193741311913, 1.0, 1.0, 0.0], 
reward next is 0.4667, 
noisyNet noise sample is [array([-0.5228905], dtype=float32), -1.3916609]. 
=============================================
[2019-04-05 11:26:54,566] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 38856: loss 9.5772
[2019-04-05 11:26:54,569] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 38857: learning rate 0.0000
[2019-04-05 11:26:54,706] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 38941: loss -0.2458
[2019-04-05 11:26:54,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 38941: learning rate 0.0000
[2019-04-05 11:26:54,894] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39053: loss -0.3231
[2019-04-05 11:26:54,896] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39054: learning rate 0.0000
[2019-04-05 11:26:54,957] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39095: loss -0.4636
[2019-04-05 11:26:54,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39096: learning rate 0.0000
[2019-04-05 11:26:55,337] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39334: loss 7.1173
[2019-04-05 11:26:55,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39336: learning rate 0.0000
[2019-04-05 11:26:56,063] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 39805: loss -1.0138
[2019-04-05 11:26:56,064] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 39805: learning rate 0.0000
[2019-04-05 11:26:56,176] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39883: loss 23.5539
[2019-04-05 11:26:56,177] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39883: learning rate 0.0000
[2019-04-05 11:26:56,201] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39897: loss 7.9750
[2019-04-05 11:26:56,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39897: learning rate 0.0000
[2019-04-05 11:26:56,283] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39950: loss -1.1005
[2019-04-05 11:26:56,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39950: learning rate 0.0000
[2019-04-05 11:26:56,376] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-05 11:26:56,377] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:26:56,377] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:26:56,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:26:56,378] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:26:56,378] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:26:56,379] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:26:56,383] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-05 11:26:56,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-05 11:26:56,411] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-05 11:27:04,445] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.032387044]
[2019-04-05 11:27:04,446] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-11.96936344666667, 81.47525435666667, 0.0, 0.0, 25.0, 23.71084712951711, 0.02686491428894518, 0.0, 1.0, 114154.6920986537]
[2019-04-05 11:27:04,446] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:27:04,446] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.01187489 0.13147663 0.12880865 0.5534583  0.1232754  0.01597608
 0.03513011], sampled 0.25559925494013414
[2019-04-05 11:27:29,886] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.032387044]
[2019-04-05 11:27:29,887] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.5091968071666667, 89.03473672666668, 0.0, 0.0, 23.5, 22.05256230717607, -0.2289282530503278, 0.0, 1.0, 62958.84103377118]
[2019-04-05 11:27:29,887] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:27:29,887] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.01136997 0.11022796 0.11748055 0.5988465  0.11337264 0.01716823
 0.03153415], sampled 0.5960646699443439
[2019-04-05 11:27:53,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.032387044]
[2019-04-05 11:27:53,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.05, 69.0, 0.0, 0.0, 21.5, 20.75817371671477, -0.6097210078465298, 0.0, 1.0, 78613.78454404557]
[2019-04-05 11:27:53,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:27:53,691] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.02121217 0.16057965 0.13041323 0.49382854 0.12558894 0.0221284
 0.04624912], sampled 0.9297225141743465
[2019-04-05 11:28:02,591] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.032387044]
[2019-04-05 11:28:02,592] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.033333333333333, 57.33333333333333, 99.16666666666666, 683.0, 20.0, 22.05456223461582, -0.4197260658787771, 1.0, 1.0, 0.0]
[2019-04-05 11:28:02,592] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:28:02,592] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.02439781 0.17361875 0.11837538 0.49433634 0.12697123 0.0213688
 0.04093171], sampled 0.42797924646483054
[2019-04-05 11:28:04,591] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.032387044]
[2019-04-05 11:28:04,591] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.286313347, 86.139215745, 0.0, 0.0, 19.0, 19.8424290627929, -0.84667921225435, 0.0, 1.0, 0.0]
[2019-04-05 11:28:04,591] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:28:04,592] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.03088767 0.12577982 0.16883154 0.43862295 0.13475813 0.03977252
 0.06134726], sampled 0.5268893652097354
[2019-04-05 11:28:16,511] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5605.0692 175124165.2810 -585.3422
[2019-04-05 11:28:35,996] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5233.8554 199030639.8725 -788.7751
[2019-04-05 11:28:40,745] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5004.5098 214538638.3377 -911.9084
[2019-04-05 11:28:41,768] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 40000, evaluation results [40000.0, 5233.8553705812155, 199030639.8725187, -788.7751432491357, 5605.069150548528, 175124165.28103486, -585.3421860499644, 5004.5097758533975, 214538638.33765277, -911.9084353942154]
[2019-04-05 11:28:42,075] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40197: loss -0.6657
[2019-04-05 11:28:42,078] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40197: learning rate 0.0000
[2019-04-05 11:28:42,193] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40263: loss -0.2156
[2019-04-05 11:28:42,195] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40263: learning rate 0.0000
[2019-04-05 11:28:42,466] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40447: loss 22.1074
[2019-04-05 11:28:42,469] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40447: learning rate 0.0000
[2019-04-05 11:28:43,169] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40898: loss 0.3658
[2019-04-05 11:28:43,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40899: learning rate 0.0000
[2019-04-05 11:28:43,706] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 41230: loss 6.9412
[2019-04-05 11:28:43,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 41232: learning rate 0.0000
[2019-04-05 11:28:43,730] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 41244: loss 18.4618
[2019-04-05 11:28:43,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 41248: learning rate 0.0000
[2019-04-05 11:28:44,019] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 41410: loss -0.2593
[2019-04-05 11:28:44,032] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 41410: learning rate 0.0000
[2019-04-05 11:28:45,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01283114 0.208553   0.04033123 0.6335766  0.07906357 0.01238819
 0.01325626], sum to 1.0000
[2019-04-05 11:28:45,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6953
[2019-04-05 11:28:45,406] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 76.0, 76.0, 85.5, 19.0, 20.2833070746776, -0.7794433893989194, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1587600.0000, 
sim time next is 1588200.0000, 
raw observation next is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 19.0, 20.3477202521318, -0.7568387324451028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6505078485687905, 0.7466666666666667, 0.2966666666666667, 0.11307550644567216, 0.08333333333333333, 0.1956433543443167, 0.24772042251829907, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32882288], dtype=float32), 2.2598119]. 
=============================================
[2019-04-05 11:28:51,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.00116556 0.08517519 0.05432585 0.8048392  0.04702656 0.00280459
 0.00466301], sum to 1.0000
[2019-04-05 11:28:51,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1160
[2019-04-05 11:28:51,104] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [5.416666666666667, 96.16666666666667, 0.0, 0.0, 23.5, 22.48954794577047, -0.2205993521722988, 0.0, 1.0, 44753.92939128305], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1663800.0000, 
sim time next is 1664400.0000, 
raw observation next is [5.333333333333334, 95.33333333333334, 0.0, 0.0, 23.0, 22.58962337037751, -0.2074084660896565, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 0.9533333333333335, 0.0, 0.0, 0.4166666666666667, 0.3824686141981258, 0.4308638446367812, 0.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.9906018], dtype=float32), 0.77087235]. 
=============================================
[2019-04-05 11:28:53,132] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 46881: loss 23.6073
[2019-04-05 11:28:53,133] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 46881: learning rate 0.0000
[2019-04-05 11:28:54,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00537427 0.10156126 0.06679647 0.72571325 0.07081043 0.01054528
 0.01919904], sum to 1.0000
[2019-04-05 11:28:54,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7224
[2019-04-05 11:28:54,172] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 21.0, 19.99235979337794, -0.7485290248432476, 0.0, 1.0, 85019.19790193498], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 21.0, 20.44790533657097, -0.7143720785861576, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.25, 0.20399211138091408, 0.26187597380461414, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.59574586], dtype=float32), -0.06554132]. 
=============================================
[2019-04-05 11:28:54,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00427538 0.06477946 0.09854488 0.72020847 0.09440927 0.00705279
 0.01072976], sum to 1.0000
[2019-04-05 11:28:54,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-05 11:28:54,317] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.1, 85.66666666666667, 70.33333333333334, 0.0, 23.0, 23.21883920547888, -0.1553567285612213, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1762800.0000, 
sim time next is 1763400.0000, 
raw observation next is [-2.2, 86.33333333333333, 75.66666666666666, 0.0, 23.0, 23.1843186425957, -0.1715564699649259, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4016620498614959, 0.8633333333333333, 0.2522222222222222, 0.0, 0.4166666666666667, 0.43202655354964153, 0.44281451001169136, 0.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.21925311], dtype=float32), 0.5971209]. 
=============================================
[2019-04-05 11:28:54,369] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47372: loss 33.6580
[2019-04-05 11:28:54,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47372: learning rate 0.0000
[2019-04-05 11:28:54,912] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47592: loss 10.0399
[2019-04-05 11:28:54,916] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47593: learning rate 0.0000
[2019-04-05 11:28:54,944] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47607: loss 15.6803
[2019-04-05 11:28:54,946] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47607: learning rate 0.0000
[2019-04-05 11:28:55,228] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47705: loss 23.0059
[2019-04-05 11:28:55,228] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47705: learning rate 0.0000
[2019-04-05 11:28:55,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00426186 0.0919361  0.08535723 0.68506896 0.10695993 0.00773872
 0.01867716], sum to 1.0000
[2019-04-05 11:28:55,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8152
[2019-04-05 11:28:55,934] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 20.0, 19.56825214854261, -0.8688476833549728, 0.0, 1.0, 75461.62280197135], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1792200.0000, 
sim time next is 1792800.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 20.0, 19.6939221538556, -0.8526684592574559, 0.0, 1.0, 49794.83820596369], 
processed observation next is [0.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.16666666666666666, 0.14116017948796658, 0.2157771802475147, 0.0, 1.0, 0.2371182771712557], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.1501862], dtype=float32), 0.17127287]. 
=============================================
[2019-04-05 11:28:56,073] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48031: loss 3.4122
[2019-04-05 11:28:56,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48031: learning rate 0.0000
[2019-04-05 11:28:56,136] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48053: loss 24.7595
[2019-04-05 11:28:56,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48053: learning rate 0.0000
[2019-04-05 11:28:56,294] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48118: loss 4.2060
[2019-04-05 11:28:56,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48118: learning rate 0.0000
[2019-04-05 11:28:56,391] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48166: loss 33.1879
[2019-04-05 11:28:56,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48166: learning rate 0.0000
[2019-04-05 11:28:56,460] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 48193: loss -0.0773
[2019-04-05 11:28:56,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 48193: learning rate 0.0000
[2019-04-05 11:28:57,354] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48595: loss 10.3335
[2019-04-05 11:28:57,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48596: learning rate 0.0000
[2019-04-05 11:28:57,405] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48617: loss 26.5269
[2019-04-05 11:28:57,407] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48617: learning rate 0.0000
[2019-04-05 11:28:57,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01383772 0.09396797 0.15014261 0.6190291  0.07678641 0.0181619
 0.02807433], sum to 1.0000
[2019-04-05 11:28:57,576] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1291
[2019-04-05 11:28:57,589] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.2, 83.66666666666667, 0.0, 0.0, 20.5, 20.21927838076017, -0.86406539026824, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1828200.0000, 
sim time next is 1828800.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 20.0, 20.1576245430068, -0.876905910999216, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.83, 0.0, 0.0, 0.16666666666666666, 0.17980204525056673, 0.207698029666928, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.21278507], dtype=float32), 0.02956699]. 
=============================================
[2019-04-05 11:28:57,884] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48821: loss 1.0091
[2019-04-05 11:28:57,887] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48822: learning rate 0.0000
[2019-04-05 11:28:57,944] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48844: loss 6.1195
[2019-04-05 11:28:57,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48844: learning rate 0.0000
[2019-04-05 11:28:58,108] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48919: loss 1.0423
[2019-04-05 11:28:58,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48919: learning rate 0.0000
[2019-04-05 11:28:58,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00389944 0.07934464 0.1168734  0.6616021  0.10429742 0.01045355
 0.02352954], sum to 1.0000
[2019-04-05 11:28:58,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3527
[2019-04-05 11:28:58,298] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 23.0, 22.48584572762795, -0.3448559497891326, 0.0, 1.0, 18755.97019078908], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1803600.0000, 
sim time next is 1804200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 23.0, 22.48419371807788, -0.3586264720955613, 0.0, 1.0, 24314.86027522678], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.4166666666666667, 0.37368280983982327, 0.38045784263481286, 0.0, 1.0, 0.11578504892965133], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.14011629], dtype=float32), 2.4747102]. 
=============================================
[2019-04-05 11:28:58,606] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 49109: loss 1.8464
[2019-04-05 11:28:58,607] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 49109: learning rate 0.0000
[2019-04-05 11:29:05,330] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00643901 0.06091772 0.0885781  0.73161614 0.0934547  0.00805732
 0.01093701], sum to 1.0000
[2019-04-05 11:29:05,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0241
[2019-04-05 11:29:05,429] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 21.0, 20.75372391551521, -0.8106585628020085, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1918800.0000, 
sim time next is 1919400.0000, 
raw observation next is [-8.9, 82.00000000000001, 0.0, 0.0, 20.0, 20.62534829283903, -0.8352372351162064, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.8200000000000002, 0.0, 0.0, 0.16666666666666666, 0.21877902440325236, 0.22158758829459788, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.07673303], dtype=float32), -0.5072333]. 
=============================================
[2019-04-05 11:29:07,538] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00295813 0.07465065 0.03043696 0.85727614 0.02926111 0.00161825
 0.00379874], sum to 1.0000
[2019-04-05 11:29:07,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2346
[2019-04-05 11:29:07,557] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.866666666666667, 75.0, 0.0, 0.0, 19.0, 19.58150779839859, -0.9618114518826331, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1970400.0000, 
sim time next is 1971000.0000, 
raw observation next is [-5.05, 77.0, 0.0, 0.0, 19.0, 19.51916159443694, -0.9834733721772849, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.32271468144044324, 0.77, 0.0, 0.0, 0.08333333333333333, 0.12659679953641176, 0.1721755426075717, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00069549], dtype=float32), -0.7465901]. 
=============================================
[2019-04-05 11:29:07,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[15.577428 ]
 [15.648306 ]
 [15.8199415]
 [15.782203 ]
 [15.789605 ]], R is [[15.38497257]
 [15.23112297]
 [15.07881165]
 [14.92802334]
 [14.77874279]].
[2019-04-05 11:29:09,141] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 54144: loss 11.9516
[2019-04-05 11:29:09,142] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 54144: learning rate 0.0000
[2019-04-05 11:29:10,267] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 54534: loss 18.5535
[2019-04-05 11:29:10,267] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 54534: learning rate 0.0000
[2019-04-05 11:29:10,826] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 54757: loss 4.0404
[2019-04-05 11:29:10,827] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 54757: learning rate 0.0000
[2019-04-05 11:29:10,828] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 54757: loss 3.8154
[2019-04-05 11:29:10,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 54757: learning rate 0.0000
[2019-04-05 11:29:12,007] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 55311: loss 16.0816
[2019-04-05 11:29:12,007] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 55311: learning rate 0.0000
[2019-04-05 11:29:12,164] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55395: loss 7.6985
[2019-04-05 11:29:12,164] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55395: learning rate 0.0000
[2019-04-05 11:29:12,854] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55731: loss 2.0550
[2019-04-05 11:29:12,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55731: learning rate 0.0000
[2019-04-05 11:29:13,103] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55832: loss 5.8596
[2019-04-05 11:29:13,110] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55833: learning rate 0.0000
[2019-04-05 11:29:13,442] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55973: loss 22.9348
[2019-04-05 11:29:13,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55973: learning rate 0.0000
[2019-04-05 11:29:14,055] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56288: loss 15.3721
[2019-04-05 11:29:14,056] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56288: learning rate 0.0000
[2019-04-05 11:29:14,391] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56486: loss 5.9290
[2019-04-05 11:29:14,392] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 56486: loss 14.1950
[2019-04-05 11:29:14,393] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56486: learning rate 0.0000
[2019-04-05 11:29:14,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 56491: learning rate 0.0000
[2019-04-05 11:29:14,800] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56710: loss 7.1743
[2019-04-05 11:29:14,801] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56710: learning rate 0.0000
[2019-04-05 11:29:15,232] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 56924: loss 2.8868
[2019-04-05 11:29:15,238] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 56925: learning rate 0.0000
[2019-04-05 11:29:15,388] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 57008: loss -4.5349
[2019-04-05 11:29:15,392] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 57009: learning rate 0.0000
[2019-04-05 11:29:16,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00107483 0.10572721 0.01966998 0.82069594 0.04671877 0.00117642
 0.00493683], sum to 1.0000
[2019-04-05 11:29:16,003] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5574
[2019-04-05 11:29:16,031] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.5, 68.0, 133.0, 0.0, 20.5, 20.38438105596645, -0.8089638819629245, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 2124600.0000, 
sim time next is 2125200.0000, 
raw observation next is [-5.4, 68.0, 129.0, 0.0, 20.5, 20.65938791639185, -0.7878425662496169, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.31301939058171746, 0.68, 0.43, 0.0, 0.20833333333333334, 0.22161565969932084, 0.2373858112501277, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2930692], dtype=float32), 1.2396494]. 
=============================================
[2019-04-05 11:29:18,282] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 58490: loss 6.5187
[2019-04-05 11:29:18,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 58494: learning rate 0.0000
[2019-04-05 11:29:18,624] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.0028042e-04 2.4964914e-02 2.6912287e-02 9.1085261e-01 3.2539260e-02
 1.1091429e-03 2.8215845e-03], sum to 1.0000
[2019-04-05 11:29:18,626] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5919
[2019-04-05 11:29:18,642] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 73.66666666666667, 0.0, 0.0, 20.5, 20.0190417640487, -0.9335224109603749, 0.0, 1.0, 62303.0897248321], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 2241600.0000, 
sim time next is 2242200.0000, 
raw observation next is [-6.1, 74.33333333333333, 0.0, 0.0, 20.5, 19.88932074532325, -0.9381387788821159, 0.0, 1.0, 124631.6523791957], 
processed observation next is [1.0, 0.9565217391304348, 0.29362880886426596, 0.7433333333333333, 0.0, 0.0, 0.20833333333333334, 0.15744339544360417, 0.18728707370596134, 0.0, 1.0, 0.5934840589485509], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.22938156], dtype=float32), -0.24077334]. 
=============================================
[2019-04-05 11:29:20,612] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00272845 0.0812925  0.06345217 0.7774877  0.06394719 0.00551021
 0.00558168], sum to 1.0000
[2019-04-05 11:29:20,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2627
[2019-04-05 11:29:20,635] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 21.5, 19.40283210311023, -0.9841371830373351, 0.0, 1.0, 196891.138520882], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 2235600.0000, 
sim time next is 2236200.0000, 
raw observation next is [-5.100000000000001, 68.5, 0.0, 0.0, 21.5, 19.36468694178809, -0.9632521115845579, 0.0, 1.0, 198223.0493418155], 
processed observation next is [1.0, 0.9130434782608695, 0.32132963988919666, 0.685, 0.0, 0.0, 0.2916666666666667, 0.11372391181567416, 0.17891596280514735, 0.0, 1.0, 0.9439192825800737], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.5400006], dtype=float32), -0.90345454]. 
=============================================
[2019-04-05 11:29:21,295] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-05 11:29:21,296] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:29:21,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:29:21,296] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:29:21,297] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:29:21,297] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:29:21,297] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:29:21,302] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-05 11:29:21,302] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-05 11:29:21,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-05 11:29:29,372] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.044510476]
[2019-04-05 11:29:29,372] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.583333333333334, 89.0, 0.0, 0.0, 19.5, 19.62056836326211, -0.8248089874156593, 0.0, 1.0, 0.0]
[2019-04-05 11:29:29,372] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:29:29,373] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.00393348 0.05421736 0.0780261  0.7646252  0.07921514 0.00735231
 0.01263041], sampled 0.42803069239407365
[2019-04-05 11:29:39,862] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.044510476]
[2019-04-05 11:29:39,862] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-9.966666666666669, 50.0, 180.1666666666667, 371.0, 21.5, 21.42838461378941, -0.623026094193366, 0.0, 1.0, 0.0]
[2019-04-05 11:29:39,863] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:29:39,864] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.00605678 0.07764821 0.08607091 0.7067594  0.09931996 0.00762199
 0.01652277], sampled 0.3144851944063284
[2019-04-05 11:29:50,022] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.044510476]
[2019-04-05 11:29:50,023] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.626549973, 92.51016459, 106.99476116, 0.0, 19.0, 19.4706405354333, -0.859413184137025, 0.0, 1.0, 0.0]
[2019-04-05 11:29:50,023] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:29:50,024] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.00312922 0.05639767 0.06452332 0.7909862  0.07059392 0.00515753
 0.00921207], sampled 0.14419725307062226
[2019-04-05 11:30:05,520] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.044510476]
[2019-04-05 11:30:05,520] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.2, 75.66666666666667, 0.0, 0.0, 19.0, 19.18416726561305, -1.131902984619536, 0.0, 1.0, 0.0]
[2019-04-05 11:30:05,520] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:30:05,521] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.0091691  0.08267708 0.11645581 0.6418828  0.1130107  0.01332175
 0.02348279], sampled 0.9649580681689222
[2019-04-05 11:30:19,677] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.044510476]
[2019-04-05 11:30:19,677] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.7, 46.33333333333333, 127.8333333333333, 723.8333333333334, 21.0, 22.88976238343755, -0.2144288602790462, 0.0, 1.0, 0.0]
[2019-04-05 11:30:19,677] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:30:19,679] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.00262974 0.04054178 0.05955796 0.80767286 0.07660455 0.00430857
 0.00868452], sampled 0.7930860175217405
[2019-04-05 11:30:45,082] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5159.9628 170899244.5333 -634.1264
[2019-04-05 11:30:52,176] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.044510476]
[2019-04-05 11:30:52,177] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.0, 27.66666666666667, 121.1666666666667, 838.0, 22.5, 24.67689915998904, 0.0969619096923896, 1.0, 1.0, 0.0]
[2019-04-05 11:30:52,177] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:30:52,178] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.00138114 0.04376587 0.03902168 0.85396934 0.05491062 0.00199306
 0.00495828], sampled 0.468036720915873
[2019-04-05 11:30:53,419] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5412.8525 191014586.4778 -1395.2059
[2019-04-05 11:30:57,432] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5373.5121 204842423.8491 -1443.5656
[2019-04-05 11:30:58,456] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 60000, evaluation results [60000.0, 5412.852506938887, 191014586.47777873, -1395.2059023501351, 5159.962779412931, 170899244.5333315, -634.126392475358, 5373.512133374114, 204842423.84906504, -1443.5655658577646]
[2019-04-05 11:31:02,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0019703  0.09156372 0.11724986 0.63799053 0.13418274 0.01159728
 0.00544564], sum to 1.0000
[2019-04-05 11:31:02,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-05 11:31:02,020] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.45, 54.5, 262.0, 74.0, 19.0, 21.6390399586799, -0.6490350489499349, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2291400.0000, 
sim time next is 2292000.0000, 
raw observation next is [-2.2, 53.33333333333334, 255.1666666666667, 73.16666666666667, 19.0, 21.49713233706574, -0.7717615947977579, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4016620498614959, 0.5333333333333334, 0.8505555555555557, 0.08084714548802947, 0.08333333333333333, 0.2914276947554783, 0.24274613506741405, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0581747], dtype=float32), -0.67744225]. 
=============================================
[2019-04-05 11:31:02,029] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[18.565735]
 [18.50431 ]
 [18.44012 ]
 [18.387646]
 [18.33796 ]], R is [[18.42463684]
 [18.24039078]
 [18.05798721]
 [17.87740707]
 [17.69863319]].
[2019-04-05 11:31:03,484] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 62459: loss 13.7989
[2019-04-05 11:31:03,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 62461: learning rate 0.0000
[2019-04-05 11:31:03,652] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 62546: loss 14.2063
[2019-04-05 11:31:03,652] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 62546: learning rate 0.0000
[2019-04-05 11:31:03,669] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 62553: loss 26.9214
[2019-04-05 11:31:03,670] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 62553: learning rate 0.0000
[2019-04-05 11:31:04,498] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.5920624e-04 1.6872572e-02 2.6495101e-02 9.2225540e-01 3.2247677e-02
 6.6002295e-04 9.1007492e-04], sum to 1.0000
[2019-04-05 11:31:04,498] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2983
[2019-04-05 11:31:04,539] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 52.33333333333334, 0.0, 0.0, 21.0, 20.5915566833045, -0.7473271038690151, 1.0, 1.0, 44037.07180830893], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [-1.2, 52.66666666666667, 0.0, 0.0, 21.0, 20.73554537462256, -0.7475101840852755, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5266666666666667, 0.0, 0.0, 0.25, 0.22796211455188006, 0.2508299386382415, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.086507], dtype=float32), -0.4548665]. 
=============================================
[2019-04-05 11:31:04,664] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63016: loss 28.8513
[2019-04-05 11:31:04,666] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63017: learning rate 0.0000
[2019-04-05 11:31:05,394] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63322: loss 13.9754
[2019-04-05 11:31:05,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63322: learning rate 0.0000
[2019-04-05 11:31:06,150] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63631: loss 1.5690
[2019-04-05 11:31:06,152] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63632: learning rate 0.0000
[2019-04-05 11:31:06,536] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 63771: loss -0.1041
[2019-04-05 11:31:06,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 63774: learning rate 0.0000
[2019-04-05 11:31:07,023] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63964: loss 10.5190
[2019-04-05 11:31:07,024] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63964: learning rate 0.0000
[2019-04-05 11:31:07,578] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64223: loss -2.0242
[2019-04-05 11:31:07,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64223: learning rate 0.0000
[2019-04-05 11:31:07,727] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64301: loss 19.8528
[2019-04-05 11:31:07,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64301: learning rate 0.0000
[2019-04-05 11:31:07,826] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0034381  0.03819615 0.03909201 0.8540439  0.05062292 0.00473672
 0.00987022], sum to 1.0000
[2019-04-05 11:31:07,829] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3191
[2019-04-05 11:31:07,836] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.566666666666667, 26.0, 13.0, 82.33333333333333, 19.5, 19.73533021320606, -1.00309574507545, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2481600.0000, 
sim time next is 2482200.0000, 
raw observation next is [2.2, 26.5, 0.0, 0.0, 19.5, 19.67263938287764, -1.025752301458941, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5235457063711911, 0.265, 0.0, 0.0, 0.125, 0.13938661523980342, 0.15808256618035302, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.4900037], dtype=float32), -0.73220336]. 
=============================================
[2019-04-05 11:31:08,226] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 64543: loss 0.0181
[2019-04-05 11:31:08,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 64543: learning rate 0.0000
[2019-04-05 11:31:08,548] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64712: loss 21.8154
[2019-04-05 11:31:08,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64712: learning rate 0.0000
[2019-04-05 11:31:08,696] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00599725 0.04956111 0.08077022 0.6963197  0.14445826 0.00548572
 0.01740773], sum to 1.0000
[2019-04-05 11:31:08,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9906
[2019-04-05 11:31:08,718] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 22.0, 21.94986067440299, -0.4487901262484302, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2342400.0000, 
sim time next is 2343000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 24.0, 21.87789823453242, -0.448261411773195, 0.0, 1.0, 162357.1870193139], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.5, 0.32315818621103504, 0.35057952940893494, 0.0, 1.0, 0.7731294619967328], 
reward next is 0.2857, 
noisyNet noise sample is [array([-0.5833947], dtype=float32), -0.5497779]. 
=============================================
[2019-04-05 11:31:08,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[15.086672]
 [15.589184]
 [15.674761]
 [16.068317]
 [16.155853]], R is [[14.90145397]
 [15.3238678 ]
 [15.7420578 ]
 [16.08463669]
 [16.42378998]].
[2019-04-05 11:31:08,981] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64959: loss 0.2655
[2019-04-05 11:31:08,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64960: learning rate 0.0000
[2019-04-05 11:31:09,316] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 65147: loss 17.4620
[2019-04-05 11:31:09,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 65147: learning rate 0.0000
[2019-04-05 11:31:11,629] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 66232: loss 24.4427
[2019-04-05 11:31:11,636] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 66237: learning rate 0.0000
[2019-04-05 11:31:12,343] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 66627: loss 3.0251
[2019-04-05 11:31:12,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 66627: learning rate 0.0000
[2019-04-05 11:31:15,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6453577e-04 1.5068613e-02 2.0588551e-02 8.9820963e-01 6.0511295e-02
 2.3067456e-04 5.1266807e-03], sum to 1.0000
[2019-04-05 11:31:15,934] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3992
[2019-04-05 11:31:15,995] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.1, 48.66666666666667, 134.0, 41.0, 24.0, 24.20829700171522, -0.1338916336807873, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2542200.0000, 
sim time next is 2542800.0000, 
raw observation next is [-1.0, 48.33333333333334, 133.5, 43.0, 24.0, 24.17963809274305, -0.1357614001763356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4349030470914128, 0.48333333333333345, 0.445, 0.04751381215469613, 0.5, 0.5149698410619209, 0.45474619994122145, 1.0, 1.0, 0.0], 
reward next is 0.2857, 
noisyNet noise sample is [array([-1.2425805], dtype=float32), 0.5418087]. 
=============================================
[2019-04-05 11:31:16,444] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 69034: loss 19.4251
[2019-04-05 11:31:16,445] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 69034: learning rate 0.0000
[2019-04-05 11:31:17,307] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 69503: loss 8.8868
[2019-04-05 11:31:17,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 69503: learning rate 0.0000
[2019-04-05 11:31:17,351] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 69525: loss 19.5648
[2019-04-05 11:31:17,353] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 69525: learning rate 0.0000
[2019-04-05 11:31:17,438] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 69573: loss 14.7842
[2019-04-05 11:31:17,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 69573: learning rate 0.0000
[2019-04-05 11:31:20,521] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 71247: loss 19.3892
[2019-04-05 11:31:20,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 71251: learning rate 0.0000
[2019-04-05 11:31:20,533] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71254: loss 8.1862
[2019-04-05 11:31:20,535] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71254: learning rate 0.0000
[2019-04-05 11:31:20,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71469: loss 17.7301
[2019-04-05 11:31:20,955] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71469: learning rate 0.0000
[2019-04-05 11:31:21,071] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71538: loss 19.6063
[2019-04-05 11:31:21,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71538: learning rate 0.0000
[2019-04-05 11:31:21,177] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 71590: loss 21.5694
[2019-04-05 11:31:21,179] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 71590: learning rate 0.0000
[2019-04-05 11:31:23,157] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 72593: loss 3.6505
[2019-04-05 11:31:23,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 72593: learning rate 0.0000
[2019-04-05 11:31:23,537] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72775: loss 10.5353
[2019-04-05 11:31:23,538] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72775: learning rate 0.0000
[2019-04-05 11:31:24,544] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 73312: loss 6.7787
[2019-04-05 11:31:24,546] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 73312: learning rate 0.0000
[2019-04-05 11:31:24,985] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 73550: loss 2.9660
[2019-04-05 11:31:24,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 73552: learning rate 0.0000
[2019-04-05 11:31:25,195] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 73676: loss 12.5926
[2019-04-05 11:31:25,197] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 73677: learning rate 0.0000
[2019-04-05 11:31:26,149] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 74148: loss 12.5045
[2019-04-05 11:31:26,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 74151: learning rate 0.0000
[2019-04-05 11:31:27,473] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 74811: loss 0.0816
[2019-04-05 11:31:27,475] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 74812: learning rate 0.0000
[2019-04-05 11:31:28,337] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.49149947e-05 3.94697348e-03 1.12501075e-02 9.67904568e-01
 1.65436938e-02 1.18404125e-04 1.51300716e-04], sum to 1.0000
[2019-04-05 11:31:28,340] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8481
[2019-04-05 11:31:28,352] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.533333333333333, 55.33333333333334, 103.1666666666667, 742.1666666666667, 19.5, 21.31461192755629, -0.61511211476195, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2730000.0000, 
sim time next is 2730600.0000, 
raw observation next is [-4.4, 55.0, 102.0, 733.0, 19.5, 21.47781410131044, -0.5925994539322531, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3407202216066482, 0.55, 0.34, 0.8099447513812155, 0.125, 0.28981784177586994, 0.30246684868924895, 1.0, 1.0, 0.0], 
reward next is 0.0026, 
noisyNet noise sample is [array([-0.04248076], dtype=float32), 2.3790274]. 
=============================================
[2019-04-05 11:31:28,550] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7086821e-04 1.4150000e-02 3.0615835e-02 9.1370600e-01 3.9907683e-02
 5.3056172e-04 9.1912097e-04], sum to 1.0000
[2019-04-05 11:31:28,554] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5960
[2019-04-05 11:31:28,561] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 94.16666666666666, 49.33333333333334, 49.66666666666667, 19.0, 19.57341371311732, -0.9980824290178193, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2911800.0000, 
sim time next is 2912400.0000, 
raw observation next is [2.0, 93.0, 38.5, 47.5, 19.0, 19.75433839042038, -0.9854909580042904, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.93, 0.12833333333333333, 0.052486187845303865, 0.08333333333333333, 0.14619486586836494, 0.17150301399856985, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.58840024], dtype=float32), 1.0167326]. 
=============================================
[2019-04-05 11:31:32,445] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 77357: loss 3.0113
[2019-04-05 11:31:32,448] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 77358: learning rate 0.0000
[2019-04-05 11:31:33,321] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 77763: loss 1.0047
[2019-04-05 11:31:33,322] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 77763: learning rate 0.0000
[2019-04-05 11:31:33,884] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 78006: loss 6.0260
[2019-04-05 11:31:33,884] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 78006: learning rate 0.0000
[2019-04-05 11:31:34,337] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 78216: loss 10.7920
[2019-04-05 11:31:34,339] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 78216: learning rate 0.0000
[2019-04-05 11:31:34,897] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 78497: loss 17.1057
[2019-04-05 11:31:34,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 78497: learning rate 0.0000
[2019-04-05 11:31:35,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00202781 0.05488484 0.03907879 0.7970682  0.10013463 0.00259367
 0.00421213], sum to 1.0000
[2019-04-05 11:31:35,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1374
[2019-04-05 11:31:35,547] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 18.55224751092447, -1.169592733443536, 0.0, 1.0, 59482.22524258226], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2964000.0000, 
sim time next is 2964600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 18.54694351181927, -1.170917231745878, 0.0, 1.0, 59359.78425760291], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.04557862598493904, 0.10969425608470733, 0.0, 1.0, 0.2826656393219186], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6342709], dtype=float32), -0.3630128]. 
=============================================
[2019-04-05 11:31:35,703] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 78918: loss 1.5221
[2019-04-05 11:31:35,705] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 78919: learning rate 0.0000
[2019-04-05 11:31:36,517] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79291: loss 18.8255
[2019-04-05 11:31:36,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79291: learning rate 0.0000
[2019-04-05 11:31:37,759] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 79827: loss 13.7049
[2019-04-05 11:31:37,762] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 79828: learning rate 0.0000
[2019-04-05 11:31:38,189] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-05 11:31:38,190] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:31:38,190] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:31:38,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:31:38,191] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:31:38,191] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:31:38,191] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:31:38,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-05 11:31:38,196] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-05 11:31:38,239] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-05 11:32:10,056] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.05410228]
[2019-04-05 11:32:10,057] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.55, 54.0, 0.0, 0.0, 25.5, 24.3055186814399, 0.07434782494568275, 0.0, 1.0, 67388.42398115304]
[2019-04-05 11:32:10,057] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:32:10,058] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.1752319e-05 4.1766865e-03 9.5842481e-03 9.7091246e-01 1.4812400e-02
 1.6050256e-04 3.0202282e-04], sampled 0.3799266971360845
[2019-04-05 11:33:01,325] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 4586.8497 180714760.5136 -463.6868
[2019-04-05 11:33:11,854] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4765.8646 197379874.6026 -1287.9912
[2019-04-05 11:33:14,123] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.05410228]
[2019-04-05 11:33:14,123] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.632798680666667, 80.04802492333334, 169.2157446666667, 0.0, 21.5, 20.94185793815582, -0.5918466859082826, 1.0, 1.0, 18680.41167023054]
[2019-04-05 11:33:14,123] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:33:14,124] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.2128005e-05 9.3481513e-03 1.1521001e-02 9.6091574e-01 1.7426489e-02
 2.0392951e-04 5.0249370e-04], sampled 0.5888891350990375
[2019-04-05 11:33:19,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4253.3234 219275116.7582 -1107.2127
[2019-04-05 11:33:20,112] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 80000, evaluation results [80000.0, 4765.864597237762, 197379874.602583, -1287.9911624119643, 4586.84974131851, 180714760.51358396, -463.6867721934173, 4253.323442497068, 219275116.7581635, -1107.2127482081803]
[2019-04-05 11:33:20,446] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80193: loss 0.9967
[2019-04-05 11:33:20,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80194: learning rate 0.0000
[2019-04-05 11:33:20,648] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80264: loss 4.0085
[2019-04-05 11:33:20,654] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80267: learning rate 0.0000
[2019-04-05 11:33:22,403] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 81104: loss 10.6493
[2019-04-05 11:33:22,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 81106: learning rate 0.0000
[2019-04-05 11:33:23,306] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 81536: loss 0.8029
[2019-04-05 11:33:23,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 81536: learning rate 0.0000
[2019-04-05 11:33:23,431] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 81569: loss 15.7386
[2019-04-05 11:33:23,432] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 81569: learning rate 0.0000
[2019-04-05 11:33:24,855] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 82298: loss 9.3691
[2019-04-05 11:33:24,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 82299: learning rate 0.0000
[2019-04-05 11:33:25,301] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 82509: loss 2.5037
[2019-04-05 11:33:25,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 82509: learning rate 0.0000
[2019-04-05 11:33:27,235] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 83625: loss 2.2841
[2019-04-05 11:33:27,237] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 83625: learning rate 0.0000
[2019-04-05 11:33:29,291] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 84887: loss 3.2056
[2019-04-05 11:33:29,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 84888: learning rate 0.0000
[2019-04-05 11:33:29,872] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 85246: loss 21.8915
[2019-04-05 11:33:29,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 85246: learning rate 0.0000
[2019-04-05 11:33:30,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0589163e-06 1.3923491e-03 1.0718454e-03 9.9465162e-01 2.8444284e-03
 1.7091392e-05 1.9643350e-05], sum to 1.0000
[2019-04-05 11:33:30,411] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0767
[2019-04-05 11:33:30,417] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.5, 20.29902244719366, -0.733742076762867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3265200.0000, 
sim time next is 3265800.0000, 
raw observation next is [-4.0, 66.0, 0.0, 0.0, 19.5, 20.18180705342763, -0.7400673856209553, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.66, 0.0, 0.0, 0.125, 0.18181725445230246, 0.25331087145968156, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.57272446], dtype=float32), 0.062516175]. 
=============================================
[2019-04-05 11:33:30,483] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 85614: loss 3.8839
[2019-04-05 11:33:30,490] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 85614: learning rate 0.0000
[2019-04-05 11:33:30,511] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 85630: loss 17.9078
[2019-04-05 11:33:30,511] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 85630: learning rate 0.0000
[2019-04-05 11:33:30,915] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 85864: loss 2.0560
[2019-04-05 11:33:30,918] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 85865: learning rate 0.0000
[2019-04-05 11:33:31,807] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 86400: loss 17.8213
[2019-04-05 11:33:31,808] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 86400: learning rate 0.0000
[2019-04-05 11:33:32,806] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87033: loss 4.8809
[2019-04-05 11:33:32,809] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87035: learning rate 0.0000
[2019-04-05 11:33:33,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.8531961e-07 2.0676829e-04 1.0478239e-03 9.9660480e-01 2.1125565e-03
 2.1926369e-05 5.5845280e-06], sum to 1.0000
[2019-04-05 11:33:33,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3789
[2019-04-05 11:33:33,856] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 100.0, 0.0, 0.0, 23.0, 22.42941183321796, -0.3748315706478783, 0.0, 1.0, 98927.06029505313], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3122400.0000, 
sim time next is 3123000.0000, 
raw observation next is [2.3, 100.0, 0.0, 0.0, 23.0, 22.43435522711, -0.3735896312873483, 0.0, 1.0, 69293.93183718075], 
processed observation next is [1.0, 0.13043478260869565, 0.5263157894736843, 1.0, 0.0, 0.0, 0.4166666666666667, 0.3695296022591667, 0.3754701229042172, 0.0, 1.0, 0.329971103986575], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.26176816], dtype=float32), 1.3005917]. 
=============================================
[2019-04-05 11:33:33,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[32.019917]
 [31.86899 ]
 [32.07053 ]
 [32.100964]
 [32.078007]], R is [[32.23393631]
 [32.340168  ]
 [32.44533539]
 [32.54945374]
 [32.65253067]].
[2019-04-05 11:33:34,300] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8662917e-06 4.7460110e-03 2.9221415e-03 9.7594088e-01 1.6251564e-02
 4.5221586e-05 9.1322589e-05], sum to 1.0000
[2019-04-05 11:33:34,304] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7853
[2019-04-05 11:33:34,324] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.4, 80.0, 111.0, 781.5, 19.5, 20.69821803775369, -0.6648890320093256, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3236400.0000, 
sim time next is 3237000.0000, 
raw observation next is [-2.333333333333333, 78.5, 111.6666666666667, 791.3333333333334, 19.5, 20.38724199699397, -0.6794334637508346, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3979686057248385, 0.785, 0.37222222222222234, 0.874401473296501, 0.125, 0.19893683308283094, 0.2735221787497218, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4700052], dtype=float32), -0.92072296]. 
=============================================
[2019-04-05 11:33:34,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[33.237568]
 [33.207676]
 [32.920002]
 [32.546124]
 [32.21547 ]], R is [[33.17379761]
 [32.84206009]
 [32.51364136]
 [32.18850708]
 [31.86662292]].
[2019-04-05 11:33:34,426] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88027: loss 8.7904
[2019-04-05 11:33:34,430] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88027: learning rate 0.0000
[2019-04-05 11:33:34,503] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88069: loss 4.1096
[2019-04-05 11:33:34,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88070: learning rate 0.0000
[2019-04-05 11:33:35,254] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88505: loss 3.4545
[2019-04-05 11:33:35,256] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88506: learning rate 0.0000
[2019-04-05 11:33:36,163] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 89064: loss 16.4512
[2019-04-05 11:33:36,166] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 89064: learning rate 0.0000
[2019-04-05 11:33:36,968] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 89546: loss 7.5302
[2019-04-05 11:33:36,971] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 89548: learning rate 0.0000
[2019-04-05 11:33:37,211] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 89691: loss 8.3642
[2019-04-05 11:33:37,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 89691: learning rate 0.0000
[2019-04-05 11:33:37,631] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 89946: loss 8.5072
[2019-04-05 11:33:37,634] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 89946: learning rate 0.0000
[2019-04-05 11:33:37,977] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.7804157e-05 1.5203338e-03 1.2442985e-03 9.9447918e-01 2.6320685e-03
 2.4816542e-05 7.1513125e-05], sum to 1.0000
[2019-04-05 11:33:37,981] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-05 11:33:37,998] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 23.5, 22.9105334329291, -0.1217502535072242, 0.0, 1.0, 34903.94853993196], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3290400.0000, 
sim time next is 3291000.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 23.5, 22.97866975630564, -0.1311207257146443, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.4583333333333333, 0.4148891463588032, 0.4562930914284519, 0.0, 1.0, 0.0], 
reward next is 0.3571, 
noisyNet noise sample is [array([-0.5901455], dtype=float32), -0.42737466]. 
=============================================
[2019-04-05 11:33:38,010] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[33.23821 ]
 [33.640926]
 [34.339516]
 [34.726242]
 [35.00282 ]], R is [[33.10211182]
 [33.12823486]
 [33.1540947 ]
 [33.17969894]
 [33.20504379]].
[2019-04-05 11:33:38,458] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 90445: loss 0.0847
[2019-04-05 11:33:38,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 90445: learning rate 0.0000
[2019-04-05 11:33:39,747] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4855810e-06 1.7978028e-04 8.9691614e-04 9.9611413e-01 2.7970700e-03
 3.4591283e-06 6.1517080e-06], sum to 1.0000
[2019-04-05 11:33:39,748] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3019
[2019-04-05 11:33:39,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1435135e-07 4.0649445e-04 8.3615794e-04 9.9573356e-01 2.9910658e-03
 4.9122568e-06 2.7336044e-05], sum to 1.0000
[2019-04-05 11:33:39,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4560
[2019-04-05 11:33:39,782] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 20.5, 21.65534492047074, -0.5067988289606294, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3507000.0000, 
sim time next is 3507600.0000, 
raw observation next is [3.0, 49.0, 104.1666666666667, 785.1666666666667, 20.5, 21.74292789308725, -0.4858099014217427, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3472222222222223, 0.8675874769797423, 0.20833333333333334, 0.3119106577572707, 0.3380633661927524, 1.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.9377133], dtype=float32), 0.0043699406]. 
=============================================
[2019-04-05 11:33:39,790] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.0, 77.0, 95.0, 505.5, 23.5, 23.77345246986395, -0.02009947731947621, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3315600.0000, 
sim time next is 3316200.0000, 
raw observation next is [-8.833333333333334, 75.83333333333334, 98.0, 542.0, 23.5, 23.8694920103696, -0.004684095398857048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.21791320406278855, 0.7583333333333334, 0.32666666666666666, 0.5988950276243094, 0.4583333333333333, 0.48912433419746676, 0.4984386348670477, 1.0, 1.0, 0.0], 
reward next is 0.3571, 
noisyNet noise sample is [array([-0.85927826], dtype=float32), -1.186084]. 
=============================================
[2019-04-05 11:33:39,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5413518e-05 3.6978989e-03 3.3826332e-03 9.8913646e-01 3.4029270e-03
 2.1515036e-04 1.3954792e-04], sum to 1.0000
[2019-04-05 11:33:39,873] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6307
[2019-04-05 11:33:39,884] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 19.0, 18.90999315589711, -1.132099688669664, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 19.0, 18.9534474785428, -1.140970226389133, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.08333333333333333, 0.07945395654523324, 0.11967659120362233, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9742941], dtype=float32), 1.1948206]. 
=============================================
[2019-04-05 11:33:41,210] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 92109: loss 1.7784
[2019-04-05 11:33:41,211] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 92109: learning rate 0.0000
[2019-04-05 11:33:43,442] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 93400: loss 4.7162
[2019-04-05 11:33:43,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 93401: learning rate 0.0000
[2019-04-05 11:33:43,632] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 93515: loss 1.0374
[2019-04-05 11:33:43,636] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 93516: learning rate 0.0000
[2019-04-05 11:33:43,880] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 93666: loss 4.8976
[2019-04-05 11:33:43,884] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 93667: learning rate 0.0000
[2019-04-05 11:33:44,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.6081996e-05 1.4376541e-02 5.9761745e-03 9.7317749e-01 6.1762780e-03
 1.1782378e-04 7.9632373e-05], sum to 1.0000
[2019-04-05 11:33:44,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9503
[2019-04-05 11:33:44,534] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 20.0, 19.9940904886965, -0.8492252071028906, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3616200.0000, 
sim time next is 3616800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 20.0, 20.04507112756706, -0.8521309477106714, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.16666666666666666, 0.1704225939639216, 0.2159563507631095, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.7455617], dtype=float32), 1.3311749]. 
=============================================
[2019-04-05 11:33:44,617] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 94076: loss 6.8261
[2019-04-05 11:33:44,620] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 94076: learning rate 0.0000
[2019-04-05 11:33:44,629] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 94080: loss 1.5665
[2019-04-05 11:33:44,634] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 94080: learning rate 0.0000
[2019-04-05 11:33:45,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.04195215e-05 7.40913674e-04 1.67117699e-03 9.93049979e-01
 4.39481810e-03 4.04996645e-05 9.22041945e-05], sum to 1.0000
[2019-04-05 11:33:45,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9428
[2019-04-05 11:33:45,040] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666666, 71.16666666666666, 0.0, 0.0, 19.0, 18.65948093241575, -1.162562667881943, 0.0, 1.0, 196429.3622306979], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3473400.0000, 
sim time next is 3474000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 18.57709675780737, -1.144088077540347, 0.0, 1.0, 178622.2959535962], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.048091396483947634, 0.11863730748655099, 0.0, 1.0, 0.8505823616837915], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1472243], dtype=float32), -1.0986048]. 
=============================================
[2019-04-05 11:33:45,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[29.009983]
 [28.872158]
 [28.890518]
 [28.927555]
 [28.905716]], R is [[29.75253487]
 [30.45500946]
 [31.15045929]
 [31.83895493]
 [32.52056503]].
[2019-04-05 11:33:45,173] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 94407: loss 0.8765
[2019-04-05 11:33:45,178] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 94409: learning rate 0.0000
[2019-04-05 11:33:45,251] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.0874624e-06 3.3011374e-03 2.0590918e-03 9.7180015e-01 2.2722933e-02
 9.4028903e-05 1.4533033e-05], sum to 1.0000
[2019-04-05 11:33:45,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5312
[2019-04-05 11:33:45,273] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 41.0, 63.0, 515.0, 22.0, 21.99044523665934, -0.3403284689451704, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 3601800.0000, 
sim time next is 3602400.0000, 
raw observation next is [0.0, 40.33333333333334, 54.83333333333334, 452.8333333333334, 22.0, 21.98792877040314, -0.3507968860829442, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.40333333333333343, 0.18277777777777782, 0.5003683241252304, 0.3333333333333333, 0.332327397533595, 0.38306770463901857, 0.0, 1.0, 0.0], 
reward next is 0.5714, 
noisyNet noise sample is [array([-0.68911606], dtype=float32), -2.2720428]. 
=============================================
[2019-04-05 11:33:45,794] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 94795: loss 4.3450
[2019-04-05 11:33:45,796] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 94796: learning rate 0.0000
[2019-04-05 11:33:46,326] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7918793e-06 2.9208027e-03 9.2813751e-04 9.9557400e-01 5.5942719e-04
 3.2867742e-06 9.5952500e-06], sum to 1.0000
[2019-04-05 11:33:46,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4699
[2019-04-05 11:33:46,345] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 19.0, 20.81818461819453, -0.7938434573073297, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3496800.0000, 
sim time next is 3497400.0000, 
raw observation next is [1.5, 59.0, 115.0, 810.0, 19.0, 20.87824999670305, -0.7069207352427006, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.59, 0.38333333333333336, 0.8950276243093923, 0.08333333333333333, 0.23985416639192092, 0.2643597549190998, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.22821957], dtype=float32), 1.1397702]. 
=============================================
[2019-04-05 11:33:46,915] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.6223645e-06 8.9548941e-04 1.8335977e-03 9.9416029e-01 3.0760663e-03
 2.0324420e-05 5.6812573e-06], sum to 1.0000
[2019-04-05 11:33:46,916] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2323
[2019-04-05 11:33:46,929] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 23.5, 23.1497236164961, -0.2175502888073373, 0.0, 1.0, 18729.2727823762], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3651600.0000, 
sim time next is 3652200.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 23.5, 23.1326206310118, -0.2208657041746809, 0.0, 1.0, 20756.27790678722], 
processed observation next is [0.0, 0.2608695652173913, 0.7257617728531857, 0.26, 0.0, 0.0, 0.4583333333333333, 0.42771838591765005, 0.4263780986084397, 0.0, 1.0, 0.09883941860374867], 
reward next is 0.3571, 
noisyNet noise sample is [array([-0.4210092], dtype=float32), 0.3436929]. 
=============================================
[2019-04-05 11:33:47,432] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95835: loss 3.4200
[2019-04-05 11:33:47,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95837: learning rate 0.0000
[2019-04-05 11:33:48,925] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96777: loss 4.6428
[2019-04-05 11:33:48,926] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96777: loss 1.2825
[2019-04-05 11:33:48,927] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96777: learning rate 0.0000
[2019-04-05 11:33:48,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96779: learning rate 0.0000
[2019-04-05 11:33:50,234] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 97592: loss 0.3137
[2019-04-05 11:33:50,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 97593: learning rate 0.0000
[2019-04-05 11:33:50,274] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 97614: loss 6.3654
[2019-04-05 11:33:50,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 97614: learning rate 0.0000
[2019-04-05 11:33:50,553] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 97800: loss 5.3429
[2019-04-05 11:33:50,555] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 97801: learning rate 0.0000
[2019-04-05 11:33:50,967] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 98067: loss 0.4729
[2019-04-05 11:33:50,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 98067: learning rate 0.0000
[2019-04-05 11:33:51,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8451574e-07 1.9177374e-04 1.7956445e-03 9.9491298e-01 3.0891912e-03
 5.8428618e-06 4.2486849e-06], sum to 1.0000
[2019-04-05 11:33:51,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4383
[2019-04-05 11:33:51,589] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 23.0, 22.85608739698313, -0.2599039946265624, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3629400.0000, 
sim time next is 3630000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 23.0, 22.89910330261093, -0.2615495407822916, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.4166666666666667, 0.40825860855091073, 0.4128168197392361, 0.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([1.7615443], dtype=float32), 0.47592187]. 
=============================================
[2019-04-05 11:33:51,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[34.282593]
 [34.21626 ]
 [34.326355]
 [34.207546]
 [34.011288]], R is [[34.16941833]
 [34.25629425]
 [34.34230042]
 [34.42744827]
 [34.51174545]].
[2019-04-05 11:33:51,967] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 98728: loss 2.8469
[2019-04-05 11:33:51,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 98729: learning rate 0.0000
[2019-04-05 11:33:53,992] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 11:33:53,994] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:33:53,995] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:33:53,996] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:33:53,996] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:33:53,996] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:33:53,997] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:33:54,000] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-05 11:33:54,022] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-05 11:33:54,036] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-05 11:35:05,551] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.06703367]
[2019-04-05 11:35:05,552] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.166666666666666, 41.66666666666667, 0.0, 0.0, 22.5, 22.04147741298141, -0.2635043857882945, 1.0, 1.0, 185943.4618863103]
[2019-04-05 11:35:05,553] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:35:05,554] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.0375286e-07 7.1632891e-04 1.3039686e-03 9.9548090e-01 2.4905303e-03
 2.1502733e-06 5.6555959e-06], sampled 0.21608794553256017
[2019-04-05 11:35:13,245] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 4568.2408 175401844.4119 -979.6221
[2019-04-05 11:35:23,165] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5251.7038 197343839.1732 -2286.7526
[2019-04-05 11:35:24,846] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4512.8449 201899349.4505 -1493.8914
[2019-04-05 11:35:25,869] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 100000, evaluation results [100000.0, 4512.844870890805, 201899349.45046806, -1493.891368209513, 4568.240782960308, 175401844.41190428, -979.622082127593, 5251.703788426591, 197343839.17319936, -2286.7526471415113]
[2019-04-05 11:35:26,854] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 100614: loss 2.1609
[2019-04-05 11:35:26,857] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 100614: learning rate 0.0000
[2019-04-05 11:35:27,548] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 101056: loss 2.4968
[2019-04-05 11:35:27,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 101056: learning rate 0.0000
[2019-04-05 11:35:28,326] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 101529: loss 1.6303
[2019-04-05 11:35:28,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 101529: learning rate 0.0000
[2019-04-05 11:35:28,600] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 101700: loss 0.0052
[2019-04-05 11:35:28,602] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 101701: learning rate 0.0000
[2019-04-05 11:35:28,622] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 101712: loss 0.3629
[2019-04-05 11:35:28,623] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 101712: learning rate 0.0000
[2019-04-05 11:35:29,641] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 102364: loss 0.4664
[2019-04-05 11:35:29,645] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 102364: learning rate 0.0000
[2019-04-05 11:35:29,677] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 102382: loss 1.7564
[2019-04-05 11:35:29,680] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 102383: learning rate 0.0000
[2019-04-05 11:35:30,066] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 102634: loss 0.9083
[2019-04-05 11:35:30,069] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 102634: learning rate 0.0000
[2019-04-05 11:35:31,291] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103340: loss 3.1590
[2019-04-05 11:35:31,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103341: learning rate 0.0000
[2019-04-05 11:35:32,569] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104102: loss 0.5748
[2019-04-05 11:35:32,570] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104102: learning rate 0.0000
[2019-04-05 11:35:33,693] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104723: loss 0.5363
[2019-04-05 11:35:33,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104723: learning rate 0.0000
[2019-04-05 11:35:34,135] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104992: loss 2.0619
[2019-04-05 11:35:34,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104993: learning rate 0.0000
[2019-04-05 11:35:35,008] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 105487: loss 0.5851
[2019-04-05 11:35:35,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 105487: learning rate 0.0000
[2019-04-05 11:35:35,144] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 105564: loss 0.2268
[2019-04-05 11:35:35,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 105564: learning rate 0.0000
[2019-04-05 11:35:35,347] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8148144e-09 6.6184698e-06 3.4902274e-04 9.9934071e-01 3.0304978e-04
 3.4382762e-08 4.3061064e-07], sum to 1.0000
[2019-04-05 11:35:35,347] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1351
[2019-04-05 11:35:35,367] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 27.66666666666667, 0.0, 0.0, 20.0, 21.70987753731541, -0.5248816917734495, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [-3.5, 28.5, 0.0, 0.0, 20.0, 21.18540221573435, -0.5687331397213538, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.36565096952908593, 0.285, 0.0, 0.0, 0.16666666666666666, 0.26545018464452913, 0.31042228675954875, 1.0, 1.0, 0.0], 
reward next is 0.1698, 
noisyNet noise sample is [array([0.10050151], dtype=float32), -0.09251293]. 
=============================================
[2019-04-05 11:35:35,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[44.37479 ]
 [44.499348]
 [44.64318 ]
 [44.80493 ]
 [44.919308]], R is [[43.92121887]
 [44.09033203]
 [44.37988663]
 [44.79323196]
 [45.20244217]].
[2019-04-05 11:35:36,198] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 106189: loss 3.3807
[2019-04-05 11:35:36,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 106189: learning rate 0.0000
[2019-04-05 11:35:37,694] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 107075: loss 2.2079
[2019-04-05 11:35:37,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 107075: learning rate 0.0000
[2019-04-05 11:35:38,913] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 107784: loss 8.7007
[2019-04-05 11:35:38,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 107784: learning rate 0.0000
[2019-04-05 11:35:40,830] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 108906: loss 4.1456
[2019-04-05 11:35:40,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 108907: learning rate 0.0000
[2019-04-05 11:35:41,751] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 109449: loss 6.7707
[2019-04-05 11:35:41,752] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 109449: learning rate 0.0000
[2019-04-05 11:35:41,851] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 109512: loss 8.6505
[2019-04-05 11:35:41,855] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 109514: learning rate 0.0000
[2019-04-05 11:35:41,912] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 109549: loss 1.1324
[2019-04-05 11:35:41,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 109552: learning rate 0.0000
[2019-04-05 11:35:42,264] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 109768: loss 6.8514
[2019-04-05 11:35:42,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 109769: learning rate 0.0000
[2019-04-05 11:35:42,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.6256656e-08 4.9396847e-05 1.7395546e-03 9.9550569e-01 2.7051242e-03
 1.3287870e-07 2.2608591e-08], sum to 1.0000
[2019-04-05 11:35:42,423] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0405
[2019-04-05 11:35:42,428] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 34.5, 15.33333333333334, 38.00000000000001, 23.5, 24.85354106382262, 0.2493585079675137, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 4125000.0000, 
sim time next is 4125600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 23.5, 24.87798746526741, 0.2402956987319537, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.34, 0.0, 0.0, 0.4583333333333333, 0.5731656221056175, 0.5800985662439846, 1.0, 1.0, 0.0], 
reward next is 0.3571, 
noisyNet noise sample is [array([0.63319314], dtype=float32), -0.23691297]. 
=============================================
[2019-04-05 11:35:42,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8809693e-05 3.8390569e-03 2.1968123e-02 9.6743023e-01 6.5735234e-03
 8.4507592e-05 8.5670821e-05], sum to 1.0000
[2019-04-05 11:35:42,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3223
[2019-04-05 11:35:42,599] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 43.33333333333334, 102.6666666666667, 602.6666666666667, 19.5, 19.11415504368301, -1.049292143288157, 0.0, 1.0, 18709.60950795991], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4180200.0000, 
sim time next is 4180800.0000, 
raw observation next is [-3.333333333333333, 41.66666666666667, 105.3333333333333, 631.3333333333333, 19.5, 19.09427195484241, -1.03540785560362, 0.0, 1.0, 9353.642936797558], 
processed observation next is [0.0, 0.391304347826087, 0.37026777469990774, 0.41666666666666674, 0.351111111111111, 0.6976058931860036, 0.125, 0.09118932957020072, 0.1548640481321267, 0.0, 1.0, 0.04454115684189313], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.3177247], dtype=float32), 1.4590825]. 
=============================================
[2019-04-05 11:35:43,412] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 110470: loss 7.1556
[2019-04-05 11:35:43,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 110471: learning rate 0.0000
[2019-04-05 11:35:43,643] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7910219e-11 7.6701972e-06 3.2948516e-04 9.9963808e-01 2.4694778e-05
 8.1615763e-09 2.6460512e-08], sum to 1.0000
[2019-04-05 11:35:43,643] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3396
[2019-04-05 11:35:43,666] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.833333333333333, 28.5, 115.3333333333333, 833.6666666666666, 24.5, 24.03444747521497, 0.2218490928812792, 1.0, 1.0, 127636.911131577], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 4021800.0000, 
sim time next is 4022400.0000, 
raw observation next is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 24.5, 24.51444348953224, 0.2890278380586347, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3610341643582641, 0.28, 0.38222222222222235, 0.9191528545119706, 0.5416666666666666, 0.5428702907943533, 0.5963426126862116, 1.0, 1.0, 0.0], 
reward next is 0.2143, 
noisyNet noise sample is [array([1.2097379], dtype=float32), -0.8207383]. 
=============================================
[2019-04-05 11:35:44,708] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 111227: loss 2.8046
[2019-04-05 11:35:44,710] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 111229: learning rate 0.0000
[2019-04-05 11:35:45,547] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111755: loss 6.5271
[2019-04-05 11:35:45,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111755: learning rate 0.0000
[2019-04-05 11:35:45,906] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111986: loss 0.0136
[2019-04-05 11:35:45,909] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111986: learning rate 0.0000
[2019-04-05 11:35:46,838] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112584: loss 7.9850
[2019-04-05 11:35:46,840] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112584: learning rate 0.0000
[2019-04-05 11:35:47,375] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112924: loss 6.4564
[2019-04-05 11:35:47,377] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112925: learning rate 0.0000
[2019-04-05 11:35:47,773] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6668101e-07 7.6607475e-04 2.4911826e-03 9.9211264e-01 4.6259956e-03
 1.0256194e-06 2.7963583e-06], sum to 1.0000
[2019-04-05 11:35:47,774] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4310
[2019-04-05 11:35:47,785] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.0, 53.5, 121.0, 822.0, 20.5, 20.30473484468224, -0.7379509734477107, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4275000.0000, 
sim time next is 4275600.0000, 
raw observation next is [6.333333333333333, 53.0, 120.8333333333333, 826.1666666666666, 20.5, 20.33770366240637, -0.7310775501046015, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6380424746075716, 0.53, 0.4027777777777777, 0.9128913443830571, 0.20833333333333334, 0.1948086385338641, 0.25630748329846614, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.8136435], dtype=float32), 1.7153347]. 
=============================================
[2019-04-05 11:35:48,290] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 113503: loss 8.0326
[2019-04-05 11:35:48,292] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 113503: learning rate 0.0000
[2019-04-05 11:35:48,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4999212e-07 2.7890128e-04 2.3787945e-04 9.9870729e-01 7.7331811e-04
 3.7705425e-07 2.0615846e-06], sum to 1.0000
[2019-04-05 11:35:48,546] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0309
[2019-04-05 11:35:48,558] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 71.0, 0.0, 0.0, 19.5, 19.1266178581943, -1.095131690203961, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4330800.0000, 
sim time next is 4331400.0000, 
raw observation next is [3.983333333333333, 70.83333333333334, 0.0, 0.0, 19.5, 19.06961238072983, -1.039557532064954, 0.0, 1.0, 196354.7898125332], 
processed observation next is [1.0, 0.13043478260869565, 0.5729455216989844, 0.7083333333333335, 0.0, 0.0, 0.125, 0.0891343650608191, 0.15348082264501536, 0.0, 1.0, 0.9350228086311105], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.51228946], dtype=float32), -0.864943]. 
=============================================
[2019-04-05 11:35:49,323] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 114178: loss 0.1509
[2019-04-05 11:35:49,324] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 114178: learning rate 0.0000
[2019-04-05 11:35:50,040] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 114657: loss 0.1946
[2019-04-05 11:35:50,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 114657: learning rate 0.0000
[2019-04-05 11:35:51,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.41942644e-09 1.13550734e-04 4.77429676e-05 9.99803245e-01
 3.53369942e-05 2.35132580e-09 3.20885434e-08], sum to 1.0000
[2019-04-05 11:35:51,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3680
[2019-04-05 11:35:51,792] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 40.0, 0.0, 0.0, 24.5, 23.96153956222382, 0.1115456418127399, 0.0, 1.0, 29240.52932749995], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 4149600.0000, 
sim time next is 4150200.0000, 
raw observation next is [-1.0, 39.5, 0.0, 0.0, 24.5, 23.9792010127537, 0.1068731575445723, 0.0, 1.0, 22450.91184039354], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.395, 0.0, 0.0, 0.5416666666666666, 0.4982667510628082, 0.5356243858481907, 0.0, 1.0, 0.10690910400187399], 
reward next is 0.2143, 
noisyNet noise sample is [array([-1.4674852], dtype=float32), 0.052390702]. 
=============================================
[2019-04-05 11:35:52,446] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 116249: loss 0.5596
[2019-04-05 11:35:52,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 116249: learning rate 0.0000
[2019-04-05 11:35:52,544] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.07810455e-10 7.96102631e-06 3.62959918e-06 9.99976039e-01
 1.23901955e-05 2.03286876e-09 4.69110306e-09], sum to 1.0000
[2019-04-05 11:35:52,547] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5487
[2019-04-05 11:35:52,554] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.100000000000001, 61.33333333333334, 0.0, 0.0, 20.0, 22.49432049537901, -0.2654422361144958, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4400400.0000, 
sim time next is 4401000.0000, 
raw observation next is [8.95, 61.5, 0.0, 0.0, 20.0, 22.32105845023746, -0.2883503293264756, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7105263157894738, 0.615, 0.0, 0.0, 0.16666666666666666, 0.3600882041864549, 0.4038832235578415, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.02636952], dtype=float32), -0.7208545]. 
=============================================
[2019-04-05 11:35:52,556] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 116325: loss 0.3012
[2019-04-05 11:35:52,558] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 116326: learning rate 0.0000
[2019-04-05 11:35:52,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[55.4907  ]
 [55.624035]
 [55.733448]
 [56.011112]
 [56.3176  ]], R is [[55.18667984]
 [55.49195862]
 [55.79418182]
 [56.09338379]
 [56.38959503]].
[2019-04-05 11:35:52,997] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 116608: loss 0.0438
[2019-04-05 11:35:52,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 116608: learning rate 0.0000
[2019-04-05 11:35:53,575] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 116978: loss 0.4178
[2019-04-05 11:35:53,577] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 116978: learning rate 0.0000
[2019-04-05 11:35:53,897] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 117182: loss 1.1324
[2019-04-05 11:35:53,900] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 117182: learning rate 0.0000
[2019-04-05 11:35:54,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0445729e-08 4.1237436e-04 7.6525501e-04 9.9663311e-01 2.1888604e-03
 1.5696509e-07 2.0294610e-07], sum to 1.0000
[2019-04-05 11:35:54,044] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1277
[2019-04-05 11:35:54,049] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 20.5, 20.9376946517625, -0.595279463034963, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4414200.0000, 
sim time next is 4414800.0000, 
raw observation next is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 20.5, 20.91534283538955, -0.6063945648431767, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6214219759926132, 0.6633333333333334, 0.0, 0.0, 0.20833333333333334, 0.24294523628246237, 0.29786847838560776, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-1.1021811], dtype=float32), 1.6351646]. 
=============================================
[2019-04-05 11:35:54,517] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 117578: loss 0.3404
[2019-04-05 11:35:54,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 117578: learning rate 0.0000
[2019-04-05 11:35:55,175] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 118006: loss 0.0454
[2019-04-05 11:35:55,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 118006: learning rate 0.0000
[2019-04-05 11:35:55,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7867569e-10 6.6979278e-06 2.0117972e-05 9.9994004e-01 3.3077100e-05
 4.0699125e-10 4.4571902e-09], sum to 1.0000
[2019-04-05 11:35:55,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8362
[2019-04-05 11:35:55,919] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.0, 49.0, 0.0, 0.0, 20.5, 22.56114422178283, -0.2524363000605193, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4644000.0000, 
sim time next is 4644600.0000, 
raw observation next is [3.833333333333333, 49.66666666666667, 0.0, 0.0, 20.5, 22.59607065721067, -0.2537613864120465, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5687903970452447, 0.4966666666666667, 0.0, 0.0, 0.20833333333333334, 0.38300588810088926, 0.4154128711959845, 1.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.5190001], dtype=float32), -0.3217846]. 
=============================================
[2019-04-05 11:35:56,113] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9710985e-09 1.1518301e-04 4.7584280e-04 9.9914443e-01 2.6450830e-04
 2.5403471e-08 1.9196882e-08], sum to 1.0000
[2019-04-05 11:35:56,114] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2057
[2019-04-05 11:35:56,126] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 75.0, 25.0, 55.0, 20.5, 21.18948184149182, -0.6660834254346212, 1.0, 1.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4469400.0000, 
sim time next is 4470000.0000, 
raw observation next is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 20.5, 21.00545855584298, -0.6714911191280867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.74, 0.06944444444444446, 0.050644567219152864, 0.20833333333333334, 0.25045487965358176, 0.27616962695730446, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.870471], dtype=float32), -0.21512045]. 
=============================================
[2019-04-05 11:35:56,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.54068 ]
 [54.645157]
 [54.790047]
 [54.96867 ]
 [55.06718 ]], R is [[53.89533997]
 [53.35638809]
 [52.82282639]
 [52.29459763]
 [51.77165222]].
[2019-04-05 11:35:56,290] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 118742: loss 0.1659
[2019-04-05 11:35:56,293] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 118742: learning rate 0.0000
[2019-04-05 11:35:57,347] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 119422: loss 0.0015
[2019-04-05 11:35:57,350] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 119423: learning rate 0.0000
[2019-04-05 11:35:57,713] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119647: loss 0.8314
[2019-04-05 11:35:57,716] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119651: learning rate 0.0000
[2019-04-05 11:35:58,251] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-05 11:35:58,253] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:35:58,254] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:35:58,255] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:35:58,255] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:35:58,256] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:35:58,256] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:35:58,260] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-05 11:35:58,275] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-05 11:35:58,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-05 11:37:01,833] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.07303619]
[2019-04-05 11:37:01,834] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.5954801506666667, 93.49597317000001, 75.69157638333334, 62.1545135, 21.5, 20.73195628365836, -0.7403236562390617, 1.0, 1.0, 0.0]
[2019-04-05 11:37:01,834] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:37:01,835] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.6253221e-08 2.4432689e-04 6.3657324e-04 9.9794096e-01 1.1771346e-03
 3.5348555e-07 6.4590319e-07], sampled 0.7998585224418789
[2019-04-05 11:37:16,918] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 4588.9907 175389681.2107 -938.0857
[2019-04-05 11:37:17,443] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5535.5655 178151240.6651 -2583.7803
[2019-04-05 11:37:36,591] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4769.0499 213864657.3360 -1562.3521
[2019-04-05 11:37:37,615] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 120000, evaluation results [120000.0, 5535.565471702825, 178151240.66508207, -2583.780291800658, 4588.990659128204, 175389681.21066055, -938.0857456281128, 4769.049949139736, 213864657.33597332, -1562.3521298764674]
[2019-04-05 11:37:37,623] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120005: loss 0.0168
[2019-04-05 11:37:37,626] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120006: learning rate 0.0000
[2019-04-05 11:37:37,796] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8751755e-10 6.1159512e-06 3.5099576e-05 9.9990547e-01 5.3295589e-05
 3.0971144e-08 2.4518938e-08], sum to 1.0000
[2019-04-05 11:37:37,798] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-05 11:37:37,808] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.666666666666667, 52.5, 120.6666666666667, 830.3333333333334, 25.0, 24.2650542740905, 0.1753950565900063, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 4276200.0000, 
sim time next is 4276800.0000, 
raw observation next is [7.0, 52.0, 120.5, 834.5, 25.0, 24.26822753219311, 0.1804313852491891, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.40166666666666667, 0.9220994475138121, 0.5833333333333334, 0.5223522943494258, 0.560143795083063, 0.0, 1.0, 0.0], 
reward next is 0.1429, 
noisyNet noise sample is [array([-0.08040184], dtype=float32), -0.5396177]. 
=============================================
[2019-04-05 11:37:38,285] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120432: loss 0.0540
[2019-04-05 11:37:38,287] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120432: learning rate 0.0000
[2019-04-05 11:37:38,492] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 120564: loss 6.6337
[2019-04-05 11:37:38,494] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 120564: learning rate 0.0000
[2019-04-05 11:37:38,849] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120797: loss 0.0911
[2019-04-05 11:37:38,852] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120798: learning rate 0.0000
[2019-04-05 11:37:41,093] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 122226: loss 4.5447
[2019-04-05 11:37:41,096] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 122227: learning rate 0.0000
[2019-04-05 11:37:41,244] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9405930e-09 5.5033128e-05 7.1131886e-05 9.9979025e-01 8.3465195e-05
 6.2882775e-09 2.3384962e-08], sum to 1.0000
[2019-04-05 11:37:41,246] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3941
[2019-04-05 11:37:41,253] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.1666666666666666, 93.33333333333334, 21.0, 0.0, 20.5, 20.27198588669926, -0.809205233575902, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4693800.0000, 
sim time next is 4694400.0000, 
raw observation next is [0.0, 92.0, 31.5, 0.0, 20.5, 20.21163181312917, -0.8209537373856609, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.105, 0.0, 0.20833333333333334, 0.1843026510940975, 0.2263487542047797, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8425651], dtype=float32), -1.1767207]. 
=============================================
[2019-04-05 11:37:41,850] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 122727: loss 2.3036
[2019-04-05 11:37:41,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 122728: learning rate 0.0000
[2019-04-05 11:37:44,530] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 124403: loss 2.9887
[2019-04-05 11:37:44,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 124405: learning rate 0.0000
[2019-04-05 11:37:44,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1073902e-08 3.4783399e-05 4.3269691e-05 9.9975151e-01 1.7025167e-04
 1.6603014e-08 1.8281013e-07], sum to 1.0000
[2019-04-05 11:37:44,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0978
[2019-04-05 11:37:44,834] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 52.33333333333333, 0.0, 0.0, 19.0, 20.9111042529228, -0.6165212636394338, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4650000.0000, 
sim time next is 4650600.0000, 
raw observation next is [2.166666666666667, 52.16666666666667, 0.0, 0.0, 19.0, 20.82084251798566, -0.6334725390281853, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5226223453370269, 0.5216666666666667, 0.0, 0.0, 0.08333333333333333, 0.23507020983213836, 0.2888424869906049, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1687202], dtype=float32), 0.1143543]. 
=============================================
[2019-04-05 11:37:45,063] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 124753: loss 6.9328
[2019-04-05 11:37:45,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 124753: learning rate 0.0000
[2019-04-05 11:37:45,080] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 124765: loss 8.9354
[2019-04-05 11:37:45,082] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 124766: learning rate 0.0000
[2019-04-05 11:37:45,419] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 124985: loss 3.5601
[2019-04-05 11:37:45,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 124986: learning rate 0.0000
[2019-04-05 11:37:45,497] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 125040: loss 5.3620
[2019-04-05 11:37:45,498] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 125040: learning rate 0.0000
[2019-04-05 11:37:47,757] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 126439: loss 10.5651
[2019-04-05 11:37:47,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 126440: learning rate 0.0000
[2019-04-05 11:37:47,783] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 126451: loss 1.0877
[2019-04-05 11:37:47,786] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 126451: learning rate 0.0000
[2019-04-05 11:37:48,113] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 126652: loss 2.8374
[2019-04-05 11:37:48,115] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 126654: learning rate 0.0000
[2019-04-05 11:37:48,912] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127136: loss 5.0814
[2019-04-05 11:37:48,914] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127136: learning rate 0.0000
[2019-04-05 11:37:49,321] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 127395: loss 6.3837
[2019-04-05 11:37:49,324] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 127398: learning rate 0.0000
[2019-04-05 11:37:49,601] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127583: loss 6.5453
[2019-04-05 11:37:49,602] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127583: learning rate 0.0000
[2019-04-05 11:37:49,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8034105e-07 5.6874671e-04 1.0566396e-03 9.9753642e-01 8.3602301e-04
 1.1773033e-06 7.4006419e-07], sum to 1.0000
[2019-04-05 11:37:49,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5689
[2019-04-05 11:37:49,613] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 39.0, 211.6666666666667, 604.5, 19.0, 18.96361138802809, -1.012935966498755, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4800000.0000, 
sim time next is 4800600.0000, 
raw observation next is [2.5, 38.5, 220.0, 568.0, 19.0, 18.97456997851008, -1.012004702409463, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5318559556786704, 0.385, 0.7333333333333333, 0.6276243093922652, 0.08333333333333333, 0.08121416487583992, 0.16266509919684569, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9174283], dtype=float32), -1.455843]. 
=============================================
[2019-04-05 11:37:50,001] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127827: loss 6.4091
[2019-04-05 11:37:50,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127827: learning rate 0.0000
[2019-04-05 11:37:50,754] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128288: loss 6.5321
[2019-04-05 11:37:50,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128289: learning rate 0.0000
[2019-04-05 11:37:53,137] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 129762: loss 0.3997
[2019-04-05 11:37:53,138] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 129762: learning rate 0.0000
[2019-04-05 11:37:55,029] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 130924: loss 0.0010
[2019-04-05 11:37:55,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 130929: learning rate 0.0000
[2019-04-05 11:37:55,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:37:55,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:37:55,348] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-05 11:37:57,420] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:37:57,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:37:57,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-05 11:37:57,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:37:57,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:37:57,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-05 11:37:57,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:37:57,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:37:57,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-05 11:37:58,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:37:58,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:37:58,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-05 11:37:59,709] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 133480: loss 0.8810
[2019-04-05 11:37:59,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 133480: learning rate 0.0000
[2019-04-05 11:38:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:00,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:00,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-05 11:38:00,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:00,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:00,752] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-05 11:38:00,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.26451319e-09 3.65939450e-05 1.45142796e-04 9.99403119e-01
 4.14744631e-04 1.11124855e-07 3.15211111e-07], sum to 1.0000
[2019-04-05 11:38:00,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1357
[2019-04-05 11:38:00,815] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 23.0, 22.54854903782572, -0.3573699871010527, 0.0, 1.0, 24184.93750323702], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4942800.0000, 
sim time next is 4943400.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 23.0, 22.53207250354024, -0.3638714350098875, 0.0, 1.0, 38120.26185216635], 
processed observation next is [1.0, 0.21739130434782608, 0.4025854108956602, 0.46666666666666673, 0.0, 0.0, 0.4166666666666667, 0.37767270862835317, 0.37870952166337085, 0.0, 1.0, 0.1815250564388874], 
reward next is 0.4286, 
noisyNet noise sample is [array([-0.50976866], dtype=float32), 1.7797221]. 
=============================================
[2019-04-05 11:38:01,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:01,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:01,210] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-05 11:38:01,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:01,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:01,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-05 11:38:01,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:01,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:01,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-05 11:38:02,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:02,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:02,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-05 11:38:02,712] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 134322: loss -7.2442
[2019-04-05 11:38:02,713] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 134322: learning rate 0.0000
[2019-04-05 11:38:03,206] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 134405: loss 0.3837
[2019-04-05 11:38:03,207] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 134405: learning rate 0.0000
[2019-04-05 11:38:06,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:06,858] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:06,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-05 11:38:07,120] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2745825e-10 2.1034468e-06 2.1257518e-05 9.9980217e-01 1.7446389e-04
 6.0113163e-09 3.4528792e-08], sum to 1.0000
[2019-04-05 11:38:07,123] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1505
[2019-04-05 11:38:07,136] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.7, 89.0, 0.0, 0.0, 20.0, 19.7247730556479, -0.8238318382159169, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 72000.0000, 
sim time next is 72600.0000, 
raw observation next is [2.516666666666667, 88.33333333333334, 0.0, 0.0, 20.0, 19.96606090266485, -0.8081681652069493, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5323176361957526, 0.8833333333333334, 0.0, 0.0, 0.16666666666666666, 0.16383840855540419, 0.23061061159768356, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.59467983], dtype=float32), 1.4000503]. 
=============================================
[2019-04-05 11:38:09,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:09,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:09,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-05 11:38:09,549] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2416395e-10 4.6089976e-05 1.5273949e-04 9.9932861e-01 4.7255025e-04
 2.2229079e-08 4.4212247e-09], sum to 1.0000
[2019-04-05 11:38:09,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9707
[2019-04-05 11:38:09,570] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 34.5, 0.0, 0.0, 24.5, 23.86155226271001, 0.1329838018862642, 0.0, 1.0, 197699.7011162001], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 5007000.0000, 
sim time next is 5007600.0000, 
raw observation next is [3.0, 34.0, 0.0, 0.0, 24.5, 23.87468874894382, 0.1771945525046391, 0.0, 1.0, 194734.3580899571], 
processed observation next is [1.0, 1.0, 0.5457063711911359, 0.34, 0.0, 0.0, 0.5416666666666666, 0.4895573957453185, 0.5590648508348797, 0.0, 1.0, 0.9273064670950338], 
reward next is 0.2143, 
noisyNet noise sample is [array([2.9289536], dtype=float32), 0.19689938]. 
=============================================
[2019-04-05 11:38:14,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:38:14,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:14,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-05 11:38:14,958] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.0889536e-09 3.5922966e-04 1.0560482e-04 9.9872190e-01 8.1292086e-04
 1.7060225e-07 2.1560058e-07], sum to 1.0000
[2019-04-05 11:38:14,960] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4126
[2019-04-05 11:38:14,974] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.200000000000001, 86.0, 90.0, 0.0, 21.0, 19.95069690436844, -0.7993924930799451, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 47400.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 21.0, 19.94187308989716, -0.8050273944163281, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 0.25, 0.16182275749143007, 0.2316575351945573, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.05024171], dtype=float32), 1.872448]. 
=============================================
[2019-04-05 11:38:14,984] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[47.559937]
 [47.51983 ]
 [47.437256]
 [47.383736]
 [47.312424]], R is [[47.86754608]
 [48.10315704]
 [48.33641434]
 [48.56733704]
 [48.79595184]].
[2019-04-05 11:38:16,728] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 11:38:16,730] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:38:16,730] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:16,731] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:38:16,731] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:38:16,731] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:16,731] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:38:16,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run8
[2019-04-05 11:38:16,746] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run8
[2019-04-05 11:38:16,767] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run8
[2019-04-05 11:38:35,905] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.07700711]
[2019-04-05 11:38:35,905] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 20.0, 20.37431730784411, -0.8806273870516773, 1.0, 1.0, 0.0]
[2019-04-05 11:38:35,906] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:38:35,907] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.4300165e-08 1.4076794e-04 3.0795496e-04 9.9899429e-01 5.5662007e-04
 1.1679362e-07 2.2732446e-07], sampled 0.027691633172388097
[2019-04-05 11:38:44,821] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.07700711]
[2019-04-05 11:38:44,821] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.216666666666667, 80.16666666666667, 67.33333333333333, 222.3333333333333, 22.0, 21.39705612306622, -0.5245660686316707, 0.0, 1.0, 0.0]
[2019-04-05 11:38:44,821] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:38:44,823] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.1767011e-08 1.4399874e-04 6.8224443e-04 9.9793363e-01 1.2393920e-03
 2.9865797e-07 4.7490724e-07], sampled 0.13757982390802814
[2019-04-05 11:39:33,880] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.07700711]
[2019-04-05 11:39:33,880] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.0, 72.0, 0.0, 0.0, 21.5, 21.52916230408688, -0.4576814478148303, 1.0, 1.0, 0.0]
[2019-04-05 11:39:33,881] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:39:33,882] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.6737318e-09 5.6568104e-05 1.7353194e-04 9.9942821e-01 3.4146881e-04
 1.7154587e-08 4.4316305e-08], sampled 0.5536191226974352
[2019-04-05 11:39:40,915] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 3454.8434 196165260.8537 -158.6695
[2019-04-05 11:39:41,696] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5147.2339 189226653.9962 -2036.4401
[2019-04-05 11:39:43,080] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.07700711]
[2019-04-05 11:39:43,081] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [14.98632293333334, 31.53233336666667, 184.9446682766667, 516.2798791166666, 22.0, 24.18409242696872, 0.1250978891755142, 1.0, 1.0, 0.0]
[2019-04-05 11:39:43,081] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:39:43,082] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7475677e-10 1.0681607e-05 3.2927026e-05 9.9988735e-01 6.8979753e-05
 2.0697297e-09 4.1908690e-09], sampled 0.7392734253066331
[2019-04-05 11:39:53,769] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4705.7906 215218316.4436 -1483.6204
[2019-04-05 11:39:54,793] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 140000, evaluation results [140000.0, 5147.233875977486, 189226653.99621314, -2036.4401250991757, 3454.843435048902, 196165260.85371792, -158.66952414131882, 4705.790575564491, 215218316.4436169, -1483.6203756797781]
[2019-04-05 11:39:55,570] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8930998e-09 2.8662145e-05 3.7892078e-04 9.9245924e-01 7.1330862e-03
 2.1613760e-08 7.8808569e-08], sum to 1.0000
[2019-04-05 11:39:55,571] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1663
[2019-04-05 11:39:55,587] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 20.5, 19.94168568802422, -0.8783630777415699, 0.0, 1.0, 51245.44003027485], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 166800.0000, 
sim time next is 167400.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 20.5, 19.97964846647307, -0.8810253517328869, 0.0, 1.0, 27998.68156895331], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.20833333333333334, 0.16497070553942242, 0.20632488275570438, 0.0, 1.0, 0.13332705509025386], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.07242721], dtype=float32), -0.2353373]. 
=============================================
[2019-04-05 11:39:56,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:39:56,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:39:56,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-05 11:39:56,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:39:56,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:39:56,423] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-05 11:39:57,874] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9896317e-09 3.7404487e-04 3.5141347e-04 9.9704701e-01 2.2272482e-03
 1.0996462e-08 2.8792908e-07], sum to 1.0000
[2019-04-05 11:39:57,874] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1698
[2019-04-05 11:39:57,887] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.700000000000001, 61.0, 146.5, 169.0, 21.0, 21.15272826793866, -0.6529074323402403, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 148.0, 106.0, 21.0, 21.20751537505403, -0.6509058374992515, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.49333333333333335, 0.11712707182320442, 0.25, 0.26729294792116914, 0.28303138750024953, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.4177914], dtype=float32), 0.3231776]. 
=============================================
[2019-04-05 11:40:01,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3095406e-07 5.2232970e-03 9.5221112e-03 9.4322550e-01 4.2027671e-02
 3.2919397e-07 8.7249219e-07], sum to 1.0000
[2019-04-05 11:40:01,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0874
[2019-04-05 11:40:01,730] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.38333333333333, 67.5, 0.0, 0.0, 24.0, 21.53812479845233, -0.5390554500371093, 0.0, 1.0, 48718.57560225706], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 285000.0000, 
sim time next is 285600.0000, 
raw observation next is [-12.46666666666667, 68.0, 0.0, 0.0, 24.0, 21.46439481434999, -0.4729180752072561, 1.0, 1.0, 202257.8454299279], 
processed observation next is [1.0, 0.30434782608695654, 0.11726685133887339, 0.68, 0.0, 0.0, 0.5, 0.28869956786249923, 0.3423606415975813, 1.0, 1.0, 0.9631325972853709], 
reward next is 0.2857, 
noisyNet noise sample is [array([-1.3348824], dtype=float32), -0.3836867]. 
=============================================
[2019-04-05 11:40:03,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1598308e-08 1.6921730e-04 1.4513593e-04 9.9713576e-01 2.5492138e-03
 4.0516289e-07 2.6918386e-07], sum to 1.0000
[2019-04-05 11:40:03,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2596
[2019-04-05 11:40:03,609] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 93.0, 60.0, 0.0, 19.5, 18.74710275207358, -1.07915574953144, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 37800.0000, 
sim time next is 38400.0000, 
raw observation next is [7.699999999999999, 93.0, 62.5, 0.0, 19.5, 18.77826964910936, -1.082596028244627, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.20833333333333334, 0.0, 0.125, 0.0648558040924468, 0.139134657251791, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.2072047], dtype=float32), 0.71955353]. 
=============================================
[2019-04-05 11:40:06,754] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1760609e-08 4.0348424e-04 1.1633740e-04 9.9923384e-01 2.4540332e-04
 4.5894399e-08 8.3206993e-07], sum to 1.0000
[2019-04-05 11:40:06,758] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3115
[2019-04-05 11:40:06,788] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.0, 38.33333333333334, 31.66666666666666, 605.6666666666667, 19.0, 19.90404211510553, -1.111859724884124, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 402600.0000, 
sim time next is 403200.0000, 
raw observation next is [-8.9, 38.0, 29.0, 555.0, 19.0, 19.91638649446184, -1.057284547803184, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.38, 0.09666666666666666, 0.6132596685082873, 0.08333333333333333, 0.15969887453848663, 0.14757181739893865, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.20190908], dtype=float32), 0.22092684]. 
=============================================
[2019-04-05 11:40:12,429] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1426219e-08 2.2828580e-04 4.1528762e-04 9.9913663e-01 2.1867189e-04
 2.6239636e-07 7.5751871e-07], sum to 1.0000
[2019-04-05 11:40:12,432] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2788
[2019-04-05 11:40:12,496] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.633333333333334, 32.66666666666667, 55.50000000000001, 0.0, 22.0, 22.19627405990155, -0.577507996759608, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 465600.0000, 
sim time next is 466200.0000, 
raw observation next is [-5.35, 32.5, 62.0, 0.0, 22.0, 22.20442676882872, -0.5730117790341935, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31440443213296404, 0.325, 0.20666666666666667, 0.0, 0.3333333333333333, 0.35036889740239346, 0.3089960736552688, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09452754], dtype=float32), 0.2752766]. 
=============================================
[2019-04-05 11:40:13,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0292338e-06 4.2872196e-03 4.7138687e-03 9.8816597e-01 2.7565632e-03
 4.5417863e-05 2.7970489e-05], sum to 1.0000
[2019-04-05 11:40:13,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0285
[2019-04-05 11:40:13,860] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.783333333333333, 84.5, 0.0, 0.0, 19.0, 18.69964928578926, -1.206382247339422, 0.0, 1.0, 32540.4471802505], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 535800.0000, 
sim time next is 536400.0000, 
raw observation next is [1.6, 85.0, 0.0, 0.0, 19.0, 18.68439038589744, -1.207949656146082, 0.0, 1.0, 41728.38937231757], 
processed observation next is [0.0, 0.21739130434782608, 0.5069252077562327, 0.85, 0.0, 0.0, 0.08333333333333333, 0.05703253215812006, 0.09735011461797265, 0.0, 1.0, 0.1987066160586551], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9733802], dtype=float32), 0.5022892]. 
=============================================
[2019-04-05 11:40:19,333] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4902201e-08 1.9858293e-04 1.6552572e-03 9.9243623e-01 5.7070167e-03
 2.0624952e-06 8.4894270e-07], sum to 1.0000
[2019-04-05 11:40:19,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3715
[2019-04-05 11:40:19,380] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 80.5, 130.0, 396.0, 20.0, 19.28961400413294, -0.9937354896510131, 0.0, 1.0, 18687.76835058119], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 563400.0000, 
sim time next is 564000.0000, 
raw observation next is [-1.066666666666667, 80.33333333333333, 131.3333333333333, 429.1666666666666, 20.0, 19.29825080910625, -0.98155416386756, 0.0, 1.0, 21626.96904362931], 
processed observation next is [0.0, 0.5217391304347826, 0.43305632502308405, 0.8033333333333332, 0.4377777777777776, 0.4742173112338857, 0.16666666666666666, 0.10818756742552083, 0.17281527871081334, 0.0, 1.0, 0.10298556687442528], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.54705656], dtype=float32), -1.278111]. 
=============================================
[2019-04-05 11:40:19,390] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[42.87205 ]
 [42.849133]
 [42.769047]
 [42.662228]
 [42.545414]], R is [[43.39294815]
 [43.81616211]
 [44.23514557]
 [44.64993668]
 [45.06058121]].
[2019-04-05 11:40:24,137] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.06961311e-08 1.08106724e-04 1.79630914e-03 9.97179508e-01
 9.15518904e-04 2.71437955e-07 3.89487525e-07], sum to 1.0000
[2019-04-05 11:40:24,140] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5440
[2019-04-05 11:40:24,153] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 18.58925631062296, -1.248996988179476, 0.0, 1.0, 78771.95409290679], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 18.58248617728993, -1.244079205555992, 0.0, 1.0, 57678.50327422793], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.048540514774160805, 0.08530693148133602, 0.0, 1.0, 0.2746595394010854], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39077368], dtype=float32), 0.995711]. 
=============================================
[2019-04-05 11:40:24,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5301231e-09 2.3905025e-04 1.2729203e-04 9.9858946e-01 1.0438986e-03
 6.3050337e-08 3.0279145e-07], sum to 1.0000
[2019-04-05 11:40:24,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7253
[2019-04-05 11:40:24,199] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 71.33333333333334, 0.0, 0.0, 21.0, 20.50488766424343, -0.8352533194356302, 0.0, 1.0, 29850.29199380078], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 692400.0000, 
sim time next is 693000.0000, 
raw observation next is [-3.65, 71.5, 0.0, 0.0, 21.0, 20.46571462761382, -0.8400917168210756, 0.0, 1.0, 55833.51168678611], 
processed observation next is [1.0, 0.0, 0.3614958448753463, 0.715, 0.0, 0.0, 0.25, 0.2054762189678184, 0.21996942772630812, 0.0, 1.0, 0.2658738651751719], 
reward next is 0.7143, 
noisyNet noise sample is [array([1.2629961], dtype=float32), 0.43427503]. 
=============================================
[2019-04-05 11:40:24,209] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[44.934334]
 [45.3218  ]
 [45.119705]
 [45.29125 ]
 [45.154373]], R is [[45.13616943]
 [45.39909363]
 [45.6593895 ]
 [45.91708374]
 [46.17219925]].
[2019-04-05 11:40:35,216] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.4703913e-09 3.2194308e-05 4.0196053e-05 9.9976593e-01 1.6167649e-04
 2.9849538e-08 1.3213424e-09], sum to 1.0000
[2019-04-05 11:40:35,217] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2092
[2019-04-05 11:40:35,233] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.05, 85.5, 0.0, 0.0, 19.5, 19.22815462928155, -1.035494877355154, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 955800.0000, 
sim time next is 956400.0000, 
raw observation next is [6.233333333333333, 84.33333333333334, 0.0, 0.0, 19.5, 19.17396889107863, -1.006111533040428, 0.0, 1.0, 150020.7634386562], 
processed observation next is [1.0, 0.043478260869565216, 0.6352723915050786, 0.8433333333333334, 0.0, 0.0, 0.125, 0.09783074092321915, 0.164629488986524, 0.0, 1.0, 0.7143845878031247], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.24252029], dtype=float32), -0.18620346]. 
=============================================
[2019-04-05 11:40:35,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7533886e-11 9.3234403e-06 3.2652720e-06 9.9996793e-01 1.9536836e-05
 5.7438726e-10 7.6630995e-09], sum to 1.0000
[2019-04-05 11:40:35,403] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1924
[2019-04-05 11:40:35,435] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 20.0, 18.97890669105371, -1.042213803944302, 1.0, 1.0, 29609.74077222777], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 936000.0000, 
sim time next is 936600.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 20.0, 19.03953956094205, -1.041406783216789, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.16666666666666666, 0.08662829674517081, 0.15286440559440365, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.206749], dtype=float32), -2.852298]. 
=============================================
[2019-04-05 11:40:37,935] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-05 11:40:37,937] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:40:37,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:40:37,938] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:40:37,939] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:40:37,942] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run9
[2019-04-05 11:40:37,939] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:40:37,943] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:40:37,960] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run9
[2019-04-05 11:40:37,960] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run9
[2019-04-05 11:41:53,893] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5063.0346 173546028.1877 -1118.6560
[2019-04-05 11:41:58,378] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5381.4187 180853253.4662 -2437.5197
[2019-04-05 11:42:15,494] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4510.3989 214796845.7120 -1491.8179
[2019-04-05 11:42:16,519] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 160000, evaluation results [160000.0, 5381.418712401887, 180853253.46617, -2437.51965116557, 5063.03463587961, 173546028.18774143, -1118.6560416241018, 4510.398912362781, 214796845.71196514, -1491.8178884928423]
[2019-04-05 11:42:20,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3653866e-07 3.8276112e-04 6.2298658e-04 9.9385697e-01 5.1278966e-03
 1.1782751e-06 8.1065455e-06], sum to 1.0000
[2019-04-05 11:42:20,111] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6772
[2019-04-05 11:42:20,117] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.6, 77.0, 0.0, 0.0, 19.5, 19.52655579551581, -0.8347184237701724, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [11.6, 78.0, 0.0, 0.0, 19.5, 19.52202081404441, -0.8347110320322549, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.78, 0.0, 0.0, 0.125, 0.12683506783703416, 0.22176298932258168, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.9373131], dtype=float32), -0.037130132]. 
=============================================
[2019-04-05 11:42:21,688] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.17891329e-07 1.01888436e-04 3.90656904e-04 9.98655438e-01
 8.49651115e-04 1.37374172e-06 5.76149148e-07], sum to 1.0000
[2019-04-05 11:42:21,692] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2656
[2019-04-05 11:42:21,698] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.0, 65.0, 0.0, 0.0, 19.5, 20.32468623052151, -0.660976150612892, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1189800.0000, 
sim time next is 1190400.0000, 
raw observation next is [17.9, 65.66666666666667, 0.0, 0.0, 19.5, 20.3249257554347, -0.6628639487541748, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9584487534626038, 0.6566666666666667, 0.0, 0.0, 0.125, 0.19374381295289153, 0.27904535041527506, 0.0, 0.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.13368298], dtype=float32), 0.9028276]. 
=============================================
[2019-04-05 11:42:28,409] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6892831e-09 2.3991748e-05 3.8622748e-04 9.9831307e-01 1.2765038e-03
 1.8845945e-07 4.3288459e-08], sum to 1.0000
[2019-04-05 11:42:28,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2139
[2019-04-05 11:42:28,423] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 22.5, 22.33801447504793, -0.1715330760372883, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 1193400.0000, 
sim time next is 1194000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 22.5, 22.3230197217131, -0.1739937011041855, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.375, 0.36025164347609157, 0.4420020996319382, 0.0, 0.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.603841], dtype=float32), 0.6875366]. 
=============================================
[2019-04-05 11:42:28,436] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[45.9029  ]
 [45.89516 ]
 [45.869484]
 [45.86476 ]
 [45.82345 ]], R is [[45.99338531]
 [46.03345108]
 [46.0731163 ]
 [46.1123848 ]
 [46.15126038]].
[2019-04-05 11:42:29,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.3759567e-08 5.3731979e-05 7.0956320e-04 9.9894935e-01 2.8696479e-04
 5.8717376e-08 3.2323649e-07], sum to 1.0000
[2019-04-05 11:42:29,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9115
[2019-04-05 11:42:29,213] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 72.0, 0.0, 0.0, 20.5, 20.16001854276409, -0.8720041841798721, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 882600.0000, 
sim time next is 883200.0000, 
raw observation next is [-0.4, 72.0, 0.0, 0.0, 20.5, 20.27998978616542, -0.8689564298144612, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.45152354570637127, 0.72, 0.0, 0.0, 0.20833333333333334, 0.1899991488471183, 0.21034785672851294, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.6063004], dtype=float32), -0.8764995]. 
=============================================
[2019-04-05 11:42:32,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3780401e-08 2.8042281e-05 6.2759937e-04 9.9900395e-01 3.4016991e-04
 1.4503757e-07 1.3362227e-07], sum to 1.0000
[2019-04-05 11:42:32,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7825
[2019-04-05 11:42:32,430] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.63333333333333, 63.66666666666667, 37.5, 0.0, 21.0, 21.46123288868654, -0.3831234952669929, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1182000.0000, 
sim time next is 1182600.0000, 
raw observation next is [18.55, 64.0, 29.0, 0.0, 21.0, 21.45371097825365, -0.385214344173896, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.976454293628809, 0.64, 0.09666666666666666, 0.0, 0.25, 0.2878092481878041, 0.3715952186087013, 0.0, 0.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.37735933], dtype=float32), 0.20140617]. 
=============================================
[2019-04-05 11:42:37,230] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.31812874e-11 9.06463720e-06 1.22513375e-05 9.99938965e-01
 3.97430449e-05 6.25568417e-11 5.56707624e-10], sum to 1.0000
[2019-04-05 11:42:37,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4874
[2019-04-05 11:42:37,250] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 92.0, 12.0, 0.0, 21.0, 20.79799978125806, -0.564982268425174, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1325400.0000, 
sim time next is 1326000.0000, 
raw observation next is [0.9000000000000001, 92.0, 15.0, 0.0, 21.0, 20.75898294963606, -0.5249608204042403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.48753462603878117, 0.92, 0.05, 0.0, 0.25, 0.22991524580300501, 0.32501305986525325, 1.0, 1.0, 0.0], 
reward next is 0.4647, 
noisyNet noise sample is [array([-1.1795795], dtype=float32), 0.46626177]. 
=============================================
[2019-04-05 11:42:37,257] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[55.946564]
 [56.009724]
 [56.024765]
 [56.269875]
 [56.160294]], R is [[56.03610611]
 [55.54021072]
 [55.13156509]
 [54.79512787]
 [54.59028625]].
[2019-04-05 11:42:40,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1198227e-10 2.8439547e-06 4.3208278e-05 9.9984753e-01 1.0629748e-04
 1.3068454e-08 1.3329866e-08], sum to 1.0000
[2019-04-05 11:42:40,982] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3108
[2019-04-05 11:42:40,990] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 22.5, 22.45118958191622, -0.1553693473715935, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 22.5, 22.4138947999651, -0.1643662996466464, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.375, 0.3678245666637583, 0.44521123345111785, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.83493584], dtype=float32), -1.8531371]. 
=============================================
[2019-04-05 11:42:41,182] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.03258790e-09 5.28316605e-06 1.21437515e-04 9.95488584e-01
 4.38459450e-03 2.42916243e-09 3.59421612e-08], sum to 1.0000
[2019-04-05 11:42:41,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8318
[2019-04-05 11:42:41,196] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.85, 92.0, 53.0, 0.0, 20.0, 20.49955754829571, -0.7371981080493404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 20.0, 20.59520419651826, -0.7240756855043253, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5106186518928902, 0.92, 0.1838888888888889, 0.0, 0.16666666666666666, 0.21626701637652168, 0.2586414381652249, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1207838], dtype=float32), -1.9290912]. 
=============================================
[2019-04-05 11:42:43,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.8530354e-10 5.9068833e-07 2.5772528e-05 9.9993396e-01 3.9651455e-05
 5.7670517e-09 6.0735750e-09], sum to 1.0000
[2019-04-05 11:42:43,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5907
[2019-04-05 11:42:43,144] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.383333333333333, 92.0, 37.66666666666667, 0.0, 21.0, 21.46828306932244, -0.494338354928835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1673400.0000, 
sim time next is 1674000.0000, 
raw observation next is [2.2, 92.0, 41.5, 0.0, 21.0, 21.46343704576589, -0.4879138297220931, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5235457063711911, 0.92, 0.13833333333333334, 0.0, 0.25, 0.28861975381382415, 0.3373620567593023, 1.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.50037485], dtype=float32), 0.22540607]. 
=============================================
[2019-04-05 11:42:43,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[59.670662]
 [59.552906]
 [59.49163 ]
 [59.561012]
 [59.716484]], R is [[59.85372162]
 [59.96947098]
 [60.08406448]
 [60.12234497]
 [60.11980438]].
[2019-04-05 11:42:48,124] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 11:42:48,126] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:42:48,127] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:42:48,128] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:42:48,128] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:42:48,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:42:48,129] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:42:48,134] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run10
[2019-04-05 11:42:48,149] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run10
[2019-04-05 11:42:48,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run10
[2019-04-05 11:43:13,462] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08228093]
[2019-04-05 11:43:13,462] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.740665953666666, 99.25421882166667, 0.0, 0.0, 20.0, 20.02714134683854, -0.8808470160528025, 0.0, 1.0, 0.0]
[2019-04-05 11:43:13,462] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:43:13,462] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.8106257e-10 4.1082590e-06 2.0931318e-05 9.9986970e-01 1.0518752e-04
 1.8753694e-09 3.1169969e-09], sampled 0.7768849846024675
[2019-04-05 11:43:32,208] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08228093]
[2019-04-05 11:43:32,209] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.883333333333333, 77.66666666666666, 224.0, 269.6666666666667, 20.0, 20.01557649955018, -0.9301266774896894, 1.0, 1.0, 0.0]
[2019-04-05 11:43:32,209] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:43:32,210] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.5856083e-09 1.7072402e-05 4.7434420e-05 9.9974161e-01 1.9384468e-04
 1.6652514e-08 3.2777134e-08], sampled 0.25580536697399137
[2019-04-05 11:43:40,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08228093]
[2019-04-05 11:43:40,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.5333333333333334, 41.66666666666667, 229.6666666666667, 62.83333333333334, 21.0, 21.76045917500076, -0.6263517521758702, 1.0, 1.0, 0.0]
[2019-04-05 11:43:40,527] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:43:40,528] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5406048e-08 3.9452520e-05 1.1100467e-04 9.9946862e-01 3.8078066e-04
 5.3010972e-08 1.2301254e-07], sampled 0.7530498909768517
[2019-04-05 11:44:02,380] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5392.9012 163621126.8323 -1457.5389
[2019-04-05 11:44:16,848] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08228093]
[2019-04-05 11:44:16,849] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 44.66666666666667, 0.0, 0.0, 21.0, 20.61582270143009, -0.715780874354147, 0.0, 1.0, 196354.9986045974]
[2019-04-05 11:44:16,849] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:44:16,850] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.6801648e-09 9.8867404e-06 5.4188331e-05 9.9967468e-01 2.6116410e-04
 2.3328223e-08 3.4457159e-08], sampled 0.9539434918714962
[2019-04-05 11:44:18,605] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5177.2513 198245614.3530 -2183.9924
[2019-04-05 11:44:19,060] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4756.7412 199930891.2397 -1568.6417
[2019-04-05 11:44:20,082] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 180000, evaluation results [180000.0, 4756.741223669478, 199930891.2396802, -1568.6417453727556, 5392.901249960787, 163621126.83234322, -1457.538940646907, 5177.251269850934, 198245614.3530378, -2183.992449667529]
[2019-04-05 11:44:20,705] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4541308e-09 8.9488094e-06 1.1977943e-04 9.9977404e-01 9.7193319e-05
 9.6277963e-09 1.5782261e-08], sum to 1.0000
[2019-04-05 11:44:20,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8756
[2019-04-05 11:44:20,717] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.733333333333333, 64.0, 152.0, 0.6666666666666665, 19.0, 19.29628863096727, -1.123521652976128, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1948800.0000, 
sim time next is 1949400.0000, 
raw observation next is [-3.65, 63.5, 137.0, 0.0, 19.0, 19.37591569195842, -1.113425542455995, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3614958448753463, 0.635, 0.45666666666666667, 0.0, 0.08333333333333333, 0.11465964099653488, 0.12885815251466837, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.09565816], dtype=float32), -1.4991822]. 
=============================================
[2019-04-05 11:44:22,229] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3752878e-11 8.8013685e-06 1.4116901e-05 9.9872166e-01 1.2553186e-03
 8.6703539e-10 4.7010982e-09], sum to 1.0000
[2019-04-05 11:44:22,229] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8820
[2019-04-05 11:44:22,250] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 96.33333333333333, 0.0, 0.0, 21.0, 21.19463575145689, -0.4988304947290901, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1650000.0000, 
sim time next is 1650600.0000, 
raw observation next is [6.9, 96.5, 0.0, 0.0, 21.0, 21.23135783776353, -0.4983739947924797, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6537396121883658, 0.965, 0.0, 0.0, 0.25, 0.2692798198136274, 0.33387533506917344, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-1.5901365], dtype=float32), -1.1065503]. 
=============================================
[2019-04-05 11:44:24,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.24712140e-09 2.49009827e-05 1.68999017e-04 9.99509811e-01
 2.96202837e-04 1.39494745e-08 5.74237511e-08], sum to 1.0000
[2019-04-05 11:44:24,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-05 11:44:24,604] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 102.0, 0.0, 19.0, 19.31467535570278, -1.16804391980518, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2025000.0000, 
sim time next is 2025600.0000, 
raw observation next is [-5.6, 83.0, 109.5, 0.0, 19.0, 19.31248519639609, -1.173469868868593, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.365, 0.0, 0.08333333333333333, 0.10937376636634077, 0.10884337704380236, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.57654625], dtype=float32), 1.041202]. 
=============================================
[2019-04-05 11:44:28,238] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6657922e-11 5.9809281e-06 8.6548907e-06 9.9983871e-01 1.4657511e-04
 1.9701210e-09 1.9339963e-08], sum to 1.0000
[2019-04-05 11:44:28,239] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4372
[2019-04-05 11:44:28,256] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.300000000000001, 79.66666666666667, 0.0, 0.0, 22.0, 21.27737234245185, -0.6232184068676737, 0.0, 1.0, 47433.0779739918], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1902000.0000, 
sim time next is 1902600.0000, 
raw observation next is [-7.3, 78.5, 0.0, 0.0, 22.0, 21.326397240371, -0.6190935185957956, 0.0, 1.0, 47400.27871264315], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.785, 0.0, 0.0, 0.3333333333333333, 0.2771997700309168, 0.29363549380140147, 0.0, 1.0, 0.22571561291734835], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.6155784], dtype=float32), -0.89554465]. 
=============================================
[2019-04-05 11:44:29,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4103766e-09 1.1714778e-05 1.6495699e-04 9.9964690e-01 1.7641862e-04
 5.6120459e-09 2.2137009e-08], sum to 1.0000
[2019-04-05 11:44:29,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7551
[2019-04-05 11:44:29,648] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 91.0, 17.5, 11.0, 22.0, 21.89873048096243, -0.5786309692434101, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1929600.0000, 
sim time next is 1930200.0000, 
raw observation next is [-9.4, 90.16666666666667, 22.66666666666667, 14.33333333333334, 22.0, 21.83010429181498, -0.5982829118123768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20221606648199447, 0.9016666666666667, 0.07555555555555557, 0.015837937384898717, 0.3333333333333333, 0.3191753576512483, 0.30057236272920773, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.38773257], dtype=float32), -2.1301124]. 
=============================================
[2019-04-05 11:44:31,736] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3120024e-08 2.4668261e-05 1.4032441e-04 9.9909234e-01 7.4252603e-04
 3.5450899e-08 1.2189670e-07], sum to 1.0000
[2019-04-05 11:44:31,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7241
[2019-04-05 11:44:31,766] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 79.00000000000001, 0.0, 0.0, 21.0, 20.38025007529924, -0.8001472563480495, 0.0, 1.0, 49199.58316984469], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1833000.0000, 
sim time next is 1833600.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 21.0, 20.38027397908477, -0.7900494961676924, 0.0, 1.0, 49174.01084160019], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.25, 0.1983561649237308, 0.23665016794410254, 0.0, 1.0, 0.23416195638857232], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.31194922], dtype=float32), 0.7737315]. 
=============================================
[2019-04-05 11:44:32,776] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0803381e-12 7.8935943e-07 7.0090880e-07 9.9984062e-01 1.5783765e-04
 4.0390458e-11 1.7224906e-10], sum to 1.0000
[2019-04-05 11:44:32,790] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7440
[2019-04-05 11:44:32,823] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 21.5, 20.68541694707127, -0.5247975146259489, 0.0, 1.0, 198531.2452101558], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1715400.0000, 
sim time next is 1716000.0000, 
raw observation next is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 21.5, 20.76151897257541, -0.4734601550539557, 0.0, 1.0, 174210.7681948656], 
processed observation next is [1.0, 0.8695652173913043, 0.4819944598337951, 0.9066666666666666, 0.0, 0.0, 0.2916666666666667, 0.23012658104795078, 0.3421799483153481, 0.0, 1.0, 0.8295750866422172], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.25269276], dtype=float32), -0.5380128]. 
=============================================
[2019-04-05 11:44:32,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[64.77716 ]
 [64.37973 ]
 [63.988495]
 [63.7854  ]
 [63.744114]], R is [[65.32536316]
 [65.31497192]
 [65.30467987]
 [65.29449463]
 [65.28440857]].
[2019-04-05 11:44:33,371] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1773998e-11 3.2497130e-07 2.9071030e-06 9.9992228e-01 7.4462485e-05
 4.5247342e-10 9.8039810e-10], sum to 1.0000
[2019-04-05 11:44:33,371] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1999
[2019-04-05 11:44:33,416] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 22.5, 21.71147127002508, -0.5170445121352252, 0.0, 1.0, 25388.70084076031], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 1881600.0000, 
sim time next is 1882200.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 22.5, 21.70673017483458, -0.5217149272452779, 0.0, 1.0, 31303.02892388853], 
processed observation next is [0.0, 0.782608695652174, 0.3356417359187443, 0.835, 0.0, 0.0, 0.375, 0.3088941812362149, 0.32609502425157405, 0.0, 1.0, 0.14906204249470728], 
reward next is 0.5000, 
noisyNet noise sample is [array([1.212533], dtype=float32), -0.42584065]. 
=============================================
[2019-04-05 11:44:37,114] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.29407355e-08 5.50027180e-05 8.75849219e-05 9.97633338e-01
 2.22394662e-03 1.47574815e-08 3.63209409e-08], sum to 1.0000
[2019-04-05 11:44:37,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3763
[2019-04-05 11:44:37,136] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 19.5, 19.59705670619564, -1.123723092173002, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2035200.0000, 
sim time next is 2035800.0000, 
raw observation next is [-4.2, 79.0, 148.0, 0.0, 19.5, 19.53469649931341, -1.070722520345447, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.49333333333333335, 0.0, 0.125, 0.12789137494278405, 0.14309249321818437, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.86253685], dtype=float32), -1.5128497]. 
=============================================
[2019-04-05 11:44:37,665] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9867847e-09 1.1708802e-05 7.2880362e-06 9.9996150e-01 1.9583773e-05
 2.0771376e-08 4.0682244e-09], sum to 1.0000
[2019-04-05 11:44:37,667] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7183
[2019-04-05 11:44:37,736] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.199999999999999, 78.83333333333334, 24.66666666666666, 12.33333333333333, 20.0, 19.35151856595272, -1.02943773803396, 1.0, 1.0, 32394.6559404043], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2101800.0000, 
sim time next is 2102400.0000, 
raw observation next is [-7.3, 79.0, 36.5, 18.5, 20.0, 19.53235438693757, -0.976704852063873, 1.0, 1.0, 48364.72652409627], 
processed observation next is [1.0, 0.34782608695652173, 0.26038781163434904, 0.79, 0.12166666666666667, 0.020441988950276244, 0.16666666666666666, 0.12769619891146414, 0.174431715978709, 1.0, 1.0, 0.2303082215433156], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5322083], dtype=float32), -0.51194775]. 
=============================================
[2019-04-05 11:44:42,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.6423855e-11 1.1537940e-06 1.9949941e-06 9.9998462e-01 1.2125579e-05
 1.1830679e-09 9.0661346e-11], sum to 1.0000
[2019-04-05 11:44:42,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7119
[2019-04-05 11:44:42,453] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.2, 77.0, 0.0, 0.0, 22.0, 21.89960577442544, -0.4820455228401641, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 22.0, 21.7462915846821, -0.5187952428945871, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.31578947368421056, 0.785, 0.0, 0.0, 0.3333333333333333, 0.3121909653901751, 0.327068252368471, 0.0, 1.0, 0.0], 
reward next is 0.5714, 
noisyNet noise sample is [array([-1.618848], dtype=float32), -0.093412265]. 
=============================================
[2019-04-05 11:45:00,083] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-05 11:45:00,090] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:45:00,091] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:45:00,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:45:00,093] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:45:00,092] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:45:00,095] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:45:00,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run11
[2019-04-05 11:45:00,113] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run11
[2019-04-05 11:45:00,113] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run11
[2019-04-05 11:45:05,377] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08410498]
[2019-04-05 11:45:05,377] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.4, 84.0, 0.0, 0.0, 19.0, 18.78041796044755, -1.178592821142608, 0.0, 1.0, 0.0]
[2019-04-05 11:45:05,378] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:45:05,379] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.41137048e-08 2.16608987e-05 8.33250451e-05 9.99526978e-01
 3.67675209e-04 1.18994656e-07 1.36424660e-07], sampled 0.06362670254875868
[2019-04-05 11:46:06,595] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08410498]
[2019-04-05 11:46:06,596] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [7.942472366, 100.0, 119.34425945, 864.84831055, 21.0, 22.40280369463978, -0.2694850044663511, 1.0, 1.0, 0.0]
[2019-04-05 11:46:06,596] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:46:06,598] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2655094e-11 3.5006178e-07 1.1045179e-06 9.9999142e-01 7.1848053e-06
 1.0743686e-10 2.2168663e-10], sampled 0.3693945107240151
[2019-04-05 11:46:07,788] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6054.1641 145755906.9182 -2107.7328
[2019-04-05 11:46:20,561] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08410498]
[2019-04-05 11:46:20,561] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.2, 65.0, 0.0, 0.0, 21.0, 20.773218904474, -0.6582089140348605, 0.0, 1.0, 0.0]
[2019-04-05 11:46:20,561] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:46:20,562] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.8455476e-09 9.5368914e-06 4.3397031e-05 9.9972802e-01 2.1895830e-04
 2.7095401e-08 4.9132360e-08], sampled 0.11265594707574933
[2019-04-05 11:46:27,121] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5045.0882 191611079.9731 -1909.4879
[2019-04-05 11:46:35,163] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5044.6827 205618364.2727 -1845.4931
[2019-04-05 11:46:36,188] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 200000, evaluation results [200000.0, 5045.088224217422, 191611079.97307628, -1909.4878811935762, 6054.164056550934, 145755906.91819218, -2107.732829760704, 5044.6827393116855, 205618364.27270085, -1845.493053915171]
[2019-04-05 11:46:44,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1369964e-09 3.2736350e-06 8.2207007e-06 9.9998426e-01 4.2781107e-06
 2.1917589e-10 1.0289775e-08], sum to 1.0000
[2019-04-05 11:46:44,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1248
[2019-04-05 11:46:44,201] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 45.0, 171.0, 64.5, 21.5, 22.02834923573776, -0.4941941435291127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 2296800.0000, 
sim time next is 2297400.0000, 
raw observation next is [-0.3166666666666667, 44.66666666666666, 154.3333333333333, 63.0, 21.5, 22.10531591862868, -0.4845628086641587, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.45383194829178214, 0.44666666666666655, 0.5144444444444443, 0.06961325966850829, 0.2916666666666667, 0.34210965988572334, 0.33847906377861375, 1.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.46592033], dtype=float32), 0.051337916]. 
=============================================
[2019-04-05 11:46:48,761] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0090240e-07 1.5411884e-04 9.0142072e-05 9.9798179e-01 1.7717296e-03
 5.6351826e-07 1.4120379e-06], sum to 1.0000
[2019-04-05 11:46:48,761] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9272
[2019-04-05 11:46:48,773] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 54.83333333333334, 112.0, 809.0, 19.0, 18.69832427264792, -1.131975516154609, 0.0, 1.0, 51447.91939694233], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3066600.0000, 
sim time next is 3067200.0000, 
raw observation next is [-3.0, 55.0, 112.5, 811.0, 19.0, 18.67308033865077, -1.126342647945008, 0.0, 1.0, 32059.09311353902], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.375, 0.8961325966850828, 0.08333333333333333, 0.056090028220897516, 0.12455245068499732, 0.0, 1.0, 0.1526623481597096], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7214151], dtype=float32), -1.5457052]. 
=============================================
[2019-04-05 11:46:55,207] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5986706e-07 1.2433178e-04 1.2184638e-03 9.9732625e-01 1.3295767e-03
 3.3509778e-07 8.1768178e-07], sum to 1.0000
[2019-04-05 11:46:55,207] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0584
[2019-04-05 11:46:55,226] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 19.5, 19.21474999160296, -1.084168561906037, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3055200.0000, 
sim time next is 3055800.0000, 
raw observation next is [-6.0, 59.83333333333334, 88.33333333333334, 451.0, 19.5, 19.18227793734876, -1.089447591314677, 0.0, 1.0, 18727.10073395013], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.5983333333333334, 0.29444444444444445, 0.4983425414364641, 0.125, 0.0985231614457301, 0.13685080289510765, 0.0, 1.0, 0.08917667016166729], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.3960525], dtype=float32), 1.2175834]. 
=============================================
[2019-04-05 11:46:58,242] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3031712e-10 7.3159276e-06 3.5629830e-06 9.9998271e-01 6.3984508e-06
 7.6143323e-09 8.1463307e-09], sum to 1.0000
[2019-04-05 11:46:58,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9067
[2019-04-05 11:46:58,281] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.966666666666667, 31.0, 17.5, 31.16666666666666, 21.5, 22.47216389154068, -0.5526395400681373, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 2568000.0000, 
sim time next is 2568600.0000, 
raw observation next is [1.6, 32.0, 0.0, 0.0, 21.5, 21.83189447582927, -0.6146906823455923, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5069252077562327, 0.32, 0.0, 0.0, 0.2916666666666667, 0.3193245396524391, 0.29510310588480254, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1046395], dtype=float32), 0.04255378]. 
=============================================
[2019-04-05 11:47:10,572] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.9875717e-11 1.8512345e-07 4.6469248e-07 9.9999630e-01 2.9754065e-06
 7.8714195e-11 2.0198923e-09], sum to 1.0000
[2019-04-05 11:47:10,572] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8980
[2019-04-05 11:47:10,584] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 88.0, 107.6666666666667, 735.5, 20.0, 21.52951588107959, -0.4957134235096887, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3234000.0000, 
sim time next is 3234600.0000, 
raw observation next is [-2.7, 86.0, 109.0, 752.0, 20.0, 21.56636098351424, -0.490415966717904, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.38781163434903054, 0.86, 0.36333333333333334, 0.830939226519337, 0.16666666666666666, 0.29719674862618667, 0.336528011094032, 1.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.22862974], dtype=float32), -2.5239096]. 
=============================================
[2019-04-05 11:47:11,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0305628e-12 2.4670872e-08 3.1817191e-07 9.9999654e-01 3.0757526e-06
 1.1087232e-11 2.2738471e-10], sum to 1.0000
[2019-04-05 11:47:11,841] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2838
[2019-04-05 11:47:11,854] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 54.0, 94.0, 673.5, 23.0, 23.18092671898928, -0.07855628186216457, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2732400.0000, 
sim time next is 2733000.0000, 
raw observation next is [-3.833333333333333, 53.33333333333333, 91.33333333333334, 653.6666666666666, 23.0, 23.54201400236574, -0.04453917131568844, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3564173591874424, 0.5333333333333333, 0.30444444444444446, 0.7222836095764272, 0.4166666666666667, 0.46183450019714495, 0.4851536095614372, 1.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.7477263], dtype=float32), 1.1121173]. 
=============================================
[2019-04-05 11:47:11,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.73698 ]
 [62.58684 ]
 [62.47984 ]
 [62.400898]
 [62.34356 ]], R is [[62.50951004]
 [62.31298447]
 [62.11842346]
 [61.92580795]
 [61.73511887]].
[2019-04-05 11:47:13,736] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-05 11:47:13,737] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:47:13,737] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:47:13,738] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:47:13,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:47:13,738] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:47:13,739] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:47:13,745] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run12
[2019-04-05 11:47:13,760] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run12
[2019-04-05 11:47:13,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run12
[2019-04-05 11:47:37,525] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08604704]
[2019-04-05 11:47:37,525] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.685866684000001, 73.008621475, 0.0, 0.0, 21.0, 20.36942041945207, -0.8456114331504473, 0.0, 1.0, 46549.74545972195]
[2019-04-05 11:47:37,525] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:47:37,526] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.0054339e-09 6.2318109e-06 2.3731385e-05 9.9982649e-01 1.4360379e-04
 2.4298245e-08 4.4454293e-08], sampled 0.4722227224248261
[2019-04-05 11:47:57,456] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08604704]
[2019-04-05 11:47:57,456] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.882415747333333, 75.86077932, 0.0, 0.0, 21.0, 20.58362378472555, -0.7585936589750767, 0.0, 1.0, 39604.2742629124]
[2019-04-05 11:47:57,457] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:47:57,457] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.4686101e-09 3.5964315e-06 7.0580704e-06 9.9994826e-01 4.1143165e-05
 4.9090603e-09 1.1606106e-08], sampled 0.8885962816171293
[2019-04-05 11:48:06,629] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08604704]
[2019-04-05 11:48:06,629] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.0, 60.0, 101.0, 751.0, 19.0, 19.07065661137027, -1.022130014439501, 0.0, 1.0, 0.0]
[2019-04-05 11:48:06,629] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:48:06,630] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.6605034e-09 2.9697576e-06 8.6557466e-06 9.9992895e-01 5.9305890e-05
 9.0128127e-09 1.3699115e-08], sampled 0.5665037948574645
[2019-04-05 11:48:14,942] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08604704]
[2019-04-05 11:48:14,942] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.133333333333335, 72.66666666666667, 0.0, 0.0, 19.5, 20.23219355068241, -0.7916970076100222, 0.0, 1.0, 0.0]
[2019-04-05 11:48:14,942] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:48:14,943] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.5172879e-09 2.8673817e-06 1.2660159e-05 9.9990404e-01 8.0444668e-05
 1.4770547e-08 1.8370990e-08], sampled 0.6250891067681275
[2019-04-05 11:48:21,589] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5987.0285 146745184.9709 -2102.8696
[2019-04-05 11:48:30,452] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08604704]
[2019-04-05 11:48:30,453] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.0, 23.0, 0.0, 0.0, 19.5, 22.28103807474099, -0.4586538578183841, 1.0, 1.0, 0.0]
[2019-04-05 11:48:30,453] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:48:30,453] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.8196051e-10 2.8115967e-06 4.4600720e-06 9.9996400e-01 2.8680171e-05
 3.3585301e-09 8.6061309e-09], sampled 0.11094088714294881
[2019-04-05 11:48:31,588] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5610.9466 173746403.5466 -2810.4380
[2019-04-05 11:48:52,274] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4711.0996 210627195.4204 -1642.2646
[2019-04-05 11:48:53,298] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 220000, evaluation results [220000.0, 5610.946564784216, 173746403.5465898, -2810.437991084607, 5987.028511691514, 146745184.97094142, -2102.869590231493, 4711.099641784254, 210627195.42037585, -1642.264627811849]
[2019-04-05 11:49:01,135] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.9709212e-12 5.4183789e-07 9.4863901e-07 9.9999809e-01 3.3469425e-07
 4.8643806e-12 2.0296774e-10], sum to 1.0000
[2019-04-05 11:49:01,137] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6872
[2019-04-05 11:49:01,143] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 21.0, 22.19075621937478, -0.2847759461364985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 3240600.0000, 
sim time next is 3241200.0000, 
raw observation next is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 21.0, 22.34445928602053, -0.2781666211237763, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7566666666666667, 0.38222222222222235, 0.907182320441989, 0.25, 0.3620382738350442, 0.4072777929587412, 1.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([1.1207469], dtype=float32), 0.54302377]. 
=============================================
[2019-04-05 11:49:06,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2066560e-10 7.7287217e-07 3.6745433e-08 9.9999750e-01 1.8176888e-06
 4.8185617e-10 6.7044853e-10], sum to 1.0000
[2019-04-05 11:49:06,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2289
[2019-04-05 11:49:06,877] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.333333333333333, 61.66666666666667, 0.0, 0.0, 20.0, 19.89623072288256, -0.9055490207510909, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3699600.0000, 
sim time next is 3700200.0000, 
raw observation next is [3.166666666666667, 62.33333333333333, 0.0, 0.0, 20.0, 19.84141205320361, -0.9177740352878369, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.5503231763619576, 0.6233333333333333, 0.0, 0.0, 0.16666666666666666, 0.15345100443363405, 0.19407532157072105, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.4159651], dtype=float32), 1.3884995]. 
=============================================
[2019-04-05 11:49:09,383] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.01121106e-12 3.81930221e-08 1.95997387e-07 9.99993443e-01
 6.29016449e-06 6.60431501e-11 1.09510546e-10], sum to 1.0000
[2019-04-05 11:49:09,388] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3709
[2019-04-05 11:49:09,397] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.666666666666667, 53.0, 67.83333333333333, 552.5, 20.5, 21.1977099254084, -0.5471060949923486, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3687600.0000, 
sim time next is 3688200.0000, 
raw observation next is [4.5, 54.5, 64.0, 522.0, 20.5, 21.19863416643534, -0.5515624398384479, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5872576177285319, 0.545, 0.21333333333333335, 0.5767955801104973, 0.20833333333333334, 0.26655284720294503, 0.31614585338718404, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.15514778], dtype=float32), -1.0819277]. 
=============================================
[2019-04-05 11:49:14,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0021577e-10 3.6555764e-06 7.4944450e-07 9.9999213e-01 3.5046005e-06
 2.6041591e-10 2.6674423e-09], sum to 1.0000
[2019-04-05 11:49:14,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3482
[2019-04-05 11:49:14,247] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 20.5, 20.2033481647034, -0.802634946494154, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3808200.0000, 
sim time next is 3808800.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 20.5, 20.23342576416672, -0.8142690722738471, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.77, 0.0, 0.0, 0.20833333333333334, 0.18611881368056013, 0.22857697590871762, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-1.4889588], dtype=float32), -1.719212]. 
=============================================
[2019-04-05 11:49:16,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0161897e-12 2.1524741e-06 9.9692308e-08 9.9999273e-01 5.0557801e-06
 6.3024301e-11 8.6202889e-10], sum to 1.0000
[2019-04-05 11:49:16,426] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1426
[2019-04-05 11:49:16,436] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 48.0, 108.3333333333333, 796.0, 20.5, 22.70486275879433, -0.2881118370567111, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3852600.0000, 
sim time next is 3853200.0000, 
raw observation next is [2.0, 48.0, 107.1666666666667, 789.0, 20.5, 22.79458404577105, -0.2732146976240017, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.35722222222222233, 0.8718232044198895, 0.20833333333333334, 0.3995486704809208, 0.40892843412533275, 1.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.4155196], dtype=float32), -2.0535593]. 
=============================================
[2019-04-05 11:49:19,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4520160e-09 3.4540951e-06 3.8054447e-06 9.9992621e-01 6.6495828e-05
 9.5767669e-09 4.4926885e-08], sum to 1.0000
[2019-04-05 11:49:19,119] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1875
[2019-04-05 11:49:19,136] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.16666666666667, 64.0, 0.0, 0.0, 20.0, 19.34363737902203, -1.042643561280066, 0.0, 1.0, 47272.42848685698], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3996600.0000, 
sim time next is 3997200.0000, 
raw observation next is [-13.33333333333333, 65.0, 0.0, 0.0, 20.0, 19.37941423201192, -1.042242675800536, 0.0, 1.0, 42891.27712190666], 
processed observation next is [1.0, 0.2608695652173913, 0.09325946445060027, 0.65, 0.0, 0.0, 0.16666666666666666, 0.11495118600099345, 0.15258577473315463, 0.0, 1.0, 0.2042441767709841], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.2057166], dtype=float32), 0.4416814]. 
=============================================
[2019-04-05 11:49:21,399] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0638092e-13 1.0898380e-07 1.0218405e-09 9.9999976e-01 1.1367159e-07
 4.9662084e-12 1.3819127e-12], sum to 1.0000
[2019-04-05 11:49:21,400] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5158
[2019-04-05 11:49:21,411] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 22.5, 22.58599216484852, -0.2722683604599191, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 3450000.0000, 
sim time next is 3450600.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 22.5, 22.40857520138085, -0.2921010213406434, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.375, 0.3673812667817374, 0.4026329928864522, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.619857], dtype=float32), -0.92732763]. 
=============================================
[2019-04-05 11:49:21,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0521319e-11 6.1964087e-08 1.8018930e-08 9.9999750e-01 2.3250034e-06
 5.2190086e-10 3.1744313e-10], sum to 1.0000
[2019-04-05 11:49:21,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4149
[2019-04-05 11:49:21,599] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 65.33333333333334, 0.0, 0.0, 21.0, 20.92668821787711, -0.642629964431999, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 3705600.0000, 
sim time next is 3706200.0000, 
raw observation next is [1.0, 67.0, 0.0, 0.0, 21.0, 20.92803653989228, -0.6519212075378966, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.67, 0.0, 0.0, 0.25, 0.24400304499102324, 0.2826929308207011, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-1.0097703], dtype=float32), 2.086274]. 
=============================================
[2019-04-05 11:49:22,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5573034e-13 8.4266283e-08 1.7588608e-08 9.9999976e-01 1.0529118e-07
 6.2094335e-13 2.7503329e-12], sum to 1.0000
[2019-04-05 11:49:22,026] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2647
[2019-04-05 11:49:22,081] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1345666e-09 5.5713640e-06 2.2264967e-06 9.9992013e-01 7.1874150e-05
 4.0057886e-09 9.0036835e-08], sum to 1.0000
[2019-04-05 11:49:22,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9954
[2019-04-05 11:49:22,099] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 20.5, 20.03963684498552, -0.8640563433193682, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3982800.0000, 
sim time next is 3983400.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 20.5, 20.07906670604105, -0.8797846014517158, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.20833333333333334, 0.17325555883675423, 0.2067384661827614, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.06662396], dtype=float32), 0.6831316]. 
=============================================
[2019-04-05 11:49:22,179] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 21.5, 21.24865680398577, -0.5313756236474916, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 3456000.0000, 
sim time next is 3456600.0000, 
raw observation next is [1.0, 84.83333333333334, 0.0, 0.0, 21.5, 21.23890070374566, -0.5394685957767599, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8483333333333334, 0.0, 0.0, 0.2916666666666667, 0.269908391978805, 0.32017713474108006, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.7017322], dtype=float32), -0.21477899]. 
=============================================
[2019-04-05 11:49:22,798] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.9045297e-11 2.8334460e-07 1.3570316e-06 9.9999142e-01 7.0345391e-06
 2.7355240e-09 1.7224645e-10], sum to 1.0000
[2019-04-05 11:49:22,803] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7835
[2019-04-05 11:49:22,816] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 22.03728672417262, -0.3797092780022601, 0.0, 1.0, 59652.99993365371], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 22.0111361891369, -0.3769400671614422, 0.0, 1.0, 62122.55509815899], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.33426134909474153, 0.3743533109461859, 0.0, 1.0, 0.29582169094361427], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.5162042], dtype=float32), -1.0611564]. 
=============================================
[2019-04-05 11:49:23,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1498706e-12 1.3673671e-06 7.3508301e-07 9.9999332e-01 4.6588852e-06
 1.1277878e-10 5.8709659e-10], sum to 1.0000
[2019-04-05 11:49:23,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5480
[2019-04-05 11:49:23,397] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 78.0, 60.0, 0.0, 19.0, 20.29898363379207, -0.815098896173612, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4464000.0000, 
sim time next is 4464600.0000, 
raw observation next is [0.0, 78.0, 56.33333333333333, 0.0, 19.0, 20.43759772113222, -0.8057569237435236, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.18777777777777777, 0.0, 0.08333333333333333, 0.20313314342768507, 0.2314143587521588, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.76508206], dtype=float32), -1.177256]. 
=============================================
[2019-04-05 11:49:23,876] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6385728e-08 3.3710385e-05 1.3037117e-05 9.9986279e-01 9.0314737e-05
 1.1317486e-08 6.5681014e-08], sum to 1.0000
[2019-04-05 11:49:23,879] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0541
[2019-04-05 11:49:23,894] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 50.66666666666667, 0.0, 0.0, 20.5, 20.16016877749312, -0.8425084554274987, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4173600.0000, 
sim time next is 4174200.0000, 
raw observation next is [-5.0, 51.5, 0.0, 0.0, 20.5, 20.22270353482326, -0.8525151664721339, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.515, 0.0, 0.0, 0.20833333333333334, 0.18522529456860504, 0.21582827784262204, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.91630393], dtype=float32), -0.27625558]. 
=============================================
[2019-04-05 11:49:23,932] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7799805e-09 1.6088663e-06 2.4762728e-06 9.9995446e-01 4.1439504e-05
 6.3888401e-09 2.8334590e-08], sum to 1.0000
[2019-04-05 11:49:23,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5189
[2019-04-05 11:49:23,951] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 19.5, 19.17878182766775, -1.113960775026595, 0.0, 1.0, 117564.7214971382], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4259400.0000, 
sim time next is 4260000.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.5, 19.15166394729198, -1.10190995892925, 0.0, 1.0, 89403.64936009377], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.125, 0.09597199560766494, 0.13269668035691665, 0.0, 1.0, 0.42573166361949416], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.0255005], dtype=float32), 0.7256573]. 
=============================================
[2019-04-05 11:49:23,963] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[43.19294 ]
 [42.824196]
 [42.616154]
 [42.68493 ]
 [42.593124]], R is [[43.96825027]
 [44.45713806]
 [44.94113541]
 [45.42029572]
 [45.89466476]].
[2019-04-05 11:49:25,239] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.3028257e-14 5.6854748e-08 8.4552667e-09 9.9999988e-01 1.2855241e-07
 7.0097183e-12 5.1992469e-11], sum to 1.0000
[2019-04-05 11:49:25,244] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2428
[2019-04-05 11:49:25,258] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.1666666666666667, 73.0, 0.0, 0.0, 22.5, 22.37849113647064, -0.1537443658218415, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 3532200.0000, 
sim time next is 3532800.0000, 
raw observation next is [-0.3333333333333333, 74.0, 0.0, 0.0, 22.5, 22.49819351779358, -0.1536018802679722, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4533702677747, 0.74, 0.0, 0.0, 0.375, 0.37484945981613177, 0.44879937324400926, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([1.4339315], dtype=float32), -1.1900576]. 
=============================================
[2019-04-05 11:49:25,842] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.9370399e-11 5.5967398e-08 4.4066187e-08 9.9999368e-01 6.3629050e-06
 1.4664199e-10 1.9307592e-10], sum to 1.0000
[2019-04-05 11:49:25,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5023
[2019-04-05 11:49:25,853] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 20.0, 19.73774716785215, -0.9149326891425402, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4142400.0000, 
sim time next is 4143000.0000, 
raw observation next is [0.1666666666666666, 42.5, 0.0, 0.0, 20.0, 19.63471391302567, -0.9210230068080906, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.425, 0.0, 0.0, 0.16666666666666666, 0.1362261594188059, 0.19299233106396982, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.30606335], dtype=float32), -1.4510937]. 
=============================================
[2019-04-05 11:49:25,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[57.69084 ]
 [57.157516]
 [57.88816 ]
 [57.476593]
 [57.693905]], R is [[58.01394653]
 [58.29095078]
 [58.56518555]
 [58.83667755]
 [59.10545349]].
[2019-04-05 11:49:27,077] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-05 11:49:27,078] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:49:27,078] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:49:27,079] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:49:27,079] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:49:27,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:49:27,081] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:49:27,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run13
[2019-04-05 11:49:27,107] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run13
[2019-04-05 11:49:27,108] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run13
[2019-04-05 11:50:36,342] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.08841198]
[2019-04-05 11:50:36,343] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 45.0, 0.0, 0.0, 19.0, 18.92658025811125, -1.191052775421997, 0.0, 1.0, 0.0]
[2019-04-05 11:50:36,343] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:50:36,344] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.75899605e-08 8.93550805e-06 2.57955326e-05 9.99846697e-01
 1.18238655e-04 1.08833973e-07 1.39268749e-07], sampled 0.9624893845677623
[2019-04-05 11:50:40,457] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5544.2178 156019284.2304 -1688.6281
[2019-04-05 11:50:45,588] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.9308 171424657.3513 -2880.0027
[2019-04-05 11:50:51,644] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5838.4417 185048419.1411 -2785.1128
[2019-04-05 11:50:52,672] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 240000, evaluation results [240000.0, 5683.9307794573915, 171424657.3512828, -2880.002680936151, 5544.217813201499, 156019284.23035407, -1688.628106296839, 5838.441675212122, 185048419.14109835, -2785.112797972731]
[2019-04-05 11:51:00,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.4222283e-13 3.7259909e-08 5.1064823e-08 9.9999881e-01 1.1848739e-06
 4.9853489e-11 1.1581781e-10], sum to 1.0000
[2019-04-05 11:51:00,004] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6828
[2019-04-05 11:51:00,014] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.966666666666667, 56.66666666666667, 100.6666666666667, 622.0, 20.0, 20.2390906530782, -0.7564161616392813, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4290600.0000, 
sim time next is 4291200.0000, 
raw observation next is [7.0, 56.0, 93.0, 605.5, 20.0, 20.29169076075169, -0.75126431698063, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6565096952908588, 0.56, 0.31, 0.669060773480663, 0.16666666666666666, 0.1909742300626407, 0.24957856100645667, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.9497483], dtype=float32), -0.14875878]. 
=============================================
[2019-04-05 11:51:00,291] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2430573e-12 1.1073798e-07 2.5755147e-08 9.9999905e-01 8.5968276e-07
 3.4037231e-11 1.3431779e-09], sum to 1.0000
[2019-04-05 11:51:00,298] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1677
[2019-04-05 11:51:00,311] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 29.0, 0.0, 0.0, 22.5, 22.3418361928338, -0.2421783904040734, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 4050000.0000, 
sim time next is 4050600.0000, 
raw observation next is [-4.166666666666667, 29.33333333333334, 0.0, 0.0, 22.5, 22.37515765067657, -0.2484723786455908, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3471837488457987, 0.2933333333333334, 0.0, 0.0, 0.375, 0.3645964708897142, 0.41717587378480303, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-1.5675304], dtype=float32), 0.59569913]. 
=============================================
[2019-04-05 11:51:01,131] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4510856e-10 3.6943820e-07 2.2109361e-07 9.9998546e-01 1.3974114e-05
 5.3111726e-10 7.2766321e-10], sum to 1.0000
[2019-04-05 11:51:01,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4575
[2019-04-05 11:51:01,168] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.0, 44.0, 106.5, 739.5, 21.0, 22.32756482378818, -0.4314916604499006, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 4010400.0000, 
sim time next is 4011000.0000, 
raw observation next is [-8.833333333333334, 43.33333333333334, 108.3333333333333, 753.3333333333334, 21.0, 22.37184269358816, -0.4216853645886449, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.21791320406278855, 0.4333333333333334, 0.361111111111111, 0.8324125230202579, 0.25, 0.36432022446568, 0.35943821180378505, 1.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.29199514], dtype=float32), -1.7033156]. 
=============================================
[2019-04-05 11:51:01,180] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[54.551975]
 [54.491745]
 [54.02853 ]
 [53.920033]
 [53.487957]], R is [[54.83354187]
 [54.99949265]
 [55.16378403]
 [55.32643509]
 [55.48745728]].
[2019-04-05 11:51:03,358] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8368229e-10 1.7610570e-07 7.2365191e-07 9.9999690e-01 2.2368115e-06
 9.4155306e-10 1.9360065e-09], sum to 1.0000
[2019-04-05 11:51:03,360] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-05 11:51:03,372] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 66.5, 123.0, 0.0, 20.5, 21.26414838704632, -0.6559050375039485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4527000.0000, 
sim time next is 4527600.0000, 
raw observation next is [0.6666666666666666, 64.66666666666667, 139.3333333333333, 2.999999999999999, 20.5, 21.24598547130811, -0.6478823796048679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.6466666666666667, 0.46444444444444427, 0.0033149171270718224, 0.20833333333333334, 0.27049878927567583, 0.28403920679837735, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.47888568], dtype=float32), -1.0831407]. 
=============================================
[2019-04-05 11:51:05,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.92095970e-12 1.87007068e-08 5.15205798e-08 9.99999881e-01
 1.00523415e-07 2.90384088e-11 2.97637991e-11], sum to 1.0000
[2019-04-05 11:51:05,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3437
[2019-04-05 11:51:05,612] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 21.5, 22.03833578139849, -0.3565061371991629, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 3873000.0000, 
sim time next is 3873600.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 21.5, 21.92516779116005, -0.3755950806469484, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.51, 0.0, 0.0, 0.2916666666666667, 0.32709731593000413, 0.37480163978435055, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.5101911], dtype=float32), -1.3549111]. 
=============================================
[2019-04-05 11:51:08,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1511922e-12 1.0667644e-08 6.0608201e-08 9.9999988e-01 1.5984490e-08
 9.0988932e-11 3.4917281e-11], sum to 1.0000
[2019-04-05 11:51:08,905] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8841
[2019-04-05 11:51:08,912] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.1, 66.0, 0.0, 0.0, 21.0, 21.57754325391677, -0.4420749857302222, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 4413600.0000, 
sim time next is 4414200.0000, 
raw observation next is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 21.0, 21.52565548686218, -0.4510601704817344, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6265004616805172, 0.6616666666666667, 0.0, 0.0, 0.25, 0.29380462390518175, 0.3496466098394218, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([1.9680734], dtype=float32), -0.91349006]. 
=============================================
[2019-04-05 11:51:14,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:51:14,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:51:14,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-05 11:51:16,150] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9663949e-12 1.3019360e-07 2.3708988e-07 9.9999905e-01 5.5911181e-07
 4.4193961e-11 2.1384677e-10], sum to 1.0000
[2019-04-05 11:51:16,150] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2299
[2019-04-05 11:51:16,181] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.833333333333334, 52.33333333333334, 97.0, 616.6666666666666, 23.0, 23.23580587999587, -0.1346334088770717, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3921000.0000, 
sim time next is 3921600.0000, 
raw observation next is [-7.666666666666667, 51.66666666666667, 98.5, 654.3333333333334, 23.0, 23.24029730379493, -0.1094243085091049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2502308402585411, 0.5166666666666667, 0.3283333333333333, 0.7230202578268877, 0.4166666666666667, 0.4366914419829107, 0.46352523049696503, 1.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([-1.194235], dtype=float32), 1.5794055]. 
=============================================
[2019-04-05 11:51:16,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0725427e-11 1.9954811e-08 1.2280045e-08 9.9999988e-01 8.9109960e-08
 9.4568950e-11 7.8525719e-10], sum to 1.0000
[2019-04-05 11:51:16,702] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0501
[2019-04-05 11:51:16,717] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 115.0, 0.0, 20.5, 21.14721828777787, -0.6509299145298052, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4699800.0000, 
sim time next is 4700400.0000, 
raw observation next is [0.0, 92.0, 130.5, 0.9999999999999998, 20.5, 21.26901177248719, -0.6383054366753715, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.435, 0.0011049723756906074, 0.20833333333333334, 0.2724176477072658, 0.2872315211082095, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.84793365], dtype=float32), 0.47629318]. 
=============================================
[2019-04-05 11:51:20,269] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.7967202e-11 3.4177110e-06 1.6089959e-06 9.9998260e-01 1.2421807e-05
 5.6953264e-09 6.7097106e-09], sum to 1.0000
[2019-04-05 11:51:20,270] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2557
[2019-04-05 11:51:20,282] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 20.5, 20.20136723753419, -0.842805139020328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4845000.0000, 
sim time next is 4845600.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 20.5, 20.18446816721157, -0.8516042461839879, 0.0, 1.0, 34095.70246448393], 
processed observation next is [0.0, 0.08695652173913043, 0.40720221606648205, 0.6, 0.0, 0.0, 0.20833333333333334, 0.18203901393429747, 0.2161319179386707, 0.0, 1.0, 0.16236048792611396], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.9024073], dtype=float32), 0.3748853]. 
=============================================
[2019-04-05 11:51:21,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4935978e-12 1.8354033e-08 1.3963179e-07 9.9999905e-01 8.7210555e-07
 5.7206206e-10 3.2831721e-10], sum to 1.0000
[2019-04-05 11:51:21,140] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8704
[2019-04-05 11:51:21,157] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.166666666666667, 32.5, 116.3333333333333, 784.6666666666667, 22.0, 23.35813355477063, -0.176788189114613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4099800.0000, 
sim time next is 4100400.0000, 
raw observation next is [-1.0, 32.0, 117.5, 792.5, 22.0, 23.43880953518011, -0.2831121239931557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.32, 0.39166666666666666, 0.8756906077348067, 0.3333333333333333, 0.4532341279316758, 0.4056292920022814, 1.0, 1.0, 0.0], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.89385647], dtype=float32), 1.4094146]. 
=============================================
[2019-04-05 11:51:22,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:51:22,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:51:22,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-05 11:51:24,754] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 11:51:24,755] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:51:24,755] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:51:24,755] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:51:24,757] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:51:24,757] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:51:24,759] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:51:24,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run14
[2019-04-05 11:51:24,776] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run14
[2019-04-05 11:51:24,788] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run14
[2019-04-05 11:51:42,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09119578]
[2019-04-05 11:51:42,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.846332236666667, 72.10313894333332, 87.66544038, 40.15515288333333, 20.0, 19.82557686244014, -0.9104017258406222, 0.0, 1.0, 0.0]
[2019-04-05 11:51:42,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:51:42,114] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.4346341e-12 1.5086860e-08 3.4814377e-08 9.9999964e-01 3.2057298e-07
 1.8551993e-11 3.0519060e-11], sampled 0.012711304708365567
[2019-04-05 11:51:43,505] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09119578]
[2019-04-05 11:51:43,506] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.6, 60.0, 91.83333333333333, 724.0, 19.0, 19.86681322097193, -1.024995552138522, 1.0, 1.0, 0.0]
[2019-04-05 11:51:43,506] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:51:43,507] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.5893211e-10 6.0317956e-07 7.6800984e-07 9.9999499e-01 3.6856354e-06
 1.6469894e-09 3.2778920e-09], sampled 0.11045700903230893
[2019-04-05 11:51:47,917] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09119578]
[2019-04-05 11:51:47,918] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.6377786035, 70.2249722, 123.6109026, 0.0, 20.0, 20.3329046216479, -0.9627642097799657, 1.0, 1.0, 0.0]
[2019-04-05 11:51:47,918] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:51:47,919] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.4484728e-10 7.5892513e-07 1.0587005e-06 9.9999356e-01 4.6551736e-06
 2.0763238e-09 5.2233529e-09], sampled 0.29952338830607594
[2019-04-05 11:52:05,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09119578]
[2019-04-05 11:52:05,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.8, 70.5, 0.0, 0.0, 19.0, 18.9033089140024, -1.164189379416262, 1.0, 1.0, 0.0]
[2019-04-05 11:52:05,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:52:05,236] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.5565586e-10 4.6423830e-07 5.7652483e-07 9.9999595e-01 2.9323980e-06
 8.5885271e-10 1.8758459e-09], sampled 0.6931405553844245
[2019-04-05 11:52:42,717] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.9308 171424657.3513 -2880.0027
[2019-04-05 11:52:42,826] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5137.9588 170868464.8040 -1159.2786
[2019-04-05 11:52:56,496] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5207.3352 197362012.6221 -2206.0424
[2019-04-05 11:52:57,519] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 260000, evaluation results [260000.0, 5683.9307794573915, 171424657.3512828, -2880.002680936151, 5137.9588460133455, 170868464.80398893, -1159.278577275141, 5207.335158748339, 197362012.62209785, -2206.0423646283225]
[2019-04-05 11:52:59,169] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.75212553e-14 3.03645535e-08 1.17897105e-08 1.00000000e+00
 2.85722308e-08 2.09352110e-12 1.64532786e-12], sum to 1.0000
[2019-04-05 11:52:59,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4256
[2019-04-05 11:52:59,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2129066e-13 4.6273367e-09 7.8940111e-08 9.9999976e-01 5.9701456e-08
 3.9730559e-11 2.8718661e-11], sum to 1.0000
[2019-04-05 11:52:59,182] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.0, 33.0, 118.3333333333333, 812.0, 21.5, 23.81431501862187, -0.04421829980596203, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 4360200.0000, 
sim time next is 4360800.0000, 
raw observation next is [13.4, 32.0, 119.1666666666667, 820.0, 21.5, 23.83113237402299, -0.01789679093261879, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8337950138504157, 0.32, 0.3972222222222223, 0.9060773480662984, 0.2916666666666667, 0.4859276978352491, 0.4940344030224604, 1.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.03999102], dtype=float32), 1.5480133]. 
=============================================
[2019-04-05 11:52:59,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3366
[2019-04-05 11:52:59,195] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9333333333333333, 73.0, 0.0, 0.0, 22.5, 22.11285224530825, -0.3290062124856272, 0.0, 1.0, 19408.85122468559], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 4503000.0000, 
sim time next is 4503600.0000, 
raw observation next is [-1.0, 73.0, 0.0, 0.0, 22.5, 22.12451721247475, -0.3269111622332378, 0.0, 1.0, 18864.13764960599], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.73, 0.0, 0.0, 0.375, 0.34370976770622913, 0.3910296125889207, 0.0, 1.0, 0.08982922690288567], 
reward next is 0.5000, 
noisyNet noise sample is [array([1.4207656], dtype=float32), 1.3873832]. 
=============================================
[2019-04-05 11:52:59,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:52:59,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:52:59,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-05 11:53:02,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:02,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:02,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-05 11:53:03,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:03,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:03,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-05 11:53:03,763] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.416837e-12 7.798064e-09 5.779424e-08 9.999994e-01 5.930820e-07
 6.461832e-11 5.309059e-10], sum to 1.0000
[2019-04-05 11:53:03,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9976
[2019-04-05 11:53:03,778] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 23.0, 22.57150230215061, -0.3075091111418458, 0.0, 1.0, 35396.24516555802], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4247400.0000, 
sim time next is 4248000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 23.0, 22.63946570250211, -0.3093058973748485, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.4166666666666667, 0.3866221418751759, 0.3968980342083839, 0.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.3235362], dtype=float32), 0.14478914]. 
=============================================
[2019-04-05 11:53:03,791] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[51.499115]
 [51.38891 ]
 [51.295506]
 [51.24473 ]
 [51.17189 ]], R is [[51.45032883]
 [51.36439514]
 [51.27931976]
 [51.19509506]
 [51.11171341]].
[2019-04-05 11:53:04,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:04,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:04,864] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-05 11:53:05,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:05,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:05,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-05 11:53:06,084] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.1376759e-14 2.3820126e-09 3.4199349e-08 9.9999988e-01 1.3914976e-07
 7.2528138e-12 1.0810599e-11], sum to 1.0000
[2019-04-05 11:53:06,086] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4163
[2019-04-05 11:53:06,107] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7333333333333334, 73.0, 0.0, 0.0, 22.5, 22.10085261766953, -0.3207724660057241, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 4501200.0000, 
sim time next is 4501800.0000, 
raw observation next is [-0.8, 73.0, 0.0, 0.0, 22.5, 22.11601164429898, -0.3322433242606617, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4404432132963989, 0.73, 0.0, 0.0, 0.375, 0.3430009703582482, 0.3892522252464461, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.51075387], dtype=float32), 0.120703936]. 
=============================================
[2019-04-05 11:53:09,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:09,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:09,204] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-05 11:53:12,603] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2169910e-09 3.4869640e-06 1.9468469e-05 9.9995387e-01 2.3283203e-05
 4.7186202e-09 1.4269491e-08], sum to 1.0000
[2019-04-05 11:53:12,604] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1571
[2019-04-05 11:53:12,638] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 19.0, 19.29135315215712, -1.078791050134389, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 120000.0000, 
sim time next is 120600.0000, 
raw observation next is [-7.8, 67.5, 45.0, 0.0, 19.0, 19.37240452371539, -1.073601292771045, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.675, 0.15, 0.0, 0.08333333333333333, 0.11436704364294907, 0.14213290240965168, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9560661], dtype=float32), 0.30472702]. 
=============================================
[2019-04-05 11:53:16,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4432326e-13 3.5261529e-08 3.3233235e-08 9.9999964e-01 3.0864621e-07
 1.3980081e-11 1.0715078e-11], sum to 1.0000
[2019-04-05 11:53:16,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0136
[2019-04-05 11:53:16,798] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.766666666666667, 88.0, 0.0, 0.0, 19.0, 18.09001381954286, -1.217874517284549, 0.0, 1.0, 36945.60095407941], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 63600.0000, 
sim time next is 64200.0000, 
raw observation next is [4.583333333333334, 88.5, 0.0, 0.0, 19.0, 18.09422163612955, -1.215501821534817, 0.0, 1.0, 30132.57596079186], 
processed observation next is [0.0, 0.7391304347826086, 0.5895660203139428, 0.885, 0.0, 0.0, 0.08333333333333333, 0.007851803010795836, 0.094832726155061, 0.0, 1.0, 0.1434884569561517], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3509066], dtype=float32), -0.28105742]. 
=============================================
[2019-04-05 11:53:16,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:16,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:16,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-05 11:53:17,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:17,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:17,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-05 11:53:23,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:23,460] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:23,495] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-05 11:53:24,433] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8754394e-10 1.9825441e-06 9.7630084e-07 9.9997294e-01 2.4061545e-05
 5.6774949e-09 3.2574146e-09], sum to 1.0000
[2019-04-05 11:53:24,434] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1261
[2019-04-05 11:53:24,453] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 22.0, 21.42640160209819, -0.5585212029956806, 0.0, 1.0, 56605.55590468627], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4859400.0000, 
sim time next is 4860000.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 22.0, 21.45414220194435, -0.5550330510935843, 0.0, 1.0, 32114.21897622502], 
processed observation next is [0.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 0.3333333333333333, 0.28784518349536253, 0.31498898296880523, 0.0, 1.0, 0.1529248522677382], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.659601], dtype=float32), -0.0054463935]. 
=============================================
[2019-04-05 11:53:24,457] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[49.56726 ]
 [49.448772]
 [49.337208]
 [49.231487]
 [49.14498 ]], R is [[49.68182755]
 [49.75643921]
 [49.83030701]
 [49.90343475]
 [49.97583008]].
[2019-04-05 11:53:27,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:27,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:27,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-05 11:53:31,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:31,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:31,646] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-05 11:53:32,941] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1048324e-08 3.2780823e-05 2.2386814e-05 9.9993372e-01 1.0796894e-05
 1.2450838e-07 2.1424587e-07], sum to 1.0000
[2019-04-05 11:53:32,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8039
[2019-04-05 11:53:32,972] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.133333333333333, 31.33333333333334, 86.0, 0.0, 19.0, 19.52371668092094, -1.205618594898838, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 468600.0000, 
sim time next is 469200.0000, 
raw observation next is [-3.766666666666667, 30.66666666666667, 92.0, 0.0, 19.0, 19.54395360693624, -1.205539763739151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.358264081255771, 0.3066666666666667, 0.30666666666666664, 0.0, 0.08333333333333333, 0.12866280057801985, 0.09815341208694968, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8980974], dtype=float32), 1.720641]. 
=============================================
[2019-04-05 11:53:34,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8483378e-09 8.0969221e-06 1.1569582e-05 9.9996102e-01 1.9236453e-05
 1.4874895e-08 4.8560338e-08], sum to 1.0000
[2019-04-05 11:53:34,193] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5026
[2019-04-05 11:53:34,207] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.716666666666667, 71.0, 0.0, 0.0, 19.0, 18.52445812949706, -1.242557515637211, 0.0, 1.0, 56207.08184507273], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 784200.0000, 
sim time next is 784800.0000, 
raw observation next is [-7.8, 71.0, 0.0, 0.0, 19.0, 18.49143752424066, -1.241251527409815, 0.0, 1.0, 64911.44936393992], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.71, 0.0, 0.0, 0.08333333333333333, 0.040953127020054936, 0.08624949086339499, 0.0, 1.0, 0.3091021398282853], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21603656], dtype=float32), 2.0031781]. 
=============================================
[2019-04-05 11:53:34,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:53:34,750] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:34,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-05 11:53:36,578] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-05 11:53:36,578] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:53:36,578] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:53:36,578] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:36,578] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:53:36,579] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:36,579] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:53:36,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run15
[2019-04-05 11:53:36,601] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run15
[2019-04-05 11:53:36,602] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run15
[2019-04-05 11:54:44,183] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6155.8996 142744099.4696 -2202.8705
[2019-04-05 11:54:45,971] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.0901533]
[2019-04-05 11:54:45,971] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.274622843666667, 99.02406568, 0.0, 0.0, 21.0, 20.57851417508429, -0.75400798199869, 0.0, 1.0, 33457.31011825558]
[2019-04-05 11:54:45,972] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:54:45,973] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.5256626e-11 4.1869594e-08 1.3094504e-07 9.9999869e-01 1.1499435e-06
 8.9063840e-11 2.0604660e-10], sampled 0.9376682968941912
[2019-04-05 11:54:53,352] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.9308 171424657.3513 -2880.0027
[2019-04-05 11:55:14,251] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4710.6711 210627195.4204 -1642.2646
[2019-04-05 11:55:15,275] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 280000, evaluation results [280000.0, 5683.9307794573915, 171424657.3512828, -2880.002680936151, 6155.899637941897, 142744099.4696188, -2202.870476255624, 4710.671070355684, 210627195.42037585, -1642.264627811849]
[2019-04-05 11:55:18,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:55:18,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:55:18,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-05 11:55:24,959] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8685641e-11 1.7757905e-07 1.5702219e-07 9.9999905e-01 6.8793901e-07
 6.0981126e-10 1.3492860e-10], sum to 1.0000
[2019-04-05 11:55:24,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9041
[2019-04-05 11:55:24,994] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.0, 82.0, 47.0, 19.5, 18.88327117500767, -1.161757449377863, 0.0, 1.0, 21790.55175759039], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 658800.0000, 
sim time next is 659400.0000, 
raw observation next is [-0.6, 54.0, 82.33333333333334, 44.0, 19.5, 18.88793094015585, -1.164101369446252, 0.0, 1.0, 24120.18371106232], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.2744444444444445, 0.04861878453038674, 0.125, 0.07399424501298757, 0.11196621018458264, 0.0, 1.0, 0.11485801767172533], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.33324948], dtype=float32), -0.16938269]. 
=============================================
[2019-04-05 11:55:28,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.7754738e-10 1.4359381e-06 3.7463542e-07 9.9999356e-01 4.6932778e-06
 3.6600829e-09 4.5528900e-10], sum to 1.0000
[2019-04-05 11:55:28,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1696
[2019-04-05 11:55:28,851] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 56.0, 81.33333333333333, 53.0, 19.0, 18.61786377066255, -1.231596707546782, 0.0, 1.0, 22425.43052078709], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 657600.0000, 
sim time next is 658200.0000, 
raw observation next is [-0.7, 55.0, 81.66666666666667, 50.0, 19.0, 18.60141136857952, -1.234271605062197, 0.0, 1.0, 22274.44257135202], 
processed observation next is [0.0, 0.6086956521739131, 0.443213296398892, 0.55, 0.27222222222222225, 0.055248618784530384, 0.08333333333333333, 0.05011761404829329, 0.08857613164593436, 0.0, 1.0, 0.10606877414929534], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.687785], dtype=float32), 0.13037582]. 
=============================================
[2019-04-05 11:55:30,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0729050e-10 2.3062626e-07 5.3334418e-07 9.9999666e-01 2.5591266e-06
 2.1787951e-09 7.7570252e-09], sum to 1.0000
[2019-04-05 11:55:30,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9384
[2019-04-05 11:55:30,700] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.28333333333333, 54.83333333333334, 0.0, 0.0, 19.5, 18.87637843844389, -1.203948812568111, 0.0, 1.0, 72583.02070347118], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 435000.0000, 
sim time next is 435600.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 19.5, 18.94419970722497, -1.201898311431428, 0.0, 1.0, 23167.57266190082], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.125, 0.07868330893541427, 0.09936722952285737, 0.0, 1.0, 0.11032177458048009], 
reward next is 0.9286, 
noisyNet noise sample is [array([2.2919226], dtype=float32), 1.2483293]. 
=============================================
[2019-04-05 11:55:34,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 11:55:34,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:55:34,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-05 11:55:35,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3425429e-10 3.3812591e-07 2.9494524e-06 9.9999571e-01 9.4721469e-07
 5.3790838e-10 1.7418539e-08], sum to 1.0000
[2019-04-05 11:55:35,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5585
[2019-04-05 11:55:35,583] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 19.0, 18.61971675225444, -1.225032646363212, 0.0, 1.0, 18732.1294237623], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 876000.0000, 
sim time next is 876600.0000, 
raw observation next is [-1.45, 77.5, 0.0, 0.0, 19.0, 18.69438389794264, -1.223759943995364, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.422437673130194, 0.775, 0.0, 0.0, 0.08333333333333333, 0.0578653248285533, 0.09208001866821203, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5582488], dtype=float32), -1.3998268]. 
=============================================
[2019-04-05 11:55:35,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2086190e-14 4.5959623e-09 5.5118945e-09 1.0000000e+00 3.3519076e-08
 1.4620541e-12 3.8073112e-12], sum to 1.0000
[2019-04-05 11:55:35,636] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5507
[2019-04-05 11:55:35,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.183333333333333, 91.5, 0.0, 0.0, 19.0, 19.10337595354252, -0.9836149453370692, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1461000.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 19.0, 19.12602486075459, -0.9880147227604085, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.08333333333333333, 0.09383540506288253, 0.17066175907986383, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20695232], dtype=float32), -1.7307774]. 
=============================================
[2019-04-05 11:55:37,189] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1400592e-08 7.6188786e-05 7.7551158e-06 9.9989295e-01 2.3007231e-05
 7.0813542e-09 8.6854826e-08], sum to 1.0000
[2019-04-05 11:55:37,191] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0638
[2019-04-05 11:55:37,205] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.15, 51.5, 0.0, 0.0, 19.0, 18.47715220667916, -1.287273783440071, 0.0, 1.0, 44476.08347698137], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 426600.0000, 
sim time next is 427200.0000, 
raw observation next is [-11.33333333333333, 52.33333333333334, 0.0, 0.0, 19.0, 18.4739583584074, -1.290926661250569, 0.0, 1.0, 46461.71675598371], 
processed observation next is [1.0, 0.9565217391304348, 0.14866112650046176, 0.5233333333333334, 0.0, 0.0, 0.08333333333333333, 0.039496529867283435, 0.069691112916477, 0.0, 1.0, 0.2212462702665891], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3477693], dtype=float32), -0.14592484]. 
=============================================
[2019-04-05 11:55:42,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3972050e-11 1.3596878e-06 5.3010484e-07 9.9999189e-01 6.3072389e-06
 2.0517536e-10 5.5612898e-10], sum to 1.0000
[2019-04-05 11:55:42,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5125
[2019-04-05 11:55:42,514] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 21.0, 20.50507713181455, -0.8266962023242382, 0.0, 1.0, 80377.6106023228], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 681000.0000, 
sim time next is 681600.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 21.0, 20.46507389927584, -0.8237857846952329, 0.0, 1.0, 78953.48483547397], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.25, 0.20542282493965333, 0.22540473843492237, 0.0, 1.0, 0.3759689754070189], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.02802318], dtype=float32), 0.8518316]. 
=============================================
[2019-04-05 11:55:42,806] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.96844173e-11 1.08424196e-07 9.15342326e-08 9.99997854e-01
 1.87391061e-06 6.35149600e-10 2.75648682e-09], sum to 1.0000
[2019-04-05 11:55:42,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6889
[2019-04-05 11:55:42,820] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.716666666666667, 92.0, 0.0, 0.0, 19.0, 19.11724379657564, -1.004464083318897, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1667400.0000, 
sim time next is 1668000.0000, 
raw observation next is [4.433333333333334, 92.0, 0.0, 0.0, 19.0, 19.10903449476029, -1.009711743553009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5854108956602032, 0.92, 0.0, 0.0, 0.08333333333333333, 0.09241954123002423, 0.16342941881566367, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34381768], dtype=float32), 1.3029382]. 
=============================================
[2019-04-05 11:55:42,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[57.738403]
 [57.84039 ]
 [58.220387]
 [58.591663]
 [58.928898]], R is [[56.79692459]
 [57.22895432]
 [57.6566658 ]
 [58.08010101]
 [58.49930191]].
[2019-04-05 11:55:45,129] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0332785e-11 5.7146501e-08 1.0615450e-08 9.9999893e-01 1.0968006e-06
 3.9308716e-11 2.0080154e-10], sum to 1.0000
[2019-04-05 11:55:45,131] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7371
[2019-04-05 11:55:45,137] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.6, 83.0, 0.0, 0.0, 19.5, 19.58594518349307, -0.8247956065827394, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1144800.0000, 
sim time next is 1145400.0000, 
raw observation next is [11.78333333333333, 82.5, 0.0, 0.0, 19.5, 19.55981762997204, -0.8297616542443548, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.789012003693444, 0.825, 0.0, 0.0, 0.125, 0.1299848024976701, 0.2234127819185484, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.342984], dtype=float32), 1.0049226]. 
=============================================
[2019-04-05 11:55:48,295] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1860720e-14 1.5132013e-09 4.0158472e-09 9.9999988e-01 1.0087182e-07
 9.6141682e-13 1.2660138e-13], sum to 1.0000
[2019-04-05 11:55:48,296] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6276
[2019-04-05 11:55:48,301] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.05, 74.0, 0.0, 0.0, 19.0, 19.67905727622055, -0.809176878833061, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1125000.0000, 
sim time next is 1125600.0000, 
raw observation next is [10.86666666666667, 75.0, 0.0, 0.0, 19.0, 19.65175855413514, -0.8170878351071673, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7636195752539245, 0.75, 0.0, 0.0, 0.08333333333333333, 0.13764654617792824, 0.2276373882976109, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4150546], dtype=float32), -0.1450224]. 
=============================================
[2019-04-05 11:55:49,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8990629e-12 1.9953459e-08 3.1576814e-08 9.9999893e-01 1.0699780e-06
 5.1260603e-11 1.3171927e-10], sum to 1.0000
[2019-04-05 11:55:49,381] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7213
[2019-04-05 11:55:49,386] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.91666666666667, 68.33333333333333, 98.66666666666666, 0.0, 19.0, 19.01444169928161, -0.9617216080729672, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1158600.0000, 
sim time next is 1159200.0000, 
raw observation next is [17.2, 67.0, 106.5, 0.0, 19.0, 19.02771100252857, -0.9565071018716572, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9390581717451525, 0.67, 0.355, 0.0, 0.08333333333333333, 0.08564258354404745, 0.18116429937611425, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7875457], dtype=float32), -0.56339633]. 
=============================================
[2019-04-05 11:55:50,870] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-05 11:55:50,872] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:55:50,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:55:50,873] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:55:50,873] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:55:50,874] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:55:50,875] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:55:50,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run16
[2019-04-05 11:55:50,901] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run16
[2019-04-05 11:55:50,916] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run16
[2019-04-05 11:56:25,084] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09215304]
[2019-04-05 11:56:25,084] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.4, 78.0, 0.0, 0.0, 19.0, 18.50832939764555, -1.297120068153857, 0.0, 1.0, 88574.12663556058]
[2019-04-05 11:56:25,084] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 11:56:25,085] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.2476513e-09 2.0157042e-06 4.3190012e-06 9.9997342e-01 2.0280557e-05
 1.5226524e-08 2.4001336e-08], sampled 0.3485611924316949
[2019-04-05 11:57:04,373] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5544.2178 156019284.2304 -1688.6281
[2019-04-05 11:57:07,703] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.9308 171424657.3513 -2880.0027
[2019-04-05 11:57:16,034] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5838.4417 185048419.1411 -2785.1128
[2019-04-05 11:57:17,059] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 300000, evaluation results [300000.0, 5683.9307794573915, 171424657.3512828, -2880.002680936151, 5544.217813201499, 156019284.23035407, -1688.628106296839, 5838.441675212122, 185048419.14109835, -2785.112797972731]
[2019-04-05 11:57:20,053] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0548276e-10 1.1661361e-07 1.3172408e-07 9.9999952e-01 1.8560807e-07
 8.3045720e-10 3.9417022e-10], sum to 1.0000
[2019-04-05 11:57:20,054] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4361
[2019-04-05 11:57:20,063] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.2333333333333334, 54.66666666666667, 123.1666666666667, 504.0, 19.0, 19.89914094106388, -1.024764159698829, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 735600.0000, 
sim time next is 736200.0000, 
raw observation next is [-0.04999999999999999, 53.5, 131.0, 449.0, 19.0, 19.8744508406544, -1.03459484780413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.461218836565097, 0.535, 0.43666666666666665, 0.49613259668508286, 0.08333333333333333, 0.15620423672120007, 0.15513505073195666, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29062918], dtype=float32), -0.57431006]. 
=============================================
[2019-04-05 11:57:28,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8337363e-12 2.4985009e-06 3.3390393e-08 9.9999678e-01 6.7562758e-07
 2.3247964e-11 3.1178119e-11], sum to 1.0000
[2019-04-05 11:57:28,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7597
[2019-04-05 11:57:28,498] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 18.85677043622991, -1.035017491274858, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1720800.0000, 
sim time next is 1721400.0000, 
raw observation next is [0.4166666666666667, 92.5, 0.0, 0.0, 19.0, 18.82043448560798, -1.094951035321405, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.47414589104339805, 0.925, 0.0, 0.0, 0.08333333333333333, 0.06836954046733161, 0.1350163215595317, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2067314], dtype=float32), 0.8370423]. 
=============================================
[2019-04-05 11:57:29,143] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5619654e-12 2.5584592e-08 6.2028898e-09 1.0000000e+00 6.8400663e-09
 8.8579890e-12 1.0220405e-11], sum to 1.0000
[2019-04-05 11:57:29,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2019
[2019-04-05 11:57:29,152] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.666666666666668, 65.0, 0.0, 0.0, 19.0, 20.70251637680093, -0.6639986895499166, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1538400.0000, 
sim time next is 1539000.0000, 
raw observation next is [8.3, 67.0, 0.0, 0.0, 19.0, 20.60634547668602, -0.6740408874912075, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6925207756232689, 0.67, 0.0, 0.0, 0.08333333333333333, 0.21719545639050164, 0.2753197041695975, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33921844], dtype=float32), -0.97887737]. 
=============================================
[2019-04-05 11:57:29,176] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.61453 ]
 [59.650055]
 [59.741264]
 [59.79231 ]
 [59.804386]], R is [[60.43032837]
 [59.82602692]
 [59.22776794]
 [58.63549042]
 [58.04913712]].
[2019-04-05 11:57:33,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7092877e-12 2.7497611e-08 1.9199841e-07 9.9999964e-01 1.3763868e-07
 2.4641811e-10 4.0503606e-10], sum to 1.0000
[2019-04-05 11:57:33,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8110
[2019-04-05 11:57:33,460] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.2, 91.0, 0.0, 0.0, 20.0, 19.66118514934234, -0.8732965816059118, 0.0, 1.0, 36336.82039320029], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1735200.0000, 
sim time next is 1735800.0000, 
raw observation next is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 20.0, 19.65662623032944, -0.8735088460253815, 0.0, 1.0, 37343.7524516426], 
processed observation next is [0.0, 0.08695652173913043, 0.4672206832871654, 0.9100000000000001, 0.0, 0.0, 0.16666666666666666, 0.13805218586078669, 0.20883038465820616, 0.0, 1.0, 0.17782739262686953], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.1609351], dtype=float32), 1.4009134]. 
=============================================
[2019-04-05 11:57:34,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9504788e-12 2.3451152e-07 1.2408221e-07 9.9999928e-01 3.0181997e-07
 3.2997591e-10 3.5187983e-10], sum to 1.0000
[2019-04-05 11:57:34,383] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4056
[2019-04-05 11:57:34,390] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.2, 83.0, 11.5, 38.0, 19.0, 18.96871563760277, -1.029075330635242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1065600.0000, 
sim time next is 1066200.0000, 
raw observation next is [12.2, 83.0, 15.0, 48.33333333333334, 19.0, 18.97907295906874, -1.023593121684343, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.05, 0.05340699815837938, 0.08333333333333333, 0.08158941325572844, 0.15880229277188565, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7051448], dtype=float32), 0.49914634]. 
=============================================
[2019-04-05 11:57:37,327] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4525522e-10 3.3271601e-07 1.0054563e-06 9.9999428e-01 4.4354265e-06
 9.7194225e-11 3.9830157e-09], sum to 1.0000
[2019-04-05 11:57:37,329] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3984
[2019-04-05 11:57:37,366] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.00000000000001, 12.0, 0.0, 19.5, 19.89366839182016, -1.03367317608758, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2049000.0000, 
sim time next is 2049600.0000, 
raw observation next is [-3.9, 82.0, 6.999999999999999, 0.0, 19.5, 19.61209439539392, -1.088077880923772, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.02333333333333333, 0.0, 0.125, 0.1343411996161601, 0.13730737302540938, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0540339], dtype=float32), -0.6208247]. 
=============================================
[2019-04-05 11:57:38,277] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6412310e-10 5.9151080e-07 3.0249268e-07 9.9999881e-01 2.8583221e-07
 8.2404894e-10 1.6421399e-09], sum to 1.0000
[2019-04-05 11:57:38,280] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9400
[2019-04-05 11:57:38,287] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.15, 71.0, 0.0, 0.0, 19.0, 19.90113486564011, -0.7539795238019521, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1200600.0000, 
sim time next is 1201200.0000, 
raw observation next is [16.96666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 19.94785545513225, -0.7532717436801701, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9325946445060022, 0.7233333333333334, 0.0, 0.0, 0.08333333333333333, 0.1623212879276874, 0.24890941877327663, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09414104], dtype=float32), -0.5326692]. 
=============================================
[2019-04-05 11:57:41,450] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4522635e-09 6.3275966e-06 2.9599178e-06 9.9998999e-01 6.8547234e-07
 2.3066253e-09 1.1823326e-08], sum to 1.0000
[2019-04-05 11:57:41,451] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8672
[2019-04-05 11:57:41,459] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 62.0, 105.6666666666667, 0.0, 19.5, 19.7175723156919, -1.080187527065705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1953600.0000, 
sim time next is 1954200.0000, 
raw observation next is [-2.9, 62.0, 99.33333333333333, 0.0, 19.5, 19.72907080131854, -1.080754302694713, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.38227146814404434, 0.62, 0.3311111111111111, 0.0, 0.125, 0.14408923344321156, 0.13974856576842898, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4901723], dtype=float32), 1.1596502]. 
=============================================
[2019-04-05 11:57:44,935] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3722881e-10 3.5662728e-07 4.3039086e-07 9.9999821e-01 9.1616062e-07
 4.2126908e-10 4.6320006e-09], sum to 1.0000
[2019-04-05 11:57:44,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4574
[2019-04-05 11:57:44,952] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 18.55640674863169, -1.256827291107213, 0.0, 1.0, 46190.81785903552], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2001600.0000, 
sim time next is 2002200.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 18.51004203209568, -1.261212119435311, 0.0, 1.0, 66311.2756967743], 
processed observation next is [1.0, 0.17391304347826086, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.04250350267463995, 0.07959596018822968, 0.0, 1.0, 0.3157679795084491], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11150958], dtype=float32), -1.1096697]. 
=============================================
[2019-04-05 11:57:47,118] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4455691e-11 4.4604064e-08 8.1653063e-08 9.9999905e-01 8.2779809e-07
 8.2448304e-10 8.2185631e-10], sum to 1.0000
[2019-04-05 11:57:47,118] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3880
[2019-04-05 11:57:47,148] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 19.0, 18.62119798332841, -1.085790239380368, 1.0, 1.0, 37621.71410362479], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1454400.0000, 
sim time next is 1455000.0000, 
raw observation next is [1.183333333333333, 91.5, 0.0, 0.0, 19.0, 18.61778344953995, -1.084687922778733, 0.0, 1.0, 25855.44102877036], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.915, 0.0, 0.0, 0.08333333333333333, 0.05148195412832912, 0.13843735907375568, 0.0, 1.0, 0.12312114775604933], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07851514], dtype=float32), -0.4797431]. 
=============================================
[2019-04-05 11:57:47,159] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[63.008297]
 [62.695793]
 [62.422916]
 [61.77995 ]
 [61.179043]], R is [[63.85108185]
 [63.21257019]
 [62.58044434]
 [62.95463943]
 [63.32509232]].
[2019-04-05 11:57:47,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0856879e-12 9.1469992e-08 7.5196255e-08 9.9999952e-01 2.1647323e-07
 1.3185282e-10 1.0462037e-10], sum to 1.0000
[2019-04-05 11:57:47,986] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6107
[2019-04-05 11:57:48,008] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 19.5, 19.16123431915957, -1.050919684708428, 0.0, 1.0, 42103.72254651202], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1980000.0000, 
sim time next is 1980600.0000, 
raw observation next is [-6.100000000000001, 83.0, 0.0, 0.0, 19.5, 19.13944306623313, -1.09505011336028, 0.0, 1.0, 51082.51358790138], 
processed observation next is [1.0, 0.9565217391304348, 0.2936288088642659, 0.83, 0.0, 0.0, 0.125, 0.09495358885276091, 0.13498329554657337, 0.0, 1.0, 0.2432500647042923], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.58132917], dtype=float32), -2.1164846]. 
=============================================
[2019-04-05 11:57:48,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8567698e-09 2.6944201e-06 4.4126830e-07 9.9999416e-01 2.5854447e-06
 2.7597276e-09 1.3194785e-08], sum to 1.0000
[2019-04-05 11:57:48,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1189
[2019-04-05 11:57:48,300] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 79.0, 152.0, 0.0, 19.0, 18.54558554140954, -1.23002037333217, 1.0, 1.0, 167208.9797707107], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2034000.0000, 
sim time next is 2034600.0000, 
raw observation next is [-4.4, 79.00000000000001, 150.6666666666667, 0.0, 19.0, 18.75622191066899, -1.215182947207537, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3407202216066482, 0.7900000000000001, 0.5022222222222223, 0.0, 0.08333333333333333, 0.06301849255574925, 0.09493901759748764, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.399409], dtype=float32), -0.90441424]. 
=============================================
[2019-04-05 11:57:49,742] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.1872755e-12 8.5582670e-08 6.5906249e-08 9.9999964e-01 1.0088085e-07
 1.0903909e-11 8.9809209e-11], sum to 1.0000
[2019-04-05 11:57:49,744] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5796
[2019-04-05 11:57:49,759] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 19.5, 19.48251438897572, -0.9624019432980838, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 19.5, 19.41708949351631, -0.9768614982237981, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 0.125, 0.11809079112635927, 0.1743795005920673, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.6527458], dtype=float32), 0.6377774]. 
=============================================
[2019-04-05 11:57:50,440] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 11:57:50,447] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:57:50,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:57:50,448] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:57:50,448] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:57:50,448] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:57:50,449] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:57:50,455] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run17
[2019-04-05 11:57:50,455] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run17
[2019-04-05 11:57:50,481] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run17
[2019-04-05 11:58:26,803] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09369522]
[2019-04-05 11:58:26,804] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.1, 71.0, 0.0, 0.0, 19.0, 18.66480540411652, -1.123675727764483, 0.0, 1.0, 33275.14712502723]
[2019-04-05 11:58:26,804] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:58:26,805] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.9596727e-10 3.8266853e-07 6.4568206e-07 9.9999583e-01 3.1771251e-06
 1.1729477e-09 1.7197089e-09], sampled 0.08438538560492492
[2019-04-05 11:58:35,093] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09369522]
[2019-04-05 11:58:35,093] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.7, 42.66666666666667, 197.3333333333333, 407.3333333333334, 19.0, 19.35436122228124, -1.004341822329161, 1.0, 1.0, 0.0]
[2019-04-05 11:58:35,094] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 11:58:35,094] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.2183977e-10 1.6523093e-06 9.9920726e-07 9.9999392e-01 3.4545335e-06
 3.1911160e-09 5.5658029e-09], sampled 0.1668170167875921
[2019-04-05 11:58:57,021] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09369522]
[2019-04-05 11:58:57,022] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.507185562333333, 43.14181056, 7.698254495, 352.9477375833334, 19.0, 18.81352885966299, -1.089566790688405, 0.0, 1.0, 0.0]
[2019-04-05 11:58:57,022] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:58:57,023] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.3939618e-11 2.0603223e-07 2.6299401e-07 9.9999797e-01 1.5395096e-06
 5.2394983e-10 7.4207251e-10], sampled 0.2564491020095664
[2019-04-05 11:58:59,216] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6151.6030 143233965.9017 -2211.0684
[2019-04-05 11:59:06,499] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09369522]
[2019-04-05 11:59:06,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [13.16262945, 25.57127822, 80.49652916, 747.9575507, 19.0, 21.41041720769957, -0.5422187163804887, 1.0, 1.0, 0.0]
[2019-04-05 11:59:06,500] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 11:59:06,500] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.5222072e-11 2.7458853e-07 1.7554719e-07 9.9999905e-01 6.2901904e-07
 3.8131398e-10 5.5736898e-10], sampled 0.08948389697066506
[2019-04-05 11:59:08,740] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.9308 171424657.3513 -2880.0027
[2019-04-05 11:59:15,065] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5833.3236 185585865.7558 -2787.3483
[2019-04-05 11:59:16,090] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 320000, evaluation results [320000.0, 5683.9307794573915, 171424657.3512828, -2880.002680936151, 6151.603040214376, 143233965.9017238, -2211.068365436391, 5833.32364870093, 185585865.75580564, -2787.3482685331523]
[2019-04-05 11:59:18,864] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3086278e-13 7.9359280e-10 4.1593982e-09 1.0000000e+00 2.4056469e-08
 1.9595779e-12 9.4292217e-13], sum to 1.0000
[2019-04-05 11:59:18,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1431
[2019-04-05 11:59:18,893] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 19.5, 19.18322602150049, -1.099398090018363, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2076600.0000, 
sim time next is 2077200.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 19.5, 19.13524904280101, -1.110250664961568, 0.0, 1.0, 42257.47631408981], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.91, 0.0, 0.0, 0.125, 0.09460408690008422, 0.12991644501281066, 0.0, 1.0, 0.20122607768614192], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.21875863], dtype=float32), -1.580598]. 
=============================================
[2019-04-05 11:59:19,358] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4965883e-13 1.2008881e-08 1.7160211e-08 1.0000000e+00 3.4034393e-08
 5.6632663e-12 6.5403351e-12], sum to 1.0000
[2019-04-05 11:59:19,366] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4539
[2019-04-05 11:59:19,378] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 79.0, 0.0, 0.0, 19.0, 18.63361137737679, -1.232485840020465, 0.0, 1.0, 23493.75072705388], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 860400.0000, 
sim time next is 861000.0000, 
raw observation next is [-2.716666666666667, 79.16666666666667, 0.0, 0.0, 19.0, 18.61309212057347, -1.23719089089959, 0.0, 1.0, 39981.15829075481], 
processed observation next is [1.0, 1.0, 0.3873499538319483, 0.7916666666666667, 0.0, 0.0, 0.08333333333333333, 0.05109101004778912, 0.08760303636680337, 0.0, 1.0, 0.19038646805121337], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6966054], dtype=float32), 0.29468614]. 
=============================================
[2019-04-05 11:59:19,385] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.517807]
 [63.317364]
 [62.9309  ]
 [62.511425]
 [62.643185]], R is [[63.93198395]
 [64.29266357]
 [64.6497345 ]
 [65.00323486]
 [65.35320282]].
[2019-04-05 11:59:24,025] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1065392e-09 1.2487268e-05 1.5384852e-06 9.9996662e-01 1.9301853e-05
 1.6990967e-08 9.8169197e-09], sum to 1.0000
[2019-04-05 11:59:24,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7596
[2019-04-05 11:59:24,047] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.583333333333333, 65.5, 66.0, 0.0, 19.0, 19.58121845779657, -1.157607979646078, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2130600.0000, 
sim time next is 2131200.0000, 
raw observation next is [-4.5, 65.0, 56.0, 0.0, 19.0, 19.36342338691206, -1.150674428952454, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.65, 0.18666666666666668, 0.0, 0.08333333333333333, 0.11361861557600506, 0.11644185701584868, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.292125], dtype=float32), -0.82500935]. 
=============================================
[2019-04-05 11:59:44,268] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.5375822e-15 2.6629441e-09 1.4374358e-09 1.0000000e+00 2.2368789e-09
 1.3920709e-13 2.8723311e-12], sum to 1.0000
[2019-04-05 11:59:44,269] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9681
[2019-04-05 11:59:44,287] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 21.0, 20.58348957846347, -0.7449243531608877, 0.0, 1.0, 52079.99254981463], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2153400.0000, 
sim time next is 2154000.0000, 
raw observation next is [-6.9, 82.66666666666667, 0.0, 0.0, 21.0, 20.58155615623071, -0.7609861032636678, 0.0, 1.0, 44562.04379869891], 
processed observation next is [1.0, 0.9565217391304348, 0.27146814404432135, 0.8266666666666667, 0.0, 0.0, 0.25, 0.21512967968589258, 0.2463379655787774, 0.0, 1.0, 0.21220020856523292], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.16970143], dtype=float32), 0.38402563]. 
=============================================
[2019-04-05 11:59:44,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[67.987  ]
 [67.20076]
 [66.84773]
 [66.29964]
 [65.27939]], R is [[68.65100861]
 [68.67878723]
 [68.70628357]
 [68.73350525]
 [68.7604599 ]].
[2019-04-05 11:59:53,276] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 11:59:53,277] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 11:59:53,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:59:53,279] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 11:59:53,280] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:59:53,281] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 11:59:53,284] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 11:59:53,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run18
[2019-04-05 11:59:53,303] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run18
[2019-04-05 11:59:53,304] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run18
[2019-04-05 12:01:01,122] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6155.8996 142744099.4696 -2202.8705
[2019-04-05 12:01:08,393] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09435386]
[2019-04-05 12:01:08,394] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.375915734666667, 84.39346259666668, 0.0, 0.0, 19.0, 18.68184238591752, -1.176350097633164, 0.0, 1.0, 162959.260051719]
[2019-04-05 12:01:08,394] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:01:08,394] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.1545704e-11 4.0500910e-07 3.4863893e-07 9.9999738e-01 1.8883223e-06
 3.1331002e-10 4.3108117e-10], sampled 0.43301614761147267
[2019-04-05 12:01:16,912] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5838.4417 185048419.1411 -2785.1128
[2019-04-05 12:01:24,445] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4819.7596 197828395.9200 -1618.9479
[2019-04-05 12:01:25,470] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 340000, evaluation results [340000.0, 4819.759572514583, 197828395.9200314, -1618.9478726637367, 6155.899637941897, 142744099.4696188, -2202.870476255624, 5838.441675212122, 185048419.14109835, -2785.112797972731]
[2019-04-05 12:01:33,963] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9716554e-09 4.8436473e-06 8.7868621e-06 9.9997997e-01 6.3355701e-06
 5.2995293e-09 2.6458103e-08], sum to 1.0000
[2019-04-05 12:01:33,964] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3263
[2019-04-05 12:01:33,971] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 19.0, 20.10670396194573, -0.8551830735490794, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3328800.0000, 
sim time next is 3329400.0000, 
raw observation next is [-5.166666666666667, 54.0, 116.6666666666667, 807.3333333333334, 19.0, 20.06802195466177, -0.8578751456841132, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.31948291782086796, 0.54, 0.388888888888889, 0.8920810313075507, 0.08333333333333333, 0.17233516288848083, 0.21404161810529562, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4407065], dtype=float32), 0.4687851]. 
=============================================
[2019-04-05 12:01:35,165] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7904157e-10 3.6930642e-06 7.2860485e-07 9.9999440e-01 1.2100230e-06
 5.7779675e-10 6.1284350e-10], sum to 1.0000
[2019-04-05 12:01:35,167] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8203
[2019-04-05 12:01:35,185] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 28.0, 88.5, 838.5, 19.0, 18.73338180280979, -1.166697167747881, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2466000.0000, 
sim time next is 2466600.0000, 
raw observation next is [1.7, 27.83333333333334, 88.0, 836.3333333333334, 19.0, 18.72491856554251, -1.167059132894711, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5096952908587258, 0.2783333333333334, 0.29333333333333333, 0.9241252302025783, 0.08333333333333333, 0.06040988046187579, 0.11098028903509631, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37625882], dtype=float32), -0.14693396]. 
=============================================
[2019-04-05 12:01:36,110] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6245785e-09 1.5446608e-06 1.9694774e-06 9.9994719e-01 4.9161015e-05
 2.9103557e-09 1.9056722e-08], sum to 1.0000
[2019-04-05 12:01:36,111] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2688
[2019-04-05 12:01:36,124] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.5, 74.0, 0.0, 0.0, 19.0, 18.68038462625474, -1.192834169874174, 0.0, 1.0, 31892.60070699072], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3385800.0000, 
sim time next is 3386400.0000, 
raw observation next is [-5.333333333333333, 73.0, 0.0, 0.0, 19.0, 18.6671797955079, -1.202099307866528, 0.0, 1.0, 38077.60880005812], 
processed observation next is [1.0, 0.17391304347826086, 0.3148661126500462, 0.73, 0.0, 0.0, 0.08333333333333333, 0.05559831629232489, 0.0993002307111573, 0.0, 1.0, 0.18132194666694343], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21036331], dtype=float32), 0.58972085]. 
=============================================
[2019-04-05 12:01:37,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.9904095e-12 2.7501299e-06 6.0410805e-08 9.9999535e-01 1.7336946e-06
 3.7229959e-11 2.3963367e-10], sum to 1.0000
[2019-04-05 12:01:37,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9675
[2019-04-05 12:01:37,134] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.166666666666667, 100.0, 0.0, 0.0, 19.0, 19.04403917521862, -1.192654155991984, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3129000.0000, 
sim time next is 3129600.0000, 
raw observation next is [3.333333333333333, 100.0, 0.0, 0.0, 19.0, 18.96154875373433, -1.187935845496228, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5549399815327793, 1.0, 0.0, 0.0, 0.08333333333333333, 0.08012906281119421, 0.10402138483459063, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02564015], dtype=float32), 0.004909015]. 
=============================================
[2019-04-05 12:01:38,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5277952e-09 1.1039984e-05 4.9162854e-06 9.9996018e-01 2.3798066e-05
 4.5151079e-08 5.5304721e-09], sum to 1.0000
[2019-04-05 12:01:38,713] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8997
[2019-04-05 12:01:38,731] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 79.0, 126.0, 0.0, 19.0, 19.25072936525734, -1.150975837322978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2037600.0000, 
sim time next is 2038200.0000, 
raw observation next is [-4.0, 80.16666666666667, 118.6666666666667, 0.0, 19.0, 19.31339020860808, -1.148420766856045, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.8016666666666667, 0.39555555555555566, 0.0, 0.08333333333333333, 0.10944918405067335, 0.11719307771465164, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.2379382], dtype=float32), 0.14617074]. 
=============================================
[2019-04-05 12:01:40,393] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3032835e-11 9.1410790e-08 4.8697771e-08 9.9999869e-01 1.1700168e-06
 1.1444599e-10 5.4077909e-10], sum to 1.0000
[2019-04-05 12:01:40,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5447
[2019-04-05 12:01:40,409] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 19.5, 19.04870296041229, -1.124547398080073, 0.0, 1.0, 32238.07050706153], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2352600.0000, 
sim time next is 2353200.0000, 
raw observation next is [-3.0, 66.33333333333333, 0.0, 0.0, 19.5, 19.03716722641413, -1.127115445668177, 0.0, 1.0, 41762.70732931824], 
processed observation next is [0.0, 0.21739130434782608, 0.3795013850415513, 0.6633333333333333, 0.0, 0.0, 0.125, 0.08643060220117753, 0.12429485144394103, 0.0, 1.0, 0.19887003490151542], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.2959536], dtype=float32), -1.6130263]. 
=============================================
[2019-04-05 12:01:41,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2506827e-13 2.2956634e-08 6.7580849e-08 9.9999988e-01 4.5119123e-08
 3.6678854e-12 5.0118396e-11], sum to 1.0000
[2019-04-05 12:01:41,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4725
[2019-04-05 12:01:41,314] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.399999999999999, 61.5, 0.0, 0.0, 19.5, 19.189864618868, -1.081377372356504, 0.0, 1.0, 18722.65683538079], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2591400.0000, 
sim time next is 2592000.0000, 
raw observation next is [-4.5, 62.0, 0.0, 0.0, 19.5, 19.15596504574307, -1.089138219200803, 0.0, 1.0, 36827.04468815078], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.62, 0.0, 0.0, 0.125, 0.09633042047858915, 0.13695392693306566, 0.0, 1.0, 0.17536687946738466], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.19163883], dtype=float32), 1.2232069]. 
=============================================
[2019-04-05 12:01:41,323] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8878549e-12 6.4052483e-07 3.2216686e-08 9.9999881e-01 5.7140852e-07
 6.9135320e-10 1.2322847e-09], sum to 1.0000
[2019-04-05 12:01:41,327] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[65.50949 ]
 [65.50633 ]
 [65.283646]
 [65.19776 ]
 [65.16229 ]], R is [[64.91725922]
 [65.1966629 ]
 [65.47327423]
 [65.74711609]
 [66.01821899]].
[2019-04-05 12:01:41,327] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4033
[2019-04-05 12:01:41,342] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.583333333333333, 86.0, 0.0, 0.0, 19.0, 18.61270399683976, -1.204512603505774, 0.0, 1.0, 18733.10370441614], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2081400.0000, 
sim time next is 2082000.0000, 
raw observation next is [-4.666666666666667, 86.0, 0.0, 0.0, 19.0, 18.69659781761152, -1.206343396444035, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.3333333333333333, 0.86, 0.0, 0.0, 0.08333333333333333, 0.05804981813429322, 0.09788553451865496, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.75507444], dtype=float32), 0.2533278]. 
=============================================
[2019-04-05 12:01:41,349] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[60.150555]
 [60.331425]
 [60.949764]
 [62.247826]
 [63.10487 ]], R is [[59.94576645]
 [60.34630966]
 [60.74284744]
 [61.13541794]
 [61.52406311]].
[2019-04-05 12:01:41,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4442476e-10 1.4865802e-06 1.7824907e-07 9.9998558e-01 1.2912536e-05
 1.0195328e-09 2.1711410e-10], sum to 1.0000
[2019-04-05 12:01:41,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9482
[2019-04-05 12:01:41,630] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.5, 19.32569255001306, -1.095224745856087, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2597400.0000, 
sim time next is 2598000.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 19.5, 19.27548390832616, -1.107332239766931, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.72, 0.0, 0.0, 0.125, 0.10629032569384655, 0.130889253411023, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([2.044552], dtype=float32), -0.92650354]. 
=============================================
[2019-04-05 12:01:41,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[56.892826]
 [57.392044]
 [58.31858 ]
 [58.61506 ]
 [59.256332]], R is [[56.69905853]
 [57.06063843]
 [57.41860199]
 [57.77298737]
 [58.12382889]].
[2019-04-05 12:01:44,430] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.6174745e-10 2.7657420e-06 5.3653133e-07 9.9999237e-01 4.2918518e-06
 1.0974488e-09 1.2944019e-08], sum to 1.0000
[2019-04-05 12:01:44,435] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7588
[2019-04-05 12:01:44,452] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.95, 79.5, 0.0, 0.0, 19.0, 18.66606840527221, -1.216646861191908, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2687400.0000, 
sim time next is 2688000.0000, 
raw observation next is [-12.26666666666667, 80.66666666666666, 0.0, 0.0, 19.0, 18.61882916392037, -1.250286091348517, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.12280701754385957, 0.8066666666666665, 0.0, 0.0, 0.08333333333333333, 0.051569096993364205, 0.08323796955049434, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1570252], dtype=float32), -2.0263574]. 
=============================================
[2019-04-05 12:01:44,459] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[53.207813]
 [53.647278]
 [54.323284]
 [54.81821 ]
 [55.54816 ]], R is [[52.89699936]
 [53.36803055]
 [53.83435059]
 [54.29600906]
 [54.75304794]].
[2019-04-05 12:01:48,095] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5440304e-09 2.0732157e-05 6.4486189e-06 9.9996793e-01 4.6428972e-06
 3.7246327e-08 1.6275402e-07], sum to 1.0000
[2019-04-05 12:01:48,097] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0055
[2019-04-05 12:01:48,115] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 69.0, 0.0, 0.0, 19.0, 18.71259842457643, -1.233676823971011, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2223600.0000, 
sim time next is 2224200.0000, 
raw observation next is [-4.5, 68.5, 0.0, 0.0, 19.0, 18.4632403364932, -1.225096853513828, 1.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.685, 0.0, 0.0, 0.08333333333333333, 0.038603361374433355, 0.09163438216205733, 1.0, 1.0, 0.9343709972347434], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5991841], dtype=float32), -1.2469057]. 
=============================================
[2019-04-05 12:01:48,589] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6229161e-10 4.6599948e-06 8.5924961e-07 9.9996912e-01 2.5437575e-05
 5.1843821e-11 2.6444169e-10], sum to 1.0000
[2019-04-05 12:01:48,592] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0754
[2019-04-05 12:01:48,602] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 18.53892508585358, -1.173649319608448, 0.0, 1.0, 84473.25539394768], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2762400.0000, 
sim time next is 2763000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 18.59557601094271, -1.165770470476848, 0.0, 1.0, 28356.21549522193], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.04963133424522592, 0.11140984317438403, 0.0, 1.0, 0.1350295975962949], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3347854], dtype=float32), 0.9914852]. 
=============================================
[2019-04-05 12:01:48,605] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[62.957344]
 [62.55113 ]
 [61.844715]
 [61.449707]
 [61.42476 ]], R is [[63.33204269]
 [63.69872284]
 [64.06173706]
 [64.42111969]
 [64.77690887]].
[2019-04-05 12:01:52,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5590979e-12 5.4840848e-07 1.7549397e-07 9.9999905e-01 2.1906787e-07
 1.1783730e-10 8.2629625e-11], sum to 1.0000
[2019-04-05 12:01:52,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9237
[2019-04-05 12:01:52,365] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.5, 19.56066752693624, -0.9253446258629748, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.5, 19.50957468852812, -0.9320628872341493, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.125, 0.12579789071067657, 0.18931237092195022, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.74062264], dtype=float32), 0.5744652]. 
=============================================
[2019-04-05 12:01:56,929] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.11149084e-10 3.57794170e-06 3.75263789e-06 9.99886513e-01
 1.06237676e-04 6.28559371e-10 7.70814801e-10], sum to 1.0000
[2019-04-05 12:01:56,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7352
[2019-04-05 12:01:56,963] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 48.66666666666667, 147.6666666666667, 56.83333333333332, 19.0, 18.70155161527661, -1.166106902746105, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2385600.0000, 
sim time next is 2386200.0000, 
raw observation next is [0.0, 47.83333333333334, 135.3333333333333, 113.6666666666666, 19.0, 18.71150638536909, -1.165575997344151, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.47833333333333344, 0.45111111111111096, 0.125598526703499, 0.08333333333333333, 0.05929219878075761, 0.11147466755194964, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.565272], dtype=float32), 0.45461965]. 
=============================================
[2019-04-05 12:01:59,723] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-05 12:01:59,726] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:01:59,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:01:59,727] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:01:59,727] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:01:59,728] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:01:59,728] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:01:59,740] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run19
[2019-04-05 12:01:59,755] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run19
[2019-04-05 12:01:59,780] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run19
[2019-04-05 12:02:46,216] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09576533]
[2019-04-05 12:02:46,217] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.683333333333333, 53.0, 0.0, 0.0, 19.0, 18.4699427481342, -1.193730315107672, 0.0, 1.0, 115680.6427190225]
[2019-04-05 12:02:46,217] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:02:46,218] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.6374345e-11 2.1672665e-06 5.5383350e-07 9.9999416e-01 3.1058460e-06
 1.5545165e-10 3.7959305e-10], sampled 0.22097763059012177
[2019-04-05 12:02:51,061] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09576533]
[2019-04-05 12:02:51,061] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.4, 73.0, 36.0, 31.5, 19.0, 18.54589140920477, -1.1030619159215, 1.0, 1.0, 33862.94512704497]
[2019-04-05 12:02:51,061] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:02:51,062] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.7328467e-10 5.5487781e-06 2.1597170e-06 9.9998164e-01 1.0573656e-05
 1.7539636e-09 3.1667871e-09], sampled 0.148337154071976
[2019-04-05 12:03:07,910] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6155.8996 142744099.4696 -2202.8705
[2019-04-05 12:03:19,719] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5438.3237 178088828.0255 -2572.5514
[2019-04-05 12:03:24,942] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5838.4417 185048419.1411 -2785.1128
[2019-04-05 12:03:25,966] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 360000, evaluation results [360000.0, 5438.323690865777, 178088828.0254616, -2572.551397154288, 6155.899637941897, 142744099.4696188, -2202.870476255624, 5838.441675212122, 185048419.14109835, -2785.112797972731]
[2019-04-05 12:03:30,790] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0080731e-13 2.9933045e-07 2.1182474e-07 9.9999940e-01 5.3258088e-08
 4.5655674e-11 6.3652768e-12], sum to 1.0000
[2019-04-05 12:03:30,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6341
[2019-04-05 12:03:30,808] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 100.0, 0.0, 0.0, 19.5, 19.31837129278752, -0.9497523265267067, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3213000.0000, 
sim time next is 3213600.0000, 
raw observation next is [-1.666666666666667, 100.0, 0.0, 0.0, 19.5, 19.21472063885212, -0.968594998073739, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4164358264081256, 1.0, 0.0, 0.0, 0.125, 0.10122671990434344, 0.17713500064208698, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.3335054], dtype=float32), 0.71465945]. 
=============================================
[2019-04-05 12:03:32,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.46507348e-10 1.02478625e-05 3.72261479e-06 9.99883771e-01
 1.02277554e-04 1.07904641e-09 1.09444569e-08], sum to 1.0000
[2019-04-05 12:03:32,567] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9610
[2019-04-05 12:03:32,580] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 75.0, 50.66666666666667, 443.1666666666667, 19.5, 21.68275746876574, -0.596894068053856, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3256800.0000, 
sim time next is 3257400.0000, 
raw observation next is [-3.833333333333333, 76.0, 42.33333333333334, 375.3333333333334, 19.5, 21.67583885666015, -0.523880983784334, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3564173591874424, 0.76, 0.14111111111111113, 0.4147329650092082, 0.125, 0.30631990472167914, 0.325373005405222, 1.0, 1.0, 0.0], 
reward next is 0.6898, 
noisyNet noise sample is [array([2.2692225], dtype=float32), -0.9552217]. 
=============================================
[2019-04-05 12:03:42,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.66471502e-12 2.25641543e-06 4.65787764e-07 9.99994993e-01
 2.21572463e-06 1.01249176e-10 3.38922397e-11], sum to 1.0000
[2019-04-05 12:03:42,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1036
[2019-04-05 12:03:42,201] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 40.0, 70.5, 579.5, 19.5, 19.38410107436587, -0.9685703718798973, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3081600.0000, 
sim time next is 3082200.0000, 
raw observation next is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 19.5, 19.38587569149227, -0.9749393141191103, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4856879039704525, 0.4533333333333334, 0.22, 0.6058931860036832, 0.125, 0.11548964095768917, 0.17502022862696323, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.2945995], dtype=float32), 0.26984313]. 
=============================================
[2019-04-05 12:03:45,021] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3785162e-12 3.3909848e-07 3.1472386e-08 9.9999928e-01 3.1370396e-07
 3.6877424e-12 4.5086684e-11], sum to 1.0000
[2019-04-05 12:03:45,022] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8545
[2019-04-05 12:03:45,050] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 59.0, 0.0, 0.0, 19.0, 18.70461882452813, -1.203799730057516, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2847000.0000, 
sim time next is 2847600.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 18.60707848539762, -1.204320509027544, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.05058987378313488, 0.09855983032415201, 0.0, 1.0, 0.9343709972347434], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16777863], dtype=float32), -0.20941295]. 
=============================================
[2019-04-05 12:03:49,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3870287e-11 4.1134459e-07 1.5944785e-08 9.9999619e-01 3.5116170e-06
 3.8815021e-11 7.7106224e-11], sum to 1.0000
[2019-04-05 12:03:49,651] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2214
[2019-04-05 12:03:49,658] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.833333333333333, 37.5, 98.66666666666666, 546.0, 19.5, 19.57773795817238, -0.9043282589957605, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4205400.0000, 
sim time next is 4206000.0000, 
raw observation next is [2.666666666666667, 38.0, 83.33333333333333, 548.0, 19.5, 19.65325014653085, -0.9014946699938043, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5364727608494922, 0.38, 0.27777777777777773, 0.605524861878453, 0.125, 0.1377708455442376, 0.1995017766687319, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-2.4940722], dtype=float32), 0.9531317]. 
=============================================
[2019-04-05 12:03:49,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[61.863724]
 [61.66063 ]
 [61.432137]
 [61.210396]
 [60.960087]], R is [[62.31268311]
 [62.61812592]
 [62.92051315]
 [63.21987915]
 [63.51625061]].
[2019-04-05 12:03:55,645] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6440591e-09 4.5464552e-05 4.8084730e-06 9.9993849e-01 1.1288522e-05
 2.9109244e-09 7.8507458e-09], sum to 1.0000
[2019-04-05 12:03:55,650] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2000
[2019-04-05 12:03:55,660] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 18.83873156518645, -1.166342457004532, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3736200.0000, 
sim time next is 3736800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 18.89821819151001, -1.171404074589942, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.07485151595916761, 0.10953197513668604, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02934153], dtype=float32), -0.47743523]. 
=============================================
[2019-04-05 12:03:57,461] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-05 12:03:57,465] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:03:57,466] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:03:57,467] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:03:57,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:03:57,468] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:03:57,469] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:03:57,473] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run20
[2019-04-05 12:03:57,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run20
[2019-04-05 12:03:57,512] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run20
[2019-04-05 12:05:05,114] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09921856]
[2019-04-05 12:05:05,114] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.346601962, 45.90285031, 105.837040145, 795.35839435, 21.0, 20.74997372928484, -0.6087179791989008, 0.0, 1.0, 0.0]
[2019-04-05 12:05:05,114] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:05:05,115] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2546359e-13 1.8752644e-07 4.3282505e-08 9.9999952e-01 2.5207478e-07
 1.8785316e-12 3.6249378e-12], sampled 0.8548343900181891
[2019-04-05 12:05:10,614] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5541.3681 156316845.3764 -1685.1218
[2019-04-05 12:05:15,366] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.9308 171424657.3513 -2880.0027
[2019-04-05 12:05:36,108] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4711.0996 210627195.4204 -1642.2646
[2019-04-05 12:05:37,131] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 380000, evaluation results [380000.0, 5683.9307794573915, 171424657.3512828, -2880.002680936151, 5541.3681322765915, 156316845.37641057, -1685.1217788019396, 4711.099641784254, 210627195.42037585, -1642.264627811849]
[2019-04-05 12:05:37,341] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8918131e-13 6.2467599e-08 8.9924526e-08 9.9999762e-01 2.1379838e-06
 5.2770908e-12 1.2088100e-10], sum to 1.0000
[2019-04-05 12:05:37,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6027
[2019-04-05 12:05:37,349] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.166666666666667, 64.83333333333334, 0.0, 0.0, 19.5, 19.81552628445867, -0.9318714759621338, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4299000.0000, 
sim time next is 4299600.0000, 
raw observation next is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 19.5, 19.76543085619142, -0.9427123190638117, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6325023084025855, 0.6566666666666667, 0.0, 0.0, 0.125, 0.14711923801595153, 0.18576256031206276, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.51793754], dtype=float32), 0.15429826]. 
=============================================
[2019-04-05 12:05:37,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2157864e-11 6.6526895e-06 1.2658120e-06 9.9997807e-01 1.3969440e-05
 9.4943942e-10 2.4426610e-08], sum to 1.0000
[2019-04-05 12:05:37,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2486
[2019-04-05 12:05:37,597] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.666666666666667, 43.33333333333334, 25.66666666666666, 241.0, 19.5, 22.0705086946578, -0.6004913298993273, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3864000.0000, 
sim time next is 3864600.0000, 
raw observation next is [2.5, 44.5, 18.0, 179.0, 19.5, 21.87334146576406, -0.5729789600853268, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5318559556786704, 0.445, 0.06, 0.19779005524861878, 0.125, 0.32277845548033834, 0.30900701330489105, 1.0, 1.0, 0.0], 
reward next is 0.1988, 
noisyNet noise sample is [array([1.40511], dtype=float32), 1.4313897]. 
=============================================
[2019-04-05 12:05:43,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:05:43,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:05:43,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-05 12:05:48,375] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1077974e-09 1.6719574e-05 7.2862003e-06 9.9993372e-01 4.2238382e-05
 3.9153868e-08 1.9946985e-08], sum to 1.0000
[2019-04-05 12:05:48,376] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1706
[2019-04-05 12:05:48,388] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.666666666666667, 39.33333333333334, 115.3333333333333, 790.5, 19.0, 21.20723460428257, -0.7528590391037021, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5048400.0000, 
sim time next is 5049000.0000, 
raw observation next is [4.0, 38.5, 116.0, 809.0, 19.0, 21.28548929656743, -0.6443516549474895, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5734072022160666, 0.385, 0.38666666666666666, 0.8939226519337017, 0.08333333333333333, 0.2737907747139525, 0.2852161150175035, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.1105578], dtype=float32), -0.84954005]. 
=============================================
[2019-04-05 12:05:48,414] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[51.74614 ]
 [51.76002 ]
 [52.55272 ]
 [52.36387 ]
 [52.251274]], R is [[51.16762543]
 [50.65594864]
 [50.14939117]
 [49.64789963]
 [49.15142059]].
[2019-04-05 12:05:51,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:05:51,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:05:51,352] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-05 12:05:54,289] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.2787397e-12 4.4957251e-06 3.7753401e-07 9.9999332e-01 1.7344088e-06
 7.6748573e-11 2.9422922e-10], sum to 1.0000
[2019-04-05 12:05:54,289] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0930
[2019-04-05 12:05:54,300] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 19.0, 18.9874792678289, -1.040429039537744, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4191600.0000, 
sim time next is 4192200.0000, 
raw observation next is [1.5, 32.0, 118.0, 847.0, 19.0, 19.00184351674893, -1.036733217090243, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5041551246537397, 0.32, 0.3933333333333333, 0.9359116022099447, 0.08333333333333333, 0.08348695972907756, 0.15442226096991898, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5220054], dtype=float32), -0.11697323]. 
=============================================
[2019-04-05 12:05:54,476] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7962199e-11 4.0356281e-06 2.4133344e-06 9.9998808e-01 5.5204277e-06
 2.0196822e-10 5.2586285e-10], sum to 1.0000
[2019-04-05 12:05:54,481] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1984
[2019-04-05 12:05:54,495] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.0, 80.33333333333333, 0.0, 20.0, 20.48084338495254, -0.807608214296969, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4697400.0000, 
sim time next is 4698000.0000, 
raw observation next is [0.0, 92.0, 89.0, 0.0, 20.0, 20.50412752773742, -0.7977566073974881, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.2966666666666667, 0.0, 0.16666666666666666, 0.20867729397811838, 0.23408113086750396, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1200941], dtype=float32), 1.0430815]. 
=============================================
[2019-04-05 12:05:54,512] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.5242  ]
 [66.37824 ]
 [67.611206]
 [69.25402 ]
 [70.38397 ]], R is [[64.3596344 ]
 [63.71603775]
 [63.0788765 ]
 [62.4480896 ]
 [61.8236084 ]].
[2019-04-05 12:05:59,510] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2969652e-16 2.0519407e-07 6.4035397e-08 9.9999952e-01 1.4642590e-07
 1.9348548e-14 3.9128499e-13], sum to 1.0000
[2019-04-05 12:05:59,513] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0081
[2019-04-05 12:05:59,519] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.5, 62.0, 0.0, 0.0, 20.0, 21.66449473450041, -0.4209785402304802, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4402800.0000, 
sim time next is 4403400.0000, 
raw observation next is [8.350000000000001, 62.16666666666667, 0.0, 0.0, 20.0, 21.58797624776539, -0.435555872747466, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6939058171745154, 0.6216666666666667, 0.0, 0.0, 0.16666666666666666, 0.29899802064711584, 0.354814709084178, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.0054213], dtype=float32), -0.05534973]. 
=============================================
[2019-04-05 12:06:01,143] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.8564298e-11 2.5340352e-05 1.4200233e-05 9.9994302e-01 1.7459923e-05
 1.0591795e-09 1.0171275e-09], sum to 1.0000
[2019-04-05 12:06:01,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7579
[2019-04-05 12:06:01,156] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 18.72723271938224, -1.231604761826584, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3732000.0000, 
sim time next is 3732600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 18.60579847320516, -1.232329280724409, 0.0, 1.0, 196902.6101450711], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.05048320610042989, 0.08922357309186364, 0.0, 1.0, 0.937631476881291], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7053548], dtype=float32), 1.0867242]. 
=============================================
[2019-04-05 12:06:02,165] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8944707e-12 1.4409901e-05 7.0534554e-07 9.9995744e-01 2.7436718e-05
 2.5574293e-11 8.6338346e-11], sum to 1.0000
[2019-04-05 12:06:02,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5509
[2019-04-05 12:06:02,177] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 20.0, 20.37916367239441, -0.7794270685008232, 0.0, 1.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4473600.0000, 
sim time next is 4474200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 20.0, 20.15792212030576, -0.8015149191329933, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.16666666666666666, 0.17982684335881327, 0.23282836028900222, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.1191744], dtype=float32), -0.6360849]. 
=============================================
[2019-04-05 12:06:02,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:06:02,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:02,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-05 12:06:03,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:06:03,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:03,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-05 12:06:03,753] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6684195e-15 4.1666539e-07 3.4376765e-08 9.9999952e-01 8.6650786e-08
 1.6562993e-13 9.5619541e-13], sum to 1.0000
[2019-04-05 12:06:03,756] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0942
[2019-04-05 12:06:03,763] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.6, 71.66666666666667, 0.0, 0.0, 19.5, 19.39013494160119, -1.024410895086731, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4304400.0000, 
sim time next is 4305000.0000, 
raw observation next is [5.500000000000001, 72.33333333333333, 0.0, 0.0, 19.5, 19.34651581034086, -1.034219157140676, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.6149584487534627, 0.7233333333333333, 0.0, 0.0, 0.125, 0.11220965086173844, 0.15526028095310798, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.40243426], dtype=float32), 0.8140116]. 
=============================================
[2019-04-05 12:06:03,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.692444]
 [86.63167 ]
 [86.57878 ]
 [86.48692 ]
 [86.47538 ]], R is [[86.77754211]
 [86.83834076]
 [86.89852905]
 [86.95811462]
 [87.0171051 ]].
[2019-04-05 12:06:05,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:06:05,142] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:05,180] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-05 12:06:08,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:06:08,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:08,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-05 12:06:08,625] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-05 12:06:08,627] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:06:08,629] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:06:08,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:08,630] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:08,631] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:06:08,633] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:08,639] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run21
[2019-04-05 12:06:08,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run21
[2019-04-05 12:06:08,666] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run21
[2019-04-05 12:06:08,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:06:08,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:06:08,690] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-05 12:06:27,756] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10253816]
[2019-04-05 12:06:27,756] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.066666666666666, 51.0, 56.66666666666667, 2.833333333333333, 19.0, 19.82372957050683, -1.055505535633254, 1.0, 1.0, 0.0]
[2019-04-05 12:06:27,756] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:06:27,757] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.2907106e-10 3.9441107e-05 4.6738746e-06 9.9994397e-01 1.1901071e-05
 2.3775537e-09 5.2113118e-09], sampled 0.9941055205477357
[2019-04-05 12:06:35,966] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10253816]
[2019-04-05 12:06:35,966] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.2, 96.0, 0.0, 0.0, 19.0, 18.69840443242517, -1.127693102499217, 0.0, 1.0, 139988.8993356055]
[2019-04-05 12:06:35,966] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:06:35,967] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0102451e-13 3.1633923e-07 8.3580645e-08 9.9999905e-01 4.4377600e-07
 1.9020777e-12 3.2061357e-12], sampled 0.1710975231171038
[2019-04-05 12:07:13,396] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10253816]
[2019-04-05 12:07:13,396] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 35.33333333333333, 84.33333333333334, 692.6666666666667, 19.0, 20.78695655046769, -0.7380603422959489, 1.0, 1.0, 0.0]
[2019-04-05 12:07:13,397] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:07:13,398] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0539775e-09 5.3649179e-05 7.3127239e-06 9.9992335e-01 1.5732778e-05
 4.2050483e-09 9.6552029e-09], sampled 0.9717596119329753
[2019-04-05 12:07:16,282] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6155.8996 142744099.4696 -2202.8705
[2019-04-05 12:07:26,782] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.9308 171424657.3513 -2880.0027
[2019-04-05 12:07:40,351] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5207.3352 197362012.6221 -2206.0424
[2019-04-05 12:07:41,376] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 400000, evaluation results [400000.0, 5683.9307794573915, 171424657.3512828, -2880.002680936151, 6155.899637941897, 142744099.4696188, -2202.870476255624, 5207.335158748339, 197362012.62209785, -2206.0423646283225]
[2019-04-05 12:07:44,172] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:07:44,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:07:44,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-05 12:07:45,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5583843e-13 4.3254568e-06 6.1638403e-08 9.9999559e-01 2.6157599e-08
 4.6610105e-10 2.6900317e-11], sum to 1.0000
[2019-04-05 12:07:45,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8844
[2019-04-05 12:07:45,341] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.916666666666666, 54.5, 102.0, 615.0, 19.5, 20.20065122723205, -0.857372695338715, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4353000.0000, 
sim time next is 4353600.0000, 
raw observation next is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 19.5, 20.36299955304879, -0.8087307060103187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6712834718374886, 0.52, 0.34833333333333333, 0.7138121546961326, 0.125, 0.1969166294207326, 0.23042309799656044, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5630231], dtype=float32), -0.9994716]. 
=============================================
[2019-04-05 12:07:47,133] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9462402e-10 7.4257427e-05 7.4099529e-07 9.9990940e-01 1.5563615e-05
 5.6377669e-10 1.2311807e-09], sum to 1.0000
[2019-04-05 12:07:47,139] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2202
[2019-04-05 12:07:47,148] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 52.0, 187.0, 24.0, 21.0, 20.88960508191116, -0.6186297657345838, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 4539600.0000, 
sim time next is 4540200.0000, 
raw observation next is [2.166666666666667, 51.5, 207.0, 32.0, 21.0, 21.07473524140353, -0.5870638776645248, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.69, 0.03535911602209945, 0.25, 0.25622793678362754, 0.3043120407784917, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.2915254], dtype=float32), -0.64843756]. 
=============================================
[2019-04-05 12:07:49,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9395923e-09 3.1357876e-04 3.0318984e-06 9.9964440e-01 3.9114089e-05
 4.5455946e-09 2.0803995e-08], sum to 1.0000
[2019-04-05 12:07:49,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8119
[2019-04-05 12:07:49,762] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.666666666666666, 76.0, 86.5, 0.0, 19.0, 19.44839655242998, -1.141942006326651, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 207600.0000, 
sim time next is 208200.0000, 
raw observation next is [-7.483333333333333, 75.5, 94.0, 0.0, 19.0, 19.48553833838017, -1.143013158860769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.25530932594644506, 0.755, 0.31333333333333335, 0.0, 0.08333333333333333, 0.12379486153168084, 0.11899561371307699, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18493383], dtype=float32), 0.41361633]. 
=============================================
[2019-04-05 12:07:57,090] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.07717901e-13 3.56075475e-07 7.64986297e-09 9.99999523e-01
 1.15376324e-07 1.67427530e-11 1.32143801e-11], sum to 1.0000
[2019-04-05 12:07:57,095] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8921
[2019-04-05 12:07:57,102] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.95, 70.5, 0.0, 0.0, 19.0, 19.06304074879026, -1.092323975665138, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4332600.0000, 
sim time next is 4333200.0000, 
raw observation next is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 19.15021636390567, -1.090316304539136, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5715604801477379, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.09585136365880587, 0.136561231820288, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49490118], dtype=float32), -0.110844485]. 
=============================================
[2019-04-05 12:07:58,027] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:07:58,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:07:58,035] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-05 12:07:59,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:07:59,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:07:59,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.8549401e-13 4.8017689e-08 7.0475892e-08 9.9999964e-01 1.7895742e-07
 4.7388612e-12 2.4451574e-11], sum to 1.0000
[2019-04-05 12:07:59,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0324
[2019-04-05 12:07:59,802] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-05 12:07:59,822] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.5, 19.07168102292783, -1.051839000707954, 0.0, 1.0, 48522.68027655139], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4762800.0000, 
sim time next is 4763400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.5, 19.0295561459603, -1.055187866866147, 0.0, 1.0, 65069.02431580675], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.125, 0.08579634549669161, 0.14827071104461767, 0.0, 1.0, 0.30985249674193693], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.5841864], dtype=float32), 0.7198076]. 
=============================================
[2019-04-05 12:08:01,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:08:01,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:01,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-05 12:08:02,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:08:02,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:03,007] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-05 12:08:03,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7754336e-10 5.1611510e-06 2.5163902e-06 9.9998546e-01 6.9066923e-06
 4.2282455e-10 2.9116329e-09], sum to 1.0000
[2019-04-05 12:08:03,110] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2202
[2019-04-05 12:08:03,146] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.0, 26.0, 123.5, 855.0, 19.5, 22.20172893702462, -0.5186732030970553, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 5054400.0000, 
sim time next is 5055000.0000, 
raw observation next is [8.166666666666668, 25.83333333333334, 123.6666666666667, 858.3333333333334, 19.5, 22.30266281581383, -0.3899891052890929, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6888273314866113, 0.2583333333333334, 0.4122222222222223, 0.9484346224677717, 0.125, 0.3585552346511524, 0.37000363157030236, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.4964105], dtype=float32), -1.0852344]. 
=============================================
[2019-04-05 12:08:03,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.130455]
 [63.622993]
 [63.115425]
 [62.56175 ]
 [62.035126]], R is [[64.87366486]
 [64.96676636]
 [65.24567413]
 [65.52178955]
 [65.79514313]].
[2019-04-05 12:08:05,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:08:05,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:05,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-05 12:08:07,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6412661e-15 1.4674977e-07 1.0646844e-06 9.9999642e-01 2.4038220e-06
 1.1384776e-12 1.4077595e-11], sum to 1.0000
[2019-04-05 12:08:07,585] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9627
[2019-04-05 12:08:07,598] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.25, 84.0, 0.0, 0.0, 19.0, 18.66331349590572, -1.228026516848691, 0.0, 1.0, 53563.04294357805], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 531000.0000, 
sim time next is 531600.0000, 
raw observation next is [3.066666666666667, 83.33333333333334, 0.0, 0.0, 19.0, 18.64716260230416, -1.225317066949235, 0.0, 1.0, 49708.9459741701], 
processed observation next is [0.0, 0.13043478260869565, 0.5475530932594646, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.05393021685868001, 0.09156097768358833, 0.0, 1.0, 0.23670926654366714], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9355604], dtype=float32), -1.0578926]. 
=============================================
[2019-04-05 12:08:09,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:08:09,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:09,726] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-05 12:08:11,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.75568839e-08 3.61183542e-04 2.02344832e-04 9.99346793e-01
 8.95098055e-05 1.13513705e-08 9.34004163e-08], sum to 1.0000
[2019-04-05 12:08:11,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6801
[2019-04-05 12:08:11,734] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 61.0, 148.0, 106.0, 20.0, 20.29046244875309, -0.8717425131395752, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 138600.0000, 
sim time next is 139200.0000, 
raw observation next is [-6.700000000000001, 61.0, 133.5, 95.83333333333334, 20.0, 20.31776324137498, -0.871509110985639, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.445, 0.10589318600368325, 0.16666666666666666, 0.1931469367812483, 0.209496963004787, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41124162], dtype=float32), -0.13868743]. 
=============================================
[2019-04-05 12:08:13,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:08:13,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:13,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-05 12:08:13,307] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.7161572e-15 1.0506620e-06 1.1416832e-06 9.9999559e-01 2.1695714e-06
 3.2758013e-12 4.0730757e-13], sum to 1.0000
[2019-04-05 12:08:13,307] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4673
[2019-04-05 12:08:13,332] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 18.67459847378893, -1.16837461791794, 0.0, 1.0, 35011.24654755316], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 590400.0000, 
sim time next is 591000.0000, 
raw observation next is [-2.8, 86.33333333333333, 0.0, 0.0, 19.0, 18.73969890775517, -1.166914291295068, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8633333333333333, 0.0, 0.0, 0.08333333333333333, 0.06164157564626412, 0.11102856956831064, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01946096], dtype=float32), -2.7113113]. 
=============================================
[2019-04-05 12:08:13,339] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.0277 ]
 [90.8758 ]
 [90.49386]
 [89.8425 ]
 [89.80002]], R is [[90.88223267]
 [90.97341156]
 [91.06367493]
 [91.15303802]
 [91.24150848]].
[2019-04-05 12:08:16,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.04426477e-14 1.03806315e-07 4.25802575e-08 9.99999046e-01
 8.41283168e-07 5.49948406e-14 3.60098724e-13], sum to 1.0000
[2019-04-05 12:08:16,681] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1530
[2019-04-05 12:08:16,721] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.116666666666667, 59.16666666666667, 158.6666666666667, 95.33333333333334, 19.0, 18.55608863545787, -1.222371230437953, 0.0, 1.0, 25155.49258819685], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 652200.0000, 
sim time next is 652800.0000, 
raw observation next is [-1.933333333333333, 59.33333333333334, 170.3333333333333, 94.16666666666666, 19.0, 18.57110125178257, -1.219083305060225, 0.0, 1.0, 23134.0486601756], 
processed observation next is [0.0, 0.5652173913043478, 0.40904893813481075, 0.5933333333333334, 0.5677777777777776, 0.10405156537753221, 0.08333333333333333, 0.04759177098188072, 0.09363889831325833, 0.0, 1.0, 0.11016213647702666], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51872915], dtype=float32), -1.405707]. 
=============================================
[2019-04-05 12:08:18,224] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-05 12:08:18,228] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:08:18,228] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:18,230] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:08:18,231] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:18,231] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:08:18,233] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:08:18,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run22
[2019-04-05 12:08:18,254] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run22
[2019-04-05 12:08:18,268] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run22
[2019-04-05 12:08:40,194] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10434191]
[2019-04-05 12:08:40,195] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 19.0, 18.52688800543421, -1.233813284330183, 1.0, 1.0, 0.0]
[2019-04-05 12:08:40,195] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:08:40,197] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.8701018e-11 8.9537434e-06 1.4470454e-06 9.9998379e-01 5.8901242e-06
 1.7596427e-10 3.8387246e-10], sampled 0.6919655843012643
[2019-04-05 12:08:42,166] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10434191]
[2019-04-05 12:08:42,167] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.2, 59.66666666666666, 0.0, 0.0, 19.0, 18.66226173963193, -1.135827897294753, 0.0, 1.0, 38924.39120821129]
[2019-04-05 12:08:42,167] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:08:42,169] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.9974256e-12 4.3526520e-06 1.1368581e-06 9.9998987e-01 4.5383949e-06
 7.8335956e-11 1.2709579e-10], sampled 0.9873988606791694
[2019-04-05 12:09:13,689] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10434191]
[2019-04-05 12:09:13,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.6666666666666666, 100.0, 0.0, 0.0, 19.0, 18.77881363533919, -1.177233139056873, 0.0, 1.0, 0.0]
[2019-04-05 12:09:13,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:09:13,691] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.4858194e-15 1.4704349e-07 1.6409150e-08 9.9999976e-01 1.2994251e-07
 5.1220499e-14 1.5222386e-13], sampled 0.39626631482382424
[2019-04-05 12:09:25,660] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6155.8996 142744099.4696 -2202.8705
[2019-04-05 12:09:36,787] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.7904 171455007.0895 -2881.7173
[2019-04-05 12:09:49,651] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5207.3352 197362012.6221 -2206.0424
[2019-04-05 12:09:50,675] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 420000, evaluation results [420000.0, 5683.790387078998, 171455007.0894565, -2881.717303039442, 6155.899637941897, 142744099.4696188, -2202.870476255624, 5207.335158748339, 197362012.62209785, -2206.0423646283225]
[2019-04-05 12:09:50,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2575753e-08 6.5667939e-04 2.8333374e-05 9.9921703e-01 9.6483833e-05
 1.4349797e-07 1.3102079e-06], sum to 1.0000
[2019-04-05 12:09:50,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7502
[2019-04-05 12:09:50,807] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 68.5, 153.0, 0.0, 19.0, 19.41001677889857, -1.157439369569614, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 214200.0000, 
sim time next is 214800.0000, 
raw observation next is [-5.4, 67.33333333333333, 149.0, 0.0, 19.0, 19.40922862297505, -1.159504849775596, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.31301939058171746, 0.6733333333333333, 0.49666666666666665, 0.0, 0.08333333333333333, 0.11743571858125416, 0.11349838340813467, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.91659915], dtype=float32), 0.25800076]. 
=============================================
[2019-04-05 12:09:54,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9518709e-12 1.8008937e-05 4.6853393e-06 9.9997532e-01 2.0678592e-06
 7.1477463e-11 2.6308562e-09], sum to 1.0000
[2019-04-05 12:09:54,802] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5002
[2019-04-05 12:09:54,812] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 71.0, 0.0, 0.0, 19.0, 18.49143752424066, -1.241251527409815, 0.0, 1.0, 64911.44936393992], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 784800.0000, 
sim time next is 785400.0000, 
raw observation next is [-7.8, 71.5, 0.0, 0.0, 19.0, 18.48920487558965, -1.240376683633158, 0.0, 1.0, 52914.34863031875], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.715, 0.0, 0.0, 0.08333333333333333, 0.04076707296580414, 0.086541105455614, 0.0, 1.0, 0.2519730887158036], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0300929], dtype=float32), 0.3515151]. 
=============================================
[2019-04-05 12:09:57,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:09:57,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:09:57,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-05 12:10:02,408] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1227407e-09 6.4023485e-04 2.2561351e-05 9.9911720e-01 2.2013942e-04
 2.8934610e-09 5.4610365e-09], sum to 1.0000
[2019-04-05 12:10:02,413] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7199
[2019-04-05 12:10:02,469] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.67084502669248, -1.269634678752337, 1.0, 1.0, 70603.12941202732], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 243000.0000, 
sim time next is 243600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.52945953614259, -1.276705802459154, 1.0, 1.0, 56635.19367805449], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.04412162801188243, 0.074431399180282, 1.0, 1.0, 0.26969139846692614], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2711494], dtype=float32), 0.017906634]. 
=============================================
[2019-04-05 12:10:02,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8941349e-09 2.9955010e-04 1.3148942e-05 9.9953938e-01 1.4783930e-04
 2.7143562e-08 6.7168024e-08], sum to 1.0000
[2019-04-05 12:10:02,536] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5487
[2019-04-05 12:10:02,550] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.0, 53.0, 0.0, 0.0, 19.5, 18.86631651692451, -1.2178818604232, 0.0, 1.0, 51168.25263706219], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 440400.0000, 
sim time next is 441000.0000, 
raw observation next is [-10.9, 52.0, 0.0, 0.0, 19.5, 18.91979832780101, -1.214876943777274, 0.0, 1.0, 24270.33621508088], 
processed observation next is [1.0, 0.08695652173913043, 0.16066481994459833, 0.52, 0.0, 0.0, 0.125, 0.07664986065008428, 0.09504101874090869, 0.0, 1.0, 0.11557302959562324], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.915475], dtype=float32), 0.7650524]. 
=============================================
[2019-04-05 12:10:02,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[58.047127]
 [58.724277]
 [58.910595]
 [59.27188 ]
 [60.352802]], R is [[57.96259308]
 [58.3115387 ]
 [58.65699387]
 [58.99899292]
 [59.33757401]].
[2019-04-05 12:10:13,134] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3390310e-13 7.5517369e-06 1.2829586e-06 9.9998343e-01 7.7968216e-06
 1.0759693e-11 2.7211317e-11], sum to 1.0000
[2019-04-05 12:10:13,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4173
[2019-04-05 12:10:13,158] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 19.30478688821114, -0.9404206231089095, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1317600.0000, 
sim time next is 1318200.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 19.0, 19.18634855325624, -0.9582090938305106, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5046168051708219, 0.92, 0.0, 0.0, 0.08333333333333333, 0.09886237943802012, 0.18059696872316314, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48829892], dtype=float32), 0.9278639]. 
=============================================
[2019-04-05 12:10:19,754] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.5667211e-10 6.3781079e-04 1.5124058e-05 9.9928814e-01 5.8882419e-05
 5.7945537e-09 2.8741105e-08], sum to 1.0000
[2019-04-05 12:10:19,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9490
[2019-04-05 12:10:19,785] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.2, 88.5, 33.0, 21.0, 19.0, 18.84751461879513, -1.246319097529518, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1931400.0000, 
sim time next is 1932000.0000, 
raw observation next is [-9.100000000000001, 87.66666666666666, 41.66666666666666, 109.0, 19.0, 18.95450943911812, -1.211618453431608, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.21052631578947364, 0.8766666666666666, 0.13888888888888887, 0.12044198895027625, 0.08333333333333333, 0.07954245325984328, 0.09612718218946403, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5222077], dtype=float32), -0.92735213]. 
=============================================
[2019-04-05 12:10:19,797] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[58.45238 ]
 [59.991863]
 [61.101315]
 [62.27938 ]
 [63.308437]], R is [[56.65197754]
 [56.08545685]
 [55.52460098]
 [54.96935654]
 [54.41966248]].
[2019-04-05 12:10:21,449] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8620485e-11 5.1610480e-04 1.4399191e-05 9.9944872e-01 2.0701285e-05
 6.2770444e-10 2.0506001e-10], sum to 1.0000
[2019-04-05 12:10:21,452] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3901
[2019-04-05 12:10:21,467] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 19.0, 18.61084452190187, -1.199139630147716, 0.0, 1.0, 18733.16471027681], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 774000.0000, 
sim time next is 774600.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 19.0, 18.60145811326614, -1.205719617481334, 0.0, 1.0, 29618.75225632238], 
processed observation next is [1.0, 1.0, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.050121509438845045, 0.09809346083955532, 0.0, 1.0, 0.14104167741105894], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0762979], dtype=float32), 0.33665103]. 
=============================================
[2019-04-05 12:10:23,613] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9889695e-10 9.2028792e-04 7.2201092e-06 9.9905032e-01 2.2087066e-05
 1.4872940e-09 1.2751288e-08], sum to 1.0000
[2019-04-05 12:10:23,614] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1807
[2019-04-05 12:10:23,627] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.33333333333333, 54.33333333333334, 160.1666666666667, 144.8333333333333, 19.0, 21.08395573889827, -0.6728411019049633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1599600.0000, 
sim time next is 1600200.0000, 
raw observation next is [12.7, 53.0, 149.0, 124.0, 19.0, 20.76097645805396, -0.6811555787075826, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8144044321329641, 0.53, 0.49666666666666665, 0.13701657458563535, 0.08333333333333333, 0.2300813715044967, 0.2729481404308058, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42869857], dtype=float32), -1.39351]. 
=============================================
[2019-04-05 12:10:24,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5326516e-10 1.4269502e-04 8.9102614e-06 9.9981147e-01 3.6988120e-05
 1.1029555e-09 5.4133104e-10], sum to 1.0000
[2019-04-05 12:10:24,525] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0975
[2019-04-05 12:10:24,540] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 9.0, 0.0, 19.0, 19.82435133833805, -0.9251688019054224, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1357200.0000, 
sim time next is 1357800.0000, 
raw observation next is [0.5, 96.0, 5.999999999999998, 0.0, 19.0, 19.38022746038101, -0.9473614859994441, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.019999999999999993, 0.0, 0.08333333333333333, 0.11501895503175093, 0.18421283800018529, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46428648], dtype=float32), 1.26979]. 
=============================================
[2019-04-05 12:10:25,407] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 12:10:25,419] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:10:25,420] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:10:25,420] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:10:25,421] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:10:25,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:10:25,421] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:10:25,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run23
[2019-04-05 12:10:25,441] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run23
[2019-04-05 12:10:25,442] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run23
[2019-04-05 12:10:56,654] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1026675]
[2019-04-05 12:10:56,654] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [14.866933985, 43.293734935, 8.240199807, 0.0, 19.0, 21.63416443519947, -0.5748517553877964, 1.0, 1.0, 0.0]
[2019-04-05 12:10:56,654] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:10:56,655] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8360997e-11 1.9163288e-05 1.7939371e-06 9.9997365e-01 5.3890199e-06
 1.0321735e-10 2.0552883e-10], sampled 0.13572888972257124
[2019-04-05 12:11:01,161] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1026675]
[2019-04-05 12:11:01,161] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.166666666666667, 75.33333333333334, 0.0, 0.0, 19.0, 18.78274384899555, -1.066419900916135, 1.0, 1.0, 0.0]
[2019-04-05 12:11:01,161] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:11:01,162] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.1533564e-11 3.7236452e-05 4.6352893e-06 9.9994540e-01 1.2789610e-05
 3.7122400e-10 8.0905732e-10], sampled 0.1667552725988768
[2019-04-05 12:11:27,639] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1026675]
[2019-04-05 12:11:27,639] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.013943619333333, 100.0, 0.0, 0.0, 19.0, 20.09382883358033, -0.7326483856555984, 0.0, 1.0, 0.0]
[2019-04-05 12:11:27,639] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:11:27,640] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.51156263e-14 8.57348823e-07 1.05824995e-07 9.99998569e-01
 5.02387365e-07 3.46705249e-13 8.91113138e-13], sampled 0.873491225366702
[2019-04-05 12:11:33,955] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6155.8996 142744099.4696 -2202.8705
[2019-04-05 12:11:35,778] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1026675]
[2019-04-05 12:11:35,778] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.076456152333333, 87.00952378166666, 0.0, 0.0, 19.0, 18.59640209428282, -1.191349535947331, 0.0, 1.0, 52844.68582261329]
[2019-04-05 12:11:35,779] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:11:35,780] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.9399243e-12 5.2131427e-06 1.4519027e-06 9.9998653e-01 6.8025270e-06
 3.9385981e-11 7.2139524e-11], sampled 0.8359515154084665
[2019-04-05 12:11:44,485] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5671.7277 171992778.5590 -2846.3429
[2019-04-05 12:11:51,362] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5838.4417 185048419.1411 -2785.1128
[2019-04-05 12:11:52,387] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 440000, evaluation results [440000.0, 5671.727718756497, 171992778.5589557, -2846.3429013930686, 6155.899637941897, 142744099.4696188, -2202.870476255624, 5838.441675212122, 185048419.14109835, -2785.112797972731]
[2019-04-05 12:11:56,258] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6935491e-07 8.7805226e-04 3.6914941e-04 9.9854690e-01 2.0474863e-04
 4.4572886e-07 4.7687661e-07], sum to 1.0000
[2019-04-05 12:11:56,261] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7971
[2019-04-05 12:11:56,274] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.883333333333333, 75.66666666666666, 191.3333333333333, 160.6666666666667, 19.0, 19.55958123859833, -1.106515038558784, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1939800.0000, 
sim time next is 1940400.0000, 
raw observation next is [-5.6, 75.0, 201.5, 123.0, 19.0, 19.54751399070287, -1.101696010570925, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.75, 0.6716666666666666, 0.13591160220994475, 0.08333333333333333, 0.12895949922523928, 0.1327679964763583, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1552103], dtype=float32), 0.06701329]. 
=============================================
[2019-04-05 12:12:06,389] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.8961817e-15 1.1690962e-05 7.6607500e-07 9.9998450e-01 3.0689678e-06
 3.6928068e-11 7.6590071e-12], sum to 1.0000
[2019-04-05 12:12:06,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4003
[2019-04-05 12:12:06,415] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 85.0, 0.0, 0.0, 19.0, 18.6777391729898, -1.224565047902128, 0.0, 1.0, 66972.68170225908], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1887600.0000, 
sim time next is 1888200.0000, 
raw observation next is [-5.6, 84.5, 0.0, 0.0, 19.0, 18.67148053048903, -1.224752330843798, 0.0, 1.0, 56752.01756314185], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.845, 0.0, 0.0, 0.08333333333333333, 0.055956710874085935, 0.09174922305206734, 0.0, 1.0, 0.2702477026816279], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0620234], dtype=float32), 0.65869576]. 
=============================================
[2019-04-05 12:12:13,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8235060e-08 8.7006316e-03 1.5024449e-04 9.9107915e-01 6.9586764e-05
 5.7328869e-08 2.9331352e-07], sum to 1.0000
[2019-04-05 12:12:13,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3541
[2019-04-05 12:12:13,585] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.300000000000001, 84.66666666666667, 76.5, 0.0, 19.0, 19.14238693285895, -1.198261506304909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2042400.0000, 
sim time next is 2043000.0000, 
raw observation next is [-4.2, 84.0, 71.0, 0.0, 19.0, 19.11055977231309, -1.201344704069385, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.34626038781163443, 0.84, 0.23666666666666666, 0.0, 0.08333333333333333, 0.09254664769275738, 0.099551765310205, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.35984126], dtype=float32), -0.5223567]. 
=============================================
[2019-04-05 12:12:13,597] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[48.514057]
 [48.32444 ]
 [48.129425]
 [47.941483]
 [47.960243]], R is [[47.85378647]
 [47.37524796]
 [46.90149689]
 [46.43248367]
 [45.96815872]].
[2019-04-05 12:12:14,326] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5770837e-12 1.9243300e-06 2.2408215e-06 9.9999034e-01 5.5127803e-06
 2.3802423e-12 6.0230670e-12], sum to 1.0000
[2019-04-05 12:12:14,328] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8208
[2019-04-05 12:12:14,338] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.1, 78.0, 0.0, 0.0, 19.0, 19.7853869988616, -0.7784647856290738, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1209600.0000, 
sim time next is 1210200.0000, 
raw observation next is [16.1, 78.33333333333333, 0.0, 0.0, 19.0, 19.76896391229364, -0.7805760107492744, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7833333333333333, 0.0, 0.0, 0.08333333333333333, 0.14741365935780326, 0.23980799641690853, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1502024], dtype=float32), -0.32998034]. 
=============================================
[2019-04-05 12:12:16,488] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2437144e-08 4.2875984e-04 3.3351831e-04 9.9909520e-01 1.4160867e-04
 2.7008809e-07 5.5477653e-07], sum to 1.0000
[2019-04-05 12:12:16,490] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2970
[2019-04-05 12:12:16,502] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.799999999999999, 82.0, 179.6666666666667, 108.3333333333333, 19.0, 19.40405169181966, -1.100919530316768, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2108400.0000, 
sim time next is 2109000.0000, 
raw observation next is [-7.8, 82.0, 185.3333333333333, 98.66666666666667, 19.0, 19.43408601229807, -1.09592754863767, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.6177777777777776, 0.10902394106813997, 0.08333333333333333, 0.11950716769150589, 0.13469081712077666, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.471879], dtype=float32), -0.17978348]. 
=============================================
[2019-04-05 12:12:16,515] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[45.501522]
 [46.79893 ]
 [47.72464 ]
 [48.629776]
 [49.906666]], R is [[44.44643402]
 [44.00196838]
 [43.56195068]
 [43.12633133]
 [42.69506836]].
[2019-04-05 12:12:22,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0480520e-12 2.7664810e-06 6.6510063e-07 9.9999535e-01 1.2300984e-06
 8.4705506e-12 4.8961364e-12], sum to 1.0000
[2019-04-05 12:12:22,893] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1968
[2019-04-05 12:12:22,906] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 99.16666666666667, 0.0, 0.0, 19.0, 18.73064528696224, -1.104261673007394, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1396200.0000, 
sim time next is 1396800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 18.61738217393931, -1.097256525089753, 0.0, 1.0, 196949.1235048031], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.051448514494942366, 0.13424782497008234, 0.0, 1.0, 0.937852969070491], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.842443], dtype=float32), -1.1160911]. 
=============================================
[2019-04-05 12:12:22,991] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9317276e-06 1.3336322e-03 1.2000463e-03 9.9611759e-01 1.3441124e-03
 1.0509033e-06 1.6831627e-06], sum to 1.0000
[2019-04-05 12:12:22,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4098
[2019-04-05 12:12:23,002] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.05, 79.0, 147.0, 0.0, 19.0, 19.16686220299447, -1.199149289598324, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2028600.0000, 
sim time next is 2029200.0000, 
raw observation next is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 19.0, 19.16112251315881, -1.201179986740617, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3277931671283472, 0.7766666666666667, 0.495, 0.0, 0.08333333333333333, 0.09676020942990071, 0.099606671086461, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.39260846], dtype=float32), -0.25302488]. 
=============================================
[2019-04-05 12:12:23,075] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0846872e-07 2.7503381e-03 1.3147984e-03 9.9539524e-01 5.3843920e-04
 1.7891865e-07 7.6196318e-07], sum to 1.0000
[2019-04-05 12:12:23,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4017
[2019-04-05 12:12:23,087] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.1, 69.0, 55.33333333333333, 47.49999999999999, 19.0, 19.68301355297994, -1.071005253478346, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2218800.0000, 
sim time next is 2219400.0000, 
raw observation next is [-4.2, 69.5, 35.0, 0.0, 19.0, 19.7010729393424, -1.153786224717806, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.34626038781163443, 0.695, 0.11666666666666667, 0.0, 0.08333333333333333, 0.1417560782785333, 0.11540459176073137, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4174261], dtype=float32), 1.6130459]. 
=============================================
[2019-04-05 12:12:24,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1175874e-14 1.5479661e-06 1.2162489e-07 9.9999046e-01 7.9033989e-06
 4.2272626e-12 1.3059779e-12], sum to 1.0000
[2019-04-05 12:12:24,815] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8926
[2019-04-05 12:12:24,829] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 19.0, 18.62482979732505, -1.191250041080615, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 19.0, 18.63596707857878, -1.201816143865354, 0.0, 1.0, 18731.31694264613], 
processed observation next is [1.0, 1.0, 0.28393351800554023, 0.765, 0.0, 0.0, 0.08333333333333333, 0.052997256548231654, 0.09939461871154864, 0.0, 1.0, 0.08919674734593395], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8568581], dtype=float32), -2.0249178]. 
=============================================
[2019-04-05 12:12:25,752] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 12:12:25,753] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:12:25,754] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:12:25,754] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:12:25,755] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:12:25,755] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:12:25,756] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:12:25,761] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run24
[2019-04-05 12:12:25,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run24
[2019-04-05 12:12:25,801] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run24
[2019-04-05 12:13:16,777] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10126645]
[2019-04-05 12:13:16,777] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 19.0, 19.40513607692019, -1.112965518882839, 1.0, 1.0, 0.0]
[2019-04-05 12:13:16,777] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:13:16,778] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.1450958e-10 1.2589575e-04 3.4870282e-05 9.9974102e-01 9.8150151e-05
 5.6624452e-09 9.6363513e-09], sampled 0.4452725376939498
[2019-04-05 12:13:33,496] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6155.8996 142744099.4696 -2202.8705
[2019-04-05 12:13:43,692] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5683.6095 171459780.8524 -2881.7066
[2019-04-05 12:13:56,285] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5441.6179 192945237.0519 -2390.0559
[2019-04-05 12:13:57,310] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 460000, evaluation results [460000.0, 5683.609457196337, 171459780.85235208, -2881.7066324972343, 6155.899637941897, 142744099.4696188, -2202.870476255624, 5441.617902447899, 192945237.0518736, -2390.05588445026]
[2019-04-05 12:14:02,804] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0572100e-15 5.4870362e-08 7.5941563e-07 9.9999785e-01 1.4298087e-06
 2.4152167e-13 3.3489203e-13], sum to 1.0000
[2019-04-05 12:14:02,804] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2938
[2019-04-05 12:14:02,826] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 18.54623953268964, -1.25458156667195, 0.0, 1.0, 44406.36082754551], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1900800.0000, 
sim time next is 1901400.0000, 
raw observation next is [-7.3, 80.83333333333334, 0.0, 0.0, 19.0, 18.54243582066054, -1.256120358423715, 0.0, 1.0, 44817.90797072931], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.8083333333333335, 0.0, 0.0, 0.08333333333333333, 0.04520298505504498, 0.08129321385876165, 0.0, 1.0, 0.2134186093844253], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2085828], dtype=float32), -0.053258996]. 
=============================================
[2019-04-05 12:14:06,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2213381e-10 4.3395557e-03 1.5931679e-05 9.9463958e-01 1.0048512e-03
 1.6771715e-08 1.9126072e-07], sum to 1.0000
[2019-04-05 12:14:06,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2527
[2019-04-05 12:14:06,026] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 94.16666666666666, 49.33333333333334, 49.66666666666667, 19.0, 19.06982866973982, -1.127709212199455, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2911800.0000, 
sim time next is 2912400.0000, 
raw observation next is [2.0, 93.0, 38.5, 47.5, 19.0, 19.24102270145213, -1.112797210239882, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.93, 0.12833333333333333, 0.052486187845303865, 0.08333333333333333, 0.10341855845434426, 0.129067596586706, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4693324], dtype=float32), 0.626215]. 
=============================================
[2019-04-05 12:14:12,434] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4222552e-06 4.3264605e-02 6.0365573e-03 9.4463944e-01 6.0490454e-03
 1.1589923e-06 7.7177301e-06], sum to 1.0000
[2019-04-05 12:14:12,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5971
[2019-04-05 12:14:12,452] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9666666666666667, 38.66666666666667, 0.0, 0.0, 19.0, 18.8483971978487, -1.187808802922925, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2575200.0000, 
sim time next is 2575800.0000, 
raw observation next is [-1.15, 40.0, 0.0, 0.0, 19.0, 18.69326850952507, -1.197619027197708, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8260869565217391, 0.4307479224376732, 0.4, 0.0, 0.0, 0.08333333333333333, 0.05777237579375575, 0.10079365760076398, 0.0, 1.0, 0.9343709972347434], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.897472], dtype=float32), 1.373266]. 
=============================================
[2019-04-05 12:14:13,156] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5565275e-12 6.1187478e-05 2.8474690e-07 9.9991953e-01 1.9058625e-05
 8.4325796e-11 9.5871254e-12], sum to 1.0000
[2019-04-05 12:14:13,157] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3845
[2019-04-05 12:14:13,170] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.399999999999999, 61.5, 0.0, 0.0, 19.0, 18.90261427956253, -1.149637226371634, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2591400.0000, 
sim time next is 2592000.0000, 
raw observation next is [-4.5, 62.0, 0.0, 0.0, 19.0, 18.90779377382551, -1.158948917554458, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.62, 0.0, 0.0, 0.08333333333333333, 0.07564948115212584, 0.11368369414851398, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29544327], dtype=float32), -0.6276513]. 
=============================================
[2019-04-05 12:14:13,181] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.29269 ]
 [77.028885]
 [77.18841 ]
 [77.48827 ]
 [77.28998 ]], R is [[76.26521301]
 [76.50256348]
 [76.7375412 ]
 [76.97016907]
 [77.20046997]].
[2019-04-05 12:14:14,063] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1170326e-07 5.8404007e-03 1.0261636e-03 9.9007261e-01 3.0557809e-03
 2.2645850e-06 2.2115587e-06], sum to 1.0000
[2019-04-05 12:14:14,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0713
[2019-04-05 12:14:14,156] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.916666666666667, 70.5, 0.0, 0.0, 19.0, 18.23796397676149, -1.30443772069256, 1.0, 1.0, 194527.1285518024], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2137800.0000, 
sim time next is 2138400.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 18.10465788464859, -1.157711259193631, 1.0, 1.0, 191784.136302307], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.008721490387382383, 0.11409624693545635, 1.0, 1.0, 0.9132577919157475], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09570843], dtype=float32), -1.1471972]. 
=============================================
[2019-04-05 12:14:17,719] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2816264e-06 1.1438466e-02 1.0647743e-03 9.7786850e-01 9.6106641e-03
 4.5871711e-06 8.7535627e-06], sum to 1.0000
[2019-04-05 12:14:17,720] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0763
[2019-04-05 12:14:17,749] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.45, 64.0, 151.0, 134.0, 19.0, 19.49373266665475, -1.114560351583913, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2118600.0000, 
sim time next is 2119200.0000, 
raw observation next is [-6.366666666666667, 64.0, 150.6666666666667, 111.6666666666667, 19.0, 19.32864861510525, -1.119215557794564, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.28624192059095105, 0.64, 0.5022222222222223, 0.1233885819521179, 0.08333333333333333, 0.11072071792543763, 0.12692814740181202, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9526513], dtype=float32), -0.7125577]. 
=============================================
[2019-04-05 12:14:19,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8901894e-12 9.4643792e-06 4.1961416e-06 9.9994528e-01 4.1131269e-05
 3.8442898e-11 1.4657386e-11], sum to 1.0000
[2019-04-05 12:14:19,946] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5487
[2019-04-05 12:14:19,962] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 18.53867468579669, -1.226403590668704, 0.0, 1.0, 63820.15368675235], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 18.52949911068681, -1.225277727379836, 0.0, 1.0, 55558.85613519074], 
processed observation next is [1.0, 0.0, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.044124925890567525, 0.09157409087338797, 0.0, 1.0, 0.26456598159614636], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8018243], dtype=float32), 0.37104154]. 
=============================================
[2019-04-05 12:14:19,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5764918e-09 1.0500434e-04 6.3647161e-04 9.9872929e-01 5.2918773e-04
 7.4205935e-09 2.9237885e-08], sum to 1.0000
[2019-04-05 12:14:19,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-05 12:14:20,015] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 63.16666666666667, 0.0, 0.0, 19.0, 18.78506950087969, -1.13829660311866, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2754600.0000, 
sim time next is 2755200.0000, 
raw observation next is [-6.0, 62.33333333333334, 0.0, 0.0, 19.0, 18.70301317089615, -1.155678736645614, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6233333333333334, 0.0, 0.0, 0.08333333333333333, 0.05858443090801243, 0.11477375445146198, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8497472], dtype=float32), -0.7581878]. 
=============================================
[2019-04-05 12:14:20,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0377945e-08 8.1930263e-03 4.9157150e-04 9.9093187e-01 3.8282876e-04
 3.3327066e-08 6.1364796e-07], sum to 1.0000
[2019-04-05 12:14:20,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8933
[2019-04-05 12:14:20,631] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 89.33333333333334, 80.16666666666667, 0.0, 20.0, 20.87092985530179, -0.700299433513511, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1682400.0000, 
sim time next is 1683000.0000, 
raw observation next is [1.1, 88.0, 83.0, 0.0, 20.0, 20.87693450238958, -0.6994837062995619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.88, 0.27666666666666667, 0.0, 0.16666666666666666, 0.2397445418657984, 0.2668387645668127, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.59705466], dtype=float32), -0.1704452]. 
=============================================
[2019-04-05 12:14:20,637] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[51.774494]
 [51.727406]
 [51.705498]
 [51.63484 ]
 [51.598118]], R is [[51.26845169]
 [50.75576782]
 [50.24821091]
 [49.74572754]
 [49.24827194]].
[2019-04-05 12:14:21,913] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4036667e-10 1.5117143e-03 3.3519183e-05 9.9504906e-01 3.4055866e-03
 1.6004408e-08 4.9336180e-08], sum to 1.0000
[2019-04-05 12:14:21,916] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7431
[2019-04-05 12:14:21,949] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 19.0, 18.67054026644709, -1.177596202172656, 0.0, 1.0, 111720.4931295865], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1976400.0000, 
sim time next is 1977000.0000, 
raw observation next is [-5.7, 78.83333333333333, 0.0, 0.0, 19.0, 18.6282227403287, -1.170347109840233, 0.0, 1.0, 97333.93441442696], 
processed observation next is [1.0, 0.9130434782608695, 0.30470914127423826, 0.7883333333333333, 0.0, 0.0, 0.08333333333333333, 0.05235189502739157, 0.10988429671992235, 0.0, 1.0, 0.4634949257829855], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30454394], dtype=float32), -1.9195119]. 
=============================================
[2019-04-05 12:14:21,956] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[65.64637 ]
 [63.376736]
 [61.79835 ]
 [60.203804]
 [59.13217 ]], R is [[67.51597595]
 [67.84082031]
 [68.16241455]
 [68.48078918]
 [68.79598236]].
[2019-04-05 12:14:24,238] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5759424e-06 5.3964686e-03 4.2344402e-03 9.6783352e-01 2.2517346e-02
 3.7520749e-06 1.2984154e-05], sum to 1.0000
[2019-04-05 12:14:24,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4435
[2019-04-05 12:14:24,259] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 19.0, 20.59012607452086, -0.7604568728004949, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3339600.0000, 
sim time next is 3340200.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 78.0, 616.3333333333334, 19.0, 20.7288384199704, -0.7460587530830667, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4025854108956602, 0.46666666666666673, 0.26, 0.6810313075506446, 0.08333333333333333, 0.2274032016642001, 0.2513137489723111, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9917055], dtype=float32), -0.32754612]. 
=============================================
[2019-04-05 12:14:29,713] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5100617e-11 1.4668646e-04 7.8131106e-06 9.9948239e-01 3.6306703e-04
 2.5825109e-11 9.5132113e-10], sum to 1.0000
[2019-04-05 12:14:29,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7879
[2019-04-05 12:14:29,731] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.100000000000001, 41.33333333333334, 0.0, 0.0, 19.5, 19.05173864244976, -1.132150058326787, 0.0, 1.0, 52153.64524074453], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2416200.0000, 
sim time next is 2416800.0000, 
raw observation next is [-5.2, 41.66666666666667, 0.0, 0.0, 19.5, 19.05185803882507, -1.127464798698413, 0.0, 1.0, 44564.01818018999], 
processed observation next is [0.0, 1.0, 0.31855955678670367, 0.41666666666666674, 0.0, 0.0, 0.125, 0.08765483656875588, 0.12417840043386234, 0.0, 1.0, 0.2122096103818571], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.6825713], dtype=float32), -1.2303671]. 
=============================================
[2019-04-05 12:14:31,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6912831e-11 2.2592774e-05 6.3149423e-06 9.9994028e-01 3.0809915e-05
 4.1359825e-11 4.1734116e-10], sum to 1.0000
[2019-04-05 12:14:31,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4504
[2019-04-05 12:14:31,255] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 18.63775635151519, -1.160669339084835, 0.0, 1.0, 18898.18445247185], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2762400.0000, 
sim time next is 2763000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 18.68013463547329, -1.163988171961434, 0.0, 1.0, 18724.84395225023], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.05667788628944089, 0.112003942679522, 0.0, 1.0, 0.08916592358214395], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1520811], dtype=float32), -0.1206874]. 
=============================================
[2019-04-05 12:14:31,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.06857 ]
 [73.23372 ]
 [73.278175]
 [72.45858 ]
 [71.39839 ]], R is [[73.22757721]
 [73.49530029]
 [73.76034546]
 [74.02274323]
 [74.28251648]].
[2019-04-05 12:14:33,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1804945e-13 3.7214274e-06 1.4765116e-06 9.9998915e-01 5.7066595e-06
 4.5859117e-13 1.1676507e-11], sum to 1.0000
[2019-04-05 12:14:33,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5060
[2019-04-05 12:14:33,665] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 18.78188657540194, -1.165452398685366, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3018600.0000, 
sim time next is 3019200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 18.78486870583711, -1.174985897070756, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.06540572548642576, 0.10833803430974798, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5426819], dtype=float32), 0.3272368]. 
=============================================
[2019-04-05 12:14:33,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4288807e-12 1.1433763e-05 5.7878324e-06 9.9941528e-01 5.6747871e-04
 7.2522557e-11 2.8409414e-10], sum to 1.0000
[2019-04-05 12:14:33,990] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1364
[2019-04-05 12:14:34,002] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 61.5, 83.0, 359.0, 19.0, 18.69503251580406, -1.184770208413629, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3054600.0000, 
sim time next is 3055200.0000, 
raw observation next is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 19.0, 18.77203088488493, -1.184197688777108, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.6066666666666666, 0.28555555555555556, 0.44751381215469616, 0.08333333333333333, 0.06433590707374422, 0.10526743707429735, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4988906], dtype=float32), 0.7970549]. 
=============================================
[2019-04-05 12:14:34,322] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 12:14:34,324] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:14:34,324] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:14:34,325] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:14:34,325] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:14:34,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:14:34,326] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:14:34,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run25
[2019-04-05 12:14:34,350] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run25
[2019-04-05 12:14:34,363] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run25
[2019-04-05 12:14:51,576] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10089493]
[2019-04-05 12:14:51,576] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 66.0, 123.3333333333333, 34.00000000000001, 19.0, 18.474032735953, -1.252086982535569, 0.0, 1.0, 35539.67511997175]
[2019-04-05 12:14:51,576] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:14:51,577] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0733471e-11 2.2583936e-05 8.9927416e-06 9.9991047e-01 5.8021455e-05
 1.5840133e-10 4.7832743e-10], sampled 0.38561768239041483
[2019-04-05 12:14:59,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10089493]
[2019-04-05 12:14:59,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [16.6, 75.0, 0.0, 0.0, 19.0, 19.86408610136281, -0.7660506865939366, 0.0, 0.0, 0.0]
[2019-04-05 12:14:59,235] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:14:59,236] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.7154359e-13 9.9630015e-06 2.4176661e-06 9.9996650e-01 2.1113770e-05
 2.0305067e-11 6.6480169e-11], sampled 0.8541142100718959
[2019-04-05 12:15:06,897] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10089493]
[2019-04-05 12:15:06,897] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.45, 72.83333333333333, 0.0, 0.0, 19.0, 20.23534336709756, -0.7037483216328054, 0.0, 1.0, 0.0]
[2019-04-05 12:15:06,897] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:15:06,898] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.0583359e-16 3.8571639e-07 8.9117641e-08 9.9999833e-01 1.2473387e-06
 2.4184019e-14 1.1450407e-13], sampled 0.6687654423405418
[2019-04-05 12:15:43,800] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6044.8160 144695393.7898 -2155.4406
[2019-04-05 12:15:50,216] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10089493]
[2019-04-05 12:15:50,217] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 72.0, 147.0, 0.0, 19.0, 19.99337511104049, -0.9385134404761702, 1.0, 1.0, 18680.41167023054]
[2019-04-05 12:15:50,217] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:15:50,218] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.6358299e-09 6.4511155e-04 1.2901671e-04 9.9884641e-01 3.7920789e-04
 3.7546229e-08 8.9340034e-08], sampled 0.9504159364428696
[2019-04-05 12:15:54,594] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5619.2007 174165250.8642 -2744.0677
[2019-04-05 12:15:59,958] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5776.3495 186484860.9448 -2738.5671
[2019-04-05 12:16:00,983] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 480000, evaluation results [480000.0, 5619.2007039766995, 174165250.86417663, -2744.067747018322, 6044.816032706946, 144695393.78982985, -2155.4406439049394, 5776.34951314447, 186484860.9447583, -2738.5670591250564]
[2019-04-05 12:16:01,237] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6634779e-13 6.9250718e-06 8.9639690e-07 9.9998891e-01 3.2715184e-06
 8.1560439e-11 1.7827581e-11], sum to 1.0000
[2019-04-05 12:16:01,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1089
[2019-04-05 12:16:01,254] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 44.66666666666667, 0.0, 0.0, 19.0, 18.60536129734416, -1.243714183172603, 0.0, 1.0, 51078.79181906278], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2420400.0000, 
sim time next is 2421000.0000, 
raw observation next is [-5.9, 45.5, 0.0, 0.0, 19.0, 18.64413516684881, -1.248711102313376, 0.0, 1.0, 23678.30633782721], 
processed observation next is [0.0, 0.0, 0.2991689750692521, 0.455, 0.0, 0.0, 0.08333333333333333, 0.05367793057073408, 0.08376296589554137, 0.0, 1.0, 0.1127538397039391], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7056293], dtype=float32), 1.1918902]. 
=============================================
[2019-04-05 12:16:01,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.01884]
 [74.4694 ]
 [75.58839]
 [77.14972]
 [77.07304]], R is [[71.78507996]
 [72.06723022]
 [72.34655762]
 [72.62309265]
 [72.89686584]].
[2019-04-05 12:16:02,468] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2431926e-11 3.9202446e-06 4.2575771e-06 9.9997663e-01 1.5158530e-05
 8.5748041e-11 5.5045346e-10], sum to 1.0000
[2019-04-05 12:16:02,471] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3030
[2019-04-05 12:16:02,495] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 59.0, 9.166666666666664, 102.6666666666667, 19.0, 18.40716131700896, -1.306753255491333, 0.0, 1.0, 46112.62641749194], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2446800.0000, 
sim time next is 2447400.0000, 
raw observation next is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 19.0, 18.4126241269325, -1.295686595903048, 0.0, 1.0, 43700.04434303685], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.585, 0.0511111111111111, 0.18268876611418042, 0.08333333333333333, 0.03438534391104161, 0.06810446803231736, 0.0, 1.0, 0.2080954492525564], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01475689], dtype=float32), 0.44677985]. 
=============================================
[2019-04-05 12:16:03,083] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9209706e-13 4.1199371e-07 8.9574831e-07 9.9999750e-01 1.2066848e-06
 1.6619123e-10 2.5014441e-10], sum to 1.0000
[2019-04-05 12:16:03,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8430
[2019-04-05 12:16:03,110] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 20.0, 19.60421297652492, -0.9869113979474508, 0.0, 1.0, 18734.43215603103], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 20.0, 19.69885666531682, -0.990082449146251, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.16666666666666666, 0.14157138877640177, 0.16997251695124968, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.3638099], dtype=float32), 0.8254699]. 
=============================================
[2019-04-05 12:16:03,188] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1021056e-11 2.0264807e-04 2.3523229e-05 9.9973685e-01 3.7006525e-05
 8.8851909e-10 6.2800312e-09], sum to 1.0000
[2019-04-05 12:16:03,189] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5384
[2019-04-05 12:16:03,214] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 19.0, 18.47068455804653, -1.267237480462416, 0.0, 1.0, 49101.60788906793], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2267400.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 19.0, 18.51324636737466, -1.268677939479236, 0.0, 1.0, 24518.81996270853], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.08333333333333333, 0.042770530614554936, 0.07710735350692133, 0.0, 1.0, 0.11675628553670729], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18256871], dtype=float32), -1.2462444]. 
=============================================
[2019-04-05 12:16:03,225] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.98451 ]
 [64.74199 ]
 [64.79116 ]
 [64.71658 ]
 [64.253265]], R is [[65.54399872]
 [65.88855743]
 [66.22967529]
 [66.56738281]
 [66.90171051]].
[2019-04-05 12:16:05,827] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0331096e-10 9.5205782e-05 1.1547690e-05 9.9988663e-01 6.5412355e-06
 5.6264375e-09 9.4015185e-10], sum to 1.0000
[2019-04-05 12:16:05,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4180
[2019-04-05 12:16:05,858] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 79.16666666666667, 0.0, 0.0, 20.0, 19.52371227726763, -0.9214712005188973, 1.0, 1.0, 30330.11584754886], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2923800.0000, 
sim time next is 2924400.0000, 
raw observation next is [-1.0, 80.33333333333334, 0.0, 0.0, 20.0, 19.50566862571487, -0.9108139636910043, 0.0, 1.0, 197187.6488840369], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8033333333333335, 0.0, 0.0, 0.16666666666666666, 0.12547238547623904, 0.1963953454363319, 0.0, 1.0, 0.9389888042096995], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.05421363], dtype=float32), -0.02030654]. 
=============================================
[2019-04-05 12:16:10,912] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.3697437e-12 3.6699468e-05 4.3924934e-05 9.9987626e-01 4.3067012e-05
 3.1909456e-10 1.9652306e-08], sum to 1.0000
[2019-04-05 12:16:10,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4484
[2019-04-05 12:16:10,928] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.5, 61.0, 0.0, 0.0, 19.5, 18.86342158244651, -1.209175340173732, 0.0, 1.0, 52275.41249691763], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2444400.0000, 
sim time next is 2445000.0000, 
raw observation next is [-9.5, 60.5, 0.0, 0.0, 19.5, 18.8594816975376, -1.209057251849849, 0.0, 1.0, 49278.37702655832], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.605, 0.0, 0.0, 0.125, 0.07162347479479998, 0.09698091605005033, 0.0, 1.0, 0.2346589382217063], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.8408244], dtype=float32), 0.6345781]. 
=============================================
[2019-04-05 12:16:10,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[68.38142 ]
 [68.22247 ]
 [68.07979 ]
 [67.910126]
 [67.788666]], R is [[68.68028259]
 [68.92205048]
 [69.16140747]
 [69.39836884]
 [69.63295746]].
[2019-04-05 12:16:13,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.15536491e-09 7.78523099e-05 1.49308238e-04 9.99660730e-01
 1.12097914e-04 1.36886702e-09 2.88131972e-08], sum to 1.0000
[2019-04-05 12:16:13,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9997
[2019-04-05 12:16:13,841] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.166666666666667, 72.0, 0.0, 0.0, 19.0, 18.63950088915322, -1.19780520360996, 0.0, 1.0, 83721.53944058913], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3387000.0000, 
sim time next is 3387600.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 18.63213073456171, -1.172809866647228, 0.0, 1.0, 66632.14604697272], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.052677561213475776, 0.10906337778425734, 0.0, 1.0, 0.31729593355701297], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39968792], dtype=float32), -0.3464418]. 
=============================================
[2019-04-05 12:16:15,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5043336e-06 1.9683044e-02 8.3248597e-03 9.6514511e-01 6.8001701e-03
 1.4445474e-05 2.4897896e-05], sum to 1.0000
[2019-04-05 12:16:15,743] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0934
[2019-04-05 12:16:15,762] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 54.0, 116.0, 805.5, 19.0, 20.80775504806213, -0.7346710797193147, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3330000.0000, 
sim time next is 3330600.0000, 
raw observation next is [-4.833333333333334, 53.33333333333333, 115.3333333333333, 803.6666666666666, 19.0, 20.6673612909022, -0.7000357483043927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.32871652816251157, 0.5333333333333333, 0.3844444444444443, 0.8880294659300184, 0.08333333333333333, 0.22228010757518332, 0.26665475056520244, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48310614], dtype=float32), -0.18082258]. 
=============================================
[2019-04-05 12:16:20,116] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9243467e-10 1.4740749e-03 4.1560354e-05 9.9664271e-01 1.8416521e-03
 1.8066528e-08 1.0592139e-08], sum to 1.0000
[2019-04-05 12:16:20,120] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8675
[2019-04-05 12:16:20,130] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 72.0, 0.0, 0.0, 19.0, 18.78472582012448, -1.151585058930583, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3820200.0000, 
sim time next is 3820800.0000, 
raw observation next is [-4.333333333333334, 73.0, 0.0, 0.0, 19.0, 18.79641893985487, -1.161570088101392, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.73, 0.0, 0.0, 0.08333333333333333, 0.06636824498790588, 0.11280997063286935, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32995927], dtype=float32), -2.583396]. 
=============================================
[2019-04-05 12:16:22,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1005060e-07 8.1985438e-04 9.4927251e-05 9.9874949e-01 3.3535034e-04
 9.3079151e-08 1.7891925e-07], sum to 1.0000
[2019-04-05 12:16:22,201] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5949
[2019-04-05 12:16:22,211] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 62.0, 0.0, 0.0, 19.0, 18.61697616546176, -1.157393522688895, 1.0, 1.0, 26987.01372988581], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2662800.0000, 
sim time next is 2663400.0000, 
raw observation next is [-1.2, 62.5, 0.0, 0.0, 19.0, 18.61797285132281, -1.161332393829589, 0.0, 1.0, 26399.25387357308], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.625, 0.0, 0.0, 0.08333333333333333, 0.051497737610234275, 0.11288920205680368, 0.0, 1.0, 0.1257107327313004], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44442412], dtype=float32), 0.7033215]. 
=============================================
[2019-04-05 12:16:31,766] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.92658344e-12 7.77360401e-06 6.58013005e-06 9.99843121e-01
 1.42529127e-04 1.15908935e-10 3.27330690e-10], sum to 1.0000
[2019-04-05 12:16:31,768] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-05 12:16:31,780] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.833333333333333, 48.66666666666667, 110.6666666666667, 809.6666666666666, 19.0, 18.77625015431009, -1.096539195360271, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3071400.0000, 
sim time next is 3072000.0000, 
raw observation next is [-1.666666666666667, 47.33333333333334, 109.8333333333333, 807.8333333333334, 19.0, 18.78282173885771, -1.097597143811343, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.4164358264081256, 0.47333333333333344, 0.366111111111111, 0.892633517495396, 0.08333333333333333, 0.06523514490480913, 0.134134285396219, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7922858], dtype=float32), 0.7320753]. 
=============================================
[2019-04-05 12:16:31,801] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.71828]
 [74.48591]
 [74.23712]
 [74.06663]
 [73.9242 ]], R is [[75.05686188]
 [75.3062973 ]
 [75.55323792]
 [75.7977066 ]
 [76.03973389]].
[2019-04-05 12:16:34,098] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-05 12:16:34,100] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:16:34,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:16:34,101] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:16:34,101] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:16:34,101] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:16:34,101] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:16:34,104] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run26
[2019-04-05 12:16:34,120] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run26
[2019-04-05 12:16:34,134] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run26
[2019-04-05 12:17:42,052] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6004.0376 146458638.4358 -2090.5665
[2019-04-05 12:17:54,277] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5510.1456 176398216.9397 -2682.4722
[2019-04-05 12:18:00,461] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5731.0691 186646922.2064 -2712.1419
[2019-04-05 12:18:01,485] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 500000, evaluation results [500000.0, 5510.145622945552, 176398216.93970984, -2682.4721684898, 6004.037580225664, 146458638.43579692, -2090.5665381426124, 5731.069127084649, 186646922.20639318, -2712.1419319551305]
[2019-04-05 12:18:02,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7755681e-05 6.1280192e-03 8.6903637e-03 9.7484457e-01 1.0187950e-02
 2.9553603e-05 9.1807269e-05], sum to 1.0000
[2019-04-05 12:18:02,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0189
[2019-04-05 12:18:02,386] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 47.66666666666667, 119.1666666666667, 830.8333333333334, 19.0, 20.26821504221804, -0.8332819602430216, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3932400.0000, 
sim time next is 3933000.0000, 
raw observation next is [-6.0, 47.0, 119.0, 835.0, 19.0, 20.24837149283019, -0.8311544169407311, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.47, 0.39666666666666667, 0.9226519337016574, 0.08333333333333333, 0.18736429106918262, 0.22294852768642295, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2382601], dtype=float32), 0.5219926]. 
=============================================
[2019-04-05 12:18:02,401] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[32.620445]
 [32.293365]
 [32.00087 ]
 [31.651632]
 [31.35238 ]], R is [[32.55678558]
 [32.23121643]
 [31.90890503]
 [31.58981705]
 [31.27392006]].
[2019-04-05 12:18:03,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7865306e-12 3.1185689e-04 9.5586138e-06 9.9900240e-01 6.7614217e-04
 2.1348322e-11 1.5660177e-10], sum to 1.0000
[2019-04-05 12:18:03,384] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1045
[2019-04-05 12:18:03,413] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 19.0, 18.88900013577944, -1.035887556663422, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2983200.0000, 
sim time next is 2983800.0000, 
raw observation next is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 19.0, 18.95373501467991, -1.031445872197389, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.5244444444444443, 0.8036832412523021, 0.08333333333333333, 0.07947791788999255, 0.15618470926753703, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7824244], dtype=float32), -0.07083579]. 
=============================================
[2019-04-05 12:18:06,116] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3992037e-13 2.7126057e-07 1.6118398e-07 9.9999523e-01 4.4270555e-06
 2.3335257e-12 2.3662317e-11], sum to 1.0000
[2019-04-05 12:18:06,119] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2851
[2019-04-05 12:18:06,126] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 19.0, 19.90723793328008, -0.8439076883271442, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3684000.0000, 
sim time next is 3684600.0000, 
raw observation next is [5.5, 48.5, 87.0, 705.0, 19.0, 19.92383510145433, -0.8423705757654004, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6149584487534627, 0.485, 0.29, 0.7790055248618785, 0.08333333333333333, 0.16031959178786082, 0.21920980807819987, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.534482], dtype=float32), -0.87922305]. 
=============================================
[2019-04-05 12:18:09,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1556807e-10 9.7726333e-06 2.6167431e-04 9.9955124e-01 1.7727600e-04
 1.5911501e-09 9.5730990e-10], sum to 1.0000
[2019-04-05 12:18:09,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6857
[2019-04-05 12:18:09,625] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 20.0, 19.43088787596846, -0.9900335327188903, 0.0, 1.0, 9358.730426195374], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3058800.0000, 
sim time next is 3059400.0000, 
raw observation next is [-4.333333333333333, 54.83333333333333, 101.3333333333333, 676.3333333333333, 20.0, 19.4167306481996, -0.9886162528354059, 0.0, 1.0, 9357.537408870221], 
processed observation next is [0.0, 0.391304347826087, 0.342566943674977, 0.5483333333333333, 0.3377777777777777, 0.747329650092081, 0.16666666666666666, 0.11806088734996667, 0.1704612490548647, 0.0, 1.0, 0.044559701947001054], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.1270583], dtype=float32), 0.2844322]. 
=============================================
[2019-04-05 12:18:11,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.33108615e-05 1.79907214e-02 4.95373970e-03 9.61970747e-01
 1.49906669e-02 2.14439442e-05 5.93843833e-05], sum to 1.0000
[2019-04-05 12:18:11,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8108
[2019-04-05 12:18:11,311] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.0, 64.0, 112.5, 790.0, 19.0, 22.11835765256033, -0.4776125414090334, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2721600.0000, 
sim time next is 2722200.0000, 
raw observation next is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 19.0, 22.13157764248713, -0.4702918141444922, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2502308402585411, 0.6316666666666667, 0.37555555555555564, 0.876243093922652, 0.08333333333333333, 0.3442981368739275, 0.3432360619518359, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40767375], dtype=float32), 0.22973135]. 
=============================================
[2019-04-05 12:18:12,726] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3563103e-09 1.1530318e-03 4.5779638e-04 9.9826717e-01 1.2194080e-04
 3.6786599e-08 3.6824300e-08], sum to 1.0000
[2019-04-05 12:18:12,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3455
[2019-04-05 12:18:12,741] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 18.54868583353116, -1.208569258948746, 0.0, 1.0, 56776.39859093373], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3394800.0000, 
sim time next is 3395400.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 19.0, 18.58733153780574, -1.200753175012985, 0.0, 1.0, 26483.69353785252], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.6416666666666667, 0.0, 0.0, 0.08333333333333333, 0.04894429481714487, 0.09974894166233834, 0.0, 1.0, 0.1261128263707263], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09186565], dtype=float32), 0.5279769]. 
=============================================
[2019-04-05 12:18:13,594] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4384088e-08 4.3402556e-03 1.8261904e-03 9.8883200e-01 4.9971947e-03
 1.0139165e-07 4.1892918e-06], sum to 1.0000
[2019-04-05 12:18:13,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7723
[2019-04-05 12:18:13,609] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 18.75833039291514, -1.193363680650499, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2776200.0000, 
sim time next is 2776800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 18.69720984957539, -1.19949044254193, 0.0, 1.0, 77341.08699645188], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.058100820797949204, 0.10016985248602335, 0.0, 1.0, 0.36829089045929464], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2607687], dtype=float32), -0.15917808]. 
=============================================
[2019-04-05 12:18:13,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2628815e-08 3.9595542e-03 1.6290242e-03 9.9352223e-01 8.8566949e-04
 1.1943699e-07 3.3066183e-06], sum to 1.0000
[2019-04-05 12:18:13,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4058
[2019-04-05 12:18:13,865] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 35.33333333333334, 0.0, 0.0, 19.0, 18.92568158843897, -1.15814975670077, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4080000.0000, 
sim time next is 4080600.0000, 
raw observation next is [-4.0, 36.0, 0.0, 0.0, 19.0, 18.84700596298148, -1.173039969321259, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.36, 0.0, 0.0, 0.08333333333333333, 0.07058383024845671, 0.10898667689291368, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39841884], dtype=float32), -0.30343476]. 
=============================================
[2019-04-05 12:18:14,939] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3600080e-06 4.4029209e-04 6.9682096e-04 9.9616832e-01 2.6841566e-03
 6.3651237e-07 8.3752002e-06], sum to 1.0000
[2019-04-05 12:18:14,946] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2801
[2019-04-05 12:18:14,956] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 19.0, 19.52795480638627, -0.908494919479932, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3267000.0000, 
sim time next is 3267600.0000, 
raw observation next is [-4.0, 69.0, 0.0, 0.0, 19.0, 19.39163456690536, -0.930594792361115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.69, 0.0, 0.0, 0.08333333333333333, 0.11596954724211332, 0.18980173587962834, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.358623], dtype=float32), 0.54159963]. 
=============================================
[2019-04-05 12:18:15,009] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2530863e-04 1.3233431e-01 1.6957453e-02 8.3013386e-01 1.9471414e-02
 1.2969789e-04 7.4800017e-04], sum to 1.0000
[2019-04-05 12:18:15,010] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3008
[2019-04-05 12:18:15,026] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 33.5, 114.0, 769.0, 19.0, 21.40142688598833, -0.7484381007694053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4098600.0000, 
sim time next is 4099200.0000, 
raw observation next is [-1.333333333333333, 33.0, 115.1666666666667, 776.8333333333334, 19.0, 21.39883358742573, -0.6469651020005728, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.33, 0.383888888888889, 0.8583793738489871, 0.08333333333333333, 0.28323613228547756, 0.2843449659998091, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94780797], dtype=float32), 2.2832942]. 
=============================================
[2019-04-05 12:18:19,017] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2914307e-10 8.9088600e-04 1.9436788e-04 9.9806148e-01 8.5317396e-04
 1.7601520e-08 5.9866316e-08], sum to 1.0000
[2019-04-05 12:18:19,017] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8014
[2019-04-05 12:18:19,031] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.5, 73.5, 0.0, 0.0, 19.0, 18.64554321704099, -1.086235220100413, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4600200.0000, 
sim time next is 4600800.0000, 
raw observation next is [-2.6, 74.0, 0.0, 0.0, 19.0, 18.99363113152946, -1.057865226624507, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3905817174515236, 0.74, 0.0, 0.0, 0.08333333333333333, 0.0828025942941218, 0.147378257791831, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.899081], dtype=float32), -1.6097373]. 
=============================================
[2019-04-05 12:18:19,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7138496e-13 4.2635601e-04 6.2625822e-06 9.9938786e-01 1.7936833e-04
 3.5751728e-11 1.9032873e-09], sum to 1.0000
[2019-04-05 12:18:19,402] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4623
[2019-04-05 12:18:19,419] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 72.33333333333333, 0.0, 0.0, 19.0, 18.69025018553847, -1.05051771782478, 0.0, 1.0, 34664.02919957316], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3286200.0000, 
sim time next is 3286800.0000, 
raw observation next is [-7.0, 70.0, 0.0, 0.0, 19.0, 18.71336506681238, -1.052095125803391, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.7, 0.0, 0.0, 0.08333333333333333, 0.0594470889010316, 0.149301624732203, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.99744374], dtype=float32), -0.7254859]. 
=============================================
[2019-04-05 12:18:19,688] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.9668751e-11 8.8690858e-06 1.2206905e-04 9.9950385e-01 3.6526035e-04
 3.3162281e-10 1.6107227e-10], sum to 1.0000
[2019-04-05 12:18:19,693] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1937
[2019-04-05 12:18:19,703] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.166666666666667, 65.83333333333334, 0.0, 0.0, 19.5, 19.55065133156638, -0.9876811827839006, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3561000.0000, 
sim time next is 3561600.0000, 
raw observation next is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 19.5, 19.47670992085471, -1.005856899764722, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.31486611265004616, 0.6666666666666667, 0.0, 0.0, 0.125, 0.12305916007122579, 0.16471436674509268, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.6688436], dtype=float32), -1.1903242]. 
=============================================
[2019-04-05 12:18:22,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5381459e-14 1.4181771e-06 7.6768833e-07 9.9999678e-01 1.1140914e-06
 8.4587515e-14 2.3995987e-12], sum to 1.0000
[2019-04-05 12:18:22,917] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-05 12:18:22,931] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.3, 75.33333333333334, 0.0, 0.0, 20.0, 19.81015822785322, -0.9397647422128143, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4322400.0000, 
sim time next is 4323000.0000, 
raw observation next is [4.25, 75.16666666666666, 0.0, 0.0, 20.0, 19.77322157792052, -0.949079965123202, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5803324099722993, 0.7516666666666666, 0.0, 0.0, 0.16666666666666666, 0.14776846482671, 0.18364001162559931, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.49380502], dtype=float32), 1.5645958]. 
=============================================
[2019-04-05 12:18:22,949] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[ 95.58315 ]
 [ 97.889656]
 [ 99.651535]
 [101.90449 ]
 [103.67181 ]], R is [[93.38990021]
 [93.31314087]
 [93.2371521 ]
 [93.16191864]
 [93.08744049]].
[2019-04-05 12:18:30,029] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9978617e-12 2.5843433e-06 1.0338599e-06 9.9998486e-01 1.1437793e-05
 5.1931007e-12 4.5762154e-11], sum to 1.0000
[2019-04-05 12:18:30,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0601
[2019-04-05 12:18:30,044] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8, 61.66666666666667, 0.0, 0.0, 19.0, 19.0860121180542, -1.046835159046233, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4580400.0000, 
sim time next is 4581000.0000, 
raw observation next is [0.7, 62.0, 0.0, 0.0, 19.0, 19.1160272818061, -1.047092058339457, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4819944598337951, 0.62, 0.0, 0.0, 0.08333333333333333, 0.09300227348384175, 0.15096931388684767, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2277361], dtype=float32), 0.73780644]. 
=============================================
[2019-04-05 12:18:30,050] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[84.90601 ]
 [86.79448 ]
 [88.516884]
 [90.81043 ]
 [91.1749  ]], R is [[83.16669464]
 [83.3350296 ]
 [83.50167847]
 [83.66666412]
 [83.83000183]].
[2019-04-05 12:18:30,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:18:30,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:18:30,350] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-05 12:18:32,881] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-05 12:18:32,888] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:18:32,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:18:32,889] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:18:32,889] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:18:32,889] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:18:32,890] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:18:32,895] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run27
[2019-04-05 12:18:32,907] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run27
[2019-04-05 12:18:32,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run27
[2019-04-05 12:19:01,666] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10260394]
[2019-04-05 12:19:01,667] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.23333333333333, 81.5, 206.3333333333333, 251.0, 19.0, 20.38437550094982, -0.7894810001527949, 1.0, 1.0, 0.0]
[2019-04-05 12:19:01,668] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:19:01,669] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.5552890e-10 2.0019605e-04 3.3942681e-05 9.9955851e-01 2.0736431e-04
 2.6226565e-09 7.4136066e-09], sampled 0.8463282875851018
[2019-04-05 12:19:41,464] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5997.0917 146916300.6761 -2044.7807
[2019-04-05 12:19:43,919] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10260394]
[2019-04-05 12:19:43,919] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.572959715833333, 74.89954723333334, 0.0, 0.0, 19.0, 18.853855508447, -1.096711469600625, 0.0, 1.0, 0.0]
[2019-04-05 12:19:43,919] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:19:43,920] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.8772743e-14 5.6597319e-06 1.5019526e-06 9.9996495e-01 2.7837570e-05
 2.3604116e-12 1.5526656e-11], sampled 0.8406447661338511
[2019-04-05 12:19:58,523] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5278.4920 184111746.6421 -2192.4453
[2019-04-05 12:19:59,490] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5521.6982 190338283.5284 -2515.4800
[2019-04-05 12:20:00,514] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 520000, evaluation results [520000.0, 5278.491953789726, 184111746.64206868, -2192.445316999778, 5997.091747749968, 146916300.6761338, -2044.78069032012, 5521.698189895528, 190338283.52843815, -2515.480033714062]
[2019-04-05 12:20:04,251] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8606848e-13 1.0067826e-06 2.1788924e-07 9.9996781e-01 3.1026393e-05
 1.0282170e-12 1.9732213e-10], sum to 1.0000
[2019-04-05 12:20:04,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0653
[2019-04-05 12:20:04,270] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 81.66666666666667, 0.0, 0.0, 19.5, 19.2660612144721, -1.004059364530386, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4750800.0000, 
sim time next is 4751400.0000, 
raw observation next is [-3.833333333333333, 82.83333333333333, 0.0, 0.0, 19.5, 19.21035136930642, -1.012063510868915, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.3564173591874424, 0.8283333333333333, 0.0, 0.0, 0.125, 0.10086261410886828, 0.1626454963770283, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.5350614], dtype=float32), -0.43435082]. 
=============================================
[2019-04-05 12:20:04,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2196836e-12 2.0751954e-06 3.4937926e-07 9.9986708e-01 1.3049843e-04
 7.1017190e-12 3.1923730e-09], sum to 1.0000
[2019-04-05 12:20:04,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5794
[2019-04-05 12:20:04,526] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.5, 19.08138576622358, -1.018546793469861, 0.0, 1.0, 30372.02922724532], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4756200.0000, 
sim time next is 4756800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.5, 19.1303287593316, -1.01653556265746, 0.0, 1.0, 18731.64269473334], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.125, 0.0941940632776334, 0.16115481244751337, 0.0, 1.0, 0.08919829854634923], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.46212965], dtype=float32), 1.158316]. 
=============================================
[2019-04-05 12:20:07,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:20:07,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:07,756] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-05 12:20:10,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4230418e-13 3.1016418e-06 3.4740533e-05 9.9990129e-01 6.0855065e-05
 5.9882967e-11 6.6207768e-09], sum to 1.0000
[2019-04-05 12:20:10,072] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3265
[2019-04-05 12:20:10,080] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 19.0, 19.22182542185799, -0.9741209628247542, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4578000.0000, 
sim time next is 4578600.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 19.0, 19.29780758842097, -0.9646216969962421, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.08333333333333333, 0.10815063236841412, 0.17845943433458597, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22412032], dtype=float32), 0.109059]. 
=============================================
[2019-04-05 12:20:10,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2155074e-10 1.8336139e-04 1.7137238e-04 9.9926132e-01 3.8391873e-04
 9.5455099e-10 5.3885561e-08], sum to 1.0000
[2019-04-05 12:20:11,000] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1227
[2019-04-05 12:20:11,017] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 18.61201662445615, -1.162202414980601, 0.0, 1.0, 60501.19548524584], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4841400.0000, 
sim time next is 4842000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 18.65061285059156, -1.156195772058663, 0.0, 1.0, 22498.24983311841], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.05421773754929671, 0.11460140931377898, 0.0, 1.0, 0.10713452301484958], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2763367], dtype=float32), 0.5361002]. 
=============================================
[2019-04-05 12:20:11,026] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.70041 ]
 [88.93704 ]
 [90.58203 ]
 [91.268936]
 [93.29595 ]], R is [[86.30052948]
 [86.43752289]
 [86.57315063]
 [86.70742035]
 [86.84034729]].
[2019-04-05 12:20:15,463] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8589192e-07 2.6029687e-02 1.8895958e-03 9.5871955e-01 1.3345781e-02
 3.7699690e-06 1.1035008e-05], sum to 1.0000
[2019-04-05 12:20:15,465] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8592
[2019-04-05 12:20:15,474] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 46.00000000000001, 107.0, 643.0, 19.0, 20.27008055763408, -0.8556632983452799, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5044200.0000, 
sim time next is 5044800.0000, 
raw observation next is [1.666666666666667, 45.0, 109.5, 670.5, 19.0, 20.34212489632415, -0.8269737017594342, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5087719298245615, 0.45, 0.365, 0.7408839779005525, 0.08333333333333333, 0.19517707469367926, 0.22434209941352193, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6195777], dtype=float32), -0.89804775]. 
=============================================
[2019-04-05 12:20:17,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:20:17,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:17,738] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-05 12:20:18,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:20:18,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:18,542] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-05 12:20:18,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6043404e-10 2.0402747e-04 1.3252196e-04 9.9896991e-01 6.9357525e-04
 1.9775262e-10 1.8089720e-09], sum to 1.0000
[2019-04-05 12:20:18,774] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4388
[2019-04-05 12:20:18,784] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.166666666666666, 26.66666666666666, 0.0, 0.0, 19.0, 18.70194982183347, -1.199299957828789, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3647400.0000, 
sim time next is 3648000.0000, 
raw observation next is [9.333333333333334, 26.33333333333334, 0.0, 0.0, 19.0, 18.73350962232688, -1.195924322755515, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.7211449676823639, 0.2633333333333334, 0.0, 0.0, 0.08333333333333333, 0.061125801860573446, 0.101358559081495, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.363669], dtype=float32), 0.8280816]. 
=============================================
[2019-04-05 12:20:18,803] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.062935]
 [82.179886]
 [80.484085]
 [80.22403 ]
 [80.3863  ]], R is [[82.30002594]
 [82.47702789]
 [82.65225983]
 [82.825737  ]
 [82.9974823 ]].
[2019-04-05 12:20:21,374] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:20:21,375] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:21,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-05 12:20:21,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9793833e-09 1.1536534e-04 7.7764962e-05 9.9187464e-01 7.9321889e-03
 4.9488618e-09 5.4691527e-09], sum to 1.0000
[2019-04-05 12:20:21,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5785
[2019-04-05 12:20:21,608] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3, 49.5, 283.0, 308.0, 19.0, 18.61284575794603, -1.140271666743334, 0.0, 1.0, 9341.117076682352], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 19.0, 18.62093745952829, -1.137637861357145, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4773776546629733, 0.4866666666666667, 0.9422222222222223, 0.3554327808471455, 0.08333333333333333, 0.05174478829402407, 0.12078737954761835, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9478687], dtype=float32), -1.9582369]. 
=============================================
[2019-04-05 12:20:21,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:20:21,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:21,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-05 12:20:22,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:20:22,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:22,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-05 12:20:27,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0365326e-07 1.2170198e-02 5.3165149e-04 9.8457885e-01 2.7171236e-03
 2.7839201e-07 1.6887125e-06], sum to 1.0000
[2019-04-05 12:20:27,849] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8318
[2019-04-05 12:20:27,857] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 89.0, 213.0, 6.0, 19.5, 20.78213273440756, -0.7716390130200951, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4707000.0000, 
sim time next is 4707600.0000, 
raw observation next is [0.6666666666666666, 88.0, 195.5, 5.0, 19.5, 20.79269392894568, -0.773980874194855, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4810710987996307, 0.88, 0.6516666666666666, 0.0055248618784530384, 0.125, 0.23272449407880677, 0.24200637526838167, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1586895], dtype=float32), 0.17381667]. 
=============================================
[2019-04-05 12:20:27,979] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0600439e-17 5.0420379e-09 5.2044955e-11 9.9999988e-01 1.6336492e-07
 3.7938072e-15 1.8150111e-16], sum to 1.0000
[2019-04-05 12:20:27,982] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5184
[2019-04-05 12:20:28,000] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.466666666666667, 75.16666666666666, 0.0, 0.0, 19.0, 18.88531348556792, -1.060202507998936, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4315800.0000, 
sim time next is 4316400.0000, 
raw observation next is [4.4, 75.0, 0.0, 0.0, 19.0, 19.06558883797505, -1.039726895928424, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.5844875346260389, 0.75, 0.0, 0.0, 0.08333333333333333, 0.08879906983125412, 0.15342436802385864, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08186718], dtype=float32), 0.19623773]. 
=============================================
[2019-04-05 12:20:29,469] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6283812e-13 1.7573673e-03 3.7778719e-04 9.9704927e-01 8.1567105e-04
 2.7363900e-10 1.1659480e-09], sum to 1.0000
[2019-04-05 12:20:29,471] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8936
[2019-04-05 12:20:29,488] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 19.5, 19.37052005524071, -0.9625962840082717, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4749600.0000, 
sim time next is 4750200.0000, 
raw observation next is [-3.5, 80.5, 0.0, 0.0, 19.5, 19.40429413000102, -0.9709073963880881, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.36565096952908593, 0.805, 0.0, 0.0, 0.125, 0.11702451083341832, 0.17636420120397064, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.380615], dtype=float32), 1.9436212]. 
=============================================
[2019-04-05 12:20:30,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:20:30,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:30,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-05 12:20:31,527] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9162601e-06 7.0134975e-02 4.2486433e-03 9.0410584e-01 2.1347838e-02
 2.2800501e-05 1.3586039e-04], sum to 1.0000
[2019-04-05 12:20:31,530] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8892
[2019-04-05 12:20:31,544] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.333333333333333, 51.0, 123.1666666666667, 822.0, 19.0, 20.75355155301376, -0.8116936068099484, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4620000.0000, 
sim time next is 4620600.0000, 
raw observation next is [2.5, 50.5, 122.0, 833.0, 19.0, 20.8124823005891, -0.7042023540029043, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5318559556786704, 0.505, 0.4066666666666667, 0.9204419889502763, 0.08333333333333333, 0.23437352504909162, 0.2652658819990319, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6445419], dtype=float32), 1.4548784]. 
=============================================
[2019-04-05 12:20:31,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3266056e-15 4.1025396e-06 6.2121671e-06 9.9996805e-01 2.1674859e-05
 2.2590682e-13 4.5320887e-12], sum to 1.0000
[2019-04-05 12:20:31,761] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7702
[2019-04-05 12:20:31,797] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 87.0, 20.5, 22.5, 19.5, 19.19365733486341, -1.0613265837634, 0.0, 1.0, 32292.35672824974], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 579600.0000, 
sim time next is 580200.0000, 
raw observation next is [-1.8, 87.0, 0.0, 0.0, 19.5, 19.19296853494806, -1.069990442774847, 0.0, 1.0, 32329.91512957825], 
processed observation next is [0.0, 0.7391304347826086, 0.41274238227146814, 0.87, 0.0, 0.0, 0.125, 0.09941404457900511, 0.143336519075051, 0.0, 1.0, 0.1539519768075155], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.4540602], dtype=float32), 0.206584]. 
=============================================
[2019-04-05 12:20:32,885] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1407574e-13 1.6929019e-06 4.3330356e-06 9.9938786e-01 6.0619600e-04
 6.4304839e-11 2.2030132e-10], sum to 1.0000
[2019-04-05 12:20:32,885] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6874
[2019-04-05 12:20:32,896] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 19.0, 19.41040938210659, -0.9462540151694845, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4662600.0000, 
sim time next is 4663200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 19.38193614537654, -0.9544487314290162, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.08333333333333333, 0.1151613454480449, 0.1818504228569946, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5592427], dtype=float32), -0.3709154]. 
=============================================
[2019-04-05 12:20:33,056] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 12:20:33,058] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:20:33,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:33,060] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:20:33,060] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:20:33,060] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:33,060] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:20:33,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run28
[2019-04-05 12:20:33,070] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run28
[2019-04-05 12:20:33,096] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run28
[2019-04-05 12:20:42,044] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10420489]
[2019-04-05 12:20:42,044] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-6.616666666666666, 60.0, 62.0, 210.3333333333333, 20.0, 19.80383850040605, -0.9141897089271924, 1.0, 1.0, 0.0]
[2019-04-05 12:20:42,045] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:20:42,046] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.7599450e-09 6.4132985e-04 2.2281082e-04 9.9807906e-01 1.0565337e-03
 4.8597897e-08 1.9144974e-07], sampled 0.35581207559849604
[2019-04-05 12:21:08,786] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10420489]
[2019-04-05 12:21:08,786] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.9, 85.0, 0.0, 0.0, 19.0, 18.6162710580078, -1.232011468542034, 0.0, 1.0, 18731.80472848989]
[2019-04-05 12:21:08,786] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:21:08,787] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.45262317e-12 3.66597524e-05 1.05343624e-05 9.99832988e-01
 1.19769014e-04 8.01096758e-11 5.26654498e-10], sampled 0.07049561079323896
[2019-04-05 12:21:40,050] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10420489]
[2019-04-05 12:21:40,050] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.6076346466666667, 74.81804984333334, 0.0, 0.0, 19.5, 19.47891274287731, -1.00487485910759, 0.0, 1.0, 0.0]
[2019-04-05 12:21:40,050] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:21:40,050] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.8423319e-15 2.8260638e-06 6.5584402e-07 9.9998474e-01 1.1663436e-05
 2.1146373e-13 1.9694136e-12], sampled 0.07449780003644624
[2019-04-05 12:21:42,898] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5821.6037 149350818.8480 -1947.3775
[2019-04-05 12:21:48,984] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10420489]
[2019-04-05 12:21:48,985] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.0, 50.0, 0.0, 0.0, 19.5, 19.43289821993761, -1.108102634989728, 0.0, 1.0, 0.0]
[2019-04-05 12:21:48,985] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:21:48,987] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.7659487e-11 7.6476383e-05 2.9603427e-05 9.9965429e-01 2.3967470e-04
 6.5531625e-10 3.2379943e-09], sampled 0.2671787322620477
[2019-04-05 12:21:50,766] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10420489]
[2019-04-05 12:21:50,767] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.322381179, 45.09572913, 0.0, 0.0, 20.5, 20.22170384776276, -0.8816643915763224, 0.0, 1.0, 0.0]
[2019-04-05 12:21:50,767] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:21:50,769] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7371018e-12 1.8904599e-05 8.3165205e-06 9.9988651e-01 8.6170854e-05
 5.6054470e-11 2.5254515e-10], sampled 0.2504876407060844
[2019-04-05 12:21:50,841] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5605.4088 172476587.8723 -2804.7911
[2019-04-05 12:22:01,636] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5413.0536 193845849.9864 -2436.8915
[2019-04-05 12:22:02,659] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 540000, evaluation results [540000.0, 5605.408846996971, 172476587.8723245, -2804.7911182609273, 5821.603697410181, 149350818.84796393, -1947.3774519464616, 5413.053641072606, 193845849.98638076, -2436.891475369611]
[2019-04-05 12:22:04,650] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4349410e-13 1.5853889e-03 5.7080124e-05 9.9817598e-01 1.8147328e-04
 1.5403630e-11 1.3953767e-10], sum to 1.0000
[2019-04-05 12:22:04,650] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1536
[2019-04-05 12:22:04,659] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.45, 86.0, 79.0, 0.0, 20.0, 19.0307099514492, -0.9922004823918087, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 52200.0000, 
sim time next is 52800.0000, 
raw observation next is [7.366666666666667, 86.0, 74.16666666666667, 0.0, 20.0, 19.08586103450585, -0.9937255121804119, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6666666666666667, 0.86, 0.24722222222222223, 0.0, 0.16666666666666666, 0.09048841954215418, 0.16875816260652934, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.0177362], dtype=float32), -0.3477898]. 
=============================================
[2019-04-05 12:22:13,758] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:22:13,760] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:13,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-05 12:22:15,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:22:15,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:15,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-05 12:22:15,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7249364e-14 4.7998293e-07 2.2194417e-06 9.9994314e-01 5.4071181e-05
 2.0506275e-11 1.7370304e-09], sum to 1.0000
[2019-04-05 12:22:15,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3810
[2019-04-05 12:22:15,986] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.333333333333333, 42.0, 0.0, 0.0, 19.0, 18.83814838249569, -1.126130168420703, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4819200.0000, 
sim time next is 4819800.0000, 
raw observation next is [1.166666666666667, 42.5, 0.0, 0.0, 19.0, 18.76560393489866, -1.142145562414905, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.49492151431209613, 0.425, 0.0, 0.0, 0.08333333333333333, 0.0638003279082217, 0.11928481252836498, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.53678846], dtype=float32), -1.6437025]. 
=============================================
[2019-04-05 12:22:16,482] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3456096e-15 7.9273451e-07 7.7777280e-08 9.9999881e-01 2.2982471e-07
 7.1743467e-14 4.2493648e-13], sum to 1.0000
[2019-04-05 12:22:16,483] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1504
[2019-04-05 12:22:16,499] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 18.93140703632191, -1.116448900955815, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4834200.0000, 
sim time next is 4834800.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 18.91521163174811, -1.126083839979608, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.07626763597900925, 0.12463872000679732, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.58864784], dtype=float32), -0.00017555547]. 
=============================================
[2019-04-05 12:22:17,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:22:17,772] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:17,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-05 12:22:18,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:22:18,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:18,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-05 12:22:19,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:22:19,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:19,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-05 12:22:23,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:22:23,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:23,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-05 12:22:27,164] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:22:27,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:27,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-05 12:22:31,508] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8078492e-11 1.0683020e-05 2.5335129e-04 9.9929416e-01 4.4169452e-04
 1.4924941e-09 8.1727087e-09], sum to 1.0000
[2019-04-05 12:22:31,509] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6098
[2019-04-05 12:22:31,547] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.283333333333334, 64.33333333333334, 92.66666666666667, 25.33333333333334, 19.0, 18.51310397617724, -1.252541679344149, 0.0, 1.0, 30747.99588531495], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 19.0, 18.50686422191827, -1.252635438006481, 0.0, 1.0, 30036.53331404737], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.6366666666666667, 0.3027777777777778, 0.03499079189686925, 0.08333333333333333, 0.042238685159855884, 0.08245485399783965, 0.0, 1.0, 0.14303111101927318], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15786342], dtype=float32), -2.5533783]. 
=============================================
[2019-04-05 12:22:36,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1463709e-12 3.9888670e-05 3.4284948e-08 9.9972457e-01 2.3554043e-04
 1.2315325e-10 1.5862522e-10], sum to 1.0000
[2019-04-05 12:22:36,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4129
[2019-04-05 12:22:36,072] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.5, 19.38858478116319, -0.9808736257034728, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1484400.0000, 
sim time next is 1485000.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.5, 19.34660790934041, -0.9921443267472755, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.125, 0.11221732577836747, 0.16928522441757485, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.8189975], dtype=float32), 0.6409428]. 
=============================================
[2019-04-05 12:22:36,083] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.788475]
 [85.39283 ]
 [85.21667 ]
 [84.91059 ]
 [84.58368 ]], R is [[85.96260071]
 [86.03154755]
 [86.09980774]
 [86.16738129]
 [86.23428345]].
[2019-04-05 12:22:39,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4708401e-09 8.7626145e-04 5.5147550e-04 9.9476457e-01 3.7742422e-03
 3.9262432e-06 2.9463230e-05], sum to 1.0000
[2019-04-05 12:22:39,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1747
[2019-04-05 12:22:39,936] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 17.87395541876336, -1.438187201731749, 0.0, 1.0, 51933.58792340968], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 368400.0000, 
sim time next is 369000.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 19.0, 17.91532705953674, -1.446896620328761, 0.0, 1.0, 51977.55880462086], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.08333333333333333, -0.007056078371938372, 0.01770112655707967, 0.0, 1.0, 0.24751218478390885], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43012252], dtype=float32), -1.4232639]. 
=============================================
[2019-04-05 12:22:39,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[57.98074 ]
 [57.803825]
 [57.67262 ]
 [57.47266 ]
 [57.343445]], R is [[58.52667236]
 [58.94140625]
 [59.35199356]
 [59.75847244]
 [60.16088867]].
[2019-04-05 12:22:40,730] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-05 12:22:40,731] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:22:40,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:40,732] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:22:40,732] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:40,733] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:22:40,734] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:22:40,740] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run29
[2019-04-05 12:22:40,754] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run29
[2019-04-05 12:22:40,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run29
[2019-04-05 12:23:13,259] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.103736356]
[2019-04-05 12:23:13,260] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.608423396333334, 79.87072099333335, 0.0, 0.0, 19.0, 19.47788132230417, -0.9506337053672693, 0.0, 1.0, 0.0]
[2019-04-05 12:23:13,260] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:23:13,261] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.2825306e-15 3.1349455e-06 7.3722885e-07 9.9996614e-01 3.0005418e-05
 4.3517693e-13 4.1461500e-12], sampled 0.045243908066488814
[2019-04-05 12:23:32,583] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.103736356]
[2019-04-05 12:23:32,583] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.9, 70.0, 34.0, 352.5, 20.5, 19.99994550158074, -0.9044579855704998, 1.0, 1.0, 0.0]
[2019-04-05 12:23:32,583] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:23:32,584] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.6966875e-11 7.7567754e-05 2.7587934e-05 9.9951053e-01 3.8431707e-04
 6.6332878e-10 3.4697722e-09], sampled 0.612271397164771
[2019-04-05 12:23:41,976] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.103736356]
[2019-04-05 12:23:41,976] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.2, 44.0, 48.0, 389.5, 21.5, 23.09307318393574, -0.2188351836975329, 1.0, 1.0, 0.0]
[2019-04-05 12:23:41,976] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:23:41,977] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.2064483e-09 5.2498950e-04 1.0295606e-04 9.9840897e-01 9.6302392e-04
 8.9258112e-09 5.1610598e-08], sampled 0.6307446237011735
[2019-04-05 12:23:49,520] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.103736356]
[2019-04-05 12:23:49,521] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.1466212495000001, 68.591287035, 0.0, 151.77208, 20.0, 19.5863814174144, -0.9633781430601337, 0.0, 1.0, 0.0]
[2019-04-05 12:23:49,521] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:23:49,522] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2081679e-12 2.1508667e-05 8.0880500e-06 9.9980694e-01 1.6354313e-04
 4.5233040e-11 2.7157587e-10], sampled 0.8801588388136407
[2019-04-05 12:23:55,947] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 5475.2557 159425412.4129 -1553.7878
[2019-04-05 12:23:59,464] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5635.5405 173047887.3950 -2805.9953
[2019-04-05 12:24:10,330] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.103736356]
[2019-04-05 12:24:10,330] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [9.238555471, 17.03478249, 32.57599344, 159.4985828, 20.0, 22.60853045198692, -0.3558771179644891, 1.0, 1.0, 0.0]
[2019-04-05 12:24:10,330] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:24:10,331] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2136003e-09 5.0661230e-04 9.1166425e-05 9.9854589e-01 8.5632602e-04
 8.2923144e-09 5.3023282e-08], sampled 0.13877051469226975
[2019-04-05 12:24:11,510] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5457.7720 193817111.4446 -2330.9861
[2019-04-05 12:24:12,535] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 560000, evaluation results [560000.0, 5635.540537500671, 173047887.39502165, -2805.9953231742297, 5475.255748309765, 159425412.41287065, -1553.7877883962715, 5457.772005331696, 193817111.44457206, -2330.9860905561554]
[2019-04-05 12:24:15,160] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3562788e-10 7.2700181e-04 3.1524108e-04 9.9787462e-01 1.0821176e-03
 8.4749896e-09 9.7528596e-07], sum to 1.0000
[2019-04-05 12:24:15,164] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2398
[2019-04-05 12:24:15,175] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 79.00000000000001, 0.0, 0.0, 19.0, 18.69723823745173, -1.196129974829437, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 871800.0000, 
sim time next is 872400.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 19.0, 18.82472091367408, -1.201716355116181, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.08333333333333333, 0.06872674280617345, 0.0994278816279397, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2632024], dtype=float32), -1.8299457]. 
=============================================
[2019-04-05 12:24:17,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4344418e-09 1.2237858e-04 8.2006416e-05 9.9917942e-01 6.1600964e-04
 8.9651685e-08 1.6262511e-08], sum to 1.0000
[2019-04-05 12:24:17,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2698
[2019-04-05 12:24:17,115] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.26666666666667, 84.33333333333334, 120.5, 0.0, 19.0, 20.70980169490677, -0.7354702754622213, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 998400.0000, 
sim time next is 999000.0000, 
raw observation next is [13.55, 83.5, 119.0, 0.0, 19.0, 20.77018298242521, -0.7162059728202886, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8379501385041552, 0.835, 0.39666666666666667, 0.0, 0.08333333333333333, 0.23084858186876764, 0.26126467572657047, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6440649], dtype=float32), 2.1776392]. 
=============================================
[2019-04-05 12:24:17,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[64.32497 ]
 [64.01351 ]
 [63.759212]
 [63.425625]
 [63.139465]], R is [[63.9550705 ]
 [63.31552124]
 [62.68236542]
 [62.05554199]
 [61.43498611]].
[2019-04-05 12:24:17,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:24:17,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:24:17,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-05 12:24:17,638] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8043919e-11 2.8805411e-03 2.3404132e-04 9.8937863e-01 7.5067519e-03
 7.0376655e-10 7.4268868e-08], sum to 1.0000
[2019-04-05 12:24:17,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0068
[2019-04-05 12:24:17,656] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.2, 79.83333333333334, 0.0, 0.0, 21.0, 20.60972941833522, -0.7862254360949669, 0.0, 1.0, 34447.49143829542], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 868200.0000, 
sim time next is 868800.0000, 
raw observation next is [-2.1, 79.66666666666667, 0.0, 0.0, 21.0, 20.57143142635405, -0.7923554093063854, 0.0, 1.0, 53549.56117225311], 
processed observation next is [1.0, 0.043478260869565216, 0.404432132963989, 0.7966666666666667, 0.0, 0.0, 0.25, 0.21428595219617078, 0.23588153023120487, 0.0, 1.0, 0.25499791034406244], 
reward next is 0.7143, 
noisyNet noise sample is [array([-1.4234703], dtype=float32), 0.37860164]. 
=============================================
[2019-04-05 12:24:18,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.8401605e-04 3.7385318e-01 2.0506417e-02 5.3116935e-01 7.2501637e-02
 4.6703644e-04 1.0183279e-03], sum to 1.0000
[2019-04-05 12:24:18,634] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7390
[2019-04-05 12:24:18,661] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 28.0, 124.0, 0.0, 19.0, 19.06768311093292, -1.266493778698995, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 478800.0000, 
sim time next is 479400.0000, 
raw observation next is [-1.1, 29.16666666666667, 122.3333333333333, 0.0, 19.0, 18.96272711254859, -1.275219642906007, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4321329639889197, 0.29166666666666674, 0.4077777777777777, 0.0, 0.08333333333333333, 0.08022725937904902, 0.07492678569799767, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2775326], dtype=float32), 1.9863979]. 
=============================================
[2019-04-05 12:24:19,844] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0836301e-08 3.7058890e-03 7.1055046e-03 9.8445475e-01 4.7301641e-03
 2.6822784e-07 3.3543145e-06], sum to 1.0000
[2019-04-05 12:24:19,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3262
[2019-04-05 12:24:19,863] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.7, 54.00000000000001, 0.0, 0.0, 20.0, 19.21018340163204, -1.143679985139533, 0.0, 1.0, 49157.90221699828], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 429600.0000, 
sim time next is 430200.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 20.0, 19.18652734559401, -1.148624411774836, 0.0, 1.0, 49120.40946844289], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.16666666666666666, 0.0988772787995007, 0.11712519607505463, 0.0, 1.0, 0.23390671175448996], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.0105003], dtype=float32), 0.037081048]. 
=============================================
[2019-04-05 12:24:21,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.0169586e-16 7.3242319e-08 2.7425523e-07 9.9999666e-01 3.0030797e-06
 1.0021018e-13 4.1679130e-13], sum to 1.0000
[2019-04-05 12:24:21,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3928
[2019-04-05 12:24:21,283] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 19.76185749745121, -0.7729241676959271, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1134600.0000, 
sim time next is 1135200.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 19.0, 19.73884959009906, -0.7776852019249687, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.08333333333333333, 0.1449041325082551, 0.24077159935834377, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17565374], dtype=float32), -1.7502837]. 
=============================================
[2019-04-05 12:24:27,505] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9684910e-14 4.0451592e-05 5.0622293e-07 9.9983466e-01 1.2449099e-04
 1.6188907e-12 6.2041969e-12], sum to 1.0000
[2019-04-05 12:24:27,508] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2349
[2019-04-05 12:24:27,543] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6333333333333333, 82.66666666666667, 85.0, 137.0, 20.0, 19.18718130314784, -1.036233493875752, 0.0, 1.0, 26653.03511211177], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 558600.0000, 
sim time next is 559200.0000, 
raw observation next is [-0.6666666666666667, 82.33333333333334, 87.0, 136.0, 20.0, 19.24095101005776, -1.036768472459674, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.44413665743305636, 0.8233333333333335, 0.29, 0.15027624309392265, 0.16666666666666666, 0.10341258417147998, 0.15441050918010868, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.12346894], dtype=float32), -0.18065177]. 
=============================================
[2019-04-05 12:24:28,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3539599e-06 1.0987706e-01 1.3202297e-02 8.5168481e-01 2.5226669e-02
 3.6397905e-06 4.2449415e-06], sum to 1.0000
[2019-04-05 12:24:28,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9979
[2019-04-05 12:24:28,064] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.6, 48.0, 0.0, 0.0, 19.0, 18.70379589062137, -1.239292997133768, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 423000.0000, 
sim time next is 423600.0000, 
raw observation next is [-10.6, 48.33333333333333, 0.0, 0.0, 19.5, 18.69411465957291, -1.238147832754976, 0.0, 1.0, 137177.7135162802], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.4833333333333333, 0.0, 0.0, 0.125, 0.05784288829774257, 0.08728405574834135, 0.0, 1.0, 0.6532272072203819], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.2955148], dtype=float32), -0.58672875]. 
=============================================
[2019-04-05 12:24:31,552] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8561079e-10 1.3452180e-02 1.0820330e-04 9.7850925e-01 7.9298867e-03
 2.4220455e-08 4.3577515e-07], sum to 1.0000
[2019-04-05 12:24:31,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1254
[2019-04-05 12:24:31,564] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 95.66666666666666, 0.0, 0.0, 19.0, 18.79468411625251, -1.115108899006982, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1482600.0000, 
sim time next is 1483200.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 18.71220944650086, -1.131522827826768, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.059350787208405066, 0.12282572405774402, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77540594], dtype=float32), -1.099548]. 
=============================================
[2019-04-05 12:24:33,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4362138e-12 2.0030113e-05 4.1129688e-06 9.9977857e-01 1.9718439e-04
 1.6305242e-10 1.4674712e-08], sum to 1.0000
[2019-04-05 12:24:33,537] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5505
[2019-04-05 12:24:33,551] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 97.0, 0.0, 0.0, 19.0, 18.88330302577715, -1.178702810341467, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 518400.0000, 
sim time next is 519000.0000, 
raw observation next is [4.0, 95.66666666666667, 0.0, 0.0, 19.0, 18.85809844928292, -1.186931778473011, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.5734072022160666, 0.9566666666666667, 0.0, 0.0, 0.08333333333333333, 0.07150820410691001, 0.1043560738423297, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28230283], dtype=float32), 0.08802437]. 
=============================================
[2019-04-05 12:24:33,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[97.26312]
 [98.86371]
 [98.77873]
 [99.01162]
 [98.41469]], R is [[96.78209686]
 [96.81427765]
 [96.846138  ]
 [96.87767792]
 [96.90890503]].
[2019-04-05 12:24:37,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.5784257e-05 3.0992560e-02 6.2616812e-03 9.3123442e-01 3.1395040e-02
 6.8747959e-06 1.3659132e-05], sum to 1.0000
[2019-04-05 12:24:37,813] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0176
[2019-04-05 12:24:37,860] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 20.0, 18.97609416058, -1.088453895511248, 1.0, 1.0, 197343.1641257506], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 842400.0000, 
sim time next is 843000.0000, 
raw observation next is [-3.9, 82.66666666666667, 0.0, 0.0, 19.0, 19.28778882657266, -1.039951965303693, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.10731573554772152, 0.15334934489876897, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.97386587], dtype=float32), -1.7592555]. 
=============================================
[2019-04-05 12:24:37,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[50.569767]
 [48.935734]
 [47.32577 ]
 [45.479458]
 [45.33208 ]], R is [[49.59684753]
 [49.10087967]
 [48.60987091]
 [48.12377167]
 [47.64253616]].
[2019-04-05 12:24:40,288] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3726064e-12 4.5172546e-05 5.5508444e-06 9.8700655e-01 1.2942776e-02
 8.5437254e-11 6.2301789e-11], sum to 1.0000
[2019-04-05 12:24:40,289] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6328
[2019-04-05 12:24:40,336] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 22.5, 21.6089328360568, -0.5391302510037416, 0.0, 1.0, 18709.0027047483], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 22.5, 21.59453462206158, -0.5423704590056867, 0.0, 1.0, 27460.88076724593], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.375, 0.299544551838465, 0.31920984699810445, 0.0, 1.0, 0.13076609889164728], 
reward next is 0.5000, 
noisyNet noise sample is [array([-2.2392805], dtype=float32), 0.023954414]. 
=============================================
[2019-04-05 12:24:44,794] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5095397e-14 1.5935731e-05 1.1404563e-04 9.9566722e-01 4.2028371e-03
 1.5368704e-10 1.1984150e-09], sum to 1.0000
[2019-04-05 12:24:44,796] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6059
[2019-04-05 12:24:44,852] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.55, 83.0, 126.0, 0.0, 20.0, 19.23376294961214, -0.9823719164451926, 0.0, 1.0, 34891.25582290217], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1773000.0000, 
sim time next is 1773600.0000, 
raw observation next is [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 20.0, 19.25305974336966, -0.9769308529748008, 0.0, 1.0, 33936.37163531959], 
processed observation next is [0.0, 0.5217391304347826, 0.38965835641735924, 0.83, 0.416111111111111, 0.0, 0.16666666666666666, 0.10442164528080511, 0.1743563823417331, 0.0, 1.0, 0.16160176969199805], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.10332975], dtype=float32), -1.7361923]. 
=============================================
[2019-04-05 12:24:45,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3492969e-07 5.6589562e-03 1.3353245e-03 9.8996687e-01 2.9377770e-03
 1.3307015e-07 1.0064084e-04], sum to 1.0000
[2019-04-05 12:24:45,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1057
[2019-04-05 12:24:45,560] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.7, 60.5, 0.0, 0.0, 20.5, 22.47163921987741, -0.2519786067086494, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1535400.0000, 
sim time next is 1536000.0000, 
raw observation next is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 20.5, 22.43446584835262, -0.2517657369799845, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7285318559556788, 0.6066666666666667, 0.0, 0.0, 0.20833333333333334, 0.36953882069605165, 0.41607808767333854, 1.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.63985664], dtype=float32), 0.3034085]. 
=============================================
[2019-04-05 12:24:45,575] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.07471 ]
 [66.08073 ]
 [66.07168 ]
 [66.15406 ]
 [66.174904]], R is [[66.24823761]
 [66.37146759]
 [66.49346924]
 [66.61425018]
 [66.73381805]].
[2019-04-05 12:24:47,958] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-05 12:24:47,959] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:24:47,959] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:24:47,961] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:24:47,961] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:24:47,961] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:24:47,962] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:24:47,966] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run30
[2019-04-05 12:24:47,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run30
[2019-04-05 12:24:47,996] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run30
[2019-04-05 12:25:17,507] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10141161]
[2019-04-05 12:25:17,507] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [18.24481125, 64.46040183, 0.0, 0.0, 23.0, 22.70184878009755, -0.08481930525645538, 0.0, 0.0, 0.0]
[2019-04-05 12:25:17,508] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:25:17,508] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.7826248e-14 5.0191293e-06 2.1938727e-06 9.9976844e-01 2.2443917e-04
 6.7316422e-12 7.6834705e-11], sampled 0.016565240950312754
[2019-04-05 12:26:24,194] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 1132.2705 225561284.4434 984.8321
[2019-04-05 12:26:29,218] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10141161]
[2019-04-05 12:26:29,219] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.2572560103333333, 64.66346859666668, 0.0, 0.0, 25.5, 25.02802968858105, 0.4222802557571156, 0.0, 1.0, 28304.390837768]
[2019-04-05 12:26:29,219] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:26:29,220] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.42202743e-12 2.36124761e-05 1.04212195e-05 9.99402046e-01
 5.63973503e-04 5.14361088e-11 8.16355261e-10], sampled 0.3020349839799914
[2019-04-05 12:26:37,537] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 2754.6345 245980717.2492 -200.1893
[2019-04-05 12:26:38,951] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 1372.1943 246895828.3503 657.5222
[2019-04-05 12:26:39,975] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 580000, evaluation results [580000.0, 1372.194275610727, 246895828.3503184, 657.5222350996783, 1132.2704977416581, 225561284.44339484, 984.8321056499769, 2754.634538378007, 245980717.24924055, -200.1893327797101]
[2019-04-05 12:26:42,894] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6104759e-16 6.8855223e-07 9.3815920e-08 9.9999166e-01 7.5429962e-06
 3.4350746e-15 1.6523871e-12], sum to 1.0000
[2019-04-05 12:26:42,894] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6904
[2019-04-05 12:26:42,909] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 25.5, 23.91638659397286, 0.1032883197100546, 0.0, 1.0, 43787.12172665759], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 1735800.0000, 
sim time next is 1736400.0000, 
raw observation next is [0.1333333333333334, 91.0, 0.0, 0.0, 25.5, 23.90200845979799, 0.1015573691579286, 0.0, 1.0, 43811.71524327283], 
processed observation next is [0.0, 0.08695652173913043, 0.46629732225300097, 0.91, 0.0, 0.0, 0.625, 0.4918340383164992, 0.5338524563859762, 0.0, 1.0, 0.20862721544415636], 
reward next is 0.0714, 
noisyNet noise sample is [array([-0.63460046], dtype=float32), 0.7092354]. 
=============================================
[2019-04-05 12:26:44,258] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.3439409e-11 2.0126126e-05 1.0098741e-04 9.1265947e-01 8.7219350e-02
 4.6499076e-09 9.1332318e-08], sum to 1.0000
[2019-04-05 12:26:44,264] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6079
[2019-04-05 12:26:44,280] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 51.0, 59.99999999999999, 19.5, 19.09594618224148, -1.126116759771008, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2364000.0000, 
sim time next is 2364600.0000, 
raw observation next is [-3.4, 69.0, 64.99999999999999, 120.0, 19.5, 19.01495435994454, -1.133193734013668, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.21666666666666662, 0.13259668508287292, 0.125, 0.08457952999537828, 0.12226875532877735, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.7227789], dtype=float32), -0.9168374]. 
=============================================
[2019-04-05 12:26:44,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0790619e-11 7.7642107e-05 2.7320746e-04 9.9854410e-01 1.1051320e-03
 5.6495714e-10 7.1067410e-09], sum to 1.0000
[2019-04-05 12:26:44,304] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9131
[2019-04-05 12:26:44,331] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8, 90.0, 0.0, 0.0, 22.5, 21.61711405236741, -0.323498988644309, 0.0, 1.0, 199191.2896388632], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 1715400.0000, 
sim time next is 1716000.0000, 
raw observation next is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 22.5, 21.68730918716217, -0.2784781214675663, 0.0, 1.0, 140848.4361180214], 
processed observation next is [1.0, 0.8695652173913043, 0.4819944598337951, 0.9066666666666666, 0.0, 0.0, 0.375, 0.3072757655968476, 0.40717395951081126, 0.0, 1.0, 0.6707068386572448], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.6835812], dtype=float32), 0.48478252]. 
=============================================
[2019-04-05 12:26:44,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[87.58336 ]
 [84.224106]
 [79.795944]
 [76.84386 ]
 [74.11093 ]], R is [[90.1260376 ]
 [89.72477722]
 [89.32752991]
 [88.93425751]
 [88.54491425]].
[2019-04-05 12:26:47,544] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2060182e-15 2.5599365e-06 5.0798772e-06 9.9955207e-01 4.4030268e-04
 2.7445487e-12 5.3319411e-12], sum to 1.0000
[2019-04-05 12:26:47,547] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4161
[2019-04-05 12:26:47,565] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 22.0, 22.00674459452513, -0.2498758940362951, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1134600.0000, 
sim time next is 1135200.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 22.0, 21.9761327428733, -0.2565995052532872, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.3333333333333333, 0.3313443952394417, 0.41446683158223757, 0.0, 1.0, 0.0], 
reward next is 0.5714, 
noisyNet noise sample is [array([-1.5557663], dtype=float32), -1.3844508]. 
=============================================
[2019-04-05 12:26:47,615] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9017330e-13 5.6366957e-07 4.7830235e-06 9.9984753e-01 1.4712202e-04
 4.2378327e-11 1.4517001e-09], sum to 1.0000
[2019-04-05 12:26:47,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1966
[2019-04-05 12:26:47,622] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.46666666666667, 64.33333333333334, 88.0, 0.0, 23.0, 22.13839130360533, -0.225449275686201, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 1178400.0000, 
sim time next is 1179000.0000, 
raw observation next is [18.55, 64.0, 80.0, 0.0, 23.0, 22.13028202703584, -0.2272431781172803, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.976454293628809, 0.64, 0.26666666666666666, 0.0, 0.4166666666666667, 0.3441901689196533, 0.42425227396090653, 0.0, 0.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.36434966], dtype=float32), 1.0937468]. 
=============================================
[2019-04-05 12:26:47,627] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[92.54057 ]
 [92.271065]
 [92.04227 ]
 [91.74939 ]
 [91.52257 ]], R is [[92.27374268]
 [91.77957916]
 [91.2903595 ]
 [90.80603027]
 [90.32654572]].
[2019-04-05 12:26:56,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7774649e-10 1.3596420e-04 8.4275892e-04 9.9676955e-01 2.2515445e-03
 1.5326540e-08 3.0478125e-07], sum to 1.0000
[2019-04-05 12:26:56,593] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7109
[2019-04-05 12:26:56,603] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 20.0, 19.82355136640537, -0.7945369673787698, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1556400.0000, 
sim time next is 1557000.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 20.0, 19.82023877926027, -0.7943937709813241, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.16666666666666666, 0.15168656493835572, 0.2352020763395586, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.6466377], dtype=float32), 1.3814843]. 
=============================================
[2019-04-05 12:26:56,606] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[94.637665]
 [96.12527 ]
 [97.65272 ]
 [99.52787 ]
 [99.41578 ]], R is [[92.61159515]
 [92.5426178 ]
 [92.47433472]
 [92.40673065]
 [92.3398056 ]].
[2019-04-05 12:27:03,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4920491e-10 2.5788863e-04 3.9020761e-06 9.9955148e-01 1.8666832e-04
 8.0742897e-09 5.7515122e-09], sum to 1.0000
[2019-04-05 12:27:03,142] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9674
[2019-04-05 12:27:03,171] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 20.5, 20.22903729883518, -0.7428220030267555, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1386600.0000, 
sim time next is 1387200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 20.5, 20.20838439876974, -0.7547392936931008, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.20833333333333334, 0.18403203323081177, 0.24842023543563307, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.70156217], dtype=float32), -0.16369338]. 
=============================================
[2019-04-05 12:27:05,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4020992e-10 4.3763990e-05 2.6277034e-04 9.3404067e-01 6.5652788e-02
 2.1719264e-10 2.3198421e-08], sum to 1.0000
[2019-04-05 12:27:05,568] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9495
[2019-04-05 12:27:05,588] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.533333333333333, 87.0, 0.0, 0.0, 23.5, 22.22898235169028, -0.3036496156946266, 0.0, 1.0, 46153.63909290239], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 1752000.0000, 
sim time next is 1752600.0000, 
raw observation next is [-1.616666666666667, 87.0, 0.0, 0.0, 24.0, 22.22144202525086, -0.3054832023316471, 0.0, 1.0, 46136.23956848903], 
processed observation next is [0.0, 0.2608695652173913, 0.4178208679593721, 0.87, 0.0, 0.0, 0.5, 0.35178683543757155, 0.3981722658894509, 0.0, 1.0, 0.2196963788975668], 
reward next is 0.2857, 
noisyNet noise sample is [array([-0.5562267], dtype=float32), -1.1226516]. 
=============================================
[2019-04-05 12:27:11,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9270361e-04 3.9794184e-02 3.8161088e-02 7.9673541e-01 1.2444771e-01
 1.3672463e-04 5.3216942e-04], sum to 1.0000
[2019-04-05 12:27:11,426] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1812
[2019-04-05 12:27:11,432] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 43.0, 129.5, 51.0, 21.5, 23.73763282035095, -0.04337410312972994, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 2300400.0000, 
sim time next is 2301000.0000, 
raw observation next is [0.9166666666666667, 43.16666666666667, 132.3333333333333, 48.0, 21.5, 23.90555336285635, -0.02497743911210103, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.48799630655586346, 0.4316666666666667, 0.44111111111111095, 0.05303867403314917, 0.2916666666666667, 0.49212944690469573, 0.49167418696263304, 1.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.64043576], dtype=float32), -0.792333]. 
=============================================
[2019-04-05 12:27:11,439] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[36.48374 ]
 [36.44938 ]
 [36.504322]
 [36.43939 ]
 [36.47637 ]], R is [[36.79764175]
 [37.07252121]
 [37.34465408]
 [37.61406326]
 [37.95220947]].
[2019-04-05 12:27:11,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9419102e-09 1.2977134e-03 1.3147021e-03 9.8288995e-01 1.4497233e-02
 5.2227396e-08 2.1339616e-07], sum to 1.0000
[2019-04-05 12:27:11,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9158
[2019-04-05 12:27:11,558] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.5, 81.5, 0.0, 0.0, 19.0, 18.96654547584133, -1.068309542764729, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2932200.0000, 
sim time next is 2932800.0000, 
raw observation next is [-1.666666666666667, 82.66666666666667, 0.0, 0.0, 19.0, 18.85492669869112, -1.088567369163423, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4164358264081256, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.07124389155759332, 0.137144210278859, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07068022], dtype=float32), 0.70532984]. 
=============================================
[2019-04-05 12:27:19,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.3198178e-05 2.5556890e-02 1.5388867e-02 8.6348915e-01 9.3669154e-02
 1.0250432e-04 1.7001423e-03], sum to 1.0000
[2019-04-05 12:27:19,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6540
[2019-04-05 12:27:19,180] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.3, 29.0, 121.5, 338.3333333333333, 22.5, 25.76268920048767, 0.3739155956778331, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 2560800.0000, 
sim time next is 2561400.0000, 
raw observation next is [3.3, 29.0, 114.0, 351.0, 22.5, 25.73607472112212, 0.3803035418480467, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.38, 0.3878453038674033, 0.375, 0.6446728934268432, 0.6267678472826822, 1.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-1.6138382], dtype=float32), -0.47367167]. 
=============================================
[2019-04-05 12:27:19,836] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 12:27:19,845] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:27:19,845] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:27:19,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:27:19,845] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:27:19,846] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:27:19,846] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:27:19,849] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run31
[2019-04-05 12:27:19,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run31
[2019-04-05 12:27:19,876] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run31
[2019-04-05 12:27:35,109] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.0962319]
[2019-04-05 12:27:35,110] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-13.3, 58.5, 62.33333333333333, 752.3333333333333, 26.0, 25.65113238740124, 0.3237767624605297, 1.0, 1.0, 91553.02415599546]
[2019-04-05 12:27:35,110] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:27:35,111] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.2037371e-06 1.0836800e-02 7.8303870e-03 8.9775229e-01 8.3424814e-02
 9.9852841e-06 1.4060017e-04], sampled 0.9011808728937933
[2019-04-05 12:28:52,312] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.0962319]
[2019-04-05 12:28:52,312] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.25, 49.0, 161.0, 234.6666666666667, 26.0, 26.14192095161155, 0.7980208576559583, 0.0, 1.0, 0.0]
[2019-04-05 12:28:52,312] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:28:52,313] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.6244606e-13 9.5126761e-06 1.0448488e-05 9.9714869e-01 2.8313473e-03
 3.7527495e-11 1.2036896e-09], sampled 0.37862432394921897
[2019-04-05 12:28:56,506] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 858.7421 227061249.1698 1115.1768
[2019-04-05 12:29:17,989] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 579.1527 256910417.3564 1289.6724
[2019-04-05 12:29:24,224] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 298.0368 272467844.2124 1101.0762
[2019-04-05 12:29:25,248] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 600000, evaluation results [600000.0, 579.1526724727494, 256910417.35644516, 1289.6724356405066, 858.7421060050546, 227061249.16984794, 1115.1768365488608, 298.0367588985095, 272467844.2124245, 1101.076200698857]
[2019-04-05 12:29:40,708] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0896926e-09 4.0569645e-04 3.0037059e-04 9.6810728e-01 3.1185534e-02
 1.0496063e-08 1.1064241e-06], sum to 1.0000
[2019-04-05 12:29:40,708] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3673
[2019-04-05 12:29:40,722] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.833333333333334, 70.0, 0.0, 0.0, 23.0, 22.78803578470291, -0.1530479983776154, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3365400.0000, 
sim time next is 3366000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 23.0, 22.68995970681455, -0.1667862367053207, 0.0, 1.0, 88194.23700701629], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.71, 0.0, 0.0, 0.4166666666666667, 0.3908299755678793, 0.44440458776489306, 0.0, 1.0, 0.419972557176268], 
reward next is 0.4286, 
noisyNet noise sample is [array([-2.0131376], dtype=float32), 0.08492209]. 
=============================================
[2019-04-05 12:29:40,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.19456 ]
 [64.49509 ]
 [64.19072 ]
 [62.960487]
 [61.90721 ]], R is [[66.49546051]
 [66.25907898]
 [66.02506256]
 [65.79338837]
 [65.56402588]].
[2019-04-05 12:29:41,419] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1282312e-04 3.7059370e-02 4.1413795e-02 7.9913068e-01 1.1970824e-01
 3.7910024e-05 2.0372642e-03], sum to 1.0000
[2019-04-05 12:29:41,419] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6823
[2019-04-05 12:29:41,428] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.2, 79.0, 148.0, 0.0, 22.5, 23.36930405276022, -0.09636980995844012, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 2035800.0000, 
sim time next is 2036400.0000, 
raw observation next is [-4.1, 79.0, 140.6666666666667, 0.0, 22.5, 23.52502543887904, -0.08281157000495572, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3490304709141275, 0.79, 0.468888888888889, 0.0, 0.375, 0.4604187865732534, 0.47239614333168145, 1.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.5710283], dtype=float32), 0.8688553]. 
=============================================
[2019-04-05 12:30:10,431] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-05 12:30:10,432] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:30:10,432] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:30:10,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:30:10,433] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:30:10,433] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:30:10,433] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:30:10,436] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run32
[2019-04-05 12:30:10,451] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run32
[2019-04-05 12:30:10,471] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run32
[2019-04-05 12:31:49,947] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 558.3571 233236635.0770 1478.4228
[2019-04-05 12:32:09,162] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 538.1936 258130071.4356 1332.6005
[2019-04-05 12:32:13,167] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 360.3571 272716609.3018 1086.4598
[2019-04-05 12:32:14,192] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 620000, evaluation results [620000.0, 538.1935995603601, 258130071.43564317, 1332.6005052365658, 558.3571428571288, 233236635.076972, 1478.422753059108, 360.35714285714516, 272716609.3018449, 1086.4597726474228]
[2019-04-05 12:32:15,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00369848 0.14170347 0.1690114  0.26486355 0.41481146 0.00090792
 0.00500368], sum to 1.0000
[2019-04-05 12:32:15,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9571
[2019-04-05 12:32:15,092] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.33333333333333, 78.5, 103.6666666666667, 632.6666666666667, 25.5, 25.77131671348105, 0.3336887133803585, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2713800.0000, 
sim time next is 2714400.0000, 
raw observation next is [-12.0, 76.0, 107.0, 643.0, 26.0, 25.84923786238743, 0.3483401417688363, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13019390581717452, 0.76, 0.3566666666666667, 0.7104972375690608, 0.6666666666666666, 0.6541031551989525, 0.6161133805896121, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.91578776], dtype=float32), 0.5115706]. 
=============================================
[2019-04-05 12:32:15,793] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2640383e-04 1.6330991e-02 1.0685798e-02 8.4930402e-01 1.2286225e-01
 1.5476357e-05 6.7501096e-04], sum to 1.0000
[2019-04-05 12:32:15,793] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1079
[2019-04-05 12:32:15,812] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.0, 65.0, 0.0, 0.0, 26.0, 23.23576911544605, -0.1060689813179216, 0.0, 1.0, 44040.65159329279], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3994800.0000, 
sim time next is 3995400.0000, 
raw observation next is [-13.0, 64.0, 0.0, 0.0, 26.0, 23.16375674805319, -0.1142345224876396, 0.0, 1.0, 43996.27149100137], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.64, 0.0, 0.0, 0.6666666666666666, 0.43031306233776573, 0.4619218258374535, 0.0, 1.0, 0.20950605471905412], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5120442], dtype=float32), 1.0083747]. 
=============================================
[2019-04-05 12:32:24,976] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7421022e-08 5.5324362e-04 1.8628540e-03 9.7923434e-01 1.8340325e-02
 1.2973777e-07 9.0181129e-06], sum to 1.0000
[2019-04-05 12:32:24,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2511
[2019-04-05 12:32:25,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.3, 61.0, 0.0, 0.0, 24.0, 22.85593622541199, -0.1352818007808408, 0.0, 1.0, 47708.27265493719], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2590800.0000, 
sim time next is 2591400.0000, 
raw observation next is [-4.399999999999999, 61.5, 0.0, 0.0, 24.0, 22.93093004954121, -0.130810031034845, 0.0, 1.0, 47225.96334492578], 
processed observation next is [1.0, 1.0, 0.3407202216066483, 0.615, 0.0, 0.0, 0.5, 0.41091083746176754, 0.45639665632171833, 0.0, 1.0, 0.2248855397377418], 
reward next is 0.2857, 
noisyNet noise sample is [array([-1.2681038], dtype=float32), -0.8559356]. 
=============================================
[2019-04-05 12:32:35,242] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.01054038 0.39536777 0.03900313 0.24311848 0.3029179  0.00046283
 0.00858952], sum to 1.0000
[2019-04-05 12:32:35,243] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5808
[2019-04-05 12:32:35,275] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 60.0, 105.5, 727.5, 20.5, 25.66166420875484, 0.3808831883941952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3837600.0000, 
sim time next is 3838200.0000, 
raw observation next is [-1.833333333333333, 60.0, 107.0, 743.3333333333334, 20.5, 25.70298455686979, 0.3941527234347496, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.41181902123730385, 0.6, 0.3566666666666667, 0.8213627992633518, 0.20833333333333334, 0.6419153797391491, 0.6313842411449165, 1.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.55972296], dtype=float32), -0.37472838]. 
=============================================
[2019-04-05 12:32:36,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1992202e-10 1.6368568e-04 8.7898475e-04 5.7288647e-01 4.2606938e-01
 9.7475549e-08 1.3584314e-06], sum to 1.0000
[2019-04-05 12:32:36,513] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0976
[2019-04-05 12:32:36,527] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.3333333333333334, 100.0, 0.0, 0.0, 26.0, 25.14187693217456, 0.2713227794192884, 0.0, 1.0, 40850.33429687725], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3105600.0000, 
sim time next is 3106200.0000, 
raw observation next is [-0.1666666666666666, 100.0, 0.0, 0.0, 26.0, 25.17733500638239, 0.2693904694483137, 0.0, 1.0, 40707.51115366843], 
processed observation next is [0.0, 0.9565217391304348, 0.4579870729455217, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5981112505318658, 0.5897968231494378, 0.0, 1.0, 0.1938452912079449], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33759785], dtype=float32), 0.15284438]. 
=============================================
[2019-04-05 12:32:38,804] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8779756e-04 2.7796072e-01 2.5405837e-02 3.7446535e-01 3.1226760e-01
 2.3633667e-04 8.7763937e-03], sum to 1.0000
[2019-04-05 12:32:38,805] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1927
[2019-04-05 12:32:38,815] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.5, 73.0, 0.0, 0.0, 19.0, 23.52181179112946, -0.007453858053448763, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3436200.0000, 
sim time next is 3436800.0000, 
raw observation next is [1.333333333333333, 75.0, 0.0, 0.0, 19.0, 23.38771142727485, -0.01485931270675087, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4995383194829178, 0.75, 0.0, 0.0, 0.08333333333333333, 0.44897595227290427, 0.49504689576441635, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7929315], dtype=float32), 1.000419]. 
=============================================
[2019-04-05 12:32:46,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5115674e-05 4.4973906e-02 1.4322389e-02 6.5007776e-01 2.9023689e-01
 4.8457532e-06 3.6907254e-04], sum to 1.0000
[2019-04-05 12:32:46,294] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2563
[2019-04-05 12:32:46,304] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 22.5, 22.67012561114569, -0.138682677225783, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 4670400.0000, 
sim time next is 4671000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 22.5, 22.70056289668805, -0.1466539810103173, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.57, 0.0, 0.0, 0.375, 0.3917135747240043, 0.45111533966322753, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.8582351], dtype=float32), 0.7684288]. 
=============================================
[2019-04-05 12:32:46,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[36.267815]
 [37.12725 ]
 [37.13622 ]
 [36.947083]
 [37.816677]], R is [[36.14528656]
 [36.28383255]
 [36.27813721]
 [36.34392548]
 [36.48048782]].
[2019-04-05 12:32:49,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3897693e-05 3.1319058e-01 1.7403120e-01 2.3532468e-01 2.7617615e-01
 2.6494190e-05 1.1970636e-03], sum to 1.0000
[2019-04-05 12:32:49,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8222
[2019-04-05 12:32:49,716] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.5, 99.16666666666666, 66.66666666666666, 559.0, 26.0, 27.43648765068635, 0.959986010664673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3168600.0000, 
sim time next is 3169200.0000, 
raw observation next is [6.4, 99.33333333333334, 62.33333333333333, 529.0, 26.0, 27.65810675503713, 0.9852585601545926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6398891966759004, 0.9933333333333334, 0.20777777777777776, 0.5845303867403315, 0.6666666666666666, 0.8048422295864276, 0.8284195200515309, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0674872], dtype=float32), 0.044522386]. 
=============================================
[2019-04-05 12:32:51,564] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4465532e-03 2.4224378e-01 1.0961264e-01 4.4026431e-01 1.9880760e-01
 3.0638545e-04 6.3186889e-03], sum to 1.0000
[2019-04-05 12:32:51,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7445
[2019-04-05 12:32:51,580] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 44.5, 18.0, 179.0, 19.0, 25.68747117915737, 0.4319720325113512, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3864600.0000, 
sim time next is 3865200.0000, 
raw observation next is [2.333333333333333, 45.66666666666667, 15.0, 149.1666666666667, 19.0, 25.45859850641002, 0.4654155656948952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5272391505078486, 0.4566666666666667, 0.05, 0.1648250460405157, 0.08333333333333333, 0.6215498755341683, 0.6551385218982984, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.115939], dtype=float32), -0.1935236]. 
=============================================
[2019-04-05 12:32:52,116] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-05 12:32:52,117] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:32:52,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:32:52,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run33
[2019-04-05 12:32:52,135] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:32:52,136] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:32:52,137] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:32:52,138] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:32:52,142] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run33
[2019-04-05 12:32:52,157] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run33
[2019-04-05 12:34:27,192] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 1481.9582 224384565.1421 1275.0610
[2019-04-05 12:34:47,059] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 1401.7668 249848520.0404 1141.2065
[2019-04-05 12:34:49,376] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 1455.6928 260594656.4163 746.0072
[2019-04-05 12:34:50,399] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 640000, evaluation results [640000.0, 1401.7667857121157, 249848520.04044563, 1141.206476937097, 1481.958175969195, 224384565.1421468, 1275.0610458098906, 1455.6927932538367, 260594656.41629103, 746.0071774414743]
[2019-04-05 12:34:54,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8429973e-05 1.3612045e-01 4.4204108e-02 4.9284497e-01 3.2650313e-01
 1.6902619e-05 2.9195097e-04], sum to 1.0000
[2019-04-05 12:34:54,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1492
[2019-04-05 12:34:54,373] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 20.247669868582, -0.7519559427855461, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 20.17477656496478, -0.770182757068176, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.18123138041373169, 0.243272414310608, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60230947], dtype=float32), -0.3039578]. 
=============================================
[2019-04-05 12:34:59,342] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2091538e-05 2.5661539e-02 3.4683228e-02 7.8305310e-01 1.5562908e-01
 2.3043198e-05 9.0792391e-04], sum to 1.0000
[2019-04-05 12:34:59,346] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2654
[2019-04-05 12:34:59,357] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 23.5, 23.92029020533237, -0.07058833014888105, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 4945200.0000, 
sim time next is 4945800.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 23.5, 23.76525260671297, -0.0924685985451855, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.3841181902123731, 0.4933333333333334, 0.0, 0.0, 0.4583333333333333, 0.4804377172260808, 0.46917713381827153, 0.0, 1.0, 0.0], 
reward next is 0.3571, 
noisyNet noise sample is [array([0.9237345], dtype=float32), 0.590033]. 
=============================================
[2019-04-05 12:35:00,181] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3946554e-06 5.0015096e-02 9.9076694e-03 6.5615761e-01 2.8355199e-01
 6.7617607e-06 3.5834656e-04], sum to 1.0000
[2019-04-05 12:35:00,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8832
[2019-04-05 12:35:00,247] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 72.0, 107.3333333333333, 76.66666666666666, 23.5, 22.01866439809994, -0.3433490927564258, 0.0, 1.0, 96751.94714190454], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 2969400.0000, 
sim time next is 2970000.0000, 
raw observation next is [-4.0, 71.0, 119.0, 90.5, 24.0, 22.47674787377303, -0.2882327009343633, 0.0, 1.0, 111968.5852891565], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.39666666666666667, 0.1, 0.5, 0.37306232281441903, 0.4039224330218789, 0.0, 1.0, 0.5331837394721738], 
reward next is 0.2857, 
noisyNet noise sample is [array([0.0822692], dtype=float32), 0.8255544]. 
=============================================
[2019-04-05 12:35:00,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[49.559563]
 [50.04222 ]
 [49.921036]
 [49.754303]
 [49.490517]], R is [[49.67580795]
 [49.53619385]
 [49.39797592]
 [49.26113892]
 [49.12567139]].
[2019-04-05 12:35:06,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:35:06,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:35:06,590] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run6
[2019-04-05 12:35:08,034] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8348962e-07 1.1925681e-02 9.6976319e-03 8.8643372e-01 9.1829918e-02
 5.9972456e-07 1.1177501e-04], sum to 1.0000
[2019-04-05 12:35:08,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3119
[2019-04-05 12:35:08,077] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 51.66666666666666, 113.1666666666667, 815.1666666666667, 26.0, 25.03383439088152, 0.3090870611074761, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3069600.0000, 
sim time next is 3070200.0000, 
raw observation next is [-2.166666666666667, 50.83333333333334, 112.3333333333333, 813.3333333333334, 26.0, 25.03936121552048, 0.3082130336202261, 0.0, 1.0, 18728.73537486792], 
processed observation next is [0.0, 0.5217391304347826, 0.4025854108956602, 0.5083333333333334, 0.37444444444444436, 0.8987108655616943, 0.6666666666666666, 0.5866134346267066, 0.6027376778734087, 0.0, 1.0, 0.08918445416603771], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29723084], dtype=float32), -0.7225236]. 
=============================================
[2019-04-05 12:35:08,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2042615e-07 1.6412286e-03 1.2652918e-03 1.8935546e-01 8.0769432e-01
 1.4720247e-07 4.3400185e-05], sum to 1.0000
[2019-04-05 12:35:08,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7769
[2019-04-05 12:35:08,778] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 25.0, 24.09010016334575, 0.00391417294087799, 0.0, 1.0, 40792.49977672808], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 4227600.0000, 
sim time next is 4228200.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 25.5, 24.0895886840906, 0.006891129207835245, 0.0, 1.0, 40736.63771173683], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.625, 0.5074657236742167, 0.5022970430692785, 0.0, 1.0, 0.19398398910350872], 
reward next is 0.0714, 
noisyNet noise sample is [array([-1.4815563], dtype=float32), -0.78387284]. 
=============================================
[2019-04-05 12:35:12,415] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9414081e-07 7.5308003e-02 9.4517004e-03 7.8508145e-01 1.2989652e-01
 1.9930373e-07 2.6115676e-04], sum to 1.0000
[2019-04-05 12:35:12,417] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7670
[2019-04-05 12:35:12,437] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 23.0, 23.28586639041773, -0.1208050183238093, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 3611400.0000, 
sim time next is 3612000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 23.0, 23.28711914435836, -0.1325291663258406, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.4166666666666667, 0.4405932620298634, 0.4558236112247198, 0.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.10941905], dtype=float32), 0.16763504]. 
=============================================
[2019-04-05 12:35:12,458] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[54.177406]
 [54.32987 ]
 [54.422913]
 [54.060257]
 [53.276478]], R is [[54.1360054 ]
 [54.02321625]
 [53.76869965]
 [53.51672745]
 [53.26727295]].
[2019-04-05 12:35:12,732] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 652272: loss 43.6439
[2019-04-05 12:35:12,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 652272: learning rate 0.0000
[2019-04-05 12:35:12,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1654721e-07 9.1437753e-03 8.0836564e-04 9.0164232e-01 8.8390574e-02
 1.5203932e-08 1.4833484e-05], sum to 1.0000
[2019-04-05 12:35:12,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5468
[2019-04-05 12:35:12,820] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.533333333333333, 94.0, 0.0, 0.0, 22.5, 20.722421205344, -0.7005261142014602, 0.0, 1.0, 41883.21035799998], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 13200.0000, 
sim time next is 13800.0000, 
raw observation next is [7.616666666666667, 93.5, 0.0, 0.0, 22.5, 20.76182176343606, -0.6935039761262057, 0.0, 1.0, 41718.93408562891], 
processed observation next is [0.0, 0.13043478260869565, 0.6735918744228995, 0.935, 0.0, 0.0, 0.375, 0.2301518136196717, 0.26883200795793144, 0.0, 1.0, 0.1986615908839472], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.12734629], dtype=float32), 0.8331787]. 
=============================================
[2019-04-05 12:35:16,664] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4748915e-03 3.5677242e-01 2.3293333e-02 1.3169891e-01 4.5352623e-01
 4.4106631e-04 2.8793074e-02], sum to 1.0000
[2019-04-05 12:35:16,665] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3048
[2019-04-05 12:35:16,692] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.833333333333333, 28.5, 115.3333333333333, 833.6666666666666, 20.0, 22.49342769275869, -0.3236808004677181, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4021800.0000, 
sim time next is 4022400.0000, 
raw observation next is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 19.0, 22.5511671785538, -0.3917671404661305, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3610341643582641, 0.28, 0.38222222222222235, 0.9191528545119706, 0.08333333333333333, 0.37926393154615007, 0.3694109531779565, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3198171], dtype=float32), 0.17492063]. 
=============================================
[2019-04-05 12:35:19,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0532865e-03 2.4035254e-01 6.0773626e-02 4.3717262e-01 2.5833029e-01
 6.5754801e-05 2.2519196e-03], sum to 1.0000
[2019-04-05 12:35:19,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8522
[2019-04-05 12:35:20,004] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 19.0, 22.0262623706569, -0.3753645981676807, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3348000.0000, 
sim time next is 3348600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 19.5, 22.06632668827271, -0.3691516163666773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.125, 0.3388605573560592, 0.37694946121110756, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.37045324], dtype=float32), -1.8512672]. 
=============================================
[2019-04-05 12:35:23,050] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6722445e-04 6.2957585e-01 9.5338032e-02 1.4741078e-01 1.2487543e-01
 4.5378783e-05 2.3873111e-03], sum to 1.0000
[2019-04-05 12:35:23,051] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4087
[2019-04-05 12:35:23,073] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 72.0, 139.1666666666667, 1.833333333333333, 19.0, 22.79811807389436, -0.3692975848726773, 1.0, 1.0, 11675.25729389408], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4722000.0000, 
sim time next is 4722600.0000, 
raw observation next is [1.0, 72.0, 131.3333333333333, 3.666666666666666, 19.0, 22.51846182111248, -0.3293409844597129, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.4377777777777776, 0.004051565377532228, 0.08333333333333333, 0.37653848509270677, 0.3902196718467624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([3.4746726], dtype=float32), -0.10919365]. 
=============================================
[2019-04-05 12:35:23,216] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7308039e-06 5.7178095e-02 6.9607268e-03 8.6956665e-02 8.4873694e-01
 3.5035512e-07 1.6551555e-04], sum to 1.0000
[2019-04-05 12:35:23,220] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1684
[2019-04-05 12:35:23,236] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 24.53889790089793, 0.1563209767846462, 0.0, 1.0, 84181.39381782837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 24.51209607625145, 0.1581572267167008, 0.0, 1.0, 57256.65451848639], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5426746730209541, 0.5527190755722337, 0.0, 1.0, 0.27265073580231614], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.85537237], dtype=float32), 0.9090631]. 
=============================================
[2019-04-05 12:35:25,156] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5169328e-04 7.7121449e-01 7.3101562e-03 1.9659030e-01 2.3637181e-02
 4.1346244e-05 1.0547882e-03], sum to 1.0000
[2019-04-05 12:35:25,160] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5339
[2019-04-05 12:35:25,184] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 34.5, 108.0, 717.0, 19.5, 24.06626290559078, -0.01335788549627929, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4959000.0000, 
sim time next is 4959600.0000, 
raw observation next is [0.3333333333333333, 33.0, 109.5, 731.3333333333333, 19.0, 24.17391962260113, 0.01046668453922329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.33, 0.365, 0.8081031307550643, 0.08333333333333333, 0.5144933018834275, 0.5034888948464078, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24924432], dtype=float32), -1.7743859]. 
=============================================
[2019-04-05 12:35:26,114] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 12:35:26,116] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:35:26,117] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:35:26,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:35:26,117] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:35:26,118] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:35:26,119] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:35:26,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run34
[2019-04-05 12:35:26,155] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run34
[2019-04-05 12:35:26,171] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run34
[2019-04-05 12:35:57,765] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09417379]
[2019-04-05 12:35:57,765] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.616666666666667, 92.0, 0.0, 0.0, 26.0, 24.87883318202933, 0.3934991391879117, 0.0, 1.0, 41063.94644247002]
[2019-04-05 12:35:57,765] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:35:57,766] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.3298359e-08 3.2888494e-02 9.1303457e-03 7.8243554e-01 1.7552722e-01
 5.7402978e-08 1.8313942e-05], sampled 0.47837585577938113
[2019-04-05 12:36:11,603] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09417379]
[2019-04-05 12:36:11,603] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.1, 86.5, 29.0, 0.0, 25.5, 24.35705255970839, 0.01444843274787928, 1.0, 1.0, 0.0]
[2019-04-05 12:36:11,603] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:36:11,603] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.9524010e-05 1.4013545e-01 2.9618563e-02 6.0414690e-01 2.2551079e-01
 7.1885947e-06 5.6161836e-04], sampled 0.3903469867877518
[2019-04-05 12:36:38,600] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09417379]
[2019-04-05 12:36:38,601] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 19.0, 25.13448847665641, 0.2046402226672482, 1.0, 1.0, 0.0]
[2019-04-05 12:36:38,601] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:36:38,602] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.8553385e-05 3.2984442e-01 3.1793106e-02 3.7301847e-01 2.6392964e-01
 1.4417395e-05 1.3214295e-03], sampled 0.32086717724963754
[2019-04-05 12:36:53,693] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 4608.8803 185038426.3236 -16.2827
[2019-04-05 12:37:08,404] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4397.8872 214602686.9867 -255.1326
[2019-04-05 12:37:12,739] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4199.8382 223633766.0864 -483.1232
[2019-04-05 12:37:13,763] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 660000, evaluation results [660000.0, 4397.887235325599, 214602686.98666796, -255.13255666426832, 4608.880343084285, 185038426.32358238, -16.28270556280718, 4199.838180962708, 223633766.08641988, -483.1232401721581]
[2019-04-05 12:37:14,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2127713e-06 2.1330841e-02 1.1343920e-01 8.0129278e-01 6.3757993e-02
 1.1412905e-05 1.6564249e-04], sum to 1.0000
[2019-04-05 12:37:14,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2556
[2019-04-05 12:37:14,543] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.1666666666666667, 93.33333333333334, 0.0, 0.0, 19.0, 20.92978189131038, -0.6262805565982601, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4680600.0000, 
sim time next is 4681200.0000, 
raw observation next is [-0.3333333333333333, 94.66666666666667, 0.0, 0.0, 19.0, 20.88577618737873, -0.6440149199195465, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4533702677747, 0.9466666666666668, 0.0, 0.0, 0.08333333333333333, 0.2404813489482276, 0.28532836002681783, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25649413], dtype=float32), 0.5062634]. 
=============================================
[2019-04-05 12:37:16,647] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.66262953e-06 2.76589990e-02 1.08257765e-02 8.54959369e-01
 1.06349371e-01 3.41107489e-06 2.01442192e-04], sum to 1.0000
[2019-04-05 12:37:16,648] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2634
[2019-04-05 12:37:16,660] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 20.5, 20.04633251006371, -0.7636846554828906, 0.0, 1.0, 18746.07160148704], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4764600.0000, 
sim time next is 4765200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 20.5, 20.10929689423016, -0.764065586519176, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.20833333333333334, 0.17577474118584657, 0.24531147116027466, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.7775824], dtype=float32), 1.3292066]. 
=============================================
[2019-04-05 12:37:17,185] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0571991e-05 4.3986088e-01 7.4149966e-03 5.1719946e-01 3.5295460e-02
 1.5433103e-05 1.7320582e-04], sum to 1.0000
[2019-04-05 12:37:17,187] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9626
[2019-04-05 12:37:17,199] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 38.66666666666667, 0.0, 0.0, 19.5, 20.19243011443485, -0.9042836938209512, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4087200.0000, 
sim time next is 4087800.0000, 
raw observation next is [-4.5, 37.5, 0.0, 0.0, 19.5, 20.17195643135815, -0.9155633343567103, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3379501385041552, 0.375, 0.0, 0.0, 0.125, 0.1809963692798459, 0.19481222188109656, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.14091], dtype=float32), 1.497579]. 
=============================================
[2019-04-05 12:37:19,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:19,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:19,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run6
[2019-04-05 12:37:19,254] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 663114: loss 15.2830
[2019-04-05 12:37:19,256] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 663115: learning rate 0.0000
[2019-04-05 12:37:24,135] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1128843e-06 5.1501255e-02 8.1233373e-03 3.8834137e-01 5.5171382e-01
 1.0119538e-05 3.0900163e-04], sum to 1.0000
[2019-04-05 12:37:24,136] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5008
[2019-04-05 12:37:24,150] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.5, 22.57391697664665, -0.3506315020208093, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4947600.0000, 
sim time next is 4948200.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 20.0, 22.457374267427, -0.3695892365976314, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.16666666666666666, 0.37144785561891663, 0.37680358780078954, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.1183441], dtype=float32), -2.050879]. 
=============================================
[2019-04-05 12:37:25,195] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 666258: loss 98.3518
[2019-04-05 12:37:25,199] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 666258: learning rate 0.0000
[2019-04-05 12:37:27,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3574032e-07 5.1967643e-02 5.0241154e-02 3.8117331e-01 5.1661646e-01
 3.3274307e-07 9.2755528e-07], sum to 1.0000
[2019-04-05 12:37:27,081] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8404
[2019-04-05 12:37:27,132] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 44.5, 122.0, 839.0, 26.0, 24.72177789917646, 0.1718202976740736, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4793400.0000, 
sim time next is 4794000.0000, 
raw observation next is [-1.110223024625157e-16, 44.0, 130.0, 830.3333333333334, 26.0, 24.71645229745742, 0.1710402379354207, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.46260387811634357, 0.44, 0.43333333333333335, 0.9174953959484347, 0.6666666666666666, 0.5597043581214516, 0.5570134126451403, 0.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0518978], dtype=float32), -0.74330586]. 
=============================================
[2019-04-05 12:37:27,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[56.78659 ]
 [56.670013]
 [56.600655]
 [56.363426]
 [56.35087 ]], R is [[56.62019348]
 [56.05399323]
 [55.49345398]
 [54.93851852]
 [54.46056366]].
[2019-04-05 12:37:28,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:28,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:29,030] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run6
[2019-04-05 12:37:30,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:30,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:30,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run6
[2019-04-05 12:37:31,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7431741e-05 1.8299556e-01 1.0551319e-02 3.9607152e-01 4.1017079e-01
 6.7381154e-07 1.9266245e-04], sum to 1.0000
[2019-04-05 12:37:31,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9149
[2019-04-05 12:37:31,682] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.55, 30.0, 115.0, 842.0, 20.0, 25.41330324960929, 0.4327480813298781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4368600.0000, 
sim time next is 4369200.0000, 
raw observation next is [14.53333333333333, 30.33333333333333, 128.3333333333333, 806.5, 20.5, 25.75351368408963, 0.4672176111102939, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8651892890120036, 0.3033333333333333, 0.42777777777777765, 0.8911602209944751, 0.20833333333333334, 0.6461261403408024, 0.6557392037034313, 1.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.2916695], dtype=float32), -0.3415219]. 
=============================================
[2019-04-05 12:37:33,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:33,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:33,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run6
[2019-04-05 12:37:34,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:34,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:34,318] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run6
[2019-04-05 12:37:34,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:34,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:34,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run6
[2019-04-05 12:37:35,058] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 671736: loss 104.9624
[2019-04-05 12:37:35,060] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 671737: learning rate 0.0000
[2019-04-05 12:37:36,791] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 672556: loss 74.8187
[2019-04-05 12:37:36,792] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 672556: learning rate 0.0000
[2019-04-05 12:37:36,921] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 672611: loss -4.0972
[2019-04-05 12:37:36,923] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 672612: learning rate 0.0000
[2019-04-05 12:37:39,144] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0744692e-07 9.9836417e-02 6.1564944e-03 8.5931474e-01 3.4639038e-02
 8.0726096e-08 5.2835829e-05], sum to 1.0000
[2019-04-05 12:37:39,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5269
[2019-04-05 12:37:39,158] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.2, 30.0, 119.6666666666667, 832.1666666666666, 19.5, 25.87706585514147, 0.4551336479486383, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4362000.0000, 
sim time next is 4362600.0000, 
raw observation next is [14.6, 29.0, 119.3333333333333, 836.3333333333334, 19.5, 25.98221246891828, 0.4793817565382789, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8670360110803325, 0.29, 0.3977777777777777, 0.9241252302025783, 0.125, 0.6651843724098567, 0.659793918846093, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.0350281], dtype=float32), 1.0233165]. 
=============================================
[2019-04-05 12:37:39,824] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 673901: loss 87.9009
[2019-04-05 12:37:39,827] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 673901: learning rate 0.0000
[2019-04-05 12:37:40,639] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:40,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:40,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run6
[2019-04-05 12:37:40,818] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 674502: loss 121.2764
[2019-04-05 12:37:40,819] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 674502: learning rate 0.0000
[2019-04-05 12:37:41,206] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 674720: loss 103.7312
[2019-04-05 12:37:41,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 674720: learning rate 0.0000
[2019-04-05 12:37:41,294] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 674779: loss -2.5202
[2019-04-05 12:37:41,295] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 674780: learning rate 0.0000
[2019-04-05 12:37:43,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.76289616e-09 3.86166833e-02 1.65839475e-02 1.01919994e-01
 8.42879295e-01 2.88266810e-09 1.44534312e-07], sum to 1.0000
[2019-04-05 12:37:43,738] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5299
[2019-04-05 12:37:43,820] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.8, 83.33333333333334, 44.66666666666667, 0.0, 23.0, 21.23361312423902, -0.4766666698315348, 0.0, 1.0, 198800.0060598762], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 56400.0000, 
sim time next is 57000.0000, 
raw observation next is [6.699999999999999, 82.66666666666667, 39.33333333333334, 0.0, 23.5, 21.43916387796835, -0.3848942324170093, 0.0, 1.0, 200340.3651139466], 
processed observation next is [0.0, 0.6521739130434783, 0.6481994459833795, 0.8266666666666667, 0.13111111111111115, 0.0, 0.4583333333333333, 0.2865969898306959, 0.3717019225276636, 0.0, 1.0, 0.954001738637841], 
reward next is 0.3571, 
noisyNet noise sample is [array([-0.9556064], dtype=float32), 1.1303893]. 
=============================================
[2019-04-05 12:37:43,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.88775]
 [73.90603]
 [73.78585]
 [73.20408]
 [73.34886]], R is [[74.78894043]
 [74.46962738]
 [74.22492981]
 [73.98268127]
 [73.81428528]].
[2019-04-05 12:37:45,101] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6782506e-06 8.8165872e-02 4.8442166e-03 4.0143964e-01 5.0545555e-01
 1.3040262e-06 8.9651861e-05], sum to 1.0000
[2019-04-05 12:37:45,102] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9733
[2019-04-05 12:37:45,118] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.2, 81.66666666666667, 0.0, 0.0, 20.5, 21.05526237893878, -0.6317494958300717, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 99600.0000, 
sim time next is 100200.0000, 
raw observation next is [-3.3, 80.33333333333334, 0.0, 0.0, 21.0, 20.89611715792911, -0.6596107909173908, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.8033333333333335, 0.0, 0.0, 0.25, 0.24134309649409236, 0.28012973636086974, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.44950953], dtype=float32), -1.8722132]. 
=============================================
[2019-04-05 12:37:45,289] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8481204e-07 1.0039399e-01 1.2545856e-02 3.7971854e-01 5.0732017e-01
 1.7364201e-07 2.1132482e-05], sum to 1.0000
[2019-04-05 12:37:45,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4400
[2019-04-05 12:37:45,304] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 79.66666666666667, 0.0, 0.0, 21.0, 20.56086858799479, -0.6987208227697947, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 4753200.0000, 
sim time next is 4753800.0000, 
raw observation next is [-4.0, 77.5, 0.0, 0.0, 21.0, 20.59904348626518, -0.7061499551822016, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.775, 0.0, 0.0, 0.25, 0.21658695718876508, 0.2646166816059328, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([1.2698725], dtype=float32), -0.8523475]. 
=============================================
[2019-04-05 12:37:46,885] A3C_AGENT_WORKER-Thread-12 INFO:Local step 42500, global step 677673: loss 85.3868
[2019-04-05 12:37:46,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 42500, global step 677673: learning rate 0.0000
[2019-04-05 12:37:48,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3759789e-04 2.6578128e-01 1.5979474e-02 4.8912081e-01 2.2575526e-01
 6.6463661e-05 3.1591007e-03], sum to 1.0000
[2019-04-05 12:37:48,189] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9030
[2019-04-05 12:37:48,202] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 19.0, 21.07136765767616, -0.689773297989812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [3.0, 41.0, 114.0, 753.5, 19.5, 21.17759198067373, -0.6560353511473991, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5457063711911359, 0.41, 0.38, 0.8325966850828729, 0.125, 0.2647993317228107, 0.28132154961753364, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.65417016], dtype=float32), 0.20187911]. 
=============================================
[2019-04-05 12:37:50,148] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2306234e-06 1.4576735e-01 9.8676514e-03 7.3721707e-01 1.0714253e-01
 1.1241778e-07 4.0219693e-06], sum to 1.0000
[2019-04-05 12:37:50,149] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0704
[2019-04-05 12:37:50,188] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 60.0, 253.5, 171.5, 19.5, 22.49257978640208, -0.365667388647993, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4874400.0000, 
sim time next is 4875000.0000, 
raw observation next is [-1.733333333333333, 58.66666666666666, 269.0, 169.0, 19.0, 22.44051046611207, -0.3697225774202549, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.41458910433979695, 0.5866666666666666, 0.8966666666666666, 0.1867403314917127, 0.08333333333333333, 0.37004253884267246, 0.3767591408599151, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30423546], dtype=float32), -0.13836385]. 
=============================================
[2019-04-05 12:37:50,193] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[56.604885]
 [56.498997]
 [56.758934]
 [56.52908 ]
 [56.776516]], R is [[56.82921982]
 [57.1894989 ]
 [57.4033165 ]
 [57.61499786]
 [57.82456207]].
[2019-04-05 12:37:50,950] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-05 12:37:50,951] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:37:50,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:50,955] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:37:50,956] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:37:50,956] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:50,957] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:50,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run35
[2019-04-05 12:37:50,975] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run35
[2019-04-05 12:37:50,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:37:50,988] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run35
[2019-04-05 12:37:50,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:37:51,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run6
[2019-04-05 12:38:31,942] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09950601]
[2019-04-05 12:38:31,942] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.48333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 25.95160284371896, 0.6679289810857999, 0.0, 1.0, 0.0]
[2019-04-05 12:38:31,942] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:38:31,943] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3198008e-09 3.9350141e-02 3.5139557e-03 7.9397684e-01 1.6315766e-01
 1.9964368e-09 1.4543592e-06], sampled 0.02593409986376094
[2019-04-05 12:38:38,320] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09950601]
[2019-04-05 12:38:38,320] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.833333333333334, 67.0, 92.5, 0.0, 19.5, 23.01596796254406, -0.2589468693487053, 1.0, 1.0, 0.0]
[2019-04-05 12:38:38,320] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:38:38,321] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.7943541e-05 3.7499341e-01 2.3674501e-02 3.6969689e-01 2.3112901e-01
 4.6472746e-06 4.8364748e-04], sampled 0.2710243142269686
[2019-04-05 12:38:55,093] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09950601]
[2019-04-05 12:38:55,093] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.833333333333333, 78.16666666666667, 0.0, 0.0, 25.5, 20.9225588316184, -0.6470583540984622, 0.0, 1.0, 45133.24770210045]
[2019-04-05 12:38:55,093] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:38:55,094] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.4801137e-07 6.6743113e-02 1.4819698e-02 6.1643958e-01 3.0196801e-01
 1.5685234e-07 2.9245781e-05], sampled 0.37115686006501947
[2019-04-05 12:39:21,898] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 3891.7680 194468166.3090 496.9551
[2019-04-05 12:39:25,787] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09950601]
[2019-04-05 12:39:25,788] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.4533243903333334, 79.69153279333334, 14.65107441166666, 0.0, 22.0, 25.36360438432548, 0.3291174444059202, 1.0, 1.0, 0.0]
[2019-04-05 12:39:25,788] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:39:25,789] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.8518123e-07 2.5967687e-01 1.2624021e-02 4.6617141e-01 2.6146132e-01
 1.7751823e-07 6.5646040e-05], sampled 0.5750884489168323
[2019-04-05 12:39:32,632] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4054.9483 218870797.9659 -131.1622
[2019-04-05 12:39:35,679] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4293.5471 225627991.4461 -308.2990
[2019-04-05 12:39:36,703] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 680000, evaluation results [680000.0, 4054.9482612242705, 218870797.9659386, -131.16217997380627, 3891.7680235481994, 194468166.30897948, 496.95512451842023, 4293.547141587544, 225627991.44611666, -308.29900645442973]
[2019-04-05 12:39:37,197] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 680227: loss 101.1210
[2019-04-05 12:39:37,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 680227: learning rate 0.0000
[2019-04-05 12:39:37,598] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 680431: loss -1.5726
[2019-04-05 12:39:37,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 680431: learning rate 0.0000
[2019-04-05 12:39:38,152] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 680742: loss -4.0398
[2019-04-05 12:39:38,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 680742: learning rate 0.0000
[2019-04-05 12:39:38,527] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2105219e-06 6.5233767e-01 9.1198524e-03 1.6912195e-01 1.6934791e-01
 7.2767023e-08 7.0369366e-05], sum to 1.0000
[2019-04-05 12:39:38,532] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4715
[2019-04-05 12:39:38,532] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 680969: loss -1.3682
[2019-04-05 12:39:38,534] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 680969: learning rate 0.0000
[2019-04-05 12:39:38,547] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.33333333333333, 18.33333333333333, 0.0, 0.0, 20.0, 26.23474039525358, 0.6217739632501754, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5082000.0000, 
sim time next is 5082600.0000, 
raw observation next is [10.16666666666667, 18.66666666666667, 0.0, 0.0, 19.0, 26.13456597656986, 0.6050860892892138, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7442289935364729, 0.1866666666666667, 0.0, 0.0, 0.08333333333333333, 0.6778804980474883, 0.7016953630964046, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18584165], dtype=float32), -0.3863991]. 
=============================================
[2019-04-05 12:39:40,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:39:40,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:39:40,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run6
[2019-04-05 12:39:41,686] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 682619: loss -3.2110
[2019-04-05 12:39:41,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 682619: learning rate 0.0000
[2019-04-05 12:39:42,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.98312135e-06 3.82775813e-01 7.33847730e-03 3.36247794e-02
 5.76137781e-01 1.93817004e-06 1.15281015e-04], sum to 1.0000
[2019-04-05 12:39:42,880] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9290
[2019-04-05 12:39:42,892] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 19.5, 20.88662201952207, -0.6842476083941151, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 180000.0000, 
sim time next is 180600.0000, 
raw observation next is [-8.9, 74.66666666666667, 0.0, 0.0, 20.0, 20.84431743865703, -0.702813828081332, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.7466666666666667, 0.0, 0.0, 0.16666666666666666, 0.2370264532214191, 0.2657287239728893, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.7236021], dtype=float32), 0.78316337]. 
=============================================
[2019-04-05 12:39:43,370] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 683444: loss 0.1309
[2019-04-05 12:39:43,371] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 683445: learning rate 0.0000
[2019-04-05 12:39:43,710] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 683633: loss -2.6022
[2019-04-05 12:39:43,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 683633: learning rate 0.0000
[2019-04-05 12:39:45,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:39:45,022] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:39:45,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run6
[2019-04-05 12:39:45,503] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 684490: loss 2.8219
[2019-04-05 12:39:45,508] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 684491: learning rate 0.0000
[2019-04-05 12:39:46,322] A3C_AGENT_WORKER-Thread-13 INFO:Local step 42500, global step 684882: loss 84.8372
[2019-04-05 12:39:46,322] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 42500, global step 684882: learning rate 0.0000
[2019-04-05 12:39:48,142] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8107726e-06 2.4818675e-01 7.9369403e-02 3.8465843e-01 2.8707656e-01
 4.4997199e-07 7.0652342e-04], sum to 1.0000
[2019-04-05 12:39:48,143] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4523
[2019-04-05 12:39:48,170] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 19.5, 21.09402150311125, -0.6339514774205982, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4944000.0000, 
sim time next is 4944600.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 19.5, 21.08827650504301, -0.6510589423411667, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.39335180055401664, 0.48, 0.0, 0.0, 0.125, 0.25735637542025075, 0.28298035255294446, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.0440416], dtype=float32), -0.4028899]. 
=============================================
[2019-04-05 12:39:48,347] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2121574e-04 4.5155191e-01 5.7211027e-02 3.1477052e-01 1.7443602e-01
 1.8913599e-05 1.8904005e-03], sum to 1.0000
[2019-04-05 12:39:48,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4950
[2019-04-05 12:39:48,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.916666666666668, 41.66666666666666, 0.0, 0.0, 19.0, 18.60337801018628, -1.109827501234832, 1.0, 1.0, 167652.8355592328], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 417000.0000, 
sim time next is 417600.0000, 
raw observation next is [-10.0, 42.0, 0.0, 0.0, 19.0, 19.03967233520078, -1.07642987489563, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.18559556786703602, 0.42, 0.0, 0.0, 0.08333333333333333, 0.0866393612667317, 0.14119004170145666, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.81784433], dtype=float32), 1.6824228]. 
=============================================
[2019-04-05 12:39:48,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:39:48,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:39:48,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run6
[2019-04-05 12:39:48,609] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 686008: loss 45.0441
[2019-04-05 12:39:48,612] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 686008: learning rate 0.0000
[2019-04-05 12:39:50,190] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43000, global step 686751: loss -2.2603
[2019-04-05 12:39:50,191] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43000, global step 686752: learning rate 0.0000
[2019-04-05 12:39:50,846] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3063352e-09 7.1647614e-02 3.5062328e-03 9.0830261e-01 1.6542437e-02
 2.8908917e-09 1.0937385e-06], sum to 1.0000
[2019-04-05 12:39:50,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0595
[2019-04-05 12:39:50,869] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.133333333333333, 87.33333333333334, 0.0, 0.0, 20.0, 20.27173434115808, -0.9041619039551193, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 526800.0000, 
sim time next is 527400.0000, 
raw observation next is [4.05, 87.0, 0.0, 0.0, 19.0, 20.19365325675146, -0.918610498120441, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.5747922437673131, 0.87, 0.0, 0.0, 0.08333333333333333, 0.1828044380626217, 0.19379650062651965, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08376314], dtype=float32), 1.2985698]. 
=============================================
[2019-04-05 12:39:51,295] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 687346: loss 72.0388
[2019-04-05 12:39:51,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 687347: learning rate 0.0000
[2019-04-05 12:39:53,144] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:39:53,144] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:39:53,178] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run6
[2019-04-05 12:39:53,562] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 688419: loss -3.5193
[2019-04-05 12:39:53,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 688419: learning rate 0.0000
[2019-04-05 12:39:54,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:39:54,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:39:54,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run6
[2019-04-05 12:39:54,584] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5741809e-05 7.8145817e-02 2.7000165e-02 3.1328475e-01 5.8094120e-01
 1.0602572e-05 6.0175284e-04], sum to 1.0000
[2019-04-05 12:39:54,585] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5345
[2019-04-05 12:39:54,617] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.383333333333333, 74.83333333333333, 0.0, 0.0, 20.0, 19.15310001612423, -1.073592764696478, 0.0, 1.0, 198037.5344657108], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 791400.0000, 
sim time next is 792000.0000, 
raw observation next is [-7.3, 75.0, 0.0, 0.0, 20.5, 19.13553636729296, -1.056062537637574, 0.0, 1.0, 199390.7497406087], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.75, 0.0, 0.0, 0.20833333333333334, 0.09462803060774672, 0.14797915412080864, 0.0, 1.0, 0.9494797606695653], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.5834147], dtype=float32), -0.4091025]. 
=============================================
[2019-04-05 12:39:54,628] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[42.457592]
 [41.67741 ]
 [41.671368]
 [41.738415]
 [41.815205]], R is [[43.37005615]
 [43.79349899]
 [44.28413391]
 [44.84129333]
 [45.39287949]].
[2019-04-05 12:39:54,657] A3C_AGENT_WORKER-Thread-11 INFO:Local step 42500, global step 688885: loss 68.3786
[2019-04-05 12:39:54,658] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 42500, global step 688885: learning rate 0.0000
[2019-04-05 12:39:55,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:39:55,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:39:55,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run6
[2019-04-05 12:39:56,876] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 689610: loss 2.0648
[2019-04-05 12:39:56,877] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 689610: learning rate 0.0000
[2019-04-05 12:39:57,458] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 689813: loss 1.4054
[2019-04-05 12:39:57,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 689813: learning rate 0.0000
[2019-04-05 12:39:59,746] A3C_AGENT_WORKER-Thread-9 INFO:Local step 42500, global step 690592: loss 113.2044
[2019-04-05 12:39:59,750] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 42500, global step 690595: learning rate 0.0000
[2019-04-05 12:40:00,559] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 690911: loss 9.8955
[2019-04-05 12:40:00,613] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 690919: learning rate 0.0000
[2019-04-05 12:40:00,704] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 690958: loss -5.1710
[2019-04-05 12:40:00,705] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 690958: learning rate 0.0000
[2019-04-05 12:40:01,120] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 691147: loss -1.5757
[2019-04-05 12:40:01,122] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 691149: learning rate 0.0000
[2019-04-05 12:40:01,389] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 691297: loss 104.6456
[2019-04-05 12:40:01,392] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 691297: learning rate 0.0000
[2019-04-05 12:40:01,545] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 691380: loss -7.2287
[2019-04-05 12:40:01,547] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 691382: learning rate 0.0000
[2019-04-05 12:40:01,762] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 691503: loss 73.1179
[2019-04-05 12:40:01,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 691503: learning rate 0.0000
[2019-04-05 12:40:01,969] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43000, global step 691610: loss -2.7005
[2019-04-05 12:40:01,972] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43000, global step 691611: learning rate 0.0000
[2019-04-05 12:40:02,566] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 691941: loss -11.6921
[2019-04-05 12:40:02,567] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 691941: learning rate 0.0000
[2019-04-05 12:40:09,019] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43500, global step 695162: loss 1.9537
[2019-04-05 12:40:09,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43500, global step 695163: learning rate 0.0000
[2019-04-05 12:40:10,060] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 695779: loss 4.5186
[2019-04-05 12:40:10,061] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 695780: learning rate 0.0000
[2019-04-05 12:40:10,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:40:10,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:40:10,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run6
[2019-04-05 12:40:11,218] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 696378: loss 13.5331
[2019-04-05 12:40:11,220] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 696378: learning rate 0.0000
[2019-04-05 12:40:11,313] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 696422: loss 18.1820
[2019-04-05 12:40:11,319] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 696422: learning rate 0.0000
[2019-04-05 12:40:11,552] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 696520: loss -1.2918
[2019-04-05 12:40:11,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 696520: learning rate 0.0000
[2019-04-05 12:40:12,170] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43000, global step 696827: loss -3.4462
[2019-04-05 12:40:12,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43000, global step 696828: learning rate 0.0000
[2019-04-05 12:40:12,750] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 697133: loss 0.7364
[2019-04-05 12:40:12,753] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 697135: learning rate 0.0000
[2019-04-05 12:40:14,708] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45500, global step 697998: loss -1.6961
[2019-04-05 12:40:14,710] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45500, global step 698000: learning rate 0.0000
[2019-04-05 12:40:17,229] A3C_AGENT_WORKER-Thread-9 INFO:Local step 43000, global step 699195: loss -4.6204
[2019-04-05 12:40:17,232] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 43000, global step 699196: learning rate 0.0000
[2019-04-05 12:40:17,370] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 699275: loss 52.0986
[2019-04-05 12:40:17,373] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 699276: learning rate 0.0000
[2019-04-05 12:40:17,497] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.8213263e-05 5.8881319e-01 3.8997125e-02 4.2136882e-02 3.2585397e-01
 1.9390645e-05 4.1012452e-03], sum to 1.0000
[2019-04-05 12:40:17,499] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7076
[2019-04-05 12:40:17,542] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 82.66666666666667, 52.83333333333333, 0.0, 20.5, 21.41350630588411, -0.7024346357933441, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 898800.0000, 
sim time next is 899400.0000, 
raw observation next is [1.1, 83.33333333333333, 57.66666666666666, 0.0, 21.0, 21.38980479251374, -0.6985320610414191, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.8333333333333333, 0.19222222222222218, 0.0, 0.25, 0.2824837327094783, 0.2671559796528603, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.495574], dtype=float32), 0.9207349]. 
=============================================
[2019-04-05 12:40:17,707] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 699438: loss -3.5666
[2019-04-05 12:40:17,714] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 699439: learning rate 0.0000
[2019-04-05 12:40:17,721] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 699443: loss 19.9402
[2019-04-05 12:40:17,722] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 699444: learning rate 0.0000
[2019-04-05 12:40:18,024] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 699607: loss -2.0234
[2019-04-05 12:40:18,025] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 699607: learning rate 0.0000
[2019-04-05 12:40:18,757] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 12:40:18,762] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:40:18,763] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:40:18,764] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:40:18,763] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:40:18,765] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:40:18,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:40:18,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run36
[2019-04-05 12:40:18,773] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run36
[2019-04-05 12:40:18,800] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run36
[2019-04-05 12:41:04,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.099212974]
[2019-04-05 12:41:04,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.900000000000002, 82.0, 0.0, 0.0, 26.0, 23.4316646659933, -0.1491609935992091, 0.0, 1.0, 45263.79086254074]
[2019-04-05 12:41:04,690] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:41:04,690] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.3390745e-07 1.0241604e-01 3.2380022e-02 4.0878448e-01 4.5630613e-01
 4.9976194e-07 1.1198798e-04], sampled 0.1838103595129269
[2019-04-05 12:41:48,866] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 3418.2970 203258444.3764 677.6960
[2019-04-05 12:42:03,986] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.099212974]
[2019-04-05 12:42:03,986] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.1666666666666667, 44.16666666666667, 0.0, 0.0, 25.5, 25.0907458274589, 0.1998558880525367, 0.0, 1.0, 0.0]
[2019-04-05 12:42:03,986] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:42:03,987] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.4941059e-07 1.0876208e-01 2.2139397e-02 4.3898636e-01 4.3005162e-01
 1.6590893e-07 6.0112110e-05], sampled 0.479763012439403
[2019-04-05 12:42:06,332] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 3162.9936 233014913.6757 522.8489
[2019-04-05 12:42:07,527] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 3514.7752 238117787.2445 40.3538
[2019-04-05 12:42:08,553] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 700000, evaluation results [700000.0, 3162.9935951297775, 233014913.67569256, 522.8489249984048, 3418.297026003741, 203258444.37642488, 677.6960214700645, 3514.7752262920358, 238117787.24446365, 40.35383869862461]
[2019-04-05 12:42:08,993] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 700235: loss -7.4515
[2019-04-05 12:42:09,002] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 700238: learning rate 0.0000
[2019-04-05 12:42:09,025] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 700252: loss -1.3129
[2019-04-05 12:42:09,027] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 700253: learning rate 0.0000
[2019-04-05 12:42:09,883] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7589142e-06 6.6308933e-01 3.5260750e-03 2.4769734e-01 8.5618369e-02
 2.1045096e-06 6.2034182e-05], sum to 1.0000
[2019-04-05 12:42:09,884] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5250
[2019-04-05 12:42:09,893] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.366666666666667, 65.0, 0.0, 0.0, 19.0, 19.02678048467528, -1.063316453318015, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 771600.0000, 
sim time next is 772200.0000, 
raw observation next is [-6.45, 65.5, 0.0, 0.0, 19.0, 18.95630511331527, -1.080947523333222, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.28393351800554023, 0.655, 0.0, 0.0, 0.08333333333333333, 0.07969209277627254, 0.13968415888892602, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14356764], dtype=float32), 0.045392502]. 
=============================================
[2019-04-05 12:42:10,450] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43500, global step 700997: loss 1.4971
[2019-04-05 12:42:10,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43500, global step 700999: learning rate 0.0000
[2019-04-05 12:42:12,089] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 701832: loss -7.3913
[2019-04-05 12:42:12,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 701833: learning rate 0.0000
[2019-04-05 12:42:12,846] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 702310: loss 20.4886
[2019-04-05 12:42:12,862] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 702310: learning rate 0.0000
[2019-04-05 12:42:13,549] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 702746: loss 20.1597
[2019-04-05 12:42:13,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 702747: learning rate 0.0000
[2019-04-05 12:42:13,790] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.08702006e-08 2.26805508e-02 7.59041309e-03 8.65952671e-01
 1.03765324e-01 2.73468070e-09 1.09387502e-05], sum to 1.0000
[2019-04-05 12:42:13,791] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5584
[2019-04-05 12:42:13,799] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 76.0, 0.0, 0.0, 21.5, 21.20294563349259, -0.4571272010451965, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1027800.0000, 
sim time next is 1028400.0000, 
raw observation next is [14.4, 75.66666666666667, 0.0, 0.0, 21.5, 21.26055647914113, -0.4508876376438684, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7566666666666667, 0.0, 0.0, 0.2916666666666667, 0.2717130399284275, 0.3497041207853772, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.31817955], dtype=float32), -1.4232862]. 
=============================================
[2019-04-05 12:42:14,497] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6684909e-05 4.5002198e-01 3.8905378e-02 1.7692485e-01 3.3313778e-01
 1.8878569e-05 9.4441429e-04], sum to 1.0000
[2019-04-05 12:42:14,498] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6932
[2019-04-05 12:42:14,563] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-10.9, 47.66666666666667, 53.83333333333334, 877.8333333333334, 21.5, 20.51684579468959, -0.8245461216969464, 1.0, 1.0, 197115.3012053661], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 394800.0000, 
sim time next is 395400.0000, 
raw observation next is [-10.7, 46.83333333333334, 52.66666666666667, 868.6666666666666, 22.0, 20.6543237372144, -0.7372655586526232, 1.0, 1.0, 198295.7536336734], 
processed observation next is [1.0, 0.5652173913043478, 0.1662049861495845, 0.46833333333333343, 0.17555555555555558, 0.9598526703499078, 0.3333333333333333, 0.22119364476786676, 0.25424481378245894, 1.0, 1.0, 0.9442654934936828], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8898555], dtype=float32), 0.76308656]. 
=============================================
[2019-04-05 12:42:14,619] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44000, global step 703373: loss -1.9428
[2019-04-05 12:42:14,621] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44000, global step 703373: learning rate 0.0000
[2019-04-05 12:42:16,930] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 704663: loss -1.5105
[2019-04-05 12:42:16,934] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 704664: learning rate 0.0000
[2019-04-05 12:42:18,368] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 705469: loss 10.6558
[2019-04-05 12:42:18,372] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 705472: learning rate 0.0000
[2019-04-05 12:42:18,407] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 705489: loss -12.2119
[2019-04-05 12:42:18,409] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 705489: learning rate 0.0000
[2019-04-05 12:42:18,510] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9363929e-08 6.0225588e-01 9.7713731e-03 3.1702001e-02 3.5626376e-01
 2.7849191e-08 6.9411130e-06], sum to 1.0000
[2019-04-05 12:42:18,511] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6655
[2019-04-05 12:42:18,527] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 19.0, 20.7613929932112, -0.6884572757230161, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [5.5, 89.0, 0.0, 0.0, 19.5, 20.75973485700383, -0.6890593201999455, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6149584487534627, 0.89, 0.0, 0.0, 0.125, 0.2299779047503193, 0.2703135599333515, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.64075625], dtype=float32), 0.089600824]. 
=============================================
[2019-04-05 12:42:18,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[58.65776 ]
 [59.539124]
 [60.498055]
 [61.45489 ]
 [62.68292 ]], R is [[58.27354431]
 [58.69081116]
 [58.96104813]
 [59.08572388]
 [59.06629562]].
[2019-04-05 12:42:19,387] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 706072: loss -0.3290
[2019-04-05 12:42:19,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 706073: learning rate 0.0000
[2019-04-05 12:42:19,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1502064e-08 2.7986310e-02 5.8326726e-03 2.5351867e-01 7.1259034e-01
 7.7068183e-09 7.1913179e-05], sum to 1.0000
[2019-04-05 12:42:19,686] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5022
[2019-04-05 12:42:19,719] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 23.3499241277617, -0.180434263046252, 0.0, 1.0, 20204.39411044005], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2063400.0000, 
sim time next is 2064000.0000, 
raw observation next is [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 23.2397864908636, -0.1949224636004523, 0.0, 1.0, 32447.42496684865], 
processed observation next is [1.0, 0.9130434782608695, 0.3545706371191136, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.4366488742386334, 0.4350258454665159, 0.0, 1.0, 0.15451154746118403], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.227895], dtype=float32), -0.24282733]. 
=============================================
[2019-04-05 12:42:19,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[61.271248]
 [60.854137]
 [60.44077 ]
 [59.00146 ]
 [58.795532]], R is [[61.67906952]
 [61.06227875]
 [60.45165634]
 [59.91857147]
 [59.46224213]].
[2019-04-05 12:42:19,853] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3091280e-06 1.1241950e-01 1.7856814e-01 3.5183266e-01 3.5696822e-01
 1.7650774e-07 2.0604747e-04], sum to 1.0000
[2019-04-05 12:42:19,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2655
[2019-04-05 12:42:19,867] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [10.16666666666667, 92.33333333333334, 54.5, 0.0, 19.0, 22.62343848133811, -0.2880567664171025, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 984000.0000, 
sim time next is 984600.0000, 
raw observation next is [10.25, 92.5, 60.0, 0.0, 19.0, 22.72963428098684, -0.2707216538696907, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7465373961218837, 0.925, 0.2, 0.0, 0.08333333333333333, 0.39413619008223666, 0.40975944871010306, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01029646], dtype=float32), 0.088453256]. 
=============================================
[2019-04-05 12:42:20,321] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43500, global step 706621: loss 1.4426
[2019-04-05 12:42:20,323] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43500, global step 706622: learning rate 0.0000
[2019-04-05 12:42:20,458] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 706697: loss 46.5126
[2019-04-05 12:42:20,460] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 706697: learning rate 0.0000
[2019-04-05 12:42:21,871] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46000, global step 707448: loss 0.2268
[2019-04-05 12:42:21,873] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46000, global step 707448: learning rate 0.0000
[2019-04-05 12:42:24,408] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 708754: loss -3.6746
[2019-04-05 12:42:24,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 708754: learning rate 0.0000
[2019-04-05 12:42:24,443] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 708774: loss 20.4906
[2019-04-05 12:42:24,443] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 708774: learning rate 0.0000
[2019-04-05 12:42:24,911] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 709036: loss 10.4028
[2019-04-05 12:42:24,915] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 709037: learning rate 0.0000
[2019-04-05 12:42:25,732] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 709518: loss -4.0854
[2019-04-05 12:42:25,735] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 709518: learning rate 0.0000
[2019-04-05 12:42:26,250] A3C_AGENT_WORKER-Thread-9 INFO:Local step 43500, global step 709819: loss 13.7495
[2019-04-05 12:42:26,252] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 43500, global step 709819: learning rate 0.0000
[2019-04-05 12:42:26,285] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44500, global step 709839: loss 11.5223
[2019-04-05 12:42:26,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44500, global step 709840: learning rate 0.0000
[2019-04-05 12:42:27,102] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44000, global step 710314: loss -4.3275
[2019-04-05 12:42:27,102] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44000, global step 710314: learning rate 0.0000
[2019-04-05 12:42:27,689] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.7788345e-09 3.1915905e-03 7.0836903e-03 9.7034901e-01 1.9371504e-02
 4.0889728e-10 4.2630659e-06], sum to 1.0000
[2019-04-05 12:42:27,689] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6992
[2019-04-05 12:42:27,702] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 84.66666666666667, 0.0, 0.0, 21.5, 22.29494558012269, -0.3628091612443008, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1808400.0000, 
sim time next is 1809000.0000, 
raw observation next is [-5.0, 84.0, 0.0, 0.0, 21.5, 22.166922881392, -0.3882535177398077, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.84, 0.0, 0.0, 0.2916666666666667, 0.3472435734493334, 0.37058216075339745, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-1.1142278], dtype=float32), 0.17874981]. 
=============================================
[2019-04-05 12:42:27,722] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[63.78112]
 [64.08227]
 [64.6024 ]
 [64.77591]
 [65.05992]], R is [[63.41154099]
 [63.42028427]
 [63.42893982]
 [63.36608124]
 [63.30385208]].
[2019-04-05 12:42:27,846] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45500, global step 710770: loss 2.9911
[2019-04-05 12:42:27,849] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45500, global step 710770: learning rate 0.0000
[2019-04-05 12:42:27,916] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 710810: loss -2.3428
[2019-04-05 12:42:27,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 710811: learning rate 0.0000
[2019-04-05 12:42:28,323] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 711035: loss 16.8995
[2019-04-05 12:42:28,324] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 711035: learning rate 0.0000
[2019-04-05 12:42:30,399] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 712149: loss 13.3876
[2019-04-05 12:42:30,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 712149: learning rate 0.0000
[2019-04-05 12:42:31,395] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 712644: loss 19.9756
[2019-04-05 12:42:31,398] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 712645: learning rate 0.0000
[2019-04-05 12:42:31,431] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0947383e-05 3.9138675e-01 1.6869156e-02 4.0076241e-01 1.8989468e-01
 1.6367640e-05 1.0296563e-03], sum to 1.0000
[2019-04-05 12:42:31,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7961
[2019-04-05 12:42:31,484] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.2, 43.66666666666667, 0.0, 0.0, 19.0, 18.60691946744682, -1.200203332540139, 0.0, 1.0, 64292.03344964398], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 418800.0000, 
sim time next is 419400.0000, 
raw observation next is [-10.3, 44.5, 0.0, 0.0, 19.0, 18.71689901550837, -1.204530079825333, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.1772853185595568, 0.445, 0.0, 0.0, 0.08333333333333333, 0.0597415846256976, 0.09848997339155563, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.066795], dtype=float32), 1.5427386]. 
=============================================
[2019-04-05 12:42:32,425] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 713252: loss 8.0085
[2019-04-05 12:42:32,429] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 713254: learning rate 0.0000
[2019-04-05 12:42:36,281] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8202521e-06 4.4033710e-02 3.3466987e-02 7.4646004e-02 8.4770215e-01
 2.6269024e-07 1.4903992e-04], sum to 1.0000
[2019-04-05 12:42:36,281] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9226
[2019-04-05 12:42:36,297] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 20.0, 19.92775531554781, -0.9179574733822008, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 2393400.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 20.5, 19.865665364879, -0.922725777035458, 0.0, 1.0, 104231.5985814997], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.20833333333333334, 0.15547211373991665, 0.19242474098818066, 0.0, 1.0, 0.496340945626189], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.04461186], dtype=float32), 0.446407]. 
=============================================
[2019-04-05 12:42:36,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[53.486217]
 [53.477634]
 [53.417744]
 [53.58786 ]
 [54.01911 ]], R is [[54.46680069]
 [54.7792778 ]
 [55.16005707]
 [55.53702545]
 [55.98165512]].
[2019-04-05 12:42:36,365] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 715476: loss -1.4292
[2019-04-05 12:42:36,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 715476: learning rate 0.0000
[2019-04-05 12:42:36,760] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44000, global step 715678: loss 19.0932
[2019-04-05 12:42:36,762] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44000, global step 715678: learning rate 0.0000
[2019-04-05 12:42:37,923] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46500, global step 716333: loss -1.5398
[2019-04-05 12:42:37,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46500, global step 716335: learning rate 0.0000
[2019-04-05 12:42:38,353] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45000, global step 716595: loss 30.8713
[2019-04-05 12:42:38,356] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45000, global step 716595: learning rate 0.0000
[2019-04-05 12:42:38,928] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44500, global step 716925: loss 8.4021
[2019-04-05 12:42:38,932] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44500, global step 716925: learning rate 0.0000
[2019-04-05 12:42:39,415] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45500, global step 717191: loss -0.4360
[2019-04-05 12:42:39,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45500, global step 717191: learning rate 0.0000
[2019-04-05 12:42:39,466] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 717218: loss 1.0733
[2019-04-05 12:42:39,469] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 717218: learning rate 0.0000
[2019-04-05 12:42:40,048] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45500, global step 717532: loss 5.0076
[2019-04-05 12:42:40,049] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45500, global step 717532: learning rate 0.0000
[2019-04-05 12:42:42,010] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 718491: loss -1.2857
[2019-04-05 12:42:42,011] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 718491: learning rate 0.0000
[2019-04-05 12:42:43,143] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 719025: loss 10.5674
[2019-04-05 12:42:43,144] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 719025: learning rate 0.0000
[2019-04-05 12:42:43,405] A3C_AGENT_WORKER-Thread-9 INFO:Local step 44000, global step 719165: loss 20.8247
[2019-04-05 12:42:43,407] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 44000, global step 719166: learning rate 0.0000
[2019-04-05 12:42:44,049] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 719536: loss -2.5887
[2019-04-05 12:42:44,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 719536: learning rate 0.0000
[2019-04-05 12:42:44,079] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45500, global step 719553: loss 0.7190
[2019-04-05 12:42:44,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45500, global step 719553: learning rate 0.0000
[2019-04-05 12:42:44,881] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-05 12:42:44,883] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:42:44,884] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:42:44,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:42:44,885] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:42:44,885] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:42:44,886] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:42:44,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run37
[2019-04-05 12:42:44,907] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run37
[2019-04-05 12:42:44,923] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run37
[2019-04-05 12:43:31,340] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.09984285]
[2019-04-05 12:43:31,341] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.666666666666667, 69.0, 5.999999999999999, 0.0, 19.0, 21.8914233368077, -0.5842642784049871, 1.0, 1.0, 0.0]
[2019-04-05 12:43:31,342] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:43:31,342] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.4454105e-05 4.8438263e-01 2.2908300e-02 1.9656646e-01 2.9568407e-01
 4.1369035e-06 4.3999602e-04], sampled 0.062100860392023005
[2019-04-05 12:44:12,688] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 4431.3309 192834467.9969 224.7233
[2019-04-05 12:44:26,521] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4556.1852 212631276.8773 -354.3858
[2019-04-05 12:44:30,304] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4575.2252 223817042.8083 -473.0363
[2019-04-05 12:44:31,329] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 720000, evaluation results [720000.0, 4556.1852434056245, 212631276.87726995, -354.3858371549293, 4431.330878152915, 192834467.9969338, 224.72325276649966, 4575.225168527989, 223817042.80828083, -473.03633350007345]
[2019-04-05 12:44:32,654] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46000, global step 720677: loss -2.2772
[2019-04-05 12:44:32,659] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46000, global step 720679: learning rate 0.0000
[2019-04-05 12:44:32,849] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.2291502e-06 7.3651874e-01 9.9788150e-03 1.6707850e-01 8.5995868e-02
 1.5408198e-06 4.1730123e-04], sum to 1.0000
[2019-04-05 12:44:32,850] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3885
[2019-04-05 12:44:32,857] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 19.0, 21.51203102217846, -0.5023822434113555, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1415400.0000, 
sim time next is 1416000.0000, 
raw observation next is [-0.4, 98.33333333333334, 41.33333333333334, 0.0, 19.0, 21.51198111158232, -0.4956986257662386, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.45152354570637127, 0.9833333333333334, 0.1377777777777778, 0.0, 0.08333333333333333, 0.2926650926318599, 0.33476712474458714, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37976438], dtype=float32), -0.38469508]. 
=============================================
[2019-04-05 12:44:32,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[45.482155]
 [45.766373]
 [46.488804]
 [46.80526 ]
 [47.641846]], R is [[46.02283478]
 [46.53878403]
 [47.05313492]
 [47.35346603]
 [47.34939957]].
[2019-04-05 12:44:33,302] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45500, global step 721057: loss -1.0394
[2019-04-05 12:44:33,304] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45500, global step 721057: learning rate 0.0000
[2019-04-05 12:44:33,472] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1134041e-06 3.0524188e-01 1.5651187e-02 3.6659008e-01 3.1048340e-01
 1.2170356e-06 2.0291395e-03], sum to 1.0000
[2019-04-05 12:44:33,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0927
[2019-04-05 12:44:33,480] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 78.0, 0.0, 19.5, 21.50628764706614, -0.5219103656890932, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1433400.0000, 
sim time next is 1434000.0000, 
raw observation next is [1.1, 92.0, 75.0, 0.0, 19.0, 21.54174364910681, -0.5137179237921449, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.25, 0.0, 0.08333333333333333, 0.2951453040922341, 0.32876069206928504, 1.0, 1.0, 0.0], 
reward next is 0.8628, 
noisyNet noise sample is [array([-0.73035026], dtype=float32), -0.78322995]. 
=============================================
[2019-04-05 12:44:33,495] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[47.990654]
 [47.873577]
 [47.722195]
 [47.60663 ]
 [47.569733]], R is [[48.51806641]
 [48.74235535]
 [48.93917465]
 [49.11936188]
 [49.21588898]].
[2019-04-05 12:44:33,924] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45500, global step 721444: loss -1.8072
[2019-04-05 12:44:33,927] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45500, global step 721444: learning rate 0.0000
[2019-04-05 12:44:34,327] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 721691: loss 2.7976
[2019-04-05 12:44:34,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 721694: learning rate 0.0000
[2019-04-05 12:44:34,725] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44500, global step 721951: loss 7.1851
[2019-04-05 12:44:34,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44500, global step 721951: learning rate 0.0000
[2019-04-05 12:44:36,647] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45000, global step 723007: loss 18.1506
[2019-04-05 12:44:36,647] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45000, global step 723007: learning rate 0.0000
[2019-04-05 12:44:37,299] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.86727833e-09 1.03222225e-02 1.53453648e-03 3.32968757e-02
 9.54841673e-01 3.35914158e-10 4.72056217e-06], sum to 1.0000
[2019-04-05 12:44:37,302] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3027
[2019-04-05 12:44:37,316] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 25.0, 22.65817803198264, -0.03506883619470277, 0.0, 1.0, 46380.89031753537], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 1290000.0000, 
sim time next is 1290600.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 25.5, 22.81291410381136, -0.01990594425662567, 0.0, 1.0, 43556.32793295271], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.625, 0.40107617531761325, 0.4933646852477915, 0.0, 1.0, 0.20741108539501288], 
reward next is 0.0714, 
noisyNet noise sample is [array([-0.01708329], dtype=float32), -0.48227087]. 
=============================================
[2019-04-05 12:44:38,582] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1531254e-08 2.4369154e-03 2.0117057e-02 5.1131284e-01 4.6599674e-01
 3.6274187e-08 1.3640587e-04], sum to 1.0000
[2019-04-05 12:44:38,582] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0400
[2019-04-05 12:44:38,592] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.433333333333334, 92.0, 0.0, 0.0, 21.5, 22.98494341360048, -0.0698958678168627, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1318800.0000, 
sim time next is 1319400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 21.5, 22.86262214430554, -0.08640945914872993, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5000000000000001, 0.92, 0.0, 0.0, 0.2916666666666667, 0.40521851202546166, 0.47119684695042335, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-1.2280426], dtype=float32), 0.23458436]. 
=============================================
[2019-04-05 12:44:38,995] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47000, global step 724237: loss -8.3250
[2019-04-05 12:44:38,996] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47000, global step 724237: learning rate 0.0000
[2019-04-05 12:44:40,090] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 724933: loss 20.7043
[2019-04-05 12:44:40,096] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 724933: learning rate 0.0000
[2019-04-05 12:44:40,422] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45500, global step 725157: loss 4.4186
[2019-04-05 12:44:40,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45500, global step 725158: learning rate 0.0000
[2019-04-05 12:44:40,494] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45500, global step 725209: loss 1.8841
[2019-04-05 12:44:40,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45500, global step 725209: learning rate 0.0000
[2019-04-05 12:44:40,701] A3C_AGENT_WORKER-Thread-9 INFO:Local step 44500, global step 725344: loss 29.0993
[2019-04-05 12:44:40,704] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 44500, global step 725344: learning rate 0.0000
[2019-04-05 12:44:41,771] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 726005: loss 19.0756
[2019-04-05 12:44:41,772] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 726006: learning rate 0.0000
[2019-04-05 12:44:42,008] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0440339e-05 5.5877930e-01 3.0424837e-02 7.5210758e-02 3.3460253e-01
 1.1482316e-05 9.5061254e-04], sum to 1.0000
[2019-04-05 12:44:42,009] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5209
[2019-04-05 12:44:42,022] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.633333333333334, 65.0, 227.8333333333333, 5.0, 19.0, 21.16491109773227, -0.7657052461902741, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [-4.45, 65.0, 227.0, 4.0, 19.0, 21.0182515345597, -0.7741584858923293, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3393351800554017, 0.65, 0.7566666666666667, 0.004419889502762431, 0.08333333333333333, 0.25152096121330825, 0.24194717136922358, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5046578], dtype=float32), -0.3449626]. 
=============================================
[2019-04-05 12:44:42,107] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4670691e-04 8.5783631e-01 3.3521425e-02 8.3871484e-02 2.4468718e-02
 1.8953697e-06 1.5349904e-04], sum to 1.0000
[2019-04-05 12:44:42,111] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3998
[2019-04-05 12:44:42,122] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.816666666666666, 64.5, 167.0, 1.333333333333333, 19.0, 20.75969201162587, -0.8223819844944856, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1948200.0000, 
sim time next is 1948800.0000, 
raw observation next is [-3.733333333333333, 64.0, 152.0, 0.6666666666666665, 19.0, 20.67988093467695, -0.8124194016831673, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.64, 0.5066666666666667, 0.000736648250460405, 0.08333333333333333, 0.2233234112230793, 0.22919353277227758, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3592374], dtype=float32), 0.46651724]. 
=============================================
[2019-04-05 12:44:42,532] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46000, global step 726415: loss -2.3563
[2019-04-05 12:44:42,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46000, global step 726416: learning rate 0.0000
[2019-04-05 12:44:43,078] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46000, global step 726649: loss -3.3601
[2019-04-05 12:44:43,082] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46000, global step 726650: learning rate 0.0000
[2019-04-05 12:44:45,894] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 728219: loss -1.5393
[2019-04-05 12:44:45,896] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 728220: learning rate 0.0000
[2019-04-05 12:44:46,003] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 728290: loss 23.6873
[2019-04-05 12:44:46,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 728290: learning rate 0.0000
[2019-04-05 12:44:47,102] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45000, global step 728895: loss 27.4157
[2019-04-05 12:44:47,104] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45000, global step 728896: learning rate 0.0000
[2019-04-05 12:44:48,726] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46000, global step 729689: loss 8.1631
[2019-04-05 12:44:48,728] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46000, global step 729690: learning rate 0.0000
[2019-04-05 12:44:50,302] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46500, global step 730534: loss 1.5050
[2019-04-05 12:44:50,305] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46500, global step 730534: learning rate 0.0000
[2019-04-05 12:44:51,203] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45500, global step 730995: loss 4.9102
[2019-04-05 12:44:51,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45500, global step 730998: learning rate 0.0000
[2019-04-05 12:44:51,522] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46000, global step 731162: loss 8.3954
[2019-04-05 12:44:51,523] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46000, global step 731162: learning rate 0.0000
[2019-04-05 12:44:52,001] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46000, global step 731414: loss 22.2492
[2019-04-05 12:44:52,002] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46000, global step 731414: learning rate 0.0000
[2019-04-05 12:44:53,079] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 732010: loss 14.8991
[2019-04-05 12:44:53,081] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 732010: learning rate 0.0000
[2019-04-05 12:44:53,190] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 732075: loss 34.9052
[2019-04-05 12:44:53,191] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 732075: learning rate 0.0000
[2019-04-05 12:44:53,412] A3C_AGENT_WORKER-Thread-9 INFO:Local step 45000, global step 732202: loss 31.3247
[2019-04-05 12:44:53,413] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 45000, global step 732202: learning rate 0.0000
[2019-04-05 12:44:53,626] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5344751e-08 6.9776779e-01 1.1180522e-03 1.7808516e-01 1.2302692e-01
 1.7476021e-08 1.9187560e-06], sum to 1.0000
[2019-04-05 12:44:53,627] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4718
[2019-04-05 12:44:53,635] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 26.83333333333333, 67.33333333333333, 714.6666666666666, 19.0, 21.54891331644036, -0.5397935934345914, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2473800.0000, 
sim time next is 2474400.0000, 
raw observation next is [3.3, 26.66666666666667, 64.66666666666667, 697.3333333333334, 19.0, 21.53711075070439, -0.5422060139965431, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2666666666666667, 0.21555555555555558, 0.7705340699815838, 0.08333333333333333, 0.2947592292253658, 0.3192646620011523, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3022281], dtype=float32), 1.1416057]. 
=============================================
[2019-04-05 12:44:54,174] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47500, global step 732625: loss 4.6171
[2019-04-05 12:44:54,176] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47500, global step 732627: learning rate 0.0000
[2019-04-05 12:44:54,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6626136e-09 5.2174669e-02 5.6842627e-04 8.4151542e-01 1.0574065e-01
 1.0145998e-09 7.7540062e-07], sum to 1.0000
[2019-04-05 12:44:54,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0479
[2019-04-05 12:44:54,755] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.25, 82.0, 0.0, 0.0, 19.5, 22.03824904315102, -0.3460667991909351, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1553400.0000, 
sim time next is 1554000.0000, 
raw observation next is [5.166666666666666, 82.0, 0.0, 0.0, 19.0, 21.93158010580213, -0.3663633733730378, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6057248384118191, 0.82, 0.0, 0.0, 0.08333333333333333, 0.32763167548351085, 0.37787887554232075, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9654338], dtype=float32), -0.28546336]. 
=============================================
[2019-04-05 12:44:54,762] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[72.018875]
 [72.364365]
 [72.5036  ]
 [73.28396 ]
 [73.21115 ]], R is [[72.04280853]
 [72.25095367]
 [72.45701599]
 [72.66101837]
 [72.8629837 ]].
[2019-04-05 12:44:56,690] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 734104: loss 14.4898
[2019-04-05 12:44:56,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 734104: learning rate 0.0000
[2019-04-05 12:44:57,026] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46000, global step 734261: loss 10.9926
[2019-04-05 12:44:57,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46000, global step 734261: learning rate 0.0000
[2019-04-05 12:44:57,773] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46000, global step 734642: loss -2.3516
[2019-04-05 12:44:57,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46000, global step 734642: learning rate 0.0000
[2019-04-05 12:45:00,188] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46500, global step 735842: loss -12.3824
[2019-04-05 12:45:00,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46500, global step 735842: learning rate 0.0000
[2019-04-05 12:45:00,219] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46500, global step 735855: loss 1.3055
[2019-04-05 12:45:00,223] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46500, global step 735855: learning rate 0.0000
[2019-04-05 12:45:00,865] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45500, global step 736235: loss 0.2842
[2019-04-05 12:45:00,866] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45500, global step 736235: learning rate 0.0000
[2019-04-05 12:45:02,964] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45500, global step 737357: loss 0.9114
[2019-04-05 12:45:02,964] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45500, global step 737357: learning rate 0.0000
[2019-04-05 12:45:04,514] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47000, global step 738047: loss -5.1156
[2019-04-05 12:45:04,515] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47000, global step 738048: learning rate 0.0000
[2019-04-05 12:45:05,843] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46500, global step 738598: loss -0.4864
[2019-04-05 12:45:05,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46500, global step 738598: learning rate 0.0000
[2019-04-05 12:45:06,633] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45500, global step 738973: loss -4.5872
[2019-04-05 12:45:06,642] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45500, global step 738973: learning rate 0.0000
[2019-04-05 12:45:07,586] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48000, global step 739483: loss -8.5440
[2019-04-05 12:45:07,586] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48000, global step 739483: learning rate 0.0000
[2019-04-05 12:45:07,755] A3C_AGENT_WORKER-Thread-9 INFO:Local step 45500, global step 739569: loss -2.5285
[2019-04-05 12:45:07,757] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 45500, global step 739569: learning rate 0.0000
[2019-04-05 12:45:08,135] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45500, global step 739767: loss -5.7756
[2019-04-05 12:45:08,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45500, global step 739768: learning rate 0.0000
[2019-04-05 12:45:08,264] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46500, global step 739829: loss -0.8105
[2019-04-05 12:45:08,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46500, global step 739829: learning rate 0.0000
[2019-04-05 12:45:08,565] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46500, global step 739978: loss -2.1181
[2019-04-05 12:45:08,566] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46500, global step 739979: learning rate 0.0000
[2019-04-05 12:45:08,605] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-05 12:45:08,606] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:45:08,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:45:08,607] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:45:08,607] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:45:08,607] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:45:08,608] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:45:08,612] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run38
[2019-04-05 12:45:08,631] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run38
[2019-04-05 12:45:08,644] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run38
[2019-04-05 12:45:26,356] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10147703]
[2019-04-05 12:45:26,356] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-13.36666666666667, 70.33333333333334, 0.0, 0.0, 21.0, 19.07744581885801, -1.114415724547877, 0.0, 1.0, 90902.55540758307]
[2019-04-05 12:45:26,357] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:45:26,357] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1145039e-05 2.0362605e-01 4.3758906e-02 2.6262268e-01 4.8951459e-01
 7.8918620e-06 4.5864077e-04], sampled 0.5466008286727297
[2019-04-05 12:46:28,794] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10147703]
[2019-04-05 12:46:28,795] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.650049371666666, 44.39901226666667, 154.2752834733334, 662.9907241000001, 19.0, 25.92793958550899, 0.5437316831849791, 1.0, 1.0, 0.0]
[2019-04-05 12:46:28,795] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:46:28,796] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.6065492e-07 4.5834124e-01 8.1165945e-03 1.6377251e-01 3.6972931e-01
 1.2690187e-07 3.9755421e-05], sampled 0.25715863703755315
[2019-04-05 12:46:39,177] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 4049.6683 198712653.9033 426.6784
[2019-04-05 12:46:52,213] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 4236.3138 220482413.7841 35.6548
[2019-04-05 12:46:55,828] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 4179.6630 232812126.3457 -227.1245
[2019-04-05 12:46:56,852] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 740000, evaluation results [740000.0, 4236.313838755795, 220482413.78411344, 35.654835783525364, 4049.668345011638, 198712653.90330377, 426.67840848725393, 4179.662987754444, 232812126.34568775, -227.12449682824214]
[2019-04-05 12:46:56,962] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46000, global step 740057: loss 1.6930
[2019-04-05 12:46:56,963] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46000, global step 740057: learning rate 0.0000
[2019-04-05 12:46:57,721] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 740486: loss 19.1550
[2019-04-05 12:46:57,722] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 740487: learning rate 0.0000
[2019-04-05 12:46:58,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.15877003e-07 1.18593395e-01 5.52175427e-03 1.77006394e-01
 6.98870420e-01 7.71054033e-07 7.01310273e-06], sum to 1.0000
[2019-04-05 12:46:58,476] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1541
[2019-04-05 12:46:58,492] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 23.0, 20.73157425760074, -0.740514688107163, 0.0, 1.0, 46849.0997259711], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2356800.0000, 
sim time next is 2357400.0000, 
raw observation next is [-3.3, 68.33333333333333, 0.0, 0.0, 23.0, 20.73586089871667, -0.7362646555270319, 0.0, 1.0, 46180.19953374724], 
processed observation next is [0.0, 0.2608695652173913, 0.37119113573407203, 0.6833333333333332, 0.0, 0.0, 0.4166666666666667, 0.22798840822638913, 0.254578448157656, 0.0, 1.0, 0.21990571206546305], 
reward next is 0.4286, 
noisyNet noise sample is [array([-2.0419102], dtype=float32), 1.4921798]. 
=============================================
[2019-04-05 12:47:01,533] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46500, global step 742476: loss -2.5681
[2019-04-05 12:47:01,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46500, global step 742478: learning rate 0.0000
[2019-04-05 12:47:01,859] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47000, global step 742672: loss -0.9161
[2019-04-05 12:47:01,863] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47000, global step 742675: learning rate 0.0000
[2019-04-05 12:47:02,598] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47000, global step 743067: loss -4.9147
[2019-04-05 12:47:02,600] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47000, global step 743067: learning rate 0.0000
[2019-04-05 12:47:02,905] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46500, global step 743227: loss 1.6082
[2019-04-05 12:47:02,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46500, global step 743227: learning rate 0.0000
[2019-04-05 12:47:04,690] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1454799e-05 1.7143132e-01 3.2988269e-02 1.2748159e-01 6.6628349e-01
 2.9126579e-06 1.8009197e-03], sum to 1.0000
[2019-04-05 12:47:04,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2797
[2019-04-05 12:47:04,706] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.383333333333333, 79.50000000000001, 48.33333333333334, 24.66666666666667, 19.0, 19.20471115598669, -1.096619644394456, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2103000.0000, 
sim time next is 2103600.0000, 
raw observation next is [-7.466666666666667, 80.0, 60.16666666666666, 30.83333333333334, 19.0, 19.10151560055404, -1.069370835550657, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25577100646352724, 0.8, 0.20055555555555551, 0.03406998158379374, 0.08333333333333333, 0.09179296671283677, 0.14354305481644766, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.374643], dtype=float32), 1.6670089]. 
=============================================
[2019-04-05 12:47:05,070] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46000, global step 744393: loss -2.0556
[2019-04-05 12:47:05,073] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46000, global step 744393: learning rate 0.0000
[2019-04-05 12:47:05,898] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2017568e-06 2.0260690e-01 2.2138748e-03 5.4538631e-01 2.4973677e-01
 2.1158337e-06 4.5769681e-05], sum to 1.0000
[2019-04-05 12:47:05,898] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1402
[2019-04-05 12:47:05,934] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 231.0, 419.6666666666666, 20.5, 20.16145268475653, -0.7517820716531024, 0.0, 1.0, 31052.4846659146], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2980200.0000, 
sim time next is 2980800.0000, 
raw observation next is [-3.0, 65.0, 218.5, 487.5, 19.5, 20.20308769051314, -0.7435508111194555, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.7283333333333334, 0.5386740331491713, 0.125, 0.183590640876095, 0.2521497296268482, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.2337401], dtype=float32), 0.1914255]. 
=============================================
[2019-04-05 12:47:07,942] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47500, global step 745902: loss 2.6669
[2019-04-05 12:47:07,944] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47500, global step 745902: learning rate 0.0000
[2019-04-05 12:47:08,294] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47000, global step 746075: loss 10.6912
[2019-04-05 12:47:08,295] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47000, global step 746075: learning rate 0.0000
[2019-04-05 12:47:08,537] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46000, global step 746208: loss -3.0967
[2019-04-05 12:47:08,539] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46000, global step 746208: learning rate 0.0000
[2019-04-05 12:47:09,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4231682e-05 8.3864385e-01 5.4534888e-03 4.8551492e-02 1.0727569e-01
 2.4763431e-06 4.8796093e-05], sum to 1.0000
[2019-04-05 12:47:09,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7394
[2019-04-05 12:47:09,459] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 55.0, 51.0, 18.0, 19.0, 23.46906827237892, -0.2496496044107356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2536200.0000, 
sim time next is 2536800.0000, 
raw observation next is [-2.8, 55.33333333333333, 65.16666666666666, 20.5, 19.0, 23.698988436095, -0.2354865692210486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.38504155124653744, 0.5533333333333332, 0.21722222222222218, 0.022651933701657457, 0.08333333333333333, 0.4749157030079167, 0.42150447692631715, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4291098], dtype=float32), 0.5467734]. 
=============================================
[2019-04-05 12:47:10,147] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5790789e-05 7.7586174e-01 5.4188287e-03 7.7451773e-02 1.4096235e-01
 4.3745913e-06 2.7512640e-04], sum to 1.0000
[2019-04-05 12:47:10,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9355
[2019-04-05 12:47:10,189] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8, 47.66666666666667, 149.5, 49.33333333333333, 19.0, 23.95890571744369, -0.1717844158900238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2544000.0000, 
sim time next is 2544600.0000, 
raw observation next is [-0.7, 47.33333333333334, 166.0, 53.66666666666666, 19.5, 23.95344308734305, -0.172980387197869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.443213296398892, 0.47333333333333344, 0.5533333333333333, 0.059300184162062605, 0.125, 0.4961202572785875, 0.44233987093404364, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([2.062895], dtype=float32), -1.9218018]. 
=============================================
[2019-04-05 12:47:10,539] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2031416e-05 1.1251888e-01 1.5918003e-01 3.2312274e-01 4.0506759e-01
 4.9472433e-06 8.3892410e-05], sum to 1.0000
[2019-04-05 12:47:10,539] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8701
[2019-04-05 12:47:10,555] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.2, 87.66666666666667, 0.0, 0.0, 20.5, 19.46781288778275, -0.9874648744101587, 0.0, 1.0, 91313.99444907781], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2085600.0000, 
sim time next is 2086200.0000, 
raw observation next is [-5.3, 88.5, 0.0, 0.0, 21.0, 19.55145108434595, -0.9781699582337566, 0.0, 1.0, 62544.65729498532], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.885, 0.0, 0.0, 0.25, 0.12928759036216242, 0.17394334725541447, 0.0, 1.0, 0.297831701404692], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.9723288], dtype=float32), 1.5215955]. 
=============================================
[2019-04-05 12:47:10,731] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48500, global step 747239: loss -5.1349
[2019-04-05 12:47:10,732] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48500, global step 747239: learning rate 0.0000
[2019-04-05 12:47:11,519] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47000, global step 747625: loss 4.2179
[2019-04-05 12:47:11,521] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47000, global step 747625: learning rate 0.0000
[2019-04-05 12:47:11,571] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47000, global step 747651: loss -4.2696
[2019-04-05 12:47:11,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47000, global step 747654: learning rate 0.0000
[2019-04-05 12:47:11,744] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45500, global step 747741: loss 2.2778
[2019-04-05 12:47:11,745] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45500, global step 747741: learning rate 0.0000
[2019-04-05 12:47:12,119] A3C_AGENT_WORKER-Thread-9 INFO:Local step 46000, global step 747893: loss -0.6474
[2019-04-05 12:47:12,120] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 46000, global step 747893: learning rate 0.0000
[2019-04-05 12:47:12,148] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46000, global step 747914: loss 15.2790
[2019-04-05 12:47:12,148] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46000, global step 747914: learning rate 0.0000
[2019-04-05 12:47:13,152] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46000, global step 748494: loss 44.5980
[2019-04-05 12:47:13,159] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46000, global step 748494: learning rate 0.0000
[2019-04-05 12:47:13,999] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46500, global step 748959: loss -3.0604
[2019-04-05 12:47:14,000] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46500, global step 748959: learning rate 0.0000
[2019-04-05 12:47:16,802] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5669692e-07 6.3857788e-01 4.5189392e-03 1.6004890e-01 1.9684003e-01
 1.8909979e-07 1.3808791e-05], sum to 1.0000
[2019-04-05 12:47:16,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1770
[2019-04-05 12:47:16,870] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.85, 37.16666666666666, 79.33333333333333, 794.3333333333334, 26.0, 24.58155270924717, 0.1008277296309559, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 2458200.0000, 
sim time next is 2458800.0000, 
raw observation next is [-2.3, 36.0, 81.0, 803.0, 25.0, 24.89688790450062, 0.1211983663071096, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.36, 0.27, 0.887292817679558, 0.5833333333333334, 0.574740658708385, 0.5403994554357032, 0.0, 1.0, 0.0], 
reward next is 0.1429, 
noisyNet noise sample is [array([-0.91380537], dtype=float32), 0.85439116]. 
=============================================
[2019-04-05 12:47:17,086] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47000, global step 750487: loss 13.2585
[2019-04-05 12:47:17,086] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47000, global step 750487: learning rate 0.0000
[2019-04-05 12:47:17,458] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47500, global step 750673: loss -0.9436
[2019-04-05 12:47:17,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47500, global step 750673: learning rate 0.0000
[2019-04-05 12:47:17,801] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47000, global step 750868: loss -4.4847
[2019-04-05 12:47:17,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47000, global step 750868: learning rate 0.0000
[2019-04-05 12:47:18,270] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47500, global step 751154: loss -0.0805
[2019-04-05 12:47:18,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47500, global step 751154: learning rate 0.0000
[2019-04-05 12:47:20,704] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46500, global step 752475: loss 10.5160
[2019-04-05 12:47:20,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46500, global step 752475: learning rate 0.0000
[2019-04-05 12:47:21,966] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48000, global step 753181: loss 4.3708
[2019-04-05 12:47:21,968] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48000, global step 753182: learning rate 0.0000
[2019-04-05 12:47:22,623] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8665446e-05 5.6876987e-01 3.5106689e-02 5.7767157e-02 3.3787170e-01
 1.2362027e-05 4.3348223e-04], sum to 1.0000
[2019-04-05 12:47:22,623] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5578
[2019-04-05 12:47:22,673] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.5, 73.5, 104.0, 615.0, 20.5, 24.22831886098955, 0.005572967400276967, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3317400.0000, 
sim time next is 3318000.0000, 
raw observation next is [-8.333333333333334, 72.33333333333333, 105.1666666666667, 635.8333333333333, 19.5, 24.23734736947328, 0.01954412556628321, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.23176361957525393, 0.7233333333333333, 0.3505555555555557, 0.7025782688766113, 0.125, 0.5197789474561066, 0.5065147085220943, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.41037562], dtype=float32), 0.7381435]. 
=============================================
[2019-04-05 12:47:22,682] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[41.628098]
 [41.458817]
 [41.09568 ]
 [41.273914]
 [41.343483]], R is [[42.38343048]
 [42.74530792]
 [42.96071243]
 [43.24539185]
 [43.3843689 ]].
[2019-04-05 12:47:23,080] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49000, global step 753776: loss 6.4266
[2019-04-05 12:47:23,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49000, global step 753777: learning rate 0.0000
[2019-04-05 12:47:23,685] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47500, global step 754034: loss -2.1456
[2019-04-05 12:47:23,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47500, global step 754034: learning rate 0.0000
[2019-04-05 12:47:24,758] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46500, global step 754576: loss 14.2548
[2019-04-05 12:47:24,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46500, global step 754577: learning rate 0.0000
[2019-04-05 12:47:26,568] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47500, global step 755516: loss 1.4729
[2019-04-05 12:47:26,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47500, global step 755520: learning rate 0.0000
[2019-04-05 12:47:27,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.52342134e-08 8.32962155e-01 9.21263266e-03 1.15713574e-01
 4.21028063e-02 2.26190423e-07 8.48677792e-06], sum to 1.0000
[2019-04-05 12:47:27,508] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1669
[2019-04-05 12:47:27,550] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 49.0, 114.3333333333333, 809.6666666666666, 23.0, 23.81908396351329, -0.02183637897977954, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 3417000.0000, 
sim time next is 3417600.0000, 
raw observation next is [3.0, 49.0, 113.6666666666667, 807.8333333333334, 22.0, 23.89799190484639, 0.03219494363940125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.378888888888889, 0.892633517495396, 0.3333333333333333, 0.49149932540386576, 0.5107316478798004, 1.0, 1.0, 0.0], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.19491887], dtype=float32), 0.90495706]. 
=============================================
[2019-04-05 12:47:27,964] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47500, global step 756216: loss -4.4861
[2019-04-05 12:47:27,970] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47500, global step 756216: learning rate 0.0000
[2019-04-05 12:47:28,452] A3C_AGENT_WORKER-Thread-9 INFO:Local step 46500, global step 756484: loss 1.6180
[2019-04-05 12:47:28,454] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 46500, global step 756484: learning rate 0.0000
[2019-04-05 12:47:28,551] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47000, global step 756533: loss -6.1331
[2019-04-05 12:47:28,552] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47000, global step 756533: learning rate 0.0000
[2019-04-05 12:47:28,562] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46500, global step 756540: loss -2.5274
[2019-04-05 12:47:28,564] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46500, global step 756542: learning rate 0.0000
[2019-04-05 12:47:29,128] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46500, global step 756833: loss -6.0124
[2019-04-05 12:47:29,129] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46500, global step 756834: learning rate 0.0000
[2019-04-05 12:47:29,295] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46000, global step 756920: loss 11.5079
[2019-04-05 12:47:29,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46000, global step 756921: learning rate 0.0000
[2019-04-05 12:47:31,711] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48000, global step 758232: loss 16.3552
[2019-04-05 12:47:31,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48000, global step 758234: learning rate 0.0000
[2019-04-05 12:47:31,712] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47500, global step 758234: loss 0.9475
[2019-04-05 12:47:31,714] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47500, global step 758234: learning rate 0.0000
[2019-04-05 12:47:32,261] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48000, global step 758534: loss 26.2910
[2019-04-05 12:47:32,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48000, global step 758534: learning rate 0.0000
[2019-04-05 12:47:33,476] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47500, global step 759176: loss 4.3027
[2019-04-05 12:47:33,479] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47500, global step 759177: learning rate 0.0000
[2019-04-05 12:47:34,372] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.8830495e-08 8.3109373e-01 2.9287897e-03 1.2727453e-01 3.8678076e-02
 2.5304388e-07 2.4553050e-05], sum to 1.0000
[2019-04-05 12:47:34,375] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4602
[2019-04-05 12:47:34,384] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 19.0, 19.55988524717628, -0.8208498802991372, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3271200.0000, 
sim time next is 3271800.0000, 
raw observation next is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 19.0, 19.53089774981265, -0.8250151167163612, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.32871652816251157, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.1275748124843874, 0.22499496109454628, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1842227], dtype=float32), -1.2261438]. 
=============================================
[2019-04-05 12:47:34,868] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 12:47:34,871] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:47:34,872] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:47:34,872] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:47:34,873] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:47:34,874] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:47:34,878] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:47:34,886] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run39
[2019-04-05 12:47:34,904] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run39
[2019-04-05 12:47:34,920] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run39
[2019-04-05 12:47:43,653] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.10293714]
[2019-04-05 12:47:43,653] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.4, 65.0, 0.0, 0.0, 19.0, 19.70646316284604, -0.9801388841747846, 1.0, 1.0, 0.0]
[2019-04-05 12:47:43,653] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:47:43,654] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.7392455e-06 5.4418403e-01 2.0024899e-02 1.7561643e-01 2.5996017e-01
 3.0446363e-06 2.0261556e-04], sampled 0.7667933183703285
[2019-04-05 12:48:53,286] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6313.3276 161785064.2630 -924.3147
[2019-04-05 12:49:04,394] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5768.0666 186810025.6010 -1744.3997
[2019-04-05 12:49:12,550] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5693.5982 205408533.2544 -1413.8646
[2019-04-05 12:49:13,575] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 760000, evaluation results [760000.0, 5768.06664309059, 186810025.60098264, -1744.3996557322382, 6313.327622002311, 161785064.26296034, -924.3147353875609, 5693.598247003513, 205408533.25436562, -1413.8645937156134]
[2019-04-05 12:49:15,132] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48000, global step 760829: loss 13.5183
[2019-04-05 12:49:15,133] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48000, global step 760829: learning rate 0.0000
[2019-04-05 12:49:15,157] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47000, global step 760841: loss -6.7153
[2019-04-05 12:49:15,158] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47000, global step 760841: learning rate 0.0000
[2019-04-05 12:49:15,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49500, global step 761176: loss 6.7961
[2019-04-05 12:49:15,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49500, global step 761177: learning rate 0.0000
[2019-04-05 12:49:16,212] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48500, global step 761390: loss -1.7391
[2019-04-05 12:49:16,212] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48500, global step 761390: learning rate 0.0000
[2019-04-05 12:49:18,465] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48000, global step 762724: loss 17.2389
[2019-04-05 12:49:18,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48000, global step 762724: learning rate 0.0000
[2019-04-05 12:49:18,573] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47000, global step 762788: loss -21.0683
[2019-04-05 12:49:18,574] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47000, global step 762789: learning rate 0.0000
[2019-04-05 12:49:20,427] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48000, global step 763799: loss 39.7746
[2019-04-05 12:49:20,429] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48000, global step 763799: learning rate 0.0000
[2019-04-05 12:49:21,669] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47000, global step 764525: loss -5.5159
[2019-04-05 12:49:21,669] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47000, global step 764525: learning rate 0.0000
[2019-04-05 12:49:22,006] A3C_AGENT_WORKER-Thread-9 INFO:Local step 47000, global step 764713: loss -10.4377
[2019-04-05 12:49:22,008] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 47000, global step 764714: learning rate 0.0000
[2019-04-05 12:49:22,304] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47500, global step 764886: loss 3.3253
[2019-04-05 12:49:22,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47500, global step 764888: learning rate 0.0000
[2019-04-05 12:49:23,679] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47000, global step 765609: loss -1.9672
[2019-04-05 12:49:23,682] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47000, global step 765611: learning rate 0.0000
[2019-04-05 12:49:24,128] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48000, global step 765824: loss 29.9331
[2019-04-05 12:49:24,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48000, global step 765824: learning rate 0.0000
[2019-04-05 12:49:24,256] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46500, global step 765889: loss 4.5728
[2019-04-05 12:49:24,258] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46500, global step 765890: learning rate 0.0000
[2019-04-05 12:49:24,272] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48500, global step 765897: loss 0.0001
[2019-04-05 12:49:24,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48500, global step 765897: learning rate 0.0000
[2019-04-05 12:49:26,229] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48000, global step 767064: loss 0.5074
[2019-04-05 12:49:26,231] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48000, global step 767065: learning rate 0.0000
[2019-04-05 12:49:26,340] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48500, global step 767120: loss 1.4790
[2019-04-05 12:49:26,343] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48500, global step 767122: learning rate 0.0000
[2019-04-05 12:49:27,939] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49000, global step 768042: loss 0.2852
[2019-04-05 12:49:27,947] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49000, global step 768042: learning rate 0.0000
[2019-04-05 12:49:28,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2365924e-06 8.3997118e-01 1.7118620e-02 1.1198964e-01 3.0881355e-02
 3.6113062e-07 3.7702142e-05], sum to 1.0000
[2019-04-05 12:49:28,401] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6938
[2019-04-05 12:49:28,417] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 100.0, 127.0, 0.0, 22.0, 21.5955910502131, -0.5667956851651074, 1.0, 1.0, 49361.74548575564], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2901600.0000, 
sim time next is 2902200.0000, 
raw observation next is [2.0, 100.0, 114.6666666666667, 0.0, 21.0, 21.62541181162944, -0.57003967268302, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.38222222222222235, 0.0, 0.25, 0.3021176509691201, 0.30998677577232664, 1.0, 1.0, 0.0], 
reward next is 0.0139, 
noisyNet noise sample is [array([0.14219701], dtype=float32), -0.5712882]. 
=============================================
[2019-04-05 12:49:28,522] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50000, global step 768379: loss 7.6320
[2019-04-05 12:49:28,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50000, global step 768380: learning rate 0.0000
[2019-04-05 12:49:28,595] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48500, global step 768419: loss -0.2758
[2019-04-05 12:49:28,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48500, global step 768419: learning rate 0.0000
[2019-04-05 12:49:30,741] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47500, global step 769750: loss -0.4003
[2019-04-05 12:49:30,744] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47500, global step 769750: learning rate 0.0000
[2019-04-05 12:49:31,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.1278886e-09 3.9232984e-01 1.7298423e-02 5.3110176e-01 5.9265044e-02
 8.4659661e-08 4.8385705e-06], sum to 1.0000
[2019-04-05 12:49:31,496] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4254
[2019-04-05 12:49:31,508] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 19.0, 19.43624721446394, -0.9398181900103859, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2928600.0000, 
sim time next is 2929200.0000, 
raw observation next is [-1.0, 80.33333333333334, 0.0, 0.0, 19.0, 19.41739430489687, -0.9472943046167597, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.11811619207473918, 0.18423523179441345, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11071089], dtype=float32), -0.10407304]. 
=============================================
[2019-04-05 12:49:32,801] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5681584e-07 7.1092439e-01 3.2833894e-03 6.1953690e-02 2.2381110e-01
 4.5400000e-08 2.7058530e-05], sum to 1.0000
[2019-04-05 12:49:32,805] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4374
[2019-04-05 12:49:32,831] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 20.0, 21.10593030728452, -0.683179447308857, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3724200.0000, 
sim time next is 3724800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 20.99861077621041, -0.7079473824399645, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.24988423135086743, 0.26401753918667853, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21488367], dtype=float32), 0.11744653]. 
=============================================
[2019-04-05 12:49:33,128] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48500, global step 771045: loss 1.2495
[2019-04-05 12:49:33,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48500, global step 771045: learning rate 0.0000
[2019-04-05 12:49:33,867] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47500, global step 771430: loss 5.5903
[2019-04-05 12:49:33,871] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47500, global step 771430: learning rate 0.0000
[2019-04-05 12:49:34,546] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48500, global step 771795: loss -3.3418
[2019-04-05 12:49:34,547] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48500, global step 771797: learning rate 0.0000
[2019-04-05 12:49:35,829] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48000, global step 772569: loss 32.8093
[2019-04-05 12:49:35,832] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48000, global step 772570: learning rate 0.0000
[2019-04-05 12:49:35,998] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47500, global step 772671: loss -0.5615
[2019-04-05 12:49:35,998] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47500, global step 772671: learning rate 0.0000
[2019-04-05 12:49:37,460] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49000, global step 773540: loss 0.3500
[2019-04-05 12:49:37,464] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49000, global step 773542: learning rate 0.0000
[2019-04-05 12:49:37,506] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48500, global step 773569: loss -3.3551
[2019-04-05 12:49:37,507] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48500, global step 773569: learning rate 0.0000
[2019-04-05 12:49:37,584] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49000, global step 773616: loss 0.2947
[2019-04-05 12:49:37,586] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49000, global step 773617: learning rate 0.0000
[2019-04-05 12:49:37,786] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9802041e-07 7.7642781e-01 1.3299287e-02 7.2054066e-02 1.3820277e-01
 1.7139614e-07 1.5640588e-05], sum to 1.0000
[2019-04-05 12:49:37,786] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5885
[2019-04-05 12:49:37,797] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 19.0, 19.93230881012886, -0.9309073368444661, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3055200.0000, 
sim time next is 3055800.0000, 
raw observation next is [-6.0, 59.83333333333334, 88.33333333333334, 451.0, 19.0, 19.87867506091905, -0.9405765295060099, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.5983333333333334, 0.29444444444444445, 0.4983425414364641, 0.08333333333333333, 0.15655625507658755, 0.18647449016466336, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.306215], dtype=float32), -0.72649807]. 
=============================================
[2019-04-05 12:49:37,896] A3C_AGENT_WORKER-Thread-9 INFO:Local step 47500, global step 773807: loss -0.3447
[2019-04-05 12:49:37,897] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 47500, global step 773807: learning rate 0.0000
[2019-04-05 12:49:38,939] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47000, global step 774421: loss -3.1982
[2019-04-05 12:49:38,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47000, global step 774422: learning rate 0.0000
[2019-04-05 12:49:39,637] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7365556e-06 8.4053087e-01 2.2586137e-02 9.7825274e-02 3.8961086e-02
 2.3467819e-06 9.1593669e-05], sum to 1.0000
[2019-04-05 12:49:39,640] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6818
[2019-04-05 12:49:39,650] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.166666666666667, 60.0, 91.0, 500.6666666666666, 19.0, 20.17390814001501, -0.8508654635609231, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3401400.0000, 
sim time next is 3402000.0000, 
raw observation next is [-1.0, 60.0, 93.0, 540.0, 19.0, 20.37541103027066, -0.839580577159011, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.6, 0.31, 0.5966850828729282, 0.08333333333333333, 0.19795091918922156, 0.220139807613663, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6922621], dtype=float32), 0.20235527]. 
=============================================
[2019-04-05 12:49:39,660] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[49.026134]
 [49.004932]
 [49.527843]
 [50.15682 ]
 [50.690388]], R is [[48.48869705]
 [48.00381088]
 [47.52377319]
 [47.04853439]
 [46.57804871]].
[2019-04-05 12:49:40,020] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47500, global step 775052: loss -2.3896
[2019-04-05 12:49:40,021] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47500, global step 775052: learning rate 0.0000
[2019-04-05 12:49:40,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50500, global step 775559: loss 2.4853
[2019-04-05 12:49:40,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50500, global step 775559: learning rate 0.0000
[2019-04-05 12:49:40,983] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49500, global step 775629: loss 5.8410
[2019-04-05 12:49:40,984] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49500, global step 775630: learning rate 0.0000
[2019-04-05 12:49:41,247] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49000, global step 775780: loss -2.8103
[2019-04-05 12:49:41,250] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49000, global step 775781: learning rate 0.0000
[2019-04-05 12:49:41,497] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48500, global step 775933: loss -3.8955
[2019-04-05 12:49:41,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48500, global step 775933: learning rate 0.0000
[2019-04-05 12:49:44,886] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48000, global step 777974: loss 35.5541
[2019-04-05 12:49:44,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48000, global step 777974: learning rate 0.0000
[2019-04-05 12:49:46,958] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49000, global step 779291: loss 1.0491
[2019-04-05 12:49:46,959] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48000, global step 779291: loss 10.2911
[2019-04-05 12:49:46,961] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49000, global step 779291: learning rate 0.0000
[2019-04-05 12:49:46,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48000, global step 779292: learning rate 0.0000
[2019-04-05 12:49:47,144] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49000, global step 779394: loss 2.8355
[2019-04-05 12:49:47,145] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49000, global step 779394: learning rate 0.0000
[2019-04-05 12:49:47,716] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.7321433e-07 2.3999296e-01 1.2649671e-02 2.7944955e-01 4.6776906e-01
 6.0422548e-07 1.3727311e-04], sum to 1.0000
[2019-04-05 12:49:47,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7103
[2019-04-05 12:49:47,727] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.5, 0.0, 0.0, 20.5, 22.09483361988716, -0.4543603643023377, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 4127400.0000, 
sim time next is 4128000.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 21.0, 21.89675922385352, -0.4805428346856257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.36, 0.0, 0.0, 0.25, 0.32472993532112654, 0.33981905510479143, 1.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.71414286], dtype=float32), -0.007525204]. 
=============================================
[2019-04-05 12:49:47,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[65.84399 ]
 [66.16663 ]
 [66.39585 ]
 [66.708626]
 [67.00898 ]], R is [[65.58618927]
 [65.71604156]
 [65.91602325]
 [66.11400604]
 [66.38143921]].
[2019-04-05 12:49:47,911] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48000, global step 779862: loss 35.6402
[2019-04-05 12:49:47,912] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48000, global step 779862: learning rate 0.0000
[2019-04-05 12:49:48,127] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-05 12:49:48,130] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:49:48,130] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:49:48,130] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:49:48,131] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:49:48,131] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:49:48,132] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:49:48,142] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run40
[2019-04-05 12:49:48,158] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run40
[2019-04-05 12:49:48,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run40
[2019-04-05 12:50:18,234] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.107951224]
[2019-04-05 12:50:18,234] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.466666666666669, 75.0, 0.0, 0.0, 19.5, 19.36969491347146, -0.9012008755522464, 0.0, 1.0, 0.0]
[2019-04-05 12:50:18,234] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:50:18,235] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4215492e-08 3.1108612e-01 5.0526196e-03 4.5962003e-01 2.2423838e-01
 2.4148044e-08 2.8187078e-06], sampled 0.6596859877382559
[2019-04-05 12:50:34,033] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.107951224]
[2019-04-05 12:50:34,033] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.578280127, 70.38941765, 0.0, 0.0, 19.0, 19.28286886367573, -1.082648399127211, 0.0, 1.0, 0.0]
[2019-04-05 12:50:34,033] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:50:34,034] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2700110e-07 6.5957952e-01 5.9906812e-03 1.8945234e-01 1.4496502e-01
 8.7768804e-08 1.2251386e-05], sampled 0.7156616004175712
[2019-04-05 12:50:58,522] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.107951224]
[2019-04-05 12:50:58,522] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [3.45, 49.0, 37.0, 87.0, 19.0, 24.10666696656534, 0.2728945352144327, 0.0, 1.0, 0.0]
[2019-04-05 12:50:58,523] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:50:58,523] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.9148360e-10 4.1930452e-01 1.8548131e-03 3.9098501e-01 1.8785536e-01
 5.2603766e-10 3.1253779e-07], sampled 0.6956384533369214
[2019-04-05 12:51:01,183] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6306.2536 149573367.7103 -1720.9007
[2019-04-05 12:51:01,475] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.107951224]
[2019-04-05 12:51:01,475] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.75, 69.5, 0.0, 0.0, 19.5, 21.09050587321278, -0.708571182397235, 0.0, 1.0, 0.0]
[2019-04-05 12:51:01,475] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:51:01,476] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3628289e-08 3.2397708e-01 6.1827186e-03 4.2371279e-01 2.4612351e-01
 2.8836844e-08 3.8283179e-06], sampled 0.6063328470256496
[2019-04-05 12:51:10,765] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5742.9692 177474539.6622 -2375.2613
[2019-04-05 12:51:17,723] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5789.4483 190414910.9243 -2310.0431
[2019-04-05 12:51:18,748] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 780000, evaluation results [780000.0, 5742.96918110766, 177474539.66224694, -2375.261327698572, 6306.253617766972, 149573367.71025693, -1720.9007039303226, 5789.448339040156, 190414910.9242674, -2310.0431290336123]
[2019-04-05 12:51:19,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5160304e-06 6.7505646e-01 2.2092606e-03 1.9045782e-01 1.3223803e-01
 1.0244287e-06 3.5845751e-05], sum to 1.0000
[2019-04-05 12:51:19,690] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8098
[2019-04-05 12:51:19,715] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.166666666666666, 41.66666666666667, 0.0, 0.0, 19.0, 19.79196052334258, -0.8989105304696391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3957000.0000, 
sim time next is 3957600.0000, 
raw observation next is [-6.333333333333333, 42.33333333333334, 0.0, 0.0, 19.0, 19.70520609130277, -0.9214575459444129, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.28716528162511545, 0.42333333333333345, 0.0, 0.0, 0.08333333333333333, 0.14210050760856419, 0.19284748468519572, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.13101585], dtype=float32), -0.6805707]. 
=============================================
[2019-04-05 12:51:19,731] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48500, global step 780609: loss 0.2076
[2019-04-05 12:51:19,735] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48500, global step 780610: learning rate 0.0000
[2019-04-05 12:51:20,549] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49000, global step 781108: loss -0.5639
[2019-04-05 12:51:20,551] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49000, global step 781109: learning rate 0.0000
[2019-04-05 12:51:21,285] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49500, global step 781560: loss 4.5698
[2019-04-05 12:51:21,301] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49500, global step 781560: learning rate 0.0000
[2019-04-05 12:51:22,333] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49500, global step 782185: loss 1.8813
[2019-04-05 12:51:22,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49500, global step 782185: learning rate 0.0000
[2019-04-05 12:51:22,617] A3C_AGENT_WORKER-Thread-9 INFO:Local step 48000, global step 782339: loss 39.1116
[2019-04-05 12:51:22,617] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 48000, global step 782339: learning rate 0.0000
[2019-04-05 12:51:22,789] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50000, global step 782431: loss 19.7289
[2019-04-05 12:51:22,790] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50000, global step 782431: learning rate 0.0000
[2019-04-05 12:51:24,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7175064e-07 4.1528499e-01 3.5996228e-02 5.2367586e-01 2.5020467e-02
 4.5168781e-07 2.1741049e-05], sum to 1.0000
[2019-04-05 12:51:24,120] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7562
[2019-04-05 12:51:24,133] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 19.90631424288658, -0.9406625772925912, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4064400.0000, 
sim time next is 4065000.0000, 
raw observation next is [-6.0, 41.00000000000001, 0.0, 0.0, 19.0, 19.88088430291449, -0.9479406967942045, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.4100000000000001, 0.0, 0.0, 0.08333333333333333, 0.15674035857620763, 0.18401976773526518, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0566077], dtype=float32), -0.59849304]. 
=============================================
[2019-04-05 12:51:24,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[56.241394]
 [57.592346]
 [59.203255]
 [61.17658 ]
 [63.251747]], R is [[55.58059692]
 [56.02479172]
 [56.4645462 ]
 [56.89990234]
 [57.1880455 ]].
[2019-04-05 12:51:24,352] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49000, global step 783417: loss -1.8388
[2019-04-05 12:51:24,353] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49000, global step 783417: learning rate 0.0000
[2019-04-05 12:51:24,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:51:24,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:51:24,490] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47500, global step 783512: loss -0.2267
[2019-04-05 12:51:24,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run7
[2019-04-05 12:51:24,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47500, global step 783513: learning rate 0.0000
[2019-04-05 12:51:24,638] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48000, global step 783585: loss 56.4430
[2019-04-05 12:51:24,646] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48000, global step 783588: learning rate 0.0000
[2019-04-05 12:51:25,588] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49500, global step 784089: loss 0.3273
[2019-04-05 12:51:25,590] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49500, global step 784089: learning rate 0.0000
[2019-04-05 12:51:25,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8185021e-06 4.0281594e-01 2.7703936e-03 1.8452743e-01 4.0978456e-01
 3.6886795e-06 9.6127827e-05], sum to 1.0000
[2019-04-05 12:51:25,940] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-05 12:51:25,948] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 25.66666666666667, 13.33333333333334, 128.6666666666667, 19.0, 20.52922643991594, -0.7505889811607757, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4038600.0000, 
sim time next is 4039200.0000, 
raw observation next is [-3.0, 26.0, 0.0, 0.0, 19.5, 20.51309406744846, -0.7678675983166726, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.26, 0.0, 0.0, 0.125, 0.209424505620705, 0.24404413389444246, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01829299], dtype=float32), 0.5651209]. 
=============================================
[2019-04-05 12:51:26,134] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.46406647e-08 8.67482185e-01 1.21658912e-03 2.40143221e-02
 1.07285425e-01 4.63500294e-10 1.50481128e-06], sum to 1.0000
[2019-04-05 12:51:26,134] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9688
[2019-04-05 12:51:26,144] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 21.039249252777, -0.667187004274421, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3477000.0000, 
sim time next is 3477600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.5, 20.99445484702846, -0.6871073265796649, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.125, 0.2495379039190384, 0.27096422447344504, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.33202046], dtype=float32), -1.864601]. 
=============================================
[2019-04-05 12:51:27,921] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.8041086e-07 8.3093256e-01 4.0127370e-03 9.5557831e-02 6.9417581e-02
 3.4117173e-07 7.8163168e-05], sum to 1.0000
[2019-04-05 12:51:27,928] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0025
[2019-04-05 12:51:27,939] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.166666666666667, 72.0, 0.0, 0.0, 19.0, 20.02479589157097, -0.9063753828245557, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3820200.0000, 
sim time next is 3820800.0000, 
raw observation next is [-4.333333333333334, 73.0, 0.0, 0.0, 19.0, 19.94774261851888, -0.9255329799001187, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.73, 0.0, 0.0, 0.08333333333333333, 0.16231188487657336, 0.1914890066999604, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50466955], dtype=float32), 0.7423052]. 
=============================================
[2019-04-05 12:51:28,991] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48500, global step 786145: loss -0.3902
[2019-04-05 12:51:28,993] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48500, global step 786146: learning rate 0.0000
[2019-04-05 12:51:29,814] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.5149018e-08 5.0040144e-02 1.4584332e-02 7.4640167e-01 1.8891834e-01
 1.1929669e-08 5.5461031e-05], sum to 1.0000
[2019-04-05 12:51:29,816] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5987
[2019-04-05 12:51:29,826] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 20.51468355974312, -0.7856862284750014, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3476400.0000, 
sim time next is 3477000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 20.43639804297794, -0.7950831016884571, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.2030331702481618, 0.23497229943718098, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40784013], dtype=float32), -0.92311233]. 
=============================================
[2019-04-05 12:51:29,837] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.00822]
 [68.25236]
 [68.50269]
 [68.74515]
 [68.94526]], R is [[67.82450867]
 [68.14626312]
 [68.39337158]
 [68.63801575]
 [68.88021088]].
[2019-04-05 12:51:31,370] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49000, global step 787679: loss -0.9826
[2019-04-05 12:51:31,373] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49000, global step 787679: learning rate 0.0000
[2019-04-05 12:51:31,455] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49500, global step 787727: loss 0.9741
[2019-04-05 12:51:31,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49500, global step 787728: learning rate 0.0000
[2019-04-05 12:51:31,622] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48500, global step 787819: loss -3.1461
[2019-04-05 12:51:31,625] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48500, global step 787820: learning rate 0.0000
[2019-04-05 12:51:31,758] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48500, global step 787887: loss -2.5553
[2019-04-05 12:51:31,759] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48500, global step 787887: learning rate 0.0000
[2019-04-05 12:51:32,254] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50000, global step 788184: loss 1.5783
[2019-04-05 12:51:32,257] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50000, global step 788185: learning rate 0.0000
[2019-04-05 12:51:32,285] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7125922e-07 2.4463794e-01 4.9146044e-04 6.7808223e-01 7.6787591e-02
 2.5921345e-09 6.3059161e-07], sum to 1.0000
[2019-04-05 12:51:32,285] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6618
[2019-04-05 12:51:32,308] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.4, 44.0, 0.0, 0.0, 19.0, 24.74171062188424, 0.2281456638947268, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [12.33333333333333, 45.0, 0.0, 0.0, 19.0, 24.70656277396404, 0.1397362668720314, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8042474607571561, 0.45, 0.0, 0.0, 0.08333333333333333, 0.5588802311636701, 0.5465787556240105, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8723128], dtype=float32), -1.9011725]. 
=============================================
[2019-04-05 12:51:32,470] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49500, global step 788313: loss 3.0269
[2019-04-05 12:51:32,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49500, global step 788314: learning rate 0.0000
[2019-04-05 12:51:32,681] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1406414e-08 6.8278235e-01 1.5588339e-01 1.0747355e-01 5.3858299e-02
 2.4483290e-08 2.3957341e-06], sum to 1.0000
[2019-04-05 12:51:32,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7139
[2019-04-05 12:51:32,692] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 100.0, 0.0, 0.0, 19.5, 19.849761161389, -0.804422770400583, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3202200.0000, 
sim time next is 3202800.0000, 
raw observation next is [0.3333333333333334, 100.0, 0.0, 0.0, 19.0, 19.7804524039097, -0.8266822213140174, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4718374884579871, 1.0, 0.0, 0.0, 0.08333333333333333, 0.1483710336591416, 0.2244392595619942, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3692085], dtype=float32), 0.002215478]. 
=============================================
[2019-04-05 12:51:33,691] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49500, global step 789060: loss 0.1838
[2019-04-05 12:51:33,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49500, global step 789060: learning rate 0.0000
[2019-04-05 12:51:34,263] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50000, global step 789389: loss 2.4826
[2019-04-05 12:51:34,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50000, global step 789389: learning rate 0.0000
[2019-04-05 12:51:34,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6245069e-12 1.1633736e-02 4.9244598e-05 7.1345073e-01 2.7486622e-01
 4.9637944e-10 4.3723823e-08], sum to 1.0000
[2019-04-05 12:51:34,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-05 12:51:34,831] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.9, 58.5, 229.0, 385.0, 19.5, 21.28808480585852, -0.5471436418693992, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4285800.0000, 
sim time next is 4286400.0000, 
raw observation next is [6.866666666666667, 59.0, 210.1666666666667, 430.0, 19.5, 21.29817564463628, -0.5364976116053303, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6528162511542014, 0.59, 0.7005555555555557, 0.47513812154696133, 0.125, 0.2748479703863567, 0.3211674627982232, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.29886127], dtype=float32), 0.70115006]. 
=============================================
[2019-04-05 12:51:35,869] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50500, global step 790340: loss -1.4418
[2019-04-05 12:51:35,871] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50500, global step 790341: learning rate 0.0000
[2019-04-05 12:51:36,058] A3C_AGENT_WORKER-Thread-9 INFO:Local step 48500, global step 790460: loss 1.6464
[2019-04-05 12:51:36,059] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 48500, global step 790460: learning rate 0.0000
[2019-04-05 12:51:36,836] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48500, global step 790962: loss -1.3496
[2019-04-05 12:51:36,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48500, global step 790962: learning rate 0.0000
[2019-04-05 12:51:36,880] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50000, global step 790987: loss 11.5047
[2019-04-05 12:51:36,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50000, global step 790987: learning rate 0.0000
[2019-04-05 12:51:37,079] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49500, global step 791107: loss 3.4310
[2019-04-05 12:51:37,081] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49500, global step 791107: learning rate 0.0000
[2019-04-05 12:51:38,027] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48000, global step 791710: loss 65.6022
[2019-04-05 12:51:38,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48000, global step 791710: learning rate 0.0000
[2019-04-05 12:51:39,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3718292e-08 2.5140572e-01 5.2337018e-03 6.9195777e-01 5.1360447e-02
 6.6477361e-07 4.1644136e-05], sum to 1.0000
[2019-04-05 12:51:39,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5700
[2019-04-05 12:51:39,356] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.383333333333333, 44.5, 109.6666666666667, 711.3333333333333, 19.0, 21.6834597881318, -0.5685867726510974, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4355400.0000, 
sim time next is 4356000.0000, 
raw observation next is [10.0, 42.0, 111.0, 728.5, 19.0, 21.84794483569288, -0.5359134528012885, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.739612188365651, 0.42, 0.37, 0.8049723756906078, 0.08333333333333333, 0.3206620696410732, 0.3213621823995705, 1.0, 1.0, 0.0], 
reward next is 0.6409, 
noisyNet noise sample is [array([-0.70216125], dtype=float32), 0.08571835]. 
=============================================
[2019-04-05 12:51:39,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.49497 ]
 [65.09672 ]
 [64.61115 ]
 [64.06867 ]
 [63.781967]], R is [[66.6665802 ]
 [66.31404877]
 [65.65090942]
 [64.99440002]
 [64.34445953]].
[2019-04-05 12:51:40,849] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49000, global step 793492: loss -0.7038
[2019-04-05 12:51:40,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49000, global step 793492: learning rate 0.0000
[2019-04-05 12:51:40,876] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7898356e-09 9.1883755e-01 5.3601013e-04 1.2762283e-02 6.7864157e-02
 7.9466544e-10 4.6986067e-08], sum to 1.0000
[2019-04-05 12:51:40,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7018
[2019-04-05 12:51:40,910] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 19.0, 18.63195064946365, -1.007402770708652, 0.0, 1.0, 195495.8204008711], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4738800.0000, 
sim time next is 4739400.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 19.0, 18.77809851586127, -0.9861567652653876, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4210526315789474, 0.815, 0.0, 0.0, 0.08333333333333333, 0.06484154298843918, 0.1712810782448708, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31130272], dtype=float32), 0.017076995]. 
=============================================
[2019-04-05 12:51:42,327] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.87643695e-09 2.97259033e-01 2.16578282e-02 1.11178584e-01
 5.69903255e-01 2.07745354e-09 1.25451118e-06], sum to 1.0000
[2019-04-05 12:51:42,332] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7999
[2019-04-05 12:51:42,349] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 59.16666666666666, 0.0, 0.0, 20.0, 19.84599001947036, -0.8372446948663671, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3877800.0000, 
sim time next is 3878400.0000, 
raw observation next is [-1.0, 58.33333333333334, 0.0, 0.0, 20.5, 19.76621714310949, -0.8376867502490636, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5833333333333335, 0.0, 0.0, 0.20833333333333334, 0.1471847619257908, 0.22077108325031214, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.935362], dtype=float32), 0.5104227]. 
=============================================
[2019-04-05 12:51:43,619] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50000, global step 795258: loss 1.1497
[2019-04-05 12:51:43,622] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50000, global step 795259: learning rate 0.0000
[2019-04-05 12:51:43,761] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49000, global step 795352: loss 1.4631
[2019-04-05 12:51:43,770] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49000, global step 795359: learning rate 0.0000
[2019-04-05 12:51:44,043] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50000, global step 795541: loss 4.4099
[2019-04-05 12:51:44,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50000, global step 795541: learning rate 0.0000
[2019-04-05 12:51:45,036] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49000, global step 796142: loss 0.5466
[2019-04-05 12:51:45,054] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49000, global step 796142: learning rate 0.0000
[2019-04-05 12:51:45,298] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50500, global step 796299: loss -1.5471
[2019-04-05 12:51:45,299] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50500, global step 796299: learning rate 0.0000
[2019-04-05 12:51:45,411] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50000, global step 796378: loss 1.7094
[2019-04-05 12:51:45,414] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50000, global step 796378: learning rate 0.0000
[2019-04-05 12:51:45,569] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49500, global step 796467: loss 1.2668
[2019-04-05 12:51:45,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49500, global step 796467: learning rate 0.0000
[2019-04-05 12:51:46,518] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50500, global step 797064: loss 2.5317
[2019-04-05 12:51:46,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50500, global step 797065: learning rate 0.0000
[2019-04-05 12:51:47,946] A3C_AGENT_WORKER-Thread-9 INFO:Local step 49000, global step 797976: loss -0.1081
[2019-04-05 12:51:47,948] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 49000, global step 797977: learning rate 0.0000
[2019-04-05 12:51:48,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:51:48,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:51:48,169] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run7
[2019-04-05 12:51:49,008] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49000, global step 798445: loss 0.1607
[2019-04-05 12:51:49,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49000, global step 798445: learning rate 0.0000
[2019-04-05 12:51:49,039] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50000, global step 798457: loss 5.4906
[2019-04-05 12:51:49,046] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50000, global step 798459: learning rate 0.0000
[2019-04-05 12:51:50,132] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50500, global step 799065: loss 0.2793
[2019-04-05 12:51:50,137] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50500, global step 799065: learning rate 0.0000
[2019-04-05 12:51:51,275] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8896419e-05 4.5949167e-01 2.5113640e-02 4.6551582e-01 4.9654663e-02
 4.6568090e-07 2.0499968e-04], sum to 1.0000
[2019-04-05 12:51:51,285] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8755
[2019-04-05 12:51:51,304] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 18.53596866161638, -1.177092582209767, 0.0, 1.0, 108504.2695667828], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3984600.0000, 
sim time next is 3985200.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 18.55809857525117, -1.161214474163975, 0.0, 1.0, 59109.061225789], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.046508214604264055, 0.11292850861200836, 0.0, 1.0, 0.28147172012280475], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5487397], dtype=float32), -0.09120991]. 
=============================================
[2019-04-05 12:51:51,416] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48500, global step 799896: loss -1.7617
[2019-04-05 12:51:51,417] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48500, global step 799896: learning rate 0.0000
[2019-04-05 12:51:51,595] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 12:51:51,597] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:51:51,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:51:51,598] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:51:51,598] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:51:51,598] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:51:51,598] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:51:51,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run41
[2019-04-05 12:51:51,617] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run41
[2019-04-05 12:51:51,632] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run41
[2019-04-05 12:52:45,479] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11080351]
[2019-04-05 12:52:45,479] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.0, 85.0, 0.0, 0.0, 19.0, 18.70710311000678, -1.082428529641075, 0.0, 1.0, 0.0]
[2019-04-05 12:52:45,479] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:52:45,480] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.4506613e-10 5.3552341e-01 1.6874840e-03 2.4402836e-01 2.1876042e-01
 1.3970493e-09 3.9996999e-07], sampled 0.33018811146069404
[2019-04-05 12:53:03,165] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6321.6872 147986306.3108 -1767.8182
[2019-04-05 12:53:14,517] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5858.6703 177092134.4305 -2362.0628
[2019-04-05 12:53:19,525] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5811.1187 190043486.6724 -2404.2366
[2019-04-05 12:53:20,550] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 800000, evaluation results [800000.0, 5858.670347082221, 177092134.43047038, -2362.062809123178, 6321.687189448656, 147986306.310843, -1767.818180544704, 5811.118713598264, 190043486.6724489, -2404.2366431536448]
[2019-04-05 12:53:23,434] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49500, global step 801685: loss 1.0168
[2019-04-05 12:53:23,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49500, global step 801687: learning rate 0.0000
[2019-04-05 12:53:26,032] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50500, global step 803184: loss 0.5575
[2019-04-05 12:53:26,034] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50500, global step 803185: learning rate 0.0000
[2019-04-05 12:53:26,202] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50500, global step 803278: loss -1.6885
[2019-04-05 12:53:26,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50500, global step 803278: learning rate 0.0000
[2019-04-05 12:53:26,724] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50000, global step 803586: loss 6.0229
[2019-04-05 12:53:26,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50000, global step 803586: learning rate 0.0000
[2019-04-05 12:53:26,949] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49500, global step 803729: loss -1.5664
[2019-04-05 12:53:26,951] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49500, global step 803731: learning rate 0.0000
[2019-04-05 12:53:27,176] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49500, global step 803868: loss 0.3748
[2019-04-05 12:53:27,178] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49500, global step 803868: learning rate 0.0000
[2019-04-05 12:53:27,382] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50500, global step 804000: loss 0.2153
[2019-04-05 12:53:27,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50500, global step 804000: learning rate 0.0000
[2019-04-05 12:53:27,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:27,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:27,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:27,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:27,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run7
[2019-04-05 12:53:27,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run7
[2019-04-05 12:53:30,184] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50500, global step 805468: loss 0.5853
[2019-04-05 12:53:30,186] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50500, global step 805469: learning rate 0.0000
[2019-04-05 12:53:31,752] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49500, global step 806338: loss 1.5784
[2019-04-05 12:53:31,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49500, global step 806339: learning rate 0.0000
[2019-04-05 12:53:31,871] A3C_AGENT_WORKER-Thread-9 INFO:Local step 49500, global step 806405: loss -1.1925
[2019-04-05 12:53:31,873] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 49500, global step 806406: learning rate 0.0000
[2019-04-05 12:53:32,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:32,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:32,170] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run7
[2019-04-05 12:53:33,233] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49000, global step 807116: loss 1.3463
[2019-04-05 12:53:33,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49000, global step 807116: learning rate 0.0000
[2019-04-05 12:53:35,429] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50000, global step 808484: loss 0.1794
[2019-04-05 12:53:35,429] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50000, global step 808484: learning rate 0.0000
[2019-04-05 12:53:35,739] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7470452e-11 1.6424820e-01 2.6537044e-04 1.3925426e-01 6.9623214e-01
 2.9311983e-10 1.7982543e-08], sum to 1.0000
[2019-04-05 12:53:35,739] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8088
[2019-04-05 12:53:35,803] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.6, 82.0, 34.0, 0.0, 21.0, 19.52234448534687, -0.8261511846618123, 0.0, 1.0, 196961.4458739178], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 57600.0000, 
sim time next is 58200.0000, 
raw observation next is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 21.5, 19.71773283903258, -0.7437562323584829, 0.0, 1.0, 198663.1611985295], 
processed observation next is [0.0, 0.6956521739130435, 0.6403508771929826, 0.8266666666666667, 0.09555555555555553, 0.0, 0.2916666666666667, 0.14314440325271485, 0.25208125588050573, 0.0, 1.0, 0.946015053326331], 
reward next is 0.6429, 
noisyNet noise sample is [array([1.1786298], dtype=float32), -0.5770156]. 
=============================================
[2019-04-05 12:53:37,383] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50000, global step 809647: loss 2.0407
[2019-04-05 12:53:37,390] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50000, global step 809647: learning rate 0.0000
[2019-04-05 12:53:37,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2116264e-08 1.0421348e-01 2.7847683e-04 3.5601854e-01 5.3948766e-01
 3.7426577e-08 1.7335269e-06], sum to 1.0000
[2019-04-05 12:53:37,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9559
[2019-04-05 12:53:37,575] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.5833333333333333, 73.0, 0.0, 0.0, 20.0, 20.02932816487701, -0.8072885752715613, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4495800.0000, 
sim time next is 4496400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 20.5, 20.03386351875141, -0.7951941028798951, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.20833333333333334, 0.1694886265626175, 0.23493529904003496, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.5366782], dtype=float32), -0.0951098]. 
=============================================
[2019-04-05 12:53:38,120] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50000, global step 810076: loss 3.6091
[2019-04-05 12:53:38,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50000, global step 810077: learning rate 0.0000
[2019-04-05 12:53:38,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:38,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:38,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:38,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:38,450] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run7
[2019-04-05 12:53:38,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run7
[2019-04-05 12:53:38,572] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1843753e-10 2.8441301e-01 7.3131680e-04 7.2154365e-03 7.0763975e-01
 4.3822734e-10 4.3345887e-07], sum to 1.0000
[2019-04-05 12:53:38,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6600
[2019-04-05 12:53:38,589] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 35.5, 82.0, 549.0, 22.0, 21.57534613068727, -0.4580217707189518, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 4811400.0000, 
sim time next is 4812000.0000, 
raw observation next is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 22.5, 21.58328993573543, -0.4549668495152734, 0.0, 1.0, 70238.55193903326], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.35, 0.24611111111111114, 0.5395948434622467, 0.375, 0.2986074946446191, 0.34834438349490887, 0.0, 1.0, 0.33446929494777744], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.6493047], dtype=float32), 0.16517371]. 
=============================================
[2019-04-05 12:53:38,600] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[95.10948]
 [95.15097]
 [95.10013]
 [95.21651]
 [95.13718]], R is [[94.9806366 ]
 [94.60225677]
 [94.29909515]
 [94.07038879]
 [93.70111084]].
[2019-04-05 12:53:38,603] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50500, global step 810372: loss -1.3725
[2019-04-05 12:53:38,604] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50500, global step 810373: learning rate 0.0000
[2019-04-05 12:53:39,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:39,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:39,137] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run7
[2019-04-05 12:53:42,022] A3C_AGENT_WORKER-Thread-9 INFO:Local step 50000, global step 811956: loss 7.4701
[2019-04-05 12:53:42,024] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 50000, global step 811956: learning rate 0.0000
[2019-04-05 12:53:42,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:42,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:42,730] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run7
[2019-04-05 12:53:42,818] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50000, global step 812342: loss 4.5625
[2019-04-05 12:53:42,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50000, global step 812342: learning rate 0.0000
[2019-04-05 12:53:47,427] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49500, global step 814592: loss 0.4710
[2019-04-05 12:53:47,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49500, global step 814592: learning rate 0.0000
[2019-04-05 12:53:47,704] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50500, global step 814758: loss -0.5777
[2019-04-05 12:53:47,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50500, global step 814758: learning rate 0.0000
[2019-04-05 12:53:49,761] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50500, global step 815878: loss -0.3789
[2019-04-05 12:53:49,764] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50500, global step 815881: learning rate 0.0000
[2019-04-05 12:53:50,661] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50500, global step 816397: loss 0.4810
[2019-04-05 12:53:50,663] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50500, global step 816398: learning rate 0.0000
[2019-04-05 12:53:51,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:53:51,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:51,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run7
[2019-04-05 12:53:55,269] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50500, global step 818720: loss 1.4146
[2019-04-05 12:53:55,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50500, global step 818722: learning rate 0.0000
[2019-04-05 12:53:55,297] A3C_AGENT_WORKER-Thread-9 INFO:Local step 50500, global step 818737: loss -1.5245
[2019-04-05 12:53:55,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 50500, global step 818737: learning rate 0.0000
[2019-04-05 12:53:57,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5204764e-08 2.9555157e-01 3.0692311e-03 2.1099709e-01 4.9037477e-01
 1.2267928e-07 7.2556809e-06], sum to 1.0000
[2019-04-05 12:53:57,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7397
[2019-04-05 12:53:57,064] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 46.66666666666667, 0.0, 0.0, 19.0, 20.38729960694458, -0.8231393242215405, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 5032200.0000, 
sim time next is 5032800.0000, 
raw observation next is [-1.0, 46.0, 0.0, 0.0, 19.5, 20.30396621225585, -0.8364032693906654, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.46, 0.0, 0.0, 0.125, 0.19199718435465427, 0.22119891020311153, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.0413015], dtype=float32), -0.8074178]. 
=============================================
[2019-04-05 12:53:57,595] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-05 12:53:57,595] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:53:57,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:57,596] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:53:57,598] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:57,599] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:53:57,599] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:53:57,607] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run42
[2019-04-05 12:53:57,621] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run42
[2019-04-05 12:53:57,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run42
[2019-04-05 12:54:26,340] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11143358]
[2019-04-05 12:54:26,341] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.533333333333333, 73.66666666666666, 0.0, 0.0, 19.0, 21.15788527337717, -0.4923793122870239, 0.0, 1.0, 0.0]
[2019-04-05 12:54:26,341] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:54:26,342] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.9698872e-10 6.3982338e-01 1.6714698e-03 1.4823224e-01 2.1027263e-01
 5.1089710e-10 2.5967773e-07], sampled 0.1457304774338921
[2019-04-05 12:55:14,646] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6439.9369 153927176.4772 -1338.7865
[2019-04-05 12:55:21,808] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5797.0813 183496396.5874 -2066.4935
[2019-04-05 12:55:23,312] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11143358]
[2019-04-05 12:55:23,312] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.326638336833334, 47.20835529333333, 208.9536849, 657.4859559333333, 19.0, 23.8425011992504, 0.02166981700968488, 1.0, 1.0, 0.0]
[2019-04-05 12:55:23,312] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:55:23,313] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.7834571e-09 7.6081651e-01 1.5768309e-03 1.0731192e-01 1.3029374e-01
 2.5608697e-09 9.5695134e-07], sampled 0.9851297916472068
[2019-04-05 12:55:29,999] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5649.8104 197291414.3440 -1901.5771
[2019-04-05 12:55:31,024] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 820000, evaluation results [820000.0, 5797.0812818788, 183496396.58742693, -2066.49349661247, 6439.936940146722, 153927176.47721842, -1338.786548829338, 5649.810403460935, 197291414.3440355, -1901.57711848726]
[2019-04-05 12:55:31,359] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50000, global step 820183: loss 1.0777
[2019-04-05 12:55:31,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50000, global step 820184: learning rate 0.0000
[2019-04-05 12:55:32,410] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8435223e-08 3.5021952e-01 3.3068039e-02 3.3127806e-01 2.8543389e-01
 9.4587751e-09 4.5626726e-07], sum to 1.0000
[2019-04-05 12:55:32,411] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1166
[2019-04-05 12:55:32,422] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.9, 65.66666666666667, 0.0, 0.0, 20.5, 20.36490408281809, -0.8486005370535964, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 677400.0000, 
sim time next is 678000.0000, 
raw observation next is [-3.0, 66.33333333333334, 0.0, 0.0, 19.5, 20.33209955174166, -0.865014024161039, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.3795013850415513, 0.6633333333333334, 0.0, 0.0, 0.125, 0.194341629311805, 0.21166199194632032, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.7650291], dtype=float32), 0.022796066]. 
=============================================
[2019-04-05 12:55:32,431] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.91448 ]
 [84.960144]
 [84.92939 ]
 [85.03231 ]
 [85.09479 ]], R is [[84.88663483]
 [84.8234787 ]
 [84.8323822 ]
 [84.84120178]
 [84.92136383]].
[2019-04-05 12:55:34,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:55:34,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:55:34,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run7
[2019-04-05 12:55:35,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:55:35,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:55:35,166] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run7
[2019-04-05 12:55:36,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:55:36,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:55:36,660] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run7
[2019-04-05 12:55:40,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9072655e-06 3.6849540e-02 1.3291541e-02 3.1430247e-01 6.3547122e-01
 7.4454651e-06 7.2880684e-05], sum to 1.0000
[2019-04-05 12:55:40,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-05 12:55:40,064] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 20.5, 19.96005454566977, -0.8618565993720763, 0.0, 1.0, 18757.25038873675], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 21.0, 20.03070615510774, -0.8460265832536827, 0.0, 1.0, 77394.84653026031], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.25, 0.16922551292564503, 0.2179911389154391, 0.0, 1.0, 0.36854688823933485], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.7361715], dtype=float32), -1.3001939]. 
=============================================
[2019-04-05 12:55:40,073] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[47.82925 ]
 [43.22222 ]
 [38.234932]
 [33.53679 ]
 [28.711884]], R is [[50.99600983]
 [51.27176285]
 [51.61618805]
 [51.88573837]
 [52.22402573]].
[2019-04-05 12:55:40,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:55:40,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:55:40,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run7
[2019-04-05 12:55:42,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:55:42,261] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:55:42,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run7
[2019-04-05 12:55:42,863] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4197779e-12 8.8825214e-01 4.2294036e-03 4.4121690e-02 6.3396633e-02
 6.2623734e-10 1.9140727e-07], sum to 1.0000
[2019-04-05 12:55:42,864] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0051
[2019-04-05 12:55:42,879] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.0, 84.66666666666667, 54.83333333333334, 0.0, 19.5, 20.77668810311661, -0.6445170805130711, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 55200.0000, 
sim time next is 55800.0000, 
raw observation next is [6.9, 84.0, 50.0, 0.0, 19.0, 20.73093577409663, -0.6516795060744572, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6537396121883658, 0.84, 0.16666666666666666, 0.0, 0.08333333333333333, 0.22757798117471909, 0.2827734979751809, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0520849], dtype=float32), -2.5509355]. 
=============================================
[2019-04-05 12:55:43,641] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5517464e-09 2.0514748e-01 1.7745941e-05 1.4426386e-01 6.5057045e-01
 2.4511346e-10 3.5967741e-07], sum to 1.0000
[2019-04-05 12:55:43,645] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6112
[2019-04-05 12:55:43,696] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 37.0, 105.5, 729.5, 22.5, 21.36628045419319, -0.4592268261783976, 0.0, 1.0, 170638.4443213892], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4807200.0000, 
sim time next is 4807800.0000, 
raw observation next is [3.0, 37.0, 97.0, 727.0, 23.0, 21.42491465747505, -0.3936705366249447, 0.0, 1.0, 197481.5465244653], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3233333333333333, 0.8033149171270718, 0.4166666666666667, 0.28540955478958746, 0.3687764877916851, 0.0, 1.0, 0.940388316783168], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.49188414], dtype=float32), -0.41542396]. 
=============================================
[2019-04-05 12:55:44,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0230743e-08 8.9697617e-01 7.7307289e-03 5.9806708e-02 3.5486143e-02
 1.6408215e-09 2.3131021e-07], sum to 1.0000
[2019-04-05 12:55:44,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8597
[2019-04-05 12:55:44,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.516666666666667, 89.5, 0.0, 0.0, 19.0, 20.8715136710594, -0.5383967447491531, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1458600.0000, 
sim time next is 1459200.0000, 
raw observation next is [1.433333333333334, 90.0, 0.0, 0.0, 19.0, 20.82094575358247, -0.5442345281877817, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.502308402585411, 0.9, 0.0, 0.0, 0.08333333333333333, 0.23507881279853926, 0.31858849060407274, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08722428], dtype=float32), 0.9980075]. 
=============================================
[2019-04-05 12:55:44,233] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50500, global step 825983: loss 1.9359
[2019-04-05 12:55:44,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50500, global step 825983: learning rate 0.0000
[2019-04-05 12:55:47,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1623533e-09 7.7675921e-01 1.9873297e-03 1.9739933e-01 2.3852985e-02
 2.8083788e-08 1.1759075e-06], sum to 1.0000
[2019-04-05 12:55:47,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3759
[2019-04-05 12:55:47,806] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.266666666666667, 96.0, 0.0, 0.0, 19.0, 18.38893183579844, -1.282192446160005, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 505200.0000, 
sim time next is 505800.0000, 
raw observation next is [1.35, 96.0, 0.0, 0.0, 19.0, 18.36218573889809, -1.239067974769374, 0.0, 1.0, 197615.5072023907], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.96, 0.0, 0.0, 0.08333333333333333, 0.03018214490817428, 0.08697734174354199, 0.0, 1.0, 0.941026224773289], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0521047], dtype=float32), 0.013262459]. 
=============================================
[2019-04-05 12:55:50,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8643069e-11 3.5325173e-01 4.1343286e-03 5.6302309e-01 7.9590060e-02
 2.1389952e-10 8.5616421e-07], sum to 1.0000
[2019-04-05 12:55:50,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6974
[2019-04-05 12:55:50,507] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 80.0, 134.3333333333333, 552.3333333333333, 24.0, 23.34446942246475, -0.1943034223539636, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 568200.0000, 
sim time next is 568800.0000, 
raw observation next is [-1.2, 80.0, 132.5, 531.0, 24.0, 23.40119891029001, -0.1907822254068718, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.8, 0.44166666666666665, 0.5867403314917127, 0.5, 0.45009990919083415, 0.43640592486437607, 0.0, 1.0, 0.0], 
reward next is 0.2857, 
noisyNet noise sample is [array([-0.39127856], dtype=float32), 1.1577142]. 
=============================================
[2019-04-05 12:55:56,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 12:55:56,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:55:56,718] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run7
[2019-04-05 12:55:57,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.7769500e-07 5.5234247e-01 3.7695013e-03 4.0687734e-01 3.6964357e-02
 4.6037752e-07 4.5313158e-05], sum to 1.0000
[2019-04-05 12:55:57,678] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1863
[2019-04-05 12:55:57,688] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.7, 57.0, 0.0, 0.0, 19.0, 20.18185726023303, -0.8994686100677244, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 324000.0000, 
sim time next is 324600.0000, 
raw observation next is [-11.8, 58.0, 0.0, 0.0, 19.0, 20.08952945020238, -0.918643328308618, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.13573407202216065, 0.58, 0.0, 0.0, 0.08333333333333333, 0.17412745418353173, 0.19378555723046068, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8392726], dtype=float32), -0.02651229]. 
=============================================
[2019-04-05 12:56:03,927] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2548256e-07 8.9985138e-01 2.2131421e-03 5.6509960e-02 4.1393556e-02
 9.9200470e-08 3.1391319e-05], sum to 1.0000
[2019-04-05 12:56:03,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8229
[2019-04-05 12:56:03,942] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.16666666666667, 51.0, 0.0, 0.0, 19.0, 24.81740323110553, 0.2091423850653603, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1099200.0000, 
sim time next is 1099800.0000, 
raw observation next is [16.9, 51.5, 0.0, 0.0, 19.0, 24.76353566064953, 0.1592252806174356, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9307479224376731, 0.515, 0.0, 0.0, 0.08333333333333333, 0.5636279717207943, 0.5530750935391452, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5896931], dtype=float32), -1.150652]. 
=============================================
[2019-04-05 12:56:05,097] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4249496e-08 8.8422918e-01 3.8758891e-03 2.9481700e-02 8.2400635e-02
 1.5892414e-08 1.2492761e-05], sum to 1.0000
[2019-04-05 12:56:05,098] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5905
[2019-04-05 12:56:05,116] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 54.5, 0.0, 0.0, 19.0, 21.22714682145849, -0.6310285852707237, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 757800.0000, 
sim time next is 758400.0000, 
raw observation next is [-3.899999999999999, 54.0, 0.0, 0.0, 19.0, 21.0424581843046, -0.6513235816702799, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.54, 0.0, 0.0, 0.08333333333333333, 0.2535381820253833, 0.28289213944324004, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1309652], dtype=float32), 0.88305604]. 
=============================================
[2019-04-05 12:56:09,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.68006321e-11 1.07253596e-01 8.33050755e-04 8.01665615e-03
 8.83896768e-01 9.22026778e-13 5.04185538e-09], sum to 1.0000
[2019-04-05 12:56:09,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1718
[2019-04-05 12:56:09,314] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.55, 99.33333333333334, 6.333333333333332, 0.0, 20.5, 22.46851494256853, -0.09967456969448951, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1271400.0000, 
sim time next is 1272000.0000, 
raw observation next is [10.9, 98.66666666666667, 0.0, 0.0, 21.0, 22.43077409590606, -0.1096933788088391, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.7645429362880888, 0.9866666666666667, 0.0, 0.0, 0.25, 0.3692311746588383, 0.46343554039705365, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([2.4148533], dtype=float32), 0.4209782]. 
=============================================
[2019-04-05 12:56:09,335] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[110.68374 ]
 [110.7431  ]
 [110.771774]
 [110.78804 ]
 [110.729935]], R is [[110.07320404]
 [109.75818634]
 [109.51774597]
 [109.35114288]
 [109.186203  ]].
[2019-04-05 12:56:10,942] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-05 12:56:10,943] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:56:10,944] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:56:10,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:56:10,946] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:56:10,946] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:56:10,948] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:56:10,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run43
[2019-04-05 12:56:10,974] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run43
[2019-04-05 12:56:10,974] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run43
[2019-04-05 12:56:16,891] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:56:16,891] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.300000000000001, 81.0, 0.0, 0.0, 19.5, 19.69031753317453, -0.9698436222565704, 0.0, 1.0, 0.0]
[2019-04-05 12:56:16,891] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:56:16,892] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.9851797e-08 3.3148810e-01 5.2966266e-03 1.9934052e-01 4.6387151e-01
 3.3159822e-08 3.1814884e-06], sampled 0.27817425669384654
[2019-04-05 12:56:22,130] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:56:22,130] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-13.81666666666667, 72.0, 0.0, 0.0, 22.5, 19.01717882452517, -1.131289164731642, 0.0, 1.0, 53277.77579735441]
[2019-04-05 12:56:22,130] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:56:22,131] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.5954670e-08 5.4148495e-01 5.5918014e-03 8.2487218e-02 3.7043062e-01
 2.8637089e-08 5.3294020e-06], sampled 0.2602980739405545
[2019-04-05 12:56:22,317] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:56:22,317] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-15.0, 69.0, 0.0, 0.0, 21.0, 18.80530269032811, -1.216658840097029, 0.0, 1.0, 62573.65286936756]
[2019-04-05 12:56:22,317] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:56:22,318] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.9024472e-06 3.8535196e-01 2.1504492e-02 1.5459920e-01 4.3838841e-01
 2.5591764e-06 1.5051696e-04], sampled 0.764474444771015
[2019-04-05 12:56:32,176] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:56:32,176] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.4666666666666667, 67.33333333333334, 134.0, 377.1666666666666, 19.5, 19.55511288785049, -1.097138420507881, 1.0, 1.0, 0.0]
[2019-04-05 12:56:32,176] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:56:32,177] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5613142e-06 6.8281215e-01 8.5656727e-03 1.0412848e-01 2.0443362e-01
 1.2871994e-06 5.7248330e-05], sampled 0.6067220652507311
[2019-04-05 12:56:35,675] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:56:35,675] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.9, 58.0, 0.0, 0.0, 20.5, 19.77062320852875, -0.8853495010987594, 0.0, 1.0, 77304.34992109188]
[2019-04-05 12:56:35,675] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:56:35,677] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.4647424e-08 3.7218651e-01 8.3070034e-03 1.7272596e-01 4.4677162e-01
 9.5266316e-08 8.7397784e-06], sampled 0.24363921987296233
[2019-04-05 12:56:59,608] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:56:59,608] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.983333333333333, 46.83333333333334, 0.0, 0.0, 21.5, 20.41248355653638, -0.7195419549377351, 0.0, 1.0, 61467.89894916299]
[2019-04-05 12:56:59,608] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:56:59,609] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.73433600e-09 5.32273710e-01 3.58135672e-03 1.01302564e-01
 3.62839669e-01 1.00503126e-08 2.73832529e-06], sampled 0.21876150675093098
[2019-04-05 12:57:00,663] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:57:00,663] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.35, 57.5, 0.0, 0.0, 19.0, 19.2772406234063, -1.061707371806318, 0.0, 1.0, 0.0]
[2019-04-05 12:57:00,664] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 12:57:00,664] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.4406280e-08 5.1084816e-01 4.3714163e-03 1.3056819e-01 3.5420904e-01
 1.6654269e-08 3.0860660e-06], sampled 0.6939879358324916
[2019-04-05 12:57:23,059] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11001958]
[2019-04-05 12:57:23,059] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.65, 51.5, 261.0, 357.0, 19.5, 24.47246818682196, 0.4353798704466987, 0.0, 1.0, 0.0]
[2019-04-05 12:57:23,059] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:57:23,060] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.4680185e-11 5.3818661e-01 8.3905767e-04 9.4075546e-02 3.6689869e-01
 7.2149085e-11 6.7305884e-08], sampled 0.6414676604400555
[2019-04-05 12:57:25,928] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6420.9536 155211779.6261 -1393.9353
[2019-04-05 12:57:37,460] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6063.6773 182458326.0513 -1884.4781
[2019-04-05 12:57:44,688] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6023.2468 195663024.1892 -1827.1004
[2019-04-05 12:57:45,713] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 840000, evaluation results [840000.0, 6063.67729054698, 182458326.05126324, -1884.4780801431068, 6420.953588712375, 155211779.62608603, -1393.935285097552, 6023.246834049999, 195663024.1892276, -1827.1004493303744]
[2019-04-05 12:57:46,620] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.22771253e-05 5.89794636e-01 4.55215834e-02 7.88820013e-02
 2.85650700e-01 2.08693818e-05 1.17994416e-04], sum to 1.0000
[2019-04-05 12:57:46,622] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2395
[2019-04-05 12:57:46,681] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.1, 41.5, 0.0, 0.0, 19.0, 18.79219331867292, -1.244706078121462, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 459000.0000, 
sim time next is 459600.0000, 
raw observation next is [-8.0, 41.0, 0.0, 0.0, 19.0, 18.95523065690653, -1.226528534279981, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.24099722991689754, 0.41, 0.0, 0.0, 0.08333333333333333, 0.07960255474221078, 0.09115715524000634, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1795753], dtype=float32), 1.9037552]. 
=============================================
[2019-04-05 12:57:56,002] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.10228959e-09 4.35703546e-01 1.04489585e-03 5.21350503e-01
 4.18867953e-02 3.74570362e-07 1.37882325e-05], sum to 1.0000
[2019-04-05 12:57:56,003] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2047
[2019-04-05 12:57:56,016] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 89.33333333333334, 0.0, 0.0, 19.0, 19.81863316154964, -0.9184161439130897, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1446000.0000, 
sim time next is 1446600.0000, 
raw observation next is [1.1, 88.66666666666667, 0.0, 0.0, 19.0, 19.39464921981952, -0.9355385674728366, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8866666666666667, 0.0, 0.0, 0.08333333333333333, 0.11622076831829335, 0.1881538108423878, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5235232], dtype=float32), -2.016545]. 
=============================================
[2019-04-05 12:57:56,172] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0380545e-07 4.3146353e-02 2.5971475e-04 5.2101593e-02 9.0434408e-01
 2.9070932e-07 1.4755529e-04], sum to 1.0000
[2019-04-05 12:57:56,176] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1260
[2019-04-05 12:57:56,204] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 84.0, 77.0, 0.0, 20.0, 21.70782712424376, -0.6548952372592575, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 901800.0000, 
sim time next is 902400.0000, 
raw observation next is [1.1, 84.0, 80.33333333333334, 0.0, 20.5, 21.66283867974779, -0.6548581017920185, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.26777777777777784, 0.0, 0.20833333333333334, 0.30523655664564914, 0.28171396606932714, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.16205533], dtype=float32), -0.703235]. 
=============================================
[2019-04-05 12:57:57,961] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.06491410e-11 9.76139128e-01 1.12195085e-04 1.59780669e-03
 2.21507400e-02 1.75315429e-09 9.44864169e-08], sum to 1.0000
[2019-04-05 12:57:57,963] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0374
[2019-04-05 12:57:57,977] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 81.0, 0.0, 0.0, 19.0, 19.34952800827518, -1.003675400603212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1899600.0000, 
sim time next is 1900200.0000, 
raw observation next is [-7.299999999999999, 81.5, 0.0, 0.0, 19.0, 19.42592613329647, -1.002170795961681, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.2603878116343491, 0.815, 0.0, 0.0, 0.08333333333333333, 0.11882717777470593, 0.165943068012773, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0574248], dtype=float32), -0.7341671]. 
=============================================
[2019-04-05 12:57:59,870] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7116573e-09 1.9588420e-01 4.7175461e-04 1.3737859e-01 6.6626447e-01
 2.2640372e-09 1.0029598e-06], sum to 1.0000
[2019-04-05 12:57:59,870] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-05 12:57:59,906] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8999999999999999, 57.0, 81.0, 56.0, 19.0, 22.7938144939757, -0.344579734360889, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 657000.0000, 
sim time next is 657600.0000, 
raw observation next is [-0.8, 56.0, 81.33333333333333, 53.0, 19.5, 22.66594225897, -0.3688362462828363, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.4404432132963989, 0.56, 0.2711111111111111, 0.05856353591160221, 0.125, 0.3888285215808332, 0.3770545845723879, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.6197459], dtype=float32), -1.5900788]. 
=============================================
[2019-04-05 12:58:00,105] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1929861e-09 5.1091347e-02 5.6342338e-03 5.4297239e-01 4.0030167e-01
 6.5421013e-10 4.0141478e-07], sum to 1.0000
[2019-04-05 12:58:00,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7830
[2019-04-05 12:58:00,115] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 20.0, 21.14154729745133, -0.4873280296639022, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1544400.0000, 
sim time next is 1545000.0000, 
raw observation next is [7.516666666666667, 74.33333333333334, 0.0, 0.0, 20.5, 21.18736252332433, -0.491614236083654, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6708217913204063, 0.7433333333333334, 0.0, 0.0, 0.20833333333333334, 0.265613543610361, 0.33612858797211537, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.02878], dtype=float32), 1.0646917]. 
=============================================
[2019-04-05 12:58:00,134] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.51718 ]
 [88.968735]
 [87.52881 ]
 [86.6496  ]
 [85.41641 ]], R is [[90.00993347]
 [89.96697235]
 [89.99588013]
 [90.02449799]
 [90.05282593]].
[2019-04-05 12:58:00,571] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1207219e-09 7.5791234e-01 9.8157348e-04 1.5201661e-01 8.9088298e-02
 4.3819099e-09 1.1475345e-06], sum to 1.0000
[2019-04-05 12:58:00,571] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9250
[2019-04-05 12:58:00,583] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 20.10817529743846, -0.6904133385912999, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1368600.0000, 
sim time next is 1369200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.5, 20.03116980239102, -0.7038785402882691, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.125, 0.16926415019925165, 0.2653738199039103, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.41116986], dtype=float32), 0.57145274]. 
=============================================
[2019-04-05 12:58:01,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1559822e-11 3.0269159e-02 1.1000820e-04 2.4729727e-03 9.6714771e-01
 2.7257113e-10 1.4809200e-07], sum to 1.0000
[2019-04-05 12:58:01,863] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7613
[2019-04-05 12:58:01,879] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.71666666666667, 78.33333333333333, 0.0, 0.0, 22.0, 20.49357282633206, -0.5693252987860673, 0.0, 1.0, 188115.2759900131], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 1055400.0000, 
sim time next is 1056000.0000, 
raw observation next is [13.63333333333333, 78.66666666666667, 0.0, 0.0, 22.5, 20.69734719171839, -0.4963154581361449, 0.0, 1.0, 110998.9319868259], 
processed observation next is [1.0, 0.21739130434782608, 0.840258541089566, 0.7866666666666667, 0.0, 0.0, 0.375, 0.22477893264319912, 0.33456151395461836, 0.0, 1.0, 0.5285663427944091], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.36449167], dtype=float32), 0.53066725]. 
=============================================
[2019-04-05 12:58:01,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.314705]
 [84.59031 ]
 [83.53324 ]
 [82.20214 ]
 [82.04889 ]], R is [[85.15192413]
 [84.8718338 ]
 [84.66597748]
 [84.53360748]
 [84.47398376]].
[2019-04-05 12:58:02,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1324236e-10 9.4360635e-02 6.2419684e-04 7.0547771e-01 1.9953261e-01
 1.3444387e-09 4.8433640e-06], sum to 1.0000
[2019-04-05 12:58:02,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4055
[2019-04-05 12:58:02,071] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.4, 88.33333333333334, 0.0, 0.0, 19.5, 19.08485874006308, -0.9732753447847177, 0.0, 1.0, 158985.7130583059], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1741200.0000, 
sim time next is 1741800.0000, 
raw observation next is [-0.5, 87.66666666666666, 0.0, 0.0, 19.5, 19.06565263613003, -0.9564081374681429, 0.0, 1.0, 109786.2334688367], 
processed observation next is [0.0, 0.13043478260869565, 0.44875346260387816, 0.8766666666666666, 0.0, 0.0, 0.125, 0.08880438634416905, 0.18119728751061903, 0.0, 1.0, 0.5227915879468414], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.63627267], dtype=float32), 1.8612179]. 
=============================================
[2019-04-05 12:58:17,912] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4111530e-06 9.1038203e-01 1.2742811e-02 3.4732278e-02 4.2118888e-02
 4.3691722e-07 2.2239703e-05], sum to 1.0000
[2019-04-05 12:58:17,917] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5979
[2019-04-05 12:58:17,935] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 19.17323278649481, -1.136546553182335, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1997400.0000, 
sim time next is 1998000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 19.11500771466303, -1.16841991937617, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.09291730955525246, 0.11052669354127669, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.674736], dtype=float32), 0.83245605]. 
=============================================
[2019-04-05 12:58:17,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[60.614075]
 [60.894375]
 [61.58163 ]
 [61.699757]
 [62.107613]], R is [[60.85007477]
 [61.24157333]
 [61.62915802]
 [62.01286697]
 [62.39273834]].
[2019-04-05 12:58:19,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4993262e-11 8.9520556e-01 3.0792283e-04 6.2318710e-03 9.8254181e-02
 2.9856859e-10 5.1351168e-07], sum to 1.0000
[2019-04-05 12:58:19,893] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9318
[2019-04-05 12:58:19,912] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.35, 54.0, 87.0, 28.0, 19.0, 23.36817700736629, -0.2445388347163372, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1528200.0000, 
sim time next is 1528800.0000, 
raw observation next is [11.06666666666667, 55.33333333333333, 72.83333333333333, 24.33333333333334, 19.5, 23.02086532886504, -0.18903575593988, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7691597414589106, 0.5533333333333332, 0.24277777777777776, 0.026887661141804794, 0.125, 0.4184054440720866, 0.4369880813533733, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.53855455], dtype=float32), -1.6523685]. 
=============================================
[2019-04-05 12:58:20,552] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-05 12:58:20,554] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 12:58:20,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:58:20,555] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 12:58:20,557] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:58:20,557] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 12:58:20,558] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 12:58:20,565] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run44
[2019-04-05 12:58:20,579] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run44
[2019-04-05 12:58:20,595] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run44
[2019-04-05 12:59:00,163] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11016247]
[2019-04-05 12:59:00,163] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.745242094, 59.99405507, 0.0, 0.0, 19.0, 18.90507960224059, -1.127637368521914, 0.0, 1.0, 0.0]
[2019-04-05 12:59:00,163] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 12:59:00,164] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.3349250e-08 7.9662591e-01 3.4840098e-03 4.2861182e-02 1.5702359e-01
 3.1286305e-08 5.1605430e-06], sampled 0.3845489128540339
[2019-04-05 12:59:11,442] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11016247]
[2019-04-05 12:59:11,442] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.3, 44.66666666666667, 0.0, 0.0, 19.0, 20.18049364743868, -0.9135420185570037, 0.0, 1.0, 0.0]
[2019-04-05 12:59:11,442] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 12:59:11,443] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7773692e-08 6.1578858e-01 4.0585054e-03 7.8458026e-02 3.0169138e-01
 1.9387594e-08 3.5232413e-06], sampled 0.4328307710287801
[2019-04-05 12:59:33,564] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6370.9353 148068607.2391 -1726.2521
[2019-04-05 12:59:44,477] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5912.8202 175914272.0312 -2368.1428
[2019-04-05 12:59:49,524] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5870.4454 189892222.9170 -2330.4775
[2019-04-05 12:59:50,549] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 860000, evaluation results [860000.0, 5912.820172922484, 175914272.0312003, -2368.142774156912, 6370.935296979581, 148068607.23907876, -1726.2520988981353, 5870.445363893989, 189892222.91698432, -2330.477508441755]
[2019-04-05 12:59:53,977] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.7825991e-09 4.8372534e-01 1.8788739e-03 7.9838015e-02 4.3455327e-01
 1.2435045e-08 4.4362741e-06], sum to 1.0000
[2019-04-05 12:59:53,979] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6391
[2019-04-05 12:59:53,985] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 21.14859894290915, -0.432199497473273, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1236000.0000, 
sim time next is 1236600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.5, 21.13854223824703, -0.4348906297008468, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.125, 0.26154518652058584, 0.3550364567663844, 0.0, 0.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.8584159], dtype=float32), 0.29677752]. 
=============================================
[2019-04-05 12:59:54,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7159492e-06 7.7578700e-01 7.8174621e-03 4.0709764e-02 1.7564642e-01
 1.7568685e-06 3.2933578e-05], sum to 1.0000
[2019-04-05 12:59:54,434] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6161
[2019-04-05 12:59:54,456] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.866666666666667, 77.66666666666667, 148.5, 0.0, 19.0, 21.05070028256893, -0.7895618972285813, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2029200.0000, 
sim time next is 2029800.0000, 
raw observation next is [-4.683333333333333, 76.33333333333333, 150.0, 0.0, 19.0, 21.03251946509057, -0.79053593227887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3328716528162512, 0.7633333333333333, 0.5, 0.0, 0.08333333333333333, 0.2527099554242141, 0.23648802257371002, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2014136], dtype=float32), 2.0099783]. 
=============================================
[2019-04-05 12:59:59,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4422925e-09 3.3186956e-03 1.2325993e-04 2.3009363e-03 9.9425715e-01
 2.0012640e-09 5.0702724e-08], sum to 1.0000
[2019-04-05 12:59:59,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1099
[2019-04-05 12:59:59,198] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 80.33333333333334, 0.0, 0.0, 20.0, 19.12021722127322, -1.078506896395969, 0.0, 1.0, 197663.2740642781], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1891200.0000, 
sim time next is 1891800.0000, 
raw observation next is [-5.9, 79.0, 0.0, 0.0, 20.5, 19.11273700192967, -1.038521337164651, 0.0, 1.0, 199253.5599476136], 
processed observation next is [0.0, 0.9130434782608695, 0.2991689750692521, 0.79, 0.0, 0.0, 0.20833333333333334, 0.09272808349413915, 0.15382622094511636, 0.0, 1.0, 0.9488264759410171], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.7341736], dtype=float32), -0.5521984]. 
=============================================
[2019-04-05 12:59:59,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4965950e-08 9.5665300e-01 3.6994808e-03 1.9446608e-02 2.0196334e-02
 3.4023255e-08 4.4502299e-06], sum to 1.0000
[2019-04-05 12:59:59,599] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3144
[2019-04-05 12:59:59,615] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.4, 93.0, 54.0, 0.0, 19.0, 20.61017467285725, -0.8585801053576686, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 919800.0000, 
sim time next is 920400.0000, 
raw observation next is [4.4, 93.0, 48.0, 0.0, 19.0, 20.61533684638746, -0.9279032447817374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.16, 0.0, 0.08333333333333333, 0.21794473719895505, 0.19069891840608755, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41860747], dtype=float32), -0.34735003]. 
=============================================
[2019-04-05 12:59:59,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2545588e-07 7.8539598e-01 1.0805055e-03 1.8794674e-02 1.9447406e-01
 5.3735698e-07 2.5379355e-04], sum to 1.0000
[2019-04-05 12:59:59,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9692
[2019-04-05 12:59:59,820] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.916666666666667, 67.5, 99.0, 0.0, 19.5, 20.1241376241782, -0.8771407502621792, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2128200.0000, 
sim time next is 2128800.0000, 
raw observation next is [-4.833333333333334, 67.0, 92.5, 0.0, 20.0, 20.51120647940628, -0.847276251763935, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.32871652816251157, 0.67, 0.30833333333333335, 0.0, 0.16666666666666666, 0.20926720661718998, 0.21757458274535502, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9383609], dtype=float32), 0.5832114]. 
=============================================
[2019-04-05 13:00:04,294] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4175229e-09 9.1217959e-01 4.5584928e-04 3.9776587e-03 8.3381675e-02
 1.4840731e-09 5.2101291e-06], sum to 1.0000
[2019-04-05 13:00:04,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7949
[2019-04-05 13:00:04,307] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.6, 89.0, 0.0, 0.0, 19.0, 18.67611765840816, -1.021474564956237, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1458000.0000, 
sim time next is 1458600.0000, 
raw observation next is [1.516666666666667, 89.5, 0.0, 0.0, 19.0, 18.68518307251716, -0.991303126982384, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.9130434782608695, 0.5046168051708219, 0.895, 0.0, 0.0, 0.08333333333333333, 0.0570985893764299, 0.16956562433920533, 0.0, 1.0, 0.9343709972347434], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2623017], dtype=float32), 1.2030865]. 
=============================================
[2019-04-05 13:00:11,316] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0645403e-08 3.7414730e-01 8.7232776e-03 2.8584909e-02 5.8854169e-01
 5.5653876e-10 2.8561581e-06], sum to 1.0000
[2019-04-05 13:00:11,317] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0611
[2019-04-05 13:00:11,348] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.583333333333333, 83.5, 0.0, 0.0, 25.5, 21.65632455806411, -0.5746657580008584, 0.0, 1.0, 45971.06181420981], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1800600.0000, 
sim time next is 1801200.0000, 
raw observation next is [-4.666666666666667, 84.0, 0.0, 0.0, 24.5, 21.58101684496315, -0.5840568239226333, 0.0, 1.0, 48363.92793616818], 
processed observation next is [0.0, 0.8695652173913043, 0.3333333333333333, 0.84, 0.0, 0.0, 0.5416666666666666, 0.2984180704135957, 0.3053143920257889, 0.0, 1.0, 0.23030441874365798], 
reward next is 0.2143, 
noisyNet noise sample is [array([0.23295112], dtype=float32), -0.18101037]. 
=============================================
[2019-04-05 13:00:11,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.23524989e-07 1.83761969e-01 8.66114045e-04 3.59109901e-02
 7.79432893e-01 1.13211684e-07 2.74487138e-05], sum to 1.0000
[2019-04-05 13:00:11,936] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5457
[2019-04-05 13:00:11,988] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4583826e-09 9.8697436e-01 3.5950245e-05 6.5574897e-03 6.4315274e-03
 2.2557647e-09 6.0659528e-07], sum to 1.0000
[2019-04-05 13:00:11,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0085
[2019-04-05 13:00:12,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 19.5, 18.95631420584888, -1.126025537677651, 1.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2229600.0000, 
sim time next is 2230200.0000, 
raw observation next is [-4.8, 70.5, 0.0, 0.0, 20.0, 18.8742646749358, -1.085071790987079, 1.0, 1.0, 196918.8530165821], 
processed observation next is [1.0, 0.8260869565217391, 0.3296398891966759, 0.705, 0.0, 0.0, 0.16666666666666666, 0.07285538957798347, 0.13830940300430697, 1.0, 1.0, 0.9377088238884863], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.45176736], dtype=float32), -0.7615339]. 
=============================================
[2019-04-05 13:00:12,013] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 19.0, 20.05054005488098, -0.8006838232262109, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1755000.0000, 
sim time next is 1755600.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 20.02570305128606, -0.8146314044565705, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.1688085876071718, 0.22845619851447652, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8152835], dtype=float32), 0.4166568]. 
=============================================
[2019-04-05 13:00:18,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6335533e-08 9.6773428e-01 6.6353898e-03 6.9660884e-03 1.8644316e-02
 8.1493243e-07 1.9058234e-05], sum to 1.0000
[2019-04-05 13:00:18,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7939
[2019-04-05 13:00:18,986] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 19.0, 19.07912050205, -1.057662529230204, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2581200.0000, 
sim time next is 2581800.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 19.0, 18.99603414597951, -1.070634516278809, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.08333333333333333, 0.08300284549829258, 0.1431218279070637, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3529071], dtype=float32), -0.79353654]. 
=============================================
[2019-04-05 13:00:19,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1298585e-11 6.9393516e-01 5.0265231e-04 4.8312941e-03 3.0073091e-01
 1.9129712e-11 1.8965057e-08], sum to 1.0000
[2019-04-05 13:00:19,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3032
[2019-04-05 13:00:19,345] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.1, 82.33333333333334, 0.0, 0.0, 26.0, 24.06846714755845, -0.03684944394193832, 0.0, 1.0, 81811.85272235624], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1794000.0000, 
sim time next is 1794600.0000, 
raw observation next is [-4.2, 82.5, 0.0, 0.0, 25.0, 24.16454572073434, -0.02524849772067299, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.34626038781163443, 0.825, 0.0, 0.0, 0.5833333333333334, 0.5137121433945282, 0.49158383409310896, 0.0, 1.0, 0.0], 
reward next is 0.1429, 
noisyNet noise sample is [array([-0.2070454], dtype=float32), -0.17193113]. 
=============================================
[2019-04-05 13:00:19,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3904690e-10 9.4975573e-01 2.1396449e-03 3.6639656e-04 4.7738072e-02
 8.8338664e-10 2.7085150e-07], sum to 1.0000
[2019-04-05 13:00:19,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4620
[2019-04-05 13:00:19,571] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.183333333333333, 91.5, 0.0, 0.0, 19.0, 21.65325710654547, -0.4166948072710852, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1461000.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 19.0, 21.62326660397426, -0.4278822233957196, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.08333333333333333, 0.30193888366452154, 0.3573725922014268, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.418011], dtype=float32), 2.2475042]. 
=============================================
[2019-04-05 13:00:19,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7628974e-07 4.9798989e-01 1.9632888e-03 2.1815132e-01 2.8184450e-01
 5.7856812e-07 5.0138467e-05], sum to 1.0000
[2019-04-05 13:00:19,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2747
[2019-04-05 13:00:19,859] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 18.72620640597187, -1.186640194705168, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2339400.0000, 
sim time next is 2340000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 18.69120737547597, -1.189184690711776, 0.0, 1.0, 90243.61645595076], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.057600614622997405, 0.10360510309607467, 0.0, 1.0, 0.42973150693309886], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04816277], dtype=float32), 1.8337132]. 
=============================================
[2019-04-05 13:00:19,872] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[61.746063]
 [62.4219  ]
 [63.126236]
 [63.364655]
 [64.43297 ]], R is [[62.89081955]
 [63.2619133 ]
 [63.62929535]
 [63.99300385]
 [64.35307312]].
[2019-04-05 13:00:25,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4741424e-06 1.4991039e-01 1.3634248e-02 7.8407206e-02 7.5785166e-01
 3.5061018e-06 1.8949009e-04], sum to 1.0000
[2019-04-05 13:00:25,582] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6128
[2019-04-05 13:00:25,658] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.5, 74.0, 110.3333333333333, 653.3333333333333, 21.5, 20.98626954357278, -0.7183466724617524, 1.0, 1.0, 198077.5589209758], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 2715000.0000, 
sim time next is 2715600.0000, 
raw observation next is [-11.0, 72.0, 113.6666666666667, 663.6666666666667, 22.0, 21.14205717425988, -0.6427465429813478, 1.0, 1.0, 199499.039748658], 
processed observation next is [1.0, 0.43478260869565216, 0.15789473684210528, 0.72, 0.378888888888889, 0.7333333333333334, 0.3333333333333333, 0.2618380978549899, 0.28575115233955073, 1.0, 1.0, 0.9499954273745619], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8492938], dtype=float32), 0.297783]. 
=============================================
[2019-04-05 13:00:27,835] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 13:00:27,836] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:00:27,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:00:27,837] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:00:27,837] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:00:27,837] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:00:27,837] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:00:27,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run45
[2019-04-05 13:00:27,841] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run45
[2019-04-05 13:00:27,880] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run45
[2019-04-05 13:00:37,962] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1100125]
[2019-04-05 13:00:37,963] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-5.75, 59.0, 176.3333333333333, 345.3333333333334, 19.0, 20.87992461887304, -0.74600698132774, 1.0, 1.0, 0.0]
[2019-04-05 13:00:37,963] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:00:37,964] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.6681056e-06 6.9724452e-01 8.5435146e-03 7.3607817e-02 2.2040814e-01
 1.9905917e-06 1.8836233e-04], sampled 0.967691837319721
[2019-04-05 13:00:38,398] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1100125]
[2019-04-05 13:00:38,398] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-6.1, 83.0, 0.0, 0.0, 20.0, 18.98335497675743, -1.041604562957482, 0.0, 1.0, 197916.5362561145]
[2019-04-05 13:00:38,398] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:00:38,398] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.8214782e-08 6.3645059e-01 3.8442335e-03 6.8213589e-02 2.9148224e-01
 4.9325024e-08 9.2341743e-06], sampled 0.6983314893779691
[2019-04-05 13:01:20,005] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1100125]
[2019-04-05 13:01:20,005] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.2164718805, 21.58542804, 13.08761221, 691.1595136, 19.5, 19.47615979462134, -1.005227640893262, 0.0, 1.0, 0.0]
[2019-04-05 13:01:20,006] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:01:20,006] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2719614e-08 4.3751216e-01 2.5395504e-03 1.3799702e-01 4.2194799e-01
 1.4007330e-08 3.2775315e-06], sampled 0.12178634015100576
[2019-04-05 13:01:44,390] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6528.5413 157927863.5980 -1179.2418
[2019-04-05 13:01:47,715] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1100125]
[2019-04-05 13:01:47,715] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.8333333333333334, 73.0, 52.66666666666666, 22.33333333333334, 19.0, 21.15438911095728, -0.647005613637762, 1.0, 1.0, 0.0]
[2019-04-05 13:01:47,715] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:01:47,716] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.1992675e-07 7.3578173e-01 3.9209779e-03 5.7359789e-02 2.0290676e-01
 1.8211941e-07 3.0077259e-05], sampled 0.575876004507802
[2019-04-05 13:01:53,091] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5906.6541 187902930.8183 -1863.1075
[2019-04-05 13:02:01,144] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5638.2056 199262294.9628 -1861.5035
[2019-04-05 13:02:02,170] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 880000, evaluation results [880000.0, 5906.65411797656, 187902930.81828302, -1863.1075135637047, 6528.5412544797655, 157927863.59804922, -1179.241821627212, 5638.205644627664, 199262294.9628313, -1861.503479853187]
[2019-04-05 13:02:25,596] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4677033e-09 7.3967099e-02 9.5615287e-05 6.0422659e-02 8.6551422e-01
 1.9707482e-09 3.8938069e-07], sum to 1.0000
[2019-04-05 13:02:25,596] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6993
[2019-04-05 13:02:25,604] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.166666666666667, 43.33333333333333, 0.0, 0.0, 19.5, 20.79870221120415, -0.7505888427926473, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2400000.0000, 
sim time next is 2400600.0000, 
raw observation next is [-2.283333333333333, 43.16666666666667, 0.0, 0.0, 20.0, 20.71002931403527, -0.7705107759609865, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.399353647276085, 0.4316666666666667, 0.0, 0.0, 0.16666666666666666, 0.22583577616960593, 0.24316307467967116, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.05191045], dtype=float32), -3.4606116]. 
=============================================
[2019-04-05 13:02:28,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.7572004e-09 6.8308510e-02 4.7095172e-04 8.3914602e-01 9.2013784e-02
 7.0942470e-08 6.0664206e-05], sum to 1.0000
[2019-04-05 13:02:28,650] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4092
[2019-04-05 13:02:28,671] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 20.5, 19.79252574337307, -0.9616404056934288, 0.0, 1.0, 199280.2914744854], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3045000.0000, 
sim time next is 3045600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 20.5, 19.71221904404117, -0.9439116089128552, 0.0, 1.0, 145356.6596081336], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.20833333333333334, 0.14268492033676416, 0.18536279702904826, 0.0, 1.0, 0.692174569562541], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.43681005], dtype=float32), 0.417855]. 
=============================================
[2019-04-05 13:02:29,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.4582305e-07 5.1275600e-02 7.9063943e-04 2.6427249e-03 9.4525331e-01
 6.9356389e-07 3.6672791e-05], sum to 1.0000
[2019-04-05 13:02:29,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0255
[2019-04-05 13:02:29,996] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.0, 20.66248996119408, -0.6418336746664126, 1.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 3352200.0000, 
sim time next is 3352800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 21.0, 20.74884349047468, -0.6422612406673996, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.25, 0.22907029087289002, 0.28591291977753347, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.7705715], dtype=float32), -0.017199539]. 
=============================================
[2019-04-05 13:02:35,090] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6211539e-08 9.5904309e-01 4.3302178e-04 8.1169903e-03 3.2398432e-02
 2.1697241e-08 8.4488993e-06], sum to 1.0000
[2019-04-05 13:02:35,094] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4036
[2019-04-05 13:02:35,105] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.5, 100.0, 0.0, 0.0, 19.0, 20.83679548973814, -0.7530431521820206, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3124200.0000, 
sim time next is 3124800.0000, 
raw observation next is [2.6, 100.0, 0.0, 0.0, 19.0, 20.75103426292846, -0.7656803477879918, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5346260387811635, 1.0, 0.0, 0.0, 0.08333333333333333, 0.22925285524403835, 0.24477321740400274, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9774836], dtype=float32), -0.24369676]. 
=============================================
[2019-04-05 13:02:36,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9630414e-07 7.9269940e-01 1.2390110e-02 1.1214576e-01 8.2730696e-02
 3.1503430e-06 3.0595660e-05], sum to 1.0000
[2019-04-05 13:02:36,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2945
[2019-04-05 13:02:36,939] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 29.0, 70.0, 162.0, 19.0, 21.29148367326643, -0.672030500043161, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2565000.0000, 
sim time next is 2565600.0000, 
raw observation next is [2.9, 29.0, 59.5, 135.8333333333333, 19.0, 21.39153616017088, -0.6478490937592477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5429362880886427, 0.29, 0.19833333333333333, 0.1500920810313075, 0.08333333333333333, 0.2826280133475733, 0.2840503020802508, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.51719165], dtype=float32), 0.5839394]. 
=============================================
[2019-04-05 13:02:39,887] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-05 13:02:39,890] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:02:39,891] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:02:39,891] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:02:39,892] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:02:39,894] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:02:39,895] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:02:39,899] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run46
[2019-04-05 13:02:39,915] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run46
[2019-04-05 13:02:39,915] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run46
[2019-04-05 13:02:46,192] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.110224426]
[2019-04-05 13:02:46,192] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.2, 66.66666666666666, 0.0, 0.0, 19.0, 19.34011372148073, -1.047799333763894, 1.0, 1.0, 0.0]
[2019-04-05 13:02:46,192] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:02:46,193] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.3562220e-06 6.8955594e-01 1.0170872e-02 5.7304673e-02 2.4282753e-01
 2.1607671e-06 1.3245729e-04], sampled 0.21741554512693162
[2019-04-05 13:02:46,407] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.110224426]
[2019-04-05 13:02:46,407] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.4, 70.0, 0.0, 0.0, 19.5, 18.7927246423514, -1.12509615275714, 0.0, 1.0, 148413.6584586644]
[2019-04-05 13:02:46,408] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:02:46,409] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2898172e-07 5.8760846e-01 3.8656481e-03 4.9207784e-02 3.5930336e-01
 7.0343908e-08 1.4586554e-05], sampled 0.17661371581471896
[2019-04-05 13:03:10,226] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.110224426]
[2019-04-05 13:03:10,226] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.366666666666667, 73.33333333333334, 0.0, 0.0, 19.5, 22.39485780918598, -0.2175427062941229, 0.0, 1.0, 0.0]
[2019-04-05 13:03:10,226] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:03:10,227] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.6579021e-09 7.1582139e-01 1.4148798e-03 3.6814328e-02 2.4594820e-01
 1.7412480e-09 1.1723340e-06], sampled 0.675672676009946
[2019-04-05 13:03:57,269] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6359.1758 156476931.5745 -1277.5980
[2019-04-05 13:03:57,333] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.110224426]
[2019-04-05 13:03:57,334] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.866666666666667, 58.66666666666667, 134.8333333333333, 610.0, 20.0, 22.81066343645376, -0.2103228897792455, 0.0, 1.0, 0.0]
[2019-04-05 13:03:57,334] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:03:57,335] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3641156e-10 5.3975612e-01 5.0715910e-04 6.0711619e-02 3.9902493e-01
 1.4455795e-10 1.3463026e-07], sampled 0.822957869202338
[2019-04-05 13:04:07,478] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6021.1730 184580968.0390 -1831.3387
[2019-04-05 13:04:12,918] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5767.6971 198967934.2983 -1788.8345
[2019-04-05 13:04:13,945] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 900000, evaluation results [900000.0, 6021.173039338331, 184580968.038987, -1831.3386617145259, 6359.175815614677, 156476931.57448304, -1277.5980029200125, 5767.697106060169, 198967934.2983108, -1788.8344636318773]
[2019-04-05 13:04:14,385] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.1850003e-09 3.5604272e-02 4.7111214e-04 9.0674037e-01 5.7181533e-02
 4.9012088e-09 2.5943746e-06], sum to 1.0000
[2019-04-05 13:04:14,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5335
[2019-04-05 13:04:14,393] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 20.5, 20.50438672962106, -0.6830371180810823, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3444600.0000, 
sim time next is 3445200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 20.5, 20.43820856506605, -0.6957186199425555, 0.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.79, 0.0, 0.0, 0.20833333333333334, 0.2031840470888374, 0.2680937933524815, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.26922473], dtype=float32), -0.7267873]. 
=============================================
[2019-04-05 13:04:15,870] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.6937516e-09 8.6924922e-01 5.5357825e-04 6.7425291e-03 1.2345471e-01
 9.8668698e-11 2.7942738e-08], sum to 1.0000
[2019-04-05 13:04:15,871] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9426
[2019-04-05 13:04:15,918] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 95.0, 723.0, 22.5, 22.08602628368627, -0.3934050573358991, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2991000.0000, 
sim time next is 2991600.0000, 
raw observation next is [-2.0, 60.0, 92.0, 709.0, 23.0, 22.0314351810092, -0.3926147091475349, 0.0, 1.0, 59854.74729924065], 
processed observation next is [0.0, 0.6521739130434783, 0.40720221606648205, 0.6, 0.30666666666666664, 0.7834254143646409, 0.4166666666666667, 0.33595293175076674, 0.36912843028415504, 0.0, 1.0, 0.2850226061868602], 
reward next is 0.4286, 
noisyNet noise sample is [array([-0.7804451], dtype=float32), 0.6139049]. 
=============================================
[2019-04-05 13:04:19,002] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2801095e-05 9.3394434e-01 1.0702032e-02 6.6252467e-03 4.8629809e-02
 1.6421162e-06 8.4074905e-05], sum to 1.0000
[2019-04-05 13:04:19,002] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3782
[2019-04-05 13:04:19,024] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-11.66666666666667, 61.33333333333334, 0.0, 0.0, 19.5, 19.85610745871644, -0.9292461625609537, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3980400.0000, 
sim time next is 3981000.0000, 
raw observation next is [-11.83333333333333, 62.16666666666666, 0.0, 0.0, 19.0, 19.78558514494983, -0.9517105849599691, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.13481071098799638, 0.6216666666666666, 0.0, 0.0, 0.08333333333333333, 0.14879876207915257, 0.18276313834667698, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01307064], dtype=float32), 1.3833128]. 
=============================================
[2019-04-05 13:04:19,034] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.6389855e-07 4.6037263e-01 7.6185954e-03 1.7182782e-01 3.6005446e-01
 1.5481331e-07 1.2542075e-04], sum to 1.0000
[2019-04-05 13:04:19,034] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0958
[2019-04-05 13:04:19,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[54.133415]
 [54.869778]
 [55.774635]
 [56.226456]
 [57.579414]], R is [[53.26054382]
 [53.6565094 ]
 [54.11994553]
 [54.5787468 ]
 [54.89010239]].
[2019-04-05 13:04:19,046] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 100.0, 169.5, 0.0, 19.5, 20.17037050169735, -0.9116006830415113, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2898000.0000, 
sim time next is 2898600.0000, 
raw observation next is [2.0, 100.0, 167.6666666666667, 0.0, 19.0, 20.22046355614835, -0.9750052678440319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5588888888888891, 0.0, 0.08333333333333333, 0.1850386296790291, 0.17499824405198938, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7789147], dtype=float32), 0.54710674]. 
=============================================
[2019-04-05 13:04:20,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.10854447e-10 8.36523890e-01 1.17205534e-04 3.65593494e-03
 1.59703016e-01 1.61487768e-10 5.63015341e-08], sum to 1.0000
[2019-04-05 13:04:20,407] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8573
[2019-04-05 13:04:20,434] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.1666666666666667, 39.16666666666666, 89.0, 707.0, 19.0, 20.92992442976433, -0.6318740933518746, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3078600.0000, 
sim time next is 3079200.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 86.5, 690.0, 19.5, 20.93848453724436, -0.6312698447637413, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4718374884579871, 0.3933333333333334, 0.28833333333333333, 0.7624309392265194, 0.125, 0.24487371143703, 0.28957671841208626, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.30557016], dtype=float32), 0.20711918]. 
=============================================
[2019-04-05 13:04:23,742] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.3108932e-06 3.4401134e-01 3.1104792e-02 9.1530643e-02 5.3298849e-01
 6.1025425e-05 2.9741233e-04], sum to 1.0000
[2019-04-05 13:04:23,746] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1591
[2019-04-05 13:04:23,754] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 40.0, 0.0, 0.0, 19.0, 18.93005144500479, -1.110767237565647, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4074000.0000, 
sim time next is 4074600.0000, 
raw observation next is [-5.0, 40.5, 0.0, 0.0, 19.0, 18.89701618158057, -1.120419582628929, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.405, 0.0, 0.0, 0.08333333333333333, 0.07475134846504765, 0.12652680579035702, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5468969], dtype=float32), -0.45810184]. 
=============================================
[2019-04-05 13:04:33,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.1286828e-08 2.6760232e-01 7.9941214e-04 1.7225501e-01 5.5933338e-01
 3.0845956e-07 9.3875260e-06], sum to 1.0000
[2019-04-05 13:04:33,813] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8811
[2019-04-05 13:04:33,836] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 21.0, 20.62566171341159, -0.6552994184809492, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 21.5, 20.55730069987776, -0.6420261217793024, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.2916666666666667, 0.21310839165648007, 0.28599129274023255, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.3899041], dtype=float32), -1.2341956]. 
=============================================
[2019-04-05 13:04:38,794] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0775646e-07 3.8872871e-01 3.9947568e-03 5.6458378e-01 4.2692132e-02
 8.9779562e-09 3.3162254e-07], sum to 1.0000
[2019-04-05 13:04:38,795] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1402
[2019-04-05 13:04:38,804] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 19.0, 22.0648084961752, -0.3336154916058719, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4414800.0000, 
sim time next is 4415400.0000, 
raw observation next is [5.55, 66.5, 0.0, 0.0, 19.0, 22.00705801365571, -0.3401882757720507, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6163434903047093, 0.665, 0.0, 0.0, 0.08333333333333333, 0.3339215011379757, 0.3866039080759831, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02919862], dtype=float32), 1.7957076]. 
=============================================
[2019-04-05 13:04:44,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.08366213e-07 8.02179396e-01 1.34279812e-02 7.54495040e-02
 1.08777806e-01 7.70050477e-08 1.64750731e-04], sum to 1.0000
[2019-04-05 13:04:44,259] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-05 13:04:44,273] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.666666666666668, 25.33333333333333, 123.0, 864.1666666666667, 19.0, 22.65333943476731, -0.3037209485850615, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5056800.0000, 
sim time next is 5057400.0000, 
raw observation next is [8.833333333333332, 25.16666666666667, 122.0, 863.3333333333334, 19.0, 22.76250462713936, -0.2801074084821141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.7072945521698984, 0.2516666666666667, 0.4066666666666667, 0.9539594843462247, 0.08333333333333333, 0.39687538559494673, 0.4066308638392953, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2432983], dtype=float32), 0.08870221]. 
=============================================
[2019-04-05 13:04:46,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:04:46,798] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:04:46,804] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run8
[2019-04-05 13:04:48,326] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3038277e-07 4.5522839e-01 6.2526874e-02 1.9504732e-01 2.8707939e-01
 7.8778834e-07 1.1660793e-04], sum to 1.0000
[2019-04-05 13:04:48,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0146
[2019-04-05 13:04:48,358] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.166666666666666, 55.66666666666666, 116.6666666666667, 802.3333333333334, 20.0, 21.93592189888534, -0.3971420053297638, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3325800.0000, 
sim time next is 3326400.0000, 
raw observation next is [-6.0, 54.0, 117.0, 804.5, 19.0, 22.20688031931424, -0.3664561097141172, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.54, 0.39, 0.888950276243094, 0.08333333333333333, 0.3505733599428534, 0.3778479634286276, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7112409], dtype=float32), -0.45955387]. 
=============================================
[2019-04-05 13:04:48,709] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 13:04:48,710] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:04:48,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:04:48,712] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:04:48,713] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:04:48,713] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:04:48,713] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:04:48,718] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run47
[2019-04-05 13:04:48,733] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run47
[2019-04-05 13:04:48,734] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run47
[2019-04-05 13:05:12,581] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11306234]
[2019-04-05 13:05:12,581] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.85, 55.16666666666667, 143.6666666666667, 441.6666666666667, 19.0, 20.98712550258923, -0.5552725960780244, 1.0, 1.0, 0.0]
[2019-04-05 13:05:12,582] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:05:12,582] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.5845554e-07 7.7843457e-01 2.1671911e-03 7.2866209e-02 1.4652041e-01
 9.5097441e-08 1.1235051e-05], sampled 0.3275869465292822
[2019-04-05 13:05:50,830] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11306234]
[2019-04-05 13:05:50,830] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.7, 80.33333333333333, 0.0, 0.0, 19.0, 19.44087601622869, -0.9242604889723861, 0.0, 1.0, 0.0]
[2019-04-05 13:05:50,830] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:05:50,831] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.1777841e-09 6.8959469e-01 1.4155656e-03 5.9477542e-02 2.4951087e-01
 4.1139807e-09 1.2769466e-06], sampled 0.5692184949195892
[2019-04-05 13:05:57,303] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11306234]
[2019-04-05 13:05:57,304] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.56666666666667, 69.0, 0.0, 0.0, 19.0, 25.20999975846646, 0.4786950097639067, 0.0, 1.0, 0.0]
[2019-04-05 13:05:57,304] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:05:57,306] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.9119746e-10 6.3050526e-01 6.3169660e-04 1.4607708e-01 2.2278577e-01
 2.4888469e-10 2.1086754e-07], sampled 0.9692622810570823
[2019-04-05 13:06:01,849] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6344.9026 154286971.3060 -1510.5187
[2019-04-05 13:06:03,591] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11306234]
[2019-04-05 13:06:03,591] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.282775504333333, 61.72781897, 1.912885324166667, 89.80204669999999, 19.0, 22.03234167117613, -0.4900098232186852, 1.0, 1.0, 0.0]
[2019-04-05 13:06:03,591] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:06:03,592] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.9654547e-08 8.1041974e-01 1.2503525e-03 5.1158495e-02 1.3716821e-01
 9.5751380e-09 3.0396027e-06], sampled 0.3822520585475725
[2019-04-05 13:06:14,456] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5802.1755 180119166.6945 -2192.3499
[2019-04-05 13:06:21,430] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5830.1600 193365120.7804 -2152.1694
[2019-04-05 13:06:22,455] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 920000, evaluation results [920000.0, 5802.175499039836, 180119166.6945107, -2192.3498857770005, 6344.902642305216, 154286971.30604005, -1510.518703634066, 5830.159986432901, 193365120.78038803, -2152.169400487828]
[2019-04-05 13:06:22,730] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8923279e-07 4.6709228e-01 1.2242353e-02 3.5426095e-01 1.6639987e-01
 1.8161990e-07 4.1725834e-06], sum to 1.0000
[2019-04-05 13:06:22,733] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4369
[2019-04-05 13:06:22,751] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 48.0, 60.0, 501.0, 19.0, 22.9640647231751, -0.285260296240668, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3342600.0000, 
sim time next is 3343200.0000, 
raw observation next is [-2.0, 48.66666666666666, 51.83333333333334, 439.6666666666667, 19.0, 22.59349270692914, -0.2997603050692177, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.4866666666666666, 0.1727777777777778, 0.48581952117863725, 0.08333333333333333, 0.3827910589107617, 0.40007989831026075, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8539774], dtype=float32), 1.5553536]. 
=============================================
[2019-04-05 13:06:23,261] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4057105e-08 1.5932287e-01 1.5470838e-02 1.5072380e-01 6.7447883e-01
 2.5920885e-08 3.5495902e-06], sum to 1.0000
[2019-04-05 13:06:23,262] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5464
[2019-04-05 13:06:23,281] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 22.0, 22.05271261956716, -0.4566230973946507, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 3726000.0000, 
sim time next is 3726600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 22.5, 22.04835465595472, -0.453078511534476, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.375, 0.33736288799622677, 0.348973829488508, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.76099676], dtype=float32), 0.1435415]. 
=============================================
[2019-04-05 13:06:23,453] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1727123e-10 4.8420048e-01 5.2775136e-05 2.0499434e-02 4.9524730e-01
 4.1714632e-10 2.2329704e-08], sum to 1.0000
[2019-04-05 13:06:23,453] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8390
[2019-04-05 13:06:23,467] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.416666666666667, 75.16666666666667, 0.0, 0.0, 19.0, 23.04282623246833, -0.2393613226914838, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4317000.0000, 
sim time next is 4317600.0000, 
raw observation next is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 19.5, 22.9936220314826, -0.2563566937147138, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.5854108956602032, 0.7533333333333334, 0.0, 0.0, 0.125, 0.4161351692902168, 0.414547768761762, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.9251422], dtype=float32), 1.0222781]. 
=============================================
[2019-04-05 13:06:24,138] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9323386e-10 9.6212275e-02 3.2401065e-04 1.4629034e-02 8.8883466e-01
 1.9780595e-09 1.4673674e-08], sum to 1.0000
[2019-04-05 13:06:24,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2029
[2019-04-05 13:06:24,219] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 20.0, 19.1718972436781, -0.8630744803672378, 0.0, 1.0, 122911.0787061075], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 20.5, 19.47254292928577, -0.7746336610653392, 0.0, 1.0, 181599.4368916347], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 0.20833333333333334, 0.12271191077381409, 0.24178877964488696, 0.0, 1.0, 0.8647592232934985], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.34494892], dtype=float32), 0.09490532]. 
=============================================
[2019-04-05 13:06:28,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4454030e-06 9.5111525e-01 1.2068519e-03 2.6418833e-02 2.1191973e-02
 1.8124854e-06 6.2797524e-05], sum to 1.0000
[2019-04-05 13:06:28,818] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8633
[2019-04-05 13:06:28,830] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 19.0, 20.48264925356458, -0.7904253286317702, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 120000.0000, 
sim time next is 120600.0000, 
raw observation next is [-7.8, 67.5, 45.0, 0.0, 19.0, 20.56342651868697, -0.785904577875307, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.675, 0.15, 0.0, 0.08333333333333333, 0.21361887655724754, 0.23803180737489768, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.29039782], dtype=float32), -1.2302014]. 
=============================================
[2019-04-05 13:06:30,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2280017e-11 9.8120934e-01 1.5647204e-04 4.0570181e-03 1.4576817e-02
 1.6603047e-11 3.9043243e-07], sum to 1.0000
[2019-04-05 13:06:30,143] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1165
[2019-04-05 13:06:30,153] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.066666666666667, 42.83333333333334, 0.0, 0.0, 20.5, 20.88046965507678, -0.6989033660607055, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4218600.0000, 
sim time next is 4219200.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 19.5, 20.84416660361816, -0.7026609863854176, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.125, 0.23701388363484663, 0.2657796712048608, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.6541095], dtype=float32), -0.9927601]. 
=============================================
[2019-04-05 13:06:34,194] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2278116e-08 9.9355894e-01 2.8786054e-03 2.0895079e-03 1.4726403e-03
 4.5465814e-08 2.5238043e-07], sum to 1.0000
[2019-04-05 13:06:34,197] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6151
[2019-04-05 13:06:34,215] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.666666666666667, 46.0, 0.0, 0.0, 19.0, 19.43112337602567, -1.039562087002928, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4239600.0000, 
sim time next is 4240200.0000, 
raw observation next is [2.833333333333333, 45.5, 0.0, 0.0, 19.0, 19.38956326035746, -1.049365946931431, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.541089566020314, 0.455, 0.0, 0.0, 0.08333333333333333, 0.11579693836312159, 0.15021135102285635, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.64640474], dtype=float32), -0.5949728]. 
=============================================
[2019-04-05 13:06:34,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0309249e-06 7.1637386e-01 5.4121937e-04 5.2685980e-02 2.3000903e-01
 9.1309909e-07 3.8706453e-04], sum to 1.0000
[2019-04-05 13:06:34,527] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8977
[2019-04-05 13:06:34,544] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 19.0, 20.3318632128556, -0.9308743401364897, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 218400.0000, 
sim time next is 219000.0000, 
raw observation next is [-4.583333333333333, 65.0, 135.6666666666667, 0.0, 19.5, 20.31925181388596, -0.9390865873336803, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3356417359187443, 0.65, 0.45222222222222236, 0.0, 0.125, 0.1932709844904966, 0.1869711375554399, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07974701], dtype=float32), 0.57385075]. 
=============================================
[2019-04-05 13:06:34,551] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[56.515568]
 [56.28332 ]
 [56.26483 ]
 [56.19392 ]
 [56.578915]], R is [[55.90308762]
 [55.34405899]
 [54.7906189 ]
 [54.24271393]
 [53.70028687]].
[2019-04-05 13:06:34,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6314617e-09 2.4965887e-01 7.8957173e-04 3.3747107e-02 7.1580428e-01
 5.7829430e-09 1.7225315e-07], sum to 1.0000
[2019-04-05 13:06:34,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7426
[2019-04-05 13:06:34,851] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 19.0, 21.36680114733276, -0.6155446603615636, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4332000.0000, 
sim time next is 4332600.0000, 
raw observation next is [3.95, 70.5, 0.0, 0.0, 19.5, 21.24944889397895, -0.636297018587612, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.57202216066482, 0.705, 0.0, 0.0, 0.125, 0.27078740783157923, 0.2879009938041293, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.475055], dtype=float32), 1.4043264]. 
=============================================
[2019-04-05 13:06:35,924] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5703898e-06 6.5601635e-01 1.4315042e-02 6.7669950e-02 2.6169717e-01
 1.2430482e-06 2.9362418e-04], sum to 1.0000
[2019-04-05 13:06:35,926] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2836
[2019-04-05 13:06:35,997] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 19.5, 20.61922407912958, -0.7313889801830689, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3314400.0000, 
sim time next is 3315000.0000, 
raw observation next is [-9.333333333333332, 78.16666666666667, 92.0, 469.0, 19.0, 20.8742397048379, -0.6894565661030446, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20406278855032323, 0.7816666666666667, 0.30666666666666664, 0.518232044198895, 0.08333333333333333, 0.23951997540315842, 0.27018114463231846, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3570758], dtype=float32), -1.3772769]. 
=============================================
[2019-04-05 13:06:36,007] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[52.635036]
 [52.872704]
 [53.616264]
 [53.553974]
 [54.17153 ]], R is [[52.43845367]
 [51.91407013]
 [51.39493179]
 [50.88098145]
 [50.37217331]].
[2019-04-05 13:06:39,151] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9433253e-09 4.6570742e-01 5.3511676e-03 2.6350087e-01 2.6542068e-01
 5.5489227e-09 1.9801964e-05], sum to 1.0000
[2019-04-05 13:06:39,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8025
[2019-04-05 13:06:39,163] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.95, 70.5, 0.0, 0.0, 21.0, 21.25650835750758, -0.6290735892147465, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 4332600.0000, 
sim time next is 4333200.0000, 
raw observation next is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 21.5, 21.19744129138618, -0.6289070764264213, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.13043478260869565, 0.5715604801477379, 0.7033333333333333, 0.0, 0.0, 0.2916666666666667, 0.2664534409488484, 0.29036430785785955, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.03487767], dtype=float32), -1.3452258]. 
=============================================
[2019-04-05 13:06:40,293] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7770683e-10 9.9456787e-01 3.5423229e-05 1.6085529e-03 3.7878857e-03
 1.7600321e-09 2.2965463e-07], sum to 1.0000
[2019-04-05 13:06:40,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5748
[2019-04-05 13:06:40,306] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.93333333333333, 28.16666666666667, 118.6666666666667, 844.6666666666667, 19.0, 24.65294463283598, 0.1308785223099315, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4363800.0000, 
sim time next is 4364400.0000, 
raw observation next is [14.86666666666667, 28.33333333333334, 118.3333333333333, 848.8333333333334, 19.0, 24.72774172741541, 0.136791304049841, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8744228993536475, 0.2833333333333334, 0.3944444444444443, 0.9379373848987109, 0.08333333333333333, 0.5606451439512842, 0.545597101349947, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.742457], dtype=float32), 1.7526368]. 
=============================================
[2019-04-05 13:06:42,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2797291e-09 1.8106905e-01 1.1051936e-03 3.8719546e-02 7.7910590e-01
 2.6728075e-09 3.5802839e-07], sum to 1.0000
[2019-04-05 13:06:42,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6746
[2019-04-05 13:06:42,026] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 21.0, 21.23966216471091, -0.5758588687749346, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 4497600.0000, 
sim time next is 4498200.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 21.5, 21.26212359981769, -0.5823733515914719, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.2916666666666667, 0.27184363331814093, 0.3058755494695094, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-1.0481343], dtype=float32), 0.5663779]. 
=============================================
[2019-04-05 13:06:43,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:06:43,082] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:06:43,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run8
[2019-04-05 13:06:45,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1574106e-06 5.6819862e-01 1.0965991e-02 1.8255025e-01 2.3817670e-01
 7.6287870e-07 1.0654231e-04], sum to 1.0000
[2019-04-05 13:06:45,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1105
[2019-04-05 13:06:45,432] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.3, 39.33333333333334, 38.83333333333334, 727.8333333333334, 19.0, 19.84689829379585, -1.005210518856298, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 400800.0000, 
sim time next is 401400.0000, 
raw observation next is [-9.2, 39.0, 37.0, 707.0, 19.0, 19.94588467722856, -0.9994499613890037, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.20775623268698065, 0.39, 0.12333333333333334, 0.7812154696132597, 0.08333333333333333, 0.16215705643571324, 0.1668500128703321, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.96268994], dtype=float32), -1.0381331]. 
=============================================
[2019-04-05 13:06:45,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2235914e-08 9.9846208e-01 6.8826885e-06 4.6892060e-04 1.0615592e-03
 1.1285971e-08 4.7836920e-07], sum to 1.0000
[2019-04-05 13:06:45,550] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0454
[2019-04-05 13:06:45,566] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 49.0, 88.0, 687.0, 19.0, 23.21180251974253, -0.1105116858616014, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3511800.0000, 
sim time next is 3512400.0000, 
raw observation next is [3.0, 49.0, 83.66666666666667, 660.0, 19.0, 23.46783841776245, -0.08262761975056336, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.2788888888888889, 0.7292817679558011, 0.08333333333333333, 0.4556532014802042, 0.47245746008314554, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3400832], dtype=float32), -0.04713286]. 
=============================================
[2019-04-05 13:06:46,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5659701e-10 9.8974693e-01 2.4602219e-04 5.6881984e-03 4.3186210e-03
 7.2058942e-10 3.4803170e-07], sum to 1.0000
[2019-04-05 13:06:46,603] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2014
[2019-04-05 13:06:46,620] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.166666666666667, 46.83333333333333, 0.0, 0.0, 19.0, 22.19117354814778, -0.3645604669244194, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3865800.0000, 
sim time next is 3866400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 22.20488238624444, -0.3664749222570087, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.35040686552036987, 0.3778416925809971, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73850965], dtype=float32), 0.9948734]. 
=============================================
[2019-04-05 13:06:47,274] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.10392254e-08 8.24781775e-01 1.11529445e-02 9.57274213e-02
 6.83282837e-02 1.72637549e-08 9.49640707e-06], sum to 1.0000
[2019-04-05 13:06:47,276] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5669
[2019-04-05 13:06:47,287] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 18.60537164436384, -1.093863928182609, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4764600.0000, 
sim time next is 4765200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 18.71279659562146, -1.089173011436038, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.05939971630178823, 0.13694232952132065, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19669917], dtype=float32), 0.23467962]. 
=============================================
[2019-04-05 13:06:51,267] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5696580e-07 3.4087670e-01 1.2833645e-04 2.1622792e-01 4.4276169e-01
 3.8804532e-07 4.8314464e-06], sum to 1.0000
[2019-04-05 13:06:51,271] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6129
[2019-04-05 13:06:51,312] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 21.5, 19.58415508831241, -0.85671564818933, 0.0, 1.0, 119652.5652128571], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4060800.0000, 
sim time next is 4061400.0000, 
raw observation next is [-6.0, 37.66666666666667, 0.0, 0.0, 22.0, 19.77467149442318, -0.8221317909993321, 0.0, 1.0, 71372.14975511305], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.3766666666666667, 0.0, 0.0, 0.3333333333333333, 0.14788929120193176, 0.2259560696668893, 0.0, 1.0, 0.3398673797862526], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.44992584], dtype=float32), 0.60687184]. 
=============================================
[2019-04-05 13:06:53,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2268640e-08 7.8026004e-02 1.8556571e-02 3.8297191e-01 5.2040040e-01
 7.4169876e-08 4.5048579e-05], sum to 1.0000
[2019-04-05 13:06:53,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5202
[2019-04-05 13:06:53,421] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.5, 20.80337701740892, -0.6385682313836916, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4667400.0000, 
sim time next is 4668000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 20.0, 20.74532165996892, -0.6475252872321754, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.16666666666666666, 0.22877680499740993, 0.2841582375892749, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.1856577], dtype=float32), -1.6353678]. 
=============================================
[2019-04-05 13:06:53,437] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.781876]
 [86.61322 ]
 [88.52108 ]
 [90.24726 ]
 [92.635254]], R is [[82.74235535]
 [82.84350586]
 [82.94364166]
 [83.11420441]
 [83.2830658 ]].
[2019-04-05 13:06:55,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:06:55,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:06:55,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run8
[2019-04-05 13:06:55,779] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-05 13:06:55,781] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:06:55,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:06:55,783] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:06:55,784] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:06:55,784] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:06:55,784] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:06:55,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run48
[2019-04-05 13:06:55,808] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run48
[2019-04-05 13:06:55,833] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run48
[2019-04-05 13:06:56,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:06:56,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:06:56,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run8
[2019-04-05 13:07:07,273] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11463156]
[2019-04-05 13:07:07,273] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-14.65635462, 62.24776807, 0.0, 0.0, 19.0, 18.09462642256925, -1.388323496127017, 0.0, 1.0, 52349.57766608064]
[2019-04-05 13:07:07,273] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:07:07,274] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.9234027e-06 4.1259417e-01 1.0404899e-02 2.0193559e-01 3.7497008e-01
 2.5804377e-06 8.9765446e-05], sampled 0.11279707318066734
[2019-04-05 13:07:12,898] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11463156]
[2019-04-05 13:07:12,898] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-9.316666666666666, 43.16666666666667, 124.6666666666667, 324.6666666666666, 19.0, 20.00587127343648, -0.9442813750363431, 0.0, 1.0, 0.0]
[2019-04-05 13:07:12,898] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:07:12,899] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.4381366e-08 6.0193950e-01 2.0453716e-03 1.4313346e-01 2.5287855e-01
 2.2013701e-08 2.9985495e-06], sampled 0.1381908094327733
[2019-04-05 13:07:46,814] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11463156]
[2019-04-05 13:07:46,815] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.9, 47.83333333333334, 0.0, 0.0, 19.0, 18.76713551984816, -1.199263024340419, 0.0, 1.0, 0.0]
[2019-04-05 13:07:46,815] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:07:46,815] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.5895124e-08 5.5038720e-01 1.9115404e-03 1.7519407e-01 2.7250457e-01
 1.6626988e-08 2.5402721e-06], sampled 0.21075212817031053
[2019-04-05 13:07:54,819] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11463156]
[2019-04-05 13:07:54,819] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.4696113525, 85.26111416666667, 0.0, 0.0, 19.0, 19.4268305146058, -1.019947495731959, 0.0, 1.0, 0.0]
[2019-04-05 13:07:54,820] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:07:54,821] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.2583490e-08 4.3013561e-01 2.1401562e-03 1.8653536e-01 3.8118479e-01
 2.8446509e-08 4.0039413e-06], sampled 0.8005135736943296
[2019-04-05 13:08:09,072] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6349.9426 149482030.5183 -1667.0259
[2019-04-05 13:08:13,950] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11463156]
[2019-04-05 13:08:13,950] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.6, 47.5, 40.0, 145.0, 19.0, 22.87204709139613, -0.3043237473977876, 1.0, 1.0, 0.0]
[2019-04-05 13:08:13,950] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:08:13,951] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.1082537e-08 8.4050316e-01 7.3396933e-04 5.6564298e-02 1.0219649e-01
 7.9657090e-09 2.0107605e-06], sampled 0.1947844117714267
[2019-04-05 13:08:19,797] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5884.4573 176335362.4547 -2270.6491
[2019-04-05 13:08:25,622] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5803.4002 190168114.2689 -2350.9659
[2019-04-05 13:08:26,649] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 940000, evaluation results [940000.0, 5884.457267455607, 176335362.45467046, -2270.6490530415526, 6349.942620806588, 149482030.51825163, -1667.0259433655067, 5803.400243784244, 190168114.26887995, -2350.9659040114984]
[2019-04-05 13:08:30,576] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5593506e-08 8.8297254e-01 1.2017784e-03 1.0118547e-01 1.4613980e-02
 1.5653954e-07 2.6073323e-05], sum to 1.0000
[2019-04-05 13:08:30,580] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8523
[2019-04-05 13:08:30,621] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.333333333333334, 73.0, 92.66666666666667, 485.8333333333333, 19.0, 21.35016011707554, -0.6013039174349768, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3832800.0000, 
sim time next is 3833400.0000, 
raw observation next is [-4.166666666666667, 72.0, 94.33333333333333, 524.6666666666667, 19.0, 21.55967794348674, -0.5789720021229342, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3471837488457987, 0.72, 0.3144444444444444, 0.5797421731123389, 0.08333333333333333, 0.296639828623895, 0.3070093326256886, 1.0, 1.0, 0.0], 
reward next is 0.2103, 
noisyNet noise sample is [array([1.8769809], dtype=float32), -0.6512295]. 
=============================================
[2019-04-05 13:08:32,462] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.03113526e-09 8.75764847e-01 1.76764722e-03 3.25477049e-02
 8.99196416e-02 7.00073666e-10 1.08306764e-07], sum to 1.0000
[2019-04-05 13:08:32,467] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7266
[2019-04-05 13:08:32,526] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 174.0, 246.0, 22.5, 22.66854600028672, -0.2755288343599082, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 185.0, 223.6666666666667, 21.5, 22.95238991178689, -0.2536532475932854, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3841181902123731, 0.6416666666666667, 0.6166666666666667, 0.24714548802946598, 0.2916666666666667, 0.41269915931557427, 0.4154489174689049, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.4392048], dtype=float32), 1.5884331]. 
=============================================
[2019-04-05 13:08:34,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:34,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:34,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run8
[2019-04-05 13:08:39,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:39,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:39,303] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run8
[2019-04-05 13:08:39,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:39,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:39,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run8
[2019-04-05 13:08:40,780] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2005169e-07 5.3229964e-01 1.4576054e-02 5.8018260e-02 3.9506632e-01
 3.1180923e-07 3.8939634e-05], sum to 1.0000
[2019-04-05 13:08:40,783] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2227
[2019-04-05 13:08:40,791] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 38.33333333333334, 0.0, 0.0, 19.0, 19.06414583593108, -1.077803278774128, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4062000.0000, 
sim time next is 4062600.0000, 
raw observation next is [-6.0, 39.0, 0.0, 0.0, 19.0, 18.97107184560799, -1.09159712786593, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.39, 0.0, 0.0, 0.08333333333333333, 0.08092265380066592, 0.13613429071135666, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46529886], dtype=float32), 0.11304124]. 
=============================================
[2019-04-05 13:08:42,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:42,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:42,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run8
[2019-04-05 13:08:44,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:44,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:44,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run8
[2019-04-05 13:08:49,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1900219e-08 3.7231272e-01 1.8265836e-02 1.1287150e-01 4.9654838e-01
 7.3368001e-09 1.5638487e-06], sum to 1.0000
[2019-04-05 13:08:49,325] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3909
[2019-04-05 13:08:49,333] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 54.5, 0.0, 0.0, 19.0, 21.82152342617528, -0.3737193331186143, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4656600.0000, 
sim time next is 4657200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 21.77321755498691, -0.3843809847759015, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.08333333333333333, 0.3144347962489092, 0.3718730050746995, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09907917], dtype=float32), 0.6048953]. 
=============================================
[2019-04-05 13:08:52,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:52,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:52,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run8
[2019-04-05 13:08:53,095] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.9703022e-11 6.1129385e-01 2.6667846e-04 3.0416814e-03 3.8539776e-01
 4.3562751e-11 9.8348512e-09], sum to 1.0000
[2019-04-05 13:08:53,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3825
[2019-04-05 13:08:53,109] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.05, 84.0, 18.0, 0.0, 19.0, 21.46092590854438, -0.5004083493658528, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 59400.0000, 
sim time next is 60000.0000, 
raw observation next is [5.866666666666667, 84.66666666666666, 15.0, 0.0, 19.0, 21.41023606648303, -0.5151171402906936, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6251154201292707, 0.8466666666666666, 0.05, 0.0, 0.08333333333333333, 0.28418633887358585, 0.32829428656976883, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6633992], dtype=float32), -3.0946987]. 
=============================================
[2019-04-05 13:08:53,125] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[101.81512 ]
 [102.05385 ]
 [102.22771 ]
 [102.47187 ]
 [102.658615]], R is [[101.64506531]
 [101.62861633]
 [101.61232758]
 [101.59620667]
 [101.50881958]].
[2019-04-05 13:08:55,251] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6905003e-11 9.9575752e-01 1.3279488e-05 7.2512112e-04 3.5039214e-03
 4.2222524e-11 1.2741002e-07], sum to 1.0000
[2019-04-05 13:08:55,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7697
[2019-04-05 13:08:55,268] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 19.0, 18.93773487726259, -1.160077318670884, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 510000.0000, 
sim time next is 510600.0000, 
raw observation next is [2.516666666666667, 92.66666666666667, 0.0, 0.0, 19.0, 18.92029697191743, -1.168381028181133, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5323176361957526, 0.9266666666666667, 0.0, 0.0, 0.08333333333333333, 0.07669141432645254, 0.11053965727295563, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3644011], dtype=float32), 1.0295458]. 
=============================================
[2019-04-05 13:08:56,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:56,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:56,807] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run8
[2019-04-05 13:08:58,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:08:58,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:08:58,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run8
[2019-04-05 13:08:59,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5068457e-08 2.3920419e-02 3.4203034e-04 9.2597300e-01 4.9760193e-02
 6.2733824e-10 4.2575907e-06], sum to 1.0000
[2019-04-05 13:08:59,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2326
[2019-04-05 13:08:59,523] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 20.37925040232073, -0.6080338555274446, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1373400.0000, 
sim time next is 1374000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 20.35792345637118, -0.6168695559496639, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.19649362136426488, 0.2943768146834454, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28083172], dtype=float32), -1.6968781]. 
=============================================
[2019-04-05 13:08:59,544] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[95.040405]
 [94.795975]
 [94.469604]
 [93.526085]
 [92.595406]], R is [[95.6438446 ]
 [95.68740845]
 [95.73053741]
 [95.77323151]
 [95.81549835]].
[2019-04-05 13:09:01,082] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0213708e-07 4.5151943e-01 1.2757175e-04 3.9713252e-03 5.4437935e-01
 7.5541628e-08 2.2330512e-06], sum to 1.0000
[2019-04-05 13:09:01,082] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4440
[2019-04-05 13:09:01,116] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.50248650889996, -1.207529356177638, 0.0, 1.0, 197546.9934758274], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 247200.0000, 
sim time next is 247800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 18.47721781263767, -1.170676605179813, 0.0, 1.0, 165572.5475569264], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.03976815105313906, 0.10977446494006234, 0.0, 1.0, 0.7884407026520305], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95955944], dtype=float32), -0.7679971]. 
=============================================
[2019-04-05 13:09:01,293] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.03119774e-07 9.89320040e-01 1.18280307e-03 4.28676931e-03
 5.20758564e-03 6.50527987e-09 2.75583443e-06], sum to 1.0000
[2019-04-05 13:09:01,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7494
[2019-04-05 13:09:01,304] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 55.5, 0.0, 0.0, 19.0, 19.83269640452725, -0.9860922117372461, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 756600.0000, 
sim time next is 757200.0000, 
raw observation next is [-3.9, 55.0, 0.0, 0.0, 19.0, 19.66104287408157, -1.028588323923637, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.55, 0.0, 0.0, 0.08333333333333333, 0.1384202395067975, 0.15713722535878763, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0944405], dtype=float32), 1.1184163]. 
=============================================
[2019-04-05 13:09:01,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:09:01,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:09:01,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run8
[2019-04-05 13:09:04,313] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-05 13:09:04,314] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:09:04,315] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:09:04,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:09:04,316] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:09:04,316] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:09:04,317] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:09:04,321] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run49
[2019-04-05 13:09:04,342] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run49
[2019-04-05 13:09:04,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run49
[2019-04-05 13:09:04,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:09:04,869] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:09:04,874] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run8
[2019-04-05 13:09:28,063] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11452219]
[2019-04-05 13:09:28,063] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.647347413, 87.96957083000001, 0.0, 0.0, 19.0, 19.01862893456001, -1.17073488956611, 0.0, 1.0, 0.0]
[2019-04-05 13:09:28,064] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:09:28,064] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.5931982e-08 4.4136983e-01 2.4954618e-03 1.1025799e-01 4.4587338e-01
 2.8109948e-08 3.2464884e-06], sampled 0.45393815577276975
[2019-04-05 13:09:52,483] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11452219]
[2019-04-05 13:09:52,483] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.399957625, 75.7000125, 0.0, 0.0, 19.0, 19.29647423302784, -1.087963401164845, 0.0, 1.0, 0.0]
[2019-04-05 13:09:52,483] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:09:52,483] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.8868365e-08 4.0035754e-01 3.0020587e-03 1.4530165e-01 4.5133424e-01
 4.6420688e-08 4.3540886e-06], sampled 0.2718822742208057
[2019-04-05 13:10:17,947] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6442.6058 153115834.9994 -1488.0145
[2019-04-05 13:10:29,498] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5974.8695 178715232.5073 -2182.4430
[2019-04-05 13:10:35,078] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5784.0008 193261569.3430 -2133.1024
[2019-04-05 13:10:36,104] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 960000, evaluation results [960000.0, 5974.869471017339, 178715232.50730968, -2182.442980179999, 6442.605772937988, 153115834.9993578, -1488.0144701970626, 5784.000811598267, 193261569.3430055, -2133.1023705459666]
[2019-04-05 13:10:38,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0995877e-10 7.8135617e-02 6.3226861e-04 2.2796332e-03 9.1895247e-01
 3.1555231e-11 2.7730920e-09], sum to 1.0000
[2019-04-05 13:10:38,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3674
[2019-04-05 13:10:38,400] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.699999999999999, 93.0, 62.5, 0.0, 25.0, 22.53562957163353, -0.2352618839835537, 0.0, 1.0, 102606.5263139535], 
current ob forecast is [], 
actual action is [25.5], 
sim time this is 38400.0000, 
sim time next is 39000.0000, 
raw observation next is [7.7, 93.0, 65.0, 0.0, 25.5, 22.85138878792441, -0.1899308222066132, 0.0, 1.0, 73299.37224320171], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.21666666666666667, 0.0, 0.625, 0.40428239899370083, 0.4366897259311289, 0.0, 1.0, 0.34904462972953193], 
reward next is 0.0714, 
noisyNet noise sample is [array([0.19305544], dtype=float32), -0.08118385]. 
=============================================
[2019-04-05 13:10:38,408] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[104.39111 ]
 [103.888   ]
 [103.35725 ]
 [101.97477 ]
 [101.919174]], R is [[103.57646942]
 [102.68356323]
 [101.87101746]
 [101.13802338]
 [100.48377991]].
[2019-04-05 13:10:38,852] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:10:38,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:10:38,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run8
[2019-04-05 13:10:40,905] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.40627568e-12 2.00964307e-04 3.72748036e-05 9.17930901e-03
 9.90582466e-01 4.81744193e-11 1.26416895e-08], sum to 1.0000
[2019-04-05 13:10:40,907] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2447
[2019-04-05 13:10:40,928] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.4, 95.5, 0.0, 0.0, 19.5, 22.4858972012895, -0.2783164268573545, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 81000.0000, 
sim time next is 81600.0000, 
raw observation next is [0.3666666666666667, 95.33333333333333, 0.0, 0.0, 20.0, 22.43123358267231, -0.2931957859358458, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.4727608494921515, 0.9533333333333333, 0.0, 0.0, 0.16666666666666666, 0.36926946522269244, 0.40226807135471804, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.0507523], dtype=float32), -1.6769028]. 
=============================================
[2019-04-05 13:10:46,355] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0376341e-05 7.6576710e-01 2.4083698e-02 1.1836698e-01 9.1158815e-02
 1.2835900e-05 5.9027516e-04], sum to 1.0000
[2019-04-05 13:10:46,355] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4168
[2019-04-05 13:10:46,434] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 19.0, 20.62893329528729, -0.8457622795250245, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 288600.0000, 
sim time next is 289200.0000, 
raw observation next is [-12.63333333333333, 69.0, 25.0, 325.8333333333334, 19.0, 20.69880121606025, -0.8105849845277396, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.11265004616805181, 0.69, 0.08333333333333333, 0.3600368324125231, 0.08333333333333333, 0.22490010133835417, 0.2298050051574201, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08620556], dtype=float32), -1.1740392]. 
=============================================
[2019-04-05 13:10:46,974] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4526528e-05 8.1756902e-01 7.0778891e-03 5.7652444e-02 1.1755957e-01
 3.4326331e-06 1.2315747e-04], sum to 1.0000
[2019-04-05 13:10:46,974] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2824
[2019-04-05 13:10:46,980] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.4, 35.66666666666667, 98.16666666666667, 0.0, 19.0, 19.95231805381393, -1.070590144638411, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 483600.0000, 
sim time next is 484200.0000, 
raw observation next is [-0.3, 36.0, 94.0, 0.0, 19.0, 19.95819769087766, -1.065637199393974, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4542936288088643, 0.36, 0.31333333333333335, 0.0, 0.08333333333333333, 0.16318314090647176, 0.14478760020200868, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.760804], dtype=float32), -3.148997]. 
=============================================
[2019-04-05 13:10:48,601] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.25701973e-07 5.57823420e-01 1.34969378e-04 3.29908043e-01
 1.12133004e-01 1.96983518e-09 3.84800273e-07], sum to 1.0000
[2019-04-05 13:10:48,601] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6545
[2019-04-05 13:10:48,609] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.00000000000001, 9.999999999999998, 0.0, 19.5, 19.59150452568849, -0.9481646345167548, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1789800.0000, 
sim time next is 1790400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.5, 19.49264169376956, -0.9693241137950847, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.125, 0.12438680781412999, 0.17689196206830513, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.01987819], dtype=float32), 1.087297]. 
=============================================
[2019-04-05 13:10:53,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:10:53,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:10:53,351] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run8
[2019-04-05 13:10:59,350] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9498246e-08 6.5953094e-01 8.1676677e-02 1.8451661e-01 7.4253932e-02
 1.0783113e-08 2.1900792e-05], sum to 1.0000
[2019-04-05 13:10:59,352] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4936
[2019-04-05 13:10:59,380] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 54.0, 82.33333333333334, 44.0, 19.0, 20.55424649446362, -0.7738506754947028, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 659400.0000, 
sim time next is 660000.0000, 
raw observation next is [-0.6, 54.00000000000001, 82.66666666666667, 41.0, 19.0, 20.54808517237001, -0.7837802590696971, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.27555555555555555, 0.045303867403314914, 0.08333333333333333, 0.21234043103083403, 0.2387399136434343, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49312237], dtype=float32), 0.7169109]. 
=============================================
[2019-04-05 13:10:59,388] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.39831]
 [82.22815]
 [82.15988]
 [82.26874]
 [82.6265 ]], R is [[82.44535065]
 [82.62089539]
 [82.79468536]
 [82.89530945]
 [82.85207367]].
[2019-04-05 13:11:00,870] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0381356e-06 6.9187725e-01 6.9637964e-03 1.4977257e-01 1.5137295e-01
 6.3153851e-07 1.0788524e-05], sum to 1.0000
[2019-04-05 13:11:00,871] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7262
[2019-04-05 13:11:00,886] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 74.0, 0.0, 0.0, 19.0, 19.53642492244313, -1.065548037060364, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 697200.0000, 
sim time next is 697800.0000, 
raw observation next is [-3.4, 74.5, 0.0, 0.0, 19.0, 19.41864988585301, -1.082803369472738, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.745, 0.0, 0.0, 0.08333333333333333, 0.11822082382108423, 0.13906554350908737, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13837874], dtype=float32), -0.22168937]. 
=============================================
[2019-04-05 13:11:01,415] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3221682e-08 4.9059653e-01 5.1997084e-04 4.9399987e-01 1.4878532e-02
 5.7033978e-10 5.0823191e-06], sum to 1.0000
[2019-04-05 13:11:01,415] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-05 13:11:01,425] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.383333333333333, 79.83333333333334, 0.0, 0.0, 19.0, 19.47476997396367, -1.02937072811009, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 863400.0000, 
sim time next is 864000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 19.0, 19.4278322629571, -1.045180051387537, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.08333333333333333, 0.11898602191309167, 0.1516066495374877, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04452239], dtype=float32), 1.2023832]. 
=============================================
[2019-04-05 13:11:01,433] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.4617  ]
 [80.63831 ]
 [80.690094]
 [80.84665 ]
 [80.826004]], R is [[78.43544006]
 [78.6510849 ]
 [78.86457825]
 [79.07593536]
 [79.28517914]].
[2019-04-05 13:11:02,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2546363e-08 1.8118149e-01 1.1338171e-04 1.3764697e-03 8.1732488e-01
 7.7297484e-09 3.7031105e-06], sum to 1.0000
[2019-04-05 13:11:02,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1208
[2019-04-05 13:11:02,972] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 92.0, 120.0, 0.0, 19.5, 21.92748272183827, -0.422425290793328, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1339200.0000, 
sim time next is 1339800.0000, 
raw observation next is [1.1, 92.0, 117.6666666666667, 0.0, 19.0, 21.92549937181457, -0.4229538646636192, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.3922222222222223, 0.0, 0.08333333333333333, 0.3271249476512142, 0.35901537844546033, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24894081], dtype=float32), -0.5923463]. 
=============================================
[2019-04-05 13:11:02,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6538945e-09 7.3982733e-01 1.6275797e-03 2.9955092e-03 2.5553098e-01
 2.5307205e-07 1.8257655e-05], sum to 1.0000
[2019-04-05 13:11:02,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1850
[2019-04-05 13:11:02,999] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-13.9, 70.0, 0.0, 0.0, 19.0, 18.72587275006059, -1.209694745118646, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 342000.0000, 
sim time next is 342600.0000, 
raw observation next is [-13.9, 69.33333333333334, 0.0, 0.0, 19.0, 18.67504693762548, -1.230252197689814, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6933333333333335, 0.0, 0.0, 0.08333333333333333, 0.05625391146879011, 0.08991593410339534, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7840056], dtype=float32), 0.9847472]. 
=============================================
[2019-04-05 13:11:04,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.30541751e-08 6.09469041e-02 1.12866955e-02 8.02205026e-01
 1.25553489e-01 6.13634299e-08 7.81712879e-06], sum to 1.0000
[2019-04-05 13:11:04,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0601
[2019-04-05 13:11:04,527] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 21.5, 20.51078575173758, -0.7574946637661903, 0.0, 1.0, 168661.7900456394], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 867000.0000, 
sim time next is 867600.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 20.5, 20.54773400966177, -0.7407489552840388, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.3988919667590028, 0.8, 0.0, 0.0, 0.20833333333333334, 0.2123111674718142, 0.2530836815719871, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.39317164], dtype=float32), 0.37095726]. 
=============================================
[2019-04-05 13:11:13,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8347336e-08 2.7625313e-01 7.9231253e-03 1.1273018e-01 6.0309315e-01
 1.9210018e-09 4.2815569e-07], sum to 1.0000
[2019-04-05 13:11:13,722] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6360
[2019-04-05 13:11:13,739] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 20.0, 20.85739163422324, -0.7750743613041099, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 524400.0000, 
sim time next is 525000.0000, 
raw observation next is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 20.5, 20.82296870666311, -0.7855802486908129, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.584949215143121, 0.8816666666666667, 0.0, 0.0, 0.20833333333333334, 0.23524739222192595, 0.23813991710306237, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.77706647], dtype=float32), 0.09257175]. 
=============================================
[2019-04-05 13:11:13,753] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.02886 ]
 [78.45006 ]
 [78.19397 ]
 [78.617386]
 [78.19493 ]], R is [[77.87125397]
 [77.94968414]
 [77.88447571]
 [77.67705536]
 [77.32885742]].
[2019-04-05 13:11:13,853] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1713529e-09 1.6104475e-01 1.0896526e-02 8.0439866e-01 2.3659574e-02
 1.2083331e-08 4.2336504e-07], sum to 1.0000
[2019-04-05 13:11:13,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0847
[2019-04-05 13:11:13,878] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 54.0, 64.33333333333334, 30.33333333333334, 19.0, 20.57482137768116, -0.8131956944986073, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 661800.0000, 
sim time next is 662400.0000, 
raw observation next is [-0.6, 54.0, 55.0, 26.5, 19.0, 20.48187652044213, -0.8300335725490254, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.44598337950138506, 0.54, 0.18333333333333332, 0.029281767955801105, 0.08333333333333333, 0.20682304337017735, 0.2233221424836582, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7712554], dtype=float32), -0.7086277]. 
=============================================
[2019-04-05 13:11:14,425] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 13:11:14,427] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:11:14,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:11:14,428] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:11:14,429] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:11:14,429] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:11:14,433] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:11:14,440] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run50
[2019-04-05 13:11:14,458] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run50
[2019-04-05 13:11:14,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run50
[2019-04-05 13:12:22,007] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11294143]
[2019-04-05 13:12:22,007] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.122774022, 63.84858629, 115.6082831266667, 797.66827325, 19.0, 21.5228096758012, -0.5716155204672039, 1.0, 1.0, 0.0]
[2019-04-05 13:12:22,007] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:12:22,008] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.5622693e-08 8.1495243e-01 1.3036527e-03 3.2403383e-02 1.5133515e-01
 2.1370338e-08 5.2753817e-06], sampled 0.18401805426939444
[2019-04-05 13:12:28,160] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6300.6814 152766815.4482 -1490.1707
[2019-04-05 13:12:39,475] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5945.4773 180945196.3790 -2128.3409
[2019-04-05 13:12:45,548] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5934.9764 194603849.6909 -2044.0277
[2019-04-05 13:12:46,573] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 980000, evaluation results [980000.0, 5945.477251634143, 180945196.3789688, -2128.3409096613655, 6300.68142967975, 152766815.44821465, -1490.1707330887946, 5934.976438849411, 194603849.6909334, -2044.027715046937]
[2019-04-05 13:12:51,629] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2520926e-06 5.4648888e-01 1.2440532e-02 9.4129682e-02 3.4691262e-01
 4.9146831e-07 2.6504360e-05], sum to 1.0000
[2019-04-05 13:12:51,630] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5678
[2019-04-05 13:12:51,658] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 72.33333333333333, 0.0, 0.0, 19.5, 19.00628034528469, -1.140808218914536, 0.0, 1.0, 39606.47219853803], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 794400.0000, 
sim time next is 795000.0000, 
raw observation next is [-7.299999999999999, 71.66666666666667, 0.0, 0.0, 19.5, 19.06213521216249, -1.146548974050843, 0.0, 1.0, 18741.67669087487], 
processed observation next is [1.0, 0.17391304347826086, 0.2603878116343491, 0.7166666666666667, 0.0, 0.0, 0.125, 0.08851126768020763, 0.117817008649719, 0.0, 1.0, 0.08924607948035651], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.2430915], dtype=float32), 0.266101]. 
=============================================
[2019-04-05 13:12:51,675] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[61.09982 ]
 [60.7252  ]
 [60.946514]
 [61.20662 ]
 [60.965458]], R is [[61.25164032]
 [61.56769562]
 [61.95201874]
 [62.33250046]
 [62.6377449 ]].
[2019-04-05 13:12:58,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2899003e-10 8.4351772e-01 1.1637557e-04 1.5084727e-02 1.4127967e-01
 2.9255498e-10 1.5700314e-06], sum to 1.0000
[2019-04-05 13:12:58,744] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7669
[2019-04-05 13:12:58,776] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 26.33333333333334, 59.66666666666667, 613.1666666666667, 19.0, 21.7124520328325, -0.5531633724302617, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [3.3, 26.16666666666667, 57.33333333333333, 546.3333333333334, 19.0, 21.6823782945668, -0.563420596111853, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.554016620498615, 0.2616666666666667, 0.1911111111111111, 0.6036832412523021, 0.08333333333333333, 0.3068648578805666, 0.3121931346293823, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2228452], dtype=float32), -1.1012057]. 
=============================================
[2019-04-05 13:13:01,945] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9335533e-06 7.9014814e-01 1.3690729e-03 1.4094332e-01 6.7525968e-02
 9.2132717e-07 9.5855012e-06], sum to 1.0000
[2019-04-05 13:13:01,949] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9823
[2019-04-05 13:13:01,960] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.6000000000000001, 92.0, 92.0, 0.0, 19.0, 20.71321736068293, -0.7335933360921025, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1429800.0000, 
sim time next is 1430400.0000, 
raw observation next is [0.7000000000000001, 92.0, 91.0, 0.0, 19.5, 20.61688493052736, -0.7127387778493056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4819944598337951, 0.92, 0.30333333333333334, 0.0, 0.125, 0.21807374421061324, 0.2624204073835648, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4783136], dtype=float32), -0.5158083]. 
=============================================
[2019-04-05 13:13:03,023] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0392033e-08 9.7998202e-01 2.7881499e-05 3.6097553e-03 1.6379789e-02
 9.4070440e-09 4.9122821e-07], sum to 1.0000
[2019-04-05 13:13:03,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2965
[2019-04-05 13:13:03,038] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.8, 96.66666666666666, 0.0, 0.0, 19.0, 21.01730846406997, -0.5453829016437689, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1651200.0000, 
sim time next is 1651800.0000, 
raw observation next is [6.699999999999999, 96.83333333333334, 0.0, 0.0, 19.0, 21.03946947447997, -0.5591099151541782, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6481994459833795, 0.9683333333333334, 0.0, 0.0, 0.08333333333333333, 0.2532891228733307, 0.3136300282819406, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05138579], dtype=float32), -1.5975716]. 
=============================================
[2019-04-05 13:13:06,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5187571e-08 9.7028446e-01 2.7169005e-03 4.9281050e-03 2.2065392e-02
 1.7575970e-09 5.1765942e-06], sum to 1.0000
[2019-04-05 13:13:06,179] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0832
[2019-04-05 13:13:06,196] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 19.0, 23.98522894924166, -0.09506078070102479, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [12.2, 54.0, 25.5, 18.5, 19.0, 23.43598229568804, -0.0822838084632297, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8005540166204987, 0.54, 0.085, 0.020441988950276244, 0.08333333333333333, 0.45299852464066986, 0.4725720638455901, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.282465], dtype=float32), 0.3883627]. 
=============================================
[2019-04-05 13:13:07,297] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.6540284e-09 2.0752735e-01 8.3033892e-04 9.7793909e-03 7.8183216e-01
 5.7065172e-08 3.0680236e-05], sum to 1.0000
[2019-04-05 13:13:07,298] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2489
[2019-04-05 13:13:07,313] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.683333333333334, 84.16666666666666, 0.0, 0.0, 21.5, 20.74802034733248, -0.641575543918209, 0.0, 1.0, 197521.4097073558], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1572600.0000, 
sim time next is 1573200.0000, 
raw observation next is [4.7, 84.0, 0.0, 0.0, 22.0, 20.70284852778179, -0.5855001381535997, 0.0, 1.0, 198919.6694247354], 
processed observation next is [1.0, 0.21739130434782608, 0.592797783933518, 0.84, 0.0, 0.0, 0.3333333333333333, 0.22523737731514912, 0.30483328728213344, 0.0, 1.0, 0.9472365210701685], 
reward next is 0.5714, 
noisyNet noise sample is [array([-0.113416], dtype=float32), 0.57258904]. 
=============================================
[2019-04-05 13:13:09,016] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0943746e-09 9.8679209e-01 5.1566568e-04 6.4435257e-03 6.2474059e-03
 9.6735677e-09 1.1835225e-06], sum to 1.0000
[2019-04-05 13:13:09,020] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3524
[2019-04-05 13:13:09,029] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.1, 78.33333333333333, 0.0, 0.0, 19.0, 21.25571734815782, -0.4248367115942484, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1210200.0000, 
sim time next is 1210800.0000, 
raw observation next is [16.1, 78.66666666666667, 0.0, 0.0, 19.0, 21.23720030757279, -0.4277376777297008, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.26976669229773265, 0.3574207740900997, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28662997], dtype=float32), -0.52691853]. 
=============================================
[2019-04-05 13:13:11,298] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3651526e-05 7.1466070e-01 1.6940701e-03 1.4281468e-02 2.6930726e-01
 1.1056209e-06 2.1784903e-05], sum to 1.0000
[2019-04-05 13:13:11,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0241
[2019-04-05 13:13:11,310] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 68.5, 132.3333333333333, 94.99999999999999, 19.0, 20.0661331547145, -0.9336027212147094, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2213400.0000, 
sim time next is 2214000.0000, 
raw observation next is [-3.9, 68.0, 138.5, 142.5, 19.0, 20.21906011551023, -0.9174182968284619, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.46166666666666667, 0.1574585635359116, 0.08333333333333333, 0.1849216762925193, 0.19419390105717937, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48227882], dtype=float32), 1.618493]. 
=============================================
[2019-04-05 13:13:11,330] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[56.330357]
 [56.22376 ]
 [56.162144]
 [56.15581 ]
 [56.136044]], R is [[55.87772369]
 [55.31894684]
 [54.76575851]
 [54.2181015 ]
 [53.67592239]].
[2019-04-05 13:13:16,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4646941e-08 6.7947483e-01 9.3960820e-04 3.3395167e-02 2.8618896e-01
 4.9590021e-09 1.3117918e-06], sum to 1.0000
[2019-04-05 13:13:16,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6793
[2019-04-05 13:13:16,716] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 19.0, 19.63241428923619, -1.002751999055068, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1835400.0000, 
sim time next is 1836000.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 19.0, 19.50655622149418, -1.027506292284189, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.2908587257617729, 0.79, 0.0, 0.0, 0.08333333333333333, 0.1255463517911816, 0.15749790257193697, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9364486], dtype=float32), -0.83010453]. 
=============================================
[2019-04-05 13:13:16,746] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.92051 ]
 [73.102554]
 [73.39514 ]
 [73.54736 ]
 [73.787384]], R is [[73.06846619]
 [73.33778381]
 [73.60440826]
 [73.86836243]
 [74.12967682]].
[2019-04-05 13:13:17,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3611740e-09 9.5227170e-01 5.3285639e-04 1.7719313e-03 4.5423020e-02
 6.4204255e-09 5.2040878e-07], sum to 1.0000
[2019-04-05 13:13:17,051] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7809
[2019-04-05 13:13:17,107] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 77.0, 124.6666666666667, 58.16666666666666, 19.0, 21.61382759083365, -0.6193804759031138, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1851600.0000, 
sim time next is 1852200.0000, 
raw observation next is [-5.6, 76.5, 120.0, 51.0, 19.0, 21.52561198261282, -0.6375771631724606, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.765, 0.4, 0.056353591160221, 0.08333333333333333, 0.2938009985510683, 0.28747427894251315, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5729684], dtype=float32), -2.177278]. 
=============================================
[2019-04-05 13:13:17,468] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4283775e-08 7.3623466e-01 1.0801867e-03 5.6259274e-02 2.0636135e-01
 2.2348061e-08 6.4455970e-05], sum to 1.0000
[2019-04-05 13:13:17,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5516
[2019-04-05 13:13:17,499] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 91.83333333333333, 0.0, 0.0, 19.0, 19.09154563849654, -1.142322870864661, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2865000.0000, 
sim time next is 2865600.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 18.9595271469241, -1.155613006736744, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.07996059557700826, 0.11479566442108531, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19315188], dtype=float32), 0.019333776]. 
=============================================
[2019-04-05 13:13:18,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9679985e-07 6.1386293e-01 1.1169689e-02 1.5293173e-01 2.2189578e-01
 7.6052623e-07 1.3844989e-04], sum to 1.0000
[2019-04-05 13:13:18,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3623
[2019-04-05 13:13:18,610] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 19.0, 19.11629078909899, -1.144174405080731, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1920600.0000, 
sim time next is 1921200.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 19.0, 19.06525602720911, -1.165813212188106, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.08333333333333333, 0.08877133560075912, 0.11139559593729802, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6851175], dtype=float32), 0.26266]. 
=============================================
[2019-04-05 13:13:20,780] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-05 13:13:20,781] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:13:20,782] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:13:20,784] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:13:20,785] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:13:20,786] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:13:20,787] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:13:21,589] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run51
[2019-04-05 13:13:21,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run51
[2019-04-05 13:13:21,625] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run51
[2019-04-05 13:14:32,955] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6244.6587 148435396.5283 -1818.6535
[2019-04-05 13:14:45,092] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11323182]
[2019-04-05 13:14:45,092] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.0, 38.5, 116.0, 809.0, 19.0, 21.94352333289733, -0.4889557217593322, 1.0, 1.0, 0.0]
[2019-04-05 13:14:45,092] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:14:45,093] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.7335400e-07 8.4245795e-01 2.6737596e-03 2.9322201e-02 1.2551647e-01
 2.3426826e-07 2.8543651e-05], sampled 0.3974943335357104
[2019-04-05 13:14:45,604] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5796.2106 173813564.2585 -2405.7531
[2019-04-05 13:14:50,229] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5893.1058 188728680.4948 -2325.6731
[2019-04-05 13:14:51,254] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1000000, evaluation results [1000000.0, 5796.210585867809, 173813564.25850084, -2405.75312724794, 6244.658726626036, 148435396.52827302, -1818.6535496260972, 5893.10581117018, 188728680.4947851, -2325.673106380334]
[2019-04-05 13:14:53,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0518245e-08 8.3003730e-02 1.9127313e-03 5.4928315e-01 3.6579993e-01
 2.0338742e-08 5.3461440e-07], sum to 1.0000
[2019-04-05 13:14:53,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5650
[2019-04-05 13:14:53,294] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.45, 92.0, 0.0, 0.0, 19.0, 20.12901892710298, -0.6691074051895799, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1308600.0000, 
sim time next is 1309200.0000, 
raw observation next is [2.366666666666667, 92.0, 0.0, 0.0, 19.0, 20.13206829486413, -0.6794209650501964, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.528162511542013, 0.92, 0.0, 0.0, 0.08333333333333333, 0.17767235790534416, 0.27352634498326783, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17712462], dtype=float32), 0.09330948]. 
=============================================
[2019-04-05 13:14:57,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5439292e-06 4.6360990e-01 7.2143762e-03 5.0185721e-02 4.7875178e-01
 3.9594997e-06 2.2972643e-04], sum to 1.0000
[2019-04-05 13:14:57,561] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3233
[2019-04-05 13:14:57,570] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.45, 65.0, 227.0, 4.0, 19.0, 20.05891560231428, -0.9993818830254142, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1945800.0000, 
sim time next is 1946400.0000, 
raw observation next is [-4.266666666666667, 65.0, 212.0, 3.333333333333333, 19.0, 19.95216856916456, -1.012152329388376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3444136657433057, 0.65, 0.7066666666666667, 0.0036832412523020255, 0.08333333333333333, 0.16268071409704662, 0.16261589020387465, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8615001], dtype=float32), 0.20057243]. 
=============================================
[2019-04-05 13:15:15,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2675427e-10 5.4006076e-01 1.0455192e-04 6.8962760e-02 3.9087170e-01
 1.9774821e-10 1.8984090e-07], sum to 1.0000
[2019-04-05 13:15:15,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4276
[2019-04-05 13:15:15,988] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.7, 52.83333333333333, 231.6666666666667, 282.6666666666667, 19.0, 20.8217890391178, -0.730044303003, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2379000.0000, 
sim time next is 2379600.0000, 
raw observation next is [-0.6, 54.0, 221.5, 212.0, 19.0, 20.77206190970798, -0.739830356585763, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.44598337950138506, 0.54, 0.7383333333333333, 0.23425414364640884, 0.08333333333333333, 0.23100515914233158, 0.253389881138079, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20033169], dtype=float32), 0.9886396]. 
=============================================
[2019-04-05 13:15:16,548] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1807222e-08 9.9194252e-01 1.0432148e-03 3.4391033e-03 3.5653338e-03
 1.3788517e-09 9.6467984e-06], sum to 1.0000
[2019-04-05 13:15:16,554] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8515
[2019-04-05 13:15:16,568] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 19.0, 19.38389414767423, -1.035268944767762, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2075400.0000, 
sim time next is 2076000.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 19.0, 19.34943634628399, -1.054190273193133, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.08333333333333333, 0.11245302885699922, 0.1486032422689557, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6455834], dtype=float32), -1.8559349]. 
=============================================
[2019-04-05 13:15:16,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.95873]
 [71.535  ]
 [73.21076]
 [74.98232]
 [76.96168]], R is [[68.30623627]
 [68.62317657]
 [68.93694305]
 [69.24757385]
 [69.55509949]].
[2019-04-05 13:15:19,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6824975e-08 1.2864967e-01 4.1551078e-03 9.3084695e-03 8.5788488e-01
 8.5199474e-09 1.8356287e-06], sum to 1.0000
[2019-04-05 13:15:19,481] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8256
[2019-04-05 13:15:19,495] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.2, 35.0, 0.0, 0.0, 19.5, 20.34645765742055, -0.8818804883046049, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2496600.0000, 
sim time next is 2497200.0000, 
raw observation next is [-1.2, 34.33333333333334, 0.0, 0.0, 20.0, 20.34082285656221, -0.8958191633138833, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.34333333333333343, 0.0, 0.0, 0.16666666666666666, 0.19506857138018407, 0.20139361222870555, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.27379096], dtype=float32), -0.25390598]. 
=============================================
[2019-04-05 13:15:20,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1844157e-05 6.0632223e-01 2.7390032e-03 4.3401956e-03 3.8635239e-01
 1.5215312e-06 2.3276798e-04], sum to 1.0000
[2019-04-05 13:15:20,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3487
[2019-04-05 13:15:20,163] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.5, 56.5, 159.3333333333333, 324.6666666666666, 19.5, 19.99746885528122, -0.9819227895042161, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [-3.0, 55.0, 163.0, 370.5, 19.0, 20.05151098775371, -0.9601692921060888, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.55, 0.5433333333333333, 0.4093922651933702, 0.08333333333333333, 0.1709592489794757, 0.1799435692979704, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.382582], dtype=float32), -1.6814867]. 
=============================================
[2019-04-05 13:15:20,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3037348e-07 9.6297431e-01 1.0457993e-02 5.0505800e-03 2.1491665e-02
 6.3331697e-08 2.5349065e-05], sum to 1.0000
[2019-04-05 13:15:20,786] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9171
[2019-04-05 13:15:20,808] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 19.0, 20.06473244167006, -0.9321816576212272, 1.0, 1.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2814600.0000, 
sim time next is 2815200.0000, 
raw observation next is [6.0, 30.0, 183.5, 86.5, 19.0, 20.19272510171822, -0.9177176069814615, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.6086956521739131, 0.6288088642659281, 0.3, 0.6116666666666667, 0.09558011049723757, 0.08333333333333333, 0.18272709180985172, 0.19409413100617948, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37499946], dtype=float32), 1.5165497]. 
=============================================
[2019-04-05 13:15:22,325] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8493969e-08 9.5868361e-01 1.2296716e-04 5.6962534e-03 3.5483118e-02
 1.4154099e-07 1.3774263e-05], sum to 1.0000
[2019-04-05 13:15:22,328] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9016
[2019-04-05 13:15:22,335] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 92.0, 15.5, 0.0, 19.0, 19.48848375879112, -0.899509378844546, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1670400.0000, 
sim time next is 1671000.0000, 
raw observation next is [3.116666666666667, 92.0, 20.33333333333334, 0.0, 19.0, 19.49428035515874, -0.907193105725998, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5489381348107111, 0.92, 0.0677777777777778, 0.0, 0.08333333333333333, 0.12452336292989497, 0.197602298091334, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1992593], dtype=float32), -0.27093118]. 
=============================================
[2019-04-05 13:15:22,357] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[71.14034 ]
 [73.118126]
 [75.57605 ]
 [78.351524]
 [81.518364]], R is [[68.27139282]
 [67.58867645]
 [66.91278839]
 [66.24365997]
 [65.58122253]].
[2019-04-05 13:15:29,579] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-05 13:15:29,580] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:15:29,581] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:15:29,582] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:15:29,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:15:29,582] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:15:29,582] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:15:29,590] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run52
[2019-04-05 13:15:29,590] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run52
[2019-04-05 13:15:29,627] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run52
[2019-04-05 13:15:34,410] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11249329]
[2019-04-05 13:15:34,410] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.5674915775, 92.152743005, 0.0, 0.0, 19.0, 20.47855328512568, -0.7287308245586605, 0.0, 1.0, 0.0]
[2019-04-05 13:15:34,410] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:15:34,411] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.1341179e-09 4.6316719e-01 6.0974591e-04 3.3252984e-02 5.0296944e-01
 1.0148382e-09 6.8933815e-07], sampled 0.19042616578986005
[2019-04-05 13:15:48,965] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11249329]
[2019-04-05 13:15:48,965] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.3, 76.0, 0.0, 0.0, 20.5, 19.94123621311349, -0.9800291359355593, 0.0, 1.0, 198677.5785030852]
[2019-04-05 13:15:48,965] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:15:48,966] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.09541383e-07 4.54108030e-01 2.99590710e-03 5.87951802e-02
 4.84089792e-01 6.60198154e-08 1.07151445e-05], sampled 0.4187701272029687
[2019-04-05 13:16:46,560] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6577.0908 152270405.6628 -1269.8931
[2019-04-05 13:16:55,837] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5932.9465 183178831.6388 -1962.6027
[2019-04-05 13:17:01,066] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5899.4079 193604563.3413 -2040.6583
[2019-04-05 13:17:02,091] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1020000, evaluation results [1020000.0, 5932.946451886708, 183178831.63884103, -1962.602748522834, 6577.0908250547145, 152270405.66275898, -1269.8931347526448, 5899.407859964946, 193604563.34125572, -2040.6582785826638]
[2019-04-05 13:17:06,321] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5525906e-05 9.4601059e-01 4.4640219e-03 2.5593454e-02 2.3724720e-02
 2.4308179e-06 1.7930343e-04], sum to 1.0000
[2019-04-05 13:17:06,322] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3370
[2019-04-05 13:17:06,332] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.1, 58.0, 224.0, 171.0, 19.5, 21.4255683269606, -0.6917744932183402, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2633400.0000, 
sim time next is 2634000.0000, 
raw observation next is [-2.833333333333333, 56.66666666666667, 227.5, 167.0, 19.0, 21.37178597263853, -0.6945289096380152, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3841181902123731, 0.5666666666666668, 0.7583333333333333, 0.18453038674033148, 0.08333333333333333, 0.2809821643865442, 0.26849036345399496, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.15356848], dtype=float32), 0.40368196]. 
=============================================
[2019-04-05 13:17:06,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[52.612   ]
 [52.43534 ]
 [52.24321 ]
 [51.974953]
 [51.42597 ]], R is [[52.25682831]
 [51.73426056]
 [51.21691895]
 [50.70475006]
 [50.19770432]].
[2019-04-05 13:17:09,407] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0832001e-04 7.1183711e-01 6.2156789e-02 7.5734600e-02 1.4973167e-01
 3.2509361e-06 3.2827235e-04], sum to 1.0000
[2019-04-05 13:17:09,407] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8918
[2019-04-05 13:17:09,451] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 75.0, 34.50000000000001, 218.3333333333333, 19.0, 19.34136674535513, -1.061917430932412, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2190000.0000, 
sim time next is 2190600.0000, 
raw observation next is [-5.6, 75.0, 41.0, 262.0, 19.5, 19.48016574319155, -1.026339326496886, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.13666666666666666, 0.28950276243093925, 0.125, 0.1233471452659624, 0.15788689116770463, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3597227], dtype=float32), 0.7295911]. 
=============================================
[2019-04-05 13:17:09,830] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9143099e-05 7.2420824e-01 1.1508611e-02 3.7317265e-02 2.2584289e-01
 7.3665492e-06 1.0765317e-03], sum to 1.0000
[2019-04-05 13:17:09,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3641
[2019-04-05 13:17:09,849] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.1, 71.66666666666667, 107.0, 300.6666666666667, 19.0, 20.18217993448509, -0.9407350511968439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2195400.0000, 
sim time next is 2196000.0000, 
raw observation next is [-5.0, 71.0, 109.5, 225.5, 19.0, 20.20653609170101, -0.9402332476921237, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.32409972299168976, 0.71, 0.365, 0.24917127071823206, 0.08333333333333333, 0.1838780076417509, 0.18658891743595876, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41011643], dtype=float32), -0.64532113]. 
=============================================
[2019-04-05 13:17:09,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[49.32569 ]
 [48.903404]
 [48.787994]
 [48.31614 ]
 [47.91472 ]], R is [[49.2512207 ]
 [48.75870895]
 [48.27112198]
 [47.78841019]
 [47.3105278 ]].
[2019-04-05 13:17:16,688] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6767164e-09 4.4107433e-02 2.8960214e-03 1.4691937e-01 8.0606699e-01
 3.5203502e-09 1.0147054e-05], sum to 1.0000
[2019-04-05 13:17:16,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6440
[2019-04-05 13:17:16,705] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.0, 60.0, 104.0, 767.3333333333334, 20.0, 20.47619780272099, -0.7080531486950937, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 2988600.0000, 
sim time next is 2989200.0000, 
raw observation next is [-2.0, 60.00000000000001, 102.5, 759.1666666666667, 20.5, 20.49033053258744, -0.7049511308818031, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6000000000000001, 0.3416666666666667, 0.8388581952117865, 0.20833333333333334, 0.2075275443822866, 0.26501628970606567, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.5885361], dtype=float32), 1.1308755]. 
=============================================
[2019-04-05 13:17:21,809] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.8347049e-08 9.4033027e-01 5.4290053e-03 8.2713354e-04 5.3410482e-02
 4.6494364e-08 3.0065519e-06], sum to 1.0000
[2019-04-05 13:17:21,810] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6396
[2019-04-05 13:17:21,823] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 20.0, 19.39722310919174, -0.9530380655557309, 0.0, 1.0, 63686.48185801158], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2940000.0000, 
sim time next is 2940600.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 19.49528645374299, -0.9371390476875304, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.12460720447858264, 0.18762031743748986, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8588352], dtype=float32), -0.1259924]. 
=============================================
[2019-04-05 13:17:24,484] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.8721480e-11 2.6837578e-01 3.4376740e-04 2.9411123e-03 7.2833925e-01
 2.7886798e-10 4.8056080e-08], sum to 1.0000
[2019-04-05 13:17:24,488] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2335
[2019-04-05 13:17:24,494] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3201563e-08 9.7075045e-01 5.3752552e-05 2.2613563e-02 6.5814932e-03
 1.3577927e-08 6.3038777e-07], sum to 1.0000
[2019-04-05 13:17:24,496] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6366
[2019-04-05 13:17:24,525] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 94.0, 743.5, 21.0, 23.89992797476754, 0.02995541457399229, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3596400.0000, 
sim time next is 3597000.0000, 
raw observation next is [-0.8333333333333334, 42.16666666666666, 91.33333333333334, 728.6666666666667, 20.0, 23.85697200852638, 0.03012785966804058, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.43951985226223456, 0.4216666666666666, 0.30444444444444446, 0.8051565377532229, 0.16666666666666666, 0.4880810007105317, 0.5100426198893468, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.19749896], dtype=float32), 0.6092456]. 
=============================================
[2019-04-05 13:17:24,529] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[100.51244 ]
 [100.59561 ]
 [100.5464  ]
 [100.656815]
 [100.60197 ]], R is [[100.59552002]
 [100.3038559 ]
 [ 99.87224579]
 [ 99.30210114]
 [ 98.80908203]].
[2019-04-05 13:17:24,542] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 54.0, 106.0, 759.0, 19.0, 21.57501009897689, -0.5611821151105847, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3061800.0000, 
sim time next is 3062400.0000, 
raw observation next is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 19.0, 21.53684559798605, -0.5648322162883571, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.356111111111111, 0.8471454880294659, 0.08333333333333333, 0.29473713316550426, 0.31172259457054763, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11515088], dtype=float32), 1.1260664]. 
=============================================
[2019-04-05 13:17:38,544] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.7285280e-05 4.2533612e-01 1.7258441e-03 1.2484210e-01 4.4801486e-01
 4.4979675e-08 3.6743149e-06], sum to 1.0000
[2019-04-05 13:17:38,547] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9575
[2019-04-05 13:17:38,564] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 52.0, 104.6666666666667, 780.1666666666667, 19.0, 21.76862550188147, -0.5963865728123429, 1.0, 1.0, 4670.102917557634], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3421200.0000, 
sim time next is 3421800.0000, 
raw observation next is [3.0, 53.5, 103.0, 775.0, 19.5, 21.46528207486423, -0.5595039291575813, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.535, 0.3433333333333333, 0.856353591160221, 0.125, 0.28877350623868586, 0.31349869028080624, 1.0, 1.0, 0.0], 
reward next is 0.3335, 
noisyNet noise sample is [array([0.8033538], dtype=float32), -0.3651429]. 
=============================================
[2019-04-05 13:17:39,109] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-05 13:17:39,110] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:17:39,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:17:39,111] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:17:39,117] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:17:39,117] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:17:39,118] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:17:39,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run53
[2019-04-05 13:17:39,143] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run53
[2019-04-05 13:17:39,159] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run53
[2019-04-05 13:18:51,652] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6228.6439 149378040.5251 -1831.8476
[2019-04-05 13:19:01,854] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5726.1447 176161427.9819 -2425.6795
[2019-04-05 13:19:08,627] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5901.9885 188649823.2043 -2339.5899
[2019-04-05 13:19:09,652] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1040000, evaluation results [1040000.0, 5726.144654162133, 176161427.98194942, -2425.6794582633624, 6228.643909901053, 149378040.525076, -1831.8475677427662, 5901.988507804327, 188649823.2043436, -2339.5899176866355]
[2019-04-05 13:19:14,388] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1292722e-05 9.0323555e-01 1.2745749e-02 7.2437054e-03 7.6609425e-02
 4.8462643e-06 1.0940505e-04], sum to 1.0000
[2019-04-05 13:19:14,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4002
[2019-04-05 13:19:14,413] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.9999999999999999, 52.0, 100.6666666666667, 675.6666666666666, 19.0, 20.43655612199414, -0.8408987127333795, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3404400.0000, 
sim time next is 3405000.0000, 
raw observation next is [1.5, 50.0, 102.3333333333333, 693.3333333333334, 19.0, 20.54587465532095, -0.8255969088101077, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5041551246537397, 0.5, 0.341111111111111, 0.7661141804788214, 0.08333333333333333, 0.2121562212767459, 0.22480103039663077, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.27797955], dtype=float32), -0.47094637]. 
=============================================
[2019-04-05 13:19:14,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[54.38707]
 [53.7831 ]
 [53.07385]
 [52.42423]
 [52.19562]], R is [[54.87555695]
 [54.3268013 ]
 [53.783535  ]
 [53.24570084]
 [52.71324539]].
[2019-04-05 13:19:17,306] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4764616e-07 9.4734925e-01 4.0712664e-04 4.9127699e-03 4.7319107e-02
 1.1926792e-07 1.1234090e-05], sum to 1.0000
[2019-04-05 13:19:17,308] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9459
[2019-04-05 13:19:17,320] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 34.0, 77.0, 630.0, 19.0, 21.90019874519111, -0.4615847952017753, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3945600.0000, 
sim time next is 3946200.0000, 
raw observation next is [-4.166666666666667, 34.66666666666667, 73.33333333333334, 598.6666666666666, 19.0, 22.00747485772573, -0.4485596527104168, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3471837488457987, 0.34666666666666673, 0.24444444444444446, 0.6615101289134437, 0.08333333333333333, 0.33395623814381076, 0.35048011576319443, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.111658], dtype=float32), -0.047947217]. 
=============================================
[2019-04-05 13:19:18,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.7459619e-05 7.0718336e-01 7.7759242e-03 8.0517165e-02 2.0422311e-01
 9.1827933e-06 2.1386593e-04], sum to 1.0000
[2019-04-05 13:19:18,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7368
[2019-04-05 13:19:18,147] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.333333333333332, 45.5, 104.6666666666667, 725.6666666666666, 19.5, 22.38922425136396, -0.4504490602322437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4009800.0000, 
sim time next is 4010400.0000, 
raw observation next is [-9.0, 44.0, 106.5, 739.5, 19.5, 22.41705338328203, -0.4395904989702293, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.21329639889196678, 0.44, 0.355, 0.8171270718232044, 0.125, 0.3680877819401693, 0.35346983367659024, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.16581708], dtype=float32), -1.1583269]. 
=============================================
[2019-04-05 13:19:18,249] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2723618e-06 8.1013060e-01 2.4033897e-03 3.1028183e-02 1.5639618e-01
 1.0913053e-06 3.8246839e-05], sum to 1.0000
[2019-04-05 13:19:18,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1817
[2019-04-05 13:19:18,267] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.8333333333333334, 60.83333333333334, 111.0, 783.3333333333333, 19.0, 21.07259094081872, -0.7726950719951767, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3495000.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 19.0, 20.96476142988143, -0.6816901275752363, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 0.08333333333333333, 0.24706345249011927, 0.2727699574749212, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.0387713], dtype=float32), -0.10810841]. 
=============================================
[2019-04-05 13:19:22,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.43404195e-08 9.93970513e-01 1.05062114e-04 2.95219914e-04
 5.62832411e-03 3.12524655e-08 8.00021553e-07], sum to 1.0000
[2019-04-05 13:19:22,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6558
[2019-04-05 13:19:22,712] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.666666666666667, 55.66666666666667, 104.3333333333333, 751.3333333333334, 19.0, 21.74455646971251, -0.5418413590746992, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2729400.0000, 
sim time next is 2730000.0000, 
raw observation next is [-4.533333333333333, 55.33333333333334, 103.1666666666667, 742.1666666666667, 19.0, 21.83077555310769, -0.5301597954689378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3370267774699908, 0.5533333333333335, 0.343888888888889, 0.8200736648250462, 0.08333333333333333, 0.3192312960923074, 0.32328006817702076, 1.0, 1.0, 0.0], 
reward next is 0.6984, 
noisyNet noise sample is [array([-0.305207], dtype=float32), -0.96531296]. 
=============================================
[2019-04-05 13:19:22,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.83131 ]
 [67.587654]
 [67.223236]
 [66.88024 ]
 [66.581566]], R is [[68.10935211]
 [68.00984955]
 [67.78902435]
 [67.43587494]
 [66.89356995]].
[2019-04-05 13:19:23,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3123026e-06 7.5542045e-01 3.5080058e-03 5.5903889e-02 1.8503143e-01
 5.5668721e-08 1.3281123e-04], sum to 1.0000
[2019-04-05 13:19:23,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9536
[2019-04-05 13:19:23,711] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 60.0, 109.0, 790.0, 19.0, 22.50358186857754, -0.3727081156767598, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3766200.0000, 
sim time next is 3766800.0000, 
raw observation next is [0.0, 60.00000000000001, 107.5, 783.0, 19.5, 22.49462313818595, -0.3709646238326656, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6000000000000001, 0.35833333333333334, 0.8651933701657458, 0.125, 0.37455192818216254, 0.37634512538911147, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.4875454], dtype=float32), -0.35674304]. 
=============================================
[2019-04-05 13:19:28,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9990624e-09 9.9217439e-01 2.0484567e-04 1.8270706e-03 5.7894601e-03
 3.1938005e-10 4.2560523e-06], sum to 1.0000
[2019-04-05 13:19:28,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1688
[2019-04-05 13:19:28,033] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 19.0, 24.35581868661036, 0.07763657077907153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3774000.0000, 
sim time next is 3774600.0000, 
raw observation next is [0.0, 60.0, 64.0, 539.0, 19.0, 23.79903528783737, 0.03027681269380373, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.21333333333333335, 0.5955801104972376, 0.08333333333333333, 0.48325294065311414, 0.5100922708979346, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44512084], dtype=float32), 0.92721826]. 
=============================================
[2019-04-05 13:19:28,543] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6656654e-09 8.2119036e-01 5.1655411e-04 8.5000647e-04 1.7743997e-01
 2.5803351e-09 3.1224308e-06], sum to 1.0000
[2019-04-05 13:19:28,545] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0748
[2019-04-05 13:19:28,556] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 20.0, 19.87862378350414, -0.8368378102523325, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3880800.0000, 
sim time next is 3881400.0000, 
raw observation next is [-1.0, 55.83333333333334, 0.0, 0.0, 19.0, 19.81604856996535, -0.8996088527831368, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5583333333333335, 0.0, 0.0, 0.08333333333333333, 0.15133738083044582, 0.20013038240562106, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01549474], dtype=float32), 0.03611833]. 
=============================================
[2019-04-05 13:19:29,399] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0089604e-07 3.2526684e-01 9.4950395e-03 4.4386941e-01 2.2136028e-01
 3.1273519e-08 7.8600096e-06], sum to 1.0000
[2019-04-05 13:19:29,405] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9980
[2019-04-05 13:19:29,419] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 93.0, 52.5, 91.5, 19.0, 19.6366478946867, -1.016039427349369, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2880000.0000, 
sim time next is 2880600.0000, 
raw observation next is [1.833333333333333, 93.0, 70.00000000000001, 113.0, 19.5, 19.53731032421112, -1.030194581156767, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5133887349953832, 0.93, 0.2333333333333334, 0.12486187845303867, 0.125, 0.12810919368426, 0.15660180628107767, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02657603], dtype=float32), -0.13713524]. 
=============================================
[2019-04-05 13:19:31,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:19:31,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:19:31,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run9
[2019-04-05 13:19:32,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9740618e-08 3.5520193e-01 5.6052486e-05 9.1567291e-03 6.3558513e-01
 2.7335230e-09 1.6798545e-07], sum to 1.0000
[2019-04-05 13:19:32,405] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1898
[2019-04-05 13:19:32,424] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 21.5, 19.91220575838703, -0.7804693400534415, 0.0, 1.0, 197526.7265535342], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3450600.0000, 
sim time next is 3451200.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 20.5, 19.97986779391237, -0.7368154707448952, 0.0, 1.0, 164480.5088371015], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.20833333333333334, 0.16498898282603078, 0.25439484308503496, 0.0, 1.0, 0.7832405182719119], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.202341], dtype=float32), 0.014121865]. 
=============================================
[2019-04-05 13:19:34,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8613121e-09 9.0202475e-01 1.1891836e-04 1.9137200e-03 9.5942549e-02
 2.6463534e-11 1.3625302e-07], sum to 1.0000
[2019-04-05 13:19:34,339] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3951
[2019-04-05 13:19:34,347] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.6, 58.5, 0.0, 0.0, 19.0, 23.24224760975252, -0.05140459995771298, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4393800.0000, 
sim time next is 4394400.0000, 
raw observation next is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 19.0, 23.18933422026467, -0.05729475190436029, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7525392428439522, 0.5866666666666666, 0.0, 0.0, 0.08333333333333333, 0.4324445183553891, 0.48090174936521324, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3616444], dtype=float32), 0.14550267]. 
=============================================
[2019-04-05 13:19:40,599] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7478322e-07 4.4418567e-01 1.6829120e-03 1.4615385e-01 4.0797520e-01
 1.9465047e-07 1.5450833e-06], sum to 1.0000
[2019-04-05 13:19:40,600] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-05 13:19:40,616] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.833333333333333, 48.66666666666667, 0.0, 0.0, 19.0, 22.05614434406183, -0.4689679173889969, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3621000.0000, 
sim time next is 3621600.0000, 
raw observation next is [-2.0, 50.0, 0.0, 0.0, 19.0, 21.98782044225763, -0.4876605415591556, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.40720221606648205, 0.5, 0.0, 0.0, 0.08333333333333333, 0.33231837018813576, 0.3374464861469481, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9643005], dtype=float32), -1.3177552]. 
=============================================
[2019-04-05 13:19:43,100] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-05 13:19:43,105] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:19:43,106] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:19:43,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:19:43,107] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:19:43,108] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:19:43,108] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:19:43,113] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run54
[2019-04-05 13:19:43,131] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run54
[2019-04-05 13:19:43,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run54
[2019-04-05 13:20:02,693] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11658919]
[2019-04-05 13:20:02,693] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.216666666666667, 38.5, 0.0, 0.0, 19.0, 19.37460414570453, -1.213202460849309, 1.0, 1.0, 0.0]
[2019-04-05 13:20:02,693] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:20:02,694] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1671391e-05 8.3593452e-01 4.6981620e-03 3.7148409e-02 1.2205553e-01
 2.3531900e-06 1.4940335e-04], sampled 0.9788719970822002
[2019-04-05 13:20:09,681] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11658919]
[2019-04-05 13:20:09,681] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [13.413547215, 97.32416643166667, 93.77197580666666, 0.0, 19.5, 20.27184614593978, -0.5663413239119398, 0.0, 1.0, 0.0]
[2019-04-05 13:20:09,681] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:20:09,682] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.7957639e-10 7.4289376e-01 2.8083907e-04 5.0153051e-02 2.0667222e-01
 1.3888520e-10 9.7770467e-08], sampled 0.5461459570672397
[2019-04-05 13:20:54,510] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6270.9902 147256010.3612 -1795.5961
[2019-04-05 13:20:56,001] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11658919]
[2019-04-05 13:20:56,002] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.923179633, 68.10475601666667, 0.0, 0.0, 19.0, 19.11023429336621, -1.053776381513428, 0.0, 1.0, 0.0]
[2019-04-05 13:20:56,002] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:20:56,002] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2926874e-07 6.2847155e-01 1.9946191e-03 8.0606237e-02 2.8892091e-01
 5.0717464e-08 6.4928231e-06], sampled 0.03559119594348137
[2019-04-05 13:21:07,130] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5935.8927 173015688.3789 -2371.1984
[2019-04-05 13:21:12,388] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5952.6645 187579641.5472 -2345.8260
[2019-04-05 13:21:13,414] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1060000, evaluation results [1060000.0, 5935.892708465038, 173015688.37885663, -2371.198442749514, 6270.990152709469, 147256010.36116594, -1795.5960688712062, 5952.6645353749955, 187579641.54717836, -2345.8260344177706]
[2019-04-05 13:21:22,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5812747e-09 6.8356538e-01 4.4939316e-05 2.2768709e-03 3.1411263e-01
 6.9097111e-10 1.5122555e-07], sum to 1.0000
[2019-04-05 13:21:22,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7769
[2019-04-05 13:21:22,574] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.6, 69.0, 0.0, 0.0, 19.0, 21.44951229881299, -0.5767628312154436, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4338000.0000, 
sim time next is 4338600.0000, 
raw observation next is [3.55, 69.33333333333333, 0.0, 0.0, 19.5, 21.36420014145425, -0.5906304055421429, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5609418282548477, 0.6933333333333332, 0.0, 0.0, 0.125, 0.2803500117878543, 0.30312319815261907, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.44558728], dtype=float32), 0.8946843]. 
=============================================
[2019-04-05 13:21:22,871] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.58214059e-08 7.99718678e-01 1.21256174e-03 3.09323166e-02
 1.68122292e-01 1.07239686e-07 1.39519880e-05], sum to 1.0000
[2019-04-05 13:21:22,873] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0965
[2019-04-05 13:21:22,883] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.666666666666666, 75.0, 0.0, 0.0, 19.0, 19.93752648420482, -0.9496644615778714, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3385200.0000, 
sim time next is 3385800.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 19.0, 19.88280944303879, -0.9678690181370911, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.3102493074792244, 0.74, 0.0, 0.0, 0.08333333333333333, 0.15690078691989928, 0.17737699395430295, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.84868145], dtype=float32), -0.7296782]. 
=============================================
[2019-04-05 13:21:27,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:21:27,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:21:27,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run9
[2019-04-05 13:21:32,599] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.13369616e-07 9.60774302e-01 9.79852281e-04 9.58286633e-04
 3.72785665e-02 1.66324987e-09 8.89140847e-06], sum to 1.0000
[2019-04-05 13:21:32,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8453
[2019-04-05 13:21:32,619] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 35.5, 23.0, 57.0, 19.5, 22.2698846752276, -0.5054092254259862, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4123800.0000, 
sim time next is 4124400.0000, 
raw observation next is [3.0, 35.0, 19.16666666666667, 47.5, 19.0, 21.78398338462168, -0.4784114270049356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.35, 0.0638888888888889, 0.052486187845303865, 0.08333333333333333, 0.3153319487184734, 0.34052952433168815, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6860917], dtype=float32), 0.004244309]. 
=============================================
[2019-04-05 13:21:33,280] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.09264406e-07 6.40958965e-01 4.79062466e-04 2.77043078e-02
 3.30852389e-01 6.45240021e-08 5.04066156e-06], sum to 1.0000
[2019-04-05 13:21:33,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4990
[2019-04-05 13:21:33,298] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.666666666666667, 51.00000000000001, 76.66666666666667, 406.6666666666667, 19.0, 19.29683703581078, -1.019219942276857, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4177200.0000, 
sim time next is 4177800.0000, 
raw observation next is [-4.5, 49.5, 92.0, 488.0, 19.0, 19.30149957455989, -1.01732599631821, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3379501385041552, 0.495, 0.30666666666666664, 0.5392265193370166, 0.08333333333333333, 0.10845829787999091, 0.16089133456059668, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2098246], dtype=float32), 0.72993475]. 
=============================================
[2019-04-05 13:21:33,876] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.2655263e-07 3.1153575e-01 1.6699766e-03 7.6425694e-02 6.1033505e-01
 5.1835638e-08 3.2758227e-05], sum to 1.0000
[2019-04-05 13:21:33,880] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4432
[2019-04-05 13:21:33,899] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 50.66666666666667, 0.0, 0.0, 19.5, 19.9041698968966, -0.9425938516825002, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4168200.0000, 
sim time next is 4168800.0000, 
raw observation next is [-4.0, 50.0, 0.0, 0.0, 20.0, 19.82970103791285, -0.9602711382798184, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.5, 0.0, 0.0, 0.16666666666666666, 0.15247508649273764, 0.17990962057339388, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.20285761], dtype=float32), 0.5354569]. 
=============================================
[2019-04-05 13:21:34,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:21:34,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:21:34,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run9
[2019-04-05 13:21:35,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:21:35,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:21:35,279] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run9
[2019-04-05 13:21:35,937] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1537192e-08 8.5757655e-01 9.6172193e-04 2.1448738e-03 1.3931668e-01
 6.4124933e-10 1.0863316e-07], sum to 1.0000
[2019-04-05 13:21:35,941] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3983
[2019-04-05 13:21:35,972] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 19.0, 22.35316594897406, -0.3366140680572772, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4191600.0000, 
sim time next is 4192200.0000, 
raw observation next is [1.5, 32.0, 118.0, 847.0, 19.0, 22.34251816015383, -0.3363825085166916, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5041551246537397, 0.32, 0.3933333333333333, 0.9359116022099447, 0.08333333333333333, 0.3618765133461525, 0.3878724971611028, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6420835], dtype=float32), 0.73985195]. 
=============================================
[2019-04-05 13:21:38,858] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.7337069e-07 8.7901318e-01 1.4280546e-03 1.1401341e-02 1.0815434e-01
 4.1846274e-08 2.2760260e-06], sum to 1.0000
[2019-04-05 13:21:38,862] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4478
[2019-04-05 13:21:38,875] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 160.5, 3.0, 19.0, 21.26998088367012, -0.6670465269091617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4708800.0000, 
sim time next is 4709400.0000, 
raw observation next is [1.0, 86.0, 143.0, 2.0, 19.0, 21.2639284392376, -0.6692729433161221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.4766666666666667, 0.0022099447513812156, 0.08333333333333333, 0.27199403660313326, 0.276909018894626, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.91497266], dtype=float32), 0.3896274]. 
=============================================
[2019-04-05 13:21:39,498] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3358257e-07 8.8653690e-01 1.0660689e-03 1.0315553e-01 9.2379041e-03
 3.3685051e-08 3.3246381e-06], sum to 1.0000
[2019-04-05 13:21:39,501] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0241
[2019-04-05 13:21:39,509] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.45, 72.5, 0.0, 0.0, 19.0, 20.97018033787634, -0.7124209947785037, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4326600.0000, 
sim time next is 4327200.0000, 
raw observation next is [4.5, 72.0, 0.0, 0.0, 19.0, 20.95627943756567, -0.7234459284480156, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.5872576177285319, 0.72, 0.0, 0.0, 0.08333333333333333, 0.2463566197971391, 0.2588513571839948, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02288121], dtype=float32), -1.2227964]. 
=============================================
[2019-04-05 13:21:45,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3808896e-08 8.2883865e-01 1.4469046e-04 7.3243767e-02 9.7730145e-02
 2.4842421e-09 4.2691816e-05], sum to 1.0000
[2019-04-05 13:21:45,125] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4318
[2019-04-05 13:21:45,132] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.45, 41.83333333333333, 0.0, 0.0, 19.5, 20.93133450766532, -0.696057015822218, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4215000.0000, 
sim time next is 4215600.0000, 
raw observation next is [1.4, 42.0, 0.0, 0.0, 19.0, 20.89802378475534, -0.70875736942163, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.5013850415512465, 0.42, 0.0, 0.0, 0.08333333333333333, 0.24150198206294485, 0.26374754352612334, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0487435], dtype=float32), 0.35526127]. 
=============================================
[2019-04-05 13:21:46,772] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-05 13:21:46,773] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:21:46,773] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:21:46,774] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:21:46,774] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:21:46,774] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:21:46,774] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:21:46,790] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run55
[2019-04-05 13:21:46,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run55
[2019-04-05 13:21:46,825] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run55
[2019-04-05 13:22:59,149] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6304.9751 146440086.9607 -1860.1381
[2019-04-05 13:23:10,758] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5887.0594 173784881.7844 -2451.8561
[2019-04-05 13:23:14,732] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5881.5335 188132595.7701 -2397.9065
[2019-04-05 13:23:15,759] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1080000, evaluation results [1080000.0, 5887.059413331143, 173784881.78439268, -2451.856131773449, 6304.975093107328, 146440086.96067506, -1860.138091318233, 5881.53348176509, 188132595.77005172, -2397.906500879307]
[2019-04-05 13:23:19,870] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1441943e-07 8.5996133e-01 1.1387899e-03 3.6197263e-03 1.3525742e-01
 1.5450387e-07 2.2424530e-05], sum to 1.0000
[2019-04-05 13:23:19,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8611
[2019-04-05 13:23:19,887] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.8, 72.0, 0.0, 0.0, 20.0, 19.01108148042664, -1.137733577828857, 0.0, 1.0, 135521.692605066], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 786000.0000, 
sim time next is 786600.0000, 
raw observation next is [-7.8, 72.5, 0.0, 0.0, 20.0, 18.97751911589289, -1.127799406131751, 0.0, 1.0, 91217.74267219604], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.725, 0.0, 0.0, 0.16666666666666666, 0.08145992632440742, 0.1240668646227497, 0.0, 1.0, 0.4343702032009335], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.92915434], dtype=float32), -0.021241833]. 
=============================================
[2019-04-05 13:23:20,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:20,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:20,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run9
[2019-04-05 13:23:20,409] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:20,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:20,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run9
[2019-04-05 13:23:20,521] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.25977085e-05 9.59640384e-01 1.55789056e-03 8.42327066e-03
 3.03334799e-02 2.45976412e-06 2.99765470e-05], sum to 1.0000
[2019-04-05 13:23:20,523] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3715
[2019-04-05 13:23:20,538] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.75, 65.0, 129.0, 0.0, 19.0, 20.26635783431112, -0.9694641868745534, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 217800.0000, 
sim time next is 218400.0000, 
raw observation next is [-4.666666666666667, 65.0, 132.3333333333333, 0.0, 19.0, 20.16071508390451, -0.976249748328973, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3333333333333333, 0.65, 0.44111111111111095, 0.0, 0.08333333333333333, 0.1800595903253758, 0.17458341722367565, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7733321], dtype=float32), 1.3220812]. 
=============================================
[2019-04-05 13:23:21,241] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3777550e-07 9.5769274e-01 3.0877083e-05 9.1679152e-03 3.3105452e-02
 1.0376868e-09 2.8128156e-06], sum to 1.0000
[2019-04-05 13:23:21,244] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0255
[2019-04-05 13:23:21,266] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.800000000000001, 49.33333333333334, 192.3333333333333, 634.6666666666666, 19.0, 22.81607740068953, -0.2376349025340709, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4630800.0000, 
sim time next is 4631400.0000, 
raw observation next is [4.85, 49.5, 203.0, 599.0, 19.0, 22.89885624000939, -0.2193042683907461, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5969529085872576, 0.495, 0.6766666666666666, 0.661878453038674, 0.08333333333333333, 0.4082380200007825, 0.42689857720308466, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74722755], dtype=float32), -0.4383138]. 
=============================================
[2019-04-05 13:23:21,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.6075806e-11 9.4828457e-01 1.8473150e-02 2.9617181e-02 3.6250630e-03
 1.6122262e-10 7.0445645e-09], sum to 1.0000
[2019-04-05 13:23:21,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2140
[2019-04-05 13:23:21,428] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 19.0, 22.74293646828741, -0.1639717034142783, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4396800.0000, 
sim time next is 4397400.0000, 
raw observation next is [9.8, 60.0, 0.0, 0.0, 19.0, 22.69909375264867, -0.1725422325968818, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7340720221606649, 0.6, 0.0, 0.0, 0.08333333333333333, 0.3915911460540557, 0.44248592246770607, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46772262], dtype=float32), 1.5928594]. 
=============================================
[2019-04-05 13:23:23,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:23,234] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:23,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run9
[2019-04-05 13:23:23,654] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0914613e-08 9.8253328e-01 1.2773895e-04 1.4585951e-02 2.7509914e-03
 5.3840936e-09 1.9661300e-06], sum to 1.0000
[2019-04-05 13:23:23,656] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6090
[2019-04-05 13:23:23,664] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 53.0, 0.0, 0.0, 19.0, 23.9403582144006, 0.04590225576722253, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4647600.0000, 
sim time next is 4648200.0000, 
raw observation next is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 19.0, 23.78888223812957, 0.02862316343821926, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.5283333333333334, 0.0, 0.0, 0.08333333333333333, 0.4824068531774642, 0.5095410544794065, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56968147], dtype=float32), -0.23283441]. 
=============================================
[2019-04-05 13:23:24,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:24,610] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:24,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run9
[2019-04-05 13:23:25,396] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8906322e-07 9.8358732e-01 1.1836096e-04 1.5326121e-02 9.6020295e-04
 1.5568412e-07 7.5618959e-06], sum to 1.0000
[2019-04-05 13:23:25,398] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1120
[2019-04-05 13:23:25,409] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.5, 44.0, 88.5, 627.0, 19.0, 19.90458764302445, -0.9678660690523541, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 309600.0000, 
sim time next is 310200.0000, 
raw observation next is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 19.0, 19.98971761034165, -0.9650143000389937, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4366666666666667, 0.28777777777777774, 0.6913443830570902, 0.08333333333333333, 0.16580980086180416, 0.17832856665366878, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.932821], dtype=float32), 0.045675114]. 
=============================================
[2019-04-05 13:23:25,467] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:25,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:25,499] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run9
[2019-04-05 13:23:30,262] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6592215e-09 9.2021561e-01 4.4953335e-06 5.3573649e-02 2.6206102e-02
 4.0203185e-11 9.5742621e-08], sum to 1.0000
[2019-04-05 13:23:30,264] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7301
[2019-04-05 13:23:30,272] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 19.0, 19.91771299554448, -0.973276504265166, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 522600.0000, 
sim time next is 523200.0000, 
raw observation next is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 19.0, 19.89021936558315, -0.9810351965621352, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.5946445060018468, 0.8866666666666667, 0.0, 0.0, 0.08333333333333333, 0.15751828046526248, 0.17298826781262158, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26354253], dtype=float32), 0.22913502]. 
=============================================
[2019-04-05 13:23:34,261] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6191455e-10 9.9121219e-01 4.8808226e-05 6.1697322e-03 2.5693432e-03
 2.2713639e-10 1.3598875e-08], sum to 1.0000
[2019-04-05 13:23:34,261] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0835
[2019-04-05 13:23:34,278] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.1, 95.0, 0.0, 0.0, 19.0, 18.93093719282238, -1.061521943644847, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 85200.0000, 
sim time next is 85800.0000, 
raw observation next is [0.04999999999999999, 95.0, 0.0, 0.0, 19.0, 18.87355843134116, -1.074747956915739, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.4639889196675901, 0.95, 0.0, 0.0, 0.08333333333333333, 0.07279653594509661, 0.141750681028087, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24062504], dtype=float32), -1.1899186]. 
=============================================
[2019-04-05 13:23:36,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5174421e-09 7.2165954e-01 1.8515623e-03 3.7378144e-02 2.3910777e-01
 1.0531049e-08 2.8542456e-06], sum to 1.0000
[2019-04-05 13:23:36,149] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5259
[2019-04-05 13:23:36,192] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 19.0, 20.44863118766238, -0.7631762195139192, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4783800.0000, 
sim time next is 4784400.0000, 
raw observation next is [-5.0, 77.0, 149.0, 420.0, 19.0, 20.44707540927578, -0.7673368621683653, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.77, 0.49666666666666665, 0.46408839779005523, 0.08333333333333333, 0.20392295077298161, 0.24422104594387825, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09163], dtype=float32), 0.83135796]. 
=============================================
[2019-04-05 13:23:38,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:38,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:38,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run9
[2019-04-05 13:23:41,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:41,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:41,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run9
[2019-04-05 13:23:41,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:41,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:41,723] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run9
[2019-04-05 13:23:42,332] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:42,332] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:42,348] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run9
[2019-04-05 13:23:43,258] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:43,259] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:43,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run9
[2019-04-05 13:23:45,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0356903e-05 6.5938514e-01 2.7447890e-03 3.5053643e-03 3.3431089e-01
 1.8472578e-07 3.3243574e-05], sum to 1.0000
[2019-04-05 13:23:45,768] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5128
[2019-04-05 13:23:45,801] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.5, 44.0, 90.66666666666667, 628.3333333333333, 19.0, 20.00399918663996, -0.9386606556184747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 309000.0000, 
sim time next is 309600.0000, 
raw observation next is [-9.5, 44.0, 88.5, 627.0, 19.0, 20.07137677026953, -1.002331756567506, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.44, 0.295, 0.6928176795580111, 0.08333333333333333, 0.17261473085579424, 0.16588941447749797, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8165316], dtype=float32), -1.1394001]. 
=============================================
[2019-04-05 13:23:45,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6429645e-06 9.7451484e-01 5.0961092e-04 9.9888900e-03 1.4902690e-02
 2.1469809e-07 8.1152735e-05], sum to 1.0000
[2019-04-05 13:23:45,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9350
[2019-04-05 13:23:45,925] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 47.0, 104.5, 615.5, 19.0, 20.74321627524169, -0.7246126943377984, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5043600.0000, 
sim time next is 5044200.0000, 
raw observation next is [1.333333333333333, 46.00000000000001, 107.0, 643.0, 19.0, 20.80774121966102, -0.705341184185141, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4995383194829178, 0.4600000000000001, 0.3566666666666667, 0.7104972375690608, 0.08333333333333333, 0.23397843497175153, 0.26488627193828634, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33500427], dtype=float32), 1.6494173]. 
=============================================
[2019-04-05 13:23:49,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:23:49,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:49,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run9
[2019-04-05 13:23:51,805] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3457904e-08 9.9403608e-01 6.3209992e-04 1.2008839e-03 4.1306592e-03
 8.3625453e-09 3.6145016e-07], sum to 1.0000
[2019-04-05 13:23:51,805] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5028
[2019-04-05 13:23:51,812] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.9, 92.16666666666667, 12.0, 0.0, 19.0, 20.303909335596, -0.8847518052701623, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [5.0, 92.0, 9.0, 0.0, 19.0, 20.47117723542729, -0.8725334369936514, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.92, 0.03, 0.0, 0.08333333333333333, 0.2059314362856076, 0.2091555210021162, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.86888105], dtype=float32), 0.5108736]. 
=============================================
[2019-04-05 13:23:55,542] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-05 13:23:55,543] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:23:55,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:55,545] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:23:55,545] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:55,545] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:23:55,546] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:23:55,554] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run56
[2019-04-05 13:23:55,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run56
[2019-04-05 13:23:55,583] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run56
[2019-04-05 13:24:13,668] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11671674]
[2019-04-05 13:24:13,668] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.835362394, 65.01262355, 129.469103745, 226.0535095, 19.0, 19.38079610564966, -1.074359645684993, 0.0, 1.0, 0.0]
[2019-04-05 13:24:13,668] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:24:13,669] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.5135881e-08 6.1041635e-01 2.7721848e-03 5.1576111e-02 3.3523101e-01
 2.4312806e-08 4.1515113e-06], sampled 0.8385082005874879
[2019-04-05 13:24:56,946] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11671674]
[2019-04-05 13:24:56,947] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.4183858276666668, 50.49989100166666, 108.92469022, 727.5601755666667, 19.0, 19.60933630742947, -0.9177912326730665, 0.0, 1.0, 0.0]
[2019-04-05 13:24:56,947] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:24:56,948] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.2046323e-09 7.3008525e-01 7.3444180e-04 3.4320686e-02 2.3485917e-01
 1.3733756e-09 4.6965397e-07], sampled 0.9824175275963914
[2019-04-05 13:25:09,100] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6335.0620 147322725.1828 -1735.9204
[2019-04-05 13:25:19,095] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5866.3845 175464230.1078 -2383.1193
[2019-04-05 13:25:25,110] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5971.2370 190657429.2518 -2290.7098
[2019-04-05 13:25:26,135] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1100000, evaluation results [1100000.0, 5866.384499217237, 175464230.1078234, -2383.1193453630544, 6335.061998033986, 147322725.18276796, -1735.9203659931247, 5971.237037722917, 190657429.25175357, -2290.7097926087295]
[2019-04-05 13:25:26,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3104383e-10 4.7157136e-01 4.0621855e-04 9.6528488e-04 5.2705711e-01
 4.2522717e-12 6.5552747e-10], sum to 1.0000
[2019-04-05 13:25:26,338] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9449
[2019-04-05 13:25:26,346] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.5, 20.75696968257667, -0.6259580597395281, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1034400.0000, 
sim time next is 1035000.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 20.0, 20.72790985597999, -0.6302752406455038, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.16666666666666666, 0.22732582133166593, 0.2899082531181654, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.04852047], dtype=float32), -0.15321825]. 
=============================================
[2019-04-05 13:25:26,367] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[104.30139]
 [103.99197]
 [104.69628]
 [104.73205]
 [104.92764]], R is [[103.53269958]
 [103.4259491 ]
 [103.39169312]
 [103.28635406]
 [103.25349426]].
[2019-04-05 13:25:28,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7113645e-05 9.2937648e-01 5.0514122e-03 6.5938206e-03 5.8644731e-02
 8.6128985e-06 2.8782323e-04], sum to 1.0000
[2019-04-05 13:25:28,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7381
[2019-04-05 13:25:28,082] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 19.0, 19.76712435664723, -1.141837808507179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 477600.0000, 
sim time next is 478200.0000, 
raw observation next is [-1.283333333333333, 27.5, 125.6666666666667, 0.0, 19.0, 19.66059748717767, -1.15397670344289, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4270544783010157, 0.275, 0.418888888888889, 0.0, 0.08333333333333333, 0.13838312393147265, 0.11534109885236998, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34178784], dtype=float32), -0.13179804]. 
=============================================
[2019-04-05 13:25:32,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6941572e-06 6.1385763e-01 3.9521318e-02 2.5947836e-01 8.7137945e-02
 6.4688415e-09 3.0286701e-06], sum to 1.0000
[2019-04-05 13:25:32,474] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7503
[2019-04-05 13:25:32,482] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.083333333333334, 94.83333333333333, 0.0, 0.0, 19.0, 19.86950260774968, -0.8657592381726104, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 951000.0000, 
sim time next is 951600.0000, 
raw observation next is [5.166666666666667, 93.66666666666666, 0.0, 0.0, 19.0, 19.81031913508224, -0.8655163107280531, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6057248384118191, 0.9366666666666665, 0.0, 0.0, 0.08333333333333333, 0.15085992792351988, 0.21149456309064896, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09222674], dtype=float32), -0.44569287]. 
=============================================
[2019-04-05 13:25:33,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:25:33,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:25:33,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run9
[2019-04-05 13:25:35,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9217356e-06 7.2494954e-01 1.0797261e-02 1.1981744e-01 1.4434217e-01
 3.8937665e-06 8.4714855e-05], sum to 1.0000
[2019-04-05 13:25:35,693] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2439
[2019-04-05 13:25:35,746] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.8, 75.83333333333334, 0.0, 0.0, 19.0, 19.34280467853126, -1.087753068369802, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [-12.8, 77.0, 0.0, 0.0, 19.0, 19.3044385533913, -1.111401461739567, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.1080332409972299, 0.77, 0.0, 0.0, 0.08333333333333333, 0.10870321278260835, 0.12953284608681095, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5312064], dtype=float32), 2.3933027]. 
=============================================
[2019-04-05 13:25:36,383] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3815938e-10 9.8276675e-01 7.5740565e-04 2.9218181e-03 1.3553555e-02
 1.5649417e-09 4.3030755e-07], sum to 1.0000
[2019-04-05 13:25:36,383] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6047
[2019-04-05 13:25:36,390] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 20.0, 21.60377264593627, -0.4401410440252233, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1059000.0000, 
sim time next is 1059600.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 19.0, 21.54864044407014, -0.4512359736809457, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.08333333333333333, 0.2957200370058451, 0.3495880087730181, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.125519], dtype=float32), 1.2672966]. 
=============================================
[2019-04-05 13:25:41,517] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.69742672e-11 1.26092464e-01 1.13805865e-04 6.27194624e-03
 8.67521822e-01 7.36811584e-11 3.54524432e-09], sum to 1.0000
[2019-04-05 13:25:41,518] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8421
[2019-04-05 13:25:41,524] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 96.66666666666666, 97.0, 0.0, 20.0, 20.76399743205943, -0.4785260917652419, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1252200.0000, 
sim time next is 1252800.0000, 
raw observation next is [14.4, 96.0, 98.0, 0.0, 20.5, 20.77682048603268, -0.4770313666855208, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8614958448753465, 0.96, 0.32666666666666666, 0.0, 0.20833333333333334, 0.23140170716939, 0.3409895444381597, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.3161647], dtype=float32), -0.42365265]. 
=============================================
[2019-04-05 13:25:47,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.62682933e-08 9.73829150e-01 1.62976212e-04 1.20054865e-02
 1.40005024e-02 8.28530855e-10 1.90639719e-06], sum to 1.0000
[2019-04-05 13:25:47,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4237
[2019-04-05 13:25:47,854] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.8, 100.0, 0.0, 0.0, 19.0, 20.09048823121238, -0.8907871938165003, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 934800.0000, 
sim time next is 935400.0000, 
raw observation next is [4.9, 100.0, 0.0, 0.0, 19.0, 19.98794991674662, -0.9042019984068145, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5983379501385043, 1.0, 0.0, 0.0, 0.08333333333333333, 0.16566249306221822, 0.19859933386439518, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.62763953], dtype=float32), 0.9102113]. 
=============================================
[2019-04-05 13:25:49,632] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4109553e-07 4.0744673e-02 2.9374336e-04 6.1412418e-04 9.5834231e-01
 3.1902190e-09 5.0119957e-06], sum to 1.0000
[2019-04-05 13:25:49,634] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9017
[2019-04-05 13:25:49,647] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 24.0, 19.71737933899627, -0.9878694535520117, 0.0, 1.0, 46908.35138066982], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 2174400.0000, 
sim time next is 2175000.0000, 
raw observation next is [-6.616666666666667, 77.5, 0.0, 0.0, 24.5, 19.77207246301944, -0.984246687514427, 0.0, 1.0, 46715.85618097811], 
processed observation next is [1.0, 0.17391304347826086, 0.2793167128347184, 0.775, 0.0, 0.0, 0.5416666666666666, 0.1476727052516201, 0.17191777082852433, 0.0, 1.0, 0.22245645800465766], 
reward next is 0.2143, 
noisyNet noise sample is [array([-0.75073665], dtype=float32), -0.30655983]. 
=============================================
[2019-04-05 13:25:49,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[67.72502]
 [67.37589]
 [67.82547]
 [67.47547]
 [67.76072]], R is [[66.84857941]
 [66.46580505]
 [66.15828705]
 [65.92527771]
 [65.76602936]].
[2019-04-05 13:25:52,753] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.8437539e-08 2.9627928e-03 5.2620154e-03 3.7214890e-02 9.5455331e-01
 1.0617495e-08 6.8467716e-06], sum to 1.0000
[2019-04-05 13:25:52,754] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6641
[2019-04-05 13:25:52,771] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 67.0, 0.0, 0.0, 20.5, 19.25564437933054, -1.081711147210284, 0.0, 1.0, 101117.817771382], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 628800.0000, 
sim time next is 629400.0000, 
raw observation next is [-4.5, 67.5, 0.0, 0.0, 21.0, 19.27203373573359, -1.072074621839145, 0.0, 1.0, 66812.31782858353], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.675, 0.0, 0.0, 0.25, 0.1060028113111325, 0.142641792720285, 0.0, 1.0, 0.31815389442182634], 
reward next is 0.7143, 
noisyNet noise sample is [array([1.3396807], dtype=float32), -0.49584475]. 
=============================================
[2019-04-05 13:25:58,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6683608e-07 8.2242072e-01 1.5046943e-03 2.8813476e-02 1.4724590e-01
 5.9783169e-08 1.4820741e-05], sum to 1.0000
[2019-04-05 13:25:58,439] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8849
[2019-04-05 13:25:58,452] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.41666666666667, 65.83333333333334, 229.6666666666667, 249.0, 19.0, 24.3749263137961, 0.2271540731257756, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1079400.0000, 
sim time next is 1080000.0000, 
raw observation next is [16.6, 65.0, 217.5, 266.0, 19.5, 24.61369266661928, -0.03133230684270688, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.922437673130194, 0.65, 0.725, 0.29392265193370165, 0.125, 0.5511410555516066, 0.4895558977190977, 1.0, 0.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.43479764], dtype=float32), -0.59092224]. 
=============================================
[2019-04-05 13:25:58,463] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.06622]
 [83.39837]
 [84.41649]
 [85.67563]
 [85.50795]], R is [[81.40686035]
 [81.59279633]
 [81.63401031]
 [81.74624634]
 [81.92878723]].
[2019-04-05 13:25:58,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2989373e-08 5.6828552e-01 9.4340177e-04 1.4099528e-02 4.1666564e-01
 1.2405269e-07 5.6499312e-06], sum to 1.0000
[2019-04-05 13:25:58,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5305
[2019-04-05 13:25:58,540] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.566666666666667, 86.66666666666667, 0.0, 0.0, 19.0, 19.15366055764017, -1.099736753395228, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 606000.0000, 
sim time next is 606600.0000, 
raw observation next is [-3.65, 86.5, 0.0, 0.0, 19.0, 19.08286268966805, -1.111753797283705, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.3614958448753463, 0.865, 0.0, 0.0, 0.08333333333333333, 0.0902385574723376, 0.12941540090543166, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0264052], dtype=float32), 0.1050658]. 
=============================================
[2019-04-05 13:26:00,374] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.05045965e-05 3.54860693e-01 3.41203017e-03 1.06430486e-01
 5.35278678e-01 5.18936680e-08 7.57759335e-06], sum to 1.0000
[2019-04-05 13:26:00,375] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9267
[2019-04-05 13:26:00,402] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.3, 79.50000000000001, 0.0, 0.0, 21.0, 19.86408486596412, -0.8957567647137505, 0.0, 1.0, 111327.7607392433], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1897800.0000, 
sim time next is 1898400.0000, 
raw observation next is [-7.300000000000001, 80.0, 0.0, 0.0, 21.5, 19.96731343916774, -0.8788704730918603, 0.0, 1.0, 68968.07320853064], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.8, 0.0, 0.0, 0.2916666666666667, 0.16394278659731162, 0.20704317563604657, 0.0, 1.0, 0.3284193962310983], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.2806984], dtype=float32), -0.5460909]. 
=============================================
[2019-04-05 13:26:01,475] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.5791343e-06 9.3103337e-01 7.5045793e-04 2.0602874e-02 4.7605962e-02
 5.7508412e-08 1.6014027e-06], sum to 1.0000
[2019-04-05 13:26:01,476] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1806
[2019-04-05 13:26:01,488] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 69.0, 0.0, 0.0, 19.0, 18.77694185238969, -1.178976918055402, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 775800.0000, 
sim time next is 776400.0000, 
raw observation next is [-7.1, 69.66666666666666, 0.0, 0.0, 19.0, 18.70134363243071, -1.197631966252344, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.2659279778393352, 0.6966666666666665, 0.0, 0.0, 0.08333333333333333, 0.058445302702559175, 0.100789344582552, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9069731], dtype=float32), 1.2174349]. 
=============================================
[2019-04-05 13:26:02,819] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-05 13:26:02,819] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:26:02,820] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:26:02,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:26:02,820] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:26:02,820] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:26:02,825] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:26:02,830] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run57
[2019-04-05 13:26:02,852] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run57
[2019-04-05 13:26:02,867] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run57
[2019-04-05 13:26:58,226] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11579602]
[2019-04-05 13:26:58,226] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-8.3, 60.66666666666667, 0.0, 0.0, 19.0, 18.82627526215032, -1.056858614641154, 0.0, 1.0, 0.0]
[2019-04-05 13:26:58,227] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:26:58,227] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.7086618e-07 6.8589330e-01 5.8726226e-03 3.3303369e-02 2.7491111e-01
 7.9455695e-08 1.8989018e-05], sampled 0.6043425337630055
[2019-04-05 13:27:14,969] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6233.1444 150516896.3689 -1730.6732
[2019-04-05 13:27:26,562] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5823.7530 175486900.0701 -2351.2231
[2019-04-05 13:27:34,033] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5922.7709 189346001.4885 -2246.6865
[2019-04-05 13:27:35,059] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1120000, evaluation results [1120000.0, 5823.753012661925, 175486900.0701149, -2351.223050895951, 6233.144432805127, 150516896.3688538, -1730.673175947209, 5922.770920043365, 189346001.4884689, -2246.686451291198]
[2019-04-05 13:27:46,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7878355e-07 8.3574122e-01 1.6022152e-04 3.0240954e-03 1.6106698e-01
 1.0162973e-08 7.1757768e-06], sum to 1.0000
[2019-04-05 13:27:46,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6646
[2019-04-05 13:27:46,285] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.283333333333333, 75.66666666666667, 0.0, 0.0, 19.0, 22.57538707947624, -0.153000166927157, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1630200.0000, 
sim time next is 1630800.0000, 
raw observation next is [7.2, 76.0, 0.0, 0.0, 19.0, 22.52870026935645, -0.1628545966187668, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.662049861495845, 0.76, 0.0, 0.0, 0.08333333333333333, 0.3773916891130374, 0.44571513446041106, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60582054], dtype=float32), 0.80857515]. 
=============================================
[2019-04-05 13:27:47,668] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0948471e-09 7.3690474e-01 6.2351020e-05 4.4703372e-03 2.5856197e-01
 1.4379888e-09 6.1031091e-07], sum to 1.0000
[2019-04-05 13:27:47,673] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9565
[2019-04-05 13:27:47,681] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.1, 77.5, 0.0, 0.0, 20.0, 21.08300524806292, -0.5813291180435761, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1045800.0000, 
sim time next is 1046400.0000, 
raw observation next is [14.2, 77.33333333333333, 0.0, 0.0, 20.5, 21.03524890910442, -0.5883509530353117, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.8559556786703602, 0.7733333333333333, 0.0, 0.0, 0.20833333333333334, 0.2529374090920351, 0.3038830156548961, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([1.6002183], dtype=float32), -0.5657679]. 
=============================================
[2019-04-05 13:27:48,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2610603e-06 8.9292586e-01 7.9673924e-04 3.2389462e-02 7.3881954e-02
 3.5822656e-09 2.6870400e-06], sum to 1.0000
[2019-04-05 13:27:48,186] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5940
[2019-04-05 13:27:48,213] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 49.0, 111.5, 0.0, 19.5, 22.54464362107774, -0.4239624026932641, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 19.0, 22.21570968951844, -0.3670291471433378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8425669436749772, 0.4933333333333334, 0.3344444444444443, 0.0, 0.08333333333333333, 0.3513091407932034, 0.37765695095222074, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.518266], dtype=float32), 0.9300648]. 
=============================================
[2019-04-05 13:27:49,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1662357e-06 2.7740374e-01 6.7545590e-04 2.0008489e-02 7.0190340e-01
 5.5762635e-09 7.7762215e-06], sum to 1.0000
[2019-04-05 13:27:49,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9182
[2019-04-05 13:27:49,045] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 19.5, 20.8321870597773, -0.5386676443680348, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1203600.0000, 
sim time next is 1204200.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 20.0, 20.80831667378337, -0.5419659146810651, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.16666666666666666, 0.2340263894819475, 0.31934469510631164, 0.0, 0.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.8178955], dtype=float32), 0.31974405]. 
=============================================
[2019-04-05 13:27:49,123] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2581406e-07 5.8840567e-01 1.2965087e-03 8.5196234e-03 4.0177089e-01
 5.1211049e-09 7.0759766e-06], sum to 1.0000
[2019-04-05 13:27:49,126] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9566
[2019-04-05 13:27:49,136] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [13.46666666666667, 61.33333333333333, 0.0, 0.0, 20.0, 21.98367386123869, -0.2787116514398973, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1111200.0000, 
sim time next is 1111800.0000, 
raw observation next is [13.38333333333333, 61.66666666666667, 0.0, 0.0, 20.5, 21.92096381706857, -0.2896915067709254, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8333333333333334, 0.6166666666666667, 0.0, 0.0, 0.20833333333333334, 0.3267469847557143, 0.40343616440969154, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([2.3081892], dtype=float32), -1.4588605]. 
=============================================
[2019-04-05 13:27:57,921] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5720622e-05 9.5102024e-01 1.0497761e-02 3.0281264e-02 7.9578068e-03
 2.2533659e-06 1.6498104e-04], sum to 1.0000
[2019-04-05 13:27:57,921] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6374
[2019-04-05 13:27:57,931] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.65, 68.0, 144.0, 0.0, 19.0, 20.0598473143372, -0.8873565158206285, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2208600.0000, 
sim time next is 2209200.0000, 
raw observation next is [-3.733333333333333, 69.0, 140.0, 0.0, 19.0, 20.31876320651941, -0.8630814325329005, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.69, 0.4666666666666667, 0.0, 0.08333333333333333, 0.19323026720995085, 0.21230618915569985, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1654269], dtype=float32), 1.6304356]. 
=============================================
[2019-04-05 13:27:59,344] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4563433e-09 1.1697163e-01 2.9018670e-03 2.5294283e-02 8.5483205e-01
 7.2931106e-10 2.2830859e-07], sum to 1.0000
[2019-04-05 13:27:59,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3110
[2019-04-05 13:27:59,418] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 24.0, 22.24913475063506, -0.3677456191880722, 0.0, 1.0, 105512.1105509729], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 24.5, 22.67663119276786, -0.3134958220508978, 0.0, 1.0, 92734.08498701733], 
processed observation next is [0.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.5416666666666666, 0.3897192660639884, 0.3955013926497007, 0.0, 1.0, 0.4415908808905587], 
reward next is 0.2143, 
noisyNet noise sample is [array([0.61053795], dtype=float32), 0.92792666]. 
=============================================
[2019-04-05 13:27:59,593] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6624132e-08 3.3677691e-01 1.5468521e-03 1.0218726e-03 6.6065305e-01
 1.1973059e-08 1.2009735e-06], sum to 1.0000
[2019-04-05 13:27:59,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6673
[2019-04-05 13:27:59,611] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 25.5, 20.67395914371652, -0.6473804588239243, 0.0, 1.0, 43827.31923445476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1401600.0000, 
sim time next is 1402200.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 20.76030809610856, -0.6454053688432461, 0.0, 1.0, 43594.65927215267], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.2300256746757133, 0.28486487705225133, 0.0, 1.0, 0.20759361558167938], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.0854978], dtype=float32), 1.158403]. 
=============================================
[2019-04-05 13:28:05,241] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.23612957e-07 8.29625964e-01 1.13026146e-03 5.88641651e-02
 1.10365264e-01 4.81177622e-08 1.39836575e-05], sum to 1.0000
[2019-04-05 13:28:05,246] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6978
[2019-04-05 13:28:05,260] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 85.0, 0.0, 0.0, 19.0, 18.81374824592955, -1.159238068135193, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1887600.0000, 
sim time next is 1888200.0000, 
raw observation next is [-5.6, 84.5, 0.0, 0.0, 19.0, 18.84818098368508, -1.167747800870368, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.845, 0.0, 0.0, 0.08333333333333333, 0.07068174864042327, 0.1107507330432107, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6579752], dtype=float32), 0.44452873]. 
=============================================
[2019-04-05 13:28:05,718] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5033425e-08 3.9021182e-01 4.7571495e-02 3.2973236e-01 2.3238809e-01
 4.8545501e-09 9.6217511e-05], sum to 1.0000
[2019-04-05 13:28:05,719] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5080
[2019-04-05 13:28:05,742] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.5, 18.75710248848252, -1.121886194609665, 0.0, 1.0, 114200.0979275017], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1890000.0000, 
sim time next is 1890600.0000, 
raw observation next is [-5.7, 81.66666666666667, 0.0, 0.0, 19.5, 18.91691629903652, -1.096942545757073, 0.0, 1.0, 48745.07505123902], 
processed observation next is [0.0, 0.9130434782608695, 0.30470914127423826, 0.8166666666666668, 0.0, 0.0, 0.125, 0.07640969158637656, 0.13435248474764236, 0.0, 1.0, 0.23211940500590011], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.15835914], dtype=float32), 0.9518643]. 
=============================================
[2019-04-05 13:28:06,973] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.3460313e-08 5.9663254e-01 1.2877457e-04 1.5356144e-01 2.4967450e-01
 8.4216323e-10 2.6580942e-06], sum to 1.0000
[2019-04-05 13:28:06,973] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6884
[2019-04-05 13:28:06,982] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 19.92454379785176, -0.8036620305883414, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1030800.0000, 
sim time next is 1031400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 19.93815586717433, -0.8075397483248251, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.16151298893119423, 0.23082008389172495, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34994256], dtype=float32), 2.0882192]. 
=============================================
[2019-04-05 13:28:09,374] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 13:28:09,377] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:28:09,378] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:28:09,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:28:09,379] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:28:09,380] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:28:09,380] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:28:09,387] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run58
[2019-04-05 13:28:09,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run58
[2019-04-05 13:28:09,423] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run58
[2019-04-05 13:28:21,653] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11592446]
[2019-04-05 13:28:21,654] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-11.1, 59.16666666666667, 0.0, 0.0, 19.0, 18.50226551213218, -1.230384023359894, 1.0, 1.0, 196605.9241964081]
[2019-04-05 13:28:21,654] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:28:21,656] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.58204595e-04 7.43562758e-01 1.49253085e-02 3.47352773e-02
 2.05746651e-01 2.51124402e-05 8.46658309e-04], sampled 0.8166214403074377
[2019-04-05 13:28:57,112] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11592446]
[2019-04-05 13:28:57,112] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.2, 33.66666666666666, 0.0, 0.0, 19.0, 20.09946525441156, -0.9756899171440586, 0.0, 1.0, 0.0]
[2019-04-05 13:28:57,112] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:28:57,113] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.9789351e-07 7.0723850e-01 4.2166566e-03 4.8566435e-02 2.3996326e-01
 1.0169543e-07 1.4228785e-05], sampled 0.43833288468627485
[2019-04-05 13:29:21,790] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6290.8558 146366644.2186 -1796.7518
[2019-04-05 13:29:34,254] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5895.1506 174093077.0275 -2388.5523
[2019-04-05 13:29:34,857] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11592446]
[2019-04-05 13:29:34,858] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.631922761, 38.09364842333333, 259.3829182166667, 372.6513616833334, 19.0, 19.25812537080716, -0.9731192997105443, 0.0, 1.0, 0.0]
[2019-04-05 13:29:34,858] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:29:34,859] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.1842541e-07 7.1095228e-01 1.7329075e-03 5.4100867e-02 2.3320843e-01
 5.3249401e-08 5.0796352e-06], sampled 0.2968683658906882
[2019-04-05 13:29:37,180] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5780.4962 190508410.3766 -2424.9739
[2019-04-05 13:29:38,207] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1140000, evaluation results [1140000.0, 5895.150579438253, 174093077.02752057, -2388.5522593500805, 6290.855824311046, 146366644.21857503, -1796.751774043572, 5780.496183737993, 190508410.37662634, -2424.973883092958]
[2019-04-05 13:29:39,736] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3130798e-07 2.5272799e-01 3.4567561e-02 3.9470695e-02 6.7320836e-01
 3.2973681e-08 2.5265303e-05], sum to 1.0000
[2019-04-05 13:29:39,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6829
[2019-04-05 13:29:39,777] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.883333333333333, 44.16666666666667, 66.0, 702.3333333333333, 19.0, 19.92636326444439, -0.9488784217906271, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2454600.0000, 
sim time next is 2455200.0000, 
raw observation next is [-5.6, 43.0, 68.5, 721.0, 19.5, 19.87486358165281, -0.9519444474502422, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.43, 0.22833333333333333, 0.7966850828729282, 0.125, 0.1562386318044009, 0.1826851841832526, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.0031453], dtype=float32), 0.3331208]. 
=============================================
[2019-04-05 13:29:47,778] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5053458e-06 9.8590821e-01 5.4715533e-04 5.9395502e-03 7.5596455e-03
 6.7873479e-08 4.3883709e-05], sum to 1.0000
[2019-04-05 13:29:47,781] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7765
[2019-04-05 13:29:47,795] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.1, 74.33333333333333, 0.0, 0.0, 19.0, 18.5871126600272, -1.168553831079259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2242200.0000, 
sim time next is 2242800.0000, 
raw observation next is [-6.2, 75.0, 0.0, 0.0, 19.0, 18.64403310793041, -1.173016414378142, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.2908587257617729, 0.75, 0.0, 0.0, 0.08333333333333333, 0.05366942566086763, 0.1089945285406193, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2810467], dtype=float32), 1.2348188]. 
=============================================
[2019-04-05 13:29:55,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2147167e-05 7.0244956e-01 6.6698785e-03 7.1177065e-02 2.1907213e-01
 1.2950041e-05 5.9622765e-04], sum to 1.0000
[2019-04-05 13:29:55,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3679
[2019-04-05 13:29:55,284] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 19.0, 19.03037799390008, -1.070892352458813, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 19.0, 18.94519912108878, -1.087108242944847, 0.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.08333333333333333, 0.07876659342406504, 0.13763058568505096, 0.0, 1.0, 0.0889543412868121], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90481883], dtype=float32), 0.32325494]. 
=============================================
[2019-04-05 13:29:56,533] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.3393387e-06 5.2167499e-01 2.1465637e-03 2.4447336e-03 4.7370711e-01
 5.5653970e-08 2.3136339e-05], sum to 1.0000
[2019-04-05 13:29:56,538] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8500
[2019-04-05 13:29:56,563] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.1, 82.16666666666667, 0.0, 0.0, 19.0, 20.02410599623348, -0.8807014560830569, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1979400.0000, 
sim time next is 1980000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.5, 20.02080844979915, -0.8887966590991677, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.2908587257617729, 0.83, 0.0, 0.0, 0.125, 0.16840070414992928, 0.2037344469669441, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.12361354], dtype=float32), 0.4959647]. 
=============================================
[2019-04-05 13:29:56,586] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.906364]
 [76.129135]
 [75.99663 ]
 [76.052155]
 [75.47986 ]], R is [[75.74213409]
 [75.98471069]
 [76.08200073]
 [76.24975586]
 [76.27297211]].
[2019-04-05 13:29:57,263] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7288032e-04 8.5654485e-01 2.4245414e-03 2.7298538e-02 1.1337732e-01
 7.4267978e-06 1.7450064e-04], sum to 1.0000
[2019-04-05 13:29:57,264] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7854
[2019-04-05 13:29:57,276] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.1833333333333333, 43.83333333333334, 105.6666666666667, 28.0, 19.0, 21.04325166672304, -0.7391861904282496, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2303400.0000, 
sim time next is 2304000.0000, 
raw observation next is [0.0, 44.0, 89.5, 21.0, 19.0, 21.1782744845959, -0.7296728742887352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.44, 0.29833333333333334, 0.023204419889502764, 0.08333333333333333, 0.2648562070496583, 0.2567757085704216, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1841697], dtype=float32), -0.8089812]. 
=============================================
[2019-04-05 13:29:57,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[49.28253 ]
 [49.12634 ]
 [49.091774]
 [49.043064]
 [48.878178]], R is [[49.05958176]
 [48.56898499]
 [48.08329391]
 [47.60246277]
 [47.12643814]].
[2019-04-05 13:29:58,557] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5528376e-05 6.3827097e-02 1.7306103e-03 1.2573844e-03 9.3308431e-01
 4.6952124e-07 2.4541201e-05], sum to 1.0000
[2019-04-05 13:29:58,558] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7781
[2019-04-05 13:29:58,570] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 19.0, 20.09592754539568, -0.9177893952653391, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [-2.3, 63.5, 0.0, 0.0, 19.5, 19.99852134845421, -0.9368765133244188, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.635, 0.0, 0.0, 0.125, 0.16654344570451762, 0.1877078288918604, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.4760766], dtype=float32), -0.34769475]. 
=============================================
[2019-04-05 13:30:00,310] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6905741e-04 9.6356821e-01 5.1057963e-03 4.1909493e-03 2.6568813e-02
 1.9247354e-05 1.7800172e-04], sum to 1.0000
[2019-04-05 13:30:00,313] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0666
[2019-04-05 13:30:00,327] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.416666666666667, 81.66666666666667, 132.0, 0.0, 19.0, 20.21789732508379, -0.9690783696281331, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2027400.0000, 
sim time next is 2028000.0000, 
raw observation next is [-5.233333333333333, 80.33333333333334, 139.5, 0.0, 19.0, 20.17818850148223, -0.9694300134693125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.31763619575253926, 0.8033333333333335, 0.465, 0.0, 0.08333333333333333, 0.1815157084568524, 0.17685666217689586, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9038789], dtype=float32), -0.0067108385]. 
=============================================
[2019-04-05 13:30:00,337] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[48.58267 ]
 [48.605988]
 [48.745136]
 [48.726093]
 [48.68777 ]], R is [[48.08587265]
 [47.6050148 ]
 [47.12896347]
 [46.65767288]
 [46.19109726]].
[2019-04-05 13:30:03,360] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7212710e-05 9.2362052e-01 1.2235312e-02 3.3211991e-02 2.9423561e-02
 1.3307625e-05 1.4480284e-03], sum to 1.0000
[2019-04-05 13:30:03,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1992
[2019-04-05 13:30:03,374] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.65, 63.5, 137.0, 0.0, 19.5, 21.2719057541147, -0.6984762858405263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1949400.0000, 
sim time next is 1950000.0000, 
raw observation next is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 19.0, 21.37640512367774, -0.6943745367413859, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3638042474607572, 0.63, 0.4427777777777776, 0.0, 0.08333333333333333, 0.28136709363981155, 0.26854182108620467, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11211985], dtype=float32), -0.44179624]. 
=============================================
[2019-04-05 13:30:03,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[47.007767]
 [47.24577 ]
 [47.51324 ]
 [47.81737 ]
 [48.18736 ]], R is [[46.36659241]
 [45.9029274 ]
 [45.44389725]
 [44.98945999]
 [44.53956604]].
[2019-04-05 13:30:03,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.2119391e-08 9.1201913e-01 9.9016579e-05 9.1275601e-03 7.8754053e-02
 1.0935437e-09 1.3183374e-07], sum to 1.0000
[2019-04-05 13:30:03,760] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5089
[2019-04-05 13:30:03,771] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 19.0, 21.19697292095657, -0.6807597220407483, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3109200.0000, 
sim time next is 3109800.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 21.11864328516106, -0.696483464223001, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.2598869404300883, 0.26783884525899965, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43451726], dtype=float32), -0.049265567]. 
=============================================
[2019-04-05 13:30:05,468] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1506079e-04 5.3887212e-01 6.9583203e-03 5.0342470e-02 4.0202540e-01
 6.6600158e-05 1.2200463e-03], sum to 1.0000
[2019-04-05 13:30:05,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2191
[2019-04-05 13:30:05,481] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.55, 27.5, 159.0, 275.0, 19.0, 21.53222835267201, -0.6828329038570643, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2557800.0000, 
sim time next is 2558400.0000, 
raw observation next is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 19.0, 21.52980224993786, -0.6841564049687315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5586334256694367, 0.28, 0.505, 0.31786372007366487, 0.08333333333333333, 0.2941501874948216, 0.27194786501042284, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21721381], dtype=float32), -0.11284839]. 
=============================================
[2019-04-05 13:30:09,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8241458e-04 6.6033745e-01 5.6194507e-02 4.2950686e-02 2.3561150e-01
 8.7204942e-05 4.4361949e-03], sum to 1.0000
[2019-04-05 13:30:09,186] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1884
[2019-04-05 13:30:09,226] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.4, 73.66666666666667, 91.83333333333333, 419.5, 19.0, 20.12801654206006, -0.9202435755937927, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2193600.0000, 
sim time next is 2194200.0000, 
raw observation next is [-5.3, 73.0, 102.0, 451.0, 19.0, 20.29639380735344, -0.8928558919281505, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.31578947368421056, 0.73, 0.34, 0.4983425414364641, 0.08333333333333333, 0.1913661506127866, 0.20238136935728315, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21456668], dtype=float32), 0.18473661]. 
=============================================
[2019-04-05 13:30:14,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3939097e-05 8.9131725e-01 7.1233802e-04 6.2125064e-03 1.0143583e-01
 1.3440216e-05 2.6470528e-04], sum to 1.0000
[2019-04-05 13:30:14,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3640
[2019-04-05 13:30:14,683] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.5, 48.16666666666667, 29.0, 0.0, 19.0, 19.30744596131396, -1.101595362444771, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2307000.0000, 
sim time next is 2307600.0000, 
raw observation next is [-0.6, 49.0, 23.0, 0.0, 19.0, 19.47118893937079, -1.081462532295651, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.44598337950138506, 0.49, 0.07666666666666666, 0.0, 0.08333333333333333, 0.12259907828089922, 0.139512489234783, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04034825], dtype=float32), -0.0516494]. 
=============================================
[2019-04-05 13:30:15,664] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4537161e-04 9.6984291e-01 3.7673055e-03 9.3387030e-03 1.6580595e-02
 1.1627569e-05 1.1349570e-04], sum to 1.0000
[2019-04-05 13:30:15,664] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8454
[2019-04-05 13:30:15,692] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.4, 47.33333333333333, 35.00000000000001, 0.0, 19.0, 21.12560587642102, -0.775413241681155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2306400.0000, 
sim time next is 2307000.0000, 
raw observation next is [-0.5, 48.16666666666667, 29.0, 0.0, 19.0, 21.0966424726766, -0.8568579227070066, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.44875346260387816, 0.4816666666666667, 0.09666666666666666, 0.0, 0.08333333333333333, 0.2580535393897166, 0.2143806924309978, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.40094814], dtype=float32), -0.14654432]. 
=============================================
[2019-04-05 13:30:15,701] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[49.06089 ]
 [48.82134 ]
 [48.7546  ]
 [48.25969 ]
 [47.904537]], R is [[48.75443649]
 [48.26689148]
 [47.78422165]
 [47.30638123]
 [46.8333168 ]].
[2019-04-05 13:30:16,976] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 13:30:16,977] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:30:16,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:30:16,980] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:30:16,980] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:30:16,980] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:30:16,981] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:30:16,993] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run59
[2019-04-05 13:30:17,012] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run59
[2019-04-05 13:30:17,029] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run59
[2019-04-05 13:30:32,367] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.114576556]
[2019-04-05 13:30:32,367] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.693272077333333, 34.87177491666667, 41.35465869166667, 0.0, 19.5, 20.31719454825925, -0.9985976945004976, 1.0, 1.0, 0.0]
[2019-04-05 13:30:32,368] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:30:32,369] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1521051e-04 7.2897953e-01 1.4089248e-02 4.2736441e-02 2.1173391e-01
 1.0596458e-04 1.7397475e-03], sampled 0.11169266622768803
[2019-04-05 13:30:54,151] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.114576556]
[2019-04-05 13:30:54,152] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.476531199666668, 82.31478163666667, 0.0, 0.0, 20.0, 19.08652660960812, -1.119527482665666, 0.0, 1.0, 184750.8932353562]
[2019-04-05 13:30:54,152] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:30:54,153] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2479516e-06 4.4486019e-01 3.2670076e-03 4.1691367e-02 5.1016265e-01
 3.2565134e-07 1.7229626e-05], sampled 0.4283488121011312
[2019-04-05 13:31:16,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.114576556]
[2019-04-05 13:31:16,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.78333333333333, 93.0, 26.33333333333333, 21.0, 21.0, 20.35949445359667, -0.7298378575988754, 0.0, 1.0, 184036.2154073031]
[2019-04-05 13:31:16,483] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:31:16,484] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.4318373e-08 5.3416622e-01 6.5849937e-04 2.0108126e-02 4.4506636e-01
 4.8391535e-09 7.2988951e-07], sampled 0.024559619801138632
[2019-04-05 13:31:30,728] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6423.9224 150944933.7675 -1595.0572
[2019-04-05 13:31:43,516] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6018.5604 180616153.7767 -2039.8107
[2019-04-05 13:31:47,849] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5868.3943 191634395.7102 -2141.7792
[2019-04-05 13:31:48,873] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1160000, evaluation results [1160000.0, 6018.560354788782, 180616153.7767043, -2039.810689915393, 6423.922433488022, 150944933.76749587, -1595.057227520503, 5868.394315013333, 191634395.71016502, -2141.779189396876]
[2019-04-05 13:31:49,544] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7013497e-06 9.5798004e-01 2.7221050e-03 2.5556952e-02 1.3681138e-02
 5.7893426e-06 5.2290110e-05], sum to 1.0000
[2019-04-05 13:31:49,544] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4564
[2019-04-05 13:31:49,553] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.666666666666666, 56.33333333333333, 0.0, 0.0, 19.0, 18.8453233673438, -1.102162465809681, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3973200.0000, 
sim time next is 3973800.0000, 
raw observation next is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 19.0, 18.75793272494385, -1.123312680732577, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.1902123730378578, 0.5716666666666668, 0.0, 0.0, 0.08333333333333333, 0.06316106041198744, 0.12556243975580764, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3305296], dtype=float32), -0.3908662]. 
=============================================
[2019-04-05 13:31:52,591] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2445820e-06 9.5000696e-01 3.3670808e-03 1.6192019e-02 3.0389698e-02
 2.3117418e-07 3.7807553e-05], sum to 1.0000
[2019-04-05 13:31:52,591] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7109
[2019-04-05 13:31:52,606] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 19.0, 19.35442192945243, -1.053849847021444, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2407800.0000, 
sim time next is 2408400.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 19.0, 19.35182015957927, -1.054629718580524, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.42, 0.0, 0.0, 0.08333333333333333, 0.11265167996493908, 0.14845676047315867, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72419703], dtype=float32), 0.66213876]. 
=============================================
[2019-04-05 13:31:52,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1868613e-05 7.0670724e-01 1.2085492e-03 8.4691877e-03 2.8345484e-01
 6.1178719e-07 1.4773131e-04], sum to 1.0000
[2019-04-05 13:31:52,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8883
[2019-04-05 13:31:52,933] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333334, 92.66666666666667, 0.0, 0.0, 19.5, 20.47920678285902, -0.8788833294641587, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2917200.0000, 
sim time next is 2917800.0000, 
raw observation next is [0.0, 92.5, 0.0, 0.0, 20.0, 20.19384077434641, -0.8315707595025014, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.925, 0.0, 0.0, 0.16666666666666666, 0.18282006452886743, 0.22280974683249954, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32733667], dtype=float32), 0.5269192]. 
=============================================
[2019-04-05 13:31:52,989] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.3144203e-06 5.8025265e-01 1.5927914e-03 5.6755370e-03 4.1244096e-01
 1.8561734e-07 3.1640797e-05], sum to 1.0000
[2019-04-05 13:31:52,992] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2081
[2019-04-05 13:31:52,999] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 49.0, 108.5, 793.5, 20.0, 22.03830657672291, -0.4389742109624188, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3506400.0000, 
sim time next is 3507000.0000, 
raw observation next is [3.0, 49.0, 106.3333333333333, 789.3333333333334, 19.0, 22.05601875946706, -0.432512418984341, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.35444444444444434, 0.8721915285451197, 0.08333333333333333, 0.3380015632889218, 0.3558291936718863, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1867954], dtype=float32), -0.4151141]. 
=============================================
[2019-04-05 13:31:53,022] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[68.2381 ]
 [68.15009]
 [68.02107]
 [67.85395]
 [67.69874]], R is [[68.68322754]
 [68.85353851]
 [69.09357452]
 [69.4026413 ]
 [69.63719177]].
[2019-04-05 13:31:54,609] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4441157e-05 4.4899964e-01 6.4906985e-03 2.9081807e-01 2.5355300e-01
 4.1494577e-06 1.1997772e-04], sum to 1.0000
[2019-04-05 13:31:54,609] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1187
[2019-04-05 13:31:54,627] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.5, 76.0, 0.0, 0.0, 21.5, 19.48048180182686, -0.9302798165936359, 0.0, 1.0, 63019.2526142706], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3303000.0000, 
sim time next is 3303600.0000, 
raw observation next is [-10.66666666666667, 76.0, 0.0, 0.0, 20.5, 19.55261354517145, -0.9194461148781201, 0.0, 1.0, 53403.17650425202], 
processed observation next is [1.0, 0.21739130434782608, 0.16712834718374878, 0.76, 0.0, 0.0, 0.20833333333333334, 0.12938446209762086, 0.1935179617072933, 0.0, 1.0, 0.2543008404964382], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.773216], dtype=float32), -0.32374567]. 
=============================================
[2019-04-05 13:31:58,100] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.7569338e-07 7.6824278e-01 1.6707804e-02 2.3695925e-02 1.9134787e-01
 3.1747437e-08 5.0758640e-06], sum to 1.0000
[2019-04-05 13:31:58,104] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8770
[2019-04-05 13:31:58,137] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 42.0, 99.33333333333333, 773.1666666666667, 19.5, 23.45695888520708, -0.06310610454301725, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3595200.0000, 
sim time next is 3595800.0000, 
raw observation next is [-1.0, 42.0, 96.66666666666667, 758.3333333333333, 20.0, 23.41586522719616, -0.07074372985879938, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.32222222222222224, 0.8379373848987108, 0.16666666666666666, 0.4513221022663467, 0.4764187567137335, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.1404251], dtype=float32), 2.0235548]. 
=============================================
[2019-04-05 13:31:59,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4882331e-04 9.1308743e-01 1.4350147e-03 7.9800859e-03 7.7113532e-02
 8.8975639e-07 1.3419378e-04], sum to 1.0000
[2019-04-05 13:31:59,835] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4232
[2019-04-05 13:31:59,847] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 77.0, 77.0, 0.0, 19.0, 19.1490856769908, -1.082699829784153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2622600.0000, 
sim time next is 2623200.0000, 
raw observation next is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 19.0, 19.48358781810951, -1.0469744525119, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.27146814404432135, 0.7633333333333334, 0.26555555555555554, 0.01675874769797421, 0.08333333333333333, 0.12363231817579241, 0.15100851582936667, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8178973], dtype=float32), 0.4393561]. 
=============================================
[2019-04-05 13:32:04,570] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7402381e-04 7.8770149e-01 2.7894033e-03 6.7870826e-03 2.0043667e-01
 9.3192402e-06 1.8019783e-03], sum to 1.0000
[2019-04-05 13:32:04,573] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5302
[2019-04-05 13:32:04,624] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.04999999999999999, 45.0, 164.0, 211.0, 19.0, 20.57838223540353, -0.8984038898531068, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2640600.0000, 
sim time next is 2641200.0000, 
raw observation next is [0.1333333333333333, 44.33333333333333, 172.6666666666667, 197.5, 19.0, 20.71933774913203, -0.79588963686612, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.46629732225300097, 0.4433333333333333, 0.5755555555555557, 0.21823204419889503, 0.08333333333333333, 0.22661147909433588, 0.23470345437796, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8060596], dtype=float32), 0.6497036]. 
=============================================
[2019-04-05 13:32:05,817] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7704781e-08 9.7201407e-01 3.5714468e-03 1.2208938e-02 1.2180967e-02
 1.6074252e-07 2.4379480e-05], sum to 1.0000
[2019-04-05 13:32:05,820] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8521
[2019-04-05 13:32:05,829] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 19.0, 20.87722165305886, -0.7356298463934178, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3025800.0000, 
sim time next is 3026400.0000, 
raw observation next is [-4.666666666666666, 69.0, 0.0, 0.0, 19.0, 20.76849287017474, -0.7580637097729662, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.33333333333333337, 0.69, 0.0, 0.0, 0.08333333333333333, 0.2307077391812283, 0.24731209674234458, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6949962], dtype=float32), 0.11803566]. 
=============================================
[2019-04-05 13:32:09,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1551278e-08 8.3340573e-01 4.2984716e-04 6.4010310e-05 1.6609497e-01
 1.4387794e-08 5.3610456e-06], sum to 1.0000
[2019-04-05 13:32:09,034] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2714
[2019-04-05 13:32:09,040] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.166666666666667, 98.83333333333334, 112.6666666666667, 817.3333333333334, 19.5, 23.44733624790798, -0.08313311032682032, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3156600.0000, 
sim time next is 3157200.0000, 
raw observation next is [7.0, 100.0, 112.5, 814.5, 19.0, 23.45490463394424, -0.0712672071953242, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.375, 0.9, 0.08333333333333333, 0.45457538616201987, 0.47624426426822525, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86133665], dtype=float32), -1.3894536]. 
=============================================
[2019-04-05 13:32:10,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2948172e-07 9.5288640e-01 5.6269215e-03 5.8029210e-03 3.5681903e-02
 2.4309273e-07 1.0240649e-06], sum to 1.0000
[2019-04-05 13:32:10,133] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5354
[2019-04-05 13:32:10,147] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 20.17702524170813, -0.893453952328617, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3113400.0000, 
sim time next is 3114000.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 20.22338076428014, -0.8978840244715034, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.18528173035667836, 0.20070532517616554, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8516728], dtype=float32), -0.460701]. 
=============================================
[2019-04-05 13:32:10,163] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[86.36107 ]
 [87.979645]
 [89.79371 ]
 [91.37397 ]
 [93.78571 ]], R is [[85.38614655]
 [85.5322876 ]
 [85.60553741]
 [85.7494812 ]
 [85.89199066]].
[2019-04-05 13:32:12,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.39223675e-05 9.23565209e-01 1.30265986e-03 2.50045187e-03
 7.25938156e-02 6.95279414e-06 1.70914063e-05], sum to 1.0000
[2019-04-05 13:32:12,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5275
[2019-04-05 13:32:12,201] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.666666666666667, 52.66666666666667, 114.6666666666667, 801.8333333333334, 19.5, 20.03300559321605, -0.7641192996685988, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3331200.0000, 
sim time next is 3331800.0000, 
raw observation next is [-4.5, 52.0, 114.0, 800.0, 19.0, 20.38052457471709, -0.7328287535276421, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.52, 0.38, 0.8839779005524862, 0.08333333333333333, 0.19837704789309077, 0.2557237488241193, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.95536405], dtype=float32), -0.6374521]. 
=============================================
[2019-04-05 13:32:18,948] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7958570e-05 2.8307116e-01 7.5584027e-04 4.0544434e-03 7.1188855e-01
 1.7482533e-07 2.1180425e-04], sum to 1.0000
[2019-04-05 13:32:18,952] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7210
[2019-04-05 13:32:18,966] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 21.0, 19.72323452091777, -0.9124783095560648, 0.0, 1.0, 119767.8585001831], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 2770200.0000, 
sim time next is 2770800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 21.5, 19.8526686996747, -0.885540559621993, 0.0, 1.0, 74250.8499160255], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.2916666666666667, 0.15438905830622515, 0.20481981345933567, 0.0, 1.0, 0.3535754757905976], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.34441587], dtype=float32), 0.036168944]. 
=============================================
[2019-04-05 13:32:24,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6992043e-07 9.7850251e-01 4.5039985e-04 3.6422962e-03 1.7345933e-02
 2.2234872e-07 5.7752019e-05], sum to 1.0000
[2019-04-05 13:32:24,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1374
[2019-04-05 13:32:24,721] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 34.0, 77.0, 630.0, 19.0, 22.02771417831654, -0.5189657158545176, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3945600.0000, 
sim time next is 3946200.0000, 
raw observation next is [-4.166666666666667, 34.66666666666667, 73.33333333333334, 598.6666666666666, 19.0, 21.82323021140657, -0.4860379124740669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3471837488457987, 0.34666666666666673, 0.24444444444444446, 0.6615101289134437, 0.08333333333333333, 0.31860251761721425, 0.3379873625086443, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34082866], dtype=float32), -0.91617817]. 
=============================================
[2019-04-05 13:32:25,782] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-05 13:32:25,791] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:32:25,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:32:25,793] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:32:25,793] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:32:25,794] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:32:25,796] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:32:25,806] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run60
[2019-04-05 13:32:25,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run60
[2019-04-05 13:32:25,841] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run60
[2019-04-05 13:33:03,592] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.115937024]
[2019-04-05 13:33:03,592] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.7, 70.0, 225.0, 196.0, 19.0, 20.32891276861464, -0.6920050862836683, 1.0, 1.0, 0.0]
[2019-04-05 13:33:03,592] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:33:03,593] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.8802026e-05 7.9370922e-01 3.4170472e-03 2.0355562e-02 1.8234709e-01
 3.8610110e-06 1.3838375e-04], sampled 0.9502983511287559
[2019-04-05 13:33:03,831] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.115937024]
[2019-04-05 13:33:03,831] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 19.18319574691866, -1.10932806339542, 0.0, 1.0, 0.0]
[2019-04-05 13:33:03,831] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:33:03,832] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.2850483e-06 4.8945102e-01 3.1489674e-03 3.8489822e-02 4.6885642e-01
 1.0818549e-06 4.9367660e-05], sampled 0.390372434987304
[2019-04-05 13:33:18,591] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.115937024]
[2019-04-05 13:33:18,592] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.0, 48.0, 0.0, 0.0, 19.5, 20.28199973011279, -0.8962889335322121, 0.0, 1.0, 0.0]
[2019-04-05 13:33:18,592] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:33:18,594] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.9316911e-07 6.7752099e-01 1.1536670e-03 2.7800249e-02 2.9352203e-01
 4.1375060e-08 2.8429179e-06], sampled 0.8285492667275851
[2019-04-05 13:33:40,083] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6360.4173 148034344.3058 -1653.0166
[2019-04-05 13:33:47,083] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.115937024]
[2019-04-05 13:33:47,084] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.0, 60.0, 0.0, 0.0, 19.5, 19.18123066217157, -1.039225106057287, 0.0, 1.0, 196217.9094192961]
[2019-04-05 13:33:47,084] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:33:47,085] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.3376491e-06 5.4862666e-01 4.2172009e-03 4.4737887e-02 4.0237072e-01
 1.0314538e-06 4.3113396e-05], sampled 0.8465335066934437
[2019-04-05 13:33:50,244] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5893.7424 179190419.9312 -2261.1640
[2019-04-05 13:33:56,079] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5840.2422 191690834.0408 -2281.1512
[2019-04-05 13:33:57,105] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1180000, evaluation results [1180000.0, 5893.742373163382, 179190419.93123624, -2261.1640122787326, 6360.417347661714, 148034344.3057537, -1653.0166175170446, 5840.242159535262, 191690834.04076046, -2281.1512348562796]
[2019-04-05 13:33:57,510] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2795037e-06 5.8362824e-01 1.0983685e-02 1.9077148e-01 2.1460617e-01
 6.1664639e-07 8.5345127e-06], sum to 1.0000
[2019-04-05 13:33:57,514] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3219
[2019-04-05 13:33:57,521] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 19.70060700449086, -0.9922971174726296, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4250400.0000, 
sim time next is 4251000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 19.64466929607795, -1.003728527971374, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.13705577467316252, 0.16542382400954203, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9188345], dtype=float32), 1.0742805]. 
=============================================
[2019-04-05 13:33:57,537] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.72389]
 [79.76465]
 [79.85117]
 [79.88008]
 [79.91544]], R is [[79.86897278]
 [80.07028198]
 [80.26957703]
 [80.39545441]
 [80.59149933]].
[2019-04-05 13:34:03,071] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.7080274e-08 9.8167062e-01 4.7289199e-05 6.1987771e-04 1.7659912e-02
 3.1463490e-10 2.2538243e-06], sum to 1.0000
[2019-04-05 13:34:03,072] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8335
[2019-04-05 13:34:03,091] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.0, 100.0, 101.0, 763.0, 20.5, 23.65372837753183, -0.01797186742910467, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3162600.0000, 
sim time next is 3163200.0000, 
raw observation next is [7.0, 100.0, 98.16666666666667, 749.0, 19.5, 23.69399217743018, -0.09303959982209248, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.32722222222222225, 0.8276243093922652, 0.125, 0.47449934811918154, 0.4689868000593025, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.7117676], dtype=float32), -0.40403208]. 
=============================================
[2019-04-05 13:34:11,170] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6739493e-07 4.1823912e-01 5.7547953e-04 6.4763995e-03 5.7450491e-01
 4.1493283e-07 2.0331558e-04], sum to 1.0000
[2019-04-05 13:34:11,173] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0663
[2019-04-05 13:34:11,189] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 18.59489954304926, -1.084296837065248, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3360000.0000, 
sim time next is 3360600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 18.57454817366076, -1.060516946436143, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.047879014471729896, 0.14649435118795232, 0.0, 1.0, 0.9343709972347434], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.6495335], dtype=float32), -1.227194]. 
=============================================
[2019-04-05 13:34:11,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:34:11,314] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:34:11,348] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run10
[2019-04-05 13:34:12,932] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1432184e-08 9.7938043e-01 1.5143551e-03 1.5842103e-03 1.7505126e-02
 2.0260066e-08 1.5790471e-05], sum to 1.0000
[2019-04-05 13:34:12,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4037
[2019-04-05 13:34:12,943] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.416666666666667, 71.83333333333333, 0.0, 0.0, 19.0, 20.79844718407164, -0.7389993650682687, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4327800.0000, 
sim time next is 4328400.0000, 
raw observation next is [4.333333333333334, 71.66666666666667, 0.0, 0.0, 19.0, 20.77465632612467, -0.7502040038216912, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.58264081255771, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.23122136051038922, 0.24993199872610294, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0342894], dtype=float32), -0.63666296]. 
=============================================
[2019-04-05 13:34:20,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8536218e-08 8.7771016e-01 7.9035917e-03 1.0902667e-01 5.3272783e-03
 3.6673634e-07 3.1884800e-05], sum to 1.0000
[2019-04-05 13:34:20,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5959
[2019-04-05 13:34:20,825] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.866666666666667, 100.0, 0.0, 0.0, 19.5, 20.09655017668113, -0.8555626858580901, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3127200.0000, 
sim time next is 3127800.0000, 
raw observation next is [2.933333333333333, 100.0, 0.0, 0.0, 19.0, 20.3513061878392, -0.8363346408932638, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.543859649122807, 1.0, 0.0, 0.0, 0.08333333333333333, 0.1959421823199333, 0.22122178636891207, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3058569], dtype=float32), 0.50820804]. 
=============================================
[2019-04-05 13:34:24,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0999995e-05 7.3662221e-01 3.0142304e-03 4.7670636e-02 2.1233582e-01
 2.6663625e-07 3.4589364e-04], sum to 1.0000
[2019-04-05 13:34:24,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6057
[2019-04-05 13:34:24,888] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.4, 80.0, 111.0, 781.5, 19.5, 21.43909540981075, -0.5067348004083921, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3236400.0000, 
sim time next is 3237000.0000, 
raw observation next is [-2.333333333333333, 78.5, 111.6666666666667, 791.3333333333334, 19.0, 21.49451261420839, -0.5773769450812413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3979686057248385, 0.785, 0.37222222222222234, 0.874401473296501, 0.08333333333333333, 0.2912093845173658, 0.3075410183062529, 1.0, 1.0, 0.0], 
reward next is 0.2262, 
noisyNet noise sample is [array([0.8550584], dtype=float32), 0.1309042]. 
=============================================
[2019-04-05 13:34:24,920] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[71.21713]
 [70.55257]
 [69.95501]
 [69.22951]
 [68.15014]], R is [[71.23388672]
 [71.38276672]
 [71.47682953]
 [71.47414398]
 [71.27359009]].
[2019-04-05 13:34:25,122] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3415329e-06 6.7523330e-01 1.3747315e-03 5.6204893e-02 2.6714694e-01
 5.0363674e-06 3.3753113e-05], sum to 1.0000
[2019-04-05 13:34:25,127] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7616
[2019-04-05 13:34:25,139] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.0, 39.0, 0.0, 0.0, 19.0, 19.72685240304587, -1.005707122472075, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4072800.0000, 
sim time next is 4073400.0000, 
raw observation next is [-5.0, 39.5, 0.0, 0.0, 19.5, 19.59189111073034, -1.029916036179732, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.395, 0.0, 0.0, 0.125, 0.13265759256086174, 0.15669465460675602, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.2912698], dtype=float32), 0.03077185]. 
=============================================
[2019-04-05 13:34:25,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8026756e-07 9.1386956e-01 1.6131940e-03 3.5516392e-03 8.0962688e-02
 6.5121512e-08 2.6837761e-06], sum to 1.0000
[2019-04-05 13:34:25,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2830
[2019-04-05 13:34:25,464] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 20.77571657328297, -0.6276617160402901, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3528600.0000, 
sim time next is 3529200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 20.66113233588615, -0.6463244622087548, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.2217610279905126, 0.2845585125970817, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03206747], dtype=float32), 0.84235984]. 
=============================================
[2019-04-05 13:34:26,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6549907e-06 4.3145052e-01 2.2268594e-04 2.1567785e-03 5.6615561e-01
 9.5907275e-08 8.7321741e-06], sum to 1.0000
[2019-04-05 13:34:26,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2563
[2019-04-05 13:34:26,759] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.333333333333333, 81.66666666666667, 130.5, 0.0, 19.0, 20.44964361131898, -0.7594477823041125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4713600.0000, 
sim time next is 4714200.0000, 
raw observation next is [1.5, 79.5, 135.0, 0.0, 19.0, 20.63936214737573, -0.7408743699531701, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5041551246537397, 0.795, 0.45, 0.0, 0.08333333333333333, 0.21994684561464428, 0.2530418766822766, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.45573193], dtype=float32), 2.2490497]. 
=============================================
[2019-04-05 13:34:29,350] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.8310919e-07 9.8229086e-01 4.5262487e-04 1.3288816e-03 1.5742611e-02
 5.2820166e-09 1.8439129e-04], sum to 1.0000
[2019-04-05 13:34:29,353] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3578
[2019-04-05 13:34:29,371] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.666666666666667, 63.33333333333334, 118.3333333333333, 827.8333333333334, 19.0, 24.1526099716784, -0.01852614951727373, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3759600.0000, 
sim time next is 3760200.0000, 
raw observation next is [-1.5, 62.5, 119.0, 829.0, 19.0, 24.14019071227318, -0.03529184243060537, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4210526315789474, 0.625, 0.39666666666666667, 0.9160220994475138, 0.08333333333333333, 0.5116825593560984, 0.4882360525231315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12479196], dtype=float32), 0.5521379]. 
=============================================
[2019-04-05 13:34:30,704] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-05 13:34:30,705] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:34:30,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:34:30,706] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:34:30,706] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:34:30,707] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:34:30,707] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:34:30,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run61
[2019-04-05 13:34:30,730] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run61
[2019-04-05 13:34:30,751] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run61
[2019-04-05 13:35:02,374] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11806306]
[2019-04-05 13:35:02,374] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.266666666666667, 85.66666666666667, 39.50000000000001, 83.33333333333334, 19.0, 20.10624016653084, -0.755988677349495, 1.0, 1.0, 0.0]
[2019-04-05 13:35:02,374] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:35:02,375] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.9332654e-07 6.8933308e-01 1.1765474e-03 1.8427007e-02 2.9105115e-01
 1.6522939e-07 1.1276578e-05], sampled 0.011541328690483743
[2019-04-05 13:35:05,996] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11806306]
[2019-04-05 13:35:05,997] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.4216362815, 84.77704783, 0.0, 0.0, 19.0, 19.54375801442436, -1.006002018604234, 0.0, 1.0, 0.0]
[2019-04-05 13:35:05,997] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:35:05,998] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8666226e-07 6.5544409e-01 8.8464946e-04 1.7245281e-02 3.2642180e-01
 4.7976197e-08 4.0028121e-06], sampled 0.8690804475578147
[2019-04-05 13:35:43,542] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6365.2308 148525719.0271 -1704.8675
[2019-04-05 13:35:54,310] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5848.9413 176408852.2337 -2359.4962
[2019-04-05 13:36:00,332] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5898.3172 188457965.9378 -2273.7923
[2019-04-05 13:36:01,358] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1200000, evaluation results [1200000.0, 5848.941280036333, 176408852.23371047, -2359.496212151003, 6365.230755684217, 148525719.02710333, -1704.867453154539, 5898.317219317869, 188457965.93778577, -2273.7922691472763]
[2019-04-05 13:36:04,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:36:04,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:04,562] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run10
[2019-04-05 13:36:09,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2861003e-06 6.2334651e-01 1.0074065e-03 1.6996037e-03 3.7394068e-01
 1.0187235e-06 1.6258072e-06], sum to 1.0000
[2019-04-05 13:36:09,171] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0056
[2019-04-05 13:36:09,182] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 19.5, 20.28368700426825, -0.7579892306406041, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4517400.0000, 
sim time next is 4518000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 19.0, 20.46654732304086, -0.7594195490225172, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4349030470914128, 0.71, 0.0, 0.0, 0.08333333333333333, 0.205545610253405, 0.24686015032582762, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61320364], dtype=float32), -0.4593996]. 
=============================================
[2019-04-05 13:36:09,191] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.405655]
 [83.24106 ]
 [83.60907 ]
 [83.24527 ]
 [82.778694]], R is [[83.70986938]
 [83.80134583]
 [83.74904633]
 [83.55442047]
 [83.4331665 ]].
[2019-04-05 13:36:14,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:36:14,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:14,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run10
[2019-04-05 13:36:15,126] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:36:15,128] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:15,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run10
[2019-04-05 13:36:15,589] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.2061299e-05 8.1981760e-01 1.8200983e-03 1.0558210e-02 1.6519214e-01
 1.4603367e-05 2.5152881e-03], sum to 1.0000
[2019-04-05 13:36:15,590] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4390
[2019-04-05 13:36:15,603] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 19.0, 19.67008285893347, -0.9782182928482218, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 128400.0000, 
sim time next is 129000.0000, 
raw observation next is [-8.3, 65.16666666666667, 166.0, 209.6666666666666, 19.0, 19.69186561221203, -0.9693489413700161, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.23268698060941828, 0.6516666666666667, 0.5533333333333333, 0.23167587476979734, 0.08333333333333333, 0.1409888010176692, 0.17688368620999462, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.30617577], dtype=float32), 0.65333563]. 
=============================================
[2019-04-05 13:36:15,618] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[51.079655]
 [50.94051 ]
 [51.055836]
 [50.98154 ]
 [51.003914]], R is [[50.7647171 ]
 [50.25706863]
 [49.75449753]
 [49.25695419]
 [48.76438522]].
[2019-04-05 13:36:19,265] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3310245e-07 7.2735858e-01 7.5915074e-03 4.4866018e-02 2.2018006e-01
 9.5484625e-09 3.7379643e-06], sum to 1.0000
[2019-04-05 13:36:19,270] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8377
[2019-04-05 13:36:19,307] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [2.0, 34.0, 166.0, 758.0, 19.0, 21.45955579137863, -0.5271599195663433, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4194000.0000, 
sim time next is 4194600.0000, 
raw observation next is [2.0, 35.0, 182.0, 728.3333333333333, 19.5, 21.46195059150777, -0.5242958167171862, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.35, 0.6066666666666667, 0.8047882136279926, 0.125, 0.2884958826256474, 0.3252347277609379, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.5207349], dtype=float32), 0.22822127]. 
=============================================
[2019-04-05 13:36:30,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:36:30,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:30,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run10
[2019-04-05 13:36:33,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:36:33,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:33,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run10
[2019-04-05 13:36:34,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:36:34,913] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:34,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run10
[2019-04-05 13:36:35,523] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-05 13:36:35,528] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:36:35,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:35,531] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:36:35,532] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:36:35,532] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:35,532] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:35,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run62
[2019-04-05 13:36:35,554] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run62
[2019-04-05 13:36:35,554] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run62
[2019-04-05 13:36:36,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:36:36,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:36:36,515] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run10
[2019-04-05 13:36:40,038] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11894533]
[2019-04-05 13:36:40,038] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.9, 85.0, 0.0, 0.0, 19.5, 18.69806020815605, -1.148814597355521, 0.0, 1.0, 198004.8635782]
[2019-04-05 13:36:40,039] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:36:40,040] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.2767854e-08 6.3617158e-01 6.3330616e-04 2.5137162e-02 3.3805609e-01
 3.3518909e-08 1.7288399e-06], sampled 0.9940261187906375
[2019-04-05 13:37:06,259] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11894088]
[2019-04-05 13:37:06,259] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.1, 88.0, 0.0, 0.0, 19.0, 18.97814549445285, -0.990574195992795, 0.0, 1.0, 0.0]
[2019-04-05 13:37:06,259] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:37:06,260] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5709689e-06 8.1563407e-01 1.0283329e-03 9.8326653e-03 1.7348471e-01
 2.5406774e-07 1.8405686e-05], sampled 0.026621815493873657
[2019-04-05 13:37:19,493] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11894088]
[2019-04-05 13:37:19,493] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.2, 52.33333333333334, 0.0, 0.0, 19.0, 19.47341212581718, -1.068139627518185, 1.0, 1.0, 0.0]
[2019-04-05 13:37:19,493] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:37:19,494] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.0780443e-05 8.1222075e-01 4.6099438e-03 1.8255858e-02 1.6453871e-01
 1.0508756e-05 3.0352524e-04], sampled 0.0868791804796335
[2019-04-05 13:37:39,175] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11894088]
[2019-04-05 13:37:39,175] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.083333333333333, 55.33333333333334, 120.6666666666667, 685.0, 19.0, 20.0347652605039, -0.8957058526648058, 1.0, 1.0, 0.0]
[2019-04-05 13:37:39,175] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:37:39,176] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1091116e-05 8.0110437e-01 2.2934615e-03 1.8958928e-02 1.7756175e-01
 2.0639295e-06 6.8300193e-05], sampled 0.7100773428187186
[2019-04-05 13:37:43,982] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11894088]
[2019-04-05 13:37:43,982] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 71.0, 0.0, 0.0, 20.0, 19.69502466272567, -0.9151270765435715, 0.0, 1.0, 0.0]
[2019-04-05 13:37:43,982] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:37:43,983] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.8368952e-07 5.8757365e-01 1.3725910e-03 2.6315825e-02 3.8472623e-01
 2.0043967e-07 1.1041592e-05], sampled 0.7261279204095951
[2019-04-05 13:37:47,403] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6278.3630 145848737.9185 -1875.6369
[2019-04-05 13:38:00,752] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6015.4267 173208721.3996 -2340.1850
[2019-04-05 13:38:04,310] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5932.3530 189877021.9786 -2332.1910
[2019-04-05 13:38:05,336] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1220000, evaluation results [1220000.0, 6015.426728205288, 173208721.39964896, -2340.185013895156, 6278.362968287545, 145848737.91848895, -1875.6368697497455, 5932.353031263803, 189877021.9786098, -2332.1909670317796]
[2019-04-05 13:38:05,657] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8207591e-07 3.7937391e-01 3.2551415e-04 1.7863099e-02 6.0242647e-01
 1.0875598e-07 1.0423632e-05], sum to 1.0000
[2019-04-05 13:38:05,659] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1883
[2019-04-05 13:38:05,670] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 20.0, 20.06047636633542, -0.864775975321273, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 7200.0000, 
sim time next is 7800.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 19.0, 20.03114865895358, -0.8721473829265283, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.08333333333333333, 0.16926238824613163, 0.20928420569115724, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.25865], dtype=float32), 0.101057634]. 
=============================================
[2019-04-05 13:38:08,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:08,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:08,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run10
[2019-04-05 13:38:12,447] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.7405809e-07 7.7438313e-01 8.3334412e-04 1.4273917e-03 2.2334607e-01
 5.1751499e-08 9.2535565e-06], sum to 1.0000
[2019-04-05 13:38:12,452] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7739
[2019-04-05 13:38:12,465] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 19.0, 18.67596173206604, -1.131268784028798, 0.0, 1.0, 23860.411377991], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4777800.0000, 
sim time next is 4778400.0000, 
raw observation next is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 19.0, 18.64559222325106, -1.137127236840254, 0.0, 1.0, 46626.11623662667], 
processed observation next is [0.0, 0.30434782608695654, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 0.08333333333333333, 0.05379935193758841, 0.12095758771991531, 0.0, 1.0, 0.2220291249363175], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37403396], dtype=float32), 1.2493877]. 
=============================================
[2019-04-05 13:38:23,283] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:23,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:23,290] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run10
[2019-04-05 13:38:23,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:23,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:23,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run10
[2019-04-05 13:38:24,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:24,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:24,358] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run10
[2019-04-05 13:38:26,017] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4319215e-05 7.5338364e-01 4.7222418e-03 1.0752237e-02 2.3086749e-01
 7.9638867e-06 2.5202637e-04], sum to 1.0000
[2019-04-05 13:38:26,020] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4040
[2019-04-05 13:38:26,060] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 84.0, 0.0, 0.0, 19.0, 19.842006562712, -1.152327041738627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 840600.0000, 
sim time next is 841200.0000, 
raw observation next is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 19.0, 19.45064927230251, -1.14470998466583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.35457063711911363, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.12088743935854247, 0.11843000511139001, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.50523746], dtype=float32), -0.002884626]. 
=============================================
[2019-04-05 13:38:27,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:27,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:27,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run10
[2019-04-05 13:38:29,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:29,572] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:29,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run10
[2019-04-05 13:38:30,509] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5158141e-07 7.6546484e-01 3.2729141e-03 1.9439630e-01 3.6865085e-02
 6.3381499e-08 5.3247174e-07], sum to 1.0000
[2019-04-05 13:38:30,512] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-05 13:38:30,522] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 20.24610863457135, -0.7906990638516581, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 24000.0000, 
sim time next is 24600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 20.21729606537762, -0.7984054795629271, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.18477467211480172, 0.23386484014569098, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25405464], dtype=float32), 0.15038152]. 
=============================================
[2019-04-05 13:38:32,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:32,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:32,035] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run10
[2019-04-05 13:38:42,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7422242e-07 6.6862452e-01 6.5752938e-03 3.6598381e-03 3.2090962e-01
 2.2935421e-07 2.2952315e-04], sum to 1.0000
[2019-04-05 13:38:42,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7867
[2019-04-05 13:38:42,729] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [10.0, 22.5, 118.0, 860.0, 19.0, 22.75158450436623, -0.2245566641130779, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 5059800.0000, 
sim time next is 5060400.0000, 
raw observation next is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 19.5, 23.06586153921459, -0.1865259141135565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7488457987072946, 0.21666666666666662, 0.3894444444444443, 0.9427255985267036, 0.125, 0.4221551282678826, 0.4378246952954812, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([2.161422], dtype=float32), 0.6240539]. 
=============================================
[2019-04-05 13:38:44,465] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2796753e-07 4.7557864e-01 1.9013467e-03 5.0222015e-01 2.0280762e-02
 1.3989834e-06 1.7129894e-05], sum to 1.0000
[2019-04-05 13:38:44,466] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2855
[2019-04-05 13:38:44,495] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 83.0, 83.0, 138.0, 19.0, 19.31903635532572, -1.041345834682737, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 558000.0000, 
sim time next is 558600.0000, 
raw observation next is [-0.6333333333333333, 82.66666666666667, 85.0, 137.0, 19.0, 19.25902800623753, -1.047626939292601, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.44506001846722076, 0.8266666666666667, 0.2833333333333333, 0.15138121546961325, 0.08333333333333333, 0.10491900051979404, 0.15079102023579968, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3094318], dtype=float32), -1.5998892]. 
=============================================
[2019-04-05 13:38:44,532] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-05 13:38:44,540] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:38:44,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:44,541] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:38:44,542] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:38:44,543] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:44,544] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:44,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run63
[2019-04-05 13:38:44,571] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run63
[2019-04-05 13:38:44,587] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run63
[2019-04-05 13:38:45,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:38:45,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:38:45,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run10
[2019-04-05 13:39:47,199] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116997086]
[2019-04-05 13:39:47,200] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.533551273500001, 100.0, 0.0, 0.0, 20.0, 22.36514609525226, -0.2674904734661424, 0.0, 1.0, 0.0]
[2019-04-05 13:39:47,201] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:39:47,202] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.4236240e-07 8.0513328e-01 3.8766029e-04 5.1933909e-03 1.8927617e-01
 3.3625113e-08 9.1238198e-06], sampled 0.055752621995406426
[2019-04-05 13:39:58,503] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6397.4610 150262622.7825 -1587.1394
[2019-04-05 13:40:09,956] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5979.1169 176389550.8751 -2181.4366
[2019-04-05 13:40:12,952] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5779.8280 192683818.9186 -2266.5191
[2019-04-05 13:40:13,976] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1240000, evaluation results [1240000.0, 5979.116873864991, 176389550.87507546, -2181.4366155940384, 6397.460987234992, 150262622.78247365, -1587.1394193448098, 5779.828000015797, 192683818.91863048, -2266.5191307427426]
[2019-04-05 13:40:24,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9977061e-05 7.8890342e-01 6.9280141e-03 4.5229155e-03 1.9952767e-01
 1.0677256e-05 3.7394097e-05], sum to 1.0000
[2019-04-05 13:40:24,527] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4671
[2019-04-05 13:40:24,540] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.1833333333333333, 73.33333333333333, 19.33333333333334, 0.0, 19.0, 19.63835713581557, -1.032907746777017, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 893400.0000, 
sim time next is 894000.0000, 
raw observation next is [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 19.0, 19.60010237966674, -0.9937407574643901, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4727608494921515, 0.7466666666666667, 0.08055555555555557, 0.0, 0.08333333333333333, 0.13334186497222836, 0.16875308084520332, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.61766094], dtype=float32), -0.11631388]. 
=============================================
[2019-04-05 13:40:24,548] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[57.339157]
 [59.881306]
 [62.203274]
 [65.24687 ]
 [68.76231 ]], R is [[55.44923019]
 [54.89473724]
 [54.34579086]
 [53.80233383]
 [53.26431274]].
[2019-04-05 13:40:33,352] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9027608e-06 5.3607816e-01 1.8463018e-03 7.9286089e-03 4.5393082e-01
 8.5420907e-06 2.0257758e-04], sum to 1.0000
[2019-04-05 13:40:33,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3051
[2019-04-05 13:40:33,360] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 19.0, 19.03473217637036, -1.003281136963671, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1495800.0000, 
sim time next is 1496400.0000, 
raw observation next is [1.1, 100.0, 0.0, 0.0, 19.0, 19.04736559609885, -1.010990547884599, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.08728046634157079, 0.16300315070513363, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.857035], dtype=float32), -1.0698824]. 
=============================================
[2019-04-05 13:40:36,252] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4811815e-08 3.8179584e-02 3.8000604e-04 3.6801307e-03 9.5775944e-01
 6.3693157e-09 7.7809676e-07], sum to 1.0000
[2019-04-05 13:40:36,253] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7571
[2019-04-05 13:40:36,270] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 85.66666666666667, 0.0, 0.0, 20.0, 19.13813138690097, -1.090724555059217, 0.0, 1.0, 198467.5748290732], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 603600.0000, 
sim time next is 604200.0000, 
raw observation next is [-3.4, 86.33333333333333, 0.0, 0.0, 20.5, 19.11295300263065, -1.067637064616704, 0.0, 1.0, 196560.2707869263], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8633333333333333, 0.0, 0.0, 0.20833333333333334, 0.0927460835525542, 0.1441209784610987, 0.0, 1.0, 0.9360012894615538], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.3111972], dtype=float32), 1.1824162]. 
=============================================
[2019-04-05 13:40:41,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2316391e-06 7.0026207e-01 1.1241653e-03 3.1018897e-03 2.9549336e-01
 6.0476637e-09 1.7328188e-05], sum to 1.0000
[2019-04-05 13:40:41,037] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0569
[2019-04-05 13:40:41,050] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.5, 20.32752386434596, -0.660362459791902, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1374600.0000, 
sim time next is 1375200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 20.31129951124406, -0.6673794531244619, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.19260829260367154, 0.277540182291846, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44618616], dtype=float32), -0.85689646]. 
=============================================
[2019-04-05 13:40:42,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6301128e-08 4.3548054e-01 1.2316018e-03 1.3483617e-02 5.4980117e-01
 2.2806622e-08 3.0229689e-06], sum to 1.0000
[2019-04-05 13:40:42,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5806
[2019-04-05 13:40:42,562] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.2, 67.0, 106.5, 0.0, 19.0, 20.26086541429903, -0.663970047505742, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1159200.0000, 
sim time next is 1159800.0000, 
raw observation next is [17.38333333333333, 66.66666666666667, 114.3333333333333, 0.0, 19.5, 20.26954067051697, -0.6598751640469137, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9441366574330563, 0.6666666666666667, 0.381111111111111, 0.0, 0.125, 0.1891283892097476, 0.2800416119843621, 0.0, 0.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([2.418319], dtype=float32), -0.6743016]. 
=============================================
[2019-04-05 13:40:44,796] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7371571e-05 9.1431744e-02 3.2194299e-04 2.1052405e-03 9.0607685e-01
 4.9259602e-06 4.1884639e-05], sum to 1.0000
[2019-04-05 13:40:44,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5646
[2019-04-05 13:40:44,813] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.1, 90.0, 0.0, 0.0, 19.0, 20.46727310112551, -0.7165483850288535, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1449000.0000, 
sim time next is 1449600.0000, 
raw observation next is [1.1, 90.66666666666666, 0.0, 0.0, 19.5, 20.36910343928621, -0.7224040003737261, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.9066666666666666, 0.0, 0.0, 0.125, 0.19742528660718417, 0.2591986665420913, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1780301], dtype=float32), 0.18100792]. 
=============================================
[2019-04-05 13:40:45,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.75049737e-04 8.75108480e-01 8.90712079e-04 1.84154939e-02
 1.05094016e-01 1.16934928e-06 2.15067848e-04], sum to 1.0000
[2019-04-05 13:40:45,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8030
[2019-04-05 13:40:45,159] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 95.0, 90.0, 0.0, 22.5, 24.4491687910182, 0.0541779402411447, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 1423800.0000, 
sim time next is 1424400.0000, 
raw observation next is [0.0, 95.0, 91.0, 0.0, 21.5, 24.48989402714843, 0.060173743470536, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.30333333333333334, 0.0, 0.2916666666666667, 0.5408245022623692, 0.5200579144901787, 1.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.3634574], dtype=float32), 0.2016884]. 
=============================================
[2019-04-05 13:40:50,691] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1054287e-05 9.0932572e-01 5.3720255e-03 7.6041226e-03 7.7645607e-02
 8.0240278e-07 4.0729581e-05], sum to 1.0000
[2019-04-05 13:40:50,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3250
[2019-04-05 13:40:50,703] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 49.0, 171.5, 20.66666666666666, 19.0, 22.87746698654434, -0.233386565677011, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1603200.0000, 
sim time next is 1603800.0000, 
raw observation next is [13.8, 49.0, 176.0, 0.0, 19.0, 22.88985078917031, -0.209162019793998, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5866666666666667, 0.0, 0.08333333333333333, 0.40748756576419254, 0.430279326735334, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21546316], dtype=float32), 0.27971157]. 
=============================================
[2019-04-05 13:40:50,795] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-05 13:40:50,797] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:40:50,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:40:50,797] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:40:50,798] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:40:50,798] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:40:50,799] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:40:50,810] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run64
[2019-04-05 13:40:50,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run64
[2019-04-05 13:40:50,826] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run64
[2019-04-05 13:42:03,856] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6431.5886 147185611.3843 -1685.5660
[2019-04-05 13:42:14,679] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1159071]
[2019-04-05 13:42:14,679] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.666666666666667, 40.0, 0.0, 0.0, 19.5, 20.14312680840379, -0.8142568521545354, 0.0, 1.0, 0.0]
[2019-04-05 13:42:14,679] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:42:14,680] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.2666251e-06 6.8463981e-01 3.5372660e-03 2.4755005e-02 2.8694314e-01
 1.5734681e-06 1.1401378e-04], sampled 0.20406638882444883
[2019-04-05 13:42:15,580] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5945.6242 175659139.4193 -2289.4621
[2019-04-05 13:42:20,962] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5841.5939 189848021.5742 -2331.9457
[2019-04-05 13:42:21,988] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1260000, evaluation results [1260000.0, 5945.624166567229, 175659139.4193346, -2289.4620647351235, 6431.588569448583, 147185611.38434047, -1685.5659652162992, 5841.593851895838, 189848021.57423216, -2331.9456600881185]
[2019-04-05 13:42:29,285] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4309996e-06 2.9220924e-01 6.6642729e-03 1.3753771e-02 6.8735057e-01
 1.1694913e-06 1.3527559e-05], sum to 1.0000
[2019-04-05 13:42:29,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3843
[2019-04-05 13:42:29,371] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.3, 85.66666666666667, 115.3333333333333, 0.0, 23.0, 20.84159999895264, -0.5754085041571281, 0.0, 1.0, 201496.4913909213], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 1768800.0000, 
sim time next is 1769400.0000, 
raw observation next is [-2.3, 85.0, 119.0, 0.0, 23.5, 21.22508576607494, -0.4836815082861281, 0.0, 1.0, 202992.6516014446], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.85, 0.39666666666666667, 0.0, 0.4583333333333333, 0.2687571471729117, 0.33877283057129065, 0.0, 1.0, 0.9666316742925933], 
reward next is 0.3571, 
noisyNet noise sample is [array([1.0891412], dtype=float32), -0.3252198]. 
=============================================
[2019-04-05 13:42:30,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2002423e-05 9.7078180e-01 1.3056793e-03 7.0926774e-04 2.7122844e-02
 5.4386496e-06 4.2968750e-05], sum to 1.0000
[2019-04-05 13:42:30,831] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9017
[2019-04-05 13:42:30,841] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 19.0, 21.64819843311308, -0.5780136457376072, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2035200.0000, 
sim time next is 2035800.0000, 
raw observation next is [-4.2, 79.0, 148.0, 0.0, 19.0, 21.66549790987988, -0.567981038110531, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.49333333333333335, 0.0, 0.08333333333333333, 0.30545815915665653, 0.3106729872964897, 1.0, 1.0, 0.0], 
reward next is 0.3202, 
noisyNet noise sample is [array([-0.37390727], dtype=float32), -0.2852946]. 
=============================================
[2019-04-05 13:42:33,646] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5056172e-05 7.1973944e-01 2.6792360e-03 4.2435699e-03 2.7315986e-01
 8.8064968e-07 1.6193920e-04], sum to 1.0000
[2019-04-05 13:42:33,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2918
[2019-04-05 13:42:33,664] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 56.0, 0.0, 0.0, 19.5, 18.88167956191364, -1.119137207738633, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2318400.0000, 
sim time next is 2319000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 19.0, 18.79892006904137, -1.117193794700129, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.08333333333333333, 0.06657667242011407, 0.12760206843329036, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5114508], dtype=float32), -0.2067527]. 
=============================================
[2019-04-05 13:42:33,671] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[62.22576 ]
 [58.839638]
 [59.36505 ]
 [56.147156]
 [52.870647]], R is [[59.95338058]
 [60.2824173 ]
 [59.67959213]
 [60.082798  ]
 [60.48197174]].
[2019-04-05 13:42:45,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4222087e-05 6.1739588e-01 1.0066904e-01 9.1799386e-02 1.8998596e-01
 4.0172122e-06 1.2140566e-04], sum to 1.0000
[2019-04-05 13:42:45,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2770
[2019-04-05 13:42:45,800] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 12.0, 0.0, 19.0, 19.81973293619708, -0.8500867801523432, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1411800.0000, 
sim time next is 1412400.0000, 
raw observation next is [-0.6, 100.0, 15.0, 0.0, 19.0, 19.74949325613087, -0.8015429538040945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.05, 0.0, 0.08333333333333333, 0.1457911046775724, 0.23281901539863517, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3332723], dtype=float32), -0.749309]. 
=============================================
[2019-04-05 13:42:56,670] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-05 13:42:56,674] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:42:56,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:42:56,678] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:42:56,678] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:42:56,679] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:42:56,680] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:42:56,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run65
[2019-04-05 13:42:56,713] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run65
[2019-04-05 13:42:56,713] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run65
[2019-04-05 13:43:34,569] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11618572]
[2019-04-05 13:43:34,569] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-8.8754108, 67.44271702, 199.3172333, 127.55178185, 19.0, 19.17474126466767, -1.085054541456029, 0.0, 1.0, 0.0]
[2019-04-05 13:43:34,569] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:43:34,570] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3946192e-05 5.3877956e-01 3.2446841e-03 3.5674635e-02 4.2216599e-01
 3.5158407e-06 1.1760868e-04], sampled 0.39355462230644955
[2019-04-05 13:44:00,766] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11618572]
[2019-04-05 13:44:00,766] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.702302791833333, 98.62299788499999, 0.0, 0.0, 19.5, 20.06647170622407, -0.9121816404183711, 0.0, 1.0, 0.0]
[2019-04-05 13:44:00,766] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:44:00,767] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.6689846e-07 5.4521114e-01 8.6609490e-04 1.6312303e-02 4.3759215e-01
 2.2716380e-07 1.7175471e-05], sampled 0.14739120449542076
[2019-04-05 13:44:04,023] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11618572]
[2019-04-05 13:44:04,024] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.2, 67.0, 177.0, 574.5, 19.0, 23.29452652888838, -0.08372628729890848, 1.0, 1.0, 0.0]
[2019-04-05 13:44:04,024] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:44:04,025] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.9553265e-06 7.6497799e-01 1.1628262e-03 9.2841526e-03 2.2449724e-01
 9.7487475e-07 6.8851848e-05], sampled 0.21461700376820758
[2019-04-05 13:44:09,967] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6423.3284 149320562.4357 -1596.1451
[2019-04-05 13:44:11,512] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11618572]
[2019-04-05 13:44:11,512] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.0, 37.0, 214.0, 669.0, 19.0, 21.10975733826145, -0.5720410730839899, 0.0, 1.0, 0.0]
[2019-04-05 13:44:11,512] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:44:11,513] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.5729782e-07 7.5179315e-01 5.2474131e-04 1.4350638e-02 2.3332059e-01
 1.1974527e-07 1.0283768e-05], sampled 0.2626110141932181
[2019-04-05 13:44:22,316] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6105.5224 176427800.4018 -2151.9514
[2019-04-05 13:44:29,301] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6013.2534 190332569.7904 -2127.9412
[2019-04-05 13:44:30,326] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1280000, evaluation results [1280000.0, 6105.522439803645, 176427800.40181357, -2151.9514406488083, 6423.328385530262, 149320562.4357313, -1596.145091911567, 6013.253363884834, 190332569.79039434, -2127.941205911927]
[2019-04-05 13:44:40,679] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4049435e-04 5.5868703e-01 2.3318355e-03 1.2977889e-02 4.2508343e-01
 1.7174208e-06 1.7758591e-04], sum to 1.0000
[2019-04-05 13:44:40,680] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9871
[2019-04-05 13:44:40,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2311472e-05 3.6075932e-01 2.5646018e-03 1.3640503e-02 6.2293863e-01
 2.4795554e-06 8.2218328e-05], sum to 1.0000
[2019-04-05 13:44:40,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0251
[2019-04-05 13:44:40,699] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.8333333333333334, 67.83333333333334, 0.0, 0.0, 21.5, 21.24429995410096, -0.6297142504099911, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3471000.0000, 
sim time next is 3471600.0000, 
raw observation next is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 20.5, 21.15808517016621, -0.65026950669343, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4810710987996307, 0.6866666666666668, 0.0, 0.0, 0.20833333333333334, 0.26317376418051747, 0.28324349776885666, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.52157164], dtype=float32), -0.8613435]. 
=============================================
[2019-04-05 13:44:40,700] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.8, 84.33333333333333, 0.0, 0.0, 19.0, 18.82219259529529, -1.147302908719426, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1993200.0000, 
sim time next is 1993800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 19.5, 18.8078844827477, -1.140321709650561, 0.0, 1.0, 198937.349711862], 
processed observation next is [1.0, 0.043478260869565216, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.125, 0.0673237068956416, 0.11989276344981303, 0.0, 1.0, 0.9473207129136286], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.10846944], dtype=float32), -0.6400769]. 
=============================================
[2019-04-05 13:44:40,796] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1802239e-05 9.7326159e-01 2.8776044e-03 2.0066809e-03 2.0828219e-02
 6.0332593e-05 9.1365696e-04], sum to 1.0000
[2019-04-05 13:44:40,797] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1546
[2019-04-05 13:44:40,833] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 19.5, 21.2104138598134, -0.7507129125966602, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2283000.0000, 
sim time next is 2283600.0000, 
raw observation next is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 19.0, 21.23284295089229, -0.7438920053360082, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.29270544783010155, 0.7466666666666667, 0.5505555555555557, 0.05322283609576428, 0.08333333333333333, 0.26940357924102426, 0.2520359982213306, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48028404], dtype=float32), -0.1647071]. 
=============================================
[2019-04-05 13:44:43,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9979739e-06 9.0829277e-01 1.9166775e-03 1.5418549e-02 7.4358180e-02
 1.9239201e-07 7.6112369e-06], sum to 1.0000
[2019-04-05 13:44:43,856] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0867
[2019-04-05 13:44:43,863] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 77.66666666666667, 64.83333333333333, 0.0, 19.0, 19.46219747728518, -1.015348588663254, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1870800.0000, 
sim time next is 1871400.0000, 
raw observation next is [-4.5, 76.33333333333333, 57.66666666666667, 0.0, 19.0, 19.37818566950632, -1.034900614568186, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.7633333333333333, 0.19222222222222224, 0.0, 0.08333333333333333, 0.11484880579219325, 0.15503312847727133, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.722309], dtype=float32), 0.580106]. 
=============================================
[2019-04-05 13:44:46,293] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.6645663e-03 7.8824508e-01 5.8890115e-03 1.6624460e-02 1.8529052e-01
 2.3784114e-05 1.2625542e-03], sum to 1.0000
[2019-04-05 13:44:46,296] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5161
[2019-04-05 13:44:46,306] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.600000000000001, 83.0, 91.00000000000001, 0.0, 19.0, 19.84916567971877, -1.029832870200137, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2023800.0000, 
sim time next is 2024400.0000, 
raw observation next is [-5.6, 83.0, 96.5, 0.0, 19.0, 19.86133919571486, -1.033188820455629, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.32166666666666666, 0.0, 0.08333333333333333, 0.15511159964290488, 0.15560372651479035, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.74404055], dtype=float32), 1.5743897]. 
=============================================
[2019-04-05 13:44:49,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.7503566e-05 8.1084514e-01 1.2798271e-03 1.1275552e-03 1.8651181e-01
 4.0191817e-06 1.5417099e-04], sum to 1.0000
[2019-04-05 13:44:49,431] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6757
[2019-04-05 13:44:49,459] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 19.0, 18.64085546779438, -1.14487908571169, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1976400.0000, 
sim time next is 1977000.0000, 
raw observation next is [-5.7, 78.83333333333333, 0.0, 0.0, 19.0, 18.62472497286224, -1.121674236197428, 0.0, 1.0, 161668.6038319734], 
processed observation next is [1.0, 0.9130434782608695, 0.30470914127423826, 0.7883333333333333, 0.0, 0.0, 0.08333333333333333, 0.052060414405186606, 0.1261085879341907, 0.0, 1.0, 0.7698504944379685], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44228026], dtype=float32), 0.46623772]. 
=============================================
[2019-04-05 13:44:49,490] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.95694]
 [69.42523]
 [68.26813]
 [66.35838]
 [64.167  ]], R is [[73.5606308 ]
 [73.82502747]
 [74.08677673]
 [74.34590912]
 [74.60244751]].
[2019-04-05 13:44:49,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5291292e-06 8.6914545e-01 1.3899796e-04 1.1912488e-03 1.2951642e-01
 1.2497933e-06 4.1097965e-06], sum to 1.0000
[2019-04-05 13:44:49,517] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4290
[2019-04-05 13:44:49,531] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 98.83333333333334, 0.0, 0.0, 19.0, 19.45456693448205, -1.065386916169697, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2872200.0000, 
sim time next is 2872800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 19.35696792263199, -1.089441924732561, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.11308066021933261, 0.13685269175581297, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23114179], dtype=float32), 0.8495953]. 
=============================================
[2019-04-05 13:44:59,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0663234e-03 5.7847297e-01 1.8251931e-02 8.7160252e-02 3.0504483e-01
 7.4462718e-05 6.9291214e-03], sum to 1.0000
[2019-04-05 13:44:59,051] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3987
[2019-04-05 13:44:59,060] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.1, 51.5, 0.0, 0.0, 19.5, 19.28908770583811, -1.018426485830179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2310600.0000, 
sim time next is 2311200.0000, 
raw observation next is [-1.2, 52.0, 0.0, 0.0, 19.0, 19.44061481506394, -1.006358958129773, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.52, 0.0, 0.0, 0.08333333333333333, 0.1200512345886618, 0.16454701395674234, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.413363], dtype=float32), -1.9074976]. 
=============================================
[2019-04-05 13:45:03,840] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4246390e-06 9.3538713e-01 8.2784047e-04 3.1079939e-02 3.2372590e-02
 2.0492257e-06 3.2598234e-04], sum to 1.0000
[2019-04-05 13:45:03,844] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5206
[2019-04-05 13:45:03,890] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 100.0, 87.5, 0.0, 22.0, 23.10273336447968, -0.2509099445904952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2905200.0000, 
sim time next is 2905800.0000, 
raw observation next is [2.0, 100.0, 86.66666666666666, 0.0, 21.0, 23.32585269345888, -0.2370316307049126, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.28888888888888886, 0.0, 0.25, 0.44382105778824005, 0.42098945643169583, 1.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.2924979], dtype=float32), 0.43511865]. 
=============================================
[2019-04-05 13:45:05,821] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5653557e-04 4.9394310e-01 5.9103124e-02 2.5583928e-02 4.1769600e-01
 1.9360536e-06 2.7153646e-03], sum to 1.0000
[2019-04-05 13:45:05,825] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2909
[2019-04-05 13:45:05,839] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 20.0, 19.89153455514175, -0.9349717039043838, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2165400.0000, 
sim time next is 2166000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 19.0, 19.92654863999709, -0.9422841556665552, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.08333333333333333, 0.16054571999975753, 0.1859052814444816, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2708356], dtype=float32), 0.4837228]. 
=============================================
[2019-04-05 13:45:05,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[66.646774]
 [66.16981 ]
 [66.02523 ]
 [65.856125]
 [65.882095]], R is [[67.09151459]
 [67.27774048]
 [67.31925201]
 [67.21748352]
 [67.18817139]].
[2019-04-05 13:45:08,838] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-05 13:45:08,839] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:45:08,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:45:08,839] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:45:08,840] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:45:08,840] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:45:08,840] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:45:08,846] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run66
[2019-04-05 13:45:08,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run66
[2019-04-05 13:45:08,864] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run66
[2019-04-05 13:45:19,928] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11599112]
[2019-04-05 13:45:19,929] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-13.5, 78.0, 0.0, 0.0, 19.0, 19.35935376515724, -1.105571512851887, 1.0, 1.0, 0.0]
[2019-04-05 13:45:19,929] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:45:19,931] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.6871704e-05 5.9835064e-01 5.5514197e-03 2.5503563e-02 3.7003639e-01
 1.8863881e-05 4.7219271e-04], sampled 0.02862476837989314
[2019-04-05 13:45:43,262] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11599112]
[2019-04-05 13:45:43,262] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.0, 85.5, 0.0, 0.0, 19.0, 19.80823957979016, -0.7900811778318725, 0.0, 1.0, 0.0]
[2019-04-05 13:45:43,262] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:45:43,262] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.45721813e-07 6.71727657e-01 2.68632342e-04 1.27236005e-02
 3.15275311e-01 4.56396272e-08 4.55745203e-06], sampled 0.9617062128620106
[2019-04-05 13:46:07,233] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11599112]
[2019-04-05 13:46:07,233] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.629761504, 97.45380336666668, 88.20051058666668, 43.81547003333333, 19.5, 20.46572788884845, -0.8609436211990805, 1.0, 1.0, 0.0]
[2019-04-05 13:46:07,234] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:46:07,235] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.6572146e-04 7.3058790e-01 4.1425116e-03 1.4644700e-02 2.4971381e-01
 1.9350879e-05 7.2606019e-04], sampled 0.2002187986475391
[2019-04-05 13:46:14,233] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11599112]
[2019-04-05 13:46:14,233] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.773721275, 63.98223960333333, 0.0, 0.0, 19.5, 19.19400964232737, -0.9985119640911712, 0.0, 1.0, 196217.9094192961]
[2019-04-05 13:46:14,233] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:46:14,234] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1606945e-05 7.1749681e-01 3.3429284e-03 1.4048729e-02 2.6452765e-01
 1.1889855e-05 5.1041209e-04], sampled 0.7089867198512017
[2019-04-05 13:46:22,395] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6484.9578 146451646.8309 -1663.9665
[2019-04-05 13:46:33,596] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5896.1338 176436790.9773 -2249.2908
[2019-04-05 13:46:38,620] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5896.2457 189514134.5286 -2285.2961
[2019-04-05 13:46:39,662] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1300000, evaluation results [1300000.0, 5896.133750180287, 176436790.9773421, -2249.2908089386588, 6484.957809394546, 146451646.83091128, -1663.9664810733052, 5896.245681110015, 189514134.5286401, -2285.296082818417]
[2019-04-05 13:46:44,636] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.7780470e-08 8.5560131e-01 2.1374186e-05 4.2559849e-03 1.4011587e-01
 1.9483227e-08 5.3447388e-06], sum to 1.0000
[2019-04-05 13:46:44,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5932
[2019-04-05 13:46:44,651] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.3, 26.16666666666667, 57.33333333333333, 546.3333333333334, 19.0, 21.29255515347696, -0.6156107481602943, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2476200.0000, 
sim time next is 2476800.0000, 
raw observation next is [3.3, 26.0, 55.0, 479.5, 19.0, 21.28281763387724, -0.6217410756895431, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.26, 0.18333333333333332, 0.5298342541436464, 0.08333333333333333, 0.27356813615643666, 0.29275297477015233, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8380594], dtype=float32), -1.7610557]. 
=============================================
[2019-04-05 13:46:48,157] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6037563e-03 7.8065443e-01 3.3847868e-02 8.1231203e-03 1.6977596e-01
 3.6725344e-04 4.6276022e-03], sum to 1.0000
[2019-04-05 13:46:48,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6720
[2019-04-05 13:46:48,169] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.1, 39.0, 225.0, 46.5, 19.0, 21.41600752826451, -0.722048109918591, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2548800.0000, 
sim time next is 2549400.0000, 
raw observation next is [1.283333333333333, 37.5, 222.6666666666667, 38.33333333333333, 19.0, 21.4541123956214, -0.7169164691251811, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4981532779316713, 0.375, 0.7422222222222223, 0.04235727440147329, 0.08333333333333333, 0.2878426996351167, 0.26102784362493964, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08840635], dtype=float32), 1.6717073]. 
=============================================
[2019-04-05 13:46:51,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8555012e-05 1.2966973e-01 3.9181239e-03 8.3834119e-03 8.5697591e-01
 1.6654088e-06 1.0126999e-03], sum to 1.0000
[2019-04-05 13:46:51,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4834
[2019-04-05 13:46:51,457] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.5, 19.32138837955104, -0.9925103557251468, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3307200.0000, 
sim time next is 3307800.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 20.0, 19.3568369073183, -0.9879781015580046, 0.0, 1.0, 68597.40697371367], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.16666666666666666, 0.113069742276525, 0.1706739661473318, 0.0, 1.0, 0.32665431892244606], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.66574657], dtype=float32), 0.7576585]. 
=============================================
[2019-04-05 13:46:58,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.9152320e-05 9.2365593e-01 1.1560121e-03 2.1635296e-02 4.8454940e-02
 1.4133873e-05 5.0245626e-03], sum to 1.0000
[2019-04-05 13:46:58,737] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7933
[2019-04-05 13:46:58,749] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 19.0, 20.89875006299417, -0.6653660714685631, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4044600.0000, 
sim time next is 4045200.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 19.0, 20.79761766792284, -0.6813079301930879, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.08333333333333333, 0.23313480566023662, 0.272897356602304, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7134567], dtype=float32), 0.24775384]. 
=============================================
[2019-04-05 13:47:04,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0708882e-06 9.9815649e-01 2.7220097e-04 4.8890663e-04 1.0580509e-03
 6.5316662e-08 1.9266985e-05], sum to 1.0000
[2019-04-05 13:47:04,368] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0998
[2019-04-05 13:47:04,387] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 20.64455888501682, -0.6736423313742383, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3528000.0000, 
sim time next is 3528600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 20.51684861758395, -0.6940828062850103, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.20973738479866247, 0.26863906457166326, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10269585], dtype=float32), 0.7460337]. 
=============================================
[2019-04-05 13:47:07,652] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2816919e-08 8.8686013e-01 2.1902100e-05 6.5070181e-04 1.1246638e-01
 3.4789025e-08 7.3194809e-07], sum to 1.0000
[2019-04-05 13:47:07,654] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1484
[2019-04-05 13:47:07,697] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.666666666666667, 69.0, 170.5, 472.5, 19.0, 21.82005734848411, -0.4643006263487393, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4786800.0000, 
sim time next is 4787400.0000, 
raw observation next is [-3.333333333333333, 67.0, 167.0, 524.0, 19.0, 21.85945198477286, -0.4608551382532086, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37026777469990774, 0.67, 0.5566666666666666, 0.5790055248618785, 0.08333333333333333, 0.3216209987310717, 0.3463816205822638, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.69648737], dtype=float32), 1.3560445]. 
=============================================
[2019-04-05 13:47:07,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0816004e-09 9.9565130e-01 3.2212691e-05 2.6790945e-05 4.2896532e-03
 4.0538346e-09 2.3614497e-08], sum to 1.0000
[2019-04-05 13:47:07,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6710
[2019-04-05 13:47:07,936] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6666666666666667, 42.33333333333334, 88.66666666666667, 713.8333333333333, 19.0, 21.00990601586065, -0.5583022000535669, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3597600.0000, 
sim time next is 3598200.0000, 
raw observation next is [-0.5, 42.5, 86.0, 699.0, 19.0, 21.06838800299225, -0.5503096722657631, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.44875346260387816, 0.425, 0.2866666666666667, 0.7723756906077348, 0.08333333333333333, 0.2556990002493542, 0.31656344257807895, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6257727], dtype=float32), 0.40224418]. 
=============================================
[2019-04-05 13:47:09,353] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8748445e-06 9.8767871e-01 5.1467970e-04 4.5180172e-03 7.2762570e-03
 2.5788056e-08 9.4269799e-06], sum to 1.0000
[2019-04-05 13:47:09,354] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0309
[2019-04-05 13:47:09,364] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 19.0, 21.22424629002272, -0.6424481206399767, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3625200.0000, 
sim time next is 3625800.0000, 
raw observation next is [-1.0, 54.16666666666666, 0.0, 0.0, 19.0, 21.12430196429422, -0.6574407575210661, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.4349030470914128, 0.5416666666666665, 0.0, 0.0, 0.08333333333333333, 0.26035849702451824, 0.2808530808263113, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5622197], dtype=float32), 1.1760973]. 
=============================================
[2019-04-05 13:47:10,220] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2605233e-08 8.8523799e-01 7.5285984e-06 3.0785997e-03 1.1167487e-01
 5.5574692e-08 1.0476360e-06], sum to 1.0000
[2019-04-05 13:47:10,221] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9417
[2019-04-05 13:47:10,235] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6666666666666667, 41.0, 100.6666666666667, 780.1666666666667, 19.0, 22.09902397522577, -0.3862779471288573, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3075600.0000, 
sim time next is 3076200.0000, 
raw observation next is [-0.5, 40.5, 99.0, 775.0, 19.5, 22.08078908925344, -0.3890670472461788, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.44875346260387816, 0.405, 0.33, 0.856353591160221, 0.125, 0.3400657574377866, 0.37031098425127373, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.06431913], dtype=float32), 0.1653001]. 
=============================================
[2019-04-05 13:47:14,681] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-05 13:47:14,683] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:47:14,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:47:14,684] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:47:14,684] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:47:14,688] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:47:14,688] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:47:14,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run67
[2019-04-05 13:47:14,694] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run67
[2019-04-05 13:47:14,739] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run67
[2019-04-05 13:48:27,373] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6270.8287 148386116.9710 -1719.7401
[2019-04-05 13:48:31,758] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116980895]
[2019-04-05 13:48:31,758] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.485847131333333, 22.988152235, 143.9036965866667, 864.3243442666667, 19.0, 20.9855209228434, -0.6873718392325502, 1.0, 1.0, 0.0]
[2019-04-05 13:48:31,759] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:48:31,759] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.12641894e-04 8.05209696e-01 2.75693182e-03 1.64418928e-02
 1.74906254e-01 1.70335388e-05 5.55535546e-04], sampled 0.17330319519551052
[2019-04-05 13:48:39,486] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5916.3995 176307044.3430 -2262.1935
[2019-04-05 13:48:43,660] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5831.1077 190995891.7203 -2286.2361
[2019-04-05 13:48:44,686] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1320000, evaluation results [1320000.0, 5916.39948032761, 176307044.34299242, -2262.1934595671732, 6270.828698757757, 148386116.97096992, -1719.7400907613717, 5831.107662178789, 190995891.72027832, -2286.236120534516]
[2019-04-05 13:48:50,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2478366e-05 9.5643085e-01 2.1596658e-03 6.0594687e-03 3.5034031e-02
 7.5507483e-06 2.9604792e-04], sum to 1.0000
[2019-04-05 13:48:50,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6641
[2019-04-05 13:48:50,573] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.166666666666667, 38.0, 99.33333333333334, 766.6666666666666, 19.0, 22.59944809554932, -0.2783976834464105, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3941400.0000, 
sim time next is 3942000.0000, 
raw observation next is [-4.0, 38.0, 96.5, 756.0, 19.0, 22.41195742880616, -0.2826299836458266, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.38, 0.32166666666666666, 0.8353591160220994, 0.08333333333333333, 0.36766311906718013, 0.40579000545139116, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6250685], dtype=float32), 1.1782007]. 
=============================================
[2019-04-05 13:48:50,585] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[62.164738]
 [62.172123]
 [62.28128 ]
 [62.306725]
 [62.325375]], R is [[62.48467255]
 [62.85982513]
 [63.23122787]
 [63.5989151 ]
 [63.96292496]].
[2019-04-05 13:48:52,474] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:48:52,475] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:48:52,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run11
[2019-04-05 13:48:56,283] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3735039e-07 8.3505619e-01 3.4900216e-04 8.1794195e-02 8.2735710e-02
 2.9659776e-07 6.4305073e-05], sum to 1.0000
[2019-04-05 13:48:56,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5445
[2019-04-05 13:48:56,296] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 19.14381984494505, -0.9845154756098817, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3454800.0000, 
sim time next is 3455400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 19.17541528368748, -0.9906122230203742, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.09795127364062346, 0.16979592565987525, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04826146], dtype=float32), 2.0323563]. 
=============================================
[2019-04-05 13:48:56,862] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9957016e-05 1.7649841e-01 7.7637155e-03 4.7305278e-03 8.0854464e-01
 4.4851680e-05 2.3478791e-03], sum to 1.0000
[2019-04-05 13:48:56,865] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7459
[2019-04-05 13:48:56,870] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.3333333333333333, 78.0, 0.0, 0.0, 19.5, 20.44978761377294, -0.7567147257050587, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4731600.0000, 
sim time next is 4732200.0000, 
raw observation next is [-0.5, 78.0, 0.0, 0.0, 20.0, 20.31725315416343, -0.7658179776275347, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.44875346260387816, 0.78, 0.0, 0.0, 0.16666666666666666, 0.19310442951361928, 0.24472734079082178, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7974903], dtype=float32), -0.062134627]. 
=============================================
[2019-04-05 13:48:58,975] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1328612: loss 5.7610
[2019-04-05 13:48:58,977] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1328613: learning rate 0.0000
[2019-04-05 13:49:00,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3190007e-06 8.9008504e-01 1.3377307e-03 7.3695700e-03 9.9816598e-02
 1.2735713e-05 1.3699433e-03], sum to 1.0000
[2019-04-05 13:49:00,173] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3339
[2019-04-05 13:49:00,198] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.6666666666666667, 65.33333333333334, 0.0, 0.0, 19.0, 21.01662150129708, -0.5958970031383156, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3526800.0000, 
sim time next is 3527400.0000, 
raw observation next is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 19.0, 20.89297958481286, -0.6187339777221701, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4718374884579871, 0.6866666666666665, 0.0, 0.0, 0.08333333333333333, 0.24108163206773833, 0.2937553407592766, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1101756], dtype=float32), -0.48115203]. 
=============================================
[2019-04-05 13:49:03,345] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0259505e-06 8.3510590e-01 3.9140023e-03 6.7924713e-03 1.5399322e-01
 5.6152309e-08 1.9133472e-04], sum to 1.0000
[2019-04-05 13:49:03,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7582
[2019-04-05 13:49:03,355] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 19.0, 20.19120008635474, -0.7647848398266014, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4567800.0000, 
sim time next is 4568400.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 19.0, 20.12831781844389, -0.7715403368235614, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.57, 0.0, 0.0, 0.08333333333333333, 0.17735981820365745, 0.24281988772547955, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21602389], dtype=float32), -1.6856338]. 
=============================================
[2019-04-05 13:49:04,323] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1918391e-06 7.4854270e-02 7.4038940e-04 2.8763222e-02 8.9562231e-01
 6.2502085e-07 1.6021582e-05], sum to 1.0000
[2019-04-05 13:49:04,327] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6932
[2019-04-05 13:49:04,341] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 20.5, 20.30809417030613, -0.8293756016300353, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 4242600.0000, 
sim time next is 4243200.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 21.0, 20.29257190220765, -0.8215489918778193, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.25, 0.1910476585173043, 0.2261503360407269, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.43628305], dtype=float32), 1.6681828]. 
=============================================
[2019-04-05 13:49:07,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0572942e-10 9.9916041e-01 1.9442727e-05 1.6677863e-05 8.0345408e-04
 8.9790386e-10 1.1318109e-07], sum to 1.0000
[2019-04-05 13:49:07,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1556
[2019-04-05 13:49:07,181] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.15, 73.0, 0.0, 0.0, 19.0, 21.47471307518299, -0.5766426482149128, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4308600.0000, 
sim time next is 4309200.0000, 
raw observation next is [5.1, 73.0, 0.0, 0.0, 19.0, 21.4233938547853, -0.5881453890321825, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6038781163434903, 0.73, 0.0, 0.0, 0.08333333333333333, 0.2852828212321083, 0.3039515369892725, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54637665], dtype=float32), 2.4584327]. 
=============================================
[2019-04-05 13:49:08,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.898996e-06 8.383757e-01 8.484495e-02 3.498019e-02 4.177356e-02
 9.309862e-07 2.085696e-05], sum to 1.0000
[2019-04-05 13:49:08,560] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5377
[2019-04-05 13:49:08,575] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 23.5, 21.50788165200571, -0.5673056168379972, 0.0, 1.0, 50230.42811071339], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 3633000.0000, 
sim time next is 3633600.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 22.5, 21.64217344573887, -0.5483469940063115, 0.0, 1.0, 44612.28911694485], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.375, 0.30351445381157244, 0.31721766866456286, 0.0, 1.0, 0.21243947198545166], 
reward next is 0.5000, 
noisyNet noise sample is [array([2.1362596], dtype=float32), 0.7760577]. 
=============================================
[2019-04-05 13:49:13,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.0856014e-07 8.5834497e-01 2.0735392e-03 2.1828832e-03 1.3735077e-01
 2.6955354e-06 4.4195353e-05], sum to 1.0000
[2019-04-05 13:49:13,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6909
[2019-04-05 13:49:13,681] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 77.5, 0.0, 0.0, 19.0, 19.45319020546447, -0.9431010095242299, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4753800.0000, 
sim time next is 4754400.0000, 
raw observation next is [-4.0, 75.33333333333333, 0.0, 0.0, 19.0, 19.37492171459152, -0.9614042964190149, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.7533333333333333, 0.0, 0.0, 0.08333333333333333, 0.11457680954929324, 0.1795319011936617, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1294041], dtype=float32), -1.2782551]. 
=============================================
[2019-04-05 13:49:14,502] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:49:14,503] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:49:14,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run11
[2019-04-05 13:49:15,986] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1338727: loss 0.1173
[2019-04-05 13:49:15,987] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1338727: learning rate 0.0000
[2019-04-05 13:49:18,001] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-05 13:49:18,003] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:49:18,004] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:49:18,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:49:18,005] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:49:18,005] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:49:18,005] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:49:18,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run68
[2019-04-05 13:49:18,028] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run68
[2019-04-05 13:49:18,045] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run68
[2019-04-05 13:50:20,375] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11847443]
[2019-04-05 13:50:20,375] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.0, 52.83333333333334, 115.6666666666667, 817.3333333333334, 19.0, 20.55299170408446, -0.6927213118933899, 1.0, 1.0, 0.0]
[2019-04-05 13:50:20,375] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:50:20,376] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.6999711e-06 9.1430134e-01 6.0491852e-04 5.6424816e-03 7.9355955e-02
 1.5629033e-06 8.4046267e-05], sampled 0.18703024155836256
[2019-04-05 13:50:28,143] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6236.4532 141858749.7648 -1985.5297
[2019-04-05 13:50:40,400] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5767.1452 172131842.1034 -2542.3789
[2019-04-05 13:50:47,014] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5883.8473 185311705.9215 -2437.1443
[2019-04-05 13:50:48,039] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1340000, evaluation results [1340000.0, 5767.145182741196, 172131842.10342112, -2542.37893803666, 6236.453155688397, 141858749.7648468, -1985.5297211324928, 5883.847300115333, 185311705.92148602, -2437.144279148452]
[2019-04-05 13:50:49,091] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1340619: loss 0.7555
[2019-04-05 13:50:49,093] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1340619: learning rate 0.0000
[2019-04-05 13:50:50,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4603238e-06 8.9292598e-01 3.6734410e-04 3.8049497e-02 6.8366081e-02
 6.8637622e-05 2.2095196e-04], sum to 1.0000
[2019-04-05 13:50:50,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7548
[2019-04-05 13:50:50,314] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 19.5, 19.26977855950586, -1.029390250646813, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3911400.0000, 
sim time next is 3912000.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 19.26629830046721, -1.047752328113225, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.10552485837226744, 0.15074922396225832, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.342172], dtype=float32), 1.3301893]. 
=============================================
[2019-04-05 13:50:50,326] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.59845]
 [78.17168]
 [78.48052]
 [79.11863]
 [79.79499]], R is [[77.01946259]
 [77.17784119]
 [77.40606689]
 [77.63200378]
 [77.85568237]].
[2019-04-05 13:50:50,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.69412834e-05 8.17958593e-01 1.80238311e-03 1.54311702e-01
 2.54592095e-02 1.49078405e-05 3.66214575e-04], sum to 1.0000
[2019-04-05 13:50:50,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7769
[2019-04-05 13:50:50,584] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.5, 74.0, 0.0, 0.0, 19.0, 18.91136698396794, -1.078153269796312, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3803400.0000, 
sim time next is 3804000.0000, 
raw observation next is [-3.666666666666667, 75.0, 0.0, 0.0, 19.0, 18.92717428396838, -1.091441061083591, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3610341643582641, 0.75, 0.0, 0.0, 0.08333333333333333, 0.07726452366403169, 0.13618631297213635, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04055345], dtype=float32), -1.6426781]. 
=============================================
[2019-04-05 13:50:50,590] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.51263]
 [74.64071]
 [75.92393]
 [77.22149]
 [79.44574]], R is [[72.811409  ]
 [73.08329773]
 [73.35246277]
 [73.61894226]
 [73.88275146]].
[2019-04-05 13:50:51,540] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4110929e-04 9.7884345e-01 5.7765888e-04 7.6516094e-03 1.2226189e-02
 8.9064371e-05 2.7082153e-04], sum to 1.0000
[2019-04-05 13:50:51,546] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8501
[2019-04-05 13:50:51,557] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 49.0, 117.6666666666667, 798.3333333333334, 19.0, 21.54922914035193, -0.5623412384218094, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3928200.0000, 
sim time next is 3928800.0000, 
raw observation next is [-6.0, 49.0, 118.8333333333333, 804.1666666666666, 19.0, 21.61865211880142, -0.5489987609761707, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.296398891966759, 0.49, 0.396111111111111, 0.8885819521178637, 0.08333333333333333, 0.30155434323345176, 0.31700041300794307, 1.0, 1.0, 0.0], 
reward next is 0.5100, 
noisyNet noise sample is [array([0.0781245], dtype=float32), -0.712178]. 
=============================================
[2019-04-05 13:50:52,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3041219e-07 9.8985916e-01 2.0496805e-04 2.4910711e-03 7.4057453e-03
 6.2954979e-07 3.7838556e-05], sum to 1.0000
[2019-04-05 13:50:52,531] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9578
[2019-04-05 13:50:52,545] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.5, 49.5, 113.0, 824.0, 19.0, 20.97864253364421, -0.6124268024344497, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3850200.0000, 
sim time next is 3850800.0000, 
raw observation next is [1.666666666666667, 49.0, 111.8333333333333, 817.0, 19.0, 21.31917890842332, -0.585882089531272, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5087719298245615, 0.49, 0.37277777777777765, 0.9027624309392265, 0.08333333333333333, 0.27659824236861014, 0.3047059701562427, 1.0, 1.0, 0.0], 
reward next is 0.1412, 
noisyNet noise sample is [array([0.03769544], dtype=float32), -0.92757934]. 
=============================================
[2019-04-05 13:50:53,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0969290e-04 5.0443411e-01 5.8108894e-03 7.3186055e-02 4.1438058e-01
 4.1057276e-05 2.0376344e-03], sum to 1.0000
[2019-04-05 13:50:53,957] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9229
[2019-04-05 13:50:53,984] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.5, 39.5, 0.0, 0.0, 19.0, 19.0328371540565, -1.103652439843602, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4069800.0000, 
sim time next is 4070400.0000, 
raw observation next is [-5.333333333333333, 39.0, 0.0, 0.0, 19.5, 19.04580852999705, -1.095368049112882, 0.0, 1.0, 187515.5186007428], 
processed observation next is [1.0, 0.08695652173913043, 0.3148661126500462, 0.39, 0.0, 0.0, 0.125, 0.0871507108330875, 0.13487731696237262, 0.0, 1.0, 0.8929310409559181], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.30086672], dtype=float32), -0.7240046]. 
=============================================
[2019-04-05 13:50:57,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.99850963e-05 9.35111761e-01 2.10600486e-03 1.15485415e-02
 5.03331646e-02 2.68121275e-05 7.73701118e-04], sum to 1.0000
[2019-04-05 13:50:57,506] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2570
[2019-04-05 13:50:57,517] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 20.65269835255477, -0.7081397534787315, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3956400.0000, 
sim time next is 3957000.0000, 
raw observation next is [-6.166666666666666, 41.66666666666667, 0.0, 0.0, 19.0, 20.5507625697758, -0.7258668406747891, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.29178208679593726, 0.41666666666666674, 0.0, 0.0, 0.08333333333333333, 0.21256354748131656, 0.25804438644173694, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37060836], dtype=float32), -0.5875253]. 
=============================================
[2019-04-05 13:50:57,525] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[52.91665 ]
 [53.46188 ]
 [53.943333]
 [54.524147]
 [55.312084]], R is [[51.98061371]
 [51.4608078 ]
 [50.94620132]
 [50.43674088]
 [49.93237305]].
[2019-04-05 13:50:58,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:50:58,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:50:58,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run11
[2019-04-05 13:50:59,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:50:59,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:50:59,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run11
[2019-04-05 13:51:01,477] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0302035e-07 6.0780311e-01 6.6247128e-04 2.4886637e-03 3.8899952e-01
 1.0005565e-06 4.5063964e-05], sum to 1.0000
[2019-04-05 13:51:01,479] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6251
[2019-04-05 13:51:01,493] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.3, 80.0, 0.0, 0.0, 19.0, 18.66990430604002, -1.207026190542746, 0.0, 1.0, 54544.0017248506], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 258000.0000, 
sim time next is 258600.0000, 
raw observation next is [-4.399999999999999, 79.5, 0.0, 0.0, 19.5, 18.61245598599917, -1.199762949285281, 0.0, 1.0, 199344.3399514632], 
processed observation next is [1.0, 1.0, 0.3407202216066483, 0.795, 0.0, 0.0, 0.125, 0.0510379988332641, 0.10007901690490632, 0.0, 1.0, 0.9492587616736342], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.00083031], dtype=float32), -0.7909406]. 
=============================================
[2019-04-05 13:51:02,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2286500e-05 7.2405350e-01 1.5033758e-03 8.4925024e-03 2.6575747e-01
 2.7974188e-06 1.7803094e-04], sum to 1.0000
[2019-04-05 13:51:02,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5771
[2019-04-05 13:51:02,437] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 50.66666666666667, 0.0, 0.0, 20.0, 20.19863973840156, -0.8223445938187567, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4168200.0000, 
sim time next is 4168800.0000, 
raw observation next is [-4.0, 50.0, 0.0, 0.0, 19.0, 20.26788303239678, -0.8235074159131236, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.5, 0.0, 0.0, 0.08333333333333333, 0.1889902526997318, 0.2254975280289588, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3549687], dtype=float32), 0.67055416]. 
=============================================
[2019-04-05 13:51:03,604] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1348932: loss -1.5597
[2019-04-05 13:51:03,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1348932: learning rate 0.0000
[2019-04-05 13:51:04,921] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1349727: loss 1.2473
[2019-04-05 13:51:04,924] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1349727: learning rate 0.0000
[2019-04-05 13:51:05,454] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1350078: loss 0.5001
[2019-04-05 13:51:05,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1350079: learning rate 0.0000
[2019-04-05 13:51:05,844] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0756148e-04 6.0042983e-01 3.7967298e-02 7.0963375e-02 2.9012164e-01
 3.1288184e-04 9.7369899e-05], sum to 1.0000
[2019-04-05 13:51:05,845] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6149
[2019-04-05 13:51:05,860] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.333333333333333, 65.0, 65.33333333333334, 173.0, 19.0, 19.84513431796575, -0.8942779480478763, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5038800.0000, 
sim time next is 5039400.0000, 
raw observation next is [-2.166666666666667, 65.0, 71.66666666666666, 245.0, 19.0, 19.91713447206072, -0.88028342617313, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.65, 0.23888888888888885, 0.27071823204419887, 0.08333333333333333, 0.15976120600506002, 0.20657219127562332, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4437084], dtype=float32), -1.6074694]. 
=============================================
[2019-04-05 13:51:06,057] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1350465: loss 0.7417
[2019-04-05 13:51:06,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1350469: learning rate 0.0000
[2019-04-05 13:51:07,063] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.00949474e-04 6.47586763e-01 7.35979946e-03 1.63365528e-02
 3.25605035e-01 1.01903584e-04 2.90900748e-03], sum to 1.0000
[2019-04-05 13:51:07,063] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1888
[2019-04-05 13:51:07,077] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 18.23889873178166, -1.337501576182194, 0.0, 1.0, 63943.10885954009], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 355800.0000, 
sim time next is 356400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 18.21178380686832, -1.338664276795034, 0.0, 1.0, 56490.3615317874], 
processed observation next is [1.0, 0.13043478260869565, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.01764865057235987, 0.05377857440165531, 0.0, 1.0, 0.26900172157994], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46597648], dtype=float32), -0.58508056]. 
=============================================
[2019-04-05 13:51:09,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:51:09,246] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:09,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run11
[2019-04-05 13:51:09,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:51:09,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:09,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run11
[2019-04-05 13:51:10,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:51:10,344] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:10,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run11
[2019-04-05 13:51:11,886] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4553286e-06 2.0433873e-01 2.7345782e-03 1.1212834e-03 7.9178983e-01
 5.7318533e-07 1.3482979e-05], sum to 1.0000
[2019-04-05 13:51:11,889] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-05 13:51:11,901] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.7, 62.0, 0.0, 0.0, 19.0, 19.62608037616908, -0.9454323060364004, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4581000.0000, 
sim time next is 4581600.0000, 
raw observation next is [0.6000000000000001, 62.33333333333334, 0.0, 0.0, 19.0, 19.62834564530012, -0.9503260086758046, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.479224376731302, 0.6233333333333334, 0.0, 0.0, 0.08333333333333333, 0.13569547044167654, 0.1832246637747318, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0628623], dtype=float32), 1.4225113]. 
=============================================
[2019-04-05 13:51:12,141] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9667489e-06 4.0750569e-01 1.1190996e-03 1.1141302e-02 5.8022946e-01
 7.1107530e-07 1.7662755e-06], sum to 1.0000
[2019-04-05 13:51:12,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8108
[2019-04-05 13:51:12,190] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.5, 41.66666666666667, 0.0, 0.0, 21.0, 20.14753116378245, -0.8471670541670108, 0.0, 1.0, 152118.7252180025], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 4214400.0000, 
sim time next is 4215000.0000, 
raw observation next is [1.45, 41.83333333333333, 0.0, 0.0, 21.5, 20.09676273271735, -0.8060185843887476, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.782608695652174, 0.5027700831024932, 0.4183333333333333, 0.0, 0.0, 0.2916666666666667, 0.17473022772644597, 0.23132713853708411, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.7356924], dtype=float32), -0.3890165]. 
=============================================
[2019-04-05 13:51:12,200] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[89.4749  ]
 [89.20556 ]
 [90.02344 ]
 [90.71649 ]
 [91.464966]], R is [[89.5614624 ]
 [89.38013458]
 [89.27204895]
 [89.23646545]
 [89.27267456]].
[2019-04-05 13:51:12,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:51:12,947] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:12,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run11
[2019-04-05 13:51:15,840] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1355559: loss -1.4578
[2019-04-05 13:51:15,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1355559: learning rate 0.0000
[2019-04-05 13:51:16,152] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9306642e-05 9.2818534e-01 5.7816313e-04 1.2539246e-03 6.9396615e-02
 3.5679345e-06 5.5297866e-04], sum to 1.0000
[2019-04-05 13:51:16,155] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5522
[2019-04-05 13:51:16,167] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 56.0, 129.0, 767.0, 19.0, 21.53611824021367, -0.5775515279886897, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4617000.0000, 
sim time next is 4617600.0000, 
raw observation next is [1.333333333333333, 54.66666666666667, 127.8333333333333, 778.0, 19.0, 21.56873178521077, -0.6565530772633684, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4995383194829178, 0.5466666666666667, 0.426111111111111, 0.8596685082872928, 0.08333333333333333, 0.2973943154342307, 0.28114897424554386, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38431492], dtype=float32), -0.25964662]. 
=============================================
[2019-04-05 13:51:16,412] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1355883: loss 1.3618
[2019-04-05 13:51:16,416] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1355883: learning rate 0.0000
[2019-04-05 13:51:16,958] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1356219: loss -2.1084
[2019-04-05 13:51:16,961] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1356223: learning rate 0.0000
[2019-04-05 13:51:17,868] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7954018e-08 8.9467329e-01 4.8031041e-04 7.1187302e-02 3.3529345e-02
 4.8562900e-09 1.2968635e-04], sum to 1.0000
[2019-04-05 13:51:17,871] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6216
[2019-04-05 13:51:17,897] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.966666666666667, 86.33333333333334, 0.0, 0.0, 19.0, 19.61487253158299, -0.8942754361631174, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 74400.0000, 
sim time next is 75000.0000, 
raw observation next is [1.783333333333333, 85.66666666666667, 0.0, 0.0, 19.0, 19.62465126636365, -0.9001114975306578, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5120036934441367, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.13538760553030413, 0.19996283415644742, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7772117], dtype=float32), -0.76515347]. 
=============================================
[2019-04-05 13:51:17,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[103.02049 ]
 [103.14814 ]
 [103.624084]
 [103.5641  ]
 [104.06202 ]], R is [[102.65788269]
 [102.63130188]
 [102.6049881 ]
 [102.57894135]
 [102.55315399]].
[2019-04-05 13:51:19,464] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85000, global step 1357700: loss 0.1072
[2019-04-05 13:51:19,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85000, global step 1357701: learning rate 0.0000
[2019-04-05 13:51:19,594] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1357766: loss 0.4676
[2019-04-05 13:51:19,597] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1357767: learning rate 0.0000
[2019-04-05 13:51:20,980] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0558012e-04 6.0770756e-01 6.6353362e-03 6.8254426e-02 3.1224999e-01
 2.2006937e-04 4.1270824e-03], sum to 1.0000
[2019-04-05 13:51:20,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5890
[2019-04-05 13:51:21,006] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-9.5, 42.0, 76.0, 550.0, 19.0, 20.4145312021391, -0.832527818116156, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 313200.0000, 
sim time next is 313800.0000, 
raw observation next is [-9.5, 42.0, 74.0, 525.6666666666667, 19.0, 20.55357436027128, -0.8237443589204543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.24666666666666667, 0.5808471454880295, 0.08333333333333333, 0.21279786335594006, 0.22541854702651523, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4810131], dtype=float32), 0.16423269]. 
=============================================
[2019-04-05 13:51:21,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:51:21,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:21,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run11
[2019-04-05 13:51:21,788] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1358916: loss 12.4391
[2019-04-05 13:51:21,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1358918: learning rate 0.0000
[2019-04-05 13:51:22,184] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1359141: loss 2.1979
[2019-04-05 13:51:22,185] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1359141: learning rate 0.0000
[2019-04-05 13:51:22,601] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1359396: loss 6.0689
[2019-04-05 13:51:22,603] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1359397: learning rate 0.0000
[2019-04-05 13:51:23,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2183994e-07 3.5884222e-01 2.4544241e-04 2.1509260e-03 6.3875932e-01
 1.5248675e-06 4.1766273e-07], sum to 1.0000
[2019-04-05 13:51:23,073] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8074
[2019-04-05 13:51:23,085] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333333, 87.0, 0.0, 0.0, 21.0, 22.02950626528323, -0.4470851761845602, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 4679400.0000, 
sim time next is 4680000.0000, 
raw observation next is [0.0, 92.0, 0.0, 0.0, 21.5, 21.92445258608214, -0.4514882098195468, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.46260387811634357, 0.92, 0.0, 0.0, 0.2916666666666667, 0.32703771550684496, 0.34950393006015107, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.14049006], dtype=float32), 1.7994292]. 
=============================================
[2019-04-05 13:51:23,099] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.984505]
 [88.685524]
 [88.3512  ]
 [88.082146]
 [87.65118 ]], R is [[88.95137787]
 [88.77615356]
 [88.67410278]
 [88.64450073]
 [88.68663025]].
[2019-04-05 13:51:23,811] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-05 13:51:23,811] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:51:23,811] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:51:23,811] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:23,812] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:51:23,812] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:23,815] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:51:23,820] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run69
[2019-04-05 13:51:23,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run69
[2019-04-05 13:51:23,856] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run69
[2019-04-05 13:52:21,214] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11913488]
[2019-04-05 13:52:21,214] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.9406394, 41.71084159, 132.7227339, 743.7593506, 19.0, 20.28127194572474, -0.8884031637424324, 1.0, 1.0, 0.0]
[2019-04-05 13:52:21,214] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:52:21,215] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.5869011e-05 7.9824531e-01 2.3477825e-03 1.5733033e-02 1.8303993e-01
 1.4136758e-05 5.3395622e-04], sampled 0.07134343600781079
[2019-04-05 13:52:36,798] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6279.1454 145879297.6813 -1756.3672
[2019-04-05 13:52:49,126] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6001.5725 174900144.4731 -2257.7306
[2019-04-05 13:52:54,086] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6059.7804 188797731.6984 -2162.3068
[2019-04-05 13:52:55,113] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1360000, evaluation results [1360000.0, 6001.572521935715, 174900144.47313303, -2257.730556398536, 6279.145445126259, 145879297.68131807, -1756.3672037440829, 6059.780421944133, 188797731.69841176, -2162.306784824776]
[2019-04-05 13:52:55,916] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0272254e-04 8.9344370e-01 5.1253359e-03 3.8097784e-02 5.9056289e-02
 4.2438605e-06 3.8700041e-03], sum to 1.0000
[2019-04-05 13:52:55,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3034
[2019-04-05 13:52:55,944] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.5, 20.58176563233385, -0.7163044120583716, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4558200.0000, 
sim time next is 4558800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 20.58414147084515, -0.7223639028959113, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.21534512257042918, 0.25921203236802953, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2877283], dtype=float32), 0.90423924]. 
=============================================
[2019-04-05 13:52:56,064] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1360523: loss -1.0511
[2019-04-05 13:52:56,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1360524: learning rate 0.0000
[2019-04-05 13:52:56,943] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3861055e-06 4.5830199e-01 1.4969014e-04 1.1468158e-02 5.3002822e-01
 3.0335573e-06 4.4505079e-05], sum to 1.0000
[2019-04-05 13:52:56,943] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6321
[2019-04-05 13:52:56,966] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 20.5, 19.24699466309674, -1.073039011587935, 0.0, 1.0, 48940.56355640705], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 191400.0000, 
sim time next is 192000.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 21.0, 19.32189294507637, -1.066998194212566, 0.0, 1.0, 48833.18686681319], 
processed observation next is [1.0, 0.21739130434782608, 0.2160664819944598, 0.78, 0.0, 0.0, 0.25, 0.11015774542303092, 0.14433393526247804, 0.0, 1.0, 0.2325389850800628], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.02964864], dtype=float32), 0.11424801]. 
=============================================
[2019-04-05 13:52:56,983] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.8485 ]
 [71.49125]
 [71.57025]
 [71.52671]
 [71.36506]], R is [[71.73723602]
 [71.80558014]
 [71.944664  ]
 [71.93950653]
 [72.00582886]].
[2019-04-05 13:53:00,857] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1363015: loss 2.0048
[2019-04-05 13:53:00,857] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1363015: learning rate 0.0000
[2019-04-05 13:53:03,215] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1364294: loss 3.7269
[2019-04-05 13:53:03,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1364298: learning rate 0.0000
[2019-04-05 13:53:04,324] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1364861: loss -2.1208
[2019-04-05 13:53:04,324] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1364861: learning rate 0.0000
[2019-04-05 13:53:05,332] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1365427: loss -0.4691
[2019-04-05 13:53:05,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1365429: learning rate 0.0000
[2019-04-05 13:53:06,998] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:53:06,998] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:07,002] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run11
[2019-04-05 13:53:07,566] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85500, global step 1366571: loss 10.4742
[2019-04-05 13:53:07,567] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85500, global step 1366571: learning rate 0.0000
[2019-04-05 13:53:08,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6108569e-09 9.9947745e-01 2.1542259e-05 2.9810096e-04 2.0182952e-04
 1.9277273e-07 8.8958870e-07], sum to 1.0000
[2019-04-05 13:53:08,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6426
[2019-04-05 13:53:08,048] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 19.0, 26.24904579384734, 0.5123946139606969, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5077200.0000, 
sim time next is 5077800.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 19.0, 26.10249986098091, 0.4948248838446644, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.08333333333333333, 0.6752083217484092, 0.6649416279482215, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5388396], dtype=float32), -1.046896]. 
=============================================
[2019-04-05 13:53:08,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:53:08,952] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:08,972] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run11
[2019-04-05 13:53:09,460] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:53:09,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:09,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run11
[2019-04-05 13:53:09,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:53:09,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:09,691] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run11
[2019-04-05 13:53:10,278] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1367775: loss 2.2715
[2019-04-05 13:53:10,279] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1367776: learning rate 0.0000
[2019-04-05 13:53:11,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0352379e-05 5.0909668e-01 7.8851636e-04 6.5423034e-02 4.2466539e-01
 2.3502153e-07 1.5775875e-05], sum to 1.0000
[2019-04-05 13:53:11,058] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6637
[2019-04-05 13:53:11,078] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 45.0, 90.5, 295.3333333333334, 19.5, 21.79456641641004, -0.4768932963681592, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4898400.0000, 
sim time next is 4899000.0000, 
raw observation next is [3.0, 45.0, 79.00000000000001, 273.6666666666667, 20.0, 21.77131459201101, -0.4872345041228201, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.26333333333333336, 0.3023941068139963, 0.16666666666666666, 0.31427621600091743, 0.33758849862572665, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.0264964], dtype=float32), 0.41750178]. 
=============================================
[2019-04-05 13:53:11,098] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[88.70566 ]
 [88.593544]
 [88.600685]
 [88.50366 ]
 [88.45539 ]], R is [[88.59723663]
 [88.63983917]
 [88.75344086]
 [88.72304535]
 [88.76438904]].
[2019-04-05 13:53:11,288] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1368173: loss 0.5854
[2019-04-05 13:53:11,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1368173: learning rate 0.0000
[2019-04-05 13:53:11,379] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1368211: loss 1.1412
[2019-04-05 13:53:11,380] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1368212: learning rate 0.0000
[2019-04-05 13:53:11,604] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1368294: loss -1.0271
[2019-04-05 13:53:11,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1368294: learning rate 0.0000
[2019-04-05 13:53:13,203] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1368924: loss 1.5289
[2019-04-05 13:53:13,205] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1368925: learning rate 0.0000
[2019-04-05 13:53:13,715] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1369155: loss 8.7071
[2019-04-05 13:53:13,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1369155: learning rate 0.0000
[2019-04-05 13:53:14,010] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.04735715e-04 8.91570151e-01 5.74657740e-03 6.24114973e-03
 9.58561078e-02 4.17203273e-06 4.77014692e-04], sum to 1.0000
[2019-04-05 13:53:14,010] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4867
[2019-04-05 13:53:14,016] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.333333333333333, 40.0, 0.0, 0.0, 19.0, 20.39936592461117, -0.7626947435310956, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5013600.0000, 
sim time next is 5014200.0000, 
raw observation next is [1.166666666666667, 40.0, 0.0, 0.0, 19.0, 20.3707389219512, -0.7678392299827359, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.49492151431209613, 0.4, 0.0, 0.0, 0.08333333333333333, 0.19756157682926676, 0.2440535900057547, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05112167], dtype=float32), -0.56496996]. 
=============================================
[2019-04-05 13:53:14,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:53:14,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:14,503] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run11
[2019-04-05 13:53:15,150] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.66167876e-07 1.08638816e-01 6.01515057e-05 5.50412666e-03
 8.85789573e-01 3.31506591e-08 6.81321490e-06], sum to 1.0000
[2019-04-05 13:53:15,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8727
[2019-04-05 13:53:15,169] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 21.5, 19.84102619697077, -0.9704531668219941, 0.0, 1.0, 59798.44245218131], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 525000.0000, 
sim time next is 525600.0000, 
raw observation next is [4.3, 88.0, 0.0, 0.0, 22.0, 19.88866442316447, -0.9592912193136037, 0.0, 1.0, 49465.07938276082], 
processed observation next is [0.0, 0.08695652173913043, 0.5817174515235458, 0.88, 0.0, 0.0, 0.3333333333333333, 0.15738870193037258, 0.18023626022879877, 0.0, 1.0, 0.23554799706076582], 
reward next is 0.5714, 
noisyNet noise sample is [array([-0.8585113], dtype=float32), 0.50017434]. 
=============================================
[2019-04-05 13:53:16,392] A3C_AGENT_WORKER-Thread-9 INFO:Local step 85000, global step 1370385: loss -0.1946
[2019-04-05 13:53:16,393] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 85000, global step 1370385: learning rate 0.0000
[2019-04-05 13:53:16,434] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1370406: loss 0.9269
[2019-04-05 13:53:16,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1370406: learning rate 0.0000
[2019-04-05 13:53:17,068] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85000, global step 1370735: loss -0.3688
[2019-04-05 13:53:17,070] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85000, global step 1370735: learning rate 0.0000
[2019-04-05 13:53:17,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.25344974e-04 8.84586513e-01 1.41138886e-03 2.22545932e-03
 1.11315586e-01 4.36287792e-06 3.31374176e-04], sum to 1.0000
[2019-04-05 13:53:17,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6098
[2019-04-05 13:53:17,817] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.3, 46.0, 85.0, 31.0, 19.0, 20.51069323634937, -0.8796933216093022, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 747000.0000, 
sim time next is 747600.0000, 
raw observation next is [-0.4, 45.66666666666667, 82.16666666666667, 26.33333333333334, 19.0, 20.55658745531436, -0.9609041101348309, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.45152354570637127, 0.4566666666666667, 0.2738888888888889, 0.02909760589318601, 0.08333333333333333, 0.21304895460953008, 0.17969862995505637, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5649687], dtype=float32), 0.6699663]. 
=============================================
[2019-04-05 13:53:18,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:53:18,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:18,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run11
[2019-04-05 13:53:18,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.06705090e-06 4.44287509e-02 1.19231154e-04 2.26515858e-03
 9.53180075e-01 2.38820974e-07 4.49956497e-06], sum to 1.0000
[2019-04-05 13:53:18,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6711
[2019-04-05 13:53:18,317] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.9, 86.0, 85.66666666666666, 0.0, 21.5, 20.60304358656165, -0.7070345197158572, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 49200.0000, 
sim time next is 49800.0000, 
raw observation next is [7.8, 86.0, 84.33333333333334, 0.0, 22.0, 20.63617500524121, -0.7045232810037793, 0.0, 1.0, 18712.95043856213], 
processed observation next is [0.0, 0.5652173913043478, 0.6786703601108034, 0.86, 0.28111111111111114, 0.0, 0.3333333333333333, 0.21968125043676748, 0.26515890633207356, 0.0, 1.0, 0.08910928780267681], 
reward next is 0.5714, 
noisyNet noise sample is [array([-1.4560875], dtype=float32), 0.9010881]. 
=============================================
[2019-04-05 13:53:19,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 13:53:19,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:19,948] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run11
[2019-04-05 13:53:20,035] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1372225: loss -1.6635
[2019-04-05 13:53:20,038] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1372226: learning rate 0.0000
[2019-04-05 13:53:20,170] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1372274: loss -0.0800
[2019-04-05 13:53:20,173] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1372274: learning rate 0.0000
[2019-04-05 13:53:21,132] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85000, global step 1372746: loss -2.3593
[2019-04-05 13:53:21,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85000, global step 1372746: learning rate 0.0000
[2019-04-05 13:53:21,942] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1373067: loss -0.0013
[2019-04-05 13:53:21,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1373067: learning rate 0.0000
[2019-04-05 13:53:23,092] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1373531: loss 0.4527
[2019-04-05 13:53:23,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1373531: learning rate 0.0000
[2019-04-05 13:53:23,681] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88000, global step 1373763: loss 2.8156
[2019-04-05 13:53:23,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88000, global step 1373763: learning rate 0.0000
[2019-04-05 13:53:24,692] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86000, global step 1374181: loss -0.9784
[2019-04-05 13:53:24,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86000, global step 1374183: learning rate 0.0000
[2019-04-05 13:53:25,016] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1374362: loss -0.4900
[2019-04-05 13:53:25,017] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1374362: learning rate 0.0000
[2019-04-05 13:53:26,803] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1375236: loss -0.7702
[2019-04-05 13:53:26,804] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1375236: learning rate 0.0000
[2019-04-05 13:53:27,007] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1375347: loss -3.0971
[2019-04-05 13:53:27,008] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1375348: learning rate 0.0000
[2019-04-05 13:53:27,455] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1375606: loss 1.1264
[2019-04-05 13:53:27,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1375606: learning rate 0.0000
[2019-04-05 13:53:27,529] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5430065e-07 4.5227497e-03 3.4276149e-04 2.5180011e-04 9.9487841e-01
 4.3898176e-08 3.7405748e-06], sum to 1.0000
[2019-04-05 13:53:27,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9139
[2019-04-05 13:53:27,581] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.25, 87.5, 0.0, 0.0, 22.0, 19.70850071339202, -0.774806805028143, 0.0, 1.0, 188832.4124328483], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 70200.0000, 
sim time next is 70800.0000, 
raw observation next is [3.066666666666667, 88.0, 0.0, 0.0, 22.5, 20.00011578860676, -0.718350573591136, 0.0, 1.0, 113842.4040238732], 
processed observation next is [0.0, 0.8260869565217391, 0.5475530932594646, 0.88, 0.0, 0.0, 0.375, 0.16667631571722996, 0.2605498088029547, 0.0, 1.0, 0.5421066858279676], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.27290636], dtype=float32), -0.36381504]. 
=============================================
[2019-04-05 13:53:29,872] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1376993: loss 8.3264
[2019-04-05 13:53:29,873] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1376993: learning rate 0.0000
[2019-04-05 13:53:30,548] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1377343: loss -1.4831
[2019-04-05 13:53:30,548] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1377343: learning rate 0.0000
[2019-04-05 13:53:31,550] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1377890: loss 0.0133
[2019-04-05 13:53:31,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1377890: learning rate 0.0000
[2019-04-05 13:53:31,974] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1378096: loss -0.9098
[2019-04-05 13:53:31,975] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1378096: learning rate 0.0000
[2019-04-05 13:53:32,797] A3C_AGENT_WORKER-Thread-9 INFO:Local step 85500, global step 1378482: loss -1.5229
[2019-04-05 13:53:32,799] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 85500, global step 1378485: learning rate 0.0000
[2019-04-05 13:53:32,984] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6467982e-04 2.7123982e-01 3.9841314e-03 6.3656069e-02 6.5547740e-01
 3.9360460e-04 4.7842916e-03], sum to 1.0000
[2019-04-05 13:53:32,985] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8699
[2019-04-05 13:53:33,029] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 22.0, 21.61566850058331, -0.6700324581235084, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 809400.0000, 
sim time next is 810000.0000, 
raw observation next is [-6.2, 75.0, 46.5, 0.0, 22.5, 21.7193351370134, -0.6621214039153075, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2908587257617729, 0.75, 0.155, 0.0, 0.375, 0.3099445947511166, 0.27929286536156417, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.224926], dtype=float32), 0.117105335]. 
=============================================
[2019-04-05 13:53:33,039] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[43.361263]
 [43.71842 ]
 [44.866306]
 [45.61076 ]
 [46.091854]], R is [[42.71522522]
 [42.28807449]
 [41.86519241]
 [41.44654083]
 [41.03207397]].
[2019-04-05 13:53:33,857] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85500, global step 1379058: loss 10.4256
[2019-04-05 13:53:33,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85500, global step 1379058: learning rate 0.0000
[2019-04-05 13:53:34,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3554775e-03 7.7052176e-01 1.5269715e-02 2.1000704e-02 1.9134550e-01
 6.1859922e-05 4.4502926e-04], sum to 1.0000
[2019-04-05 13:53:34,486] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5840
[2019-04-05 13:53:34,499] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-14.83333333333333, 69.0, 0.0, 0.0, 25.0, 19.16848463694942, -1.134513796442226, 0.0, 1.0, 51614.71763538971], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 351600.0000, 
sim time next is 352200.0000, 
raw observation next is [-14.91666666666667, 69.0, 0.0, 0.0, 24.0, 19.21960246240315, -1.132019380801454, 0.0, 1.0, 51699.59431608539], 
processed observation next is [1.0, 0.043478260869565216, 0.04939981532779307, 0.69, 0.0, 0.0, 0.5, 0.10163353853359573, 0.12266020639951532, 0.0, 1.0, 0.24618854436231138], 
reward next is 0.2857, 
noisyNet noise sample is [array([0.42571843], dtype=float32), -0.40294188]. 
=============================================
[2019-04-05 13:53:35,452] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-05 13:53:35,453] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:53:35,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:35,454] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:53:35,454] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:35,455] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:53:35,456] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:53:35,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run70
[2019-04-05 13:53:35,478] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run70
[2019-04-05 13:53:35,504] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run70
[2019-04-05 13:53:50,978] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116939805]
[2019-04-05 13:53:50,978] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.821051763666667, 37.47816625999999, 0.0, 0.0, 20.0, 20.29932000272504, -1.018905258084095, 1.0, 1.0, 0.0]
[2019-04-05 13:53:50,979] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:53:50,979] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.6737138e-03 6.8300515e-01 1.0997869e-02 2.7751364e-02 2.7120385e-01
 2.8389652e-04 5.0841942e-03], sampled 0.9189829693873265
[2019-04-05 13:54:50,018] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6259.5101 149461822.1615 -1602.6074
[2019-04-05 13:54:51,001] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116939805]
[2019-04-05 13:54:51,001] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.753586756, 45.9972331, 42.84534753333333, 830.82479715, 20.0, 21.91452941630417, -0.4509608326277826, 1.0, 1.0, 0.0]
[2019-04-05 13:54:51,001] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:54:51,002] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1639395e-05 7.8502780e-01 1.6142570e-03 1.0103554e-02 2.0265643e-01
 8.8439601e-06 5.2743335e-04], sampled 0.803740835187467
[2019-04-05 13:55:02,183] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6037.8833 179427955.6195 -2002.5382
[2019-04-05 13:55:06,970] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116939805]
[2019-04-05 13:55:06,970] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [10.11577576, 30.42597095, 124.639770556, 604.92838665, 19.0, 23.49039325402816, -0.1484343860510324, 1.0, 1.0, 0.0]
[2019-04-05 13:55:06,970] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:55:06,971] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.3968968e-05 8.2596779e-01 1.1896119e-03 8.8953953e-03 1.6350584e-01
 4.9978530e-06 3.9253169e-04], sampled 0.8020004683597072
[2019-04-05 13:55:08,183] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5886.8442 193101667.5444 -2063.2994
[2019-04-05 13:55:09,208] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1380000, evaluation results [1380000.0, 6037.883327922534, 179427955.61954063, -2002.538223410032, 6259.510126736345, 149461822.16153243, -1602.6073588724876, 5886.844225847203, 193101667.54437864, -2063.2993644126345]
[2019-04-05 13:55:09,435] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1380109: loss -0.1785
[2019-04-05 13:55:09,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1380109: learning rate 0.0000
[2019-04-05 13:55:10,335] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1380544: loss 3.4434
[2019-04-05 13:55:10,341] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1380544: learning rate 0.0000
[2019-04-05 13:55:10,875] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1380845: loss 1.3119
[2019-04-05 13:55:10,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1380845: learning rate 0.0000
[2019-04-05 13:55:12,178] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.01428927 0.4753924  0.01693178 0.09248113 0.39403242 0.0014957
 0.00537729], sum to 1.0000
[2019-04-05 13:55:12,179] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-05 13:55:12,272] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1381622: loss 0.2761
[2019-04-05 13:55:12,275] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-11.51666666666667, 55.66666666666666, 0.0, 0.0, 22.0, 19.51211045187186, -0.9455862002824665, 1.0, 1.0, 197444.6704383187], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 323400.0000, 
sim time next is 324000.0000, 
raw observation next is [-11.7, 57.0, 0.0, 0.0, 22.5, 19.76698687217792, -0.874964734857408, 1.0, 1.0, 199033.5842260321], 
processed observation next is [1.0, 0.782608695652174, 0.13850415512465375, 0.57, 0.0, 0.0, 0.375, 0.14724890601482668, 0.208345088380864, 1.0, 1.0, 0.9477789725049148], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.19777541], dtype=float32), -0.6887661]. 
=============================================
[2019-04-05 13:55:12,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1381622: learning rate 0.0000
[2019-04-05 13:55:12,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[41.393764]
 [40.854153]
 [40.69219 ]
 [40.51961 ]
 [40.172962]], R is [[41.38525772]
 [40.97140503]
 [40.56169128]
 [40.15607452]
 [39.75451279]].
[2019-04-05 13:55:12,287] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85500, global step 1381631: loss -0.7105
[2019-04-05 13:55:12,288] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85500, global step 1381631: learning rate 0.0000
[2019-04-05 13:55:13,215] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1382115: loss -0.6418
[2019-04-05 13:55:13,218] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1382116: learning rate 0.0000
[2019-04-05 13:55:13,309] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86500, global step 1382167: loss -2.5678
[2019-04-05 13:55:13,310] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86500, global step 1382167: learning rate 0.0000
[2019-04-05 13:55:14,992] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88500, global step 1383035: loss 0.6600
[2019-04-05 13:55:14,995] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88500, global step 1383036: learning rate 0.0000
[2019-04-05 13:55:15,008] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1383043: loss -15.5415
[2019-04-05 13:55:15,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1383043: learning rate 0.0000
[2019-04-05 13:55:16,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3625644e-03 4.0957096e-01 4.8706565e-02 2.1068895e-02 5.1479554e-01
 1.3008605e-04 2.3653000e-03], sum to 1.0000
[2019-04-05 13:55:16,107] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0325
[2019-04-05 13:55:16,149] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.6, 54.5, 106.0, 658.0, 19.0, 21.1877150584276, -0.7323653755640821, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 300600.0000, 
sim time next is 301200.0000, 
raw observation next is [-10.6, 52.66666666666667, 102.1666666666667, 674.6666666666666, 19.0, 21.14878851086734, -0.7397951228153352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5266666666666667, 0.34055555555555567, 0.74548802946593, 0.08333333333333333, 0.2623990425722784, 0.2534016257282216, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2431104], dtype=float32), 1.0741359]. 
=============================================
[2019-04-05 13:55:17,125] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1384150: loss -3.9000
[2019-04-05 13:55:17,128] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1384151: learning rate 0.0000
[2019-04-05 13:55:18,771] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88000, global step 1385000: loss 3.6561
[2019-04-05 13:55:18,772] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88000, global step 1385000: learning rate 0.0000
[2019-04-05 13:55:20,195] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1385799: loss 2.6996
[2019-04-05 13:55:20,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1385801: learning rate 0.0000
[2019-04-05 13:55:21,061] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1386228: loss 3.9016
[2019-04-05 13:55:21,063] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1386230: learning rate 0.0000
[2019-04-05 13:55:22,728] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1387230: loss 1.9446
[2019-04-05 13:55:22,729] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1387232: learning rate 0.0000
[2019-04-05 13:55:22,917] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1387345: loss -2.4143
[2019-04-05 13:55:22,918] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1387346: learning rate 0.0000
[2019-04-05 13:55:22,968] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1387380: loss 1.2012
[2019-04-05 13:55:22,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1387380: learning rate 0.0000
[2019-04-05 13:55:23,278] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1387557: loss 0.2520
[2019-04-05 13:55:23,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1387558: learning rate 0.0000
[2019-04-05 13:55:23,770] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1387823: loss -2.2190
[2019-04-05 13:55:23,773] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1387824: learning rate 0.0000
[2019-04-05 13:55:23,950] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87000, global step 1387922: loss 4.9218
[2019-04-05 13:55:23,957] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87000, global step 1387924: learning rate 0.0000
[2019-04-05 13:55:24,285] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1388117: loss 2.8082
[2019-04-05 13:55:24,287] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1388118: learning rate 0.0000
[2019-04-05 13:55:24,836] A3C_AGENT_WORKER-Thread-9 INFO:Local step 86000, global step 1388448: loss -0.7801
[2019-04-05 13:55:24,838] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 86000, global step 1388448: learning rate 0.0000
[2019-04-05 13:55:26,521] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86000, global step 1389421: loss -2.8615
[2019-04-05 13:55:26,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86000, global step 1389422: learning rate 0.0000
[2019-04-05 13:55:26,757] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4310662e-08 2.7015731e-01 1.7234357e-03 3.1292713e-03 7.2498906e-01
 1.8502542e-08 8.0044498e-07], sum to 1.0000
[2019-04-05 13:55:26,758] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1264
[2019-04-05 13:55:26,774] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 25.5, 23.15380072626096, -0.0841299009451028, 0.0, 1.0, 39128.03671673047], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1057200.0000, 
sim time next is 1057800.0000, 
raw observation next is [13.38333333333333, 79.66666666666667, 0.0, 0.0, 24.5, 23.22023193232598, -0.07367686714530149, 0.0, 1.0, 38812.41060505374], 
processed observation next is [1.0, 0.21739130434782608, 0.8333333333333334, 0.7966666666666667, 0.0, 0.0, 0.5416666666666666, 0.43501932769383167, 0.4754410442848995, 0.0, 1.0, 0.18482100288120829], 
reward next is 0.2143, 
noisyNet noise sample is [array([-2.3552718], dtype=float32), 0.7849448]. 
=============================================
[2019-04-05 13:55:28,167] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.8442580e-07 8.8905406e-01 7.3639717e-04 1.9966360e-02 9.0229571e-02
 1.1604382e-05 1.3714539e-06], sum to 1.0000
[2019-04-05 13:55:28,170] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4895
[2019-04-05 13:55:28,180] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.433333333333334, 92.0, 0.0, 0.0, 19.5, 22.22519483041387, -0.2313166726523742, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1318800.0000, 
sim time next is 1319400.0000, 
raw observation next is [1.35, 92.0, 0.0, 0.0, 19.0, 22.10250018517867, -0.2473620258198657, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5000000000000001, 0.92, 0.0, 0.0, 0.08333333333333333, 0.3418750154315558, 0.4175459913933781, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5807079], dtype=float32), -1.0053302]. 
=============================================
[2019-04-05 13:55:30,567] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86000, global step 1391765: loss -2.4772
[2019-04-05 13:55:30,571] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86000, global step 1391766: learning rate 0.0000
[2019-04-05 13:55:31,653] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89000, global step 1392295: loss 43.6945
[2019-04-05 13:55:31,654] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89000, global step 1392295: learning rate 0.0000
[2019-04-05 13:55:31,668] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1392299: loss 5.8047
[2019-04-05 13:55:31,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1392299: learning rate 0.0000
[2019-04-05 13:55:33,168] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1393047: loss 0.1883
[2019-04-05 13:55:33,175] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1393048: learning rate 0.0000
[2019-04-05 13:55:33,527] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1393236: loss -2.5948
[2019-04-05 13:55:33,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1393236: learning rate 0.0000
[2019-04-05 13:55:33,797] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9490572e-04 5.5525649e-01 1.1388451e-03 2.5567288e-02 3.9920843e-01
 8.2932502e-07 1.8633232e-02], sum to 1.0000
[2019-04-05 13:55:33,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0393
[2019-04-05 13:55:33,850] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.050000000000001, 41.83333333333334, 71.0, 739.6666666666667, 22.0, 21.22837161162212, -0.6979214477405923, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2455800.0000, 
sim time next is 2456400.0000, 
raw observation next is [-4.5, 40.66666666666667, 73.5, 758.3333333333333, 21.0, 21.1871676040203, -0.7007853910740397, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3379501385041552, 0.40666666666666673, 0.245, 0.8379373848987108, 0.25, 0.26559730033502493, 0.2664048696419868, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.94254], dtype=float32), -0.5254754]. 
=============================================
[2019-04-05 13:55:34,304] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1393599: loss 1.4445
[2019-04-05 13:55:34,327] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1393600: learning rate 0.0000
[2019-04-05 13:55:34,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5662266e-05 7.3047477e-01 1.0446557e-02 2.5254742e-03 2.5644153e-01
 1.0610191e-05 8.5336091e-05], sum to 1.0000
[2019-04-05 13:55:34,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9290
[2019-04-05 13:55:34,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 31.0, 88.0, 837.0, 19.0, 20.93060951479706, -0.7209238518320279, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2462400.0000, 
sim time next is 2463000.0000, 
raw observation next is [-0.2333333333333333, 30.5, 88.66666666666667, 839.6666666666666, 19.5, 20.88535994554507, -0.7276101697282567, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.456140350877193, 0.305, 0.29555555555555557, 0.9278084714548802, 0.125, 0.24044666212875576, 0.2574632767572478, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.14778228], dtype=float32), 1.0796947]. 
=============================================
[2019-04-05 13:55:34,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[84.12849 ]
 [83.660286]
 [83.31832 ]
 [82.875206]
 [82.41175 ]], R is [[84.48468018]
 [84.63983154]
 [84.72200775]
 [84.87478638]
 [84.95461273]].
[2019-04-05 13:55:35,590] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1394319: loss 1.8698
[2019-04-05 13:55:35,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1394322: learning rate 0.0000
[2019-04-05 13:55:35,825] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88500, global step 1394466: loss 0.0507
[2019-04-05 13:55:35,826] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88500, global step 1394466: learning rate 0.0000
[2019-04-05 13:55:36,064] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1394608: loss 0.4700
[2019-04-05 13:55:36,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1394610: learning rate 0.0000
[2019-04-05 13:55:36,473] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87500, global step 1394838: loss 0.5050
[2019-04-05 13:55:36,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87500, global step 1394838: learning rate 0.0000
[2019-04-05 13:55:36,549] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4326472e-03 3.9260206e-01 1.3289227e-02 1.0347534e-02 5.7749397e-01
 3.4794456e-04 3.4866172e-03], sum to 1.0000
[2019-04-05 13:55:36,550] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5982
[2019-04-05 13:55:36,559] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88000, global step 1394886: loss 2.2800
[2019-04-05 13:55:36,559] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88000, global step 1394886: learning rate 0.0000
[2019-04-05 13:55:36,576] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88000, global step 1394895: loss 6.7467
[2019-04-05 13:55:36,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88000, global step 1394896: learning rate 0.0000
[2019-04-05 13:55:36,633] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.1, 79.0, 85.66666666666667, 0.0, 20.5, 19.58591192055966, -0.9791190763672265, 1.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 826800.0000, 
sim time next is 827400.0000, 
raw observation next is [-4.0, 79.0, 80.33333333333333, 0.0, 21.0, 19.8723930411197, -0.8800072216786538, 1.0, 1.0, 197283.0093698145], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.2677777777777778, 0.0, 0.25, 0.1560327534266417, 0.20666425944044875, 1.0, 1.0, 0.9394429017610214], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1717418], dtype=float32), 0.9405633]. 
=============================================
[2019-04-05 13:55:38,601] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1395975: loss 0.6652
[2019-04-05 13:55:38,602] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1395976: learning rate 0.0000
[2019-04-05 13:55:39,936] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5617395e-05 2.8162482e-01 7.9084968e-04 1.4053115e-01 5.7672799e-01
 3.1034673e-05 2.6851319e-04], sum to 1.0000
[2019-04-05 13:55:39,936] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5827
[2019-04-05 13:55:39,951] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.633333333333333, 64.0, 0.0, 0.0, 19.5, 19.66404773821655, -1.028093318654814, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 675600.0000, 
sim time next is 676200.0000, 
raw observation next is [-2.716666666666667, 64.5, 0.0, 0.0, 20.0, 19.5626560418022, -1.030945025412346, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.8260869565217391, 0.3873499538319483, 0.645, 0.0, 0.0, 0.16666666666666666, 0.13022133681684997, 0.15635165819588467, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.0560121], dtype=float32), 0.6684984]. 
=============================================
[2019-04-05 13:55:40,073] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1396737: loss 0.2297
[2019-04-05 13:55:40,074] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1396737: learning rate 0.0000
[2019-04-05 13:55:41,355] A3C_AGENT_WORKER-Thread-9 INFO:Local step 86500, global step 1397488: loss 1.6569
[2019-04-05 13:55:41,357] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 86500, global step 1397488: learning rate 0.0000
[2019-04-05 13:55:43,056] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86500, global step 1398482: loss 1.0145
[2019-04-05 13:55:43,060] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86500, global step 1398484: learning rate 0.0000
[2019-04-05 13:55:45,344] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3190394e-07 9.1988206e-01 3.2714539e-04 1.1195406e-02 6.8585739e-02
 2.2496213e-06 7.2270800e-06], sum to 1.0000
[2019-04-05 13:55:45,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6930
[2019-04-05 13:55:45,356] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 19.0, 18.94271709365942, -1.010936994670181, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1753800.0000, 
sim time next is 1754400.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 18.94726532698744, -1.020336348427265, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.07893877724895344, 0.15988788385757835, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.9168854], dtype=float32), 0.4267655]. 
=============================================
[2019-04-05 13:55:45,365] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1399767: loss 0.0587
[2019-04-05 13:55:45,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1399767: learning rate 0.0000
[2019-04-05 13:55:45,769] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 13:55:45,775] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:55:45,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:55:45,776] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:55:45,776] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:55:45,777] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:55:45,777] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:55:45,786] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run71
[2019-04-05 13:55:45,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run71
[2019-04-05 13:55:45,820] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run71
[2019-04-05 13:56:56,594] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11642961]
[2019-04-05 13:56:56,595] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.972112678, 67.08362087666667, 0.0, 0.0, 19.0, 20.39958681582573, -0.7363758093870462, 0.0, 1.0, 0.0]
[2019-04-05 13:56:56,595] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:56:56,595] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8774395e-05 7.7160168e-01 1.1733685e-03 1.0520947e-02 2.1640816e-01
 6.4675378e-06 2.7064976e-04], sampled 0.38912324959138567
[2019-04-05 13:56:58,548] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6406.4765 145365227.2954 -1725.6690
[2019-04-05 13:57:10,861] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6022.0622 174812134.6214 -2213.4750
[2019-04-05 13:57:17,660] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5964.0892 189648346.7434 -2219.4432
[2019-04-05 13:57:18,686] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1400000, evaluation results [1400000.0, 6022.062152466885, 174812134.6214391, -2213.474985702202, 6406.476537494145, 145365227.29541185, -1725.6690155172264, 5964.089174995419, 189648346.74335292, -2219.4431820655454]
[2019-04-05 13:57:19,347] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88000, global step 1400372: loss 0.8736
[2019-04-05 13:57:19,348] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88000, global step 1400372: learning rate 0.0000
[2019-04-05 13:57:19,669] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.5665015e-05 8.4832197e-01 2.8116524e-03 6.3556165e-02 8.5076265e-02
 2.4723387e-05 1.2363627e-04], sum to 1.0000
[2019-04-05 13:57:19,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2989
[2019-04-05 13:57:19,694] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.2, 81.0, 0.0, 0.0, 26.0, 20.86385740796073, -0.7054178158909931, 0.0, 1.0, 49481.08310425522], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1830600.0000, 
sim time next is 1831200.0000, 
raw observation next is [-6.199999999999999, 80.33333333333334, 0.0, 0.0, 25.0, 20.84554606531819, -0.7086033897948009, 0.0, 1.0, 49466.21008984382], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.8033333333333335, 0.0, 0.0, 0.5833333333333334, 0.23712883877651572, 0.2637988700683997, 0.0, 1.0, 0.23555338138020868], 
reward next is 0.1429, 
noisyNet noise sample is [array([1.7764757], dtype=float32), -0.17256819]. 
=============================================
[2019-04-05 13:57:19,844] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86500, global step 1400662: loss 1.4903
[2019-04-05 13:57:19,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86500, global step 1400663: learning rate 0.0000
[2019-04-05 13:57:20,760] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88000, global step 1401179: loss 14.0087
[2019-04-05 13:57:20,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88000, global step 1401180: learning rate 0.0000
[2019-04-05 13:57:20,819] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89500, global step 1401209: loss 0.5380
[2019-04-05 13:57:20,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89500, global step 1401210: learning rate 0.0000
[2019-04-05 13:57:21,839] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1401775: loss 1.4763
[2019-04-05 13:57:21,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1401775: learning rate 0.0000
[2019-04-05 13:57:22,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3588180e-07 1.7569920e-01 1.8456901e-04 7.3664403e-04 8.2316250e-01
 1.5794749e-06 2.1528130e-04], sum to 1.0000
[2019-04-05 13:57:22,345] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0337
[2019-04-05 13:57:22,358] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.366666666666667, 77.0, 0.0, 0.0, 20.0, 18.68069163292613, -1.124986267574735, 0.0, 1.0, 174054.8592355257], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 877200.0000, 
sim time next is 877800.0000, 
raw observation next is [-1.283333333333333, 76.5, 0.0, 0.0, 19.0, 18.90451418647542, -1.088851416371, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.4270544783010157, 0.765, 0.0, 0.0, 0.08333333333333333, 0.07537618220628506, 0.13704952787633337, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3565268], dtype=float32), 0.15787287]. 
=============================================
[2019-04-05 13:57:22,792] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1402335: loss 6.0809
[2019-04-05 13:57:22,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1402335: learning rate 0.0000
[2019-04-05 13:57:22,903] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1402404: loss 1.0406
[2019-04-05 13:57:22,904] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1402404: learning rate 0.0000
[2019-04-05 13:57:23,012] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88000, global step 1402467: loss 21.1518
[2019-04-05 13:57:23,014] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88000, global step 1402468: learning rate 0.0000
[2019-04-05 13:57:23,333] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88000, global step 1402639: loss -1.1846
[2019-04-05 13:57:23,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88000, global step 1402639: learning rate 0.0000
[2019-04-05 13:57:24,119] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1403084: loss -1.7763
[2019-04-05 13:57:24,122] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1403084: learning rate 0.0000
[2019-04-05 13:57:25,521] A3C_AGENT_WORKER-Thread-9 INFO:Local step 87000, global step 1403919: loss 0.0863
[2019-04-05 13:57:25,524] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 87000, global step 1403922: learning rate 0.0000
[2019-04-05 13:57:25,889] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88500, global step 1404119: loss 2.4112
[2019-04-05 13:57:25,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88500, global step 1404119: learning rate 0.0000
[2019-04-05 13:57:25,968] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89000, global step 1404163: loss 15.9348
[2019-04-05 13:57:25,969] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89000, global step 1404163: learning rate 0.0000
[2019-04-05 13:57:26,609] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87000, global step 1404512: loss 0.0016
[2019-04-05 13:57:26,613] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87000, global step 1404513: learning rate 0.0000
[2019-04-05 13:57:26,810] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88500, global step 1404630: loss -4.7208
[2019-04-05 13:57:26,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88500, global step 1404630: learning rate 0.0000
[2019-04-05 13:57:26,933] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.59446416e-05 8.26806784e-01 3.73074407e-04 1.06268674e-01
 6.59200102e-02 2.51458168e-05 5.50299475e-04], sum to 1.0000
[2019-04-05 13:57:26,935] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9857
[2019-04-05 13:57:26,943] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.2, 59.0, 0.0, 0.0, 19.0, 22.7710177768692, -0.1046672526930465, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1107600.0000, 
sim time next is 1108200.0000, 
raw observation next is [14.0, 59.5, 0.0, 0.0, 19.5, 22.69441492392716, -0.115104261046029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8504155124653741, 0.595, 0.0, 0.0, 0.125, 0.39120124366059655, 0.46163191298465706, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.5985297], dtype=float32), -0.7711945]. 
=============================================
[2019-04-05 13:57:27,225] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4397907e-05 9.6627200e-01 3.0135224e-04 2.9964754e-03 3.0263964e-02
 2.4503759e-06 1.3939045e-04], sum to 1.0000
[2019-04-05 13:57:27,226] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4202
[2019-04-05 13:57:27,250] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 50.0, 50.5, 540.5, 19.0, 18.75088833130103, -1.176727513815724, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2451600.0000, 
sim time next is 2452200.0000, 
raw observation next is [-7.016666666666667, 48.83333333333334, 54.0, 582.0, 19.0, 18.73772626458057, -1.179517635808817, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2682363804247461, 0.48833333333333345, 0.18, 0.6430939226519337, 0.08333333333333333, 0.06147718871504745, 0.1068274547303943, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30620545], dtype=float32), 0.5091888]. 
=============================================
[2019-04-05 13:57:28,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7361520e-06 5.9712797e-01 1.4408189e-03 1.0538818e-03 3.9876887e-01
 6.2536556e-06 1.5953919e-03], sum to 1.0000
[2019-04-05 13:57:28,125] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2115
[2019-04-05 13:57:28,151] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 21.0, 20.56410860313879, -0.7370670467157026, 0.0, 1.0, 67546.73342329438], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2149200.0000, 
sim time next is 2149800.0000, 
raw observation next is [-5.783333333333333, 83.0, 0.0, 0.0, 20.0, 20.52921466462789, -0.7429478112625484, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3024007386888274, 0.83, 0.0, 0.0, 0.16666666666666666, 0.21076788871899085, 0.25235072957915056, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.6324295], dtype=float32), -0.31793404]. 
=============================================
[2019-04-05 13:57:29,212] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5280740e-07 9.5465404e-01 1.0690729e-06 6.3176116e-04 4.4711210e-02
 2.2440569e-09 1.7645273e-06], sum to 1.0000
[2019-04-05 13:57:29,217] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3060
[2019-04-05 13:57:29,224] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 19.0, 21.27662667934444, -0.4668731679934082, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1057200.0000, 
sim time next is 1057800.0000, 
raw observation next is [13.38333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 21.32400136545495, -0.467395088833559, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8333333333333334, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.27700011378791256, 0.34420163705548035, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23446728], dtype=float32), 2.053429]. 
=============================================
[2019-04-05 13:57:30,503] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87000, global step 1406772: loss 3.5697
[2019-04-05 13:57:30,503] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87000, global step 1406772: learning rate 0.0000
[2019-04-05 13:57:31,119] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3639717e-05 7.1301681e-01 9.1315022e-05 6.0270703e-04 2.8621691e-01
 1.7669764e-06 4.6835397e-05], sum to 1.0000
[2019-04-05 13:57:31,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0125
[2019-04-05 13:57:31,130] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.1, 82.16666666666667, 0.0, 0.0, 19.0, 19.31846413423048, -1.010394999298755, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1979400.0000, 
sim time next is 1980000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.0, 19.23553956084605, -1.026967517753403, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.2908587257617729, 0.83, 0.0, 0.0, 0.08333333333333333, 0.10296163007050414, 0.157677494082199, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01075366], dtype=float32), 0.68774205]. 
=============================================
[2019-04-05 13:57:31,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.69579]
 [74.28834]
 [74.36729]
 [74.38509]
 [74.18563]], R is [[73.31613922]
 [73.58297729]
 [73.84714508]
 [74.1086731 ]
 [74.36758423]].
[2019-04-05 13:57:31,611] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88000, global step 1407451: loss 6.6425
[2019-04-05 13:57:31,612] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88000, global step 1407452: learning rate 0.0000
[2019-04-05 13:57:33,545] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1408578: loss 0.9135
[2019-04-05 13:57:33,546] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1408578: learning rate 0.0000
[2019-04-05 13:57:34,444] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1409083: loss 6.0033
[2019-04-05 13:57:34,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1409083: learning rate 0.0000
[2019-04-05 13:57:34,718] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1409218: loss 1.3713
[2019-04-05 13:57:34,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1409219: learning rate 0.0000
[2019-04-05 13:57:35,014] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1409361: loss 7.6380
[2019-04-05 13:57:35,014] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1409361: learning rate 0.0000
[2019-04-05 13:57:36,058] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90000, global step 1409980: loss -3.7044
[2019-04-05 13:57:36,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90000, global step 1409982: learning rate 0.0000
[2019-04-05 13:57:37,372] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88500, global step 1410761: loss -7.9537
[2019-04-05 13:57:37,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88500, global step 1410761: learning rate 0.0000
[2019-04-05 13:57:37,982] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87500, global step 1411102: loss 0.3309
[2019-04-05 13:57:37,982] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87500, global step 1411102: learning rate 0.0000
[2019-04-05 13:57:38,663] A3C_AGENT_WORKER-Thread-9 INFO:Local step 87500, global step 1411460: loss 0.0016
[2019-04-05 13:57:38,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 87500, global step 1411461: learning rate 0.0000
[2019-04-05 13:57:38,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9047711e-05 7.2618181e-01 2.3187783e-04 1.3917015e-04 2.7341217e-01
 1.5518987e-07 1.5773572e-05], sum to 1.0000
[2019-04-05 13:57:38,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4384
[2019-04-05 13:57:38,745] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.266666666666666, 71.33333333333333, 0.0, 0.0, 20.0, 22.44354443957909, -0.2040801866346792, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1626000.0000, 
sim time next is 1626600.0000, 
raw observation next is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 19.0, 22.37790048874855, -0.2132944052033658, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6837488457987074, 0.7266666666666667, 0.0, 0.0, 0.08333333333333333, 0.3648250407290459, 0.4289018649322114, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44559246], dtype=float32), 0.4392421]. 
=============================================
[2019-04-05 13:57:39,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6952628e-06 2.4847253e-03 3.6788618e-04 1.4210728e-01 8.5419166e-01
 2.9395967e-06 8.4074686e-04], sum to 1.0000
[2019-04-05 13:57:39,981] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1905
[2019-04-05 13:57:40,001] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.25, 94.5, 0.0, 0.0, 21.5, 20.45289805832684, -0.5842788045011637, 0.0, 1.0, 198580.0084963844], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1665000.0000, 
sim time next is 1665600.0000, 
raw observation next is [5.166666666666666, 93.66666666666666, 0.0, 0.0, 22.0, 20.63070767504708, -0.513275580376921, 0.0, 1.0, 161634.6639844141], 
processed observation next is [1.0, 0.2608695652173913, 0.6057248384118191, 0.9366666666666665, 0.0, 0.0, 0.3333333333333333, 0.21922563958725672, 0.3289081398743597, 0.0, 1.0, 0.7696888761162576], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.05404956], dtype=float32), -0.1191085]. 
=============================================
[2019-04-05 13:57:40,406] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88500, global step 1412378: loss 2.8449
[2019-04-05 13:57:40,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88500, global step 1412380: learning rate 0.0000
[2019-04-05 13:57:40,554] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88500, global step 1412453: loss -4.9696
[2019-04-05 13:57:40,558] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88500, global step 1412456: learning rate 0.0000
[2019-04-05 13:57:40,565] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1364403e-03 7.5030130e-01 6.9512963e-02 2.8792271e-02 1.4266351e-01
 2.9323544e-04 4.3002516e-03], sum to 1.0000
[2019-04-05 13:57:40,568] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4674
[2019-04-05 13:57:40,592] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-14.83333333333333, 84.33333333333334, 53.33333333333334, 220.0, 19.0, 19.14402844915946, -1.037043549070665, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2707800.0000, 
sim time next is 2708400.0000, 
raw observation next is [-14.66666666666667, 85.66666666666667, 66.66666666666667, 275.0, 19.0, 19.40909318179103, -0.9665167422654023, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.05632502308402576, 0.8566666666666667, 0.22222222222222224, 0.30386740331491713, 0.08333333333333333, 0.11742443181591906, 0.17782775257819924, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29086435], dtype=float32), -0.7180632]. 
=============================================
[2019-04-05 13:57:40,699] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88500, global step 1412527: loss 9.2835
[2019-04-05 13:57:40,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88500, global step 1412527: learning rate 0.0000
[2019-04-05 13:57:41,229] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89500, global step 1412813: loss -3.4453
[2019-04-05 13:57:41,231] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89500, global step 1412813: learning rate 0.0000
[2019-04-05 13:57:42,338] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89000, global step 1413417: loss 20.5368
[2019-04-05 13:57:42,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89000, global step 1413419: learning rate 0.0000
[2019-04-05 13:57:42,340] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.96587553e-06 9.70432997e-01 2.62032874e-04 1.08299675e-04
 2.89714169e-02 7.34926118e-07 2.22600836e-04], sum to 1.0000
[2019-04-05 13:57:42,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7531
[2019-04-05 13:57:42,361] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 20.74818429841808, -0.699663488729089, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2146800.0000, 
sim time next is 2147400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 20.60305291404731, -0.7238064809281451, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.2169210761706092, 0.2587311730239516, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5135646], dtype=float32), 1.2279693]. 
=============================================
[2019-04-05 13:57:42,702] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89000, global step 1413639: loss 16.0820
[2019-04-05 13:57:42,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89000, global step 1413640: learning rate 0.0000
[2019-04-05 13:57:43,904] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87500, global step 1414372: loss -0.6567
[2019-04-05 13:57:43,907] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87500, global step 1414373: learning rate 0.0000
[2019-04-05 13:57:46,172] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1415649: loss 0.7063
[2019-04-05 13:57:46,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1415650: learning rate 0.0000
[2019-04-05 13:57:46,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8004133e-05 8.2079375e-01 2.1544054e-04 1.4616281e-03 1.7724809e-01
 1.4307402e-06 2.5172275e-04], sum to 1.0000
[2019-04-05 13:57:46,731] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8737
[2019-04-05 13:57:46,774] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 100.0, 32.5, 0.0, 19.0, 22.74061718854329, -0.2703536357393774, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1501200.0000, 
sim time next is 1501800.0000, 
raw observation next is [1.7, 100.0, 37.33333333333334, 0.0, 19.5, 22.78266403956341, -0.2691913731592436, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5096952908587258, 1.0, 0.12444444444444448, 0.0, 0.125, 0.39855533663028425, 0.4102695422802521, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.19507231], dtype=float32), -0.60045534]. 
=============================================
[2019-04-05 13:57:47,425] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88000, global step 1416335: loss 6.7708
[2019-04-05 13:57:47,427] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88000, global step 1416335: learning rate 0.0000
[2019-04-05 13:57:47,873] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1416597: loss -0.0206
[2019-04-05 13:57:47,874] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1416597: learning rate 0.0000
[2019-04-05 13:57:48,385] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88500, global step 1416860: loss 3.0305
[2019-04-05 13:57:48,386] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88500, global step 1416861: learning rate 0.0000
[2019-04-05 13:57:48,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.02590691e-04 6.90825403e-01 8.23062193e-03 1.21968985e-01
 1.76564381e-01 2.26994045e-04 1.28107157e-03], sum to 1.0000
[2019-04-05 13:57:48,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0482
[2019-04-05 13:57:48,945] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 63.5, 0.0, 0.0, 19.5, 19.03771788510954, -1.106580996921749, 0.0, 1.0, 197732.0191841033], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2334600.0000, 
sim time next is 2335200.0000, 
raw observation next is [-2.3, 63.0, 0.0, 0.0, 19.0, 19.00072344128696, -1.102101841519781, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.63, 0.0, 0.0, 0.08333333333333333, 0.08339362010724678, 0.13263271949340635, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45264584], dtype=float32), -0.7719081]. 
=============================================
[2019-04-05 13:57:49,757] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88000, global step 1417524: loss 7.7046
[2019-04-05 13:57:49,759] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88000, global step 1417524: learning rate 0.0000
[2019-04-05 13:57:49,955] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90500, global step 1417635: loss 5.3817
[2019-04-05 13:57:49,956] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90500, global step 1417635: learning rate 0.0000
[2019-04-05 13:57:50,612] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8892823e-04 8.1068158e-01 6.2723672e-03 2.3001949e-03 1.7845288e-01
 3.9102356e-06 1.8001868e-03], sum to 1.0000
[2019-04-05 13:57:50,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1623
[2019-04-05 13:57:50,620] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.7, 55.66666666666667, 0.0, 0.0, 19.0, 19.01364374520327, -1.064239424958822, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2319000.0000, 
sim time next is 2319600.0000, 
raw observation next is [-1.7, 55.33333333333334, 0.0, 0.0, 19.0, 18.96501237883567, -1.076599979305882, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.5533333333333335, 0.0, 0.0, 0.08333333333333333, 0.08041769823630585, 0.14113334023137267, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08123402], dtype=float32), -0.5063955]. 
=============================================
[2019-04-05 13:57:50,754] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88000, global step 1418101: loss 8.0460
[2019-04-05 13:57:50,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88000, global step 1418101: learning rate 0.0000
[2019-04-05 13:57:52,515] A3C_AGENT_WORKER-Thread-9 INFO:Local step 88000, global step 1419028: loss 7.6985
[2019-04-05 13:57:52,518] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 88000, global step 1419029: learning rate 0.0000
[2019-04-05 13:57:54,209] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89000, global step 1419812: loss 7.0332
[2019-04-05 13:57:54,211] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89000, global step 1419813: learning rate 0.0000
[2019-04-05 13:57:54,621] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 13:57:54,623] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 13:57:54,623] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 13:57:54,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:57:54,624] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 13:57:54,625] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:57:54,625] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 13:57:54,634] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run72
[2019-04-05 13:57:54,653] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run72
[2019-04-05 13:57:54,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run72
[2019-04-05 13:58:55,944] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116166666]
[2019-04-05 13:58:55,945] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.974192309166668, 75.61316599333334, 0.0, 0.0, 19.0, 18.88812010753788, -1.141893463408913, 0.0, 1.0, 0.0]
[2019-04-05 13:58:55,945] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:58:55,947] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.9378871e-05 6.7609763e-01 1.9184991e-03 2.2726972e-02 2.9896489e-01
 1.2049633e-05 2.6056948e-04], sampled 0.5494551586099342
[2019-04-05 13:59:02,299] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116166666]
[2019-04-05 13:59:02,299] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [17.61666666666667, 40.16666666666667, 52.0, 219.6666666666666, 19.0, 24.58966271779537, 0.2231343703826528, 1.0, 0.0, 0.0]
[2019-04-05 13:59:02,300] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 13:59:02,301] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.6255869e-04 8.8543165e-01 2.2716194e-03 1.0038681e-02 1.0040860e-01
 6.5008913e-05 1.5218364e-03], sampled 0.5265363449769397
[2019-04-05 13:59:07,286] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116166666]
[2019-04-05 13:59:07,286] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 34.5, 15.33333333333334, 38.00000000000001, 19.0, 21.85754265354896, -0.473024039490993, 1.0, 1.0, 0.0]
[2019-04-05 13:59:07,287] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 13:59:07,288] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.6420452e-04 8.3352423e-01 3.1138430e-03 1.0080474e-02 1.5068141e-01
 6.8168789e-05 2.1676633e-03], sampled 0.5015299688125417
[2019-04-05 13:59:07,399] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6293.1710 145405012.6879 -1819.0140
[2019-04-05 13:59:11,424] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.116166666]
[2019-04-05 13:59:11,425] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.075719184666667, 32.02948866333334, 0.0, 0.0, 19.0, 18.91896874262873, -1.19692032016425, 0.0, 1.0, 0.0]
[2019-04-05 13:59:11,425] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 13:59:11,426] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.4731724e-05 6.3366908e-01 3.9443816e-03 4.5818292e-02 3.1553867e-01
 6.6908491e-05 8.6796179e-04], sampled 0.07482797821745157
[2019-04-05 13:59:19,512] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5896.7091 174713773.2331 -2368.1310
[2019-04-05 13:59:24,298] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5937.9106 189760986.3751 -2260.1439
[2019-04-05 13:59:25,324] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1420000, evaluation results [1420000.0, 5896.709131445014, 174713773.2331403, -2368.130981998013, 6293.17104836911, 145405012.68793443, -1819.0140385862787, 5937.910570089555, 189760986.3750668, -2260.143888072214]
[2019-04-05 13:59:25,469] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6527409e-06 6.0698634e-01 7.2808634e-03 2.7210906e-01 1.1332392e-01
 1.1532226e-06 2.9697607e-04], sum to 1.0000
[2019-04-05 13:59:25,470] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1480
[2019-04-05 13:59:25,530] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 65.0, 145.5, 745.5, 20.5, 23.14122726533539, -0.1929123438592625, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2984400.0000, 
sim time next is 2985000.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 133.6666666666667, 763.6666666666666, 19.5, 23.09344367739758, -0.1997226592449315, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.3841181902123731, 0.6416666666666667, 0.4455555555555557, 0.8438305709023941, 0.125, 0.4244536397831317, 0.4334257802516895, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.24685048], dtype=float32), -0.69103795]. 
=============================================
[2019-04-05 13:59:25,543] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.731674]
 [85.7858  ]
 [84.69963 ]
 [83.983185]
 [82.896614]], R is [[87.75466156]
 [87.66282654]
 [87.42906189]
 [87.05477142]
 [86.68422699]].
[2019-04-05 13:59:26,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9853875e-06 4.6750233e-01 5.6015160e-05 3.4061633e-04 5.3198320e-01
 5.9493885e-07 1.1225903e-04], sum to 1.0000
[2019-04-05 13:59:26,240] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1497
[2019-04-05 13:59:26,265] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 20.5, 19.32583489550325, -0.9012351882822222, 0.0, 1.0, 196518.3948064044], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3453000.0000, 
sim time next is 3453600.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.5, 19.40418116829434, -0.8806730737203371, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.125, 0.11701509735786164, 0.20644230875988764, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.1906932], dtype=float32), -0.52122706]. 
=============================================
[2019-04-05 13:59:27,160] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90000, global step 1420897: loss 23.8818
[2019-04-05 13:59:27,161] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90000, global step 1420897: learning rate 0.0000
[2019-04-05 13:59:27,647] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88000, global step 1421138: loss 8.7336
[2019-04-05 13:59:27,648] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88000, global step 1421138: learning rate 0.0000
[2019-04-05 13:59:27,693] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89000, global step 1421159: loss 12.0697
[2019-04-05 13:59:27,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89000, global step 1421159: learning rate 0.0000
[2019-04-05 13:59:27,804] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89000, global step 1421207: loss 10.2128
[2019-04-05 13:59:27,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89000, global step 1421207: learning rate 0.0000
[2019-04-05 13:59:27,810] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.19655706e-04 4.71292138e-01 4.16729553e-03 4.85166311e-02
 4.69687164e-01 2.83038698e-05 6.18876144e-03], sum to 1.0000
[2019-04-05 13:59:27,816] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1157
[2019-04-05 13:59:27,844] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 18.90770684652491, -1.109132144826854, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 19.04759688596442, -1.100737757790943, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.08729974049703504, 0.1330874140696857, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16323315], dtype=float32), 1.9207184]. 
=============================================
[2019-04-05 13:59:27,850] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89000, global step 1421222: loss 4.8395
[2019-04-05 13:59:27,852] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89000, global step 1421223: learning rate 0.0000
[2019-04-05 13:59:28,143] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89500, global step 1421374: loss -0.7361
[2019-04-05 13:59:28,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89500, global step 1421374: learning rate 0.0000
[2019-04-05 13:59:28,594] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89500, global step 1421613: loss 14.2164
[2019-04-05 13:59:28,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89500, global step 1421613: learning rate 0.0000
[2019-04-05 13:59:29,321] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88000, global step 1421988: loss 2.0757
[2019-04-05 13:59:29,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88000, global step 1421988: learning rate 0.0000
[2019-04-05 13:59:32,430] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88000, global step 1423703: loss 3.5215
[2019-04-05 13:59:32,431] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88000, global step 1423703: learning rate 0.0000
[2019-04-05 13:59:33,816] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91000, global step 1424464: loss 0.8321
[2019-04-05 13:59:33,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91000, global step 1424465: learning rate 0.0000
[2019-04-05 13:59:33,844] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.7456436e-05 9.2934394e-01 2.3420209e-02 2.2360831e-02 2.1466622e-02
 3.0131794e-06 3.3179419e-03], sum to 1.0000
[2019-04-05 13:59:33,846] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8024
[2019-04-05 13:59:33,856] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 19.0913037855792, -1.048025760879873, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2840400.0000, 
sim time next is 2841000.0000, 
raw observation next is [2.0, 44.00000000000001, 0.0, 0.0, 19.0, 19.05243147337358, -1.056135395060704, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44000000000000006, 0.0, 0.0, 0.08333333333333333, 0.08770262278113172, 0.14795486831309868, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.1495233], dtype=float32), -0.40287387]. 
=============================================
[2019-04-05 13:59:33,866] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.47608 ]
 [70.88495 ]
 [70.38281 ]
 [69.43813 ]
 [67.421005]], R is [[71.86765289]
 [72.14897919]
 [72.42749023]
 [72.70321655]
 [72.97618866]].
[2019-04-05 13:59:34,066] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88500, global step 1424622: loss -5.6526
[2019-04-05 13:59:34,074] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88500, global step 1424626: learning rate 0.0000
[2019-04-05 13:59:35,323] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89000, global step 1425397: loss 10.9565
[2019-04-05 13:59:35,326] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89000, global step 1425397: learning rate 0.0000
[2019-04-05 13:59:36,738] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88500, global step 1426214: loss -5.9137
[2019-04-05 13:59:36,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88500, global step 1426217: learning rate 0.0000
[2019-04-05 13:59:38,623] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88500, global step 1427147: loss -5.0800
[2019-04-05 13:59:38,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88500, global step 1427147: learning rate 0.0000
[2019-04-05 13:59:38,884] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89500, global step 1427299: loss 2.9662
[2019-04-05 13:59:38,885] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89500, global step 1427299: learning rate 0.0000
[2019-04-05 13:59:39,655] A3C_AGENT_WORKER-Thread-9 INFO:Local step 88500, global step 1427751: loss 0.8844
[2019-04-05 13:59:39,661] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 88500, global step 1427754: learning rate 0.0000
[2019-04-05 13:59:40,049] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90500, global step 1427966: loss 28.5864
[2019-04-05 13:59:40,051] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90500, global step 1427969: learning rate 0.0000
[2019-04-05 13:59:41,547] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89500, global step 1428718: loss -0.2996
[2019-04-05 13:59:41,547] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89500, global step 1428718: learning rate 0.0000
[2019-04-05 13:59:41,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00623542 0.59989744 0.01067282 0.06632361 0.3129364  0.00062133
 0.003313  ], sum to 1.0000
[2019-04-05 13:59:41,689] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5330
[2019-04-05 13:59:41,755] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.33333333333333, 78.5, 103.6666666666667, 632.6666666666667, 19.0, 22.30844655522056, -0.4772039270438047, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2713800.0000, 
sim time next is 2714400.0000, 
raw observation next is [-12.0, 76.0, 107.0, 643.0, 19.0, 22.3703119488821, -0.4638488706294028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.13019390581717452, 0.76, 0.3566666666666667, 0.7104972375690608, 0.08333333333333333, 0.3641926624068417, 0.34538370979019906, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49572116], dtype=float32), 0.16471218]. 
=============================================
[2019-04-05 13:59:42,083] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2364762e-05 7.4938095e-01 2.4918690e-02 7.5913072e-02 1.4858297e-01
 1.6815962e-04 1.0037886e-03], sum to 1.0000
[2019-04-05 13:59:42,084] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1245
[2019-04-05 13:59:42,094] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 19.63968023950641, -0.9578531789524688, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3378000.0000, 
sim time next is 3378600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 19.599320120972, -0.9756751776909782, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.1332766767476666, 0.17477494076967393, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01404328], dtype=float32), 0.37169275]. 
=============================================
[2019-04-05 13:59:42,212] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89500, global step 1429086: loss -2.9270
[2019-04-05 13:59:42,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89500, global step 1429086: learning rate 0.0000
[2019-04-05 13:59:42,459] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89500, global step 1429210: loss 12.0525
[2019-04-05 13:59:42,459] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89500, global step 1429210: learning rate 0.0000
[2019-04-05 13:59:42,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90000, global step 1429272: loss 9.9234
[2019-04-05 13:59:42,581] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90000, global step 1429274: learning rate 0.0000
[2019-04-05 13:59:42,690] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90000, global step 1429330: loss -8.6817
[2019-04-05 13:59:42,695] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90000, global step 1429331: learning rate 0.0000
[2019-04-05 13:59:44,369] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88500, global step 1430248: loss -2.1823
[2019-04-05 13:59:44,371] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88500, global step 1430250: learning rate 0.0000
[2019-04-05 13:59:45,177] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91500, global step 1430657: loss -0.2490
[2019-04-05 13:59:45,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91500, global step 1430657: learning rate 0.0000
[2019-04-05 13:59:45,872] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88500, global step 1430966: loss -0.2104
[2019-04-05 13:59:45,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88500, global step 1430967: learning rate 0.0000
[2019-04-05 13:59:48,719] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88500, global step 1432539: loss -0.6051
[2019-04-05 13:59:48,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88500, global step 1432539: learning rate 0.0000
[2019-04-05 13:59:50,179] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89000, global step 1433345: loss 13.7037
[2019-04-05 13:59:50,182] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89000, global step 1433345: learning rate 0.0000
[2019-04-05 13:59:50,282] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2121415e-04 9.0117466e-01 4.6643703e-03 4.7567114e-03 8.9040428e-02
 1.8685596e-04 5.5710636e-05], sum to 1.0000
[2019-04-05 13:59:50,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9321
[2019-04-05 13:59:50,301] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 19.0, 18.72748655476178, -1.185761797217834, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2354400.0000, 
sim time next is 2355000.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 19.5, 18.72822020947387, -1.15280185182707, 0.0, 1.0, 199421.680567489], 
processed observation next is [0.0, 0.2608695652173913, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.125, 0.060685017456155954, 0.11573271605764335, 0.0, 1.0, 0.9496270503213762], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.4535139], dtype=float32), 0.5448677]. 
=============================================
[2019-04-05 13:59:50,312] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.09289 ]
 [79.17457 ]
 [79.37148 ]
 [79.42782 ]
 [79.534874]], R is [[80.25035095]
 [80.44784546]
 [80.64337158]
 [80.83693695]
 [81.02857208]].
[2019-04-05 13:59:50,612] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89500, global step 1433573: loss -1.3109
[2019-04-05 13:59:50,615] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89500, global step 1433574: learning rate 0.0000
[2019-04-05 13:59:51,249] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7128093e-05 4.3884382e-01 1.4787092e-03 4.1862089e-02 5.1226836e-01
 4.6688045e-04 5.0530583e-03], sum to 1.0000
[2019-04-05 13:59:51,254] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8727
[2019-04-05 13:59:51,282] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.533333333333334, 77.0, 0.0, 0.0, 19.5, 19.32306855094615, -1.104015370467597, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2175600.0000, 
sim time next is 2176200.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 20.0, 19.25281845544232, -1.109726615668442, 0.0, 1.0, 148288.3042048332], 
processed observation next is [1.0, 0.17391304347826086, 0.28393351800554023, 0.765, 0.0, 0.0, 0.16666666666666666, 0.10440153795352665, 0.13009112811051934, 0.0, 1.0, 0.7061347819277771], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.56871766], dtype=float32), 2.5161135]. 
=============================================
[2019-04-05 13:59:52,349] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91000, global step 1434472: loss 11.2632
[2019-04-05 13:59:52,351] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91000, global step 1434475: learning rate 0.0000
[2019-04-05 13:59:53,093] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9522945e-04 7.6504415e-01 1.5220524e-02 3.0148739e-02 1.8787554e-01
 2.1777060e-04 6.9796969e-04], sum to 1.0000
[2019-04-05 13:59:53,097] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7165
[2019-04-05 13:59:53,107] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.666666666666667, 37.0, 102.0, 644.0, 19.0, 20.96817805314792, -0.7236091860912333, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4094400.0000, 
sim time next is 4095000.0000, 
raw observation next is [-2.5, 36.5, 104.0, 679.0, 19.0, 21.11614434270701, -0.6953011317512678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39335180055401664, 0.365, 0.3466666666666667, 0.7502762430939226, 0.08333333333333333, 0.2596786952255841, 0.2682329560829107, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7733219], dtype=float32), -0.66783273]. 
=============================================
[2019-04-05 13:59:53,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[40.801434]
 [40.36993 ]
 [39.8323  ]
 [39.425854]
 [39.705578]], R is [[41.06387329]
 [40.65323639]
 [40.2467041 ]
 [39.84423828]
 [39.44579697]].
[2019-04-05 13:59:53,481] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89000, global step 1435078: loss 19.4988
[2019-04-05 13:59:53,483] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89000, global step 1435078: learning rate 0.0000
[2019-04-05 13:59:53,861] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90000, global step 1435261: loss -1.7727
[2019-04-05 13:59:53,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90000, global step 1435262: learning rate 0.0000
[2019-04-05 13:59:54,669] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89000, global step 1435654: loss 4.5642
[2019-04-05 13:59:54,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89000, global step 1435654: learning rate 0.0000
[2019-04-05 13:59:56,213] A3C_AGENT_WORKER-Thread-9 INFO:Local step 89000, global step 1436369: loss 8.7812
[2019-04-05 13:59:56,221] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 89000, global step 1436369: learning rate 0.0000
[2019-04-05 13:59:56,410] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90500, global step 1436456: loss 0.0364
[2019-04-05 13:59:56,411] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90500, global step 1436456: learning rate 0.0000
[2019-04-05 13:59:56,909] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90500, global step 1436704: loss 3.7154
[2019-04-05 13:59:56,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90500, global step 1436704: learning rate 0.0000
[2019-04-05 13:59:57,000] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90000, global step 1436748: loss 0.6733
[2019-04-05 13:59:57,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90000, global step 1436749: learning rate 0.0000
[2019-04-05 13:59:57,103] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90000, global step 1436801: loss -3.2788
[2019-04-05 13:59:57,105] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90000, global step 1436802: learning rate 0.0000
[2019-04-05 13:59:57,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3059130e-06 8.5987501e-02 3.5087762e-03 1.0772045e-02 8.9943141e-01
 3.3865563e-05 2.6313766e-04], sum to 1.0000
[2019-04-05 13:59:57,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4435
[2019-04-05 13:59:57,265] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.6, 28.0, 88.5, 838.5, 19.0, 19.29693126841044, -1.028443167534497, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2466000.0000, 
sim time next is 2466600.0000, 
raw observation next is [1.7, 27.83333333333334, 88.0, 836.3333333333334, 19.5, 19.30068574196157, -1.027504962981705, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.5096952908587258, 0.2783333333333334, 0.29333333333333333, 0.9241252302025783, 0.125, 0.10839047849679743, 0.157498345672765, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([2.8710952], dtype=float32), -0.37256593]. 
=============================================
[2019-04-05 13:59:58,909] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92000, global step 1437749: loss 0.7865
[2019-04-05 13:59:58,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92000, global step 1437749: learning rate 0.0000
[2019-04-05 13:59:59,303] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90000, global step 1437962: loss 0.9462
[2019-04-05 13:59:59,304] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90000, global step 1437962: learning rate 0.0000
[2019-04-05 14:00:00,308] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89000, global step 1438552: loss 22.6489
[2019-04-05 14:00:00,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89000, global step 1438552: learning rate 0.0000
[2019-04-05 14:00:02,875] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-05 14:00:02,877] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:00:02,877] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:00:02,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:00:02,878] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:00:02,881] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89000, global step 1440000: loss 13.6803
[2019-04-05 14:00:02,878] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:00:02,883] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:00:02,883] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89000, global step 1440000: learning rate 0.0000
[2019-04-05 14:00:02,892] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run73
[2019-04-05 14:00:02,909] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run73
[2019-04-05 14:00:02,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run73
[2019-04-05 14:01:16,726] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6335.8441 145127331.0515 -1764.3139
[2019-04-05 14:01:22,041] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1160051]
[2019-04-05 14:01:22,041] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.268697767, 32.14264637, 0.0, 0.0, 19.0, 20.41977178082261, -0.7535451543315452, 0.0, 1.0, 0.0]
[2019-04-05 14:01:22,041] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:01:22,042] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1318751e-05 7.7512395e-01 3.5720516e-03 1.4092711e-02 2.0624988e-01
 3.3673296e-05 8.6646323e-04], sampled 0.21360347675255387
[2019-04-05 14:01:26,700] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5877.1443 174016902.2659 -2352.5023
[2019-04-05 14:01:34,552] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5972.1008 188403799.9649 -2228.1596
[2019-04-05 14:01:35,578] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1440000, evaluation results [1440000.0, 5877.144303302423, 174016902.26585233, -2352.502316238219, 6335.844120656566, 145127331.05154136, -1764.3138887835128, 5972.100802804588, 188403799.96491137, -2228.1595940667225]
[2019-04-05 14:01:37,138] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91500, global step 1440923: loss -1.0319
[2019-04-05 14:01:37,139] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91500, global step 1440923: learning rate 0.0000
[2019-04-05 14:01:37,869] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89500, global step 1441345: loss -1.1629
[2019-04-05 14:01:37,871] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89500, global step 1441345: learning rate 0.0000
[2019-04-05 14:01:38,421] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89000, global step 1441661: loss 10.8838
[2019-04-05 14:01:38,427] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89000, global step 1441663: learning rate 0.0000
[2019-04-05 14:01:38,644] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90000, global step 1441791: loss 5.0018
[2019-04-05 14:01:38,646] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90000, global step 1441791: learning rate 0.0000
[2019-04-05 14:01:41,727] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90500, global step 1443433: loss 14.0751
[2019-04-05 14:01:41,728] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90500, global step 1443433: learning rate 0.0000
[2019-04-05 14:01:41,937] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91000, global step 1443567: loss 2.0072
[2019-04-05 14:01:41,940] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91000, global step 1443567: learning rate 0.0000
[2019-04-05 14:01:42,304] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89500, global step 1443782: loss 8.2319
[2019-04-05 14:01:42,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89500, global step 1443783: learning rate 0.0000
[2019-04-05 14:01:42,562] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91000, global step 1443936: loss 7.4075
[2019-04-05 14:01:42,563] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91000, global step 1443937: learning rate 0.0000
[2019-04-05 14:01:42,920] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90500, global step 1444139: loss -1.8414
[2019-04-05 14:01:42,922] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90500, global step 1444139: learning rate 0.0000
[2019-04-05 14:01:43,304] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90500, global step 1444380: loss 2.5889
[2019-04-05 14:01:43,308] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90500, global step 1444381: learning rate 0.0000
[2019-04-05 14:01:43,567] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89500, global step 1444537: loss 18.2468
[2019-04-05 14:01:43,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89500, global step 1444537: learning rate 0.0000
[2019-04-05 14:01:44,197] A3C_AGENT_WORKER-Thread-9 INFO:Local step 89500, global step 1444915: loss -1.1301
[2019-04-05 14:01:44,198] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 89500, global step 1444915: learning rate 0.0000
[2019-04-05 14:01:44,419] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92500, global step 1445045: loss 31.0172
[2019-04-05 14:01:44,421] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92500, global step 1445046: learning rate 0.0000
[2019-04-05 14:01:44,907] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9485793e-05 9.5006841e-01 1.9284412e-03 8.2707539e-04 4.5385435e-02
 2.1435053e-05 1.7296248e-03], sum to 1.0000
[2019-04-05 14:01:44,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6595
[2019-04-05 14:01:44,916] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.333333333333333, 58.33333333333334, 0.0, 0.0, 19.0, 20.09195060173654, -0.8002614208388971, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3356400.0000, 
sim time next is 3357000.0000, 
raw observation next is [-3.5, 60.0, 0.0, 0.0, 19.0, 20.00416092734801, -0.8184727917961253, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.36565096952908593, 0.6, 0.0, 0.0, 0.08333333333333333, 0.1670134106123342, 0.22717573606795824, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05606354], dtype=float32), 2.095595]. 
=============================================
[2019-04-05 14:01:44,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[60.75477 ]
 [57.866703]
 [54.808178]
 [56.008274]
 [56.882744]], R is [[63.45814133]
 [63.82355881]
 [64.18532562]
 [63.54347229]
 [62.90803909]].
[2019-04-05 14:01:45,276] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90500, global step 1445546: loss 2.4748
[2019-04-05 14:01:45,278] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90500, global step 1445547: learning rate 0.0000
[2019-04-05 14:01:47,859] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7724234e-07 3.4548557e-01 5.2342522e-05 2.0077515e-04 6.5421861e-01
 3.0240631e-06 3.9444625e-05], sum to 1.0000
[2019-04-05 14:01:47,862] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8939
[2019-04-05 14:01:47,879] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 20.0, 20.20960504297377, -0.733471920951215, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 2930400.0000, 
sim time next is 2931000.0000, 
raw observation next is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 20.5, 20.30348245953951, -0.7782035743794257, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.43028624192059095, 0.7916666666666667, 0.0, 0.0, 0.20833333333333334, 0.19195687162829245, 0.24059880854019144, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.14320233], dtype=float32), -0.05969424]. 
=============================================
[2019-04-05 14:01:47,901] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.26269]
 [86.68661]
 [86.75808]
 [85.23757]
 [85.14982]], R is [[85.846138  ]
 [85.84481812]
 [85.70066071]
 [85.62937164]
 [85.63021851]].
[2019-04-05 14:01:48,349] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89500, global step 1447321: loss -1.4301
[2019-04-05 14:01:48,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89500, global step 1447322: learning rate 0.0000
[2019-04-05 14:01:50,801] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89500, global step 1448679: loss -5.7413
[2019-04-05 14:01:50,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89500, global step 1448680: learning rate 0.0000
[2019-04-05 14:01:51,461] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92000, global step 1449056: loss 0.6354
[2019-04-05 14:01:51,465] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92000, global step 1449058: learning rate 0.0000
[2019-04-05 14:01:52,147] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90500, global step 1449428: loss -0.6880
[2019-04-05 14:01:52,148] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90500, global step 1449428: learning rate 0.0000
[2019-04-05 14:01:52,907] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90000, global step 1449872: loss 1.0056
[2019-04-05 14:01:52,908] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90000, global step 1449872: learning rate 0.0000
[2019-04-05 14:01:53,038] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89500, global step 1449953: loss 0.2116
[2019-04-05 14:01:53,044] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89500, global step 1449957: learning rate 0.0000
[2019-04-05 14:01:54,233] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9236384e-05 4.1863143e-01 4.8394916e-03 8.8740781e-02 4.7946098e-01
 1.5565618e-04 8.1524644e-03], sum to 1.0000
[2019-04-05 14:01:54,233] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8473
[2019-04-05 14:01:54,257] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 21.5, 19.16191778387154, -0.9637515105251954, 0.0, 1.0, 67475.46578424484], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4770000.0000, 
sim time next is 4770600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 22.0, 19.30607409509465, -0.9412998427652831, 0.0, 1.0, 53921.71243439916], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.3333333333333333, 0.10883950792455425, 0.18623338574490564, 0.0, 1.0, 0.2567700592114246], 
reward next is 0.5714, 
noisyNet noise sample is [array([-0.1692731], dtype=float32), -1.1019105]. 
=============================================
[2019-04-05 14:01:54,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7624048e-07 9.8499185e-01 1.8024853e-05 1.4358565e-02 6.2894059e-04
 5.3105259e-07 1.8296058e-06], sum to 1.0000
[2019-04-05 14:01:54,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3252
[2019-04-05 14:01:54,578] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 102.0, 788.0, 19.0, 20.19930831922668, -0.7518345309409287, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3594600.0000, 
sim time next is 3595200.0000, 
raw observation next is [-1.0, 42.0, 99.33333333333333, 773.1666666666667, 19.0, 20.22261668930668, -0.7520938233490062, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.3311111111111111, 0.854327808471455, 0.08333333333333333, 0.1852180574422233, 0.2493020588836646, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7526437], dtype=float32), -0.0710662]. 
=============================================
[2019-04-05 14:01:55,150] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91500, global step 1451173: loss 2.2259
[2019-04-05 14:01:55,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91500, global step 1451178: learning rate 0.0000
[2019-04-05 14:01:55,290] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91000, global step 1451248: loss 6.9585
[2019-04-05 14:01:55,292] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91000, global step 1451248: learning rate 0.0000
[2019-04-05 14:01:55,714] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91000, global step 1451458: loss 0.0560
[2019-04-05 14:01:55,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91000, global step 1451459: learning rate 0.0000
[2019-04-05 14:01:55,804] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91500, global step 1451510: loss 2.3082
[2019-04-05 14:01:55,806] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91500, global step 1451510: learning rate 0.0000
[2019-04-05 14:01:57,153] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91000, global step 1452270: loss 6.7707
[2019-04-05 14:01:57,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91000, global step 1452270: learning rate 0.0000
[2019-04-05 14:01:57,412] A3C_AGENT_WORKER-Thread-14 INFO:Local step 93000, global step 1452426: loss 0.1521
[2019-04-05 14:01:57,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 93000, global step 1452427: learning rate 0.0000
[2019-04-05 14:01:57,776] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90000, global step 1452648: loss 5.1964
[2019-04-05 14:01:57,778] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90000, global step 1452650: learning rate 0.0000
[2019-04-05 14:01:58,664] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90000, global step 1453225: loss 5.8038
[2019-04-05 14:01:58,665] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90000, global step 1453226: learning rate 0.0000
[2019-04-05 14:01:59,113] A3C_AGENT_WORKER-Thread-9 INFO:Local step 90000, global step 1453498: loss -2.0205
[2019-04-05 14:01:59,114] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 90000, global step 1453498: learning rate 0.0000
[2019-04-05 14:01:59,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00217508 0.44431245 0.01342765 0.00104036 0.5301384  0.00637046
 0.00253559], sum to 1.0000
[2019-04-05 14:01:59,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0760
[2019-04-05 14:01:59,181] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 100.0, 89.16666666666666, 0.0, 19.0, 20.50212113093488, -0.8650314271950554, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2904000.0000, 
sim time next is 2904600.0000, 
raw observation next is [2.0, 100.0, 88.33333333333334, 0.0, 19.0, 20.47620398500172, -0.9389199277258654, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.29444444444444445, 0.0, 0.08333333333333333, 0.20635033208347675, 0.18702669075804487, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3163447], dtype=float32), 0.14412525]. 
=============================================
[2019-04-05 14:02:00,179] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91000, global step 1454071: loss 0.6178
[2019-04-05 14:02:00,180] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91000, global step 1454071: learning rate 0.0000
[2019-04-05 14:02:02,929] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92500, global step 1455722: loss 50.9695
[2019-04-05 14:02:02,930] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92500, global step 1455723: learning rate 0.0000
[2019-04-05 14:02:03,188] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90000, global step 1455872: loss -2.3029
[2019-04-05 14:02:03,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90000, global step 1455872: learning rate 0.0000
[2019-04-05 14:02:03,310] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2888709e-03 6.8343371e-01 7.0440499e-03 3.1739123e-02 2.7064234e-01
 1.9785221e-04 5.6540594e-03], sum to 1.0000
[2019-04-05 14:02:03,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9797
[2019-04-05 14:02:03,330] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [1.666666666666667, 29.66666666666667, 115.5, 788.6666666666667, 19.0, 21.29704359774544, -0.6981083507552865, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4962000.0000, 
sim time next is 4962600.0000, 
raw observation next is [2.0, 29.5, 117.0, 803.0, 19.5, 21.35315776785516, -0.6792942056761969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.518005540166205, 0.295, 0.39, 0.887292817679558, 0.125, 0.2794298139879299, 0.2735685981079344, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.87339675], dtype=float32), -0.1756869]. 
=============================================
[2019-04-05 14:02:04,132] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2056768e-06 9.5992303e-01 1.3198645e-03 1.6563638e-03 3.7091378e-02
 1.9721439e-07 7.8687590e-06], sum to 1.0000
[2019-04-05 14:02:04,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5114
[2019-04-05 14:02:04,144] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 19.0, 19.46487989728966, -1.001816163556713, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3001800.0000, 
sim time next is 3002400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 19.3971040226198, -1.021496740978282, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.11642533521831666, 0.15950108634057267, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0022498], dtype=float32), -0.77999943]. 
=============================================
[2019-04-05 14:02:05,022] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90000, global step 1456967: loss -2.5105
[2019-04-05 14:02:05,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90000, global step 1456970: learning rate 0.0000
[2019-04-05 14:02:05,144] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91000, global step 1457036: loss 6.9294
[2019-04-05 14:02:05,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91000, global step 1457036: learning rate 0.0000
[2019-04-05 14:02:06,850] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90500, global step 1458071: loss 23.9988
[2019-04-05 14:02:06,851] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90500, global step 1458071: learning rate 0.0000
[2019-04-05 14:02:07,300] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90000, global step 1458313: loss -0.8151
[2019-04-05 14:02:07,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90000, global step 1458313: learning rate 0.0000
[2019-04-05 14:02:07,587] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91500, global step 1458484: loss 6.4083
[2019-04-05 14:02:07,587] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91500, global step 1458485: learning rate 0.0000
[2019-04-05 14:02:08,261] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91500, global step 1458888: loss -0.5430
[2019-04-05 14:02:08,263] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91500, global step 1458891: learning rate 0.0000
[2019-04-05 14:02:08,853] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92000, global step 1459279: loss 0.0357
[2019-04-05 14:02:08,859] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92000, global step 1459280: learning rate 0.0000
[2019-04-05 14:02:09,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:02:09,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:02:09,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run12
[2019-04-05 14:02:09,946] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92000, global step 1459909: loss 0.0945
[2019-04-05 14:02:09,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92000, global step 1459909: learning rate 0.0000
[2019-04-05 14:02:10,106] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-05 14:02:10,110] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:02:10,110] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:02:10,110] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:02:10,110] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:02:10,111] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:02:10,111] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:02:10,120] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run74
[2019-04-05 14:02:10,121] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run74
[2019-04-05 14:02:10,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run74
[2019-04-05 14:03:08,151] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11810767]
[2019-04-05 14:03:08,152] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.833333333333333, 54.16666666666666, 109.3333333333333, 789.6666666666667, 19.0, 19.90563681340563, -0.8980815538679314, 0.0, 1.0, 0.0]
[2019-04-05 14:03:08,152] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:03:08,153] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.0538583e-06 8.1083131e-01 5.4087053e-04 1.4948839e-02 1.7361568e-01
 4.6125524e-06 5.5580804e-05], sampled 0.975715733610485
[2019-04-05 14:03:13,215] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11810767]
[2019-04-05 14:03:13,216] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-4.366666666666667, 72.33333333333334, 0.0, 0.0, 19.0, 19.09752462255693, -1.112759649652944, 0.0, 1.0, 0.0]
[2019-04-05 14:03:13,216] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:03:13,217] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.0986135e-05 7.0784205e-01 1.4899742e-03 2.0901065e-02 2.6954296e-01
 1.9401659e-05 1.9348160e-04], sampled 0.5908087860789553
[2019-04-05 14:03:21,963] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6236.8183 144181292.0957 -1866.0402
[2019-04-05 14:03:31,296] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11810767]
[2019-04-05 14:03:31,296] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [13.40101086, 24.99306918, 149.235923935, 234.9721724, 19.0, 22.64569234784729, -0.2827775739755563, 1.0, 1.0, 0.0]
[2019-04-05 14:03:31,296] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:03:31,298] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7337352e-05 9.0505534e-01 6.9773564e-04 6.4081945e-03 8.7544754e-02
 1.0630764e-05 2.5600480e-04], sampled 0.457609355812985
[2019-04-05 14:03:35,104] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5935.2061 173250867.9213 -2342.6363
[2019-04-05 14:03:40,859] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5968.6828 186466440.1277 -2368.8691
[2019-04-05 14:03:41,886] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1460000, evaluation results [1460000.0, 5935.206061267521, 173250867.92134297, -2342.636334838308, 6236.818343620349, 144181292.0956513, -1866.040230807209, 5968.682809093699, 186466440.12772763, -2368.8690955450943]
[2019-04-05 14:03:42,089] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91500, global step 1460116: loss 35.1410
[2019-04-05 14:03:42,090] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91500, global step 1460116: learning rate 0.0000
[2019-04-05 14:03:43,167] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90500, global step 1460752: loss 0.9001
[2019-04-05 14:03:43,169] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90500, global step 1460753: learning rate 0.0000
[2019-04-05 14:03:43,604] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.3102583e-05 8.0428666e-01 1.0811306e-02 5.7909906e-02 1.2566194e-01
 4.6103953e-05 1.2009720e-03], sum to 1.0000
[2019-04-05 14:03:43,608] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4104
[2019-04-05 14:03:43,623] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 18.75934832206367, -1.040802652608475, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3278400.0000, 
sim time next is 3279000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 18.71370776876996, -1.049350841667381, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.05947564739749674, 0.15021638611087304, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5083397], dtype=float32), 1.1139063]. 
=============================================
[2019-04-05 14:03:43,643] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[78.00777 ]
 [78.39394 ]
 [77.75577 ]
 [77.99911 ]
 [78.430916]], R is [[77.89831543]
 [78.11933136]
 [78.266716  ]
 [78.48404694]
 [78.69920349]].
[2019-04-05 14:03:43,744] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91500, global step 1461082: loss 0.6506
[2019-04-05 14:03:43,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91500, global step 1461085: learning rate 0.0000
[2019-04-05 14:03:44,255] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90500, global step 1461382: loss -0.0306
[2019-04-05 14:03:44,257] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90500, global step 1461382: learning rate 0.0000
[2019-04-05 14:03:44,457] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8310702e-06 8.3541131e-01 9.6711492e-06 2.8431569e-03 1.6172668e-01
 2.0631882e-07 7.2041707e-06], sum to 1.0000
[2019-04-05 14:03:44,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6135
[2019-04-05 14:03:44,467] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.833333333333333, 94.16666666666666, 0.0, 0.0, 19.0, 20.1310042421538, -0.7049350221627767, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3197400.0000, 
sim time next is 3198000.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 19.0, 20.06528533049623, -0.7200520061477801, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.1721071108746859, 0.25998266461740666, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4418867], dtype=float32), -1.7616042]. 
=============================================
[2019-04-05 14:03:44,472] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[90.713524]
 [91.92518 ]
 [94.11495 ]
 [94.45739 ]
 [94.841255]], R is [[89.6741333 ]
 [89.77738953]
 [89.87961578]
 [89.9808197 ]
 [90.08100891]].
[2019-04-05 14:03:44,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.09061499e-05 9.74626362e-01 1.56274508e-03 1.10115344e-03
 1.51254395e-02 7.86896853e-05 7.44460803e-03], sum to 1.0000
[2019-04-05 14:03:44,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8899
[2019-04-05 14:03:44,902] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.833333333333333, 28.5, 115.3333333333333, 833.6666666666666, 19.0, 21.4038439333301, -0.595736379984639, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4021800.0000, 
sim time next is 4022400.0000, 
raw observation next is [-3.666666666666667, 28.0, 114.6666666666667, 831.8333333333334, 19.0, 21.45079929036352, -0.5807486488558021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3610341643582641, 0.28, 0.38222222222222235, 0.9191528545119706, 0.08333333333333333, 0.28756660753029334, 0.30641711704806596, 1.0, 1.0, 0.0], 
reward next is 0.1925, 
noisyNet noise sample is [array([0.2711524], dtype=float32), 0.87639815]. 
=============================================
[2019-04-05 14:03:46,415] A3C_AGENT_WORKER-Thread-9 INFO:Local step 90500, global step 1462669: loss 9.7964
[2019-04-05 14:03:46,417] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 90500, global step 1462670: learning rate 0.0000
[2019-04-05 14:03:47,539] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90500, global step 1463392: loss 0.1084
[2019-04-05 14:03:47,541] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90500, global step 1463392: learning rate 0.0000
[2019-04-05 14:03:47,546] A3C_AGENT_WORKER-Thread-4 INFO:Local step 93000, global step 1463393: loss 0.3639
[2019-04-05 14:03:47,547] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 93000, global step 1463394: learning rate 0.0000
[2019-04-05 14:03:47,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0374273e-04 7.4769002e-01 3.6638399e-04 3.7403863e-02 2.1394157e-01
 4.9955212e-05 3.4447422e-04], sum to 1.0000
[2019-04-05 14:03:47,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5938
[2019-04-05 14:03:47,831] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.5, 19.11649600309286, -0.9727349318583096, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3276600.0000, 
sim time next is 3277200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 19.05458434843083, -0.9864656928112686, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.08788202903590243, 0.1711781023962438, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.871872], dtype=float32), -2.7860405]. 
=============================================
[2019-04-05 14:03:47,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3950571e-03 7.2059029e-01 1.3788348e-02 1.2647940e-01 1.3184227e-01
 2.6272816e-04 4.6418463e-03], sum to 1.0000
[2019-04-05 14:03:47,964] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3552
[2019-04-05 14:03:47,980] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 28.5, 120.0, 841.0, 19.0, 21.52427631560529, -0.670899861010699, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4105800.0000, 
sim time next is 4106400.0000, 
raw observation next is [2.333333333333333, 28.66666666666666, 119.3333333333333, 839.1666666666667, 19.0, 21.56343673910407, -0.5809312870551087, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5272391505078486, 0.2866666666666666, 0.3977777777777777, 0.9272559852670351, 0.08333333333333333, 0.2969530615920058, 0.3063562376482971, 1.0, 1.0, 0.0], 
reward next is 0.1907, 
noisyNet noise sample is [array([-0.19965374], dtype=float32), -0.5352499]. 
=============================================
[2019-04-05 14:03:49,840] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91500, global step 1464829: loss 2.0637
[2019-04-05 14:03:49,841] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91500, global step 1464829: learning rate 0.0000
[2019-04-05 14:03:50,755] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90500, global step 1465356: loss 13.1076
[2019-04-05 14:03:50,761] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90500, global step 1465356: learning rate 0.0000
[2019-04-05 14:03:51,138] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91000, global step 1465564: loss 2.3996
[2019-04-05 14:03:51,143] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91000, global step 1465565: learning rate 0.0000
[2019-04-05 14:03:52,272] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92500, global step 1466221: loss 33.2047
[2019-04-05 14:03:52,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92500, global step 1466221: learning rate 0.0000
[2019-04-05 14:03:52,442] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6925111e-06 9.5657903e-01 1.5828013e-03 3.8282375e-03 3.7896991e-02
 1.9460219e-06 1.0240596e-04], sum to 1.0000
[2019-04-05 14:03:52,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1223
[2019-04-05 14:03:52,460] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.833333333333333, 49.5, 29.33333333333333, 275.6666666666666, 19.0, 22.45749515011417, -0.3352386796783685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3517800.0000, 
sim time next is 3518400.0000, 
raw observation next is [2.666666666666667, 50.0, 21.16666666666666, 213.3333333333333, 19.0, 22.68663460559507, -0.3540247226248741, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5364727608494922, 0.5, 0.07055555555555554, 0.2357274401473296, 0.08333333333333333, 0.39055288379958925, 0.38199175912504196, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3247515], dtype=float32), 0.94784683]. 
=============================================
[2019-04-05 14:03:52,858] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90500, global step 1466588: loss 9.9099
[2019-04-05 14:03:52,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90500, global step 1466589: learning rate 0.0000
[2019-04-05 14:03:53,321] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92500, global step 1466877: loss 43.0910
[2019-04-05 14:03:53,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92500, global step 1466877: learning rate 0.0000
[2019-04-05 14:03:53,453] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92000, global step 1466958: loss 0.0783
[2019-04-05 14:03:53,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92000, global step 1466960: learning rate 0.0000
[2019-04-05 14:03:53,509] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92000, global step 1466995: loss 0.0633
[2019-04-05 14:03:53,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92000, global step 1466995: learning rate 0.0000
[2019-04-05 14:03:55,251] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.36936591e-07 6.16225600e-01 1.09193956e-04 3.64465453e-03
 3.30078423e-01 9.10935996e-05 4.98504564e-02], sum to 1.0000
[2019-04-05 14:03:55,257] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7484
[2019-04-05 14:03:55,271] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.333333333333333, 64.0, 0.0, 0.0, 19.0, 20.39306047253073, -0.7354972896368847, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3540000.0000, 
sim time next is 3540600.0000, 
raw observation next is [-1.5, 63.0, 0.0, 0.0, 19.5, 20.38975419030999, -0.747522754400753, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.4210526315789474, 0.63, 0.0, 0.0, 0.125, 0.1991461825258325, 0.2508257485330823, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.7568081], dtype=float32), -1.5048225]. 
=============================================
[2019-04-05 14:03:56,122] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92000, global step 1468550: loss 0.0089
[2019-04-05 14:03:56,124] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92000, global step 1468551: learning rate 0.0000
[2019-04-05 14:03:56,728] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91000, global step 1468934: loss 1.2874
[2019-04-05 14:03:56,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91000, global step 1468936: learning rate 0.0000
[2019-04-05 14:03:57,257] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92000, global step 1469277: loss 0.0076
[2019-04-05 14:03:57,258] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92000, global step 1469278: learning rate 0.0000
[2019-04-05 14:03:57,962] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91000, global step 1469743: loss 2.3391
[2019-04-05 14:03:57,964] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91000, global step 1469743: learning rate 0.0000
[2019-04-05 14:03:58,917] A3C_AGENT_WORKER-Thread-9 INFO:Local step 91000, global step 1470369: loss 13.7599
[2019-04-05 14:03:58,919] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 91000, global step 1470371: learning rate 0.0000
[2019-04-05 14:03:59,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:03:59,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:03:59,695] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run12
[2019-04-05 14:04:00,583] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3726398e-05 7.5755733e-01 9.5012356e-03 3.4979153e-02 1.8914296e-01
 1.6315942e-04 8.5825091e-03], sum to 1.0000
[2019-04-05 14:04:00,586] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3558
[2019-04-05 14:04:00,608] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.966666666666667, 74.33333333333333, 15.33333333333333, 81.83333333333331, 20.0, 19.39936580980718, -1.002563948055121, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4347600.0000, 
sim time next is 4348200.0000, 
raw observation next is [2.983333333333333, 74.16666666666667, 30.66666666666666, 163.6666666666666, 19.0, 19.41820726275425, -0.9907600159748582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5452446906740537, 0.7416666666666667, 0.1022222222222222, 0.18084714548802938, 0.08333333333333333, 0.11818393856285425, 0.16974666134171393, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18173821], dtype=float32), -1.2823365]. 
=============================================
[2019-04-05 14:04:02,125] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91000, global step 1472272: loss 2.1824
[2019-04-05 14:04:02,131] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91000, global step 1472272: learning rate 0.0000
[2019-04-05 14:04:02,823] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92000, global step 1472726: loss 1.5988
[2019-04-05 14:04:02,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92000, global step 1472726: learning rate 0.0000
[2019-04-05 14:04:02,956] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7141337e-06 1.3578105e-01 2.1633964e-03 2.0588802e-02 8.3210492e-01
 3.9149527e-06 9.3521457e-03], sum to 1.0000
[2019-04-05 14:04:02,956] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7491
[2019-04-05 14:04:02,966] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 80.5, 0.0, 0.0, 19.5, 20.86407478968248, -0.6487516816926663, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4746600.0000, 
sim time next is 4747200.0000, 
raw observation next is [-3.0, 79.33333333333333, 0.0, 0.0, 20.0, 20.67317493333751, -0.6770341876000763, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.7933333333333333, 0.0, 0.0, 0.16666666666666666, 0.22276457777812583, 0.27432193746664124, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.6480485], dtype=float32), 0.31043425]. 
=============================================
[2019-04-05 14:04:03,573] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91000, global step 1473197: loss 0.0055
[2019-04-05 14:04:03,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91000, global step 1473197: learning rate 0.0000
[2019-04-05 14:04:03,741] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91500, global step 1473285: loss 0.3352
[2019-04-05 14:04:03,742] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91500, global step 1473285: learning rate 0.0000
[2019-04-05 14:04:04,846] A3C_AGENT_WORKER-Thread-16 INFO:Local step 93000, global step 1473948: loss 0.4329
[2019-04-05 14:04:04,850] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 93000, global step 1473948: learning rate 0.0000
[2019-04-05 14:04:04,939] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92500, global step 1473997: loss 39.5603
[2019-04-05 14:04:04,941] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92500, global step 1473998: learning rate 0.0000
[2019-04-05 14:04:05,445] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92500, global step 1474322: loss 3.4779
[2019-04-05 14:04:05,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92500, global step 1474322: learning rate 0.0000
[2019-04-05 14:04:05,797] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91000, global step 1474564: loss 0.4925
[2019-04-05 14:04:05,798] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91000, global step 1474564: learning rate 0.0000
[2019-04-05 14:04:06,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.0930058e-05 5.2130717e-01 7.1701785e-03 3.7627317e-02 4.3297091e-01
 1.3733722e-04 6.9616211e-04], sum to 1.0000
[2019-04-05 14:04:06,317] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3315
[2019-04-05 14:04:06,331] A3C_AGENT_WORKER-Thread-18 INFO:Local step 93000, global step 1474902: loss 0.0946
[2019-04-05 14:04:06,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 93000, global step 1474904: learning rate 0.0000
[2019-04-05 14:04:06,370] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 72.0, 32.99999999999999, 233.6666666666666, 19.0, 19.72846648548054, -0.9682826229197806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [-4.0, 71.0, 47.0, 282.5, 19.0, 19.86697169986687, -0.959946343468353, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.15666666666666668, 0.31215469613259667, 0.08333333333333333, 0.1555809749889058, 0.180017885510549, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0342071], dtype=float32), -0.18999858]. 
=============================================
[2019-04-05 14:04:06,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[61.68615 ]
 [64.708305]
 [68.1742  ]
 [72.373436]
 [77.61663 ]], R is [[64.69564819]
 [64.0486908 ]
 [63.40820312]
 [62.77412033]
 [62.14638138]].
[2019-04-05 14:04:07,338] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92500, global step 1475531: loss 42.5685
[2019-04-05 14:04:07,340] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92500, global step 1475532: learning rate 0.0000
[2019-04-05 14:04:08,054] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92500, global step 1475979: loss 36.6707
[2019-04-05 14:04:08,055] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92500, global step 1475979: learning rate 0.0000
[2019-04-05 14:04:08,206] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91500, global step 1476065: loss 0.0450
[2019-04-05 14:04:08,207] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91500, global step 1476065: learning rate 0.0000
[2019-04-05 14:04:09,383] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91500, global step 1476777: loss 0.1510
[2019-04-05 14:04:09,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91500, global step 1476777: learning rate 0.0000
[2019-04-05 14:04:11,919] A3C_AGENT_WORKER-Thread-9 INFO:Local step 91500, global step 1478324: loss 22.0514
[2019-04-05 14:04:11,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 91500, global step 1478325: learning rate 0.0000
[2019-04-05 14:04:13,216] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4721818e-04 8.1807083e-01 6.7765103e-04 2.6081340e-02 1.5207252e-01
 9.2936432e-05 2.8574695e-03], sum to 1.0000
[2019-04-05 14:04:13,219] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2648
[2019-04-05 14:04:13,233] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.5, 21.34142748492993, -0.5696103819248685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3954600.0000, 
sim time next is 3955200.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 21.22616418519519, -0.5865966271557096, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.2688470154329326, 0.3044677909480968, 1.0, 1.0, 0.0], 
reward next is 0.1340, 
noisyNet noise sample is [array([0.2316169], dtype=float32), -0.88492423]. 
=============================================
[2019-04-05 14:04:14,073] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91500, global step 1479644: loss 1.9663
[2019-04-05 14:04:14,076] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91500, global step 1479645: learning rate 0.0000
[2019-04-05 14:04:14,394] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92500, global step 1479824: loss 35.0268
[2019-04-05 14:04:14,395] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92500, global step 1479824: learning rate 0.0000
[2019-04-05 14:04:14,701] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-05 14:04:14,704] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:04:14,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:04:14,705] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:04:14,706] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:04:14,706] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:04:14,707] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:04:14,718] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run75
[2019-04-05 14:04:14,736] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run75
[2019-04-05 14:04:14,753] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run75
[2019-04-05 14:04:19,537] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11908513]
[2019-04-05 14:04:19,538] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.566666666666667, 69.66666666666667, 106.3333333333333, 548.8333333333334, 19.0, 19.05592520356686, -1.046091140143079, 0.0, 1.0, 0.0]
[2019-04-05 14:04:19,538] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:04:19,540] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.9769538e-07 8.7035131e-01 2.3683408e-04 1.0867528e-02 1.1851761e-01
 1.3258278e-06 2.4717658e-05], sampled 0.42807549436699954
[2019-04-05 14:04:39,122] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11908513]
[2019-04-05 14:04:39,123] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.133333333333333, 58.66666666666667, 0.0, 0.0, 19.5, 18.96343437650214, -1.073110008434745, 0.0, 1.0, 158230.6604160642]
[2019-04-05 14:04:39,123] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:04:39,124] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.1327158e-06 7.8499669e-01 1.2529313e-03 2.6986590e-02 1.8659768e-01
 1.4118281e-05 1.4487565e-04], sampled 0.6520929589638388
[2019-04-05 14:04:40,612] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11908513]
[2019-04-05 14:04:40,612] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.6, 74.0, 0.0, 0.0, 19.0, 19.02231840451771, -1.086316414718011, 0.0, 1.0, 0.0]
[2019-04-05 14:04:40,612] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:04:40,613] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.6004819e-06 8.4111977e-01 7.4957957e-04 1.2734744e-02 1.4528029e-01
 4.9725713e-06 1.0703825e-04], sampled 0.9039060436413769
[2019-04-05 14:04:46,173] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11908513]
[2019-04-05 14:04:46,173] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.8, 81.0, 0.0, 0.0, 19.0, 18.9385450954587, -0.97499343318947, 0.0, 1.0, 0.0]
[2019-04-05 14:04:46,174] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:04:46,175] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.6055491e-06 8.6914825e-01 4.4031523e-04 8.3568580e-03 1.2199526e-01
 2.0576827e-06 5.5664110e-05], sampled 0.3326146541631594
[2019-04-05 14:04:56,683] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11908513]
[2019-04-05 14:04:56,684] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.1, 69.0, 55.33333333333333, 47.49999999999999, 19.5, 19.2612189294466, -1.148886293241981, 1.0, 1.0, 0.0]
[2019-04-05 14:04:56,684] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:04:56,684] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.3499624e-03 7.9256815e-01 8.1093041e-03 2.2317983e-02 1.6966259e-01
 5.4844981e-04 5.4435614e-03], sampled 0.05188454922300956
[2019-04-05 14:05:25,905] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6212.2168 142819860.6577 -1969.5672
[2019-04-05 14:05:38,003] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11908513]
[2019-04-05 14:05:38,004] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.487670729333333, 83.32895614666667, 0.0, 0.0, 19.5, 18.56579639141827, -1.146347503456868, 0.0, 1.0, 196217.9094192961]
[2019-04-05 14:05:38,004] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:05:38,005] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.3254898e-06 8.0244011e-01 1.1582653e-03 1.4416838e-02 1.8181114e-01
 8.9083160e-06 1.5935309e-04], sampled 0.41102571395053955
[2019-04-05 14:05:38,109] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5882.9739 171928366.1243 -2524.8892
[2019-04-05 14:05:42,382] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5830.7454 185620341.9442 -2510.9902
[2019-04-05 14:05:43,408] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1480000, evaluation results [1480000.0, 5882.973853439287, 171928366.12428656, -2524.889154767574, 6212.216798271341, 142819860.65772808, -1969.567238214136, 5830.7454004268275, 185620341.94420674, -2510.990161979418]
[2019-04-05 14:05:45,172] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91500, global step 1481023: loss 0.1322
[2019-04-05 14:05:45,173] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91500, global step 1481024: learning rate 0.0000
[2019-04-05 14:05:45,986] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:05:45,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:05:46,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run12
[2019-04-05 14:05:46,098] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4315858e-07 8.6367685e-01 7.7122357e-05 1.0617101e-01 2.9982459e-02
 1.7647170e-06 9.0632355e-05], sum to 1.0000
[2019-04-05 14:05:46,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7748
[2019-04-05 14:05:46,107] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.6, 19.33333333333334, 0.0, 0.0, 19.0, 22.52508670549568, -0.3212306044015477, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5091600.0000, 
sim time next is 5092200.0000, 
raw observation next is [8.55, 19.5, 0.0, 0.0, 19.0, 22.39453170171067, -0.3431867864108556, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6994459833795015, 0.195, 0.0, 0.0, 0.08333333333333333, 0.36621097514255574, 0.38560440452971484, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03983385], dtype=float32), 0.08673076]. 
=============================================
[2019-04-05 14:05:46,448] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92000, global step 1481767: loss -0.9537
[2019-04-05 14:05:46,451] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92000, global step 1481768: learning rate 0.0000
[2019-04-05 14:05:46,541] A3C_AGENT_WORKER-Thread-17 INFO:Local step 93000, global step 1481814: loss 0.1648
[2019-04-05 14:05:46,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 93000, global step 1481814: learning rate 0.0000
[2019-04-05 14:05:46,728] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91500, global step 1481916: loss 0.0581
[2019-04-05 14:05:46,729] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91500, global step 1481916: learning rate 0.0000
[2019-04-05 14:05:46,897] A3C_AGENT_WORKER-Thread-3 INFO:Local step 93000, global step 1482018: loss 0.0443
[2019-04-05 14:05:46,899] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 93000, global step 1482020: learning rate 0.0000
[2019-04-05 14:05:47,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:05:47,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:05:47,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run12
[2019-04-05 14:05:48,219] A3C_AGENT_WORKER-Thread-12 INFO:Local step 93000, global step 1482793: loss 0.0310
[2019-04-05 14:05:48,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 93000, global step 1482794: learning rate 0.0000
[2019-04-05 14:05:48,320] A3C_AGENT_WORKER-Thread-15 INFO:Local step 93000, global step 1482852: loss -0.3934
[2019-04-05 14:05:48,322] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 93000, global step 1482853: learning rate 0.0000
[2019-04-05 14:05:49,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9631006e-06 4.4190571e-01 6.1443844e-04 4.9362692e-01 6.2889129e-02
 9.2348419e-06 9.4461569e-04], sum to 1.0000
[2019-04-05 14:05:49,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2378
[2019-04-05 14:05:49,770] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.666666666666667, 44.66666666666667, 223.8333333333333, 382.0, 19.0, 20.21443614426842, -0.7919375012791073, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4891200.0000, 
sim time next is 4891800.0000, 
raw observation next is [2.833333333333333, 44.83333333333333, 211.6666666666667, 390.0, 19.0, 20.21306501714758, -0.7896366477953921, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.541089566020314, 0.4483333333333333, 0.7055555555555557, 0.430939226519337, 0.08333333333333333, 0.18442208476229846, 0.23678778406820264, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09072261], dtype=float32), 1.9508848]. 
=============================================
[2019-04-05 14:05:50,266] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92000, global step 1483871: loss -2.9783
[2019-04-05 14:05:50,267] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92000, global step 1483871: learning rate 0.0000
[2019-04-05 14:05:51,987] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92000, global step 1484783: loss -0.3293
[2019-04-05 14:05:51,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92000, global step 1484784: learning rate 0.0000
[2019-04-05 14:05:52,138] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.23710173e-06 6.01506829e-01 2.90973717e-03 1.61049038e-01
 2.33325064e-01 1.07911044e-04 1.09417865e-03], sum to 1.0000
[2019-04-05 14:05:52,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1424
[2019-04-05 14:05:52,154] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.833333333333334, 54.83333333333334, 176.3333333333333, 676.6666666666666, 20.0, 19.6495798664448, -0.9016719728153154, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4272600.0000, 
sim time next is 4273200.0000, 
raw observation next is [5.0, 55.0, 162.5, 713.0, 19.0, 19.66942161993207, -0.8940054552685136, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6011080332409973, 0.55, 0.5416666666666666, 0.7878453038674034, 0.08333333333333333, 0.1391184683276725, 0.20199818157716212, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.648828], dtype=float32), -0.816963]. 
=============================================
[2019-04-05 14:05:52,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6431352e-08 9.3963718e-01 3.5127614e-05 8.3316257e-03 5.1980920e-02
 1.0257005e-07 1.4957410e-05], sum to 1.0000
[2019-04-05 14:05:52,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1544
[2019-04-05 14:05:52,784] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 20.05160412519272, -0.9102128671066856, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4246200.0000, 
sim time next is 4246800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 19.98607342692545, -0.9222620241700614, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.16550611891045422, 0.1925793252766462, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.20219], dtype=float32), -0.5306276]. 
=============================================
[2019-04-05 14:05:53,225] A3C_AGENT_WORKER-Thread-9 INFO:Local step 92000, global step 1485590: loss 18.2777
[2019-04-05 14:05:53,226] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 92000, global step 1485592: learning rate 0.0000
[2019-04-05 14:05:54,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7136432e-05 9.7137862e-01 4.0050896e-04 5.3336372e-04 2.7640903e-02
 3.3900929e-07 2.9167491e-05], sum to 1.0000
[2019-04-05 14:05:54,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5544
[2019-04-05 14:05:54,820] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.9, 68.33333333333334, 0.0, 0.0, 19.0, 18.75004534850236, -1.183387711841786, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 775200.0000, 
sim time next is 775800.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 19.0, 18.66544435848671, -1.191408670758632, 0.0, 1.0, 104430.170665345], 
processed observation next is [1.0, 1.0, 0.2686980609418283, 0.69, 0.0, 0.0, 0.08333333333333333, 0.05545369654055913, 0.10286377641378934, 0.0, 1.0, 0.4972865269778333], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9958307], dtype=float32), 0.37449977]. 
=============================================
[2019-04-05 14:05:55,175] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6384296e-04 2.8644666e-01 1.9499083e-03 7.8880481e-02 6.3128012e-01
 1.4846468e-04 1.0304927e-03], sum to 1.0000
[2019-04-05 14:05:55,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0490
[2019-04-05 14:05:55,189] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.43333333333333, 40.66666666666666, 112.3333333333333, 745.6666666666667, 19.5, 22.00064285942247, -0.5041145037425466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4356600.0000, 
sim time next is 4357200.0000, 
raw observation next is [10.86666666666667, 39.33333333333334, 113.6666666666667, 762.8333333333333, 19.0, 22.13220317300513, -0.4737409381372868, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7636195752539245, 0.3933333333333334, 0.378888888888889, 0.8429097605893185, 0.08333333333333333, 0.3443502644170942, 0.34208635395423775, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8827521], dtype=float32), 0.19821766]. 
=============================================
[2019-04-05 14:05:55,201] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92000, global step 1486768: loss 0.3874
[2019-04-05 14:05:55,204] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92000, global step 1486768: learning rate 0.0000
[2019-04-05 14:05:55,703] A3C_AGENT_WORKER-Thread-10 INFO:Local step 93000, global step 1487080: loss -0.1805
[2019-04-05 14:05:55,705] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 93000, global step 1487080: learning rate 0.0000
[2019-04-05 14:05:56,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2206004e-05 5.1424724e-01 1.2842714e-03 2.4825251e-02 4.5810559e-01
 2.4140490e-05 1.4812092e-03], sum to 1.0000
[2019-04-05 14:05:56,610] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4317
[2019-04-05 14:05:56,618] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 20.0, 19.63837104837926, -0.9493812354498369, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5025600.0000, 
sim time next is 5026200.0000, 
raw observation next is [-1.0, 54.16666666666667, 0.0, 0.0, 19.0, 19.57861004146894, -0.9522783861054679, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.5416666666666667, 0.0, 0.0, 0.08333333333333333, 0.13155083678907845, 0.18257387129817737, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.764172], dtype=float32), -0.18661852]. 
=============================================
[2019-04-05 14:05:57,342] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92500, global step 1488102: loss 22.8303
[2019-04-05 14:05:57,343] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92500, global step 1488102: learning rate 0.0000
[2019-04-05 14:05:57,875] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92000, global step 1488377: loss 0.5548
[2019-04-05 14:05:57,877] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92000, global step 1488377: learning rate 0.0000
[2019-04-05 14:05:58,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:05:58,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:05:58,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run12
[2019-04-05 14:05:58,983] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:05:58,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:05:58,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run12
[2019-04-05 14:06:00,885] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92000, global step 1490020: loss -0.3070
[2019-04-05 14:06:00,886] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92000, global step 1490020: learning rate 0.0000
[2019-04-05 14:06:00,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:06:00,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:06:00,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run12
[2019-04-05 14:06:01,293] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:06:01,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:06:01,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run12
[2019-04-05 14:06:01,390] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92500, global step 1490232: loss 22.7733
[2019-04-05 14:06:01,391] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92500, global step 1490232: learning rate 0.0000
[2019-04-05 14:06:03,189] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92500, global step 1491066: loss 37.6618
[2019-04-05 14:06:03,191] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92500, global step 1491067: learning rate 0.0000
[2019-04-05 14:06:05,001] A3C_AGENT_WORKER-Thread-9 INFO:Local step 92500, global step 1491909: loss 13.7062
[2019-04-05 14:06:05,002] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 92500, global step 1491911: learning rate 0.0000
[2019-04-05 14:06:05,862] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92500, global step 1492385: loss 42.1713
[2019-04-05 14:06:05,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92500, global step 1492386: learning rate 0.0000
[2019-04-05 14:06:08,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:06:08,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:06:08,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run12
[2019-04-05 14:06:08,485] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92500, global step 1494032: loss 35.0336
[2019-04-05 14:06:08,488] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92500, global step 1494032: learning rate 0.0000
[2019-04-05 14:06:10,846] A3C_AGENT_WORKER-Thread-5 INFO:Local step 93000, global step 1495218: loss 0.1040
[2019-04-05 14:06:10,847] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 93000, global step 1495218: learning rate 0.0000
[2019-04-05 14:06:10,947] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3370492e-06 9.8848152e-01 7.1685536e-05 7.4203699e-03 3.9742249e-03
 7.2831455e-08 5.0702372e-05], sum to 1.0000
[2019-04-05 14:06:10,949] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7532
[2019-04-05 14:06:10,973] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 19.0, 22.30677539269512, -0.346407907398642, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4659000.0000, 
sim time next is 4659600.0000, 
raw observation next is [2.0, 57.00000000000001, 0.0, 0.0, 19.0, 22.25030918746069, -0.3710133442218368, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.5700000000000001, 0.0, 0.0, 0.08333333333333333, 0.35419243228839087, 0.37632888525938774, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9318557], dtype=float32), -0.3594097]. 
=============================================
[2019-04-05 14:06:11,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1726991e-05 4.7979832e-01 3.7684478e-03 3.4163821e-02 4.8186043e-01
 9.3045383e-05 2.6415431e-04], sum to 1.0000
[2019-04-05 14:06:11,560] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9403
[2019-04-05 14:06:11,585] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.3333333333333333, 87.0, 0.0, 0.0, 19.0, 20.22913743954409, -0.8137848400855855, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4679400.0000, 
sim time next is 4680000.0000, 
raw observation next is [0.0, 92.0, 0.0, 0.0, 19.5, 20.11767636288532, -0.8194430885875589, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.46260387811634357, 0.92, 0.0, 0.0, 0.125, 0.17647303024044328, 0.22685230380414703, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.7424144], dtype=float32), 1.4349054]. 
=============================================
[2019-04-05 14:06:11,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.605576]
 [83.35977 ]
 [83.06155 ]
 [83.089516]
 [82.768   ]], R is [[84.13356781]
 [84.29223633]
 [84.30645752]
 [84.39196777]
 [84.54804993]].
[2019-04-05 14:06:11,736] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5092834e-04 8.5358793e-01 3.3957234e-03 4.3934811e-02 8.1839897e-02
 6.3116907e-04 1.6259588e-02], sum to 1.0000
[2019-04-05 14:06:11,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3922
[2019-04-05 14:06:11,763] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-14.75, 69.0, 0.0, 0.0, 19.5, 18.52676590435425, -1.271286367146397, 0.0, 1.0, 112761.0073880921], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 351000.0000, 
sim time next is 351600.0000, 
raw observation next is [-14.83333333333333, 69.0, 0.0, 0.0, 19.0, 18.53269949697005, -1.267410705150366, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.05170821791320413, 0.69, 0.0, 0.0, 0.08333333333333333, 0.04439162474750417, 0.077529764949878, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2587658], dtype=float32), -0.30577508]. 
=============================================
[2019-04-05 14:06:12,062] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92500, global step 1495902: loss 20.8955
[2019-04-05 14:06:12,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92500, global step 1495903: learning rate 0.0000
[2019-04-05 14:06:13,452] A3C_AGENT_WORKER-Thread-2 INFO:Local step 93000, global step 1496658: loss 0.0339
[2019-04-05 14:06:13,454] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 93000, global step 1496658: learning rate 0.0000
[2019-04-05 14:06:15,441] A3C_AGENT_WORKER-Thread-13 INFO:Local step 93000, global step 1497838: loss -0.4330
[2019-04-05 14:06:15,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 93000, global step 1497839: learning rate 0.0000
[2019-04-05 14:06:17,289] A3C_AGENT_WORKER-Thread-9 INFO:Local step 93000, global step 1498831: loss 1.9527
[2019-04-05 14:06:17,290] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 93000, global step 1498832: learning rate 0.0000
[2019-04-05 14:06:17,735] A3C_AGENT_WORKER-Thread-11 INFO:Local step 93000, global step 1499075: loss -0.1782
[2019-04-05 14:06:17,738] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 93000, global step 1499076: learning rate 0.0000
[2019-04-05 14:06:19,453] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-05 14:06:19,455] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:06:19,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:06:19,457] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:06:19,459] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:06:19,490] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:06:19,520] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:06:20,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run76
[2019-04-05 14:06:21,077] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run76
[2019-04-05 14:06:21,097] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run76
[2019-04-05 14:06:49,712] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.119140215]
[2019-04-05 14:06:49,712] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [5.0, 79.0, 0.0, 0.0, 19.0, 18.84731698487994, -1.014083015305697, 0.0, 1.0, 0.0]
[2019-04-05 14:06:49,713] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:06:49,714] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.50447204e-06 7.40725338e-01 3.83757841e-04 1.73671264e-02
 2.41404787e-01 4.34000685e-06 1.13168244e-04], sampled 0.3043023595225053
[2019-04-05 14:07:03,827] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.119140215]
[2019-04-05 14:07:03,827] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-4.4, 76.0, 0.0, 0.0, 19.0, 18.9708702932409, -1.17742733474535, 0.0, 1.0, 0.0]
[2019-04-05 14:07:03,827] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:07:03,828] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.0474864e-05 6.7607373e-01 1.5871173e-03 3.1108703e-02 2.9084033e-01
 2.3279585e-05 3.5634174e-04], sampled 0.9175758803855554
[2019-04-05 14:07:33,822] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6334.5593 143980070.2992 -1829.2266
[2019-04-05 14:07:45,179] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5965.6468 174031919.4105 -2366.9161
[2019-04-05 14:07:51,949] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5946.0367 187003850.2773 -2319.6747
[2019-04-05 14:07:52,975] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1500000, evaluation results [1500000.0, 5965.646803507005, 174031919.41046983, -2366.916089848006, 6334.5593258329745, 143980070.2991969, -1829.2265570050865, 5946.03667949152, 187003850.27730635, -2319.6747284490816]
[2019-04-05 14:07:54,853] A3C_AGENT_WORKER-Thread-20 INFO:Local step 93000, global step 1501022: loss -0.0650
[2019-04-05 14:07:54,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 93000, global step 1501022: learning rate 0.0000
[2019-04-05 14:07:57,305] A3C_AGENT_WORKER-Thread-19 INFO:Local step 93000, global step 1502486: loss 0.7088
[2019-04-05 14:07:57,305] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 93000, global step 1502486: learning rate 0.0000
[2019-04-05 14:07:57,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:07:57,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:07:57,391] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run12
[2019-04-05 14:07:57,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:07:57,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:07:57,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run12
[2019-04-05 14:08:00,383] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:08:00,383] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:00,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run12
[2019-04-05 14:08:02,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:08:02,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:02,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run12
[2019-04-05 14:08:03,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:08:03,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:03,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run12
[2019-04-05 14:08:04,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7775517e-04 8.5311300e-01 1.7493133e-03 8.9138950e-04 1.4151505e-01
 3.7341483e-06 2.5497742e-03], sum to 1.0000
[2019-04-05 14:08:04,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2703
[2019-04-05 14:08:04,498] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 19.0, 23.52090419594892, -0.1130776683412528, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [11.5, 19.5, 111.0, 819.0, 19.0, 23.63569273101329, -0.08919444789837916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7811634349030472, 0.195, 0.37, 0.9049723756906077, 0.08333333333333333, 0.46964106091777413, 0.47026851736720693, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6938796], dtype=float32), 0.3856855]. 
=============================================
[2019-04-05 14:08:05,647] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5961840e-07 4.5641425e-01 6.1198283e-05 2.3400029e-03 5.4101688e-01
 1.3348055e-06 1.6584252e-04], sum to 1.0000
[2019-04-05 14:08:05,648] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7401
[2019-04-05 14:08:05,653] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [11.6, 77.0, 0.0, 0.0, 20.0, 21.54169654432116, -0.3698554053131184, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [11.6, 78.0, 0.0, 0.0, 20.5, 21.52149201463675, -0.3723310275825927, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.78, 0.0, 0.0, 0.20833333333333334, 0.29345766788639577, 0.3758896574724691, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.3074711], dtype=float32), -0.1501909]. 
=============================================
[2019-04-05 14:08:06,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:08:06,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:06,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run12
[2019-04-05 14:08:07,560] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9086836e-04 8.8198483e-01 2.0049373e-02 4.0957080e-03 8.2901046e-02
 2.1976803e-03 8.0804052e-03], sum to 1.0000
[2019-04-05 14:08:07,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8061
[2019-04-05 14:08:07,613] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.666666666666666, 28.66666666666667, 106.0, 0.0, 19.0, 20.53873187221463, -0.9727916399058145, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 471000.0000, 
sim time next is 471600.0000, 
raw observation next is [-2.3, 28.0, 110.0, 0.0, 19.0, 20.54924412112032, -0.9685366573167139, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3988919667590028, 0.28, 0.36666666666666664, 0.0, 0.08333333333333333, 0.21243701009336005, 0.17715444756109536, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7050011], dtype=float32), 0.37231895]. 
=============================================
[2019-04-05 14:08:09,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:08:09,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:09,044] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run12
[2019-04-05 14:08:16,718] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0659702e-06 9.9476552e-01 1.7213495e-04 6.4961257e-04 3.7119985e-03
 2.7203089e-06 6.9397624e-04], sum to 1.0000
[2019-04-05 14:08:16,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2420
[2019-04-05 14:08:16,731] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 19.5, 19.79658112299021, -0.9906570466076033, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 699600.0000, 
sim time next is 700200.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 19.0, 19.85692166889046, -0.9859175096557763, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.08333333333333333, 0.1547434724075384, 0.17136083011474124, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.747277], dtype=float32), 0.21646443]. 
=============================================
[2019-04-05 14:08:21,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0491249e-03 5.9659588e-01 8.9126723e-03 2.7721646e-01 1.1282141e-01
 4.6902496e-04 2.9353141e-03], sum to 1.0000
[2019-04-05 14:08:21,428] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3154
[2019-04-05 14:08:21,450] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.0, 68.5, 0.0, 0.0, 19.5, 18.75994919170006, -1.207327967935046, 0.0, 1.0, 51102.66964480675], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 282600.0000, 
sim time next is 283200.0000, 
raw observation next is [-12.1, 68.0, 0.0, 0.0, 19.0, 18.7419664794479, -1.216050335697712, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.12742382271468145, 0.68, 0.0, 0.0, 0.08333333333333333, 0.061830539953991646, 0.09464988810076269, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24585696], dtype=float32), -0.8570766]. 
=============================================
[2019-04-05 14:08:22,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.5754017e-04 6.6680425e-01 2.8680931e-03 9.0383861e-04 3.2605475e-01
 2.3009077e-04 2.3814135e-03], sum to 1.0000
[2019-04-05 14:08:22,167] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4693
[2019-04-05 14:08:22,235] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.616666666666667, 61.0, 136.0, 523.6666666666666, 24.0, 22.71673467796332, -0.1612656009903581, 1.0, 1.0, 200854.2610301448], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 133800.0000, 
sim time next is 134400.0000, 
raw observation next is [-7.433333333333334, 61.0, 137.5, 503.8333333333334, 23.0, 23.26514204495376, -0.07938660180900785, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.2566943674976916, 0.61, 0.4583333333333333, 0.5567219152854513, 0.4166666666666667, 0.4387618370794799, 0.47353779939699736, 1.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([-0.42764515], dtype=float32), 0.013721055]. 
=============================================
[2019-04-05 14:08:22,824] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9412572e-03 7.8264159e-01 9.8306192e-03 9.1220997e-03 1.9176504e-01
 3.6833316e-04 4.3309331e-03], sum to 1.0000
[2019-04-05 14:08:22,825] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3104
[2019-04-05 14:08:22,858] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 77.66666666666667, 96.33333333333334, 0.0, 19.5, 18.94687009131922, -1.167618761413512, 1.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 823800.0000, 
sim time next is 824400.0000, 
raw observation next is [-4.5, 79.0, 95.0, 0.0, 19.0, 19.11380265810108, -1.135019390065059, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.31666666666666665, 0.0, 0.08333333333333333, 0.09281688817508993, 0.12166020331164702, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3030672], dtype=float32), 0.53380287]. 
=============================================
[2019-04-05 14:08:24,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4371020e-04 5.2890879e-01 1.1319762e-02 1.5679009e-02 4.3947256e-01
 3.6297340e-04 3.5132093e-03], sum to 1.0000
[2019-04-05 14:08:24,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2734
[2019-04-05 14:08:24,629] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 22.5, 18.15210361277411, -1.366391668520003, 0.0, 1.0, 51849.80997637991], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 367800.0000, 
sim time next is 368400.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 21.5, 18.16656382844414, -1.36017470824673, 0.0, 1.0, 51881.42433952251], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.2916666666666667, 0.013880319037011665, 0.046608430584423356, 0.0, 1.0, 0.24705440161677386], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.19857311], dtype=float32), 1.0850434]. 
=============================================
[2019-04-05 14:08:24,856] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.01663202 0.6255587  0.01220997 0.02976262 0.3064526  0.00204848
 0.00733565], sum to 1.0000
[2019-04-05 14:08:24,856] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0960
[2019-04-05 14:08:24,876] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.6, 50.83333333333334, 98.33333333333333, 691.3333333333334, 19.0, 21.27947363789559, -0.7075031806269853, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 301800.0000, 
sim time next is 302400.0000, 
raw observation next is [-10.6, 49.0, 94.5, 708.0, 19.0, 21.23660572579263, -0.7105100787111166, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.1689750692520776, 0.49, 0.315, 0.7823204419889502, 0.08333333333333333, 0.2697171438160524, 0.2631633070962945, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0594504], dtype=float32), -0.18326244]. 
=============================================
[2019-04-05 14:08:28,828] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.01211162 0.57224554 0.01817567 0.02272231 0.369825   0.00083584
 0.00408401], sum to 1.0000
[2019-04-05 14:08:28,830] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3980
[2019-04-05 14:08:28,836] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 45.0, 76.5, 17.0, 19.5, 20.6477742123223, -0.8462380123784726, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 748800.0000, 
sim time next is 749400.0000, 
raw observation next is [-0.9666666666666666, 46.5, 73.66666666666666, 12.33333333333333, 19.0, 20.72896358411118, -0.8345927809330567, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.43582640812557716, 0.465, 0.24555555555555553, 0.013627992633517492, 0.08333333333333333, 0.2274136320092651, 0.22180240635564777, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.13160744], dtype=float32), 0.6357842]. 
=============================================
[2019-04-05 14:08:32,329] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4167572e-06 2.8011471e-01 3.6684496e-03 1.6411346e-01 5.5176985e-01
 3.8814060e-06 3.2022197e-04], sum to 1.0000
[2019-04-05 14:08:32,330] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2104
[2019-04-05 14:08:32,348] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.333333333333333, 80.66666666666666, 0.0, 0.0, 22.5, 21.16715928989271, -0.6169061244366983, 0.0, 1.0, 41523.85985133343], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 960000.0000, 
sim time next is 960600.0000, 
raw observation next is [7.516666666666667, 80.33333333333334, 0.0, 0.0, 21.5, 21.18947494818165, -0.618883486884194, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6708217913204063, 0.8033333333333335, 0.0, 0.0, 0.2916666666666667, 0.2657895790151376, 0.2937055043719353, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.34705102], dtype=float32), -0.8264611]. 
=============================================
[2019-04-05 14:08:32,957] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-05 14:08:32,958] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:08:32,958] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:08:32,958] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:32,959] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:32,959] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:08:32,960] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:08:32,966] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run77
[2019-04-05 14:08:32,986] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run77
[2019-04-05 14:08:33,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run77
[2019-04-05 14:08:48,176] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11707865]
[2019-04-05 14:08:48,176] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.966666666666667, 86.66666666666666, 0.0, 0.0, 20.0, 19.24728893347476, -1.075829480456106, 0.0, 1.0, 168437.7998588137]
[2019-04-05 14:08:48,176] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:08:48,177] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.6878864e-06 7.5421625e-01 4.6171961e-04 1.3346758e-02 2.3183611e-01
 5.6928370e-06 1.3185917e-04], sampled 0.6456957100425218
[2019-04-05 14:08:59,739] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11707865]
[2019-04-05 14:08:59,739] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.0, 92.0, 0.0, 0.0, 19.0, 19.14523400870004, -0.8857918065916647, 0.0, 1.0, 0.0]
[2019-04-05 14:08:59,739] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:08:59,739] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.6446605e-06 7.4300945e-01 7.0761429e-04 1.5715715e-02 2.4037109e-01
 8.6740556e-06 1.8483882e-04], sampled 0.8632445993257211
[2019-04-05 14:09:30,165] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11707865]
[2019-04-05 14:09:30,165] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.421391408, 60.56878711, 153.4125014666666, 259.1827753666666, 19.0, 19.29525427092694, -1.091811624311396, 1.0, 1.0, 0.0]
[2019-04-05 14:09:30,165] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:09:30,166] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.00191293 0.59530103 0.01750787 0.04564758 0.32796496 0.0009661
 0.01069954], sampled 0.29767070347945135
[2019-04-05 14:09:44,789] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6361.5779 146398391.6083 -1791.7650
[2019-04-05 14:09:57,354] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5945.9583 172333193.7248 -2318.2411
[2019-04-05 14:10:03,610] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5993.0434 187728694.3384 -2315.2598
[2019-04-05 14:10:04,636] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1520000, evaluation results [1520000.0, 5945.958344596324, 172333193.7247656, -2318.241112899586, 6361.577911621531, 146398391.6083487, -1791.7650280290486, 5993.043387278985, 187728694.33839387, -2315.2597530078842]
[2019-04-05 14:10:05,327] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.19168403e-04 8.81486297e-01 3.45676439e-03 3.38702532e-03
 1.06031045e-01 6.44559856e-04 4.77509340e-03], sum to 1.0000
[2019-04-05 14:10:05,328] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-05 14:10:05,402] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-16.36666666666667, 79.0, 0.0, 0.0, 19.0, 17.99672794802503, -1.306604579681309, 1.0, 1.0, 202146.238767552], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 372000.0000, 
sim time next is 372600.0000, 
raw observation next is [-16.45, 79.5, 0.0, 0.0, 19.0, 18.49500127951536, -1.226720221505871, 1.0, 1.0, 101918.6941580256], 
processed observation next is [1.0, 0.30434782608695654, 0.006925207756232688, 0.795, 0.0, 0.0, 0.08333333333333333, 0.041250106626280036, 0.09109325949804299, 1.0, 1.0, 0.4853271150382171], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21414268], dtype=float32), 0.7770419]. 
=============================================
[2019-04-05 14:10:06,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8940299e-05 7.7223635e-01 4.4423514e-03 1.6138360e-02 2.0702302e-01
 2.3174855e-06 1.0860531e-04], sum to 1.0000
[2019-04-05 14:10:06,372] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3620
[2019-04-05 14:10:06,379] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.183333333333333, 91.5, 0.0, 0.0, 19.0, 19.18750042929624, -0.9407003493804003, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1461000.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 19.0, 19.1849454218494, -0.9480491254991769, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.08333333333333333, 0.09874545182078324, 0.18398362483360772, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24949133], dtype=float32), -0.5280675]. 
=============================================
[2019-04-05 14:10:13,070] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9714068e-07 7.7300525e-01 2.1909645e-03 3.6167596e-02 1.8736294e-01
 1.4789095e-05 1.2578268e-03], sum to 1.0000
[2019-04-05 14:10:13,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5729
[2019-04-05 14:10:13,077] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 21.10591954462619, -0.4408928215445605, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1236000.0000, 
sim time next is 1236600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 21.09592194957148, -0.4435782174366341, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.2579934957976233, 0.35214059418778865, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95245254], dtype=float32), -0.22440682]. 
=============================================
[2019-04-05 14:10:14,736] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3744931e-06 7.9156607e-01 4.0902116e-04 2.0575077e-04 2.0778210e-01
 2.4686369e-06 3.0265719e-05], sum to 1.0000
[2019-04-05 14:10:14,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6317
[2019-04-05 14:10:14,756] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 19.16383822969392, -0.9576236561604694, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1387800.0000, 
sim time next is 1388400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 19.06674675826328, -0.9780121099049236, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.08889556318860681, 0.17399596336502546, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.67713255], dtype=float32), 0.21960483]. 
=============================================
[2019-04-05 14:10:14,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0556074e-07 3.0982113e-01 3.8187595e-03 1.0745877e-02 6.7511171e-01
 5.5336582e-06 4.9652619e-04], sum to 1.0000
[2019-04-05 14:10:14,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2960
[2019-04-05 14:10:14,991] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.883333333333333, 86.33333333333334, 0.0, 0.0, 19.5, 19.39791494209247, -1.073954178973562, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 528600.0000, 
sim time next is 529200.0000, 
raw observation next is [3.8, 86.0, 0.0, 0.0, 19.0, 19.31375993554898, -1.08911763501746, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.5678670360110805, 0.86, 0.0, 0.0, 0.08333333333333333, 0.10947999462908165, 0.13696078832751332, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.87788355], dtype=float32), 1.070102]. 
=============================================
[2019-04-05 14:10:20,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0272317e-04 8.7799811e-01 1.5617125e-03 5.2193464e-03 1.1063718e-01
 1.1107082e-03 3.1702346e-03], sum to 1.0000
[2019-04-05 14:10:20,421] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6752
[2019-04-05 14:10:20,433] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 19.0, 18.68140652546881, -1.227897067659719, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 802200.0000, 
sim time next is 802800.0000, 
raw observation next is [-6.7, 67.0, 0.0, 0.0, 19.0, 18.67598446591555, -1.236765911526979, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.67, 0.0, 0.0, 0.08333333333333333, 0.05633203882629593, 0.0877446961576737, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0288057], dtype=float32), -0.5169102]. 
=============================================
[2019-04-05 14:10:30,017] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9332876e-06 9.8974413e-01 5.9594330e-04 1.5107743e-03 7.8078369e-03
 5.3221869e-05 2.8409899e-04], sum to 1.0000
[2019-04-05 14:10:30,019] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2380
[2019-04-05 14:10:30,029] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 19.58657422355166, -0.9484553379654992, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 943800.0000, 
sim time next is 944400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 19.55466671851233, -0.9569525456002762, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.1295555598760275, 0.18101581813324127, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5488315], dtype=float32), 0.027988374]. 
=============================================
[2019-04-05 14:10:31,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3078180e-05 1.7247781e-01 8.7983683e-03 7.3419064e-02 7.4312812e-01
 9.3743702e-06 2.1141774e-03], sum to 1.0000
[2019-04-05 14:10:31,965] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3871
[2019-04-05 14:10:32,008] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.8, 85.0, 99.0, 0.0, 21.5, 22.43184566152199, -0.3808503012527418, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 1780200.0000, 
sim time next is 1780800.0000, 
raw observation next is [-2.8, 85.66666666666667, 93.5, 0.0, 22.0, 22.30451706775579, -0.4033896864985612, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8566666666666667, 0.31166666666666665, 0.0, 0.3333333333333333, 0.35870975564631574, 0.3655367711671462, 0.0, 1.0, 0.0], 
reward next is 0.5714, 
noisyNet noise sample is [array([-0.13729236], dtype=float32), 1.9907779]. 
=============================================
[2019-04-05 14:10:34,630] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0631788e-05 9.2737228e-01 2.8312468e-04 1.1772109e-02 5.9463739e-02
 3.2114890e-06 1.0549859e-03], sum to 1.0000
[2019-04-05 14:10:34,630] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2428
[2019-04-05 14:10:34,635] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.3, 65.0, 143.5, 0.0, 19.0, 21.77121671808115, -0.2978632749658749, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1173600.0000, 
sim time next is 1174200.0000, 
raw observation next is [18.3, 65.0, 138.3333333333333, 0.0, 19.0, 21.79321098218481, -0.2961417270135632, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.46111111111111097, 0.0, 0.08333333333333333, 0.3161009151820675, 0.40128609099547896, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6892581], dtype=float32), 0.36089802]. 
=============================================
[2019-04-05 14:10:35,280] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.7750541e-05 4.5993420e-01 5.2529816e-02 9.7905146e-03 4.7673813e-01
 2.8018983e-05 8.8156905e-04], sum to 1.0000
[2019-04-05 14:10:35,282] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7707
[2019-04-05 14:10:35,289] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.5, 21.66137919853204, -0.3358136336705002, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1193400.0000, 
sim time next is 1194000.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 20.0, 21.64643459349442, -0.3385478594292235, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.16666666666666666, 0.30386954945786826, 0.38715071352359215, 0.0, 0.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.2920731], dtype=float32), 0.119936846]. 
=============================================
[2019-04-05 14:10:35,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.736244]
 [80.77547 ]
 [80.54131 ]
 [80.47416 ]
 [80.38807 ]], R is [[80.73978424]
 [80.86096191]
 [81.05235291]
 [81.24182892]
 [81.28655243]].
[2019-04-05 14:10:37,303] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4691603e-06 9.2836338e-01 3.7425209e-04 3.8372073e-02 2.7759906e-02
 7.0254799e-05 5.0576101e-03], sum to 1.0000
[2019-04-05 14:10:37,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7171
[2019-04-05 14:10:37,311] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 20.77135249278781, -0.5175513375913326, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 20.75436202912204, -0.5204456996562875, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.2295301690935032, 0.32651810011457083, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6417977], dtype=float32), -0.33532518]. 
=============================================
[2019-04-05 14:10:39,705] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 14:10:39,706] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:10:39,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:10:39,709] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:10:39,710] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:10:39,710] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:10:39,710] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:10:39,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run78
[2019-04-05 14:10:39,749] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run78
[2019-04-05 14:10:39,765] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run78
[2019-04-05 14:11:25,345] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11708457]
[2019-04-05 14:11:25,345] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.934020221333334, 81.80253991000001, 0.0, 0.0, 19.5, 18.48813551225444, -1.191641849155887, 0.0, 1.0, 108758.8837444722]
[2019-04-05 14:11:25,346] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:11:25,347] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.2491371e-05 7.5051010e-01 2.2048401e-03 1.5709516e-02 2.3051304e-01
 4.8256563e-05 9.9177030e-04], sampled 0.8552130878373533
[2019-04-05 14:11:50,756] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6253.6208 144663373.8031 -1883.9535
[2019-04-05 14:12:03,506] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5886.7189 172114460.2377 -2410.5597
[2019-04-05 14:12:09,267] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5850.2522 187442308.3289 -2368.9804
[2019-04-05 14:12:10,293] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1540000, evaluation results [1540000.0, 5886.71892333566, 172114460.23774484, -2410.5596818410013, 6253.620776574316, 144663373.80312818, -1883.9535463713644, 5850.252232988999, 187442308.3289104, -2368.9803526233854]
[2019-04-05 14:12:10,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1152671e-06 3.3509880e-01 5.6002680e-03 5.8732474e-01 6.9367468e-02
 1.2594326e-05 2.5920318e-03], sum to 1.0000
[2019-04-05 14:12:10,614] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4383
[2019-04-05 14:12:10,625] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 19.0, 20.88749415893726, -0.5477237570159202, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [2.1, 92.0, 0.0, 0.0, 19.0, 20.84849848711583, -0.5561530624894429, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5207756232686982, 0.92, 0.0, 0.0, 0.08333333333333333, 0.23737487392631915, 0.31461564583685236, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2787162], dtype=float32), 0.84807754]. 
=============================================
[2019-04-05 14:12:10,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.48054 ]
 [89.94948 ]
 [90.23061 ]
 [90.639336]
 [90.91859 ]], R is [[89.20545197]
 [89.31340027]
 [89.4202652 ]
 [89.52606201]
 [89.63080597]].
[2019-04-05 14:12:20,449] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2308488e-03 9.0651709e-01 2.0291458e-03 3.7860586e-03 7.8785144e-02
 2.4464127e-04 2.4071655e-03], sum to 1.0000
[2019-04-05 14:12:20,451] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1666
[2019-04-05 14:12:20,529] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.899999999999999, 82.0, 0.0, 0.0, 20.0, 18.88448268878548, -1.087413535721757, 1.0, 1.0, 196385.9471196583], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2054400.0000, 
sim time next is 2055000.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 18.99137335235873, -1.054310966511173, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.08261444602989403, 0.14856301116294235, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.05838454], dtype=float32), -0.6651706]. 
=============================================
[2019-04-05 14:12:20,539] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[47.537716]
 [46.5391  ]
 [45.06127 ]
 [45.215202]
 [45.101273]], R is [[46.8077507 ]
 [46.33967209]
 [45.87627411]
 [45.41751099]
 [44.96333694]].
[2019-04-05 14:12:23,535] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0654526e-04 7.9319394e-01 2.6582784e-03 6.0900995e-03 1.9439611e-01
 7.6702709e-05 3.3782909e-03], sum to 1.0000
[2019-04-05 14:12:23,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7727
[2019-04-05 14:12:23,548] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 18.81630732377349, -1.094435968321836, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2158800.0000, 
sim time next is 2159400.0000, 
raw observation next is [-7.299999999999999, 82.0, 0.0, 0.0, 19.0, 18.89532718386693, -1.094441081294408, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.2603878116343491, 0.82, 0.0, 0.0, 0.08333333333333333, 0.0746105986555774, 0.1351863062351973, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2313917], dtype=float32), 1.1986467]. 
=============================================
[2019-04-05 14:12:24,599] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8091834e-03 7.9949176e-01 7.3016686e-03 1.5268342e-02 1.7309459e-01
 1.8373084e-04 2.8507123e-03], sum to 1.0000
[2019-04-05 14:12:24,599] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2043
[2019-04-05 14:12:24,623] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.6, 76.0, 76.0, 85.5, 19.0, 22.37908440271082, -0.3395924257748804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1587600.0000, 
sim time next is 1588200.0000, 
raw observation next is [6.783333333333333, 74.66666666666667, 89.0, 102.3333333333333, 19.0, 22.39902376017389, -0.3230420341401627, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6505078485687905, 0.7466666666666667, 0.2966666666666667, 0.11307550644567216, 0.08333333333333333, 0.3665853133478241, 0.39231932195327907, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1409381], dtype=float32), 1.4126481]. 
=============================================
[2019-04-05 14:12:27,280] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.4871256e-06 9.5429868e-01 4.6261895e-04 1.4445231e-03 4.3545824e-02
 8.4714207e-05 1.5704072e-04], sum to 1.0000
[2019-04-05 14:12:27,285] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8001
[2019-04-05 14:12:27,295] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 20.96561867214073, -0.6001362873197275, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1408800.0000, 
sim time next is 1409400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 20.99245450264322, -0.606501927788941, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.24937120855360165, 0.2978326907370197, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4261193], dtype=float32), -0.020954497]. 
=============================================
[2019-04-05 14:12:33,072] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4693918e-05 9.0837824e-01 1.7621381e-02 2.5495905e-02 4.8087265e-02
 3.9923684e-06 3.9855175e-04], sum to 1.0000
[2019-04-05 14:12:33,076] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2207
[2019-04-05 14:12:33,092] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.05, 79.0, 0.0, 0.0, 19.0, 20.99333019027141, -0.5707250269131775, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1549800.0000, 
sim time next is 1550400.0000, 
raw observation next is [5.866666666666667, 80.0, 0.0, 0.0, 19.0, 20.85543546149664, -0.5899735788119204, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6251154201292707, 0.8, 0.0, 0.0, 0.08333333333333333, 0.23795295512472, 0.3033421403960265, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52348435], dtype=float32), 0.8659922]. 
=============================================
[2019-04-05 14:12:34,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.7632692e-05 6.0310390e-02 7.3314062e-05 1.6222695e-02 9.2312658e-01
 4.6433655e-05 1.2297330e-04], sum to 1.0000
[2019-04-05 14:12:34,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5922
[2019-04-05 14:12:34,220] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.45, 78.5, 0.0, 0.0, 20.5, 18.90703178922113, -1.091284840697815, 0.0, 1.0, 119143.2931051627], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1837800.0000, 
sim time next is 1838400.0000, 
raw observation next is [-6.533333333333333, 78.33333333333334, 0.0, 0.0, 21.0, 18.96953598083159, -1.073555950624457, 0.0, 1.0, 73814.06512235707], 
processed observation next is [0.0, 0.2608695652173913, 0.2816251154201293, 0.7833333333333334, 0.0, 0.0, 0.25, 0.08079466506929922, 0.14214801645851435, 0.0, 1.0, 0.35149554820170037], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.34123427], dtype=float32), -1.3253688]. 
=============================================
[2019-04-05 14:12:40,650] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.7215260e-05 6.1117494e-01 2.0913735e-03 8.9024741e-04 3.8438579e-01
 1.1623052e-04 1.2542558e-03], sum to 1.0000
[2019-04-05 14:12:40,650] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0910
[2019-04-05 14:12:40,664] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 19.0, 19.42138414451423, -1.026462707351576, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2251800.0000, 
sim time next is 2252400.0000, 
raw observation next is [-7.1, 79.66666666666667, 0.0, 0.0, 19.0, 19.4296047225169, -1.042797461102154, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.2659279778393352, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.11913372687640826, 0.152400846299282, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1362913], dtype=float32), 1.4078059]. 
=============================================
[2019-04-05 14:12:43,671] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3043847e-03 6.8136621e-01 5.8362852e-03 3.7331104e-03 2.9976839e-01
 3.3448847e-05 6.9582011e-03], sum to 1.0000
[2019-04-05 14:12:43,672] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3140
[2019-04-05 14:12:43,697] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.666666666666667, 52.66666666666667, 88.66666666666667, 633.8333333333334, 19.0, 21.75570945384095, -0.5553441225393688, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2733600.0000, 
sim time next is 2734200.0000, 
raw observation next is [-3.5, 52.0, 86.0, 614.0, 19.0, 21.8182553775066, -0.5463802562218977, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.36565096952908593, 0.52, 0.2866666666666667, 0.6784530386740332, 0.08333333333333333, 0.3181879481255499, 0.3178732479260341, 1.0, 1.0, 0.0], 
reward next is 0.5362, 
noisyNet noise sample is [array([-0.23942806], dtype=float32), -1.3628852]. 
=============================================
[2019-04-05 14:12:47,228] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 14:12:47,229] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:12:47,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:12:47,230] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:12:47,231] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:12:47,231] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:12:47,234] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:12:47,244] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run79
[2019-04-05 14:12:47,262] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run79
[2019-04-05 14:12:47,278] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run79
[2019-04-05 14:13:17,472] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11736472]
[2019-04-05 14:13:17,472] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.6, 67.0, 0.0, 0.0, 19.0, 20.33335268519298, -0.6963190036772399, 0.0, 1.0, 0.0]
[2019-04-05 14:13:17,473] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:13:17,474] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.9245997e-06 8.2883054e-01 6.0982286e-04 4.9623968e-03 1.6511245e-01
 5.1086699e-06 4.7763946e-04], sampled 0.45740675499896166
[2019-04-05 14:13:18,118] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11736472]
[2019-04-05 14:13:18,119] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [8.266666666666667, 65.66666666666667, 185.8333333333333, 96.0, 19.0, 21.3747129164692, -0.5845569579857217, 1.0, 1.0, 0.0]
[2019-04-05 14:13:18,119] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:13:18,120] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.4702351e-04 7.9326111e-01 3.9342460e-03 9.6890256e-03 1.8500254e-01
 1.9985855e-04 7.5661205e-03], sampled 0.1842179315179142
[2019-04-05 14:13:43,935] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11736472]
[2019-04-05 14:13:43,935] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.0, 82.66666666666667, 0.0, 0.0, 19.0, 18.66678014174451, -1.010556502180755, 0.0, 1.0, 72290.09421429168]
[2019-04-05 14:13:43,936] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:13:43,937] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [3.8620776e-05 8.1062293e-01 1.8166938e-03 4.0637134e-03 1.8107516e-01
 4.8618054e-05 2.3343312e-03], sampled 0.4712649470577579
[2019-04-05 14:13:52,406] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11736472]
[2019-04-05 14:13:52,406] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 19.0, 20.88613234523262, -0.6575952550931244, 1.0, 1.0, 0.0]
[2019-04-05 14:13:52,407] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:13:52,407] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.0404431e-04 8.5593933e-01 2.3988737e-03 5.7017682e-03 1.3057187e-01
 9.5954121e-05 5.0881142e-03], sampled 0.955843583181046
[2019-04-05 14:14:00,507] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6287.5609 144555733.5170 -1792.8000
[2019-04-05 14:14:13,397] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5975.1391 173326350.2994 -2234.7052
[2019-04-05 14:14:15,025] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11736472]
[2019-04-05 14:14:15,026] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.3253147335, 52.386839325, 0.0, 0.0, 19.0, 19.78710057331733, -0.8744639650728571, 0.0, 1.0, 0.0]
[2019-04-05 14:14:15,026] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:14:15,026] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.1322546e-05 7.0151395e-01 3.7037779e-03 1.4274435e-02 2.7661622e-01
 8.8884808e-05 3.7614258e-03], sampled 0.6381969628052458
[2019-04-05 14:14:20,626] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5974.4202 187372539.8347 -2238.6513
[2019-04-05 14:14:21,651] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1560000, evaluation results [1560000.0, 5975.139093603635, 173326350.29940027, -2234.7052348954167, 6287.5608809306295, 144555733.51699, -1792.7999573710504, 5974.420218415514, 187372539.83474672, -2238.6512769203687]
[2019-04-05 14:14:24,680] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3503110e-04 6.0454857e-01 1.4891030e-02 1.0765394e-02 3.3640304e-01
 3.4835513e-04 3.2808591e-02], sum to 1.0000
[2019-04-05 14:14:24,681] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1457
[2019-04-05 14:14:24,699] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 20.5, 20.36652527785423, -0.844569419144784, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.5, 20.42506289696489, -0.8482776643651498, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.125, 0.20208857474707406, 0.21724077854495005, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.03348951], dtype=float32), 1.5198333]. 
=============================================
[2019-04-05 14:14:26,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3875618e-05 2.3177002e-01 2.1403792e-04 3.5558254e-03 7.6219976e-01
 4.0112887e-04 1.8353525e-03], sum to 1.0000
[2019-04-05 14:14:26,508] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2910
[2019-04-05 14:14:26,527] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.0, 74.66666666666667, 0.0, 0.0, 19.5, 18.62668381187325, -1.051615447927779, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3285600.0000, 
sim time next is 3286200.0000, 
raw observation next is [-7.0, 72.33333333333333, 0.0, 0.0, 20.0, 18.66336446601087, -1.029337017603464, 0.0, 1.0, 197033.8672464419], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.7233333333333333, 0.0, 0.0, 0.16666666666666666, 0.05528037216757239, 0.15688766079884534, 0.0, 1.0, 0.9382565106973424], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.28565395], dtype=float32), -2.4453075]. 
=============================================
[2019-04-05 14:14:27,957] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1114874e-04 8.5344690e-01 4.2197988e-03 2.7007690e-02 1.0696851e-01
 5.3581094e-05 8.0923839e-03], sum to 1.0000
[2019-04-05 14:14:27,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4015
[2019-04-05 14:14:28,018] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 96.5, 78.0, 0.0, 22.5, 21.3718167526966, -0.5889028196477355, 1.0, 1.0, 140836.2216982832], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 2889000.0000, 
sim time next is 2889600.0000, 
raw observation next is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 21.5, 21.70098191768889, -0.5445652793458056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4810710987996307, 0.9533333333333335, 0.265, 0.0, 0.2916666666666667, 0.3084151598074074, 0.3184782402180648, 1.0, 1.0, 0.0], 
reward next is 0.1972, 
noisyNet noise sample is [array([1.0492074], dtype=float32), 0.23054665]. 
=============================================
[2019-04-05 14:14:28,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4499043e-05 9.8650593e-01 1.2287888e-04 7.4649492e-04 1.1801132e-02
 2.1526695e-07 7.6886947e-04], sum to 1.0000
[2019-04-05 14:14:28,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4687
[2019-04-05 14:14:28,848] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 55.5, 0.0, 0.0, 19.0, 20.53258272819466, -0.6921856396190021, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3875400.0000, 
sim time next is 3876000.0000, 
raw observation next is [-0.3333333333333333, 57.0, 0.0, 0.0, 19.0, 20.43972861511615, -0.7098119597248611, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4533702677747, 0.57, 0.0, 0.0, 0.08333333333333333, 0.20331071792634572, 0.2633960134250463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1762685], dtype=float32), 0.19589348]. 
=============================================
[2019-04-05 14:14:28,860] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.50302 ]
 [75.26399 ]
 [72.84512 ]
 [70.290115]
 [67.0972  ]], R is [[77.76369476]
 [77.9860611 ]
 [78.20619965]
 [78.42414093]
 [78.63990021]].
[2019-04-05 14:14:35,305] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00073435 0.29114366 0.0630345  0.03964138 0.58937114 0.00093047
 0.01514449], sum to 1.0000
[2019-04-05 14:14:35,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1625
[2019-04-05 14:14:35,324] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 71.0, 45.5, 253.0, 19.0, 20.29092314733516, -0.8081970317372534, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3484800.0000, 
sim time next is 3485400.0000, 
raw observation next is [-1.0, 71.0, 59.66666666666667, 301.6666666666667, 19.0, 20.20500298082125, -0.8114789046812123, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.1988888888888889, 0.33333333333333337, 0.08333333333333333, 0.1837502484017707, 0.22950703177292922, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.16708349], dtype=float32), -1.1638955]. 
=============================================
[2019-04-05 14:14:38,788] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8609435e-05 9.1127789e-01 2.3150158e-03 1.1426409e-03 8.3395101e-02
 2.4242165e-05 1.7766047e-03], sum to 1.0000
[2019-04-05 14:14:38,790] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8027
[2019-04-05 14:14:38,801] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 18.5738472406312, -1.157654667626554, 0.0, 1.0, 197115.21060605], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2352000.0000, 
sim time next is 2352600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 19.0, 18.58981892041082, -1.124396833659389, 0.0, 1.0, 119929.7421117672], 
processed observation next is [0.0, 0.21739130434782608, 0.37673130193905824, 0.67, 0.0, 0.0, 0.08333333333333333, 0.04915157670090172, 0.12520105544687032, 0.0, 1.0, 0.5710940100560343], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.98921865], dtype=float32), 0.21838199]. 
=============================================
[2019-04-05 14:14:42,493] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8165629e-06 1.2834072e-01 2.6363393e-04 2.4390480e-04 8.7072170e-01
 5.0388476e-06 4.2308151e-04], sum to 1.0000
[2019-04-05 14:14:42,497] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0301
[2019-04-05 14:14:42,511] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 21.0, 19.50746560016061, -0.9390752362027025, 0.0, 1.0, 117026.2228047193], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2339400.0000, 
sim time next is 2340000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 20.0, 19.69041391006095, -0.9207417103003849, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.16666666666666666, 0.14086782583841254, 0.19308609656653838, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([2.309345], dtype=float32), -1.6368027]. 
=============================================
[2019-04-05 14:14:42,518] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[76.17149 ]
 [75.50613 ]
 [74.161934]
 [72.38247 ]
 [71.74154 ]], R is [[77.52342224]
 [77.46247864]
 [77.47356415]
 [77.55596924]
 [77.70898438]].
[2019-04-05 14:14:45,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.5252860e-07 6.7713308e-01 1.0366327e-02 1.1903213e-02 3.0037656e-01
 3.6227004e-05 1.8408684e-04], sum to 1.0000
[2019-04-05 14:14:45,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9480
[2019-04-05 14:14:45,313] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 21.0, 21.82182138204985, -0.5450328522514166, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 3009000.0000, 
sim time next is 3009600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 21.5, 21.63722853464467, -0.5766097638053789, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.2916666666666667, 0.30310237788705585, 0.30779674539820706, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.06453907], dtype=float32), 0.51611257]. 
=============================================
[2019-04-05 14:14:46,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3419972e-05 6.0937691e-01 1.6025540e-04 9.4663566e-03 3.6822066e-01
 3.9013116e-06 1.2748583e-02], sum to 1.0000
[2019-04-05 14:14:46,119] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5280
[2019-04-05 14:14:46,127] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 73.16666666666667, 0.0, 0.0, 19.0, 20.20848805298385, -0.8175367586738821, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2855400.0000, 
sim time next is 2856000.0000, 
raw observation next is [1.0, 74.33333333333334, 0.0, 0.0, 19.0, 20.21282437620146, -0.8258079863679164, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7433333333333334, 0.0, 0.0, 0.08333333333333333, 0.1844020313501217, 0.22473067121069454, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39671382], dtype=float32), -1.4269164]. 
=============================================
[2019-04-05 14:14:46,132] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.9197 ]
 [78.68262]
 [77.93854]
 [77.37127]
 [77.91535]], R is [[80.99790955]
 [81.18793488]
 [81.30462646]
 [81.27729797]
 [81.3216629 ]].
[2019-04-05 14:14:46,236] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.8978694e-05 9.3570203e-01 4.0763668e-03 3.9574024e-04 2.0631975e-02
 5.9808270e-05 3.9055087e-02], sum to 1.0000
[2019-04-05 14:14:46,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3607
[2019-04-05 14:14:46,261] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 21.0, 22.6735150157998, -0.2924195125332142, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3341400.0000, 
sim time next is 3342000.0000, 
raw observation next is [-2.0, 47.33333333333334, 64.5, 529.8333333333333, 20.0, 22.23871285896017, -0.3379495612840899, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.47333333333333344, 0.215, 0.5854511970534069, 0.16666666666666666, 0.3532260715800142, 0.3873501462386367, 1.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.40924543], dtype=float32), 2.8378136]. 
=============================================
[2019-04-05 14:14:46,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[57.960835]
 [58.168907]
 [58.175743]
 [58.204712]
 [58.17022 ]], R is [[58.02072906]
 [58.15480804]
 [58.57326126]
 [58.98752975]
 [59.39765549]].
[2019-04-05 14:14:47,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4472113e-06 5.7330251e-01 4.8524316e-04 4.5840582e-04 4.2053932e-01
 3.0989686e-06 5.2070017e-03], sum to 1.0000
[2019-04-05 14:14:47,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7887
[2019-04-05 14:14:47,040] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 19.0, 19.69880310680715, -0.9832730494835694, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3110400.0000, 
sim time next is 3111000.0000, 
raw observation next is [0.1666666666666667, 100.0, 0.0, 0.0, 19.5, 19.63785431353634, -0.9978590334202133, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4672206832871654, 1.0, 0.0, 0.0, 0.125, 0.13648785946136174, 0.16738032219326224, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.270859], dtype=float32), 2.0546167]. 
=============================================
[2019-04-05 14:14:47,052] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[90.9607 ]
 [93.43548]
 [93.78713]
 [93.97474]
 [94.24883]], R is [[89.2361145 ]
 [89.34375763]
 [89.30745697]
 [89.34295654]
 [89.44953156]].
[2019-04-05 14:14:49,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7453396e-05 4.8006210e-01 8.0611579e-02 2.3162395e-02 4.1486442e-01
 1.5109734e-04 1.1108728e-03], sum to 1.0000
[2019-04-05 14:14:49,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4455
[2019-04-05 14:14:49,571] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 21.0, 19.26580092293499, -0.8747828055552368, 0.0, 1.0, 160013.4654236073], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3813600.0000, 
sim time next is 3814200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 20.0, 19.64603412513381, -0.8532701392346381, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.16666666666666666, 0.13716951042781744, 0.21557662025512062, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.16584134], dtype=float32), -1.5263096]. 
=============================================
[2019-04-05 14:14:58,971] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-05 14:14:58,973] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:14:58,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:14:58,975] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:14:58,976] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:14:58,976] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:14:58,977] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:14:58,986] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run80
[2019-04-05 14:14:59,011] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run80
[2019-04-05 14:14:59,011] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run80
[2019-04-05 14:15:38,614] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11831048]
[2019-04-05 14:15:38,614] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.410054052, 82.13614052, 0.0, 0.0, 19.0, 18.89920524846627, -1.181889175833053, 0.0, 1.0, 0.0]
[2019-04-05 14:15:38,614] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:15:38,615] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.5027697e-05 7.1807832e-01 2.9421542e-03 9.8409755e-03 2.6694110e-01
 3.3095970e-05 2.1392375e-03], sampled 0.526064534463912
[2019-04-05 14:16:09,420] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11831048]
[2019-04-05 14:16:09,420] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.96666666666667, 72.0, 0.0, 0.0, 19.5, 23.74480769295256, 0.2131020307299596, 0.0, 1.0, 0.0]
[2019-04-05 14:16:09,420] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:16:09,421] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3533631e-07 8.6316448e-01 1.4976371e-04 2.4427394e-03 1.3410883e-01
 6.1615430e-07 1.3339086e-04], sampled 0.9563670274697903
[2019-04-05 14:16:13,026] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6424.3345 144709950.4174 -1679.2876
[2019-04-05 14:16:24,360] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5890.8669 175272510.3926 -2286.1549
[2019-04-05 14:16:32,028] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5989.1036 189825908.3626 -2184.8103
[2019-04-05 14:16:33,054] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1580000, evaluation results [1580000.0, 5890.866900297691, 175272510.3925622, -2286.1548917009995, 6424.33449519618, 144709950.41738656, -1679.2875668643235, 5989.103633606363, 189825908.3625694, -2184.810345549093]
[2019-04-05 14:16:35,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.0594670e-06 9.5668161e-01 2.4611345e-03 1.1531894e-02 2.8040776e-02
 2.1401986e-04 1.0635295e-03], sum to 1.0000
[2019-04-05 14:16:35,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3436
[2019-04-05 14:16:35,569] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 61.66666666666667, 0.0, 0.0, 19.0, 19.49860203405733, -1.007043623903271, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3392400.0000, 
sim time next is 3393000.0000, 
raw observation next is [-3.0, 62.5, 0.0, 0.0, 19.0, 19.52624819034239, -1.020877869818265, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.625, 0.0, 0.0, 0.08333333333333333, 0.12718734919519914, 0.159707376727245, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.202841], dtype=float32), -1.1601549]. 
=============================================
[2019-04-05 14:16:35,578] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.18431 ]
 [73.643555]
 [73.52359 ]
 [73.853676]
 [74.00826 ]], R is [[73.37760162]
 [73.64382935]
 [73.90739441]
 [74.1683197 ]
 [74.42663574]].
[2019-04-05 14:16:36,589] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01163366 0.70180243 0.02201304 0.03138532 0.12021536 0.00733161
 0.1056186 ], sum to 1.0000
[2019-04-05 14:16:36,595] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5920
[2019-04-05 14:16:36,633] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.800000000000001, 75.66666666666666, 82.33333333333333, 30.33333333333333, 19.0, 21.11262302502868, -0.7527250259022754, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2623800.0000, 
sim time next is 2624400.0000, 
raw observation next is [-6.7, 75.0, 85.0, 45.5, 19.0, 21.22897044770662, -0.7555219379747036, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.2770083102493075, 0.75, 0.2833333333333333, 0.05027624309392265, 0.08333333333333333, 0.26908087064221825, 0.24815935400843214, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7373631], dtype=float32), 0.40790048]. 
=============================================
[2019-04-05 14:16:36,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1283927e-06 9.5596045e-02 3.9746682e-04 6.1925827e-03 8.9736927e-01
 6.4540677e-06 4.3598085e-04], sum to 1.0000
[2019-04-05 14:16:36,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-05 14:16:36,943] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.0, 67.0, 0.0, 0.0, 21.0, 20.29158771525994, -0.7993547739924943, 0.0, 1.0, 196596.1808019798], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 3712800.0000, 
sim time next is 3713400.0000, 
raw observation next is [-3.0, 68.0, 0.0, 0.0, 21.5, 20.21801659707245, -0.7805306789627252, 0.0, 1.0, 197975.8938481456], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.68, 0.0, 0.0, 0.2916666666666667, 0.18483471642270408, 0.23982310701242493, 0.0, 1.0, 0.9427423516578363], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.9883019], dtype=float32), -0.19710495]. 
=============================================
[2019-04-05 14:16:38,495] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9341196e-04 8.4351963e-01 8.4215393e-03 1.0801834e-02 3.5113033e-02
 1.5008493e-04 1.0150041e-01], sum to 1.0000
[2019-04-05 14:16:38,498] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6011
[2019-04-05 14:16:38,510] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 19.0, 21.38540866298205, -0.6358345579849167, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2742000.0000, 
sim time next is 2742600.0000, 
raw observation next is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 19.0, 21.03061277802567, -0.6671140466188646, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3564173591874424, 0.5333333333333333, 0.0, 0.0, 0.08333333333333333, 0.2525510648354725, 0.2776286511270451, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.20040627], dtype=float32), 0.42520973]. 
=============================================
[2019-04-05 14:16:43,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6008836e-05 9.8599482e-01 1.8701802e-03 2.1525272e-03 3.7929218e-03
 1.5577672e-05 6.0779680e-03], sum to 1.0000
[2019-04-05 14:16:43,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1185
[2019-04-05 14:16:43,851] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.0, 23.05046146936592, -0.1696351264003678, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3520800.0000, 
sim time next is 3521400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 23.07799790260234, -0.1779075460029119, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.42316649188352845, 0.4406974846656961, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0663238], dtype=float32), 0.26714498]. 
=============================================
[2019-04-05 14:16:47,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5849993e-04 5.7215381e-01 7.6290807e-03 3.2401551e-03 3.0975580e-01
 4.5651759e-05 1.0691713e-01], sum to 1.0000
[2019-04-05 14:16:47,395] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5652
[2019-04-05 14:16:47,407] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.166666666666667, 65.5, 76.33333333333334, 640.3333333333334, 21.0, 22.25519127167673, -0.3635773477396391, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3426600.0000, 
sim time next is 3427200.0000, 
raw observation next is [2.0, 67.0, 72.5, 608.5, 20.0, 22.48271692803251, -0.3445473619223574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.24166666666666667, 0.6723756906077348, 0.16666666666666666, 0.37355974400270914, 0.3851508793592142, 1.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([2.4612925], dtype=float32), -1.0593848]. 
=============================================
[2019-04-05 14:16:54,973] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.6920478e-06 7.8382015e-01 2.5457313e-04 5.4150447e-03 2.1047199e-01
 4.8744323e-06 2.5589299e-05], sum to 1.0000
[2019-04-05 14:16:54,976] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2244
[2019-04-05 14:16:54,998] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 19.0, 20.07960027996469, -0.8310208433268965, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 19.0, 20.04933240259525, -0.8336991044052859, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 0.08333333333333333, 0.17077770021627092, 0.22210029853157134, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52952176], dtype=float32), 0.7656243]. 
=============================================
[2019-04-05 14:16:55,014] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[94.54405 ]
 [94.502594]
 [94.15504 ]
 [93.8612  ]
 [93.670815]], R is [[94.81552887]
 [94.86737823]
 [94.9187088 ]
 [94.96952057]
 [95.0198288 ]].
[2019-04-05 14:16:56,967] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:16:56,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:16:56,992] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run13
[2019-04-05 14:16:59,845] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5547930e-04 7.1454668e-01 2.2643486e-03 9.5553003e-04 2.8010005e-01
 1.6241948e-04 1.7154802e-03], sum to 1.0000
[2019-04-05 14:16:59,846] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5796
[2019-04-05 14:16:59,867] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.866666666666667, 43.5, 143.0, 141.0, 19.0, 24.28927848376751, 0.1117313244767481, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4637400.0000, 
sim time next is 4638000.0000, 
raw observation next is [5.733333333333333, 44.0, 130.0, 144.0, 19.0, 24.13859694751122, 0.08886747022527862, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6214219759926132, 0.44, 0.43333333333333335, 0.1591160220994475, 0.08333333333333333, 0.5115497456259351, 0.5296224900750929, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0401063], dtype=float32), -1.0660781]. 
=============================================
[2019-04-05 14:16:59,885] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[68.31334 ]
 [68.99619 ]
 [69.605446]
 [70.55619 ]
 [71.709145]], R is [[68.22699738]
 [68.54473114]
 [68.85928345]
 [69.09926605]
 [69.19398499]].
[2019-04-05 14:17:03,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7574776e-04 1.9041681e-01 2.2263573e-02 2.1051347e-02 7.5535661e-01
 1.2040079e-03 9.4320299e-03], sum to 1.0000
[2019-04-05 14:17:03,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8022
[2019-04-05 14:17:03,418] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 19.91913710529344, -0.9641302480173118, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3987000.0000, 
sim time next is 3987600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.5, 19.78094004059636, -0.9855143883042762, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.125, 0.1484116700496966, 0.17149520389857462, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.4283135], dtype=float32), 1.1104134]. 
=============================================
[2019-04-05 14:17:08,377] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 14:17:08,378] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:17:08,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:17:08,378] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:17:08,378] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:17:08,379] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:17:08,379] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:17:08,387] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run81
[2019-04-05 14:17:08,408] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run81
[2019-04-05 14:17:08,409] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run81
[2019-04-05 14:17:34,622] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11992318]
[2019-04-05 14:17:34,623] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [12.237844825, 75.49976088166666, 0.0, 0.0, 19.0, 19.22975560549477, -0.9240197156201936, 0.0, 1.0, 0.0]
[2019-04-05 14:17:34,623] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:17:34,624] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.6874660e-07 8.2088363e-01 2.1133031e-04 4.8519834e-03 1.7385955e-01
 1.9652443e-06 1.9104635e-04], sampled 0.26703864504196617
[2019-04-05 14:18:06,396] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11992318]
[2019-04-05 14:18:06,396] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.7860928711666668, 40.76331766833333, 74.81296381333333, 824.9527786000001, 19.0, 20.64373660966152, -0.8509130612963404, 1.0, 1.0, 0.0]
[2019-04-05 14:18:06,396] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:18:06,396] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.9812837e-04 7.7337152e-01 5.3150230e-03 1.1511476e-02 1.9588386e-01
 3.6140342e-04 1.2858597e-02], sampled 0.8360738120953691
[2019-04-05 14:18:20,820] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6288.4573 143001439.9060 -1809.6607
[2019-04-05 14:18:32,528] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5806.4804 174503234.7082 -2365.3966
[2019-04-05 14:18:39,041] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6012.3212 188057890.9486 -2281.3684
[2019-04-05 14:18:40,068] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1600000, evaluation results [1600000.0, 5806.48035500703, 174503234.70824286, -2365.396591190039, 6288.457330752785, 143001439.9059937, -1809.6607402105399, 6012.321155143183, 188057890.9486004, -2281.368444135248]
[2019-04-05 14:18:40,153] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8650353e-06 2.5069648e-01 1.9468101e-02 3.3657353e-03 7.2630590e-01
 8.7760396e-05 6.6185916e-05], sum to 1.0000
[2019-04-05 14:18:40,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0948
[2019-04-05 14:18:40,178] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.666666666666666, 49.33333333333333, 0.0, 0.0, 20.0, 19.33978940849602, -1.002040419554681, 0.0, 1.0, 199269.943242106], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4171200.0000, 
sim time next is 4171800.0000, 
raw observation next is [-4.833333333333334, 49.16666666666667, 0.0, 0.0, 19.0, 19.33844469623482, -0.9942513973242536, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.32871652816251157, 0.4916666666666667, 0.0, 0.0, 0.08333333333333333, 0.11153705801956833, 0.16858286755858212, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7308266], dtype=float32), 1.5205464]. 
=============================================
[2019-04-05 14:18:44,874] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8844996e-06 9.7000688e-01 8.4124890e-04 6.5067490e-03 1.1752765e-02
 1.4072327e-05 1.0874434e-02], sum to 1.0000
[2019-04-05 14:18:44,877] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1309
[2019-04-05 14:18:44,886] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 19.0, 20.11580664109812, -0.7914169152069807, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4204200.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 19.0, 20.14018392184645, -0.7828823270977336, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 0.08333333333333333, 0.17834866015387085, 0.23903922430075544, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08702409], dtype=float32), -1.5263544]. 
=============================================
[2019-04-05 14:18:45,366] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.6772987e-07 9.7094250e-01 5.3697597e-05 1.3365765e-03 2.4305750e-02
 9.1269849e-06 3.3513745e-03], sum to 1.0000
[2019-04-05 14:18:45,366] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8012
[2019-04-05 14:18:45,376] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.5, 46.0, 114.0, 812.0, 20.5, 21.85681161851348, -0.3957453947301102, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3591000.0000, 
sim time next is 3591600.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 112.0, 808.0, 19.5, 21.83429468443236, -0.3969305447213466, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.42566943674976926, 0.4466666666666667, 0.37333333333333335, 0.8928176795580111, 0.125, 0.3195245570360301, 0.36768981842621784, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.15755591], dtype=float32), -0.64804184]. 
=============================================
[2019-04-05 14:18:52,228] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.61832832e-05 3.32902551e-01 7.16507947e-03 1.11144342e-01
 5.00566840e-01 1.07518536e-04 4.80975322e-02], sum to 1.0000
[2019-04-05 14:18:52,229] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-05 14:18:52,293] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.666666666666667, 70.0, 73.83333333333334, 374.3333333333334, 20.5, 19.49306595085495, -0.7834877613359034, 0.0, 1.0, 71068.37281696772], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3572400.0000, 
sim time next is 3573000.0000, 
raw observation next is [-6.5, 70.0, 88.0, 425.0, 19.5, 20.1414151887956, -0.7133618512019821, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.28254847645429365, 0.7, 0.29333333333333333, 0.4696132596685083, 0.125, 0.1784512657329668, 0.26221271626600595, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.63698584], dtype=float32), -0.24755676]. 
=============================================
[2019-04-05 14:18:52,302] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[89.47696]
 [88.58288]
 [87.62073]
 [86.53224]
 [85.20673]], R is [[90.38214874]
 [90.26403809]
 [90.14710999]
 [90.10277557]
 [90.13032532]].
[2019-04-05 14:18:52,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:18:52,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:18:52,462] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run13
[2019-04-05 14:18:54,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6176913e-05 4.3863800e-01 1.2819791e-04 5.6513781e-03 5.3646529e-01
 9.1905778e-05 1.8999040e-02], sum to 1.0000
[2019-04-05 14:18:54,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9926
[2019-04-05 14:18:54,542] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 73.0, 0.0, 0.0, 21.5, 18.89855771881544, -1.07129590698288, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 3820800.0000, 
sim time next is 3821400.0000, 
raw observation next is [-4.5, 74.0, 0.0, 0.0, 23.5, 18.90691619458589, -1.043671852191911, 0.0, 1.0, 197428.8560834827], 
processed observation next is [1.0, 0.21739130434782608, 0.3379501385041552, 0.74, 0.0, 0.0, 0.4583333333333333, 0.07557634954882413, 0.15210938260269632, 0.0, 1.0, 0.9401374099213462], 
reward next is 0.3571, 
noisyNet noise sample is [array([0.5945716], dtype=float32), 0.29602697]. 
=============================================
[2019-04-05 14:18:58,626] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6357043e-07 9.2375898e-01 1.5759431e-05 2.1924882e-03 7.2962776e-02
 1.0108872e-06 1.0686934e-03], sum to 1.0000
[2019-04-05 14:18:58,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0109
[2019-04-05 14:18:58,665] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.833333333333333, 40.5, 186.6666666666667, 714.0, 19.0, 22.19126446000003, -0.3458626248146981, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4798200.0000, 
sim time next is 4798800.0000, 
raw observation next is [2.0, 40.0, 195.0, 677.5, 19.0, 22.19433760891286, -0.3476228630012502, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.4, 0.65, 0.7486187845303868, 0.08333333333333333, 0.3495281340760717, 0.38412571233291654, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.9787736], dtype=float32), -1.0773108]. 
=============================================
[2019-04-05 14:19:12,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:19:12,019] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:19:12,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run13
[2019-04-05 14:19:12,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:19:12,454] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:19:12,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run13
[2019-04-05 14:19:14,507] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-05 14:19:14,508] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:19:14,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:19:14,510] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:19:14,510] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:19:14,511] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:19:14,511] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:19:14,518] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run82
[2019-04-05 14:19:14,546] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run82
[2019-04-05 14:19:14,568] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run82
[2019-04-05 14:19:30,551] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12055258]
[2019-04-05 14:19:30,552] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.7, 87.0, 20.5, 22.5, 19.0, 18.67982054521944, -1.136445563632328, 0.0, 1.0, 82010.94136744089]
[2019-04-05 14:19:30,552] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:19:30,553] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.8239897e-06 6.8636304e-01 8.8148733e-04 9.4951615e-03 3.0159119e-01
 2.0832276e-05 1.6414656e-03], sampled 0.4601313997842623
[2019-04-05 14:19:44,465] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12055258]
[2019-04-05 14:19:44,465] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.01666666666667, 66.16666666666667, 0.0, 0.0, 19.0, 20.69074921407401, -0.6364949821651561, 0.0, 1.0, 0.0]
[2019-04-05 14:19:44,465] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:19:44,466] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.5176106e-07 8.1659019e-01 2.0969719e-04 2.7624769e-03 1.8006459e-01
 1.3453692e-06 3.7134055e-04], sampled 0.5941350495502169
[2019-04-05 14:20:26,830] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6412.1731 144985525.5939 -1705.8086
[2019-04-05 14:20:32,608] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12055258]
[2019-04-05 14:20:32,608] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.333333333333333, 67.33333333333334, 157.1666666666667, 452.6666666666667, 19.5, 20.2798005312166, -0.8303248384362001, 1.0, 1.0, 0.0]
[2019-04-05 14:20:32,609] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:20:32,609] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.7938587e-04 6.7462528e-01 5.7981829e-03 1.9623796e-02 2.8546232e-01
 4.0242184e-04 1.3608576e-02], sampled 0.18637137045343177
[2019-04-05 14:20:38,662] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5864.0303 175008721.1400 -2314.0787
[2019-04-05 14:20:45,091] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5978.6273 187338159.8653 -2269.7440
[2019-04-05 14:20:46,118] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1620000, evaluation results [1620000.0, 5864.030268217216, 175008721.13999358, -2314.078729081131, 6412.173141763283, 144985525.59389707, -1705.8086217175244, 5978.627301162729, 187338159.86528394, -2269.743973608364]
[2019-04-05 14:20:51,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:20:51,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:20:51,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run13
[2019-04-05 14:20:53,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:20:53,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:20:53,344] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run13
[2019-04-05 14:20:54,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:20:54,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:20:54,104] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run13
[2019-04-05 14:20:56,903] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01703798 0.36309707 0.02278104 0.01751373 0.51600695 0.0007666
 0.06279672], sum to 1.0000
[2019-04-05 14:20:56,904] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4608
[2019-04-05 14:20:56,942] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.85, 76.5, 79.0, 0.0, 19.5, 21.2616719222104, -0.7422691759349748, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 207000.0000, 
sim time next is 207600.0000, 
raw observation next is [-7.666666666666666, 76.0, 86.5, 0.0, 20.0, 21.27239861112966, -0.741129581160599, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.25023084025854114, 0.76, 0.28833333333333333, 0.0, 0.16666666666666666, 0.272699884260805, 0.25295680627980033, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3470537], dtype=float32), 0.25035027]. 
=============================================
[2019-04-05 14:20:57,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:20:57,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:20:57,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run13
[2019-04-05 14:21:01,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5350663e-04 9.7460741e-01 1.2616758e-03 3.7776623e-03 1.7306700e-02
 2.9849785e-04 2.5945199e-03], sum to 1.0000
[2019-04-05 14:21:01,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1133
[2019-04-05 14:21:01,301] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 19.0, 22.31532340107182, -0.4916239347062986, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5050200.0000, 
sim time next is 5050800.0000, 
raw observation next is [5.0, 36.0, 119.5, 827.0, 19.0, 22.35500901128829, -0.3620746892362256, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.36, 0.3983333333333333, 0.9138121546961326, 0.08333333333333333, 0.36291741760735735, 0.3793084369212581, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4627675], dtype=float32), 1.3663907]. 
=============================================
[2019-04-05 14:21:03,807] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8518260e-07 9.2899919e-01 5.2822009e-04 1.7635021e-04 6.9887891e-02
 1.4081839e-07 4.0813093e-04], sum to 1.0000
[2019-04-05 14:21:03,808] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5290
[2019-04-05 14:21:03,822] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 23.5, 21.42369094684431, -0.5637521755402817, 0.0, 1.0, 44549.13657952612], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 4686600.0000, 
sim time next is 4687200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 22.5, 21.46322246821904, -0.556552653237128, 0.0, 1.0, 44160.35570263476], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.375, 0.2886018723515867, 0.31448244892095734, 0.0, 1.0, 0.2102874081077846], 
reward next is 0.5000, 
noisyNet noise sample is [array([1.0875499], dtype=float32), 0.9489934]. 
=============================================
[2019-04-05 14:21:04,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:21:04,004] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:21:04,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run13
[2019-04-05 14:21:05,035] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.7068725e-05 9.2071846e-02 3.5299570e-03 5.6820270e-03 8.8940489e-01
 5.1616739e-06 9.2090238e-03], sum to 1.0000
[2019-04-05 14:21:05,036] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6510
[2019-04-05 14:21:05,069] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-12.9, 77.83333333333334, 0.0, 0.0, 21.0, 18.90624919309586, -1.093898241523231, 0.0, 1.0, 104172.0821670423], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 335400.0000, 
sim time next is 336000.0000, 
raw observation next is [-13.0, 78.66666666666667, 0.0, 0.0, 21.5, 19.02636102312902, -1.078932041840932, 0.0, 1.0, 70445.75796091568], 
processed observation next is [1.0, 0.9130434782608695, 0.10249307479224376, 0.7866666666666667, 0.0, 0.0, 0.2916666666666667, 0.08553008526075168, 0.14035598605302266, 0.0, 1.0, 0.33545599029007467], 
reward next is 0.6429, 
noisyNet noise sample is [array([1.1930008], dtype=float32), 0.82141674]. 
=============================================
[2019-04-05 14:21:05,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.27985 ]
 [68.45714 ]
 [67.607346]
 [64.18991 ]
 [62.333202]], R is [[70.07993317]
 [70.09342194]
 [70.17819977]
 [70.33355713]
 [70.55879211]].
[2019-04-05 14:21:20,532] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:21:20,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:21:20,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run13
[2019-04-05 14:21:21,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:21:21,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:21:21,747] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run13
[2019-04-05 14:21:21,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:21:21,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:21:21,955] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run13
[2019-04-05 14:21:23,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1701050e-04 9.0516990e-01 1.8370482e-03 4.5406516e-03 8.2614996e-02
 2.5575314e-04 5.4646777e-03], sum to 1.0000
[2019-04-05 14:21:23,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7225
[2019-04-05 14:21:23,396] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.900000000000002, 37.33333333333334, 23.66666666666666, 453.6666666666667, 19.0, 20.84737008826725, -0.7971958704276018, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 404400.0000, 
sim time next is 405000.0000, 
raw observation next is [-8.9, 37.0, 21.0, 403.0, 19.0, 20.8514964239134, -0.8864495652580562, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.37, 0.07, 0.4453038674033149, 0.08333333333333333, 0.2376247019927833, 0.20451681158064794, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.45255297], dtype=float32), -0.621762]. 
=============================================
[2019-04-05 14:21:23,404] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[55.77681 ]
 [56.522575]
 [57.221863]
 [57.805893]
 [58.44096 ]], R is [[54.31293869]
 [53.76980972]
 [53.23211288]
 [52.69979095]
 [52.17279434]].
[2019-04-05 14:21:23,455] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-05 14:21:23,457] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:21:23,457] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:21:23,458] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:21:23,458] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:21:23,458] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:21:23,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run83
[2019-04-05 14:21:23,465] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:21:23,484] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run83
[2019-04-05 14:21:23,500] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run83
[2019-04-05 14:22:35,434] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6261.4958 142584142.7680 -1889.2104
[2019-04-05 14:22:46,404] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5869.7795 172024183.0649 -2432.7791
[2019-04-05 14:22:53,656] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6000.6526 186533086.4397 -2335.6179
[2019-04-05 14:22:54,683] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1640000, evaluation results [1640000.0, 5869.779528350157, 172024183.06491512, -2432.77908806248, 6261.495840006883, 142584142.7679634, -1889.2103507280272, 6000.652573585838, 186533086.43971804, -2335.617858739214]
[2019-04-05 14:22:56,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:22:56,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:22:56,427] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run13
[2019-04-05 14:22:56,815] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8745626e-07 9.7947836e-01 2.2188075e-04 1.2275788e-03 1.8846478e-02
 5.2643679e-07 2.2440888e-04], sum to 1.0000
[2019-04-05 14:22:56,816] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4320
[2019-04-05 14:22:56,832] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 20.0, 20.66898377723752, -0.8379518933489386, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 20.64951520858943, -0.8512346815520866, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.22079293404911926, 0.21625510614930446, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.208896], dtype=float32), -2.7853136]. 
=============================================
[2019-04-05 14:22:56,840] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.586624]
 [82.57263 ]
 [82.8072  ]
 [82.90622 ]
 [83.327576]], R is [[82.68275452]
 [82.7130661 ]
 [82.60021973]
 [82.34564209]
 [81.95075989]].
[2019-04-05 14:22:57,022] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5734382e-04 8.2906693e-01 4.4408315e-04 2.1099047e-04 1.6915101e-01
 1.7827933e-05 6.5175688e-04], sum to 1.0000
[2019-04-05 14:22:57,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2339
[2019-04-05 14:22:57,060] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.9, 47.66666666666667, 53.83333333333334, 877.8333333333334, 19.0, 21.65891669383922, -0.6896048754169216, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 394800.0000, 
sim time next is 395400.0000, 
raw observation next is [-10.7, 46.83333333333334, 52.66666666666667, 868.6666666666666, 19.0, 21.47280567944515, -0.64900789612242, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1662049861495845, 0.46833333333333343, 0.17555555555555558, 0.9598526703499078, 0.08333333333333333, 0.28940047328709567, 0.28366403462586, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0840726], dtype=float32), 0.6652375]. 
=============================================
[2019-04-05 14:23:00,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:23:00,735] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:23:00,758] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run13
[2019-04-05 14:23:01,482] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2003819e-07 8.8220263e-01 2.1547067e-04 2.2436867e-03 1.1531550e-01
 2.4639246e-06 1.9753654e-05], sum to 1.0000
[2019-04-05 14:23:01,483] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6105
[2019-04-05 14:23:01,487] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.33333333333333, 94.0, 0.0, 0.0, 19.0, 21.08384458433926, -0.4497603829062959, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1225200.0000, 
sim time next is 1225800.0000, 
raw observation next is [15.25, 94.5, 0.0, 0.0, 19.0, 21.07385887325271, -0.4524758697895968, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8850415512465375, 0.945, 0.0, 0.0, 0.08333333333333333, 0.25615490610439257, 0.3491747100701344, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7311506], dtype=float32), 1.4020866]. 
=============================================
[2019-04-05 14:23:01,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:23:01,888] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:23:01,914] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run13
[2019-04-05 14:23:04,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2462374e-06 8.7452477e-01 6.3099735e-03 2.0898622e-02 9.6900128e-02
 1.2402976e-06 1.3599505e-03], sum to 1.0000
[2019-04-05 14:23:04,173] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0266
[2019-04-05 14:23:04,189] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.283333333333333, 78.83333333333334, 0.0, 0.0, 19.0, 18.79288270734256, -1.167897970124423, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1836600.0000, 
sim time next is 1837200.0000, 
raw observation next is [-6.366666666666667, 78.66666666666667, 0.0, 0.0, 19.0, 18.67773739785476, -1.179971261556879, 0.0, 1.0, 101917.5550053932], 
processed observation next is [0.0, 0.2608695652173913, 0.28624192059095105, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.056478116487896614, 0.10667624614770703, 0.0, 1.0, 0.48532169050187235], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8952596], dtype=float32), -0.997095]. 
=============================================
[2019-04-05 14:23:04,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:23:04,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:23:04,828] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run13
[2019-04-05 14:23:12,718] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6138342e-03 7.2160196e-01 2.7682215e-02 1.2957731e-02 2.2272302e-01
 2.0539301e-04 1.3215800e-02], sum to 1.0000
[2019-04-05 14:23:12,719] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8499
[2019-04-05 14:23:12,742] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-13.3, 81.16666666666667, 0.0, 0.0, 19.0, 18.61040606006433, -1.227358928405584, 0.0, 1.0, 137782.3737102638], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 337800.0000, 
sim time next is 338400.0000, 
raw observation next is [-13.4, 82.0, 0.0, 0.0, 19.5, 18.49640137136925, -1.223274101197289, 0.0, 1.0, 199083.430575334], 
processed observation next is [1.0, 0.9565217391304348, 0.09141274238227146, 0.82, 0.0, 0.0, 0.125, 0.0413667809474374, 0.09224196626757035, 0.0, 1.0, 0.948016336073019], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.22301239], dtype=float32), -0.89710826]. 
=============================================
[2019-04-05 14:23:19,292] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7356471e-07 9.7389936e-01 7.0881877e-05 9.0820988e-04 2.5114810e-02
 1.2838438e-08 6.2542204e-06], sum to 1.0000
[2019-04-05 14:23:19,293] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0275
[2019-04-05 14:23:19,298] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.25, 98.0, 0.0, 0.0, 19.0, 20.70029038809201, -0.505516989716616, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1272600.0000, 
sim time next is 1273200.0000, 
raw observation next is [9.600000000000001, 97.33333333333334, 0.0, 0.0, 19.0, 20.65965643004356, -0.5072102031360813, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.7285318559556788, 0.9733333333333334, 0.0, 0.0, 0.08333333333333333, 0.22163803583696348, 0.3309299322879729, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93869126], dtype=float32), 1.0658066]. 
=============================================
[2019-04-05 14:23:19,430] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3255551e-03 8.5035402e-01 3.4181054e-03 1.1716203e-03 1.0836452e-01
 1.7146344e-04 3.5194736e-02], sum to 1.0000
[2019-04-05 14:23:19,431] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9644
[2019-04-05 14:23:19,450] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-9.683333333333334, 43.16666666666666, 39.33333333333333, 319.0, 19.0, 21.58313676643211, -0.6760816919508867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 317400.0000, 
sim time next is 318000.0000, 
raw observation next is [-9.866666666666667, 44.33333333333334, 31.66666666666666, 279.5, 19.5, 21.29843476914584, -0.7123984437054519, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.18928901200369344, 0.4433333333333334, 0.10555555555555554, 0.30883977900552484, 0.125, 0.2748695640954866, 0.2625338520981827, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24953337], dtype=float32), 0.95573014]. 
=============================================
[2019-04-05 14:23:19,454] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[45.050217]
 [45.633213]
 [46.24143 ]
 [46.646793]
 [47.209816]], R is [[44.0122757 ]
 [43.572155  ]
 [43.13643265]
 [42.7050705 ]
 [42.27801895]].
[2019-04-05 14:23:21,869] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.89524158e-04 8.00580263e-01 7.57287350e-03 4.27437127e-02
 1.00368686e-01 1.17035692e-04 4.84279469e-02], sum to 1.0000
[2019-04-05 14:23:21,870] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2697
[2019-04-05 14:23:21,895] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 83.0, 122.5, 0.0, 19.0, 19.6535753650055, -0.9160628193314139, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1774800.0000, 
sim time next is 1775400.0000, 
raw observation next is [-2.8, 83.0, 121.3333333333333, 0.0, 19.0, 19.56827055307585, -0.9344770674633791, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.40444444444444433, 0.0, 0.08333333333333333, 0.1306892127563207, 0.18850764417887364, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18860193], dtype=float32), 1.5328147]. 
=============================================
[2019-04-05 14:23:29,497] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4374835e-05 9.4619787e-01 6.4358197e-04 4.9961172e-04 5.1891562e-02
 1.1167790e-04 5.7130220e-04], sum to 1.0000
[2019-04-05 14:23:29,503] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6746
[2019-04-05 14:23:29,518] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.216666666666667, 93.5, 92.0, 705.3333333333334, 19.0, 22.36532039658766, -0.2925508350078792, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1511400.0000, 
sim time next is 1512000.0000, 
raw observation next is [4.4, 93.0, 94.0, 704.0, 19.0, 22.54363047597758, -0.2687502794726203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5844875346260389, 0.93, 0.31333333333333335, 0.7779005524861878, 0.08333333333333333, 0.3786358729981316, 0.4104165735091265, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4487892], dtype=float32), -2.545802]. 
=============================================
[2019-04-05 14:23:29,535] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.356285]
 [72.90018 ]
 [71.33815 ]
 [69.61597 ]
 [67.894745]], R is [[75.85531616]
 [76.09676361]
 [76.33580017]
 [76.5724411 ]
 [76.80671692]].
[2019-04-05 14:23:30,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.01556125e-07 7.72822082e-01 1.52611712e-04 4.75235167e-04
 2.24958092e-01 3.10136897e-06 1.58878474e-03], sum to 1.0000
[2019-04-05 14:23:30,783] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1370
[2019-04-05 14:23:30,794] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.5, 20.19163005371533, -0.6757943347922725, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1137600.0000, 
sim time next is 1138200.0000, 
raw observation next is [11.18333333333333, 77.0, 0.0, 0.0, 19.0, 20.18382346081232, -0.6811400812748287, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.7723915050784858, 0.77, 0.0, 0.0, 0.08333333333333333, 0.18198528840102668, 0.27295330624172376, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.643224], dtype=float32), 0.8370105]. 
=============================================
[2019-04-05 14:23:32,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9375511e-07 2.7116179e-01 4.6447930e-03 8.7829860e-05 7.1652389e-01
 1.4141237e-05 7.5668455e-03], sum to 1.0000
[2019-04-05 14:23:32,349] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6102
[2019-04-05 14:23:32,359] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 20.0, 22.04711673813127, -0.3740240357617567, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1568400.0000, 
sim time next is 1569000.0000, 
raw observation next is [4.566666666666666, 85.16666666666667, 0.0, 0.0, 20.5, 22.02522873487923, -0.3790266432978973, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5891043397968606, 0.8516666666666667, 0.0, 0.0, 0.20833333333333334, 0.33543572790660264, 0.37365778556736756, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.666297], dtype=float32), 0.18941508]. 
=============================================
[2019-04-05 14:23:32,374] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[91.02093 ]
 [91.23353 ]
 [91.51324 ]
 [91.541466]
 [91.76684 ]], R is [[90.77404022]
 [90.72344208]
 [90.74478149]
 [90.83733368]
 [90.85753632]].
[2019-04-05 14:23:33,393] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0207825e-05 9.6324855e-01 1.3091498e-04 5.4126151e-04 3.5878044e-02
 2.2640645e-06 1.3879695e-04], sum to 1.0000
[2019-04-05 14:23:33,393] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4830
[2019-04-05 14:23:33,401] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.766666666666667, 64.33333333333334, 0.0, 0.0, 19.5, 22.20798022565729, -0.3138084965959531, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1622400.0000, 
sim time next is 1623000.0000, 
raw observation next is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 19.0, 22.1254263127862, -0.3185837765089269, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7280701754385965, 0.6516666666666666, 0.0, 0.0, 0.08333333333333333, 0.34378552606551666, 0.3938054078303577, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20277831], dtype=float32), -0.53358084]. 
=============================================
[2019-04-05 14:23:33,422] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.404335]
 [72.00122 ]
 [71.77871 ]
 [71.57851 ]
 [71.387436]], R is [[76.58102417]
 [76.74378967]
 [76.97634888]
 [77.20658875]
 [77.43452454]].
[2019-04-05 14:23:33,513] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 14:23:33,516] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:23:33,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:23:33,516] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:23:33,517] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:23:33,517] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:23:33,519] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:23:33,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run84
[2019-04-05 14:23:33,544] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run84
[2019-04-05 14:23:33,563] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run84
[2019-04-05 14:24:32,422] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11845944]
[2019-04-05 14:24:32,423] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [0.0, 100.0, 0.0, 0.0, 19.0, 18.7348594415254, -1.20589621544966, 0.0, 1.0, 0.0]
[2019-04-05 14:24:32,423] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:24:32,425] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0486422e-05 7.6702791e-01 1.6106805e-03 6.1862194e-03 2.2199483e-01
 2.9379818e-05 3.1404276e-03], sampled 0.4510001395390888
[2019-04-05 14:24:46,800] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6391.4379 142210429.9167 -1754.5346
[2019-04-05 14:24:58,279] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5938.4775 172967003.2832 -2313.1170
[2019-04-05 14:25:05,211] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5952.7549 185345395.9019 -2324.1238
[2019-04-05 14:25:06,237] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1660000, evaluation results [1660000.0, 5938.477486612504, 172967003.2832377, -2313.117047340303, 6391.437937524728, 142210429.91668653, -1754.5346154058686, 5952.754895106064, 185345395.9018761, -2324.1238227835456]
[2019-04-05 14:25:10,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5235239e-04 8.3349890e-01 9.7116726e-03 5.1357597e-03 9.8004892e-02
 1.4775521e-04 5.3148627e-02], sum to 1.0000
[2019-04-05 14:25:10,384] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0728
[2019-04-05 14:25:10,391] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 56.0, 0.0, 0.0, 19.0, 21.0577110055107, -0.6740744446538717, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 756000.0000, 
sim time next is 756600.0000, 
raw observation next is [-3.9, 55.5, 0.0, 0.0, 19.0, 20.98004349439078, -0.7027826721148297, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.555, 0.0, 0.0, 0.08333333333333333, 0.2483369578658984, 0.2657391092950568, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8401363], dtype=float32), 0.028237324]. 
=============================================
[2019-04-05 14:25:11,911] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.02138222 0.39539894 0.04693893 0.01369117 0.47179443 0.00057593
 0.05021842], sum to 1.0000
[2019-04-05 14:25:11,911] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8743
[2019-04-05 14:25:11,919] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.083333333333333, 53.83333333333334, 0.0, 0.0, 19.5, 20.32213099029757, -0.7949530117818685, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 760200.0000, 
sim time next is 760800.0000, 
raw observation next is [-4.266666666666667, 54.66666666666667, 0.0, 0.0, 20.0, 20.32066529532261, -0.8150132231216315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3444136657433057, 0.5466666666666667, 0.0, 0.0, 0.16666666666666666, 0.19338877461021742, 0.22832892562612284, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.88016915], dtype=float32), 0.2365625]. 
=============================================
[2019-04-05 14:25:12,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7589543e-05 9.8076481e-01 1.6284331e-04 8.6088553e-03 9.9387225e-03
 1.3685822e-05 4.7347846e-04], sum to 1.0000
[2019-04-05 14:25:12,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8721
[2019-04-05 14:25:12,455] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 21.03370517719058, -0.6035329656676628, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1399800.0000, 
sim time next is 1400400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 20.91220972673933, -0.6279947825947849, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.24268414389494422, 0.29066840580173836, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65789986], dtype=float32), -0.37864372]. 
=============================================
[2019-04-05 14:25:14,670] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4079848e-05 7.0615637e-01 1.4523710e-02 7.7324761e-03 2.6850054e-01
 1.6318976e-05 3.0565723e-03], sum to 1.0000
[2019-04-05 14:25:14,670] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1430
[2019-04-05 14:25:14,690] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 54.00000000000001, 0.0, 0.0, 19.5, 19.29042760945774, -1.139673910879272, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2532000.0000, 
sim time next is 2532600.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 19.0, 19.28419993552063, -1.14171230224774, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 0.08333333333333333, 0.10701666129338576, 0.11942923258408666, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.67839843], dtype=float32), 0.6741506]. 
=============================================
[2019-04-05 14:25:15,985] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0160899e-05 6.2141281e-01 4.2015769e-02 1.7542996e-02 3.1695709e-01
 9.8548862e-06 2.0512852e-03], sum to 1.0000
[2019-04-05 14:25:15,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8537
[2019-04-05 14:25:15,995] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.416666666666667, 77.0, 0.0, 0.0, 20.0, 20.94976495947616, -0.5817572085324912, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1548600.0000, 
sim time next is 1549200.0000, 
raw observation next is [6.233333333333333, 78.0, 0.0, 0.0, 20.5, 20.87466918574212, -0.6025826300279064, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6352723915050786, 0.78, 0.0, 0.0, 0.20833333333333334, 0.2395557654785101, 0.2991391233240312, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.42439383], dtype=float32), -0.81606543]. 
=============================================
[2019-04-05 14:25:18,650] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1125648e-05 3.2461447e-01 1.0527016e-03 3.3628352e-03 6.6024727e-01
 1.4330640e-04 1.0538211e-02], sum to 1.0000
[2019-04-05 14:25:18,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1629
[2019-04-05 14:25:18,686] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 71.0, 161.6666666666667, 33.33333333333334, 19.5, 19.12129175286479, -1.055870299734847, 0.0, 1.0, 74990.15548729346], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1862400.0000, 
sim time next is 1863000.0000, 
raw observation next is [-4.5, 71.0, 170.0, 40.0, 19.0, 19.15147016203581, -1.049693467647375, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.5666666666666667, 0.04419889502762431, 0.08333333333333333, 0.09595584683631753, 0.15010217745087498, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3799405], dtype=float32), -0.5675112]. 
=============================================
[2019-04-05 14:25:18,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.085976]
 [69.97872 ]
 [70.22343 ]
 [70.55509 ]
 [70.6117  ]], R is [[70.07117462]
 [70.29903412]
 [70.59604645]
 [70.89008331]
 [71.10975647]].
[2019-04-05 14:25:28,220] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.2632826e-05 9.7106588e-01 2.0210820e-03 1.4506618e-03 2.3803199e-02
 8.5784071e-05 1.5407066e-03], sum to 1.0000
[2019-04-05 14:25:28,224] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1408
[2019-04-05 14:25:28,232] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.3, 65.0, 159.0, 0.0, 19.0, 21.12749225169368, -0.4398918785052257, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1171800.0000, 
sim time next is 1172400.0000, 
raw observation next is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 21.13745213679921, -0.4358912091693528, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5127777777777777, 0.0, 0.08333333333333333, 0.2614543447332676, 0.35470293027688243, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0999633], dtype=float32), 2.3884017]. 
=============================================
[2019-04-05 14:25:30,281] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4435033e-07 3.4964275e-01 1.6496379e-04 2.9829907e-01 3.5147616e-01
 4.7089248e-05 3.6966507e-04], sum to 1.0000
[2019-04-05 14:25:30,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7731
[2019-04-05 14:25:30,290] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.4, 97.33333333333334, 96.0, 0.0, 19.0, 20.57976979474943, -0.5187148953645541, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1251600.0000, 
sim time next is 1252200.0000, 
raw observation next is [14.4, 96.66666666666666, 97.0, 0.0, 19.5, 20.58471405708327, -0.5166575693282348, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9666666666666666, 0.3233333333333333, 0.0, 0.125, 0.21539283809027263, 0.32778081022392175, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([1.0363816], dtype=float32), 0.6087923]. 
=============================================
[2019-04-05 14:25:33,487] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2960928e-06 9.8764926e-01 1.0970526e-04 2.0977289e-03 9.7516207e-03
 1.3152527e-05 3.7422229e-04], sum to 1.0000
[2019-04-05 14:25:33,494] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8674
[2019-04-05 14:25:33,502] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 75.0, 114.0, 0.0, 19.0, 22.43878120615019, -0.3112347129494596, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1074600.0000, 
sim time next is 1075200.0000, 
raw observation next is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 19.0, 22.51317986594678, -0.2901169504698533, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8716528162511544, 0.7333333333333334, 0.4577777777777776, 0.03959484346224677, 0.08333333333333333, 0.37609832216223155, 0.40329434984338225, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85817754], dtype=float32), 1.0568447]. 
=============================================
[2019-04-05 14:25:38,085] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9267128e-04 9.2989695e-01 3.2135641e-04 3.6808117e-03 1.8640509e-02
 6.5978245e-05 4.6801675e-02], sum to 1.0000
[2019-04-05 14:25:38,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7970
[2019-04-05 14:25:38,160] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.8, 62.0, 74.0, 0.0, 21.0, 25.13556445050023, -0.02976082121525006, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1956600.0000, 
sim time next is 1957200.0000, 
raw observation next is [-2.8, 62.0, 66.66666666666667, 0.0, 20.0, 24.5454876862725, -0.01252558961326937, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.22222222222222224, 0.0, 0.16666666666666666, 0.5454573071893751, 0.4958248034622435, 1.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.4406965], dtype=float32), -1.2155805]. 
=============================================
[2019-04-05 14:25:39,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8906761e-05 9.8678988e-01 1.1385155e-03 3.2541156e-03 3.3803692e-03
 2.3670804e-05 5.3946045e-03], sum to 1.0000
[2019-04-05 14:25:39,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9111
[2019-04-05 14:25:39,096] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 20.24601976173644, -0.8724410527674372, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3022200.0000, 
sim time next is 3022800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 20.18902669415715, -0.8895402825961422, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.18241889117976248, 0.20348657246795263, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00238034], dtype=float32), -0.1968744]. 
=============================================
[2019-04-05 14:25:41,844] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 14:25:41,847] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:25:41,848] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:25:41,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:25:41,849] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:25:41,850] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:25:41,851] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:25:41,861] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run85
[2019-04-05 14:25:41,862] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run85
[2019-04-05 14:25:41,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run85
[2019-04-05 14:26:13,005] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11900558]
[2019-04-05 14:26:13,005] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.2, 75.5, 217.0, 214.0, 19.0, 20.29635138800033, -0.7897393441198374, 1.0, 1.0, 0.0]
[2019-04-05 14:26:13,005] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:26:13,007] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.4573246e-04 8.4746206e-01 2.2752101e-03 6.7381910e-03 1.2909991e-01
 1.9171482e-04 1.3887267e-02], sampled 0.7694910261218727
[2019-04-05 14:26:48,154] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11900558]
[2019-04-05 14:26:48,154] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [6.033333333333333, 89.16666666666667, 0.0, 0.0, 19.0, 19.71225045360307, -0.8786789165051515, 0.0, 1.0, 0.0]
[2019-04-05 14:26:48,155] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:26:48,156] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7835607e-06 8.4991246e-01 3.3046579e-04 6.7614075e-03 1.4189233e-01
 9.2466171e-06 1.0923154e-03], sampled 0.9614656624271739
[2019-04-05 14:26:54,250] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6286.7951 140770078.9285 -1882.1405
[2019-04-05 14:27:07,966] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5887.4138 172948521.9766 -2342.9997
[2019-04-05 14:27:13,570] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6062.0897 186998809.3411 -2242.7042
[2019-04-05 14:27:14,596] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1680000, evaluation results [1680000.0, 5887.413791422705, 172948521.97660255, -2342.9997197439548, 6286.795089029198, 140770078.9285001, -1882.1405415143395, 6062.089714678636, 186998809.34113124, -2242.704169461751]
[2019-04-05 14:27:15,106] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.4046685e-05 6.8632424e-01 6.0687410e-03 1.7072625e-01 1.3394196e-01
 8.7739510e-04 1.9673591e-03], sum to 1.0000
[2019-04-05 14:27:15,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9094
[2019-04-05 14:27:15,160] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.6, 77.0, 124.6666666666667, 58.16666666666666, 19.0, 18.90206844709632, -1.15093867629624, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1851600.0000, 
sim time next is 1852200.0000, 
raw observation next is [-5.6, 76.5, 120.0, 51.0, 19.5, 18.82587136989314, -1.150433978896151, 0.0, 1.0, 134385.4843374456], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.765, 0.4, 0.056353591160221, 0.125, 0.06882261415776163, 0.1165220070346163, 0.0, 1.0, 0.63993087779736], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.6698479], dtype=float32), 0.51819974]. 
=============================================
[2019-04-05 14:27:17,195] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0015458  0.37528253 0.00705213 0.00247666 0.47083277 0.00059195
 0.14221816], sum to 1.0000
[2019-04-05 14:27:17,198] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2104
[2019-04-05 14:27:17,218] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 82.00000000000001, 0.0, 0.0, 19.0, 19.72927972971998, -1.04963752203649, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2052600.0000, 
sim time next is 2053200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 19.41781272317399, -1.07272242231715, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.11815106026449929, 0.14242585922761664, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5942498], dtype=float32), 0.5152238]. 
=============================================
[2019-04-05 14:27:36,621] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7514205e-05 8.9780551e-01 2.3976230e-04 3.4201385e-03 9.7887650e-02
 2.3662516e-05 5.9577037e-04], sum to 1.0000
[2019-04-05 14:27:36,622] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4930
[2019-04-05 14:27:36,638] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.0, 72.0, 0.0, 0.0, 19.5, 19.79183490944398, -0.9064416983045941, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2678400.0000, 
sim time next is 2679000.0000, 
raw observation next is [-7.333333333333334, 71.5, 0.0, 0.0, 19.0, 19.85180868564811, -0.9130877603137906, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.2594644506001847, 0.715, 0.0, 0.0, 0.08333333333333333, 0.15431739047067575, 0.19563741322873648, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7593855], dtype=float32), -2.0469797]. 
=============================================
[2019-04-05 14:27:36,648] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[70.40667]
 [72.62925]
 [72.61017]
 [72.89313]
 [73.10304]], R is [[69.76760101]
 [69.99849701]
 [70.08422852]
 [70.24052429]
 [70.25240326]].
[2019-04-05 14:27:37,406] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2169005e-05 1.3224658e-01 1.5361976e-02 1.6939351e-02 8.2423800e-01
 1.8861884e-04 1.1013283e-02], sum to 1.0000
[2019-04-05 14:27:37,407] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3316
[2019-04-05 14:27:37,422] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.583333333333333, 65.0, 0.0, 0.0, 22.0, 20.07350585616474, -0.7576282140095821, 0.0, 1.0, 181373.6605015512], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 3013800.0000, 
sim time next is 3014400.0000, 
raw observation next is [-3.666666666666667, 65.0, 0.0, 0.0, 22.5, 20.29639667009753, -0.7132887437987313, 0.0, 1.0, 104315.8855346239], 
processed observation next is [0.0, 0.9130434782608695, 0.3610341643582641, 0.65, 0.0, 0.0, 0.375, 0.1913663891747941, 0.2622370854004229, 0.0, 1.0, 0.4967423120696376], 
reward next is 0.5000, 
noisyNet noise sample is [array([-0.8622662], dtype=float32), 1.3318523]. 
=============================================
[2019-04-05 14:27:44,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7408567e-05 9.6348113e-01 1.5998144e-03 5.3147494e-04 3.0581895e-02
 3.8523198e-05 3.7498942e-03], sum to 1.0000
[2019-04-05 14:27:44,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6675
[2019-04-05 14:27:44,456] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.700000000000001, 81.33333333333334, 0.0, 0.0, 19.0, 19.36281175032472, -1.092003853416412, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2096400.0000, 
sim time next is 2097000.0000, 
raw observation next is [-6.7, 80.5, 0.0, 0.0, 19.0, 19.17666010994719, -1.117508914969302, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.805, 0.0, 0.0, 0.08333333333333333, 0.09805500916226577, 0.127497028343566, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0275912], dtype=float32), -0.34368742]. 
=============================================
[2019-04-05 14:27:44,463] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.347466]
 [72.8967  ]
 [73.38178 ]
 [73.95991 ]
 [74.16726 ]], R is [[72.27094269]
 [72.54823303]
 [72.82275391]
 [73.0945282 ]
 [73.36358643]].
[2019-04-05 14:27:45,181] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5459866e-05 7.5953072e-01 1.3578295e-03 1.1062040e-02 2.1063125e-01
 5.4048887e-04 1.6822295e-02], sum to 1.0000
[2019-04-05 14:27:45,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7178
[2019-04-05 14:27:45,211] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 47.0, 86.0, 341.0, 19.0, 19.64041689464626, -0.9320834712760767, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2388600.0000, 
sim time next is 2389200.0000, 
raw observation next is [0.0, 47.0, 84.83333333333334, 293.8333333333334, 19.0, 19.63352400523275, -0.9400445877607795, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.2827777777777778, 0.3246777163904237, 0.08333333333333333, 0.13612700043606255, 0.18665180407974016, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60289264], dtype=float32), 0.40726313]. 
=============================================
[2019-04-05 14:27:51,968] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-05 14:27:51,970] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:27:51,970] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:27:51,971] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:27:51,971] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:27:51,971] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:27:51,972] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:27:51,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run86
[2019-04-05 14:27:52,004] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run86
[2019-04-05 14:27:52,026] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run86
[2019-04-05 14:28:18,814] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11931299]
[2019-04-05 14:28:18,815] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.333333333333333, 80.33333333333333, 56.66666666666666, 232.6666666666667, 19.0, 19.06886161958045, -1.095500857587158, 0.0, 1.0, 0.0]
[2019-04-05 14:28:18,816] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:28:18,818] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3613504e-05 7.5848764e-01 1.3039627e-03 1.1397883e-02 2.2619039e-01
 3.5032812e-05 2.5713649e-03], sampled 0.16681236768210372
[2019-04-05 14:28:25,804] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11931299]
[2019-04-05 14:28:25,804] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.466666666666667, 83.0, 124.8333333333333, 0.0, 19.0, 20.25766006198015, -0.7861388172781486, 0.0, 1.0, 0.0]
[2019-04-05 14:28:25,804] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:28:25,805] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5231757e-05 7.8847098e-01 1.2266551e-03 7.0095160e-03 2.0079920e-01
 2.8776049e-05 2.4496047e-03], sampled 0.6725483981577796
[2019-04-05 14:28:45,697] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11931299]
[2019-04-05 14:28:45,697] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.376595068, 38.40750849, 230.0731368, 136.55475605, 19.5, 20.43243152074832, -0.846827511177148, 1.0, 1.0, 0.0]
[2019-04-05 14:28:45,697] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:28:45,698] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.00379874 0.7337297  0.01128434 0.01509394 0.1939872  0.00145121
 0.04065489], sampled 0.6536759008386189
[2019-04-05 14:28:46,112] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11931299]
[2019-04-05 14:28:46,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.904696060666667, 49.64387623333333, 0.0, 0.0, 21.0, 18.94482566224138, -1.071992247497437, 0.0, 1.0, 196217.9094192961]
[2019-04-05 14:28:46,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:28:46,114] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.0812632e-05 7.9112703e-01 3.2143001e-03 7.8693833e-03 1.9066396e-01
 8.7704662e-05 6.9768252e-03], sampled 0.27947494480774226
[2019-04-05 14:29:06,219] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6398.7450 142385841.4838 -1751.4580
[2019-04-05 14:29:14,244] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.11931299]
[2019-04-05 14:29:14,244] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.611166903, 64.71509332000001, 0.0, 0.0, 19.0, 20.19993045626045, -0.8673227013110232, 0.0, 1.0, 0.0]
[2019-04-05 14:29:14,244] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:29:14,245] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.5384558e-06 8.1130517e-01 6.8085786e-04 7.4313981e-03 1.7901747e-01
 2.0973019e-05 1.5386596e-03], sampled 0.43943481264918227
[2019-04-05 14:29:19,573] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5995.4071 173158101.2034 -2275.8882
[2019-04-05 14:29:24,533] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6010.1267 187574070.1010 -2225.4703
[2019-04-05 14:29:25,561] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1700000, evaluation results [1700000.0, 5995.407146692238, 173158101.20344928, -2275.888210872978, 6398.745001670695, 142385841.48377842, -1751.4579537531854, 6010.126721970208, 187574070.10103747, -2225.470287294018]
[2019-04-05 14:29:30,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6534801e-05 9.6073437e-01 4.4586716e-04 1.4625605e-04 3.3358578e-02
 4.4527544e-05 5.2539674e-03], sum to 1.0000
[2019-04-05 14:29:30,603] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3653
[2019-04-05 14:29:30,620] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 26.0, 109.0, 812.0, 19.0, 23.26618696938604, -0.1495063232692413, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4024800.0000, 
sim time next is 4025400.0000, 
raw observation next is [-2.833333333333333, 25.0, 107.3333333333333, 806.0, 19.0, 23.11368787400031, -0.1647642022339335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3841181902123731, 0.25, 0.3577777777777777, 0.8906077348066298, 0.08333333333333333, 0.42614065616669244, 0.4450785992553555, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.088115], dtype=float32), 0.6531634]. 
=============================================
[2019-04-05 14:29:31,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9411571e-05 9.6921074e-01 2.3084430e-03 5.9963623e-03 2.0986345e-02
 6.1319144e-05 1.4074751e-03], sum to 1.0000
[2019-04-05 14:29:31,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2190
[2019-04-05 14:29:31,973] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 19.2428309345003, -1.042056316259941, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4066200.0000, 
sim time next is 4066800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 19.25257755560375, -1.054312865233373, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.10438146296697919, 0.1485623782555423, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0646617], dtype=float32), -0.4402709]. 
=============================================
[2019-04-05 14:29:32,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1245199e-04 9.1752279e-01 3.7252225e-03 2.5355427e-03 1.9241963e-02
 6.8023423e-05 5.6293972e-02], sum to 1.0000
[2019-04-05 14:29:32,485] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7708
[2019-04-05 14:29:32,492] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 19.0, 20.96703126813827, -0.6562192198409809, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2051400.0000, 
sim time next is 2052000.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 20.98390377344457, -0.6553568450503607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.24865864778704752, 0.2815477183165464, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5539535], dtype=float32), 0.858447]. 
=============================================
[2019-04-05 14:29:32,498] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[57.753723]
 [57.57358 ]
 [57.315407]
 [57.120895]
 [56.850544]], R is [[57.40001678]
 [56.82601547]
 [56.25775528]
 [55.69517899]
 [55.13822937]].
[2019-04-05 14:29:39,812] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.22335227e-03 8.34552228e-01 1.11570721e-02 3.42955813e-02
 1.14993095e-01 1.16577779e-04 3.66212032e-03], sum to 1.0000
[2019-04-05 14:29:39,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2548
[2019-04-05 14:29:39,833] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 19.0, 18.81212643636505, -1.181213183739895, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2267400.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 19.0, 18.79034958351054, -1.19694790097734, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.08333333333333333, 0.06586246529254502, 0.10101736634088665, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3103988], dtype=float32), -0.5111186]. 
=============================================
[2019-04-05 14:29:39,841] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.608215]
 [70.7846  ]
 [71.06996 ]
 [71.21552 ]
 [71.57911 ]], R is [[70.70855713]
 [71.00147247]
 [71.29145813]
 [71.57854462]
 [71.86276245]].
[2019-04-05 14:29:55,172] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3994729e-04 6.5497732e-01 3.1023601e-03 1.6117864e-03 2.7941832e-01
 1.8965249e-04 6.0360543e-02], sum to 1.0000
[2019-04-05 14:29:55,176] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5827
[2019-04-05 14:29:55,187] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [6.666666666666666, 26.0, 114.1666666666667, 0.0, 19.0, 20.78894745651813, -0.7703854476103985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2817600.0000, 
sim time next is 2818200.0000, 
raw observation next is [6.833333333333334, 25.0, 110.3333333333333, 0.0, 19.0, 20.94971173160451, -0.7559012994796731, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.6086956521739131, 0.651892890120037, 0.25, 0.36777777777777765, 0.0, 0.08333333333333333, 0.24580931096704242, 0.2480329001734423, 1.0, 1.0, 0.04447717064340604], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23119107], dtype=float32), 1.8603374]. 
=============================================
[2019-04-05 14:29:58,792] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7323831e-05 8.4358352e-01 1.1450413e-03 9.4333175e-04 1.3687353e-01
 3.7031030e-05 1.7400172e-02], sum to 1.0000
[2019-04-05 14:29:58,796] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2973
[2019-04-05 14:29:58,812] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.166666666666667, 50.66666666666667, 111.3333333333333, 784.0, 19.0, 23.47173083233768, -0.02365002281615902, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3333000.0000, 
sim time next is 3333600.0000, 
raw observation next is [-4.0, 50.0, 110.0, 776.0, 19.0, 23.74072242531321, -0.002882520165279012, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.5, 0.36666666666666664, 0.8574585635359117, 0.08333333333333333, 0.47839353544276736, 0.49903915994490705, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5033259], dtype=float32), -0.8621972]. 
=============================================
[2019-04-05 14:29:59,553] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6202849e-04 8.6920238e-01 4.1527618e-03 1.6304928e-04 1.1467515e-01
 3.0067058e-05 1.1214477e-02], sum to 1.0000
[2019-04-05 14:29:59,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5151
[2019-04-05 14:29:59,569] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 19.0, 22.14660810729956, -0.3618905269980774, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3780600.0000, 
sim time next is 3781200.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 19.0, 22.24548041217005, -0.4838317066694215, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.35379003434750417, 0.33872276444352617, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.155868], dtype=float32), -0.52497035]. 
=============================================
[2019-04-05 14:30:01,584] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.0815260e-04 7.3738670e-01 4.9555395e-03 8.7825470e-03 1.4815482e-01
 2.5694659e-05 9.9886514e-02], sum to 1.0000
[2019-04-05 14:30:01,585] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6515
[2019-04-05 14:30:01,598] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 77.0, 34.0, 307.5, 19.0, 22.61417477069041, -0.2499633422373281, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3258000.0000, 
sim time next is 3258600.0000, 
raw observation next is [-4.0, 75.0, 25.66666666666666, 239.6666666666666, 19.0, 22.71729916716171, -0.3601745090457335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.75, 0.08555555555555554, 0.2648250460405156, 0.08333333333333333, 0.3931082639301424, 0.3799418303180888, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0643084], dtype=float32), 1.7463102]. 
=============================================
[2019-04-05 14:30:02,607] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 14:30:02,608] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:30:02,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:30:02,609] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:30:02,609] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:30:02,610] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:30:02,610] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:30:02,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run87
[2019-04-05 14:30:02,637] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run87
[2019-04-05 14:30:02,656] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run87
[2019-04-05 14:30:27,498] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.120615035]
[2019-04-05 14:30:27,498] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.982185796, 88.69160864, 0.0, 0.0, 19.0, 18.51272815835177, -1.105159674798296, 0.0, 1.0, 177871.0837325757]
[2019-04-05 14:30:27,498] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:30:27,499] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3825075e-06 9.2895228e-01 1.8064109e-04 1.7140744e-03 6.8073519e-02
 5.7137254e-06 1.0714523e-03], sampled 0.9809023582084561
[2019-04-05 14:30:28,466] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.120615035]
[2019-04-05 14:30:28,467] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [8.950000000000001, 96.66666666666666, 0.0, 0.0, 19.0, 19.51373379232679, -0.7832280639243995, 0.0, 1.0, 0.0]
[2019-04-05 14:30:28,467] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:30:28,468] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.9233257e-07 9.0266532e-01 1.0503815e-04 2.4794762e-03 9.4269566e-02
 1.4026144e-06 4.7892323e-04], sampled 0.2818726810325004
[2019-04-05 14:30:33,913] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.120615035]
[2019-04-05 14:30:33,914] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.307177, 100.0, 0.0, 0.0, 19.0, 19.14692508835068, -1.029357984026472, 1.0, 1.0, 0.0]
[2019-04-05 14:30:33,915] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:30:33,916] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.6990752e-06 8.4587067e-01 6.1153702e-04 5.9038559e-03 1.4587812e-01
 1.6649317e-05 1.7135007e-03], sampled 0.9788172681162177
[2019-04-05 14:31:05,871] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.120615035]
[2019-04-05 14:31:05,871] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.385962788, 74.47516922, 0.0, 0.0, 19.0, 19.52348221486598, -1.029859400257423, 0.0, 1.0, 0.0]
[2019-04-05 14:31:05,872] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:31:05,873] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.9297191e-06 8.2460535e-01 9.3404064e-04 7.2128871e-03 1.6433263e-01
 2.5365945e-05 2.8807926e-03], sampled 0.8998321626265063
[2019-04-05 14:31:16,231] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6316.9454 140474867.5491 -1867.6385
[2019-04-05 14:31:28,080] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5949.8690 170436775.9062 -2369.6740
[2019-04-05 14:31:33,509] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6015.1222 184951928.5053 -2310.9022
[2019-04-05 14:31:34,535] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1720000, evaluation results [1720000.0, 5949.868968805121, 170436775.9061794, -2369.673981167467, 6316.945401356826, 140474867.5490837, -1867.6384722440475, 6015.122235479606, 184951928.50533912, -2310.9021796731486]
[2019-04-05 14:31:39,170] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9383170e-06 3.6696252e-01 3.3814820e-06 9.5289986e-05 6.3293064e-01
 2.1168443e-08 5.2673613e-06], sum to 1.0000
[2019-04-05 14:31:39,173] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7531
[2019-04-05 14:31:39,185] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.25, 73.0, 0.0, 0.0, 21.0, 23.21544186279836, -0.1837523275865943, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 4307400.0000, 
sim time next is 4308000.0000, 
raw observation next is [5.199999999999999, 73.0, 0.0, 0.0, 21.5, 23.15409509942318, -0.1961341867966064, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.6066481994459835, 0.73, 0.0, 0.0, 0.2916666666666667, 0.42950792495193174, 0.4346219377344645, 0.0, 1.0, 0.0], 
reward next is 0.6429, 
noisyNet noise sample is [array([-0.2678547], dtype=float32), -0.4960296]. 
=============================================
[2019-04-05 14:31:39,203] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[103.550735]
 [103.739586]
 [103.82561 ]
 [103.982315]
 [103.9727  ]], R is [[102.92525482]
 [102.61029053]
 [102.36990356]
 [102.20334625]
 [102.10988617]].
[2019-04-05 14:31:45,142] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7350917e-06 9.3723661e-01 6.2076477e-05 4.2793225e-03 5.6854106e-02
 1.3038792e-05 1.5531274e-03], sum to 1.0000
[2019-04-05 14:31:45,143] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1045
[2019-04-05 14:31:45,151] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.666666666666667, 100.0, 0.0, 0.0, 19.0, 22.41100347344699, -0.2343066389346188, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3180000.0000, 
sim time next is 3180600.0000, 
raw observation next is [3.5, 100.0, 0.0, 0.0, 19.0, 22.29554181621903, -0.254945526001043, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5595567867036012, 1.0, 0.0, 0.0, 0.08333333333333333, 0.3579618180182524, 0.41501815799965236, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.14403], dtype=float32), 1.4853187]. 
=============================================
[2019-04-05 14:31:45,581] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6337619e-07 8.7844473e-01 2.2132811e-03 3.4204859e-03 1.1115437e-01
 1.6069156e-06 4.7646426e-03], sum to 1.0000
[2019-04-05 14:31:45,581] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2790
[2019-04-05 14:31:45,595] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 98.83333333333334, 0.0, 0.0, 19.0, 19.22483618325222, -1.102582145344815, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2872200.0000, 
sim time next is 2872800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 19.10605796958022, -1.128647096186205, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.09217149746501836, 0.12378430127126501, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0478767], dtype=float32), -0.28855535]. 
=============================================
[2019-04-05 14:31:47,446] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0938812e-05 3.1901139e-01 4.7757951e-04 6.1076302e-02 6.1807245e-01
 6.7331355e-05 1.2639912e-03], sum to 1.0000
[2019-04-05 14:31:47,448] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1734
[2019-04-05 14:31:47,464] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 88.33333333333334, 0.0, 0.0, 23.0, 21.0042241024325, -0.6879534796400207, 0.0, 1.0, 54413.96819234148], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 2863200.0000, 
sim time next is 2863800.0000, 
raw observation next is [1.0, 89.5, 0.0, 0.0, 22.0, 21.04645987098683, -0.6952270873396592, 0.0, 1.0, 55200.22734745018], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.895, 0.0, 0.0, 0.3333333333333333, 0.25387165591556915, 0.26825763755344695, 0.0, 1.0, 0.26285822546404847], 
reward next is 0.5714, 
noisyNet noise sample is [array([-0.591931], dtype=float32), -2.892049]. 
=============================================
[2019-04-05 14:31:48,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:31:48,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:31:48,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run14
[2019-04-05 14:31:54,181] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5321303e-06 4.2662725e-01 1.1151760e-03 5.3942108e-01 2.2359340e-02
 9.6815475e-06 1.0465001e-02], sum to 1.0000
[2019-04-05 14:31:54,182] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2855
[2019-04-05 14:31:54,188] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.5, 70.83333333333333, 0.0, 0.0, 19.0, 21.49610869726064, -0.5650854538924418, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3708600.0000, 
sim time next is 3709200.0000, 
raw observation next is [-1.0, 69.66666666666667, 0.0, 0.0, 19.0, 21.40960015177759, -0.5842626194004543, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.6966666666666668, 0.0, 0.0, 0.08333333333333333, 0.2841333459814657, 0.3052457935331819, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5114857], dtype=float32), -0.32087848]. 
=============================================
[2019-04-05 14:31:56,205] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8143493e-06 9.8782676e-01 1.3859905e-04 1.3623442e-04 1.1419172e-02
 3.0209725e-07 4.7714383e-04], sum to 1.0000
[2019-04-05 14:31:56,206] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1055
[2019-04-05 14:31:56,214] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 20.19641522909651, -0.7295184156689855, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4741200.0000, 
sim time next is 4741800.0000, 
raw observation next is [-2.166666666666667, 84.83333333333334, 0.0, 0.0, 19.0, 20.14432182020802, -0.7429065811459256, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4025854108956602, 0.8483333333333334, 0.0, 0.0, 0.08333333333333333, 0.17869348501733504, 0.25236447295135817, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1841406], dtype=float32), 1.245781]. 
=============================================
[2019-04-05 14:31:59,884] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2759132e-07 9.9993515e-01 1.7326003e-06 3.5462930e-05 1.3890305e-05
 4.7597959e-07 1.2287201e-05], sum to 1.0000
[2019-04-05 14:31:59,885] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5830
[2019-04-05 14:31:59,904] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 19.0, 23.97019347134314, -0.1097946175696006, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4365600.0000, 
sim time next is 4366200.0000, 
raw observation next is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 19.0, 23.97943733150081, 0.01382105611015793, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8688827331486613, 0.2883333333333334, 0.39, 0.9384898710865562, 0.08333333333333333, 0.4982864442917343, 0.504607018703386, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.94079304], dtype=float32), -1.0067372]. 
=============================================
[2019-04-05 14:32:04,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0259921e-05 8.1160253e-01 6.9526578e-03 2.4306582e-02 1.2750213e-01
 3.8066276e-04 2.9205224e-02], sum to 1.0000
[2019-04-05 14:32:04,334] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1157
[2019-04-05 14:32:04,353] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [3.0, 37.0, 58.0, 247.0, 19.0, 25.65926685994647, 0.3218380430458904, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4122000.0000, 
sim time next is 4122600.0000, 
raw observation next is [3.0, 36.5, 46.33333333333333, 183.6666666666666, 19.0, 25.40811868631576, 0.2865266919928891, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.365, 0.15444444444444444, 0.20294659300184154, 0.08333333333333333, 0.6173432238596467, 0.595508897330963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0544713], dtype=float32), 0.028836628]. 
=============================================
[2019-04-05 14:32:05,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5233442e-06 2.3804241e-01 4.3595058e-04 4.3426566e-03 7.5634861e-01
 1.0777282e-05 8.1510050e-04], sum to 1.0000
[2019-04-05 14:32:05,547] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2994
[2019-04-05 14:32:05,558] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8166666666666667, 71.33333333333334, 0.0, 0.0, 20.0, 20.36429959276201, -0.7846951781916908, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4510200.0000, 
sim time next is 4510800.0000, 
raw observation next is [-0.8, 71.0, 0.0, 0.0, 20.5, 20.26657096923769, -0.7962366161018974, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.4404432132963989, 0.71, 0.0, 0.0, 0.20833333333333334, 0.1888809141031409, 0.23458779463270085, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.94624543], dtype=float32), 0.54468817]. 
=============================================
[2019-04-05 14:32:09,604] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-05 14:32:09,604] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:32:09,605] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:32:09,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:32:09,606] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:32:09,606] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:32:09,608] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:32:09,618] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run88
[2019-04-05 14:32:09,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run88
[2019-04-05 14:32:09,638] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run88
[2019-04-05 14:33:18,178] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12193428]
[2019-04-05 14:33:18,181] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.85, 88.0, 77.33333333333334, 68.33333333333333, 19.0, 24.19170344864868, 0.3347024740411753, 0.0, 1.0, 0.0]
[2019-04-05 14:33:18,181] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:33:18,182] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.13120810e-08 9.52524841e-01 2.86574232e-05 1.52968208e-03
 4.58050705e-02 1.39392071e-07 1.11648005e-04], sampled 0.12721258488588283
[2019-04-05 14:33:21,090] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6233.2316 141832409.6529 -1995.3364
[2019-04-05 14:33:33,411] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5813.6225 171210397.4383 -2465.1243
[2019-04-05 14:33:41,009] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5926.3927 185178719.4458 -2339.0482
[2019-04-05 14:33:42,036] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1740000, evaluation results [1740000.0, 5813.622496938426, 171210397.43826783, -2465.1243294319597, 6233.231581094865, 141832409.6528519, -1995.3363545988666, 5926.3926810951025, 185178719.44578722, -2339.048194752059]
[2019-04-05 14:33:42,562] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1251065e-04 9.7735786e-01 8.3577749e-04 9.1215475e-03 1.0047539e-02
 1.9434596e-04 1.9304377e-03], sum to 1.0000
[2019-04-05 14:33:42,565] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4897
[2019-04-05 14:33:42,578] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5000000000000001, 50.0, 102.0, 588.0, 19.0, 20.74206909167393, -0.6986359550599778, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5043000.0000, 
sim time next is 5043600.0000, 
raw observation next is [1.0, 47.0, 104.5, 615.5, 19.0, 20.93672379702821, -0.6783971949264097, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4903047091412743, 0.47, 0.34833333333333333, 0.6801104972375691, 0.08333333333333333, 0.24472698308568427, 0.2738676016911968, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32360175], dtype=float32), 0.7876287]. 
=============================================
[2019-04-05 14:33:45,704] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:33:45,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:33:45,731] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run14
[2019-04-05 14:33:49,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5008939e-05 8.7373596e-01 3.3195611e-05 2.6421566e-03 5.0048102e-02
 1.4218745e-05 7.3511310e-02], sum to 1.0000
[2019-04-05 14:33:49,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3430
[2019-04-05 14:33:49,601] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 19.0, 22.49406732808683, -0.3469259812598144, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3858000.0000, 
sim time next is 3858600.0000, 
raw observation next is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 19.0, 22.54161030496871, -0.4177366520410755, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.2644444444444445, 0.7311233885819522, 0.08333333333333333, 0.3784675254140592, 0.3607544493196415, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3899605], dtype=float32), -1.6169419]. 
=============================================
[2019-04-05 14:33:53,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0429909e-03 8.5163599e-01 6.3755605e-03 6.9035389e-03 7.3554970e-02
 2.9824709e-04 6.0188651e-02], sum to 1.0000
[2019-04-05 14:33:53,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0596
[2019-04-05 14:33:53,248] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.83333333333334, 0.0, 0.0, 19.0, 19.97879562293077, -0.8290449161548823, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4565400.0000, 
sim time next is 4566000.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 19.0, 19.93851459494013, -0.840225115486939, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.08333333333333333, 0.16154288291167754, 0.21992496150435367, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2839518], dtype=float32), -1.1762218]. 
=============================================
[2019-04-05 14:33:53,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[71.4939  ]
 [72.24561 ]
 [73.059135]
 [68.87162 ]
 [69.0801  ]], R is [[74.61489105]
 [73.8687439 ]
 [73.13005829]
 [73.39875793]
 [72.66477203]].
[2019-04-05 14:33:53,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.4122221e-09 9.9655521e-01 2.2836731e-04 5.1063973e-05 3.1305500e-03
 3.1083567e-07 3.4483339e-05], sum to 1.0000
[2019-04-05 14:33:53,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0471
[2019-04-05 14:33:53,837] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 19.0, 21.31311497837194, -0.5481437804920499, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3669000.0000, 
sim time next is 3669600.0000, 
raw observation next is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 19.0, 21.32019484207975, -0.5433166832441068, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.7211449676823639, 0.31, 0.383888888888889, 0.8918968692449355, 0.08333333333333333, 0.27668290350664587, 0.31889443891863106, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66553783], dtype=float32), 2.0726745]. 
=============================================
[2019-04-05 14:33:58,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8749901e-04 8.7077558e-01 7.6543707e-03 1.1368204e-02 6.2970795e-02
 5.1880376e-03 4.1355584e-02], sum to 1.0000
[2019-04-05 14:33:58,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0711
[2019-04-05 14:33:58,960] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.833333333333333, 34.66666666666667, 61.33333333333334, 312.6666666666667, 19.0, 18.79548648525905, -1.131986605767449, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4090200.0000, 
sim time next is 4090800.0000, 
raw observation next is [-3.666666666666667, 35.33333333333334, 76.66666666666667, 390.8333333333334, 19.0, 18.88480441319618, -1.089587285211257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3610341643582641, 0.35333333333333344, 0.2555555555555556, 0.43186003683241264, 0.08333333333333333, 0.07373370109968154, 0.13680423826291435, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2668005], dtype=float32), 0.84970385]. 
=============================================
[2019-04-05 14:34:00,010] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4995437e-06 9.9859637e-01 7.5975677e-06 9.9643294e-05 1.1233629e-03
 9.3711205e-06 1.6226248e-04], sum to 1.0000
[2019-04-05 14:34:00,010] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8922
[2019-04-05 14:34:00,026] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.166666666666667, 67.33333333333334, 0.0, 0.0, 23.0, 21.24446238014962, -0.6118332779565299, 0.0, 1.0, 42670.21772164668], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 4590600.0000, 
sim time next is 4591200.0000, 
raw observation next is [-1.233333333333333, 67.66666666666667, 0.0, 0.0, 22.0, 21.29882857770626, -0.5985288815812337, 0.0, 1.0, 42314.35964629861], 
processed observation next is [1.0, 0.13043478260869565, 0.4284395198522623, 0.6766666666666667, 0.0, 0.0, 0.3333333333333333, 0.27490238147552165, 0.3004903728062554, 0.0, 1.0, 0.20149695069666004], 
reward next is 0.5714, 
noisyNet noise sample is [array([0.7081867], dtype=float32), 0.3676788]. 
=============================================
[2019-04-05 14:34:00,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:34:00,898] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:34:00,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run14
[2019-04-05 14:34:04,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:34:04,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:34:04,148] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run14
[2019-04-05 14:34:06,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0988871e-06 9.8284596e-01 1.8157125e-04 4.6126184e-04 1.6209582e-02
 7.1078830e-05 2.2644929e-04], sum to 1.0000
[2019-04-05 14:34:06,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6684
[2019-04-05 14:34:06,795] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 18.69750998675356, -1.158273962089425, 0.0, 1.0, 118647.0156651413], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3982200.0000, 
sim time next is 3982800.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 19.0, 18.58619391245229, -1.149242796685627, 0.0, 1.0, 138158.6647177763], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.08333333333333333, 0.0488494927043576, 0.1169190677714577, 0.0, 1.0, 0.6578984034179823], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2845755], dtype=float32), 0.60820615]. 
=============================================
[2019-04-05 14:34:09,777] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7383204e-04 7.7117509e-01 7.3621655e-04 2.9997744e-03 2.0079124e-01
 8.1518209e-03 1.5172071e-02], sum to 1.0000
[2019-04-05 14:34:09,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6415
[2019-04-05 14:34:09,795] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-14.0, 69.0, 0.0, 0.0, 19.0, 18.51755275276086, -1.217886251089667, 0.0, 1.0, 40292.27413614161], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3999600.0000, 
sim time next is 4000200.0000, 
raw observation next is [-13.83333333333333, 68.0, 0.0, 0.0, 19.0, 18.54022330186886, -1.221053528311485, 0.0, 1.0, 27307.27565828646], 
processed observation next is [1.0, 0.30434782608695654, 0.07940904893813489, 0.68, 0.0, 0.0, 0.08333333333333333, 0.04501860848907165, 0.09298215722950502, 0.0, 1.0, 0.13003464599184028], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14685398], dtype=float32), 0.8339485]. 
=============================================
[2019-04-05 14:34:13,125] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4047233e-06 9.9387485e-01 2.0564752e-04 2.3221334e-03 3.3943804e-03
 5.5800359e-07 2.0087603e-04], sum to 1.0000
[2019-04-05 14:34:13,129] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8483
[2019-04-05 14:34:13,137] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.666666666666667, 38.0, 83.33333333333333, 548.0, 19.0, 22.93492132849495, -0.1915115067679304, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4206000.0000, 
sim time next is 4206600.0000, 
raw observation next is [2.5, 38.5, 68.0, 550.0, 19.0, 22.94528893305237, -0.1875473842916511, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5318559556786704, 0.385, 0.22666666666666666, 0.6077348066298343, 0.08333333333333333, 0.4121074110876976, 0.4374842052361163, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3619241], dtype=float32), -0.5533245]. 
=============================================
[2019-04-05 14:34:13,673] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6196005e-04 7.6882291e-01 4.6273186e-03 1.0671748e-02 1.9630268e-01
 5.7178404e-04 1.8241676e-02], sum to 1.0000
[2019-04-05 14:34:13,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2220
[2019-04-05 14:34:13,723] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.166666666666667, 37.33333333333334, 96.0, 539.0, 19.0, 21.8639288928205, -0.5802306674073326, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4092600.0000, 
sim time next is 4093200.0000, 
raw observation next is [-3.0, 38.0, 98.0, 574.0, 19.0, 21.91709517011375, -0.5640850855352739, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3795013850415513, 0.38, 0.32666666666666666, 0.6342541436464089, 0.08333333333333333, 0.3264245975094792, 0.3119716381549087, 1.0, 1.0, 0.0], 
reward next is 0.3591, 
noisyNet noise sample is [array([-0.40187377], dtype=float32), -1.2440974]. 
=============================================
[2019-04-05 14:34:14,527] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1711433e-05 9.1946417e-01 2.0716442e-03 7.1454071e-04 7.7076443e-02
 1.5170015e-04 5.0980982e-04], sum to 1.0000
[2019-04-05 14:34:14,528] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3123
[2019-04-05 14:34:14,553] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.6, 47.0, 0.0, 0.0, 19.0, 19.46536230651846, -1.088411351558269, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 421200.0000, 
sim time next is 421800.0000, 
raw observation next is [-10.6, 47.33333333333334, 0.0, 0.0, 19.0, 19.34070457333872, -1.11776444788772, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47333333333333344, 0.0, 0.0, 0.08333333333333333, 0.11172538111155994, 0.12741185070409333, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.29678], dtype=float32), -0.19917946]. 
=============================================
[2019-04-05 14:34:15,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:34:15,034] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:34:15,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run14
[2019-04-05 14:34:15,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3652233e-04 8.8862121e-01 6.5945520e-04 1.4728019e-03 9.6731544e-02
 5.1844405e-04 1.1860057e-02], sum to 1.0000
[2019-04-05 14:34:15,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8255
[2019-04-05 14:34:15,627] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.5, 118.0, 0.0, 19.0, 22.02812331455599, -0.5103241235481812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4534200.0000, 
sim time next is 4534800.0000, 
raw observation next is [2.0, 51.0, 119.5, 0.0, 19.0, 22.00872385917784, -0.5165606777011496, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.51, 0.3983333333333333, 0.0, 0.08333333333333333, 0.33406032159815346, 0.32781310743295017, 1.0, 1.0, 0.0], 
reward next is 0.8344, 
noisyNet noise sample is [array([-0.81963205], dtype=float32), -0.64078677]. 
=============================================
[2019-04-05 14:34:15,698] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-05 14:34:15,699] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:34:15,699] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:34:15,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:34:15,700] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:34:15,701] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:34:15,703] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:34:15,710] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run89
[2019-04-05 14:34:15,731] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run89
[2019-04-05 14:34:15,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run89
[2019-04-05 14:34:24,645] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12198087]
[2019-04-05 14:34:24,645] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-9.3, 69.0, 0.0, 0.0, 19.5, 18.7113646496629, -1.213367197717857, 0.0, 1.0, 122088.6699220411]
[2019-04-05 14:34:24,646] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:34:24,646] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.7107706e-05 7.4674958e-01 1.8294012e-03 2.0785559e-02 2.2632734e-01
 9.1074398e-05 4.1899458e-03], sampled 0.402865318732055
[2019-04-05 14:34:53,257] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12198087]
[2019-04-05 14:34:53,257] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.847616375, 75.3462901, 66.558113655, 171.4249874, 19.0, 18.75165614785097, -1.194594583789073, 0.0, 1.0, 0.0]
[2019-04-05 14:34:53,257] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:34:53,258] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.9341996e-05 7.8367001e-01 1.2593851e-03 1.4838490e-02 1.9654986e-01
 5.0385468e-05 3.6125649e-03], sampled 0.5370276007365501
[2019-04-05 14:35:27,809] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6295.4049 142672044.7571 -1882.6780
[2019-04-05 14:35:39,279] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5876.2140 172148532.7258 -2448.7161
[2019-04-05 14:35:45,125] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5896.2749 185316677.9420 -2416.0214
[2019-04-05 14:35:46,150] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1760000, evaluation results [1760000.0, 5876.213953340281, 172148532.72578543, -2448.716146926342, 6295.404879549278, 142672044.75714788, -1882.6780321217923, 5896.274869246292, 185316677.94197437, -2416.0213955443273]
[2019-04-05 14:35:46,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6033690e-04 3.3574039e-01 4.6154420e-04 6.2206909e-03 6.5542352e-01
 1.5046472e-05 1.9784875e-03], sum to 1.0000
[2019-04-05 14:35:46,421] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1092
[2019-04-05 14:35:46,438] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 19.0, 18.81133488170679, -1.167516133947381, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 180000.0000, 
sim time next is 180600.0000, 
raw observation next is [-8.9, 74.66666666666667, 0.0, 0.0, 19.0, 18.7215548503863, -1.189967908311844, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.060129570865525096, 0.10334403056271868, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0383784], dtype=float32), 1.4296027]. 
=============================================
[2019-04-05 14:35:46,866] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5409543e-05 6.2180001e-01 9.7282481e-04 5.3080712e-02 1.6873680e-01
 1.0088498e-04 1.5526332e-01], sum to 1.0000
[2019-04-05 14:35:46,872] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6096
[2019-04-05 14:35:46,892] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 21.5, 19.49765401515712, -0.8528784430927986, 0.0, 1.0, 165416.6544821159], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4158000.0000, 
sim time next is 4158600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 20.5, 19.79777886820427, -0.8048313048850755, 0.0, 1.0, 92147.09684480965], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.20833333333333334, 0.14981490568368905, 0.2317228983716415, 0.0, 1.0, 0.4387956992609984], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.8354179], dtype=float32), 0.6857965]. 
=============================================
[2019-04-05 14:35:47,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:35:47,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:35:47,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run14
[2019-04-05 14:35:47,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:35:47,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:35:47,606] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run14
[2019-04-05 14:35:52,363] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:35:52,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:35:52,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run14
[2019-04-05 14:35:52,795] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0172965e-05 9.9939895e-01 6.4745851e-05 6.3776839e-05 3.1160782e-04
 2.2903806e-07 1.4052258e-04], sum to 1.0000
[2019-04-05 14:35:52,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6550
[2019-04-05 14:35:52,810] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.15, 49.5, 107.0, 677.0, 19.0, 21.87901527691892, -0.4761935390529611, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4354200.0000, 
sim time next is 4354800.0000, 
raw observation next is [8.766666666666666, 47.0, 108.3333333333333, 694.1666666666667, 19.0, 22.14806935066473, -0.4354936569166595, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7054478301015698, 0.47, 0.361111111111111, 0.767034990791897, 0.08333333333333333, 0.34567244588872753, 0.35483544769444686, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.550016], dtype=float32), -0.8427714]. 
=============================================
[2019-04-05 14:35:55,512] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1322601e-06 9.9242121e-01 3.9936067e-06 7.2640949e-05 7.4857320e-03
 8.8496890e-08 1.5278778e-05], sum to 1.0000
[2019-04-05 14:35:55,513] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6806
[2019-04-05 14:35:55,525] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 19.5, 20.99728561388988, -0.583873285723672, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4431600.0000, 
sim time next is 4432200.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 19.0, 20.92971901385046, -0.5998375457797274, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.08333333333333333, 0.2441432511542049, 0.30005415140675756, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1687874], dtype=float32), 0.23652036]. 
=============================================
[2019-04-05 14:35:56,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:35:56,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:35:56,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run14
[2019-04-05 14:35:59,776] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1006864e-04 9.6629304e-01 2.3241655e-04 1.2507611e-02 1.9155752e-02
 6.0783594e-04 1.0934324e-03], sum to 1.0000
[2019-04-05 14:35:59,778] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0623
[2019-04-05 14:35:59,784] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.0, 19.97202558253175, -0.8134647212356788, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4665600.0000, 
sim time next is 4666200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 19.90198186589721, -0.8283471689419223, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.15849848882476744, 0.22388427701935923, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3850517], dtype=float32), 1.4202714]. 
=============================================
[2019-04-05 14:36:02,696] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.60000626e-06 8.48766685e-01 4.39495430e-04 2.93015707e-02
 1.21436805e-01 3.73046487e-06 4.61091549e-05], sum to 1.0000
[2019-04-05 14:36:02,701] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5437
[2019-04-05 14:36:02,707] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.8, 44.33333333333334, 266.0, 385.6666666666667, 19.0, 20.29915273857147, -0.7748458758792328, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4887600.0000, 
sim time next is 4888200.0000, 
raw observation next is [1.9, 44.16666666666667, 260.0, 383.3333333333333, 19.0, 20.32362785653815, -0.771909012786285, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.4416666666666667, 0.8666666666666667, 0.42357274401473294, 0.08333333333333333, 0.19363565471151242, 0.242696995737905, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5814998], dtype=float32), -0.00024789252]. 
=============================================
[2019-04-05 14:36:05,266] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3962921e-06 9.4528699e-01 1.0293689e-04 1.0205673e-03 5.2437596e-02
 1.7362718e-05 1.1331522e-03], sum to 1.0000
[2019-04-05 14:36:05,267] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2806
[2019-04-05 14:36:05,287] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.333333333333333, 40.0, 0.0, 0.0, 19.0, 19.99933192887714, -0.8418211697383738, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5013600.0000, 
sim time next is 5014200.0000, 
raw observation next is [1.166666666666667, 40.0, 0.0, 0.0, 19.0, 19.97119975348096, -0.8473503989440223, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.49492151431209613, 0.4, 0.0, 0.0, 0.08333333333333333, 0.16426664612341332, 0.21754986701865922, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3313235], dtype=float32), -3.2955925]. 
=============================================
[2019-04-05 14:36:06,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.0794346e-04 7.8508860e-01 9.7170603e-03 3.2661773e-02 1.0732132e-01
 3.4599299e-05 6.4368755e-02], sum to 1.0000
[2019-04-05 14:36:06,574] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4010
[2019-04-05 14:36:06,635] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 65.0, 78.0, 317.0, 21.5, 20.37032529959875, -0.7096736481141623, 1.0, 1.0, 30993.06840139788], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 5040000.0000, 
sim time next is 5040600.0000, 
raw observation next is [-1.5, 62.00000000000001, 84.33333333333333, 389.0, 20.5, 20.88678165626858, -0.6565224193821172, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.6200000000000001, 0.2811111111111111, 0.4298342541436464, 0.20833333333333334, 0.2405651380223818, 0.28115919353929425, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.19660704], dtype=float32), 0.80688655]. 
=============================================
[2019-04-05 14:36:10,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:36:10,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:36:10,476] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run14
[2019-04-05 14:36:11,662] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:36:11,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:36:11,683] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run14
[2019-04-05 14:36:12,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:36:12,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:36:12,078] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run14
[2019-04-05 14:36:12,601] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.00802368 0.826249   0.02404562 0.01490327 0.07521852 0.00148993
 0.05006991], sum to 1.0000
[2019-04-05 14:36:12,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0310
[2019-04-05 14:36:12,662] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.916666666666667, 32.83333333333334, 49.0, 0.0, 21.0, 20.82172647762947, -0.8554839888296663, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 465000.0000, 
sim time next is 465600.0000, 
raw observation next is [-5.633333333333334, 32.66666666666667, 55.50000000000001, 0.0, 20.0, 21.15662351457305, -0.8228502036607845, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30655586334256696, 0.3266666666666667, 0.18500000000000003, 0.0, 0.16666666666666666, 0.26305195954775407, 0.2257165987797385, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.57826436], dtype=float32), 0.96170056]. 
=============================================
[2019-04-05 14:36:15,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1178491e-06 9.8134905e-01 7.4930175e-04 1.1295429e-02 2.9968838e-03
 6.7973879e-06 3.6013059e-03], sum to 1.0000
[2019-04-05 14:36:15,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8361
[2019-04-05 14:36:15,819] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.2, 56.0, 300.0, 164.0, 19.0, 20.42579325581096, -0.8053954484067712, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4876200.0000, 
sim time next is 4876800.0000, 
raw observation next is [-0.9333333333333333, 54.66666666666667, 297.1666666666666, 188.0, 19.0, 20.39365068521275, -0.8063657687471788, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.4367497691597415, 0.5466666666666667, 0.9905555555555552, 0.20773480662983426, 0.08333333333333333, 0.19947089043439595, 0.23121141041760707, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1347152], dtype=float32), 1.0472168]. 
=============================================
[2019-04-05 14:36:19,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:36:19,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:36:19,050] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run14
[2019-04-05 14:36:24,217] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-05 14:36:24,223] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:36:24,224] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:36:24,224] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:36:24,224] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:36:24,228] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:36:24,229] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:36:24,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run90
[2019-04-05 14:36:24,252] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run90
[2019-04-05 14:36:24,265] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run90
[2019-04-05 14:36:54,879] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12095248]
[2019-04-05 14:36:54,879] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.6, 97.0, 0.0, 0.0, 19.5, 19.2848895438818, -0.9481593008855902, 0.0, 1.0, 0.0]
[2019-04-05 14:36:54,880] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:36:54,881] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.9234136e-07 8.6861360e-01 2.1248798e-04 4.9976413e-03 1.2537815e-01
 4.9682753e-06 7.9235865e-04], sampled 0.30723384820693567
[2019-04-05 14:37:37,821] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6367.4936 142318114.4818 -1827.9970
[2019-04-05 14:37:48,955] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5831.2068 173806045.2688 -2385.4525
[2019-04-05 14:37:55,808] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5964.3001 186475255.5500 -2298.0447
[2019-04-05 14:37:56,834] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1780000, evaluation results [1780000.0, 5831.20677717012, 173806045.2688242, -2385.4525092118183, 6367.493597264843, 142318114.4817668, -1827.9970312856613, 5964.300065065558, 186475255.5500411, -2298.0446799017495]
[2019-04-05 14:37:58,227] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:37:58,228] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:37:58,270] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run14
[2019-04-05 14:37:58,414] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:37:58,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:37:58,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run14
[2019-04-05 14:37:59,840] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:37:59,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:37:59,855] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run14
[2019-04-05 14:38:02,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8999105e-04 9.3943703e-01 7.1243825e-03 2.3137713e-03 4.9464241e-02
 2.0315402e-04 8.6732133e-04], sum to 1.0000
[2019-04-05 14:38:02,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2441
[2019-04-05 14:38:02,967] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.55, 76.0, 29.0, 0.0, 19.0, 20.54139595120736, -0.8041921778460037, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 894600.0000, 
sim time next is 895200.0000, 
raw observation next is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 19.0, 20.73755307363174, -0.7838345412265673, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4829178208679595, 0.7733333333333333, 0.10722222222222219, 0.0, 0.08333333333333333, 0.22812942280264506, 0.23872181959114425, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.21530959], dtype=float32), 0.12080526]. 
=============================================
[2019-04-05 14:38:03,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3254615e-05 1.9844554e-01 2.0692800e-03 1.4235966e-02 7.6075798e-01
 4.0002164e-05 2.4428023e-02], sum to 1.0000
[2019-04-05 14:38:03,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7428
[2019-04-05 14:38:03,530] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.383333333333333, 59.83333333333333, 148.3333333333333, 80.66666666666667, 19.5, 18.7186035772244, -1.168961954478081, 0.0, 1.0, 69149.38445599416], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 654600.0000, 
sim time next is 655200.0000, 
raw observation next is [-1.2, 60.0, 131.5, 74.5, 20.0, 18.73751398506771, -1.12740374890132, 0.0, 1.0, 197000.601185258], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.6, 0.43833333333333335, 0.08232044198895028, 0.16666666666666666, 0.06145949875564257, 0.12419875036622667, 0.0, 1.0, 0.938098100882181], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.4374689], dtype=float32), -0.11481286]. 
=============================================
[2019-04-05 14:38:11,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8770501e-05 2.7729526e-01 1.1996635e-03 6.0571148e-04 6.6021907e-01
 1.9074378e-06 6.0659654e-02], sum to 1.0000
[2019-04-05 14:38:11,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3853
[2019-04-05 14:38:11,782] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [5.0, 96.66666666666666, 0.0, 0.0, 21.0, 20.3026336398435, -0.7164105170120721, 0.0, 1.0, 197594.78385388], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 942600.0000, 
sim time next is 943200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 21.5, 20.32195246055546, -0.6853351634924576, 0.0, 1.0, 198955.6874469254], 
processed observation next is [1.0, 0.9565217391304348, 0.6011080332409973, 0.96, 0.0, 0.0, 0.2916666666666667, 0.1934960383796215, 0.27155494550251413, 0.0, 1.0, 0.9474080354615496], 
reward next is 0.6429, 
noisyNet noise sample is [array([0.41254073], dtype=float32), -0.9537197]. 
=============================================
[2019-04-05 14:38:12,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7257990e-03 2.5539306e-01 8.3375545e-03 1.7653069e-02 6.8779159e-01
 3.2032776e-04 2.5778549e-02], sum to 1.0000
[2019-04-05 14:38:12,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4066
[2019-04-05 14:38:12,084] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-15.78333333333333, 88.5, 29.66666666666666, 564.0, 21.5, 21.93698692333999, -0.6211804618595693, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [22.0], 
sim time this is 377400.0000, 
sim time next is 378000.0000, 
raw observation next is [-15.6, 90.0, 32.0, 607.5, 22.0, 22.01545749734547, -0.6045722933613767, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.030470914127423816, 0.9, 0.10666666666666667, 0.6712707182320442, 0.3333333333333333, 0.33462145811212246, 0.2984759022128744, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.84702003], dtype=float32), 0.5099183]. 
=============================================
[2019-04-05 14:38:12,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[41.19511 ]
 [39.064404]
 [38.608402]
 [37.99182 ]
 [38.00674 ]], R is [[41.67677689]
 [41.26000977]
 [40.84740829]
 [40.43893433]
 [40.0345459 ]].
[2019-04-05 14:38:12,683] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4073770e-04 8.4642720e-01 5.4194848e-03 4.9153710e-04 1.1665805e-01
 2.8996862e-04 3.0473134e-02], sum to 1.0000
[2019-04-05 14:38:12,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0256
[2019-04-05 14:38:12,723] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.06666666666667, 51.0, 57.5, 902.0, 19.0, 21.01792647896227, -0.8331936371926627, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 391200.0000, 
sim time next is 391800.0000, 
raw observation next is [-11.88333333333333, 51.0, 57.0, 899.0, 19.0, 20.6474808311735, -0.8310824282698364, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.13342566943674988, 0.51, 0.19, 0.9933701657458563, 0.08333333333333333, 0.22062340259779165, 0.22297252391005454, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5108291], dtype=float32), -1.1443706]. 
=============================================
[2019-04-05 14:38:16,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4400501e-06 9.8214585e-01 2.1540350e-04 1.5342764e-04 1.7388847e-02
 9.0078765e-06 8.4986685e-05], sum to 1.0000
[2019-04-05 14:38:16,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0520
[2019-04-05 14:38:16,434] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 19.0, 21.51265852423183, -0.3318998724309482, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1288800.0000, 
sim time next is 1289400.0000, 
raw observation next is [5.500000000000001, 100.0, 0.0, 0.0, 19.0, 21.47226147294987, -0.3414868783524611, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.2893551227458226, 0.3861710405491796, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72424215], dtype=float32), -0.02003658]. 
=============================================
[2019-04-05 14:38:17,418] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2542651e-05 4.5117781e-01 3.0254989e-03 1.1704671e-02 5.0953954e-01
 1.6213374e-04 2.4317836e-02], sum to 1.0000
[2019-04-05 14:38:17,423] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7091
[2019-04-05 14:38:17,430] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.4, 93.0, 42.00000000000001, 0.0, 19.5, 20.78646573059719, -0.7915466364827733, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 921000.0000, 
sim time next is 921600.0000, 
raw observation next is [4.4, 93.0, 36.0, 0.0, 20.0, 20.87705864659984, -0.783642550535227, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5844875346260389, 0.93, 0.12, 0.0, 0.16666666666666666, 0.2397548872166532, 0.23878581648825767, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.2737982], dtype=float32), -1.4845908]. 
=============================================
[2019-04-05 14:38:23,770] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4753371e-06 3.1318185e-01 6.7107560e-04 1.6713905e-04 6.8056303e-01
 2.5503787e-05 5.3878925e-03], sum to 1.0000
[2019-04-05 14:38:23,771] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6626
[2019-04-05 14:38:23,831] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.8666666666666667, 80.83333333333333, 116.3333333333333, 309.0000000000001, 25.5, 23.16180316943385, -0.1615309624752446, 0.0, 1.0, 98123.28710733155], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 562200.0000, 
sim time next is 562800.0000, 
raw observation next is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 26.0, 23.51120225431998, -0.1046863856038899, 0.0, 1.0, 84700.95398294691], 
processed observation next is [0.0, 0.5217391304347826, 0.4367497691597415, 0.8066666666666668, 0.4105555555555557, 0.38950276243093923, 0.6666666666666666, 0.45926685452666494, 0.46510453813203667, 0.0, 1.0, 0.403337876109271], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0827484], dtype=float32), -0.07629197]. 
=============================================
[2019-04-05 14:38:27,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3812916e-05 4.5843527e-01 5.9430236e-03 2.0521290e-03 5.2359796e-01
 6.1827227e-06 9.9516576e-03], sum to 1.0000
[2019-04-05 14:38:27,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0461
[2019-04-05 14:38:27,758] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.6, 91.33333333333333, 0.0, 0.0, 19.0, 21.01626996732358, -0.477863193910134, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 21.00066377999619, -0.4808197727534431, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.25005531499968264, 0.339726742415519, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4372781], dtype=float32), 1.5924875]. 
=============================================
[2019-04-05 14:38:30,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2753377e-05 8.9471042e-01 2.4603338e-03 6.0784049e-02 1.4534485e-02
 1.3050986e-04 2.7307432e-02], sum to 1.0000
[2019-04-05 14:38:30,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5495
[2019-04-05 14:38:30,498] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.3, 42.0, 0.0, 0.0, 19.0, 19.11503901536882, -1.137655457896973, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2417400.0000, 
sim time next is 2418000.0000, 
raw observation next is [-5.4, 42.33333333333333, 0.0, 0.0, 19.0, 19.0590914205436, -1.157900196587034, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.31301939058171746, 0.4233333333333333, 0.0, 0.0, 0.08333333333333333, 0.08825761837863322, 0.11403326780432203, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1040237], dtype=float32), 1.1353267]. 
=============================================
[2019-04-05 14:38:30,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[64.74155]
 [65.16354]
 [65.63366]
 [66.30472]
 [66.67706]], R is [[64.52308655]
 [64.87785339]
 [65.22907257]
 [65.57678223]
 [65.92101288]].
[2019-04-05 14:38:32,819] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6438823e-04 9.6973455e-01 1.8171739e-03 4.9558063e-03 1.2082925e-02
 9.6115706e-05 1.0849088e-02], sum to 1.0000
[2019-04-05 14:38:32,819] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4099
[2019-04-05 14:38:32,868] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.2, 59.0, 0.0, 0.0, 19.0, 19.55440597904557, -0.9838954218462116, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 764400.0000, 
sim time next is 765000.0000, 
raw observation next is [-5.3, 59.5, 0.0, 0.0, 19.0, 19.46814107736429, -1.007972797761037, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.31578947368421056, 0.595, 0.0, 0.0, 0.08333333333333333, 0.12234508978035752, 0.16400906741298762, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8023932], dtype=float32), 0.002031512]. 
=============================================
[2019-04-05 14:38:32,880] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[54.17873 ]
 [50.155506]
 [50.485516]
 [50.450428]
 [51.022724]], R is [[58.33737183]
 [58.7539978 ]
 [58.16645813]
 [57.58479309]
 [57.00894547]].
[2019-04-05 14:38:34,140] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.07449115e-05 9.86648381e-01 7.00653327e-05 5.11956681e-03
 7.66696222e-03 1.08228676e-04 3.46061192e-04], sum to 1.0000
[2019-04-05 14:38:34,149] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1952
[2019-04-05 14:38:34,160] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.2, 84.0, 0.0, 0.0, 19.0, 18.91951700037317, -1.086114206771157, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2068200.0000, 
sim time next is 2068800.0000, 
raw observation next is [-4.3, 84.66666666666666, 0.0, 0.0, 19.0, 18.90953911934582, -1.098756789193772, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.34349030470914127, 0.8466666666666666, 0.0, 0.0, 0.08333333333333333, 0.07579492661215159, 0.13374773693540934, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.834826], dtype=float32), -1.4041469]. 
=============================================
[2019-04-05 14:38:35,647] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-05 14:38:35,649] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:38:35,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:38:35,650] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:38:35,650] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:38:35,651] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:38:35,652] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:38:35,662] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run91
[2019-04-05 14:38:35,680] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run91
[2019-04-05 14:38:35,696] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run91
[2019-04-05 14:39:34,055] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.120231956]
[2019-04-05 14:39:34,055] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.550000000000001, 74.5, 0.0, 0.0, 19.0, 18.81230022874959, -1.080653350383435, 0.0, 1.0, 0.0]
[2019-04-05 14:39:34,056] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:39:34,057] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.0517489e-06 8.6372262e-01 6.2876748e-04 5.7550431e-03 1.2702131e-01
 2.1217927e-05 2.8469323e-03], sampled 0.18288602569484158
[2019-04-05 14:39:47,949] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6363.1655 141530574.1924 -1843.1533
[2019-04-05 14:40:01,936] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5982.2702 171749916.1082 -2324.5634
[2019-04-05 14:40:05,745] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5982.5915 185685303.3801 -2252.9252
[2019-04-05 14:40:06,769] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1800000, evaluation results [1800000.0, 5982.270233008409, 171749916.10817635, -2324.563355500403, 6363.1654826003605, 141530574.19243318, -1843.1532744269846, 5982.591514635394, 185685303.38013548, -2252.925162018301]
[2019-04-05 14:40:13,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1136300e-04 3.5777259e-01 9.1329584e-04 3.0816831e-02 3.5390401e-01
 6.8805984e-04 2.5579390e-01], sum to 1.0000
[2019-04-05 14:40:13,822] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8329
[2019-04-05 14:40:13,838] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 23.0, 18.96652931048087, -0.9943700258275011, 0.0, 1.0, 196681.8218047913], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1483200.0000, 
sim time next is 1483800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 25.0, 18.9839756774558, -0.9321961312061345, 0.0, 1.0, 198325.2431223491], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.5833333333333334, 0.08199797312131658, 0.1892679562646218, 0.0, 1.0, 0.9444059196302338], 
reward next is 0.1429, 
noisyNet noise sample is [array([0.3045155], dtype=float32), 1.3718989]. 
=============================================
[2019-04-05 14:40:15,577] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7256846e-06 9.4125348e-01 1.9730128e-04 5.2583427e-04 4.4175971e-02
 3.3757362e-05 1.3811984e-02], sum to 1.0000
[2019-04-05 14:40:15,578] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1467
[2019-04-05 14:40:15,591] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 21.0, 20.51029963250178, -0.6965664131715905, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 1404000.0000, 
sim time next is 1404600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 20.0, 20.57834078521965, -0.7027289267592688, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.44598337950138506, 1.0, 0.0, 0.0, 0.16666666666666666, 0.21486173210163736, 0.26575702441357707, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.3925191], dtype=float32), -0.8425727]. 
=============================================
[2019-04-05 14:40:16,759] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.9885662e-06 8.0702156e-01 4.8026445e-04 1.2294088e-04 1.6301467e-01
 2.3361128e-05 2.9328190e-02], sum to 1.0000
[2019-04-05 14:40:16,761] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5245
[2019-04-05 14:40:16,770] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [9.033333333333333, 63.0, 0.0, 0.0, 19.0, 23.09634982777402, -0.1028283464060028, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1537800.0000, 
sim time next is 1538400.0000, 
raw observation next is [8.666666666666668, 65.0, 0.0, 0.0, 19.0, 23.01614062394909, -0.1185764929528136, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7026777469990768, 0.65, 0.0, 0.0, 0.08333333333333333, 0.4180117186624243, 0.4604745023490621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1535209], dtype=float32), 0.5606233]. 
=============================================
[2019-04-05 14:40:20,011] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1816752e-04 9.0001929e-01 1.7662876e-03 4.2271591e-03 6.6977419e-02
 3.5170568e-04 2.5939990e-02], sum to 1.0000
[2019-04-05 14:40:20,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4411
[2019-04-05 14:40:20,047] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.233333333333333, 79.0, 0.0, 0.0, 19.0, 21.92058326120077, -0.5149848793069377, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1971600.0000, 
sim time next is 1972200.0000, 
raw observation next is [-5.416666666666667, 81.0, 0.0, 0.0, 19.0, 21.77806949277464, -0.5388374445086546, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3125577100646353, 0.81, 0.0, 0.0, 0.08333333333333333, 0.31483912439788675, 0.32038751849711516, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.93391573], dtype=float32), 0.85227776]. 
=============================================
[2019-04-05 14:40:23,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.89044589e-06 7.93363333e-01 6.01019565e-05 1.05995703e-02
 1.80450216e-01 9.71442387e-06 1.55091565e-02], sum to 1.0000
[2019-04-05 14:40:23,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2213
[2019-04-05 14:40:23,967] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [15.0, 100.0, 58.66666666666667, 0.0, 19.0, 20.37224112332937, -0.5894777848423686, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1242600.0000, 
sim time next is 1243200.0000, 
raw observation next is [15.0, 100.0, 66.33333333333334, 0.0, 19.0, 20.36821319647563, -0.5884482876914735, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.22111111111111115, 0.0, 0.08333333333333333, 0.19735109970630246, 0.30385057076950883, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21265659], dtype=float32), 0.096495375]. 
=============================================
[2019-04-05 14:40:32,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.01580175 0.6597556  0.03872377 0.00979693 0.16569084 0.00673509
 0.10349606], sum to 1.0000
[2019-04-05 14:40:32,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4588
[2019-04-05 14:40:32,763] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7536620e-05 9.5683610e-01 3.2538970e-04 1.5986500e-03 3.8659874e-02
 3.7402701e-06 2.5487549e-03], sum to 1.0000
[2019-04-05 14:40:32,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0992
[2019-04-05 14:40:32,770] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 75.0, 61.33333333333333, 325.0, 19.0, 21.15661628843636, -0.7159122953931784, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2191800.0000, 
sim time next is 2192400.0000, 
raw observation next is [-5.6, 75.0, 71.5, 356.5, 19.0, 21.23493735818392, -0.7066654650878443, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30747922437673136, 0.75, 0.23833333333333334, 0.3939226519337017, 0.08333333333333333, 0.2695781131819934, 0.2644448449707186, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9913993], dtype=float32), 1.0260822]. 
=============================================
[2019-04-05 14:40:32,778] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.41666666666667, 92.83333333333333, 72.0, 0.0, 19.0, 21.26582297227136, -0.6216985820348537, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 985800.0000, 
sim time next is 986400.0000, 
raw observation next is [10.5, 93.0, 78.0, 0.0, 19.0, 21.35705577011632, -0.6022272275799697, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7534626038781165, 0.93, 0.26, 0.0, 0.08333333333333333, 0.2797546475096935, 0.29925759080667674, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.50279576], dtype=float32), -1.217679]. 
=============================================
[2019-04-05 14:40:37,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.86414877e-06 8.96349013e-01 4.86136050e-05 8.90765514e-05
 1.02837265e-01 1.67166654e-05 6.57378056e-04], sum to 1.0000
[2019-04-05 14:40:37,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7151
[2019-04-05 14:40:37,103] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.0, 59.5, 0.0, 0.0, 19.0, 23.0726048828592, -0.01087680059741194, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1108200.0000, 
sim time next is 1108800.0000, 
raw observation next is [13.8, 60.0, 0.0, 0.0, 19.0, 23.01192337432983, -0.02442590185456132, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.844875346260388, 0.6, 0.0, 0.0, 0.08333333333333333, 0.4176602811941524, 0.4918580327151462, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1522707], dtype=float32), -1.6411028]. 
=============================================
[2019-04-05 14:40:41,390] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.00316408 0.88813066 0.00266833 0.00862391 0.01667579 0.00464451
 0.07609269], sum to 1.0000
[2019-04-05 14:40:41,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9748
[2019-04-05 14:40:41,418] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 64.5, 167.0, 1.333333333333333, 19.0, 21.31665855235168, -0.7247506906700715, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 1948200.0000, 
sim time next is 1948800.0000, 
raw observation next is [-3.733333333333333, 64.0, 152.0, 0.6666666666666665, 21.0, 21.2269768539352, -0.7172711284402759, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.64, 0.5066666666666667, 0.000736648250460405, 0.25, 0.26891473782793324, 0.2609096238532414, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.43690205], dtype=float32), -0.47755674]. 
=============================================
[2019-04-05 14:40:42,971] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-05 14:40:42,973] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:40:42,973] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:40:42,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:40:42,975] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:40:42,976] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:40:42,978] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:40:42,988] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run92
[2019-04-05 14:40:43,006] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run92
[2019-04-05 14:40:43,026] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run92
[2019-04-05 14:41:12,533] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.120802365]
[2019-04-05 14:41:12,533] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [6.9, 81.0, 0.0, 0.0, 19.0, 19.87779072052211, -0.7755831723975527, 0.0, 1.0, 0.0]
[2019-04-05 14:41:12,533] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:41:12,534] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.8939074e-06 9.5066154e-01 1.6680332e-04 1.5408723e-03 4.5729987e-02
 7.6439928e-06 1.8911886e-03], sampled 0.42919182402413647
[2019-04-05 14:41:51,260] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.120802365]
[2019-04-05 14:41:51,261] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 51.0, 0.0, 0.0, 19.5, 20.17931937303629, -0.8016327498072996, 0.0, 1.0, 0.0]
[2019-04-05 14:41:51,261] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:41:51,262] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.1141437e-05 9.2726934e-01 7.6925394e-04 2.5502339e-03 6.1190262e-02
 6.8706817e-05 8.1111137e-03], sampled 0.724625335231104
[2019-04-05 14:41:54,750] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6372.7463 139445156.9442 -1874.3969
[2019-04-05 14:42:06,753] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5855.8761 170894090.2592 -2450.4256
[2019-04-05 14:42:13,487] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5928.0236 185991949.8447 -2399.0389
[2019-04-05 14:42:14,515] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1820000, evaluation results [1820000.0, 5855.876050507086, 170894090.25915852, -2450.4255972767814, 6372.746306352714, 139445156.9442049, -1874.3968965971624, 5928.023563724758, 185991949.8446618, -2399.0389319317605]
[2019-04-05 14:42:23,648] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3006504e-05 1.5170860e-01 1.6075554e-03 2.3641244e-04 8.4459776e-01
 1.3010461e-05 1.8236951e-03], sum to 1.0000
[2019-04-05 14:42:23,649] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0065
[2019-04-05 14:42:23,670] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 20.27977091029719, -0.7971203369980581, 0.0, 1.0, 52217.78278591159], 
current ob forecast is [], 
actual action is [25.0], 
sim time this is 1806000.0000, 
sim time next is 1806600.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 25.0, 20.38062571774467, -0.7904860219968644, 0.0, 1.0, 51462.46230747621], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.5833333333333334, 0.1983854764787226, 0.23650465933437856, 0.0, 1.0, 0.2450593443213153], 
reward next is 0.1429, 
noisyNet noise sample is [array([-1.6636167], dtype=float32), -0.50684017]. 
=============================================
[2019-04-05 14:42:24,726] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0924415e-05 8.8772237e-01 2.2633918e-04 2.5269412e-03 1.0850861e-01
 4.9271334e-06 9.9981402e-04], sum to 1.0000
[2019-04-05 14:42:24,727] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1154
[2019-04-05 14:42:24,739] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.2, 81.66666666666667, 0.0, 0.0, 19.0, 18.89230362445405, -1.124160566274554, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1830000.0000, 
sim time next is 1830600.0000, 
raw observation next is [-6.2, 81.0, 0.0, 0.0, 19.0, 18.81262007082266, -1.144182420896088, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.81, 0.0, 0.0, 0.08333333333333333, 0.0677183392352217, 0.118605859701304, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1540786], dtype=float32), 0.28160357]. 
=============================================
[2019-04-05 14:42:24,850] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0173494e-07 9.9976343e-01 7.1277267e-05 1.7999291e-06 6.2543484e-05
 4.8312877e-06 9.6000069e-05], sum to 1.0000
[2019-04-05 14:42:24,853] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8943
[2019-04-05 14:42:24,867] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.45, 73.5, 0.0, 0.0, 19.0, 21.54247270898807, -0.4122392970626998, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [7.533333333333333, 73.66666666666666, 0.0, 0.0, 19.0, 21.47630807579992, -0.42249378193649, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6712834718374886, 0.7366666666666666, 0.0, 0.0, 0.08333333333333333, 0.28969233964999336, 0.35916873935450333, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4870812], dtype=float32), -0.18454547]. 
=============================================
[2019-04-05 14:42:47,011] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5715244e-03 8.6876309e-01 1.0833961e-03 3.9165276e-03 1.1753585e-01
 5.8922777e-04 6.5404293e-03], sum to 1.0000
[2019-04-05 14:42:47,014] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8743
[2019-04-05 14:42:47,026] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.0, 0.0, 0.0, 19.0, 20.43800537320984, -0.8334293466873613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2656800.0000, 
sim time next is 2657400.0000, 
raw observation next is [-0.7, 55.0, 0.0, 0.0, 19.0, 19.98092721104769, -0.8872114613621024, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.443213296398892, 0.55, 0.0, 0.0, 0.08333333333333333, 0.16507726758730742, 0.20426284621263255, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.32231], dtype=float32), -0.6709445]. 
=============================================
[2019-04-05 14:42:47,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3815524e-06 9.8518306e-01 1.5943397e-05 2.7498777e-04 1.4131246e-02
 1.8026178e-06 3.9161401e-04], sum to 1.0000
[2019-04-05 14:42:47,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3205
[2019-04-05 14:42:47,233] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.933333333333333, 100.0, 0.0, 0.0, 21.5, 21.39642141094969, -0.6360525801043019, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 3127800.0000, 
sim time next is 3128400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 20.5, 21.44748156115148, -0.6177219819570408, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5457063711911359, 1.0, 0.0, 0.0, 0.20833333333333334, 0.2872901300959567, 0.29409267268098643, 0.0, 1.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.1456702], dtype=float32), 1.5676057]. 
=============================================
[2019-04-05 14:42:52,107] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-05 14:42:52,108] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:42:52,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:42:52,108] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:42:52,108] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:42:52,108] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:42:52,109] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:42:52,113] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run93
[2019-04-05 14:42:52,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run93
[2019-04-05 14:42:52,150] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run93
[2019-04-05 14:43:27,250] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12106547]
[2019-04-05 14:43:27,250] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.6, 88.0, 0.0, 0.0, 19.0, 19.02525447770652, -0.9427509340053106, 0.0, 1.0, 0.0]
[2019-04-05 14:43:27,250] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:43:27,251] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.1216763e-06 8.9445353e-01 3.1991542e-04 1.8267297e-03 9.9925913e-02
 1.4090654e-05 3.4556910e-03], sampled 0.7069919153412174
[2019-04-05 14:43:58,932] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12106547]
[2019-04-05 14:43:58,933] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.2, 79.83333333333334, 0.0, 0.0, 19.5, 19.84850561330148, -0.8509751067604568, 0.0, 1.0, 0.0]
[2019-04-05 14:43:58,933] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:43:58,935] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.10606593e-07 9.00478780e-01 1.20054145e-04 3.08963866e-03
 9.47406590e-02 6.58504950e-06 1.56353239e-03], sampled 0.5132714413191053
[2019-04-05 14:44:01,134] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12106547]
[2019-04-05 14:44:01,134] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.5, 84.5, 101.0, 242.0, 19.0, 22.36575361506097, -0.1200912535383569, 0.0, 1.0, 0.0]
[2019-04-05 14:44:01,134] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:44:01,135] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.1767723e-07 9.4041497e-01 6.1879051e-05 9.9093199e-04 5.7524554e-02
 1.6648304e-06 1.0056704e-03], sampled 0.5279644139040184
[2019-04-05 14:44:05,144] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6366.4802 141528918.6395 -1785.3848
[2019-04-05 14:44:06,917] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12106547]
[2019-04-05 14:44:06,917] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.666666666666667, 34.0, 112.3333333333333, 754.0, 19.0, 20.88090388916811, -0.7778775970706059, 1.0, 1.0, 0.0]
[2019-04-05 14:44:06,917] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:44:06,918] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.0448850e-04 8.5476142e-01 2.5792979e-03 7.5999834e-03 1.0784087e-01
 3.8857697e-04 2.6425377e-02], sampled 0.9226990165564527
[2019-04-05 14:44:18,754] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6049.4944 171952875.0660 -2257.6162
[2019-04-05 14:44:25,909] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6182.6753 185299049.3502 -2125.2428
[2019-04-05 14:44:26,935] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1840000, evaluation results [1840000.0, 6049.494365970473, 171952875.06598413, -2257.616192434773, 6366.480200919821, 141528918.63950056, -1785.3847966276064, 6182.675270979827, 185299049.35022724, -2125.242844596428]
[2019-04-05 14:44:27,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00286057 0.80940104 0.00898029 0.00521671 0.10721445 0.00085978
 0.06546717], sum to 1.0000
[2019-04-05 14:44:27,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6273
[2019-04-05 14:44:27,908] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.8, 82.00000000000001, 140.0, 91.0, 19.0, 19.56100669978912, -1.030987474009303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2106600.0000, 
sim time next is 2107200.0000, 
raw observation next is [-7.8, 82.0, 157.0, 104.5, 19.0, 19.63417741672566, -1.01290105974915, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.5233333333333333, 0.11546961325966851, 0.08333333333333333, 0.13618145139380497, 0.16236631341695004, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6882468], dtype=float32), -0.7543584]. 
=============================================
[2019-04-05 14:44:29,111] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.4485096e-04 9.1990113e-01 8.9337891e-03 1.0184474e-02 9.3664154e-03
 2.8362360e-03 4.8033051e-02], sum to 1.0000
[2019-04-05 14:44:29,112] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9280
[2019-04-05 14:44:29,141] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.2, 91.0, 0.0, 0.0, 24.0, 19.94957294705929, -0.9315073231951655, 0.0, 1.0, 48248.35262077489], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 2269800.0000, 
sim time next is 2270400.0000, 
raw observation next is [-9.3, 91.0, 0.0, 0.0, 23.5, 19.99697423988392, -0.9329560149226342, 0.0, 1.0, 48013.62073292043], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.91, 0.0, 0.0, 0.4583333333333333, 0.16641451999032655, 0.18901466169245526, 0.0, 1.0, 0.228636289204383], 
reward next is 0.3571, 
noisyNet noise sample is [array([-1.2704458], dtype=float32), -1.3635633]. 
=============================================
[2019-04-05 14:44:35,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3406965e-03 5.4003733e-01 9.6136041e-04 3.3490099e-03 3.8979235e-01
 9.8473341e-05 6.3420780e-02], sum to 1.0000
[2019-04-05 14:44:35,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5123
[2019-04-05 14:44:35,999] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.533333333333333, 77.0, 0.0, 0.0, 22.0, 19.04102192074791, -0.9756528699211063, 0.0, 1.0, 85514.60941963835], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2245200.0000, 
sim time next is 2245800.0000, 
raw observation next is [-6.616666666666667, 77.5, 0.0, 0.0, 21.0, 19.33108630535749, -0.9466857073268272, 0.0, 1.0, 62264.55603563341], 
processed observation next is [1.0, 1.0, 0.2793167128347184, 0.775, 0.0, 0.0, 0.25, 0.11092385877979094, 0.18443809755772425, 0.0, 1.0, 0.29649788588396864], 
reward next is 0.7143, 
noisyNet noise sample is [array([-1.1452487], dtype=float32), 0.51014405]. 
=============================================
[2019-04-05 14:44:39,342] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.2608638e-04 8.0904967e-01 1.5302055e-03 1.3076029e-02 1.1034799e-01
 5.9968902e-04 6.4570390e-02], sum to 1.0000
[2019-04-05 14:44:39,345] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9154
[2019-04-05 14:44:39,392] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 19.5, 21.87618791395287, -0.614272389597638, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 2283000.0000, 
sim time next is 2283600.0000, 
raw observation next is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 20.0, 21.89199871356844, -0.6083122490133064, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.29270544783010155, 0.7466666666666667, 0.5505555555555557, 0.05322283609576428, 0.16666666666666666, 0.3243332261307034, 0.29722925032889785, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5078229], dtype=float32), 1.0648404]. 
=============================================
[2019-04-05 14:44:40,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.35351238e-06 8.85511398e-01 2.14694563e-04 3.79320025e-03
 1.06574714e-01 4.91934770e-05 3.84836621e-03], sum to 1.0000
[2019-04-05 14:44:40,007] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4034
[2019-04-05 14:44:40,017] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 19.01892645668138, -1.05341746017212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3893400.0000, 
sim time next is 3894000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 18.97089166686316, -1.068667496046897, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.08090763890526335, 0.14377750131770103, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2312087], dtype=float32), 0.58113724]. 
=============================================
[2019-04-05 14:44:40,021] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.56413 ]
 [75.65481 ]
 [75.182625]
 [73.31094 ]
 [72.42308 ]], R is [[77.14730072]
 [77.3758316 ]
 [77.60207367]
 [77.75463104]
 [77.97708893]].
[2019-04-05 14:44:41,252] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.4798263e-05 9.3499792e-01 2.7747701e-03 9.1653963e-04 2.5722953e-02
 5.5891680e-05 3.5437159e-02], sum to 1.0000
[2019-04-05 14:44:41,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2737
[2019-04-05 14:44:41,276] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.583333333333333, 63.0, 0.0, 0.0, 19.0, 19.75947898717664, -0.9555902073173339, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2592600.0000, 
sim time next is 2593200.0000, 
raw observation next is [-4.666666666666667, 64.0, 0.0, 0.0, 19.0, 19.7532270046901, -0.9672144671377861, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3333333333333333, 0.64, 0.0, 0.0, 0.08333333333333333, 0.14610225039084165, 0.17759517762073795, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3341327], dtype=float32), -0.6893809]. 
=============================================
[2019-04-05 14:44:52,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9383218e-05 9.1645426e-01 1.6244375e-03 3.0703987e-03 5.6745455e-02
 1.7439741e-04 2.1911846e-02], sum to 1.0000
[2019-04-05 14:44:52,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5795
[2019-04-05 14:44:52,395] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-0.5, 78.0, 0.0, 0.0, 19.0, 19.99931002121703, -0.8552010506952992, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4732200.0000, 
sim time next is 4732800.0000, 
raw observation next is [-0.6666666666666666, 78.0, 0.0, 0.0, 19.0, 19.88119314928497, -0.8810082738591009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.78, 0.0, 0.0, 0.08333333333333333, 0.15676609577374764, 0.20633057538029972, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34400743], dtype=float32), 1.3960754]. 
=============================================
[2019-04-05 14:44:58,119] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0449214e-05 6.7722833e-01 6.7577571e-02 5.0533105e-02 1.8415567e-01
 3.5088134e-04 2.0144012e-02], sum to 1.0000
[2019-04-05 14:44:58,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1300
[2019-04-05 14:44:58,136] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 19.03284626675674, -1.104175759603803, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2961600.0000, 
sim time next is 2962200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 18.91793730273263, -1.129906803916439, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.07649477522771904, 0.12336439869452032, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30677283], dtype=float32), -1.8293388]. 
=============================================
[2019-04-05 14:45:03,585] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-05 14:45:03,586] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:45:03,587] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:45:03,588] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:45:03,589] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:45:03,589] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:45:03,590] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:45:03,604] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run94
[2019-04-05 14:45:03,622] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run94
[2019-04-05 14:45:03,641] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run94
[2019-04-05 14:45:08,184] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12230207]
[2019-04-05 14:45:08,184] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.4047648358333331, 90.55660658833334, 0.0, 0.0, 19.0, 18.55351071948301, -1.035665769035958, 0.0, 1.0, 81708.18377735687]
[2019-04-05 14:45:08,185] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:45:08,186] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.6611846e-07 8.6922246e-01 1.6724845e-04 3.3219787e-03 1.2602907e-01
 6.4162855e-06 1.2520629e-03], sampled 0.4401825191490224
[2019-04-05 14:45:32,804] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12230207]
[2019-04-05 14:45:32,804] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.3964903253333334, 91.98057612, 30.87730868333334, 0.0, 19.5, 19.31661844174076, -0.9400563298583693, 0.0, 1.0, 0.0]
[2019-04-05 14:45:32,804] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:45:32,805] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.2577149e-06 8.2796037e-01 3.6123404e-04 4.9598869e-03 1.6347240e-01
 2.5934532e-05 3.2159078e-03], sampled 0.993166443178293
[2019-04-05 14:45:34,796] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12230207]
[2019-04-05 14:45:34,797] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 19.0, 22.41723866998733, -0.4726341377346893, 1.0, 1.0, 0.0]
[2019-04-05 14:45:34,797] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:45:34,798] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3669729e-05 9.1813326e-01 3.5333057e-04 2.2619530e-03 7.2706722e-02
 4.0056479e-05 6.4809839e-03], sampled 0.30412655643432274
[2019-04-05 14:45:56,909] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12230207]
[2019-04-05 14:45:56,910] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-13.33333333333333, 86.0, 94.16666666666667, 565.0, 20.0, 20.62505104052338, -0.7810301705857943, 1.0, 1.0, 0.0]
[2019-04-05 14:45:56,910] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:45:56,911] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.00139328 0.6983032  0.00754838 0.01728111 0.22060895 0.00121072
 0.05365426], sampled 0.2814458519459443
[2019-04-05 14:46:04,667] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12230207]
[2019-04-05 14:46:04,667] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.3333333333333334, 56.33333333333333, 195.3333333333333, 523.5, 19.0, 20.83913645594375, -0.6539423816125428, 1.0, 1.0, 0.0]
[2019-04-05 14:46:04,668] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:46:04,669] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4322023e-04 8.8886178e-01 9.4105082e-04 3.9959219e-03 9.0592675e-02
 1.3358086e-04 1.5331834e-02], sampled 0.1922528642366177
[2019-04-05 14:46:08,476] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12230207]
[2019-04-05 14:46:08,477] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.45, 81.0, 93.0, 232.0, 20.5, 19.27071506741896, -0.9475340153757301, 0.0, 1.0, 196217.9094192961]
[2019-04-05 14:46:08,477] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:46:08,479] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.39823117e-07 8.80732656e-01 7.58549213e-05 3.96578480e-03
 1.14570126e-01 4.48956234e-06 6.50707458e-04], sampled 0.3310284962461756
[2019-04-05 14:46:15,305] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6265.1594 142119056.3641 -1898.5823
[2019-04-05 14:46:29,907] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5922.5994 172577675.7976 -2277.7414
[2019-04-05 14:46:34,223] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5948.5374 186692405.7795 -2304.2145
[2019-04-05 14:46:35,248] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1860000, evaluation results [1860000.0, 5922.599396161409, 172577675.79764706, -2277.7413696733474, 6265.159443716486, 142119056.36411867, -1898.582324877612, 5948.5373955888645, 186692405.77945858, -2304.214536675839]
[2019-04-05 14:46:37,750] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5251986e-04 5.0831258e-01 6.9173262e-04 2.3588814e-02 4.5559108e-01
 3.0297448e-04 1.1360210e-02], sum to 1.0000
[2019-04-05 14:46:37,751] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5013
[2019-04-05 14:46:37,776] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 22.0, 20.31567230381426, -0.6933047084286282, 0.0, 1.0, 69897.09950049587], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 3801600.0000, 
sim time next is 3802200.0000, 
raw observation next is [-3.166666666666667, 72.0, 0.0, 0.0, 21.0, 20.52150895075038, -0.6759038137721819, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3748845798707295, 0.72, 0.0, 0.0, 0.25, 0.21012574589586505, 0.27469872874260604, 0.0, 1.0, 0.0], 
reward next is 0.7143, 
noisyNet noise sample is [array([-0.88901246], dtype=float32), -0.10528674]. 
=============================================
[2019-04-05 14:46:40,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:46:40,558] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:46:40,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run15
[2019-04-05 14:46:43,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6950361e-05 1.6714284e-01 1.5126732e-03 1.1574014e-02 7.8713125e-01
 1.0108964e-03 3.1581391e-02], sum to 1.0000
[2019-04-05 14:46:43,986] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8290
[2019-04-05 14:46:43,997] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.0, 41.0, 0.0, 0.0, 20.0, 20.24433990477499, -0.7727660679687753, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 4148400.0000, 
sim time next is 4149000.0000, 
raw observation next is [-1.0, 40.5, 0.0, 0.0, 20.5, 20.19480190034299, -0.7681075849591733, 0.0, 1.0, 196217.9094192961], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.405, 0.0, 0.0, 0.20833333333333334, 0.18290015836191595, 0.24396413834694222, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.7857, 
noisyNet noise sample is [array([0.21316631], dtype=float32), -0.40315768]. 
=============================================
[2019-04-05 14:46:44,018] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[74.83047]
 [74.71797]
 [73.94456]
 [75.51035]
 [75.94216]], R is [[75.61526489]
 [75.71625519]
 [75.67337799]
 [75.70235443]
 [75.80247498]].
[2019-04-05 14:46:45,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9084878e-05 8.8806927e-01 1.8096606e-03 2.6349898e-03 9.4993711e-02
 5.9794798e-04 1.1795288e-02], sum to 1.0000
[2019-04-05 14:46:45,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6747
[2019-04-05 14:46:45,775] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 18.68602828689447, -1.191268527505973, 0.0, 1.0, 98915.49275228687], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 19.0, 18.59864111574097, -1.186636378361946, 0.0, 1.0, 115661.5092024969], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.08333333333333333, 0.049886759645080936, 0.104454540546018, 0.0, 1.0, 0.5507690914404615], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9335159], dtype=float32), 0.15877807]. 
=============================================
[2019-04-05 14:46:45,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2797377e-05 8.6140990e-01 5.9338529e-03 2.2135749e-03 9.3505204e-02
 1.0935686e-04 3.6815278e-02], sum to 1.0000
[2019-04-05 14:46:45,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5871
[2019-04-05 14:46:45,903] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 20.27658006062421, -0.8628201524799922, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3103200.0000, 
sim time next is 3103800.0000, 
raw observation next is [-0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 20.20397776817552, -0.8792880956983414, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.43951985226223456, 1.0, 0.0, 0.0, 0.08333333333333333, 0.18366481401462664, 0.20690396810055287, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14491178], dtype=float32), 1.1121917]. 
=============================================
[2019-04-05 14:46:47,750] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.5107504e-07 9.8919898e-01 2.7342365e-04 2.9696746e-05 1.0393166e-02
 7.8263149e-08 1.0422724e-04], sum to 1.0000
[2019-04-05 14:46:47,752] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3408
[2019-04-05 14:46:47,765] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [8.0, 93.0, 113.5, 814.0, 20.5, 22.76238541885604, -0.2720153442174664, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3153600.0000, 
sim time next is 3154200.0000, 
raw observation next is [7.833333333333334, 94.16666666666666, 113.3333333333333, 817.0, 19.5, 22.79740444872409, -0.2570058554809602, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6795937211449677, 0.9416666666666665, 0.37777777777777766, 0.9027624309392265, 0.125, 0.3997837040603409, 0.4143313815063466, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.8655243], dtype=float32), -0.25390345]. 
=============================================
[2019-04-05 14:46:53,213] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8237727e-04 7.6945877e-01 3.3811352e-03 1.7640302e-02 1.3702336e-01
 1.7451561e-03 7.0468903e-02], sum to 1.0000
[2019-04-05 14:46:53,214] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0027
[2019-04-05 14:46:53,244] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 19.0, 20.02301471020281, -0.847702227049164, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3397800.0000, 
sim time next is 3398400.0000, 
raw observation next is [-2.0, 60.0, 44.5, 264.5, 19.0, 20.25299909614529, -0.8421071235294942, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.6, 0.14833333333333334, 0.29226519337016577, 0.08333333333333333, 0.1877499246787743, 0.2192976254901686, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.86090153], dtype=float32), 0.20836012]. 
=============================================
[2019-04-05 14:46:55,759] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.11741266e-04 8.16582382e-01 2.48812209e-03 1.62903406e-02
 1.53672904e-01 1.01236925e-04 1.03532095e-02], sum to 1.0000
[2019-04-05 14:46:55,771] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3428
[2019-04-05 14:46:55,783] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-13.0, 64.0, 0.0, 0.0, 19.0, 19.22827200299154, -1.058113465913474, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3995400.0000, 
sim time next is 3996000.0000, 
raw observation next is [-13.0, 63.0, 0.0, 0.0, 19.0, 19.23934049071682, -1.066453252627017, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.10249307479224376, 0.63, 0.0, 0.0, 0.08333333333333333, 0.1032783742264017, 0.14451558245766102, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6717398], dtype=float32), 0.1522622]. 
=============================================
[2019-04-05 14:46:55,792] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[61.836864]
 [61.97672 ]
 [62.152164]
 [62.21992 ]
 [62.235638]], R is [[61.8991394 ]
 [62.28014755]
 [62.65734482]
 [63.03077316]
 [63.25761032]].
[2019-04-05 14:47:02,175] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.5956907e-07 6.8413228e-01 4.3784206e-05 2.1713756e-03 3.0297360e-01
 7.3714866e-05 1.0604489e-02], sum to 1.0000
[2019-04-05 14:47:02,180] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1792
[2019-04-05 14:47:02,187] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 42.0, 102.0, 788.0, 19.0, 20.93513622818403, -0.5853987045662984, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3594600.0000, 
sim time next is 3595200.0000, 
raw observation next is [-1.0, 42.0, 99.33333333333333, 773.1666666666667, 19.0, 20.95218028499423, -0.5906948629211557, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.3311111111111111, 0.854327808471455, 0.08333333333333333, 0.24601502374951925, 0.30310171235961475, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3995749], dtype=float32), -1.3694098]. 
=============================================
[2019-04-05 14:47:06,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:47:06,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:47:06,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run15
[2019-04-05 14:47:08,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7132043e-05 9.6858335e-01 1.9623530e-03 1.3326603e-03 2.3966128e-02
 5.5390366e-05 4.0829368e-03], sum to 1.0000
[2019-04-05 14:47:08,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8261
[2019-04-05 14:47:08,099] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 19.0, 22.74898560816793, -0.2445846075648633, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3782400.0000, 
sim time next is 3783000.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 19.0, 22.64800022296042, -0.2594307314354478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.3873333519133683, 0.4135230895215174, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.507091], dtype=float32), -0.36747235]. 
=============================================
[2019-04-05 14:47:08,110] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.2708  ]
 [73.787025]
 [74.458015]
 [75.17881 ]
 [76.1907  ]], R is [[73.18327332]
 [73.4514389 ]
 [73.71692657]
 [73.97975922]
 [74.23995972]].
[2019-04-05 14:47:08,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7156752e-06 9.1073549e-01 7.8024532e-05 9.3194313e-04 8.7839499e-02
 1.9865472e-05 3.9239074e-04], sum to 1.0000
[2019-04-05 14:47:08,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9747
[2019-04-05 14:47:08,925] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 20.21718232024052, -0.7446935465671588, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.5, 20.41830422019837, -0.7319144321256318, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.125, 0.2015253516831974, 0.2560285226247894, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([2.2619793], dtype=float32), 0.31091204]. 
=============================================
[2019-04-05 14:47:09,478] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-05 14:47:09,482] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:47:09,483] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:47:09,484] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:47:09,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:47:09,484] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:47:09,484] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:47:09,492] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run95
[2019-04-05 14:47:09,509] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run95
[2019-04-05 14:47:09,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run95
[2019-04-05 14:48:00,522] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123491295]
[2019-04-05 14:48:00,523] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.8, 82.5, 0.0, 0.0, 19.0, 18.79839793659135, -1.08909750797953, 0.0, 1.0, 0.0]
[2019-04-05 14:48:00,523] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:48:00,524] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.1653567e-07 9.3326950e-01 6.6231551e-05 3.3424709e-03 6.2731810e-02
 3.5034350e-06 5.8616808e-04], sampled 0.4318842235400471
[2019-04-05 14:48:09,543] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123491295]
[2019-04-05 14:48:09,543] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.108089358, 68.929650055, 0.0, 0.0, 19.0, 18.82871318903272, -1.131962353825558, 0.0, 1.0, 0.0]
[2019-04-05 14:48:09,543] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:48:09,544] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0821707e-05 8.7958938e-01 7.3875941e-04 7.1907435e-03 1.0785411e-01
 4.2898839e-05 4.5731864e-03], sampled 0.05879109297255325
[2019-04-05 14:48:20,003] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6238.9198 142643636.2108 -1993.8310
[2019-04-05 14:48:28,913] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123491295]
[2019-04-05 14:48:28,914] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 79.33333333333333, 0.0, 0.0, 19.0, 18.83263498764281, -1.104077081018402, 0.0, 1.0, 0.0]
[2019-04-05 14:48:28,914] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:48:28,914] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.5312134e-06 9.1455722e-01 3.8808340e-04 3.2689094e-03 7.8468792e-02
 1.7661421e-05 3.2939164e-03], sampled 0.9173963510227758
[2019-04-05 14:48:33,245] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5848.8088 170951157.1341 -2454.3237
[2019-04-05 14:48:38,680] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5927.0117 183614645.5275 -2504.0259
[2019-04-05 14:48:39,707] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1880000, evaluation results [1880000.0, 5848.808790232391, 170951157.13414338, -2454.323659417027, 6238.919826397122, 142643636.2107564, -1993.8310306165572, 5927.01170634501, 183614645.52748236, -2504.025882873664]
[2019-04-05 14:48:40,242] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2471190e-06 8.7291425e-01 1.4459986e-05 4.7240690e-05 1.2533343e-01
 4.8607882e-05 1.6397020e-03], sum to 1.0000
[2019-04-05 14:48:40,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3426
[2019-04-05 14:48:40,260] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 67.0, 20.16666666666666, 186.3333333333333, 19.0, 23.30416533528578, -0.1901985345196746, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3432000.0000, 
sim time next is 3432600.0000, 
raw observation next is [2.0, 67.0, 12.0, 121.0, 19.0, 23.26046627847382, -0.2273021294858131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.04, 0.13370165745856355, 0.08333333333333333, 0.4383721898728184, 0.42423262350472896, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48215118], dtype=float32), 0.4733985]. 
=============================================
[2019-04-05 14:48:40,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.9914954e-07 3.4797999e-01 1.3939211e-03 7.0729338e-02 5.7772297e-01
 3.9080474e-05 2.1338335e-03], sum to 1.0000
[2019-04-05 14:48:40,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4350
[2019-04-05 14:48:40,462] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.5, 45.5, 132.3333333333333, 802.6666666666666, 19.5, 19.19626908609424, -0.9432155240335506, 0.0, 1.0, 93956.6843673172], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4792200.0000, 
sim time next is 4792800.0000, 
raw observation next is [-1.0, 45.0, 127.1666666666667, 820.8333333333334, 20.0, 19.22940666939919, -0.9202606421098224, 0.0, 1.0, 47000.74677683703], 
processed observation next is [0.0, 0.4782608695652174, 0.4349030470914128, 0.45, 0.423888888888889, 0.9069981583793739, 0.16666666666666666, 0.10245055578326578, 0.1932464526300592, 0.0, 1.0, 0.22381307988970017], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.0633681], dtype=float32), -1.6882092]. 
=============================================
[2019-04-05 14:48:44,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6393838e-05 9.7362095e-01 4.0101771e-05 1.0134241e-03 1.7896123e-02
 1.2548063e-06 7.4116373e-03], sum to 1.0000
[2019-04-05 14:48:44,094] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7440
[2019-04-05 14:48:44,121] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.666666666666667, 50.0, 21.16666666666666, 213.3333333333333, 19.0, 23.29871571123545, -0.317371671467485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3518400.0000, 
sim time next is 3519000.0000, 
raw observation next is [2.5, 50.5, 13.0, 151.0, 19.0, 23.12348398174809, -0.2562958514463115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5318559556786704, 0.505, 0.043333333333333335, 0.16685082872928178, 0.08333333333333333, 0.4269569984790076, 0.4145680495178962, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6295547], dtype=float32), -0.4753793]. 
=============================================
[2019-04-05 14:48:44,131] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.591446]
 [80.65574 ]
 [81.50052 ]
 [82.15274 ]
 [82.76433 ]], R is [[78.68096161]
 [78.89414978]
 [79.10520935]
 [79.24272919]
 [79.45030212]].
[2019-04-05 14:48:44,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2015889e-04 9.7898245e-01 1.4035336e-03 8.6326775e-04 7.4346974e-03
 5.3150310e-05 1.1142755e-02], sum to 1.0000
[2019-04-05 14:48:44,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8062
[2019-04-05 14:48:44,349] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.166666666666667, 39.16666666666666, 112.3333333333333, 812.0, 19.0, 20.89875548950825, -0.6824659279264295, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3937800.0000, 
sim time next is 3938400.0000, 
raw observation next is [-5.0, 38.0, 110.5, 806.0, 19.0, 20.95080148432182, -0.671648378274947, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.32409972299168976, 0.38, 0.36833333333333335, 0.8906077348066298, 0.08333333333333333, 0.24590012369348488, 0.2761172072416843, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0733432], dtype=float32), -1.7498398]. 
=============================================
[2019-04-05 14:48:46,612] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7845861e-06 1.1443349e-01 7.0494128e-04 6.3837981e-03 8.6975121e-01
 1.1562922e-05 8.7131113e-03], sum to 1.0000
[2019-04-05 14:48:46,613] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1942
[2019-04-05 14:48:46,622] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.5, 19.52127572932043, -0.9772416845510642, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3813600.0000, 
sim time next is 3814200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 20.0, 19.48721076275832, -0.9787612750408878, 0.0, 1.0, 196932.0278941172], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.16666666666666666, 0.12393423022985998, 0.1737462416530374, 0.0, 1.0, 0.9377715614005581], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.1208138], dtype=float32), 0.013824241]. 
=============================================
[2019-04-05 14:48:47,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:48:47,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:48:47,387] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run15
[2019-04-05 14:48:48,137] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5739432e-04 3.9794803e-01 2.5414480e-03 8.9583024e-02 4.2122257e-01
 1.3601354e-04 8.8411510e-02], sum to 1.0000
[2019-04-05 14:48:48,139] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5049
[2019-04-05 14:48:48,147] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.333333333333334, 24.66666666666666, 118.0, 860.8333333333334, 19.0, 21.98766478340935, -0.4725923606059987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 4972800.0000, 
sim time next is 4973400.0000, 
raw observation next is [7.5, 25.0, 117.0, 860.0, 19.5, 22.16091576052977, -0.4576978609877685, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6703601108033241, 0.25, 0.39, 0.9502762430939227, 0.125, 0.34674298004414766, 0.34743404633741054, 1.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.23088923], dtype=float32), -1.2687316]. 
=============================================
[2019-04-05 14:48:51,855] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5468936e-05 8.5005808e-01 3.2665674e-04 1.6480487e-02 1.2714668e-01
 2.9372344e-05 5.8932076e-03], sum to 1.0000
[2019-04-05 14:48:51,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4903
[2019-04-05 14:48:51,869] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 19.0, 18.81164843015058, -1.079702747505644, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4514400.0000, 
sim time next is 4515000.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 19.0, 18.78743525306512, -1.090006983563922, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.08333333333333333, 0.06561960442209329, 0.13666433881202603, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.632342], dtype=float32), -0.5933483]. 
=============================================
[2019-04-05 14:48:51,876] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.903946]
 [80.01684 ]
 [80.25789 ]
 [80.53256 ]
 [79.737885]], R is [[79.81005096]
 [80.01194763]
 [80.21183014]
 [80.40971375]
 [80.53418732]].
[2019-04-05 14:48:53,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:48:53,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:48:53,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run15
[2019-04-05 14:49:00,027] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.0785478e-05 5.0632125e-01 2.4856883e-03 1.9000830e-04 4.9075788e-01
 3.8602125e-06 1.7054079e-04], sum to 1.0000
[2019-04-05 14:49:00,029] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7696
[2019-04-05 14:49:00,049] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 19.5, 18.95084217810304, -1.096361657670634, 0.0, 1.0, 197906.3247983299], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4260000.0000, 
sim time next is 4260600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 20.0, 19.03414363547862, -1.03978247462944, 0.0, 1.0, 199241.3496364348], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.16666666666666666, 0.08617863628988509, 0.15340584179018668, 0.0, 1.0, 0.9487683316020705], 
reward next is 0.8571, 
noisyNet noise sample is [array([-0.22742118], dtype=float32), -0.9677931]. 
=============================================
[2019-04-05 14:49:00,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0515437e-05 8.9138985e-01 4.6537328e-03 8.2064769e-04 6.2540345e-02
 2.7041515e-05 4.0547792e-02], sum to 1.0000
[2019-04-05 14:49:00,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3922
[2019-04-05 14:49:00,598] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.3, 59.5, 0.0, 0.0, 19.0, 20.51137980864589, -0.7671382894127602, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 765000.0000, 
sim time next is 765600.0000, 
raw observation next is [-5.4, 60.0, 0.0, 0.0, 19.0, 20.38456633426627, -0.792437324226607, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.31301939058171746, 0.6, 0.0, 0.0, 0.08333333333333333, 0.19871386118885592, 0.23585422525779767, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3095119], dtype=float32), 0.99318296]. 
=============================================
[2019-04-05 14:49:01,800] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7586395e-06 3.0123672e-01 1.0300330e-03 3.8896883e-03 6.7643756e-01
 6.5958542e-05 1.7335206e-02], sum to 1.0000
[2019-04-05 14:49:01,806] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8258
[2019-04-05 14:49:01,814] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 19.5, 21.66749987451319, -0.5185871638371277, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4297200.0000, 
sim time next is 4297800.0000, 
raw observation next is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 20.0, 21.63578641720808, -0.5310854184700778, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6361957525392429, 0.6333333333333334, 0.05333333333333334, 0.16795580110497238, 0.16666666666666666, 0.3029822014340067, 0.32297152717664074, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([-1.5687844], dtype=float32), -0.42458016]. 
=============================================
[2019-04-05 14:49:02,843] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2374782e-04 9.3272066e-01 5.1233605e-03 1.6284110e-02 2.4106935e-02
 1.2390288e-04 2.1417310e-02], sum to 1.0000
[2019-04-05 14:49:02,845] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0464
[2019-04-05 14:49:02,867] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.333333333333333, 46.00000000000001, 107.0, 643.0, 19.0, 20.72264278343401, -0.7222966570058164, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5044200.0000, 
sim time next is 5044800.0000, 
raw observation next is [1.666666666666667, 45.0, 109.5, 670.5, 19.0, 20.82988244859263, -0.6896940429794117, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5087719298245615, 0.45, 0.365, 0.7408839779005525, 0.08333333333333333, 0.23582353738271924, 0.27010198567352944, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20824178], dtype=float32), -2.3225164]. 
=============================================
[2019-04-05 14:49:03,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:49:03,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:49:03,521] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run15
[2019-04-05 14:49:05,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1969110e-05 9.0719885e-01 3.1300506e-04 7.1974210e-03 7.6525889e-02
 1.5655866e-04 8.5664932e-03], sum to 1.0000
[2019-04-05 14:49:05,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7574
[2019-04-05 14:49:05,343] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [1.0, 86.0, 236.6666666666667, 126.8333333333333, 19.0, 21.51735642821457, -0.5376953090907551, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4443600.0000, 
sim time next is 4444200.0000, 
raw observation next is [1.0, 86.0, 251.0, 146.0, 19.0, 21.59213875592231, -0.5227561548916159, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.8366666666666667, 0.16132596685082873, 0.08333333333333333, 0.2993448963268592, 0.32574794836946136, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.7724, 
noisyNet noise sample is [array([-0.9783243], dtype=float32), 0.5556196]. 
=============================================
[2019-04-05 14:49:05,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:49:05,818] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:49:05,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run15
[2019-04-05 14:49:12,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:49:12,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:49:12,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run15
[2019-04-05 14:49:14,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:49:14,118] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:49:14,144] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run15
[2019-04-05 14:49:14,201] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-05 14:49:14,202] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:49:14,202] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:49:14,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run96
[2019-04-05 14:49:14,220] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:49:14,221] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:49:14,222] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:49:14,222] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:49:14,226] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run96
[2019-04-05 14:49:14,242] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run96
[2019-04-05 14:49:44,390] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123658046]
[2019-04-05 14:49:44,391] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.8, 85.5, 0.0, 0.0, 19.0, 18.75849208834577, -1.064744895223602, 0.0, 1.0, 0.0]
[2019-04-05 14:49:44,391] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:49:44,392] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.46074024e-06 8.81035447e-01 1.66253682e-04 5.36159566e-03
 1.11805856e-01 1.03365737e-05 1.61906704e-03], sampled 0.9950070360084491
[2019-04-05 14:50:04,033] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123658046]
[2019-04-05 14:50:04,034] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.857739010666667, 42.74039450833333, 0.0, 0.0, 19.0, 18.6459588204621, -1.164575818870367, 0.0, 1.0, 130094.6687929547]
[2019-04-05 14:50:04,034] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:50:04,034] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.0015190e-05 8.5208291e-01 1.0921211e-03 1.2223964e-02 1.2504584e-01
 1.2013302e-04 9.4050523e-03], sampled 0.7033152538786183
[2019-04-05 14:50:25,341] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6268.7017 141897120.2548 -1913.1577
[2019-04-05 14:50:37,041] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5840.4102 169701027.4700 -2475.4349
[2019-04-05 14:50:41,579] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123658046]
[2019-04-05 14:50:41,579] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.9332280216666666, 50.58459480666667, 0.0, 0.0, 19.0, 18.56854038184504, -1.187585101861609, 0.0, 1.0, 197507.9873538577]
[2019-04-05 14:50:41,579] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:50:41,580] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7713313e-05 8.2903647e-01 1.3886098e-03 1.2705280e-02 1.4536139e-01
 1.1010781e-04 1.1370501e-02], sampled 0.21249876220913622
[2019-04-05 14:50:43,881] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5987.1691 185077665.2382 -2367.5056
[2019-04-05 14:50:44,907] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1900000, evaluation results [1900000.0, 5840.410249838053, 169701027.47001734, -2475.434919171064, 6268.70171662098, 141897120.25481886, -1913.1576845750747, 5987.1691144990755, 185077665.23824987, -2367.5055889818714]
[2019-04-05 14:50:45,675] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2360084e-06 9.2359918e-01 1.9266306e-05 4.3051245e-05 6.5810703e-02
 5.0886269e-05 1.0468624e-02], sum to 1.0000
[2019-04-05 14:50:45,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9290
[2019-04-05 14:50:45,684] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [14.0, 59.5, 0.0, 0.0, 19.0, 22.50865714647155, -0.1626469754942255, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 1108200.0000, 
sim time next is 1108800.0000, 
raw observation next is [13.8, 60.0, 0.0, 0.0, 19.5, 22.43035766909091, -0.1780964569603796, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.844875346260388, 0.6, 0.0, 0.0, 0.125, 0.36919647242424247, 0.44063451434654016, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.1426817], dtype=float32), 1.9224144]. 
=============================================
[2019-04-05 14:50:47,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:50:47,308] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:50:47,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run15
[2019-04-05 14:50:48,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3480825e-06 6.0974354e-01 1.5113665e-03 7.3118433e-03 3.5769948e-01
 1.4931387e-06 2.3726007e-02], sum to 1.0000
[2019-04-05 14:50:48,191] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-05 14:50:48,196] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.0, 65.0, 0.0, 0.0, 20.0, 21.33904980362786, -0.4117132155826024, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [20.5], 
sim time this is 1189800.0000, 
sim time next is 1190400.0000, 
raw observation next is [17.9, 65.66666666666667, 0.0, 0.0, 20.5, 21.3348921225279, -0.4146393562002926, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.9584487534626038, 0.6566666666666667, 0.0, 0.0, 0.20833333333333334, 0.27790767687732504, 0.36178688126656916, 0.0, 0.0, 0.0], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.2746588], dtype=float32), 0.9634938]. 
=============================================
[2019-04-05 14:50:52,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6346665e-05 7.1592677e-01 1.0063999e-04 6.5421341e-03 2.7549997e-01
 7.6975426e-05 1.8271441e-03], sum to 1.0000
[2019-04-05 14:50:52,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5942
[2019-04-05 14:50:52,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.5, 92.0, 54.5, 0.0, 19.0, 21.5063529854278, -0.4908798936442859, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1330800.0000, 
sim time next is 1331400.0000, 
raw observation next is [0.5, 92.0, 64.0, 0.0, 19.0, 21.5456771822499, -0.490595537214767, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.21333333333333335, 0.0, 0.08333333333333333, 0.295473098520825, 0.3364681542617443, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.321218], dtype=float32), -0.4245499]. 
=============================================
[2019-04-05 14:50:52,989] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.8089470e-04 7.8163642e-01 8.6437119e-03 1.4988238e-03 7.3676504e-02
 2.2344038e-05 1.3394137e-01], sum to 1.0000
[2019-04-05 14:50:52,990] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5646
[2019-04-05 14:50:53,000] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 20.0, 22.25887235727102, -0.2905532426393664, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 4651200.0000, 
sim time next is 4651800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 22.16750158036712, -0.3053947949939104, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.3472917983639266, 0.3982017350020299, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68707573], dtype=float32), 1.0100695]. 
=============================================
[2019-04-05 14:50:57,345] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00562523 0.6803953  0.00905589 0.00920649 0.18260457 0.00139623
 0.11171626], sum to 1.0000
[2019-04-05 14:50:57,346] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7118
[2019-04-05 14:50:57,367] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.033333333333333, 29.33333333333333, 102.0, 0.0, 19.0, 20.42722705293711, -0.9869942942727922, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 470400.0000, 
sim time next is 471000.0000, 
raw observation next is [-2.666666666666666, 28.66666666666667, 106.0, 0.0, 19.5, 20.41766254086969, -0.9869722010302496, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3887349953831949, 0.28666666666666674, 0.35333333333333333, 0.0, 0.125, 0.20147187840580738, 0.17100926632325011, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6456511], dtype=float32), 1.0717418]. 
=============================================
[2019-04-05 14:50:57,376] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[39.705524]
 [39.880127]
 [39.30148 ]
 [38.908493]
 [38.739834]], R is [[39.57215118]
 [39.17642975]
 [38.78466415]
 [38.39681625]
 [38.0128479 ]].
[2019-04-05 14:50:57,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3899284e-03 1.5749547e-01 4.2798356e-03 3.4291524e-02 6.7597377e-01
 2.5050822e-04 1.2631899e-01], sum to 1.0000
[2019-04-05 14:50:57,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9926
[2019-04-05 14:50:57,586] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 129.8333333333333, 227.3333333333333, 21.0, 20.71679508503746, -0.7241286733612459, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 4610400.0000, 
sim time next is 4611000.0000, 
raw observation next is [-2.0, 71.0, 136.6666666666667, 283.6666666666666, 23.0, 20.92999438233487, -0.6517376052509706, 1.0, 1.0, 197484.8627277106], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.71, 0.4555555555555557, 0.3134438305709023, 0.4166666666666667, 0.2441661985279057, 0.2827541315830098, 1.0, 1.0, 0.9404041082271933], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3729366], dtype=float32), 0.3936174]. 
=============================================
[2019-04-05 14:50:57,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[55.095703]
 [55.211296]
 [55.837723]
 [57.946995]
 [60.15188 ]], R is [[55.70023727]
 [55.14323425]
 [54.59180069]
 [54.04588318]
 [53.5054245 ]].
[2019-04-05 14:51:02,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:51:02,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:02,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run15
[2019-04-05 14:51:03,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:51:03,230] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:03,254] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run15
[2019-04-05 14:51:03,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3176485e-05 9.5345372e-01 5.5621407e-04 2.0174058e-03 1.9229002e-02
 4.2020769e-05 2.4638543e-02], sum to 1.0000
[2019-04-05 14:51:03,863] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9246
[2019-04-05 14:51:03,884] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.8, 49.0, 133.8333333333333, 0.0, 19.0, 23.74131885395293, -0.1240900293869353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1608000.0000, 
sim time next is 1608600.0000, 
raw observation next is [13.8, 49.0, 122.6666666666667, 0.0, 19.0, 23.38627146688754, -0.0768613829405883, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.408888888888889, 0.0, 0.08333333333333333, 0.4488559555739616, 0.47437953901980395, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3086771], dtype=float32), -0.9743065]. 
=============================================
[2019-04-05 14:51:04,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:51:04,830] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:04,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run15
[2019-04-05 14:51:12,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:51:12,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:12,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run15
[2019-04-05 14:51:16,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:51:16,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:16,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run15
[2019-04-05 14:51:19,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:51:19,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:19,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run15
[2019-04-05 14:51:20,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3476315e-05 6.9924051e-01 5.5633904e-05 2.9013714e-02 2.4498481e-01
 1.6668348e-05 2.6675234e-02], sum to 1.0000
[2019-04-05 14:51:20,886] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1472
[2019-04-05 14:51:20,896] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 19.0, 26.39905711536485, 0.5586945638329114, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 5077800.0000, 
sim time next is 5078400.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 19.0, 26.21367223464652, 0.5337982402109512, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.08333333333333333, 0.6844726862205434, 0.6779327467369837, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08122873], dtype=float32), 0.56658655]. 
=============================================
[2019-04-05 14:51:22,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:51:22,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:22,715] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run15
[2019-04-05 14:51:24,517] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-05 14:51:24,518] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:51:24,519] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:51:24,519] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:51:24,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:24,521] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:24,520] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:51:24,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run97
[2019-04-05 14:51:24,544] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run97
[2019-04-05 14:51:24,561] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run97
[2019-04-05 14:51:50,711] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12236557]
[2019-04-05 14:51:50,712] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [17.7, 67.0, 0.0, 0.0, 19.5, 20.67973050762672, -0.5705977863409, 0.0, 0.0, 0.0]
[2019-04-05 14:51:50,712] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:51:50,712] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2367607e-05 8.9460546e-01 8.1886863e-04 5.9455168e-03 9.2538342e-02
 4.6963614e-05 6.0325428e-03], sampled 0.9619811065073967
[2019-04-05 14:51:53,880] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12236557]
[2019-04-05 14:51:53,881] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.8, 100.0, 42.16666666666667, 0.0, 19.0, 19.59407471508274, -0.9693449818024185, 1.0, 1.0, 0.0]
[2019-04-05 14:51:53,881] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:51:53,882] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.2919750e-04 7.7455038e-01 2.4908010e-03 9.2228316e-03 1.8885168e-01
 3.0037988e-04 2.4354765e-02], sampled 0.6272393171107958
[2019-04-05 14:52:04,389] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12236557]
[2019-04-05 14:52:04,390] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.009940322333333, 70.66949025333334, 0.0, 0.0, 19.5, 18.71738971536638, -1.190198506889594, 0.0, 1.0, 55583.79033523746]
[2019-04-05 14:52:04,390] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:52:04,392] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.0828221e-05 6.5349966e-01 2.2085374e-03 8.6549744e-03 3.1581625e-01
 1.5246066e-04 1.9607233e-02], sampled 0.4074074366410876
[2019-04-05 14:52:11,564] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12236557]
[2019-04-05 14:52:11,564] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-0.4, 70.5, 0.0, 0.0, 19.0, 19.04555140474486, -1.08242380446142, 0.0, 1.0, 0.0]
[2019-04-05 14:52:11,564] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:52:11,566] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.9212454e-06 8.0951673e-01 5.9627427e-04 6.7832666e-03 1.7903897e-01
 3.2992073e-05 4.0228334e-03], sampled 0.4119463676256401
[2019-04-05 14:52:18,955] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12236557]
[2019-04-05 14:52:18,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.06666666666667, 83.66666666666666, 0.0, 0.0, 19.0, 21.01359965521144, -0.5211508440802477, 0.0, 1.0, 0.0]
[2019-04-05 14:52:18,956] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:52:18,956] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8303494e-07 9.0976322e-01 1.0434903e-04 1.1079495e-03 8.7285019e-02
 2.0619318e-06 1.7370487e-03], sampled 0.0048121583243219135
[2019-04-05 14:52:29,030] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12236557]
[2019-04-05 14:52:29,030] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.30149126, 94.14029731, 0.0, 0.0, 19.0, 18.92112827434152, -1.014267735482517, 0.0, 1.0, 0.0]
[2019-04-05 14:52:29,030] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:52:29,031] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.2703626e-06 8.0576545e-01 2.4865376e-04 2.9636573e-03 1.8689807e-01
 1.4849982e-05 4.1060578e-03], sampled 0.8499552963846475
[2019-04-05 14:52:30,961] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12236557]
[2019-04-05 14:52:30,961] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-8.361844106, 65.49879767833333, 91.55174196666667, 761.2297985666667, 19.0, 20.20186408670924, -0.8123201953895535, 1.0, 1.0, 0.0]
[2019-04-05 14:52:30,961] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:52:30,962] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.1851662e-04 8.7490094e-01 1.3463895e-03 2.8844334e-03 9.8354101e-02
 1.3727744e-04 2.2158362e-02], sampled 0.26127697601742583
[2019-04-05 14:52:38,378] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6425.6354 142947882.9728 -1704.8094
[2019-04-05 14:52:50,851] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5957.4699 172864651.4106 -2334.0398
[2019-04-05 14:52:55,113] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6020.6526 186769061.8863 -2255.9306
[2019-04-05 14:52:56,139] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1920000, evaluation results [1920000.0, 5957.469949955547, 172864651.41055116, -2334.0397533956334, 6425.635407774428, 142947882.9728341, -1704.8094359370214, 6020.652568496423, 186769061.88631198, -2255.9305729300177]
[2019-04-05 14:53:03,362] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0968033e-05 7.8799748e-01 5.3856293e-05 3.2236087e-04 2.1044682e-01
 9.2109221e-06 1.1393453e-03], sum to 1.0000
[2019-04-05 14:53:03,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2276
[2019-04-05 14:53:03,374] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 19.0, 19.70281233141828, -0.9727185105054797, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 181200.0000, 
sim time next is 181800.0000, 
raw observation next is [-8.9, 76.0, 0.0, 0.0, 19.0, 19.68576724929066, -0.9803974292030553, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.76, 0.0, 0.0, 0.08333333333333333, 0.14048060410755495, 0.1732008569323149, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7130568], dtype=float32), -1.6657265]. 
=============================================
[2019-04-05 14:53:20,337] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6375370e-04 4.4855028e-01 1.0063283e-03 1.5954903e-03 4.8564646e-01
 1.4445311e-04 6.2693208e-02], sum to 1.0000
[2019-04-05 14:53:20,337] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0938
[2019-04-05 14:53:20,354] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 19.5, 21.17906270529551, -0.5811801804347972, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1334400.0000, 
sim time next is 1335000.0000, 
raw observation next is [1.0, 92.0, 110.3333333333333, 0.0, 19.0, 21.21588220487461, -0.5746092923366578, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.92, 0.36777777777777765, 0.0, 0.08333333333333333, 0.26799018373955086, 0.30846356922111406, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.2539, 
noisyNet noise sample is [array([0.0249919], dtype=float32), -0.08877484]. 
=============================================
[2019-04-05 14:53:20,371] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[62.964825]
 [63.012653]
 [62.93395 ]
 [62.867413]
 [62.897415]], R is [[62.69037628]
 [62.18024063]
 [61.55843735]
 [60.94285202]
 [60.33342361]].
[2019-04-05 14:53:31,192] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1459712e-07 9.3372244e-01 1.6670261e-04 1.0765645e-04 6.4440973e-02
 4.3162260e-07 1.5616482e-03], sum to 1.0000
[2019-04-05 14:53:31,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3670
[2019-04-05 14:53:31,203] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [4.616666666666666, 84.83333333333334, 0.0, 0.0, 19.5, 21.72646375182265, -0.4473503553377984, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1570200.0000, 
sim time next is 1570800.0000, 
raw observation next is [4.633333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 21.65396937167298, -0.462314732634229, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5909510618651893, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.3044974476394149, 0.3458950891219237, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05560667], dtype=float32), 0.8992205]. 
=============================================
[2019-04-05 14:53:33,312] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-05 14:53:33,313] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:53:33,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:53:33,315] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:53:33,316] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:53:33,317] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:53:33,317] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:53:33,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run98
[2019-04-05 14:53:33,349] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run98
[2019-04-05 14:53:33,365] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run98
[2019-04-05 14:53:47,608] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1217801]
[2019-04-05 14:53:47,608] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.1, 27.0, 118.0, 0.0, 20.0, 20.45498965449292, -0.9958420332100114, 1.0, 1.0, 0.0]
[2019-04-05 14:53:47,608] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:53:47,608] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.00392018 0.7321892  0.01083596 0.01388083 0.15582407 0.00215483
 0.08119503], sampled 0.8866293003188468
[2019-04-05 14:54:02,176] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1217801]
[2019-04-05 14:54:02,177] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.1957141015, 87.76877539, 43.62298395, 0.0, 19.0, 18.86646646559634, -1.01627209066194, 0.0, 1.0, 0.0]
[2019-04-05 14:54:02,177] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:54:02,177] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3974602e-05 8.3854181e-01 6.1054970e-04 6.0794731e-03 1.4970644e-01
 4.1413197e-05 5.0062542e-03], sampled 0.2956570945628587
[2019-04-05 14:54:02,243] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1217801]
[2019-04-05 14:54:02,243] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.165129440333333, 77.70799648166667, 12.35092347333333, 0.0, 19.0, 18.61294803104207, -1.063273104110855, 0.0, 1.0, 29775.17764275633]
[2019-04-05 14:54:02,243] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:54:02,243] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.5610914e-05 8.2073683e-01 7.5814116e-04 5.9093866e-03 1.6610698e-01
 5.0274179e-05 6.4227809e-03], sampled 0.2081230287185868
[2019-04-05 14:54:04,466] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.1217801]
[2019-04-05 14:54:04,466] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.93836763, 92.43711738, 0.0, 0.0, 19.5, 19.07570100861306, -1.040108818392214, 0.0, 1.0, 197516.7914337934]
[2019-04-05 14:54:04,466] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:54:04,467] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.1372923e-06 8.3471543e-01 3.1415978e-04 6.0152630e-03 1.5608753e-01
 2.1125812e-05 2.8423928e-03], sampled 0.002674387770323694
[2019-04-05 14:54:45,595] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6255.5147 141732560.5917 -1875.8619
[2019-04-05 14:54:58,572] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 6019.2547 171521078.6064 -2280.7993
[2019-04-05 14:55:04,897] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6093.2673 185903568.0414 -2198.3970
[2019-04-05 14:55:05,924] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1940000, evaluation results [1940000.0, 6019.25465807053, 171521078.6063593, -2280.7993163805236, 6255.5146928431095, 141732560.59170255, -1875.8618788768138, 6093.267266490774, 185903568.0414208, -2198.396960437305]
[2019-04-05 14:55:13,469] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0892240e-06 5.7428336e-01 9.8511788e-05 2.0096854e-03 4.1835383e-01
 3.6940932e-05 5.2126185e-03], sum to 1.0000
[2019-04-05 14:55:13,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5364
[2019-04-05 14:55:13,525] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.6, 75.0, 152.0, 66.0, 23.5, 21.97837098340503, -0.4562009739149211, 0.0, 1.0, 103214.8267113663], 
current ob forecast is [], 
actual action is [22.5], 
sim time this is 1854000.0000, 
sim time next is 1854600.0000, 
raw observation next is [-5.5, 74.33333333333333, 162.6666666666667, 71.0, 22.5, 22.27593816122462, -0.424718745377588, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.3102493074792244, 0.7433333333333333, 0.5422222222222224, 0.07845303867403315, 0.375, 0.35632818010205164, 0.3584270848741373, 0.0, 1.0, 0.0], 
reward next is 0.5000, 
noisyNet noise sample is [array([-1.0988446], dtype=float32), 0.6074859]. 
=============================================
[2019-04-05 14:55:14,594] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.8037853e-04 7.8141898e-01 9.2755450e-04 1.7686311e-02 1.9709338e-01
 7.3336414e-05 2.0200028e-03], sum to 1.0000
[2019-04-05 14:55:14,597] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8043
[2019-04-05 14:55:14,610] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.6, 45.0, 42.5, 37.0, 19.0, 19.29111374340032, -1.04305404160765, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2394000.0000, 
sim time next is 2394600.0000, 
raw observation next is [-0.7833333333333333, 44.83333333333334, 30.33333333333333, 30.0, 19.5, 19.254835884976, -1.057894020243141, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.44090489381348114, 0.4483333333333334, 0.1011111111111111, 0.03314917127071823, 0.125, 0.10456965708133333, 0.14736865991895298, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-1.5400691], dtype=float32), -0.32843298]. 
=============================================
[2019-04-05 14:55:14,791] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4278737e-03 8.0021679e-01 3.3741555e-04 5.0221791e-04 1.1565783e-02
 7.6388123e-06 1.8494228e-01], sum to 1.0000
[2019-04-05 14:55:14,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7587
[2019-04-05 14:55:14,807] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [10.13333333333333, 59.66666666666667, 213.3333333333333, 222.1666666666667, 19.0, 22.25350220666905, -0.3622366517557236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1596000.0000, 
sim time next is 1596600.0000, 
raw observation next is [10.5, 59.0, 216.0, 249.0, 19.0, 22.32710246785788, -0.3428556420377129, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7534626038781165, 0.59, 0.72, 0.2751381215469613, 0.08333333333333333, 0.36059187232149004, 0.38571478598742903, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.81390005], dtype=float32), 0.06643494]. 
=============================================
[2019-04-05 14:55:14,877] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.70157965e-04 2.57031798e-01 1.27146505e-02 5.11698471e-03
 7.17448771e-01 1.72289278e-04 7.24540185e-03], sum to 1.0000
[2019-04-05 14:55:14,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9079
[2019-04-05 14:55:14,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5576161e-05 8.7770295e-01 3.5799638e-04 2.3897193e-04 8.0732919e-02
 1.2659035e-05 4.0868957e-02], sum to 1.0000
[2019-04-05 14:55:14,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8445
[2019-04-05 14:55:14,922] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 24.0, 22.94122064758121, -0.3203758959484523, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [24.5], 
sim time this is 1798200.0000, 
sim time next is 1798800.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 24.5, 22.93219977864985, -0.3460070740089463, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.5416666666666666, 0.4110166482208208, 0.38466430866368456, 0.0, 1.0, 0.0], 
reward next is 0.2143, 
noisyNet noise sample is [array([0.79448396], dtype=float32), -0.7041396]. 
=============================================
[2019-04-05 14:55:14,974] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 71.0, 108.1666666666667, 0.0, 24.0, 23.82614026715819, -0.1590940490963919, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 819600.0000, 
sim time next is 820200.0000, 
raw observation next is [-4.5, 71.0, 106.3333333333333, 0.0, 23.0, 23.67012298368423, -0.1988814886399616, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.35444444444444434, 0.0, 0.4166666666666667, 0.4725102486403525, 0.4337061704533461, 1.0, 1.0, 0.0], 
reward next is 0.4286, 
noisyNet noise sample is [array([-0.04071828], dtype=float32), 1.3178736]. 
=============================================
[2019-04-05 14:55:17,170] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.2515424e-05 5.3351969e-01 7.2637941e-03 1.8693764e-02 3.9088377e-01
 7.5352532e-06 4.9538907e-02], sum to 1.0000
[2019-04-05 14:55:17,171] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4144
[2019-04-05 14:55:17,177] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.1, 82.5, 0.0, 0.0, 19.0, 20.91166856902581, -0.4986211031270793, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1216200.0000, 
sim time next is 1216800.0000, 
raw observation next is [16.1, 83.0, 0.0, 0.0, 19.0, 20.89877796776154, -0.500793147750063, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9085872576177286, 0.83, 0.0, 0.0, 0.08333333333333333, 0.24156483064679493, 0.333068950749979, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0486151], dtype=float32), -0.060489576]. 
=============================================
[2019-04-05 14:55:18,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.3278845e-06 9.1503668e-01 1.1346512e-04 1.4380412e-02 6.7081027e-02
 8.3904069e-06 3.3765913e-03], sum to 1.0000
[2019-04-05 14:55:18,429] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2101
[2019-04-05 14:55:18,434] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 19.0, 20.80348844895927, -0.6063020048577962, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1016400.0000, 
sim time next is 1017000.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 19.0, 20.90230851835498, -0.5920052025825199, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.08333333333333333, 0.24185904319624849, 0.3026649324724934, 1.0, 1.0, 0.0], 
reward next is 0.0799, 
noisyNet noise sample is [array([-0.51179224], dtype=float32), -1.855607]. 
=============================================
[2019-04-05 14:55:18,447] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[88.168365]
 [88.05267 ]
 [87.712135]
 [87.51389 ]
 [87.306595]], R is [[87.44911957]
 [86.57463074]
 [85.70888519]
 [84.85179901]
 [84.63008881]].
[2019-04-05 14:55:24,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.49764505e-03 6.26332462e-01 1.08713948e-03 4.28313063e-03
 3.08257669e-01 1.15376184e-04 5.84265254e-02], sum to 1.0000
[2019-04-05 14:55:24,553] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1827
[2019-04-05 14:55:24,591] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.1, 62.0, 112.0, 0.0, 20.0, 22.49924627373846, -0.4738609331284392, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1953000.0000, 
sim time next is 1953600.0000, 
raw observation next is [-3.0, 62.0, 105.6666666666667, 0.0, 19.0, 22.54063636983607, -0.4724126428246926, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.62, 0.3522222222222223, 0.0, 0.08333333333333333, 0.3783863641530057, 0.3425291190584358, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15875931], dtype=float32), 0.3772508]. 
=============================================
[2019-04-05 14:55:27,513] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4002736e-05 9.1673195e-01 7.2853095e-03 4.9966399e-04 6.3283041e-02
 3.9737954e-04 1.1768634e-02], sum to 1.0000
[2019-04-05 14:55:27,517] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6451
[2019-04-05 14:55:27,571] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.333333333333333, 44.33333333333334, 0.0, 0.0, 19.5, 20.81337816800735, -0.7434800079587514, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2396400.0000, 
sim time next is 2397000.0000, 
raw observation next is [-1.516666666666667, 44.16666666666667, 0.0, 0.0, 19.0, 20.99952151933957, -0.7364193851321882, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.4205909510618652, 0.4416666666666667, 0.0, 0.0, 0.08333333333333333, 0.24996012661163083, 0.25452687162260396, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5776304], dtype=float32), 1.5469407]. 
=============================================
[2019-04-05 14:55:27,579] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.21473 ]
 [74.44724 ]
 [75.164154]
 [74.79026 ]
 [74.70437 ]], R is [[74.03855133]
 [74.22673798]
 [74.27018738]
 [74.17034912]
 [73.9286499 ]].
[2019-04-05 14:55:30,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5449051e-05 4.6982187e-01 1.6307458e-02 6.3859066e-03 4.9470362e-01
 1.2539489e-05 1.2723124e-02], sum to 1.0000
[2019-04-05 14:55:30,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0026
[2019-04-05 14:55:30,880] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [16.18333333333333, 77.5, 0.0, 0.0, 19.0, 21.12767107647124, -0.4640555294853605, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1209000.0000, 
sim time next is 1209600.0000, 
raw observation next is [16.1, 78.0, 0.0, 0.0, 19.0, 21.1092725959842, -0.4669316923954363, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.78, 0.0, 0.0, 0.08333333333333333, 0.2591060496653501, 0.34435610253485455, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5400096], dtype=float32), -0.9250125]. 
=============================================
[2019-04-05 14:55:34,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7705183e-03 9.6327388e-01 2.6886859e-03 1.7199345e-03 1.0763810e-02
 1.2512413e-04 1.6658019e-02], sum to 1.0000
[2019-04-05 14:55:34,482] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7396
[2019-04-05 14:55:34,500] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.266666666666667, 76.33333333333334, 9.166666666666666, 1.666666666666667, 19.0, 20.88101079466024, -0.8461582635961798, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1963200.0000, 
sim time next is 1963800.0000, 
raw observation next is [-4.45, 77.0, 0.0, 0.0, 19.0, 20.69379115052762, -0.8946141707803607, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3393351800554017, 0.77, 0.0, 0.0, 0.08333333333333333, 0.22448259587730165, 0.20179527640654646, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2941579], dtype=float32), 0.32997814]. 
=============================================
[2019-04-05 14:55:38,310] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5807857e-04 6.6818094e-01 8.2407251e-04 3.2974929e-03 3.2518390e-01
 2.0614361e-04 1.9493782e-03], sum to 1.0000
[2019-04-05 14:55:38,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3233
[2019-04-05 14:55:38,327] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 19.98063276398028, -0.919915439947573, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3380400.0000, 
sim time next is 3381000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 19.92314439790626, -0.9366017649575099, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.16026203315885498, 0.18779941168083003, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4799331], dtype=float32), 0.124362625]. 
=============================================
[2019-04-05 14:55:38,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[69.76501]
 [70.17513]
 [70.62205]
 [71.58992]
 [71.49653]], R is [[69.72428894]
 [70.0270462 ]
 [70.25534821]
 [70.55279541]
 [70.84726715]].
[2019-04-05 14:55:42,251] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-05 14:55:42,252] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:55:42,252] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:55:42,252] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:55:42,253] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:55:42,253] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:55:42,254] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:55:42,265] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run99
[2019-04-05 14:55:42,283] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run99
[2019-04-05 14:55:42,303] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run99
[2019-04-05 14:56:12,086] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12201956]
[2019-04-05 14:56:12,086] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [13.8, 49.0, 167.0, 41.33333333333332, 19.0, 21.39070184889268, -0.5649672970047975, 1.0, 1.0, 0.0]
[2019-04-05 14:56:12,086] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-05 14:56:12,087] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.18102485e-04 9.22073722e-01 6.29401649e-04 2.33046710e-03
 6.28380850e-02 6.60338919e-05 1.19442008e-02], sampled 0.48435907389298494
[2019-04-05 14:56:15,129] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12201956]
[2019-04-05 14:56:15,130] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.23333333333333, 81.0, 0.0, 0.0, 19.0, 19.6457127383459, -0.8325890873376395, 0.0, 1.0, 0.0]
[2019-04-05 14:56:15,130] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:56:15,132] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.1791345e-07 9.2682624e-01 8.8347580e-05 2.4670700e-03 6.9781236e-02
 3.1365337e-06 8.3330786e-04], sampled 0.7490790462726095
[2019-04-05 14:56:40,058] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12201956]
[2019-04-05 14:56:40,058] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.73333333333333, 58.66666666666666, 0.0, 0.0, 19.0, 19.81356292220599, -0.9174724636779099, 0.0, 1.0, 0.0]
[2019-04-05 14:56:40,059] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:56:40,059] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.2675825e-06 9.1846067e-01 2.6707974e-04 2.9528444e-03 7.5885244e-02
 9.7894590e-06 2.4211798e-03], sampled 0.5877876036145014
[2019-04-05 14:56:46,438] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.12201956]
[2019-04-05 14:56:46,438] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [8.725340009, 94.72386348666666, 25.05983595166666, 459.4564795333333, 19.0, 18.77266944976588, -1.139350265779553, 1.0, 1.0, 0.0]
[2019-04-05 14:56:46,438] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:56:46,439] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.4706714e-05 8.5331780e-01 9.6951734e-04 5.7228948e-03 1.3129666e-01
 7.9480706e-05 8.5489554e-03], sampled 0.5644956535741493
[2019-04-05 14:56:54,642] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6332.6794 141901338.0949 -1849.7334
[2019-04-05 14:57:05,850] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5930.1704 172278899.9416 -2355.9945
[2019-04-05 14:57:14,298] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6072.5175 185131265.0166 -2275.1096
[2019-04-05 14:57:15,330] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1960000, evaluation results [1960000.0, 5930.170433361736, 172278899.94158512, -2355.9944617286656, 6332.679359544383, 141901338.09494743, -1849.7334400829047, 6072.517471650985, 185131265.01656047, -2275.1095546648417]
[2019-04-05 14:57:22,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2265357e-04 1.0957532e-01 1.2412715e-03 4.3552266e-03 8.7607980e-01
 1.0830574e-05 8.3147995e-03], sum to 1.0000
[2019-04-05 14:57:22,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3931
[2019-04-05 14:57:22,658] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-1.666666666666667, 47.33333333333333, 0.0, 0.0, 19.5, 21.79002066444776, -0.5351999729232287, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 3620400.0000, 
sim time next is 3621000.0000, 
raw observation next is [-1.833333333333333, 48.66666666666667, 0.0, 0.0, 20.0, 21.692060657169, -0.5506995021468264, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.41181902123730385, 0.4866666666666667, 0.0, 0.0, 0.16666666666666666, 0.3076717214307501, 0.3164334992843912, 0.0, 1.0, 0.0], 
reward next is 0.8571, 
noisyNet noise sample is [array([0.49596447], dtype=float32), -0.24180155]. 
=============================================
[2019-04-05 14:57:22,674] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.367386]
 [76.5878  ]
 [76.82594 ]
 [77.07718 ]
 [77.39336 ]], R is [[76.06700134]
 [76.23490906]
 [76.47255707]
 [76.56497192]
 [76.72789764]].
[2019-04-05 14:57:26,816] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4258985e-04 9.7023308e-01 6.0574193e-03 6.1029345e-03 4.3112133e-03
 1.8806789e-04 1.2964680e-02], sum to 1.0000
[2019-04-05 14:57:26,818] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0892
[2019-04-05 14:57:26,856] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 71.0, 161.6666666666667, 33.33333333333334, 19.0, 20.2426964599379, -0.8706398023687107, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1862400.0000, 
sim time next is 1863000.0000, 
raw observation next is [-4.5, 71.0, 170.0, 40.0, 19.0, 20.15421393741676, -0.8850734771734893, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.5666666666666667, 0.04419889502762431, 0.08333333333333333, 0.1795178281180633, 0.2049755076088369, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8287392], dtype=float32), 0.107319]. 
=============================================
[2019-04-05 14:57:26,860] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[74.00512 ]
 [74.31753 ]
 [74.79576 ]
 [75.1029  ]
 [75.434456]], R is [[73.96196747]
 [74.22235107]
 [74.48012543]
 [74.73532867]
 [74.98797607]].
[2019-04-05 14:57:27,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00228487 0.18145142 0.00215179 0.23524414 0.5654101  0.00102452
 0.01243326], sum to 1.0000
[2019-04-05 14:57:27,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8224
[2019-04-05 14:57:27,126] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.383333333333334, 48.83333333333333, 0.0, 0.0, 23.0, 19.55178871612689, -1.049021116228724, 0.0, 1.0, 49283.64630926606], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 2423400.0000, 
sim time next is 2424000.0000, 
raw observation next is [-6.566666666666666, 49.66666666666666, 0.0, 0.0, 23.5, 19.57228645532082, -1.040640133117477, 0.0, 1.0, 49087.91634880083], 
processed observation next is [0.0, 0.043478260869565216, 0.28070175438596495, 0.4966666666666666, 0.0, 0.0, 0.4583333333333333, 0.1310238712767351, 0.15311995562750766, 0.0, 1.0, 0.23375198261333727], 
reward next is 0.3571, 
noisyNet noise sample is [array([1.1484426], dtype=float32), 1.5438802]. 
=============================================
[2019-04-05 14:57:27,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.88887]
 [67.89006]
 [67.00632]
 [66.68701]
 [66.48006]], R is [[69.23442841]
 [68.97065735]
 [68.70952606]
 [68.52243042]
 [68.40863037]].
[2019-04-05 14:57:42,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9011655e-04 9.2472512e-01 7.6736016e-03 2.8350770e-03 4.3748021e-02
 7.2438631e-04 2.0103727e-02], sum to 1.0000
[2019-04-05 14:57:42,596] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0327
[2019-04-05 14:57:42,615] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 19.0, 21.81799751331308, -0.5351339134226791, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2294400.0000, 
sim time next is 2295000.0000, 
raw observation next is [-1.15, 48.0, 221.0, 69.0, 19.0, 21.92952195817708, -0.5199349504124412, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4307479224376732, 0.48, 0.7366666666666667, 0.07624309392265194, 0.08333333333333333, 0.32746016318142335, 0.3266883498625196, 1.0, 1.0, 0.0], 
reward next is 0.8007, 
noisyNet noise sample is [array([1.3687729], dtype=float32), 0.2086308]. 
=============================================
[2019-04-05 14:57:42,627] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[50.230568]
 [50.445427]
 [50.69016 ]
 [51.05552 ]
 [51.16962 ]], R is [[50.18939209]
 [50.33616257]
 [50.33906937]
 [50.30000687]
 [50.24298096]].
[2019-04-05 14:57:45,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4179192e-05 8.8866043e-01 8.3063438e-04 9.0366456e-04 1.0616623e-01
 1.2336467e-05 3.3625362e-03], sum to 1.0000
[2019-04-05 14:57:45,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0188
[2019-04-05 14:57:45,596] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 19.0, 19.42242238085712, -1.023701975710054, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3109200.0000, 
sim time next is 3109800.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 19.4249866941328, -1.029974915718406, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.11874889117773346, 0.1566750280938647, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21223064], dtype=float32), 0.6109577]. 
=============================================
[2019-04-05 14:57:46,964] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6500211e-05 9.1207904e-01 8.0660620e-04 1.8732670e-03 7.6925516e-02
 4.9245473e-06 8.2941102e-03], sum to 1.0000
[2019-04-05 14:57:46,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5401
[2019-04-05 14:57:46,971] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 44.00000000000001, 0.0, 0.0, 19.0, 18.80832323197956, -1.116139939086792, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2841000.0000, 
sim time next is 2841600.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 18.77445395334351, -1.124754826766627, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.06453782944529252, 0.12508172441112433, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7989284], dtype=float32), 0.054697093]. 
=============================================
[2019-04-05 14:57:47,128] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.1524407e-06 9.9722397e-01 2.8754456e-04 3.0030930e-04 1.6574860e-03
 1.3342332e-06 5.2026962e-04], sum to 1.0000
[2019-04-05 14:57:47,128] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5472
[2019-04-05 14:57:47,141] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 93.0, 17.5, 48.49999999999999, 19.0, 20.4036820475537, -0.8309885855315806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2878800.0000, 
sim time next is 2879400.0000, 
raw observation next is [2.0, 93.0, 34.99999999999999, 69.99999999999999, 19.0, 20.35912464369233, -0.8394697114269492, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.11666666666666664, 0.07734806629834252, 0.08333333333333333, 0.1965937203076941, 0.22017676285768362, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7778516], dtype=float32), -0.7286222]. 
=============================================
[2019-04-05 14:57:52,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5458949e-05 8.4406219e-02 4.7440978e-04 7.0326176e-04 9.1094673e-01
 5.1095965e-05 3.4027323e-03], sum to 1.0000
[2019-04-05 14:57:52,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1831
[2019-04-05 14:57:52,623] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-6.616666666666667, 77.5, 0.0, 0.0, 19.0, 19.39743460563766, -1.075334011906253, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2175000.0000, 
sim time next is 2175600.0000, 
raw observation next is [-6.533333333333334, 77.0, 0.0, 0.0, 19.5, 19.29842569630868, -1.096140911209772, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.28162511542012925, 0.77, 0.0, 0.0, 0.125, 0.10820214135905666, 0.13461969626340933, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([-0.15221922], dtype=float32), 0.67449886]. 
=============================================
[2019-04-05 14:57:52,860] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-05 14:57:52,860] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 14:57:52,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:57:52,861] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 14:57:52,862] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 14:57:52,862] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:57:52,862] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:57:52,873] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run100
[2019-04-05 14:57:52,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run100
[2019-04-05 14:57:52,921] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run100
[2019-04-05 14:58:22,603] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123064935]
[2019-04-05 14:58:22,603] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [5.833333333333334, 86.83333333333333, 0.0, 0.0, 19.0, 19.00972097844865, -0.9722749342365912, 0.0, 1.0, 0.0]
[2019-04-05 14:58:22,603] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:58:22,604] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.6085942e-06 8.7006122e-01 1.2315490e-04 3.3211242e-03 1.2465209e-01
 6.2985077e-06 1.8346045e-03], sampled 0.8304624781476359
[2019-04-05 14:58:57,610] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123064935]
[2019-04-05 14:58:57,611] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [17.11666666666667, 30.33333333333333, 121.3333333333333, 847.3333333333333, 19.0, 21.03730936491797, -0.5599944141076505, 0.0, 0.0, 0.0]
[2019-04-05 14:58:57,611] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:58:57,612] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.8406944e-07 9.6369517e-01 3.0920593e-05 1.5026700e-03 3.4255747e-02
 1.7459651e-06 5.1339238e-04], sampled 0.8299715920568962
[2019-04-05 14:58:59,054] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123064935]
[2019-04-05 14:58:59,054] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.5, 35.66666666666667, 80.16666666666667, 392.6666666666667, 19.0, 23.86914727398107, 0.04623673087786754, 1.0, 1.0, 0.0]
[2019-04-05 14:58:59,054] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-05 14:58:59,055] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.9596888e-06 9.6703678e-01 7.5392367e-05 6.3829293e-04 2.8713325e-02
 4.3691530e-06 3.5268699e-03], sampled 0.32684356303805717
[2019-04-05 14:58:59,662] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123064935]
[2019-04-05 14:58:59,662] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.973239605666667, 62.21432840333333, 127.5251971133333, 687.1203658166667, 19.0, 22.26511736501341, -0.3424877506682254, 1.0, 1.0, 0.0]
[2019-04-05 14:58:59,662] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 14:58:59,663] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.6867382e-05 9.2121190e-01 5.1882223e-04 1.6523871e-03 6.1864965e-02
 3.5447152e-05 1.4649638e-02], sampled 0.3433851612435187
[2019-04-05 14:59:04,682] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6305.8581 141769181.4371 -1855.2350
[2019-04-05 14:59:18,962] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5980.1182 172122580.2833 -2275.8193
[2019-04-05 14:59:24,374] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 6025.6373 185015068.2793 -2228.9046
[2019-04-05 14:59:25,400] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1980000, evaluation results [1980000.0, 5980.118238930925, 172122580.28325343, -2275.8193317674827, 6305.858081308068, 141769181.4370578, -1855.2350189292943, 6025.637325833842, 185015068.27926117, -2228.9045685767837]
[2019-04-05 14:59:38,069] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8205106e-06 8.1402957e-01 8.5153653e-05 4.2296955e-03 1.8139637e-01
 1.9499088e-05 2.3692264e-04], sum to 1.0000
[2019-04-05 14:59:38,070] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0438
[2019-04-05 14:59:38,091] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-5.0, 65.0, 0.0, 0.0, 19.0, 19.55787803314465, -0.966254052714401, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3560400.0000, 
sim time next is 3561000.0000, 
raw observation next is [-5.166666666666667, 65.83333333333334, 0.0, 0.0, 19.0, 19.45497022101083, -0.987465822802129, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.31948291782086796, 0.6583333333333334, 0.0, 0.0, 0.08333333333333333, 0.12124751841756905, 0.17084472573262366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1266305], dtype=float32), 0.5414192]. 
=============================================
[2019-04-05 14:59:38,103] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.677475]
 [81.669655]
 [81.90238 ]
 [82.36854 ]
 [82.78915 ]], R is [[80.62268066]
 [80.81645203]
 [81.00828552]
 [81.19820404]
 [81.38622284]].
[2019-04-05 14:59:46,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.6164146e-06 7.9234403e-01 1.1355276e-02 1.4955567e-02 6.2104814e-02
 3.3611283e-04 1.1889565e-01], sum to 1.0000
[2019-04-05 14:59:46,953] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8020
[2019-04-05 14:59:46,973] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.55, 55.5, 0.0, 0.0, 20.0, 20.20509611787514, -0.8780724471972942, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.5], 
sim time this is 2529000.0000, 
sim time next is 2529600.0000, 
raw observation next is [-2.633333333333333, 55.0, 0.0, 0.0, 19.5, 20.2323288359155, -0.8497511375026151, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.38965835641735924, 0.55, 0.0, 0.0, 0.125, 0.18602740299295828, 0.21674962083246163, 0.0, 1.0, 0.0], 
reward next is 0.9286, 
noisyNet noise sample is [array([0.61810005], dtype=float32), -0.13712351]. 
=============================================
[2019-04-05 14:59:50,466] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4911242e-05 9.8711246e-01 3.5156052e-05 1.4370827e-04 4.5265676e-03
 4.0323448e-05 8.1269313e-03], sum to 1.0000
[2019-04-05 14:59:50,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9194
[2019-04-05 14:59:50,474] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [2.0, 48.0, 106.0, 782.0, 19.5, 22.66197271260176, -0.2991536164917943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3853800.0000, 
sim time next is 3854400.0000, 
raw observation next is [2.0, 48.0, 102.8333333333333, 771.1666666666667, 19.0, 22.7442738202471, -0.2843602283643689, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.3427777777777777, 0.8521178637200737, 0.08333333333333333, 0.3953561516872582, 0.405213257211877, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11690701], dtype=float32), 0.09112951]. 
=============================================
[2019-04-05 14:59:55,657] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6429494e-05 7.8074992e-01 9.5090311e-04 4.2473190e-03 1.7691040e-01
 1.8071482e-04 3.6924444e-02], sum to 1.0000
[2019-04-05 14:59:55,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0164
[2019-04-05 14:59:55,670] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.166666666666667, 45.66666666666666, 0.0, 0.0, 20.0, 19.58047072715558, -0.8930352725151834, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3964200.0000, 
sim time next is 3964800.0000, 
raw observation next is [-7.333333333333334, 46.33333333333334, 0.0, 0.0, 19.0, 19.52104159121904, -0.8959729583429493, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.2594644506001847, 0.46333333333333343, 0.0, 0.0, 0.08333333333333333, 0.12675346593491987, 0.2013423472190169, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1549287], dtype=float32), -1.9192512]. 
=============================================
[2019-04-05 14:59:55,850] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.7029971e-05 4.3357250e-01 4.1765883e-04 1.7724026e-02 5.1905346e-01
 3.0118632e-04 2.8884040e-02], sum to 1.0000
[2019-04-05 14:59:55,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5317
[2019-04-05 14:59:55,861] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.15, 72.0, 0.0, 0.0, 19.5, 19.36514886370568, -0.9465254730892209, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [20.0], 
sim time this is 4487400.0000, 
sim time next is 4488000.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 20.0, 19.24053881229216, -0.9518355586198995, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.9565217391304348, 0.4570637119113574, 0.72, 0.0, 0.0, 0.16666666666666666, 0.10337823435768012, 0.18272148046003348, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.8571, 
noisyNet noise sample is [array([-2.7290394], dtype=float32), -1.0894707]. 
=============================================
[2019-04-05 14:59:55,868] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.225204]
 [81.05206 ]
 [82.20557 ]
 [82.93388 ]
 [83.37157 ]], R is [[80.22085571]
 [80.34722137]
 [80.54374695]
 [80.73831177]
 [80.8595047 ]].
[2019-04-05 14:59:56,160] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-05 14:59:56,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 14:59:56,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run16
[2019-04-05 15:00:00,866] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5389993e-06 3.4849641e-01 1.5778233e-03 4.8750369e-03 6.3004482e-01
 1.0950844e-05 1.4993439e-02], sum to 1.0000
[2019-04-05 15:00:00,871] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7344
[2019-04-05 15:00:00,901] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-2.666666666666667, 63.33333333333333, 0.0, 0.0, 21.0, 20.60188766417478, -0.7647212245401684, 0.0, 1.0, 175584.3185333255], 
current ob forecast is [], 
actual action is [21.5], 
sim time this is 3008400.0000, 
sim time next is 3009000.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 21.5, 20.55949046155139, -0.7424053739890989, 0.0, 1.0, 199033.7440543688], 
processed observation next is [0.0, 0.8260869565217391, 0.3841181902123731, 0.6416666666666667, 0.0, 0.0, 0.2916666666666667, 0.21329087179594927, 0.2525315420036337, 0.0, 1.0, 0.9477797335922324], 
reward next is 0.6429, 
noisyNet noise sample is [array([-1.4779171], dtype=float32), 2.1203842]. 
=============================================
[2019-04-05 15:00:00,919] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[81.192535]
 [80.31322 ]
 [80.76066 ]
 [80.740944]
 [80.73903 ]], R is [[81.91654968]
 [81.8116684 ]
 [81.77926636]
 [81.81861115]
 [81.92900085]].
[2019-04-05 15:00:01,349] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-05 15:00:01,353] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-05 15:00:01,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 15:00:01,356] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-05 15:00:01,356] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-05 15:00:01,356] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 15:00:01,357] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-05 15:00:02,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run101
[2019-04-05 15:00:02,900] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run101
[2019-04-05 15:00:02,921] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/43/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run101
[2019-04-05 15:00:58,511] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([5.1914983e-05], dtype=float32), 0.123853475]
[2019-04-05 15:00:58,511] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.9883765431666667, 47.92954878333334, 0.0, 0.0, 19.0, 20.53240084799883, -0.8418945532061102, 0.0, 1.0, 0.0]
[2019-04-05 15:00:58,512] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-05 15:00:58,512] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.47628656e-04 8.78029823e-01 1.27872091e-03 3.31318821e-03
 9.82199982e-02 1.07296175e-04 1.89033635e-02], sampled 0.42003258147359623
[2019-04-05 15:01:15,779] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 6413.4877 141517324.0021 -1820.6209
[2019-04-05 15:01:26,879] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 5937.9616 171665930.2406 -2387.6252
[2019-04-05 15:01:34,326] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 5971.6220 186906825.8249 -2259.3533
[2019-04-05 15:01:35,352] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 2000000, evaluation results [2000000.0, 5937.961572104892, 171665930.24060822, -2387.6252338514046, 6413.4876781042185, 141517324.00214946, -1820.6209485105267, 5971.6220219775605, 186906825.8249205, -2259.3533337453787]
