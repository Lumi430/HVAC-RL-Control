Using TensorFlow backend.
[2019-04-08 15:01:03,635] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v3', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v2', eval_act_func='part4_v4', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=20000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=1e-05, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2000000, metric_func='part4_v2', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v2-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_heuri_v7', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=25.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=11, test_env=['Part4-Light-Pit-Test-v3', 'Part4-Light-Pit-Test-v4'], test_mode='Multiple', train_act_func='part4_v4', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-08 15:01:03,635] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-08 15:01:03.675921: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-08 15:01:22,529] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-08 15:01:22,529] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v2', 'Part4-Light-Pit-Test-v3', 'Part4-Light-Pit-Test-v4'] ...
[2019-04-08 15:01:22,561] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation worker starts!
[2019-04-08 15:01:22,570] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation worker starts!
[2019-04-08 15:01:22,578] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation worker starts!
[2019-04-08 15:01:22,578] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:22,578] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-08 15:01:22,659] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:22,660] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res2/Eplus-env-sub_run1
[2019-04-08 15:01:23,579] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:23,580] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-08 15:01:23,678] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:23,679] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res3/Eplus-env-sub_run1
[2019-04-08 15:01:24,581] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:24,582] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-08 15:01:24,675] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:24,676] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res4/Eplus-env-sub_run1
[2019-04-08 15:01:25,582] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:25,583] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-08 15:01:25,673] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:25,675] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res5/Eplus-env-sub_run1
[2019-04-08 15:01:26,584] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:26,585] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-08 15:01:26,708] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:26,709] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res6/Eplus-env-sub_run1
[2019-04-08 15:01:27,586] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:27,587] A3C_AGENT_WORKER-Thread-7 INFO:Local worker starts!
[2019-04-08 15:01:27,729] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:27,730] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res7/Eplus-env-sub_run1
[2019-04-08 15:01:28,588] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:28,591] A3C_AGENT_WORKER-Thread-8 INFO:Local worker starts!
[2019-04-08 15:01:28,698] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:28,700] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res8/Eplus-env-sub_run1
[2019-04-08 15:01:28,841] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-08 15:01:28,841] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:01:28,842] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:01:28,842] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:28,842] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:01:28,842] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:28,843] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:28,845] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run1
[2019-04-08 15:01:28,845] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run1
[2019-04-08 15:01:28,858] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run1
[2019-04-08 15:01:29,592] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:29,593] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-08 15:01:29,738] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:29,740] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res9/Eplus-env-sub_run1
[2019-04-08 15:01:30,594] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:30,595] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-08 15:01:30,757] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:30,759] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res10/Eplus-env-sub_run1
[2019-04-08 15:01:31,596] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:31,596] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-08 15:01:31,939] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:31,963] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res11/Eplus-env-sub_run1
[2019-04-08 15:01:32,597] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:32,598] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-08 15:01:32,769] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:32,771] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res12/Eplus-env-sub_run1
[2019-04-08 15:01:33,599] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:33,599] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-08 15:01:33,938] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:33,940] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res13/Eplus-env-sub_run1
[2019-04-08 15:01:34,600] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:34,601] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-08 15:01:34,853] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:34,855] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res14/Eplus-env-sub_run1
[2019-04-08 15:01:35,602] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:35,603] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-08 15:01:35,974] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:35,976] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res15/Eplus-env-sub_run1
[2019-04-08 15:01:36,610] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:36,611] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-08 15:01:36,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:36,968] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res16/Eplus-env-sub_run1
[2019-04-08 15:01:37,612] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-08 15:01:37,612] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-08 15:01:37,873] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:01:37,874] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res17/Eplus-env-sub_run1
[2019-04-08 15:01:43,478] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-08 15:01:43,479] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [0.3, 95.0, 0.0, 0.0, 19.0, 24.18771900560494, 0.170680719350085, 0.0, 1.0, 40.0, 0.0]
[2019-04-08 15:01:43,480] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:01:43,480] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [0.07474726 0.10131389 0.06512538 0.13360462 0.08434069 0.10906304
 0.07484332 0.10010678 0.07721093 0.05036206 0.12928209], sampled 0.950435106352596
[2019-04-08 15:01:58,334] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-08 15:01:58,334] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-2.081390984, 84.22980667, 0.0, 0.0, 19.0, 22.36345401042977, -0.3369414778130065, 0.0, 1.0, 45.0, 22725.93206369663]
[2019-04-08 15:01:58,334] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:01:58,335] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [0.06662358 0.09180076 0.06479476 0.14421521 0.09258124 0.12287879
 0.07201493 0.09522799 0.07675921 0.06080159 0.11230186], sampled 0.6974438741769936
[2019-04-08 15:02:04,984] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-08 15:02:04,985] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [0.06666666666666665, 62.33333333333333, 167.1666666666667, 190.6666666666667, 22.5, 26.73158449913658, 0.8132965745042697, 1.0, 1.0, 55.0, 65356.09468450407]
[2019-04-08 15:02:04,985] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:02:04,985] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [0.08736458 0.11887947 0.06849016 0.11259569 0.10198416 0.10150437
 0.07872256 0.08239698 0.08059479 0.05846559 0.10900162], sampled 0.5358467013525753
[2019-04-08 15:02:58,592] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-08 15:02:58,592] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-6.0, 77.0, 0.0, 0.0, 19.0, 23.74410902109686, -0.02124561176557327, 0.0, 1.0, 30.0, 0.0]
[2019-04-08 15:02:58,592] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:02:58,593] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [0.07040395 0.09373297 0.05978205 0.13669205 0.07697675 0.11771492
 0.06619494 0.11547163 0.07850932 0.05614713 0.12837434], sampled 0.5293065430350886
[2019-04-08 15:03:12,930] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-08 15:03:12,930] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [18.38333333333333, 27.16666666666666, 107.0, 775.0, 19.0, 28.55210214291732, 1.241655219760327, 0.0, 1.0, 20.0, 0.0]
[2019-04-08 15:03:12,930] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:03:12,932] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [0.05017187 0.10441223 0.07293067 0.15700263 0.09062533 0.08977742
 0.0708652  0.11156114 0.06686183 0.05330367 0.13248795], sampled 0.5558874380257517
[2019-04-08 15:03:23,438] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 7086.8822 290016247.7670 2295.0123
[2019-04-08 15:03:23,460] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:03:23,600] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:03:33,335] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 7012.3466 310278185.9934 1760.2930
[2019-04-08 15:03:33,356] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:03:33,465] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:03:33,726] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6968.8728 318695203.4064 1323.4297
[2019-04-08 15:03:33,748] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:03:33,883] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:03:34,750] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7012.346586724625, 310278185.99340683, 1760.292974950194, 7086.882245345868, 290016247.76695275, 2295.012314106515, 6968.872780727162, 318695203.4063956, 1323.429694998077]
[2019-04-08 15:03:34,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.08466899 0.08483482 0.0732435  0.12529723 0.08382526 0.10320731
 0.08874505 0.08989803 0.09680073 0.08326221 0.08621678], sum to 1.0000
[2019-04-08 15:03:34,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2373
[2019-04-08 15:03:35,484] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.6, 95.5, 0.0, 0.0, 19.0, 20.95334111554477, -0.5223106410276556, 0.0, 1.0, 35.0, 0.0], 
current ob forecast is [], 
actual action is [8.6, 25.0], 
sim time this is 1800.0000, 
sim time next is 2400.0000, 
raw observation next is [4.8, 95.66666666666667, 0.0, 0.0, 19.0, 21.02413205842829, -0.5134961119217105, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.0, 0.5955678670360112, 0.9566666666666667, 0.0, 0.0, 0.08333333333333333, 0.2520110048690241, 0.3288346293594298, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3584274], dtype=float32), 0.923967]. 
=============================================
[2019-04-08 15:03:37,784] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.08149678 0.10384082 0.05790149 0.13749354 0.09256679 0.08927394
 0.06071466 0.10472129 0.11007851 0.06887167 0.09304056], sum to 1.0000
[2019-04-08 15:03:37,788] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8763
[2019-04-08 15:03:37,905] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [7.533333333333333, 94.0, 0.0, 0.0, 19.0, 22.54272505677982, -0.2429408142592561, 0.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [12.533333333333333, 60.0], 
sim time this is 13200.0000, 
sim time next is 13800.0000, 
raw observation next is [7.616666666666667, 93.5, 0.0, 0.0, 19.0, 22.64502471962479, -0.2239575360912183, 0.0, 1.0, 60.0, 143724.1961659972], 
processed observation next is [0.0, 0.13043478260869565, 0.6735918744228995, 0.935, 0.0, 0.0, 0.08333333333333333, 0.38708539330206576, 0.42534748796959393, 0.0, 1.0, 0.9, 0.6844009341237961], 
reward next is 0.3156, 
noisyNet noise sample is [array([-0.48598272], dtype=float32), 0.82312745]. 
=============================================
[2019-04-08 15:03:38,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07128505 0.08853078 0.06572189 0.13817109 0.07891458 0.11588219
 0.07861318 0.09553249 0.0807046  0.06328514 0.12335904], sum to 1.0000
[2019-04-08 15:03:38,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3244
[2019-04-08 15:03:38,315] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 22.85607558052854, -0.1907615351463397, 0.0, 0.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [12.7, 30.0], 
sim time this is 19800.0000, 
sim time next is 20400.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 22.76936715723069, -0.2393462795854019, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.39744726310255746, 0.4202179068048661, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26618734], dtype=float32), 0.6205169]. 
=============================================
[2019-04-08 15:03:38,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.06846558 0.09492394 0.06588741 0.149359   0.08960886 0.10797808
 0.07723195 0.10555246 0.07334388 0.05432567 0.11332319], sum to 1.0000
[2019-04-08 15:03:38,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5023
[2019-04-08 15:03:38,475] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 21.81736057752585, -0.4045257868222853, 0.0, 0.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [12.7, 40.0], 
sim time this is 20400.0000, 
sim time next is 21000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 21.78446341318711, -0.4428916565786068, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.31537195109892596, 0.3523694478071311, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2175822], dtype=float32), 0.19978495]. 
=============================================
[2019-04-08 15:03:38,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[0.37840682]
 [0.14386845]
 [0.29124275]
 [0.35730776]
 [0.36783698]], R is [[1.17297626]
 [2.16124654]
 [3.05048418]
 [4.01997948]
 [4.97977972]].
[2019-04-08 15:03:38,623] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.07766391 0.09466305 0.06303854 0.14973556 0.07798821 0.11467987
 0.07585064 0.09608176 0.07712969 0.05928623 0.11388258], sum to 1.0000
[2019-04-08 15:03:38,628] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9881
[2019-04-08 15:03:38,745] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 23.10660853369298, -0.06304000449775302, 0.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 24000.0000, 
sim time next is 24600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 23.23678794384753, -0.03387147766415824, 0.0, 1.0, 65.0, 176318.0757056436], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.4363989953206276, 0.4887095074452806, 0.0, 1.0, 1.0, 0.8396098843125885], 
reward next is 0.1604, 
noisyNet noise sample is [array([0.7070813], dtype=float32), 0.33880633]. 
=============================================
[2019-04-08 15:03:39,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.09018371 0.10492303 0.05453564 0.13370167 0.08670724 0.10417776
 0.06477959 0.10343955 0.08675573 0.06237051 0.10842554], sum to 1.0000
[2019-04-08 15:03:39,306] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9263
[2019-04-08 15:03:39,328] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 49.0, 0.0, 19.0, 22.89367213470652, -0.1379479183657997, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 36000.0000, 
sim time next is 36600.0000, 
raw observation next is [7.7, 93.0, 52.66666666666667, 0.0, 19.0, 22.88061097538155, -0.1253307506125203, 0.0, 1.0, 65.0, 201323.4110708447], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.17555555555555558, 0.0, 0.08333333333333333, 0.40671758128179586, 0.4582230831291599, 0.0, 1.0, 1.0, 0.9586829098611653], 
reward next is 0.0413, 
noisyNet noise sample is [array([-0.02331386], dtype=float32), -0.6171853]. 
=============================================
[2019-04-08 15:03:39,580] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.07668588 0.08936329 0.06398758 0.12088614 0.07547373 0.10683673
 0.06808824 0.12247833 0.08795103 0.06408879 0.1241603 ], sum to 1.0000
[2019-04-08 15:03:39,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6179
[2019-04-08 15:03:39,593] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [7.7, 93.0, 67.5, 0.0, 19.0, 23.87171998731311, 0.06060422468689838, 0.0, 0.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [12.7, 50.0], 
sim time this is 39600.0000, 
sim time next is 40200.0000, 
raw observation next is [7.7, 93.0, 70.0, 0.0, 19.0, 23.90807372805035, -0.01380601009850253, 0.0, 1.0, 15.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.23333333333333334, 0.0, 0.08333333333333333, 0.4923394773375292, 0.4953979966338325, 0.0, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.379736], dtype=float32), -0.3396477]. 
=============================================
[2019-04-08 15:03:39,793] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.08046281 0.11323167 0.05700293 0.13056979 0.1034306  0.08645732
 0.07154229 0.10052207 0.08543341 0.06667782 0.1046694 ], sum to 1.0000
[2019-04-08 15:03:39,799] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4010
[2019-04-08 15:03:39,921] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [8.0, 89.5, 96.0, 0.0, 19.0, 23.54985520611257, -0.04279082871903609, 0.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [13.0, 35.0], 
sim time this is 45000.0000, 
sim time next is 45600.0000, 
raw observation next is [8.1, 88.33333333333333, 94.5, 0.0, 19.0, 23.40316026997503, -0.0231562219136441, 0.0, 1.0, 35.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6869806094182825, 0.8833333333333333, 0.315, 0.0, 0.08333333333333333, 0.4502633558312524, 0.49228125936211864, 0.0, 1.0, 0.4, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.955993], dtype=float32), 0.2002095]. 
=============================================
[2019-04-08 15:03:40,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.07729252 0.08893494 0.06102985 0.1499007  0.07718513 0.10398783
 0.06813335 0.09671005 0.08397379 0.05930147 0.13355033], sum to 1.0000
[2019-04-08 15:03:40,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2270
[2019-04-08 15:03:40,175] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 19.0, 23.5986218850987, 0.02673678077876221, 0.0, 1.0, 45.0, 0.0], 
current ob forecast is [], 
actual action is [12.533333333333333, 25.0], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [7.45, 86.0, 79.0, 0.0, 19.0, 23.595197132434, 0.03935888787914881, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6689750692520776, 0.86, 0.2633333333333333, 0.0, 0.08333333333333333, 0.4662664277028332, 0.5131196292930497, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82948565], dtype=float32), 1.6470833]. 
=============================================
[2019-04-08 15:03:40,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.07576903 0.08356219 0.06585577 0.13752794 0.06387791 0.10868537
 0.08843744 0.09466209 0.07629597 0.05236593 0.15296035], sum to 1.0000
[2019-04-08 15:03:40,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3968
[2019-04-08 15:03:40,409] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [7.616666666666667, 86.0, 81.66666666666666, 0.0, 19.0, 23.7918377372623, 0.07291922833688319, 0.0, 1.0, 45.0, 0.0], 
current ob forecast is [], 
actual action is [12.616666666666667, 35.0], 
sim time this is 51000.0000, 
sim time next is 51600.0000, 
raw observation next is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 19.0, 23.80868804200026, 0.06773251257211944, 0.0, 1.0, 35.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6712834718374886, 0.86, 0.26777777777777784, 0.0, 0.08333333333333333, 0.48405733683335495, 0.5225775041907065, 0.0, 1.0, 0.4, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6716661], dtype=float32), 1.160938]. 
=============================================
[2019-04-08 15:03:40,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.08205576 0.09671833 0.06870174 0.14672343 0.0671901  0.12279377
 0.08113518 0.09013074 0.06774568 0.05058063 0.1262247 ], sum to 1.0000
[2019-04-08 15:03:40,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2496
[2019-04-08 15:03:40,470] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.699999999999999, 82.66666666666667, 39.33333333333334, 0.0, 19.0, 23.31034902630467, -0.02452742385319974, 0.0, 1.0, 55.0, 200001.4617611521], 
current ob forecast is [], 
actual action is [11.7, 30.0], 
sim time this is 57000.0000, 
sim time next is 57600.0000, 
raw observation next is [6.6, 82.0, 34.0, 0.0, 19.0, 23.31015624262274, -0.01727957454875083, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6454293628808865, 0.82, 0.11333333333333333, 0.0, 0.08333333333333333, 0.4425130202185616, 0.49424014181708303, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.4414148], dtype=float32), 0.023610214]. 
=============================================
[2019-04-08 15:03:40,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.07699466 0.08784164 0.0571061  0.15164636 0.07976846 0.11203499
 0.07207623 0.0953109  0.08641798 0.06224166 0.11856105], sum to 1.0000
[2019-04-08 15:03:40,707] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3535
[2019-04-08 15:03:40,860] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.6, 82.0, 34.0, 0.0, 19.0, 24.35242428305843, 0.2068332400353868, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [11.6, 30.0], 
sim time this is 57600.0000, 
sim time next is 58200.0000, 
raw observation next is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 19.0, 24.38808773180619, 0.2000579289757505, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6403508771929826, 0.8266666666666667, 0.09555555555555553, 0.0, 0.08333333333333333, 0.5323406443171826, 0.5666859763252502, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8136315], dtype=float32), 0.0024669145]. 
=============================================
[2019-04-08 15:03:40,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.09175467 0.09841248 0.06287836 0.12765978 0.07737675 0.10250087
 0.06698661 0.11279385 0.06782804 0.04916095 0.14264764], sum to 1.0000
[2019-04-08 15:03:40,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1167
[2019-04-08 15:03:41,051] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [7.1, 85.33333333333334, 59.66666666666666, 0.0, 19.0, 24.31956992277135, 0.1835750884137952, 0.0, 1.0, 35.0, 0.0], 
current ob forecast is [], 
actual action is [12.1, 20.0], 
sim time this is 54600.0000, 
sim time next is 55200.0000, 
raw observation next is [7.0, 84.66666666666667, 54.83333333333334, 0.0, 19.0, 24.29306120145846, 0.1732440298150686, 0.0, 1.0, 20.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6565096952908588, 0.8466666666666667, 0.18277777777777782, 0.0, 0.08333333333333333, 0.5244217667882051, 0.5577480099383562, 0.0, 1.0, 0.1, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0953579], dtype=float32), 1.3739837]. 
=============================================
[2019-04-08 15:03:41,297] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07845776 0.0909301  0.06635584 0.14665915 0.07830613 0.10146079
 0.07779582 0.09548439 0.08495598 0.05556943 0.12402463], sum to 1.0000
[2019-04-08 15:03:41,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7128
[2019-04-08 15:03:41,315] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 7, 
current raw observation is [4.4, 89.0, 0.0, 0.0, 19.0, 24.69566464904521, 0.2755018978467613, 0.0, 1.0, 65.0, 114933.3442657182], 
current ob forecast is [], 
actual action is [9.4, 50.0], 
sim time this is 64800.0000, 
sim time next is 65400.0000, 
raw observation next is [4.300000000000001, 88.5, 0.0, 0.0, 19.0, 24.72905116452912, 0.280980051981657, 0.0, 1.0, 50.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.5817174515235458, 0.885, 0.0, 0.0, 0.08333333333333333, 0.5607542637107601, 0.593660017327219, 0.0, 1.0, 0.7, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2872638], dtype=float32), -0.66327745]. 
=============================================
[2019-04-08 15:03:41,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.07331683 0.08627002 0.0719013  0.1247419  0.08006634 0.09239137
 0.08042214 0.12329359 0.08683671 0.05969607 0.12106375], sum to 1.0000
[2019-04-08 15:03:41,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8311
[2019-04-08 15:03:41,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.07504541 0.0972624  0.06374688 0.14141752 0.08743495 0.10923015
 0.07457839 0.09185436 0.08694492 0.0560318  0.11645321], sum to 1.0000
[2019-04-08 15:03:41,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4525
[2019-04-08 15:03:41,420] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [3.25, 87.5, 0.0, 0.0, 19.0, 23.64039733477123, 0.05137112162902143, 0.0, 1.0, 55.0, 43372.33911935521], 
current ob forecast is [], 
actual action is [8.25, 60.0], 
sim time this is 70200.0000, 
sim time next is 70800.0000, 
raw observation next is [3.066666666666667, 88.0, 0.0, 0.0, 19.0, 23.66404763213829, 0.06974846046084017, 0.0, 1.0, 60.0, 125375.1553204901], 
processed observation next is [0.0, 0.8260869565217391, 0.5475530932594646, 0.88, 0.0, 0.0, 0.08333333333333333, 0.4720039693448574, 0.52324948682028, 0.0, 1.0, 0.9, 0.597024549145191], 
reward next is 0.4030, 
noisyNet noise sample is [array([0.01990612], dtype=float32), 0.9392853]. 
=============================================
[2019-04-08 15:03:41,534] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [4.0, 87.0, 0.0, 0.0, 19.0, 24.26110258079592, 0.2009112084832168, 0.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [9.0, 35.0], 
sim time this is 67200.0000, 
sim time next is 67800.0000, 
raw observation next is [3.9, 86.5, 0.0, 0.0, 19.0, 24.26842661924764, 0.191637550273523, 0.0, 1.0, 35.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.5706371191135734, 0.865, 0.0, 0.0, 0.08333333333333333, 0.5223688849373035, 0.5638791834245077, 0.0, 1.0, 0.4, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5081283], dtype=float32), 0.48216593]. 
=============================================
[2019-04-08 15:03:41,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.07611686 0.09106678 0.0623005  0.13338274 0.0769854  0.12371793
 0.07795514 0.0902482  0.07969529 0.05919678 0.12933433], sum to 1.0000
[2019-04-08 15:03:41,988] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1170
[2019-04-08 15:03:42,000] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [0.4, 95.5, 0.0, 0.0, 19.0, 23.36501628952963, -0.01488032919350701, 0.0, 1.0, 50.0, 18768.36505675166], 
current ob forecast is [], 
actual action is [5.4, 20.0], 
sim time this is 81000.0000, 
sim time next is 81600.0000, 
raw observation next is [0.3666666666666667, 95.33333333333333, 0.0, 0.0, 19.0, 23.37259716218098, -0.01957695764899331, 0.0, 1.0, 20.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.4727608494921515, 0.9533333333333333, 0.0, 0.0, 0.08333333333333333, 0.4477164301817484, 0.4934743474503356, 0.0, 1.0, 0.1, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17998657], dtype=float32), 0.60885423]. 
=============================================
[2019-04-08 15:03:42,398] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.07350002 0.09094692 0.0626398  0.1604109  0.08078337 0.11802835
 0.08421425 0.0882933  0.07460861 0.04925094 0.11732363], sum to 1.0000
[2019-04-08 15:03:42,411] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0217
[2019-04-08 15:03:42,432] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 23.84842097092983, 0.1042429369291718, 0.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [-4.5, 15.0], 
sim time this is 79200.0000, 
sim time next is 79800.0000, 
raw observation next is [0.4666666666666667, 95.83333333333333, 0.0, 0.0, 19.0, 23.84807632879414, 0.09341310769488324, 0.0, 0.0, 15.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.4755309325946445, 0.9583333333333333, 0.0, 0.0, 0.08333333333333333, 0.4873396940661783, 0.5311377025649611, 0.0, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7935746], dtype=float32), 1.504188]. 
=============================================
[2019-04-08 15:03:42,563] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.06644644 0.08013394 0.06245173 0.16872336 0.07912066 0.13357419
 0.08022002 0.09029808 0.06477031 0.04733833 0.12692297], sum to 1.0000
[2019-04-08 15:03:42,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6511
[2019-04-08 15:03:42,585] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.04999999999999999, 95.0, 0.0, 0.0, 19.0, 24.10794817147469, 0.142973222215059, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.05, 45.0], 
sim time this is 85800.0000, 
sim time next is 86400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 24.08085545533045, 0.1322232875347807, 0.0, 1.0, 45.0, 0.0], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.5067379546108709, 0.5440744291782602, 0.0, 1.0, 0.6, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04421188], dtype=float32), 0.8725539]. 
=============================================
[2019-04-08 15:03:42,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.09162586 0.10033686 0.06833452 0.14930958 0.07930864 0.11753671
 0.07343121 0.08593807 0.07308593 0.04782377 0.11326887], sum to 1.0000
[2019-04-08 15:03:42,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0433
[2019-04-08 15:03:42,859] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-0.5, 91.66666666666667, 0.0, 0.0, 19.0, 24.00728989450269, 0.1320050259757481, 0.0, 1.0, 45.0, 0.0], 
current ob forecast is [], 
actual action is [-5.5, 15.0], 
sim time this is 89400.0000, 
sim time next is 90000.0000, 
raw observation next is [-0.6, 91.0, 0.0, 0.0, 19.0, 24.09338968860268, 0.125253183829404, 0.0, 0.0, 15.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.91, 0.0, 0.0, 0.08333333333333333, 0.5077824740502234, 0.541751061276468, 0.0, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9879686], dtype=float32), -0.8796645]. 
=============================================
[2019-04-08 15:03:42,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[0.40387663]
 [0.4638236 ]
 [0.41374126]
 [0.42999795]
 [0.35830984]], R is [[1.57501113]
 [2.55926108]
 [3.42175341]
 [3.42280579]
 [3.43010998]].
[2019-04-08 15:03:42,997] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [0.09578214 0.06901082 0.08295482 0.18232702 0.05777526 0.10103688
 0.09178341 0.07863595 0.06462745 0.05296965 0.12309659], sum to 1.0000
[2019-04-08 15:03:42,999] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2006
[2019-04-08 15:03:43,115] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.066666666666667, 89.66666666666667, 0.0, 0.0, 19.0, 24.35819813272888, 0.09888999833280461, 0.0, 0.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [2.933333333333333, 25.0], 
sim time this is 94800.0000, 
sim time next is 95400.0000, 
raw observation next is [-2.25, 89.0, 0.0, 0.0, 19.0, 24.18572847096824, 0.005186341786090028, 0.0, 1.0, 14.99999999999999, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40027700831024937, 0.89, 0.0, 0.0, 0.08333333333333333, 0.5154773725806866, 0.5017287805953633, 0.0, 1.0, -2.1316282072803005e-16, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6492936], dtype=float32), -1.3357836]. 
=============================================
[2019-04-08 15:03:43,148] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.09076099 0.0791189  0.05894082 0.14313804 0.07595726 0.11680437
 0.07533617 0.0922406  0.09332673 0.06160094 0.11277515], sum to 1.0000
[2019-04-08 15:03:43,148] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5398
[2019-04-08 15:03:43,164] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.666666666666667, 78.16666666666667, 0.0, 0.0, 19.0, 22.74091239394182, -0.1380386495905292, 0.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [1.333333333333333, 40.0], 
sim time this is 101400.0000, 
sim time next is 102000.0000, 
raw observation next is [-3.933333333333334, 77.33333333333334, 0.0, 0.0, 19.0, 22.87617332232967, -0.142838024664641, 0.0, 1.0, 40.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.3536472760849492, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.40634777686080586, 0.4523873251117863, 0.0, 1.0, 0.5, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2853556], dtype=float32), -1.9365147]. 
=============================================
[2019-04-08 15:03:43,165] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.08195195 0.08630411 0.05183107 0.1294042  0.08971445 0.10889719
 0.07476763 0.09993262 0.09151626 0.04830156 0.13737898], sum to 1.0000
[2019-04-08 15:03:43,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8357
[2019-04-08 15:03:43,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[0.35416886]
 [0.1546594 ]
 [0.28037176]
 [0.39933077]
 [0.46085498]], R is [[1.4368583 ]
 [2.42248964]
 [2.44260144]
 [3.20286226]
 [4.17083359]].
[2019-04-08 15:03:43,185] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 9, 
current raw observation is [-2.9, 85.66666666666667, 0.0, 0.0, 19.0, 23.89248677064682, 0.0634760885602285, 0.0, 1.0, 50.0, 0.0], 
current ob forecast is [], 
actual action is [2.1, 60.0], 
sim time this is 97800.0000, 
sim time next is 98400.0000, 
raw observation next is [-3.0, 84.33333333333334, 0.0, 0.0, 19.0, 23.93236246065295, 0.06431256495118969, 0.0, 1.0, 60.0, 101613.8621465846], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.4943635383877458, 0.5214375216503966, 0.0, 1.0, 0.9, 0.4838755340313552], 
reward next is 0.5161, 
noisyNet noise sample is [array([-0.36207107], dtype=float32), 0.10970296]. 
=============================================
[2019-04-08 15:03:43,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.10180913 0.07857136 0.07590888 0.12493385 0.07982207 0.10324425
 0.07879002 0.0924464  0.08877198 0.06765284 0.10804918], sum to 1.0000
[2019-04-08 15:03:43,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6209
[2019-04-08 15:03:43,335] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 19.0, 23.77560757163175, -0.04656346918193043, 0.0, 1.0, 15.00000000000001, 0.0], 
current ob forecast is [], 
actual action is [2.566666666666667, 45.0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 19.0, 23.45035635564298, -0.02439640758713563, 0.0, 1.0, 45.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.3901200369344414, 0.8766666666666666, 0.0, 0.0, 0.08333333333333333, 0.4541963629702484, 0.49186786413762146, 0.0, 1.0, 0.6, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4188433], dtype=float32), 1.8888142]. 
=============================================
[2019-04-08 15:03:43,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.07600163 0.06961808 0.08575481 0.14694594 0.0542255  0.12256949
 0.10954546 0.07170053 0.07903068 0.06440115 0.12020677], sum to 1.0000
[2019-04-08 15:03:43,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0268
[2019-04-08 15:03:43,491] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.416666666666667, 74.83333333333333, 0.0, 0.0, 19.0, 22.18337216651993, -0.2937050385947702, 0.0, 1.0, 50.0, 129385.7847248465], 
current ob forecast is [], 
actual action is [-11.416666666666668, 15.0], 
sim time this is 107400.0000, 
sim time next is 108000.0000, 
raw observation next is [-6.7, 75.0, 0.0, 0.0, 19.0, 22.1023595205323, -0.2987615775525682, 0.0, 0.0, 15.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.75, 0.0, 0.0, 0.08333333333333333, 0.34186329337769167, 0.40041280748247726, 0.0, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9690831], dtype=float32), 1.529224]. 
=============================================
[2019-04-08 15:03:43,498] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[0.5647676 ]
 [0.476698  ]
 [0.5316103 ]
 [0.51424295]
 [0.43638834]], R is [[1.53614974]
 [1.90466547]
 [2.88561869]
 [3.85676241]
 [4.81819487]].
[2019-04-08 15:03:44,574] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07060606 0.08288456 0.10014366 0.10815263 0.08082071 0.10762177
 0.11606623 0.08414653 0.07203575 0.06948231 0.10803983], sum to 1.0000
[2019-04-08 15:03:44,574] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4529
[2019-04-08 15:03:44,774] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-7.466666666666667, 65.66666666666667, 30.83333333333334, 7.5, 22.5, 22.08607512761043, -0.2365489205384241, 1.0, 1.0, 59.99999999999998, 200537.6685402755], 
current ob forecast is [], 
actual action is [-2.466666666666667, 40.0], 
sim time this is 116400.0000, 
sim time next is 117000.0000, 
raw observation next is [-7.55, 64.5, 37.0, 9.0, 22.5, 22.53220804564118, -0.1659416129559488, 1.0, 1.0, 39.99999999999999, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.25346260387811637, 0.645, 0.12333333333333334, 0.009944751381215469, 0.375, 0.3776840038034317, 0.44468612901468374, 1.0, 1.0, 0.49999999999999983, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37024084], dtype=float32), 0.57138145]. 
=============================================
[2019-04-08 15:03:44,787] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[0.38905442]
 [0.5899047 ]
 [0.5243286 ]
 [0.75455904]
 [0.7471741 ]], R is [[1.59297645]
 [1.62210548]
 [2.60588455]
 [2.63002777]
 [3.57401252]].
[2019-04-08 15:03:44,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.08127683 0.08597777 0.08168548 0.13600063 0.06626707 0.10761469
 0.09220164 0.07171012 0.08415242 0.06505331 0.12806004], sum to 1.0000
[2019-04-08 15:03:44,856] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1587
[2019-04-08 15:03:44,984] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 22.5, 23.48294337411012, -0.04256846580945885, 1.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [-2.8, 20.0], 
sim time this is 120000.0000, 
sim time next is 120600.0000, 
raw observation next is [-7.8, 67.5, 45.0, 0.0, 22.5, 23.79280060096055, -0.02075263185352358, 1.0, 1.0, 20.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.675, 0.15, 0.0, 0.375, 0.4827333834133792, 0.49308245604882545, 1.0, 1.0, 0.1, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7738597], dtype=float32), 0.87259245]. 
=============================================
[2019-04-08 15:03:45,008] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.07087735 0.09707111 0.06899516 0.13439947 0.09842149 0.10958286
 0.07173815 0.07242791 0.0745792  0.05134745 0.1505599 ], sum to 1.0000
[2019-04-08 15:03:45,011] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4694
[2019-04-08 15:03:45,123] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-7.8, 84.0, 188.0, 28.33333333333334, 22.5, 24.86592271516882, 0.2406608941357103, 1.0, 1.0, 35.0, 0.0], 
current ob forecast is [], 
actual action is [-2.8, 35.0], 
sim time this is 125400.0000, 
sim time next is 126000.0000, 
raw observation next is [-7.8, 86.0, 187.0, 24.5, 22.5, 24.86761716343786, 0.2356155073995104, 1.0, 1.0, 35.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.24653739612188366, 0.86, 0.6233333333333333, 0.02707182320441989, 0.375, 0.5723014302864883, 0.5785385024665034, 1.0, 1.0, 0.4, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12238237], dtype=float32), -1.2524828]. 
=============================================
[2019-04-08 15:03:45,132] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[0.75472087]
 [0.98129004]
 [0.92753565]
 [1.0087663 ]
 [0.9700982 ]], R is [[1.96061945]
 [2.94101334]
 [3.91160321]
 [4.22143316]
 [5.17921877]].
[2019-04-08 15:03:45,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.06808211 0.10030896 0.09314877 0.12613784 0.08144658 0.11281072
 0.0765488  0.08361994 0.05791882 0.04087346 0.15910393], sum to 1.0000
[2019-04-08 15:03:45,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0453
[2019-04-08 15:03:45,517] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-8.2, 69.33333333333334, 175.0, 111.3333333333333, 22.5, 23.55109220811547, -0.04503845852110864, 1.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [-13.2, 15.0], 
sim time this is 128400.0000, 
sim time next is 129000.0000, 
raw observation next is [-8.3, 65.16666666666667, 166.0, 209.6666666666666, 22.5, 23.56517083582738, -0.03795166523655582, 1.0, 0.0, 15.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.23268698060941828, 0.6516666666666667, 0.5533333333333333, 0.23167587476979734, 0.375, 0.4637642363189484, 0.487349444921148, 1.0, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7737507], dtype=float32), 0.029102707]. 
=============================================
[2019-04-08 15:03:45,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[0.96599764]
 [1.076944  ]
 [1.1012801 ]
 [1.0524808 ]
 [1.0065492 ]], R is [[1.88950205]
 [2.8706069 ]
 [3.84190083]
 [4.80348206]
 [5.75544739]].
[2019-04-08 15:03:46,075] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06822376 0.09163712 0.0936057  0.15368024 0.08793237 0.0892195
 0.10129166 0.06968196 0.07143731 0.05662047 0.11666988], sum to 1.0000
[2019-04-08 15:03:46,077] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2724
[2019-04-08 15:03:46,088] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.800000000000001, 65.16666666666667, 38.33333333333333, 17.0, 22.5, 24.65992985503721, 0.2708166101026178, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-1.8000000000000007, 30.0], 
sim time this is 144600.0000, 
sim time next is 145200.0000, 
raw observation next is [-6.9, 66.33333333333334, 32.66666666666666, 9.999999999999998, 22.5, 24.88345248356561, 0.2824334532635772, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.27146814404432135, 0.6633333333333334, 0.10888888888888885, 0.011049723756906075, 0.375, 0.5736210402971341, 0.5941444844211924, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10789394], dtype=float32), -0.28454924]. 
=============================================
[2019-04-08 15:03:47,136] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.06765227 0.08821302 0.09759384 0.16295403 0.07587191 0.10621507
 0.09969814 0.06951157 0.0642718  0.03692298 0.13109529], sum to 1.0000
[2019-04-08 15:03:47,138] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6699
[2019-04-08 15:03:47,162] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 70.5, 0.0, 0.0, 19.0, 22.52846500563593, -0.1660849717542282, 0.0, 1.0, 50.0, 18765.97866311782], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 45.0], 
sim time this is 165000.0000, 
sim time next is 165600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 22.51835784005997, -0.1747328207237758, 0.0, 1.0, 45.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.3765298200049975, 0.4417557264254081, 0.0, 1.0, 0.6, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8826543], dtype=float32), -0.78496933]. 
=============================================
[2019-04-08 15:03:47,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.06429867 0.08905951 0.10209753 0.15786551 0.0789577  0.08244599
 0.08212081 0.07361536 0.06903955 0.04854589 0.15195353], sum to 1.0000
[2019-04-08 15:03:47,747] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2602
[2019-04-08 15:03:47,840] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 22.5, 23.65661373931928, 0.0439486872287578, 0.0, 1.0, 55.0, 48523.36886039517], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 30.0], 
sim time this is 158400.0000, 
sim time next is 159000.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 19.0, 23.64622525989336, 0.03248612231532191, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.08333333333333333, 0.47051877165777994, 0.5108287074384407, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19830085], dtype=float32), -0.70263135]. 
=============================================
[2019-04-08 15:03:47,855] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[1.4722234]
 [1.5558314]
 [1.4600142]
 [1.4735539]
 [1.7143786]], R is [[2.77014089]
 [3.5113759 ]
 [4.47626209]
 [4.46506548]
 [5.42041492]].
[2019-04-08 15:03:49,502] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.07518306 0.06056362 0.09321266 0.2551655  0.05898434 0.08134849
 0.09705607 0.06835753 0.06202104 0.05073096 0.09737685], sum to 1.0000
[2019-04-08 15:03:49,502] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9878
[2019-04-08 15:03:49,529] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 20.64246471311842, -0.6848509142414775, 0.0, 1.0, 65.0, 199529.079830871], 
current ob forecast is [], 
actual action is [-3.900000000000002, 65.0], 
sim time this is 192000.0000, 
sim time next is 192600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 20.60680739858872, -0.6653209170166016, 0.0, 1.0, 65.0, 200825.0261208193], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.21723394988239342, 0.2782263609944661, 0.0, 1.0, 1.0, 0.9563096481943776], 
reward next is 0.0437, 
noisyNet noise sample is [array([-0.5339935], dtype=float32), -0.48477876]. 
=============================================
[2019-04-08 15:03:50,140] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.07100105 0.0730485  0.09979815 0.16176637 0.07123354 0.09046014
 0.11258621 0.06975536 0.07090449 0.06416843 0.11527774], sum to 1.0000
[2019-04-08 15:03:50,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2656
[2019-04-08 15:03:50,226] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 22.5, 21.19756695621518, -0.5395827824636434, 0.0, 1.0, 29.99999999999997, 0.0], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 30.0], 
sim time this is 199800.0000, 
sim time next is 200400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 22.5, 21.40780897904602, -0.5331817709378223, 1.0, 1.0, 29.99999999999999, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.375, 0.28398408158716837, 0.3222727430207259, 1.0, 1.0, 0.29999999999999977, 0.0], 
reward next is 0.9430, 
noisyNet noise sample is [array([-0.03375587], dtype=float32), -0.9382316]. 
=============================================
[2019-04-08 15:03:51,739] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.0510711  0.14106593 0.14425546 0.1476898  0.07787912 0.07199334
 0.07997333 0.05551429 0.04594789 0.0277254  0.15688436], sum to 1.0000
[2019-04-08 15:03:51,740] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6854
[2019-04-08 15:03:51,774] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 60.5, 56.0, 0.0, 22.5, 24.00408183077369, -0.03215192869866994, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.9, 45.0], 
sim time this is 228600.0000, 
sim time next is 229200.0000, 
raw observation next is [-3.2, 61.0, 49.66666666666667, 0.0, 22.5, 24.03623051078918, -0.0270994815433115, 1.0, 1.0, 45.0, 47304.75014980906], 
processed observation next is [1.0, 0.6521739130434783, 0.37396121883656513, 0.61, 0.16555555555555557, 0.0, 0.375, 0.5030192092324318, 0.49096683948556286, 1.0, 1.0, 0.6, 0.22526071499909076], 
reward next is 0.7747, 
noisyNet noise sample is [array([0.36497003], dtype=float32), 0.75516236]. 
=============================================
[2019-04-08 15:03:51,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04008222 0.11960977 0.14552641 0.16111293 0.06930546 0.08089296
 0.09166871 0.06550138 0.04703559 0.02963564 0.1496289 ], sum to 1.0000
[2019-04-08 15:03:51,946] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2182
[2019-04-08 15:03:51,959] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 59.0, 86.5, 0.0, 22.5, 24.58537799964668, 0.08574449111584341, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.2, 25.0], 
sim time this is 226800.0000, 
sim time next is 227400.0000, 
raw observation next is [-2.9, 59.5, 76.33333333333333, 0.0, 22.5, 24.65602776175801, 0.0920908032544326, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38227146814404434, 0.595, 0.2544444444444444, 0.0, 0.375, 0.5546689801465009, 0.5306969344181441, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24864985], dtype=float32), -0.3606881]. 
=============================================
[2019-04-08 15:03:52,507] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.04510302 0.12238991 0.13758883 0.1341116  0.07783169 0.07129472
 0.11178022 0.05668359 0.05035198 0.02809316 0.16477132], sum to 1.0000
[2019-04-08 15:03:52,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5828
[2019-04-08 15:03:52,523] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 62.0, 37.0, 0.0, 22.5, 23.55789902723684, -0.1690022096825556, 1.0, 1.0, 15.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 40.0], 
sim time this is 230400.0000, 
sim time next is 231000.0000, 
raw observation next is [-3.4, 62.5, 30.66666666666666, 0.0, 22.5, 23.48212811573825, -0.1504736520044763, 1.0, 1.0, 40.0, 99681.26169347568], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.625, 0.1022222222222222, 0.0, 0.375, 0.45684400964485405, 0.44984211599850793, 1.0, 1.0, 0.5, 0.4746726747308366], 
reward next is 0.5253, 
noisyNet noise sample is [array([-0.19220881], dtype=float32), 0.5477695]. 
=============================================
[2019-04-08 15:03:52,554] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[2.5432756]
 [2.5900571]
 [2.637641 ]
 [2.7028   ]
 [2.7252922]], R is [[3.16658592]
 [4.13492012]
 [5.09357119]
 [6.04263544]
 [6.98220921]].
[2019-04-08 15:03:52,562] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.04359132 0.08697364 0.13769013 0.2353572  0.06320148 0.07658496
 0.10320356 0.05323795 0.05008624 0.02858043 0.12149309], sum to 1.0000
[2019-04-08 15:03:52,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3001
[2019-04-08 15:03:52,575] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 77.33333333333334, 0.0, 0.0, 19.0, 21.89561815958775, -0.3855303705291855, 0.0, 1.0, 45.0, 32884.57976806565], 
current ob forecast is [], 
actual action is [1.1, 30.0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [-3.9, 78.5, 0.0, 0.0, 19.0, 21.76110090169017, -0.3959968564861012, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.785, 0.0, 0.0, 0.08333333333333333, 0.31342507514084755, 0.3680010478379663, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19321893], dtype=float32), -0.05096473]. 
=============================================
[2019-04-08 15:03:52,964] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.04255137 0.12264694 0.17463064 0.16158497 0.07415595 0.07483149
 0.10116561 0.04081484 0.03753942 0.01894209 0.15113674], sum to 1.0000
[2019-04-08 15:03:52,967] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3472
[2019-04-08 15:03:53,026] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 65.0, 139.0, 0.0, 22.5, 24.84130131721584, 0.1408598749340571, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 219600.0000, 
sim time next is 220200.0000, 
raw observation next is [-4.316666666666666, 64.5, 142.3333333333333, 0.0, 22.5, 24.85554783979411, 0.1355796219123505, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.34302862419205915, 0.645, 0.4744444444444443, 0.0, 0.375, 0.5712956533161758, 0.5451932073041168, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30338115], dtype=float32), 0.75336534]. 
=============================================
[2019-04-08 15:03:53,163] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03861244 0.10055552 0.14825861 0.241292   0.08009357 0.05102862
 0.12627533 0.04156332 0.03913761 0.02939293 0.10379004], sum to 1.0000
[2019-04-08 15:03:53,166] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4225
[2019-04-08 15:03:53,218] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 23.15555801064934, -0.1723606164465444, 1.0, 1.0, 50.0, 126425.2684135157], 
current ob forecast is [], 
actual action is [1.6, 30.0], 
sim time this is 240000.0000, 
sim time next is 240600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 22.9626540661644, -0.1783674775730683, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.41355450551370004, 0.44054417414231056, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8009236], dtype=float32), -0.492027]. 
=============================================
[2019-04-08 15:03:53,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.04250578 0.09098278 0.15290156 0.1751597  0.0843394  0.07902324
 0.11449547 0.05820492 0.04280718 0.02426731 0.13531262], sum to 1.0000
[2019-04-08 15:03:53,289] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6704
[2019-04-08 15:03:53,303] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 23.89445059449026, 0.01676630017210628, 1.0, 1.0, 45.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 45.0], 
sim time this is 237600.0000, 
sim time next is 238200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 24.10014466497606, 0.037065377913262, 1.0, 1.0, 45.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.5083453887480051, 0.5123551259710873, 1.0, 1.0, 0.6, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88251793], dtype=float32), -0.69759035]. 
=============================================
[2019-04-08 15:03:53,977] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.04092296 0.11363295 0.1442176  0.19986062 0.06898472 0.06448971
 0.114439   0.03306594 0.03274407 0.01525643 0.17238604], sum to 1.0000
[2019-04-08 15:03:53,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5016
[2019-04-08 15:03:54,003] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 19.0, 23.8626792152658, 0.08478601484894216, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 35.0], 
sim time this is 245400.0000, 
sim time next is 246000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 19.0, 23.92270634823613, 0.08169740550022712, 0.0, 1.0, 35.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.08333333333333333, 0.4935588623530108, 0.5272324685000757, 0.0, 1.0, 0.4, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15085487], dtype=float32), -0.7174917]. 
=============================================
[2019-04-08 15:03:54,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[3.3982358]
 [3.5904813]
 [3.3777442]
 [3.2141109]
 [3.3474221]], R is [[4.48519135]
 [5.44033957]
 [5.42532635]
 [6.37107325]
 [7.30736256]].
[2019-04-08 15:03:54,431] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0650637  0.05262472 0.10573111 0.20657644 0.07108049 0.09165695
 0.1576409  0.05417746 0.04984983 0.04434517 0.10125327], sum to 1.0000
[2019-04-08 15:03:54,431] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2711
[2019-04-08 15:03:54,567] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-11.8, 69.5, 0.0, 0.0, 19.0, 19.73396355055888, -0.8472101339914712, 0.0, 1.0, 60.0, 200127.1977393272], 
current ob forecast is [], 
actual action is [-6.800000000000001, 30.0], 
sim time this is 281400.0000, 
sim time next is 282000.0000, 
raw observation next is [-11.9, 69.0, 0.0, 0.0, 19.0, 19.71909163017884, -0.8406760529719058, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.13296398891966757, 0.69, 0.0, 0.0, 0.08333333333333333, 0.14325763584823678, 0.21977464900936472, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.84274817], dtype=float32), -0.086130045]. 
=============================================
[2019-04-08 15:03:54,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[2.3016856]
 [2.286493 ]
 [2.381735 ]
 [2.1604958]
 [2.2333004]], R is [[3.12793064]
 [3.1436646 ]
 [4.11222792]
 [5.07110548]
 [6.02039433]].
[2019-04-08 15:03:54,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.05093351 0.07657392 0.12684356 0.2809973  0.05969137 0.06547742
 0.12723072 0.04861266 0.04595635 0.02668003 0.09100311], sum to 1.0000
[2019-04-08 15:03:54,681] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9758
[2019-04-08 15:03:54,702] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.866666666666667, 77.00000000000001, 0.0, 0.0, 19.0, 22.66933326418535, -0.2197974468044479, 0.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [0.13333333333333286, 65.0], 
sim time this is 259800.0000, 
sim time next is 260400.0000, 
raw observation next is [-5.233333333333334, 75.0, 0.0, 0.0, 19.0, 22.59792270017186, -0.2126746448496793, 0.0, 1.0, 65.0, 200157.9585500728], 
processed observation next is [1.0, 0.0, 0.31763619575253926, 0.75, 0.0, 0.0, 0.08333333333333333, 0.38316022501432173, 0.4291084517167736, 0.0, 1.0, 1.0, 0.9531331359527276], 
reward next is 0.0469, 
noisyNet noise sample is [array([0.18047448], dtype=float32), -2.1541345]. 
=============================================
[2019-04-08 15:03:54,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.06101342 0.06740738 0.10774317 0.23219697 0.06776729 0.08170097
 0.11946958 0.05971998 0.05112275 0.04086146 0.11099703], sum to 1.0000
[2019-04-08 15:03:54,746] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0999
[2019-04-08 15:03:54,788] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-10.6, 67.0, 0.0, 0.0, 19.0, 20.27934704818026, -0.7386776469706295, 0.0, 1.0, 65.0, 201186.0740721174], 
current ob forecast is [], 
actual action is [-5.6, 20.0], 
sim time this is 277200.0000, 
sim time next is 277800.0000, 
raw observation next is [-10.78333333333333, 67.5, 0.0, 0.0, 19.0, 20.21979911161659, -0.7080801413004781, 0.0, 1.0, 20.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.1638965835641737, 0.675, 0.0, 0.0, 0.08333333333333333, 0.18498325930138262, 0.26397328623317395, 0.0, 1.0, 0.1, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7235216], dtype=float32), 0.122258484]. 
=============================================
[2019-04-08 15:03:54,973] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.04533004 0.06989746 0.11756396 0.28104535 0.06479002 0.0685278
 0.11815666 0.05575816 0.04710445 0.04074929 0.09107676], sum to 1.0000
[2019-04-08 15:03:54,973] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8213
[2019-04-08 15:03:54,989] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-10.6, 67.0, 0.0, 0.0, 19.0, 21.86671409235224, -0.4215346443930411, 0.0, 1.0, 65.0, 201283.3539995964], 
current ob forecast is [], 
actual action is [-5.6, 55.0], 
sim time this is 277200.0000, 
sim time next is 277800.0000, 
raw observation next is [-10.78333333333333, 67.5, 0.0, 0.0, 19.0, 21.79575476176214, -0.3934269837090819, 0.0, 1.0, 55.0, 111139.7853661305], 
processed observation next is [1.0, 0.21739130434782608, 0.1638965835641737, 0.675, 0.0, 0.0, 0.08333333333333333, 0.31631289681351166, 0.3688576720969727, 0.0, 1.0, 0.8, 0.52923707317205], 
reward next is 0.4708, 
noisyNet noise sample is [array([-0.55359393], dtype=float32), 1.2379309]. 
=============================================
[2019-04-08 15:03:55,285] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.05951441 0.06164453 0.12347683 0.19542257 0.07092085 0.08169594
 0.14374477 0.06103512 0.05350047 0.05414253 0.094902  ], sum to 1.0000
[2019-04-08 15:03:55,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6676
[2019-04-08 15:03:55,308] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.2, 67.5, 0.0, 0.0, 19.0, 19.7430080159841, -0.8488218292799886, 0.0, 1.0, 45.0, 47563.56417442593], 
current ob forecast is [], 
actual action is [-7.199999999999999, 45.0], 
sim time this is 283800.0000, 
sim time next is 284400.0000, 
raw observation next is [-12.3, 67.0, 0.0, 0.0, 19.0, 19.82587161111588, -0.8500234707755837, 0.0, 1.0, 45.0, 44324.28658787815], 
processed observation next is [1.0, 0.30434782608695654, 0.12188365650969527, 0.67, 0.0, 0.0, 0.08333333333333333, 0.1521559675929899, 0.21665884307480543, 0.0, 1.0, 0.6, 0.21106803137084834], 
reward next is 0.7889, 
noisyNet noise sample is [array([-0.63486856], dtype=float32), -0.69762874]. 
=============================================
[2019-04-08 15:03:55,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.06523664 0.05469678 0.08897737 0.24409007 0.07335161 0.08902532
 0.11974888 0.06537171 0.0457515  0.02769479 0.12605529], sum to 1.0000
[2019-04-08 15:03:55,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8198
[2019-04-08 15:03:55,449] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-9.4, 69.5, 0.0, 0.0, 19.0, 22.15470813405934, -0.3621863432830286, 0.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [-4.4, 55.0], 
sim time this is 273000.0000, 
sim time next is 273600.0000, 
raw observation next is [-9.5, 70.0, 0.0, 0.0, 19.0, 22.09786873328109, -0.3689723461911763, 0.0, 1.0, 55.0, 197904.1573837673], 
processed observation next is [1.0, 0.17391304347826086, 0.1994459833795014, 0.7, 0.0, 0.0, 0.08333333333333333, 0.3414890611067574, 0.37700921793627457, 0.0, 1.0, 0.8, 0.942400749446511], 
reward next is 0.0576, 
noisyNet noise sample is [array([0.965886], dtype=float32), 0.82848644]. 
=============================================
[2019-04-08 15:03:56,138] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7673: loss 35.0379
[2019-04-08 15:03:56,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7673: learning rate 0.0000
[2019-04-08 15:03:56,465] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7736: loss 33.3399
[2019-04-08 15:03:56,467] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7736: learning rate 0.0000
[2019-04-08 15:03:56,888] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 7840: loss 31.7182
[2019-04-08 15:03:56,888] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 7840: learning rate 0.0000
[2019-04-08 15:03:56,943] A3C_AGENT_WORKER-Thread-8 INFO:Local step 500, global step 7851: loss 25.5017
[2019-04-08 15:03:56,946] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 500, global step 7851: learning rate 0.0000
[2019-04-08 15:03:56,950] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7852: loss 31.1690
[2019-04-08 15:03:56,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7852: learning rate 0.0000
[2019-04-08 15:03:57,182] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7907: loss 24.7636
[2019-04-08 15:03:57,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7907: learning rate 0.0000
[2019-04-08 15:03:57,270] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7927: loss 10.9415
[2019-04-08 15:03:57,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7927: learning rate 0.0000
[2019-04-08 15:03:57,500] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7984: loss 17.2760
[2019-04-08 15:03:57,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7984: learning rate 0.0000
[2019-04-08 15:03:57,574] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8007: loss 35.4474
[2019-04-08 15:03:57,575] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8007: learning rate 0.0000
[2019-04-08 15:03:57,671] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8028: loss 42.8119
[2019-04-08 15:03:57,674] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8028: learning rate 0.0000
[2019-04-08 15:03:57,792] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8058: loss 38.8785
[2019-04-08 15:03:57,793] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8058: learning rate 0.0000
[2019-04-08 15:03:58,028] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 8113: loss 31.4245
[2019-04-08 15:03:58,028] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 8113: learning rate 0.0000
[2019-04-08 15:03:58,030] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8113: loss 13.9882
[2019-04-08 15:03:58,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8113: learning rate 0.0000
[2019-04-08 15:03:58,296] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 8177: loss 24.7429
[2019-04-08 15:03:58,296] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 8177: learning rate 0.0000
[2019-04-08 15:03:59,667] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8519: loss 30.3541
[2019-04-08 15:03:59,668] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8519: learning rate 0.0000
[2019-04-08 15:03:59,754] A3C_AGENT_WORKER-Thread-7 INFO:Local step 500, global step 8538: loss 12.7245
[2019-04-08 15:03:59,754] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 500, global step 8538: learning rate 0.0000
[2019-04-08 15:04:00,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0261418  0.06455187 0.20207064 0.26910588 0.06303233 0.03590654
 0.1653094  0.02463496 0.02153586 0.01062373 0.11708713], sum to 1.0000
[2019-04-08 15:04:00,349] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6283
[2019-04-08 15:04:00,402] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-11.7, 57.0, 0.0, 0.0, 22.5, 24.5626116374392, 0.1260156323091045, 1.0, 1.0, 55.0, 199700.131060408], 
current ob forecast is [], 
actual action is [-6.699999999999999, 40.0], 
sim time this is 324000.0000, 
sim time next is 324600.0000, 
raw observation next is [-11.8, 58.0, 0.0, 0.0, 22.5, 24.47624087455237, 0.1289369000018875, 1.0, 1.0, 40.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.13573407202216065, 0.58, 0.0, 0.0, 0.375, 0.5396867395460309, 0.5429789666672958, 1.0, 1.0, 0.5, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3164954], dtype=float32), -0.013801595]. 
=============================================
[2019-04-08 15:04:00,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.01432792 0.12933017 0.24583443 0.25573754 0.04483866 0.05545015
 0.10029163 0.01518719 0.0157066  0.00615108 0.1171447 ], sum to 1.0000
[2019-04-08 15:04:00,643] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9324
[2019-04-08 15:04:00,710] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 44.0, 92.83333333333334, 629.6666666666667, 22.5, 25.71032864907346, 0.4555005503745004, 1.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 308400.0000, 
sim time next is 309000.0000, 
raw observation next is [-9.5, 44.0, 90.66666666666667, 628.3333333333333, 22.5, 25.94993864045339, 0.3597066547407508, 1.0, 1.0, 65.0, 199694.5686427391], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.3022222222222222, 0.6942909760589318, 0.375, 0.6624948867044491, 0.6199022182469169, 1.0, 1.0, 1.0, 0.9509265173463766], 
reward next is 0.0491, 
noisyNet noise sample is [array([-0.69074875], dtype=float32), 0.12396448]. 
=============================================
[2019-04-08 15:04:00,723] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[5.2063904]
 [5.3480735]
 [5.423631 ]
 [5.4352837]
 [5.4535327]], R is [[5.14975405]
 [6.09825659]
 [6.49527073]
 [7.43031788]
 [8.35601425]].
[2019-04-08 15:04:00,827] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.03183889 0.04605805 0.15121168 0.4080941  0.0449951  0.05936494
 0.08529318 0.03096681 0.02488875 0.01188129 0.10540726], sum to 1.0000
[2019-04-08 15:04:00,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5059
[2019-04-08 15:04:00,865] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 19.0, 20.89719685529558, -0.6388085526198607, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-8.9, 30.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 19.0, 20.88522701792053, -0.6465578339655768, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.08333333333333333, 0.24043558482671093, 0.2844807220114744, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97201914], dtype=float32), -0.025786847]. 
=============================================
[2019-04-08 15:04:01,737] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.04026433 0.0659292  0.17294353 0.3242792  0.05132187 0.04850468
 0.13802372 0.02560195 0.02864905 0.01612223 0.08836018], sum to 1.0000
[2019-04-08 15:04:01,740] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6990
[2019-04-08 15:04:01,761] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-13.9, 66.0, 0.0, 0.0, 19.0, 20.34557728217679, -0.7487952048536046, 0.0, 1.0, 65.0, 200359.5629957801], 
current ob forecast is [], 
actual action is [-8.9, 40.0], 
sim time this is 345600.0000, 
sim time next is 346200.0000, 
raw observation next is [-14.0, 66.5, 0.0, 0.0, 19.0, 20.37509945158253, -0.7417860407305175, 0.0, 1.0, 40.0, 0.0], 
processed observation next is [1.0, 0.0, 0.07479224376731301, 0.665, 0.0, 0.0, 0.08333333333333333, 0.19792495429854404, 0.25273798642316087, 0.0, 1.0, 0.5, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10413761], dtype=float32), 0.7402507]. 
=============================================
[2019-04-08 15:04:02,169] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.03746473 0.0539074  0.13669379 0.3128408  0.04589549 0.10669813
 0.11694171 0.03492592 0.0295338  0.01989025 0.10520801], sum to 1.0000
[2019-04-08 15:04:02,169] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2485
[2019-04-08 15:04:02,181] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 19.20094485409233, -1.063660066741955, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-11.2, 30.0], 
sim time this is 367800.0000, 
sim time next is 368400.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 19.0, 19.16758785287657, -1.074795985392152, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.08333333333333333, 0.09729898773971406, 0.14173467153594932, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52596164], dtype=float32), -2.0226912]. 
=============================================
[2019-04-08 15:04:02,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.04334885 0.05638947 0.14826654 0.303509   0.05601194 0.07080834
 0.11846656 0.04272834 0.03148279 0.0214361  0.10755206], sum to 1.0000
[2019-04-08 15:04:02,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4750
[2019-04-08 15:04:02,727] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.3, 71.0, 0.0, 0.0, 19.0, 19.11504330507564, -1.049682895856533, 0.0, 1.0, 40.0, 121149.7929692468], 
current ob forecast is [], 
actual action is [-10.3, 30.0], 
sim time this is 358200.0000, 
sim time next is 358800.0000, 
raw observation next is [-15.4, 71.66666666666667, 0.0, 0.0, 19.0, 19.08086982773097, -1.066718581288529, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.03601108033240995, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.09007248564424764, 0.14442713957049036, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3110497], dtype=float32), -0.36705914]. 
=============================================
[2019-04-08 15:04:02,944] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.02290304 0.06679321 0.18112938 0.346362   0.05219597 0.05109317
 0.11262065 0.01811237 0.02935989 0.00976258 0.10966779], sum to 1.0000
[2019-04-08 15:04:02,946] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8674
[2019-04-08 15:04:02,965] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 8, 
current raw observation is [-13.65, 76.0, 0.0, 0.0, 19.0, 22.91763020506044, -0.1323090747112242, 0.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [-8.65, 55.0], 
sim time this is 340200.0000, 
sim time next is 340800.0000, 
raw observation next is [-13.73333333333333, 74.0, 0.0, 0.0, 19.0, 23.06861248774435, -0.1280476909884762, 0.0, 1.0, 55.0, 84889.7479088261], 
processed observation next is [1.0, 0.9565217391304348, 0.08217913204062795, 0.74, 0.0, 0.0, 0.08333333333333333, 0.4223843739786958, 0.4573174363371746, 0.0, 1.0, 0.8, 0.4042368948039338], 
reward next is 0.5958, 
noisyNet noise sample is [array([2.6486852], dtype=float32), -1.3348577]. 
=============================================
[2019-04-08 15:04:04,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01416058 0.10589685 0.17265481 0.31413087 0.03091337 0.07736876
 0.12467658 0.01888423 0.01447626 0.00591322 0.1209245 ], sum to 1.0000
[2019-04-08 15:04:04,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9433
[2019-04-08 15:04:04,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03383508 0.06060049 0.16326033 0.316706   0.04532517 0.06933678
 0.12627387 0.0302159  0.01792029 0.01034517 0.12618096], sum to 1.0000
[2019-04-08 15:04:04,116] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8739
[2019-04-08 15:04:04,137] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.4, 60.0, 64.5, 746.5, 22.5, 21.81507296282924, -0.5657811549763738, 1.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [-8.4, 65.0], 
sim time this is 385200.0000, 
sim time next is 385800.0000, 
raw observation next is [-13.3, 58.5, 62.33333333333333, 752.3333333333333, 22.5, 21.85339541356731, -0.5251774333593466, 1.0, 1.0, 65.0, 198875.549114528], 
processed observation next is [1.0, 0.4782608695652174, 0.09418282548476452, 0.585, 0.20777777777777776, 0.8313075506445672, 0.375, 0.3211162844639424, 0.32494085554688446, 1.0, 1.0, 1.0, 0.9470264243548953], 
reward next is 0.2914, 
noisyNet noise sample is [array([-1.0543176], dtype=float32), -0.6781257]. 
=============================================
[2019-04-08 15:04:04,168] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 21.47126788154025, -0.584304055664037, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-10.6, 65.0], 
sim time this is 360000.0000, 
sim time next is 360600.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 21.30147576036394, -0.5988634973191012, 0.0, 1.0, 65.0, 202592.9155468324], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.27512298003032826, 0.30037883422696626, 0.0, 1.0, 1.0, 0.9647281692706305], 
reward next is 0.0353, 
noisyNet noise sample is [array([0.40490696], dtype=float32), -0.32190415]. 
=============================================
[2019-04-08 15:04:04,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0085473  0.05828347 0.3005114  0.33285555 0.02581561 0.05325722
 0.1080361  0.01217718 0.01048    0.00547065 0.0845655 ], sum to 1.0000
[2019-04-08 15:04:04,686] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4676
[2019-04-08 15:04:04,743] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-13.1, 55.5, 58.0, 764.0, 22.5, 21.52189265375321, -0.6338803839850946, 1.0, 1.0, 35.0, 41080.63143511144], 
current ob forecast is [], 
actual action is [-8.1, 30.0], 
sim time this is 387000.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 22.5, 21.52404660111446, -0.6285092112426961, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 0.375, 0.29367055009287163, 0.29049692958576795, 1.0, 1.0, 0.3, 0.0], 
reward next is 0.1343, 
noisyNet noise sample is [array([-0.43241397], dtype=float32), 0.0612097]. 
=============================================
[2019-04-08 15:04:05,102] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03481909 0.05861101 0.10358319 0.47767466 0.04586155 0.05195613
 0.09047408 0.02892281 0.01311216 0.00653694 0.08844846], sum to 1.0000
[2019-04-08 15:04:05,102] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3716
[2019-04-08 15:04:05,131] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-14.2, 67.5, 0.0, 0.0, 19.0, 21.39630053017854, -0.5442358836240072, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-9.2, 30.0], 
sim time this is 347400.0000, 
sim time next is 348000.0000, 
raw observation next is [-14.3, 68.0, 0.0, 0.0, 19.0, 21.31016341746668, -0.5627830392738712, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.0, 0.06648199445983377, 0.68, 0.0, 0.0, 0.08333333333333333, 0.2758469514555566, 0.31240565357537625, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3728899], dtype=float32), -1.0994197]. 
=============================================
[2019-04-08 15:04:05,160] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[5.4124103]
 [5.147478 ]
 [5.261916 ]
 [5.3215756]
 [5.3860807]], R is [[5.94097948]
 [6.88156986]
 [7.81275415]
 [8.73462677]
 [9.64728069]].
[2019-04-08 15:04:05,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.03217128 0.05114448 0.1897163  0.30561924 0.04032912 0.06641192
 0.1411008  0.04051542 0.01890471 0.01818013 0.09590668], sum to 1.0000
[2019-04-08 15:04:05,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7994
[2019-04-08 15:04:05,393] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.61666666666667, 80.5, 8.333333333333332, 192.3333333333333, 22.5, 20.49581803405626, -0.794347815847101, 1.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [-11.61666666666667, 45.0], 
sim time this is 373800.0000, 
sim time next is 374400.0000, 
raw observation next is [-16.7, 81.0, 12.5, 263.5, 22.5, 20.40811435602296, -0.794602000150309, 0.0, 1.0, 45.0, 80366.66334117787], 
processed observation next is [1.0, 0.34782608695652173, 0.0, 0.81, 0.041666666666666664, 0.29116022099447514, 0.375, 0.20067619633524666, 0.23513266661656365, 0.0, 1.0, 0.6, 0.3826983968627518], 
reward next is 0.6173, 
noisyNet noise sample is [array([0.83598924], dtype=float32), 0.2254148]. 
=============================================
[2019-04-08 15:04:05,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.01035506 0.07821301 0.26431584 0.34863818 0.02248593 0.04424063
 0.11806609 0.01254239 0.00878093 0.00391652 0.08844554], sum to 1.0000
[2019-04-08 15:04:05,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9985
[2019-04-08 15:04:05,711] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.43333333333333, 51.00000000000001, 58.0, 881.5, 22.5, 22.04352013345361, -0.5446933746487684, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-7.43333333333333, 65.0], 
sim time this is 390000.0000, 
sim time next is 390600.0000, 
raw observation next is [-12.25, 51.0, 58.0, 905.0, 22.5, 21.94165453310585, -0.5119329899410237, 1.0, 1.0, 65.0, 198921.1826905024], 
processed observation next is [1.0, 0.5217391304347826, 0.12326869806094183, 0.51, 0.19333333333333333, 1.0, 0.375, 0.3284712110921542, 0.32935567001965876, 1.0, 1.0, 1.0, 0.9472437270976305], 
reward next is 0.1658, 
noisyNet noise sample is [array([1.5346613], dtype=float32), -0.9681276]. 
=============================================
[2019-04-08 15:04:05,824] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.02948264 0.06082685 0.15159363 0.39240998 0.04995964 0.06735542
 0.11862602 0.02463448 0.01238477 0.01188194 0.08084458], sum to 1.0000
[2019-04-08 15:04:05,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1394
[2019-04-08 15:04:05,892] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-15.78333333333333, 88.5, 29.66666666666666, 564.0, 22.5, 21.0748187570627, -0.6819637633073304, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-10.78333333333333, 25.0], 
sim time this is 377400.0000, 
sim time next is 378000.0000, 
raw observation next is [-15.6, 90.0, 32.0, 607.5, 22.5, 21.12717150339271, -0.6705197758221164, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.030470914127423816, 0.9, 0.10666666666666667, 0.6712707182320442, 0.375, 0.2605976252827258, 0.2764934080592945, 1.0, 1.0, 0.2, 0.0], 
reward next is 0.2861, 
noisyNet noise sample is [array([-1.0096278], dtype=float32), -1.3360144]. 
=============================================
[2019-04-08 15:04:05,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[5.382368 ]
 [5.339842 ]
 [5.242467 ]
 [5.317762 ]
 [5.5853252]], R is [[6.0345335 ]
 [6.27487755]
 [7.01706362]
 [7.73874235]
 [8.50237179]].
[2019-04-08 15:04:06,400] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.03802655 0.05276297 0.12694196 0.3956326  0.04206347 0.0581161
 0.1284051  0.03645242 0.02064776 0.011309   0.089642  ], sum to 1.0000
[2019-04-08 15:04:06,400] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3746
[2019-04-08 15:04:06,416] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 21.28813438395891, -0.6019994382064296, 0.0, 1.0, 45.0, 0.0], 
current ob forecast is [], 
actual action is [-10.6, 30.0], 
sim time this is 360600.0000, 
sim time next is 361200.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 21.22140593198848, -0.624815226189278, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.26845049433237317, 0.2917282579369073, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1904372], dtype=float32), -0.22275533]. 
=============================================
[2019-04-08 15:04:06,694] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00735483 0.046439   0.3389844  0.3201204  0.03373024 0.02417236
 0.12533864 0.00973613 0.00543553 0.003166   0.08552255], sum to 1.0000
[2019-04-08 15:04:06,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5557
[2019-04-08 15:04:06,776] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.8, 51.0, 58.0, 834.5, 22.5, 24.01436724367714, -0.02858441858180909, 1.0, 1.0, 65.0, 200454.6747050211], 
current ob forecast is [], 
actual action is [-7.800000000000001, 30.0], 
sim time this is 388800.0000, 
sim time next is 389400.0000, 
raw observation next is [-12.61666666666667, 51.0, 58.0, 858.0, 22.5, 24.14968772828887, -0.1232485139926147, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.11311172668513378, 0.51, 0.19333333333333333, 0.9480662983425414, 0.375, 0.512473977357406, 0.45891716200246174, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01912741], dtype=float32), 0.29298198]. 
=============================================
[2019-04-08 15:04:07,177] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00993805 0.08119874 0.31553414 0.27224523 0.04395348 0.04831669
 0.13982756 0.01685144 0.01252242 0.00349109 0.05612125], sum to 1.0000
[2019-04-08 15:04:07,177] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8104
[2019-04-08 15:04:07,211] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.5, 46.16666666666667, 0.0, 0.0, 19.0, 22.15415521065825, -0.4554666404839083, 0.0, 1.0, 30.00000000000002, 0.0], 
current ob forecast is [], 
actual action is [-5.5, 45.0], 
sim time this is 420600.0000, 
sim time next is 421200.0000, 
raw observation next is [-10.6, 47.0, 0.0, 0.0, 19.0, 21.93978819935975, -0.4696787247013541, 0.0, 1.0, 45.0, 138201.7700803102], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.47, 0.0, 0.0, 0.08333333333333333, 0.3283156832799792, 0.34344042509954864, 0.0, 1.0, 0.6, 0.6581036670490962], 
reward next is 0.3419, 
noisyNet noise sample is [array([-1.2776232], dtype=float32), 1.8659611]. 
=============================================
[2019-04-08 15:04:09,134] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.02207129 0.04245045 0.17622446 0.40142784 0.04510384 0.04283739
 0.1381802  0.02170157 0.01226702 0.00633081 0.09140505], sum to 1.0000
[2019-04-08 15:04:09,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1894
[2019-04-08 15:04:09,151] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 19.79395376921487, -0.9379641199736078, 0.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [-6.699999999999999, 25.0], 
sim time this is 430800.0000, 
sim time next is 431400.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 19.74406847991768, -0.9598351335740554, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.14533903999313993, 0.1800549554753149, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07955913], dtype=float32), -0.11321374]. 
=============================================
[2019-04-08 15:04:09,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00650338 0.07416651 0.35597834 0.34315255 0.02525011 0.01840243
 0.09498368 0.00939738 0.00431477 0.00170371 0.06614719], sum to 1.0000
[2019-04-08 15:04:09,713] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2595
[2019-04-08 15:04:09,774] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-9.2, 38.0, 0.0, 0.0, 22.5, 23.79298854779227, -0.1377117879783335, 1.0, 1.0, 65.0, 199293.5999933322], 
current ob forecast is [], 
actual action is [-4.199999999999999, 25.0], 
sim time this is 408600.0000, 
sim time next is 409200.0000, 
raw observation next is [-9.3, 38.66666666666666, 0.0, 0.0, 22.5, 23.57002443281154, -0.1321613973215697, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.20498614958448752, 0.38666666666666655, 0.0, 0.0, 0.375, 0.46416870273429495, 0.4559462008928101, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.420785], dtype=float32), -0.35610148]. 
=============================================
[2019-04-08 15:04:10,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.01363316 0.04194985 0.19975156 0.4143502  0.03253088 0.02986163
 0.11003229 0.01254726 0.0085381  0.00663754 0.13016751], sum to 1.0000
[2019-04-08 15:04:10,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2879
[2019-04-08 15:04:10,352] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 19.0, 20.76338110297379, -0.7273817348358564, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-5.6, 30.0], 
sim time this is 424800.0000, 
sim time next is 425400.0000, 
raw observation next is [-10.78333333333333, 49.83333333333334, 0.0, 0.0, 19.0, 20.68995923691184, -0.7493446876168957, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.1638965835641737, 0.4983333333333334, 0.0, 0.0, 0.08333333333333333, 0.2241632697426533, 0.2502184374610348, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.162345], dtype=float32), 0.90710866]. 
=============================================
[2019-04-08 15:04:11,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.02273412 0.05052403 0.18221353 0.35738844 0.035714   0.08802643
 0.15258424 0.01973413 0.01279542 0.00778212 0.07050353], sum to 1.0000
[2019-04-08 15:04:11,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0527
[2019-04-08 15:04:11,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-8.583333333333334, 43.16666666666667, 0.0, 0.0, 19.0, 19.1792755046386, -1.137725862222096, 0.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [-3.583333333333334, 20.0], 
sim time this is 456600.0000, 
sim time next is 457200.0000, 
raw observation next is [-8.4, 43.0, 0.0, 0.0, 19.0, 19.16502836167273, -1.147765995829192, 0.0, 1.0, 20.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2299168975069252, 0.43, 0.0, 0.0, 0.08333333333333333, 0.09708569680606072, 0.1174113347236027, 0.0, 1.0, 0.1, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9044528], dtype=float32), -0.64539033]. 
=============================================
[2019-04-08 15:04:12,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.02380736 0.05321144 0.1569021  0.39903983 0.03034374 0.04841827
 0.12285245 0.02567678 0.01518376 0.00817069 0.11639355], sum to 1.0000
[2019-04-08 15:04:12,181] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9537
[2019-04-08 15:04:12,200] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 43.0, 0.0, 0.0, 19.0, 18.7122180160769, -1.227389318451521, 0.0, 1.0, 40.0, 19242.27606939506], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [-8.3, 42.5, 0.0, 0.0, 22.5, 18.58978531697822, -1.194617022394074, 0.0, 1.0, 65.0, 199272.3245478001], 
processed observation next is [1.0, 0.30434782608695654, 0.23268698060941828, 0.425, 0.0, 0.0, 0.375, 0.04914877641485157, 0.10179432586864201, 0.0, 1.0, 1.0, 0.9489158311800004], 
reward next is 0.1565, 
noisyNet noise sample is [array([0.7756768], dtype=float32), 1.8715392]. 
=============================================
[2019-04-08 15:04:13,074] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.01005897 0.06221942 0.33017862 0.3161956  0.02729237 0.03422567
 0.09021366 0.01182007 0.00625667 0.00157091 0.10996806], sum to 1.0000
[2019-04-08 15:04:13,075] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8312
[2019-04-08 15:04:13,128] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 26.5, 129.0, 0.0, 22.5, 21.79765033858231, -0.6243497840540878, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [3.55, 45.0], 
sim time this is 477000.0000, 
sim time next is 477600.0000, 
raw observation next is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 22.5, 21.97586259334748, -0.6035623762471701, 1.0, 1.0, 45.0, 120952.9241314852], 
processed observation next is [1.0, 0.5217391304347826, 0.42474607571560485, 0.27, 0.42444444444444435, 0.0, 0.375, 0.3313218827789566, 0.2988125412509433, 1.0, 1.0, 0.6, 0.5759663053880247], 
reward next is 0.5197, 
noisyNet noise sample is [array([-0.19977094], dtype=float32), -0.8158654]. 
=============================================
[2019-04-08 15:04:13,959] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.00962552 0.0810545  0.29034182 0.28449738 0.02674965 0.03235314
 0.11640999 0.00776758 0.00434246 0.00166312 0.1451948 ], sum to 1.0000
[2019-04-08 15:04:13,959] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3875
[2019-04-08 15:04:14,006] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3666666666666667, 36.0, 62.33333333333334, 0.0, 22.5, 22.46930339774716, -0.4175566292217865, 1.0, 1.0, 40.0, 18720.32912066616], 
current ob forecast is [], 
actual action is [5.366666666666667, 30.0], 
sim time this is 487200.0000, 
sim time next is 487800.0000, 
raw observation next is [0.55, 35.5, 56.0, 0.0, 22.5, 22.9406682862747, -0.3781514499942835, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4778393351800555, 0.355, 0.18666666666666668, 0.0, 0.375, 0.41172235718955835, 0.3739495166685722, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8979172], dtype=float32), 0.8201907]. 
=============================================
[2019-04-08 15:04:14,142] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00465445 0.07572613 0.29474202 0.35573432 0.03592903 0.03202118
 0.09909783 0.00525866 0.00218126 0.00109286 0.09356223], sum to 1.0000
[2019-04-08 15:04:14,144] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4397
[2019-04-08 15:04:14,198] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.09999999999999998, 36.66666666666667, 81.33333333333334, 0.0, 22.5, 22.93523479979818, -0.3728520791860206, 1.0, 1.0, 30.00000000000009, 0.0], 
current ob forecast is [], 
actual action is [4.9, 30.0], 
sim time this is 485400.0000, 
sim time next is 486000.0000, 
raw observation next is [0.0, 37.0, 75.0, 0.0, 22.5, 22.79503827103715, -0.395996291130999, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.37, 0.25, 0.0, 0.375, 0.3995865225864292, 0.36800123628966697, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54395884], dtype=float32), -0.7890705]. 
=============================================
[2019-04-08 15:04:14,202] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[9.7397  ]
 [9.589156]
 [9.669733]
 [9.243534]
 [9.190835]], R is [[10.51074409]
 [11.40563679]
 [12.2915802 ]
 [12.84484959]
 [13.7164011 ]].
[2019-04-08 15:04:14,237] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00596531 0.04876004 0.23453021 0.42148247 0.01515562 0.03102686
 0.14513241 0.00781707 0.00329321 0.00105795 0.08577888], sum to 1.0000
[2019-04-08 15:04:14,237] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3289
[2019-04-08 15:04:14,259] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 41.5, 13.33333333333334, 0.0, 22.5, 22.21515195492719, -0.607301993687301, 1.0, 1.0, 30.00000000000004, 0.0], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 492600.0000, 
sim time next is 493200.0000, 
raw observation next is [1.1, 43.0, 10.0, 0.0, 22.5, 22.07893180354799, -0.5443475591591289, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.43, 0.03333333333333333, 0.0, 0.375, 0.3399109836289993, 0.3185508136136237, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.620389], dtype=float32), 0.49638847]. 
=============================================
[2019-04-08 15:04:14,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00379621 0.09462254 0.28795797 0.26598838 0.01595756 0.02879668
 0.11873677 0.0062617  0.00208553 0.00094605 0.17485067], sum to 1.0000
[2019-04-08 15:04:14,812] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9685
[2019-04-08 15:04:14,928] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 25.0, 125.5, 0.0, 22.5, 23.43118782873137, -0.2558338829634619, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 475200.0000, 
sim time next is 475800.0000, 
raw observation next is [-1.616666666666667, 25.5, 126.6666666666667, 0.0, 22.5, 23.54037949797629, -0.2102925723922785, 1.0, 1.0, 65.0, 199175.0506355819], 
processed observation next is [1.0, 0.5217391304347826, 0.4178208679593721, 0.255, 0.42222222222222233, 0.0, 0.375, 0.4616982914980241, 0.4299024758692405, 1.0, 1.0, 1.0, 0.9484526220741994], 
reward next is 0.0515, 
noisyNet noise sample is [array([-0.26145226], dtype=float32), -0.4963083]. 
=============================================
[2019-04-08 15:04:15,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.01027643 0.05353056 0.2354445  0.40762106 0.02726524 0.02170993
 0.15198365 0.00780641 0.00316176 0.00071992 0.08048054], sum to 1.0000
[2019-04-08 15:04:15,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3442
[2019-04-08 15:04:15,726] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.783333333333333, 95.33333333333333, 0.0, 0.0, 19.0, 20.33288884656595, -0.8303344233038213, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [6.783333333333333, 30.0], 
sim time this is 508200.0000, 
sim time next is 508800.0000, 
raw observation next is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 19.0, 20.27628735802088, -0.8426893343206592, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5170821791320407, 0.9466666666666665, 0.0, 0.0, 0.08333333333333333, 0.1896906131684067, 0.21910355522644695, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48386675], dtype=float32), -0.73341507]. 
=============================================
[2019-04-08 15:04:17,303] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00182777 0.04377393 0.2561798  0.47254145 0.01523536 0.01431357
 0.10427694 0.00196904 0.00169794 0.00048454 0.08769952], sum to 1.0000
[2019-04-08 15:04:17,303] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5266
[2019-04-08 15:04:17,342] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.183333333333333, 96.0, 0.0, 0.0, 19.0, 22.84364811956869, -0.2542832719158581, 1.0, 1.0, 65.0, 199744.1714957428], 
current ob forecast is [], 
actual action is [6.183333333333333, 30.0], 
sim time this is 504600.0000, 
sim time next is 505200.0000, 
raw observation next is [1.266666666666667, 96.0, 0.0, 0.0, 19.0, 22.8921394434459, -0.2468950071461696, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4976915974145891, 0.96, 0.0, 0.0, 0.08333333333333333, 0.4076782869538249, 0.41770166428461014, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8680594], dtype=float32), 1.1452719]. 
=============================================
[2019-04-08 15:04:17,606] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.0052803  0.02661677 0.21778755 0.45168567 0.02311112 0.02899698
 0.16041532 0.00569732 0.00202826 0.0011058  0.07727496], sum to 1.0000
[2019-04-08 15:04:17,607] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1271
[2019-04-08 15:04:17,642] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [3.8, 97.0, 0.0, 0.0, 19.0, 21.90540456674911, -0.4336595785471375, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [8.8, 25.0], 
sim time this is 518400.0000, 
sim time next is 519000.0000, 
raw observation next is [4.0, 95.66666666666667, 0.0, 0.0, 19.0, 21.9654943829525, -0.4308111641091878, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.0, 0.5734072022160666, 0.9566666666666667, 0.0, 0.0, 0.08333333333333333, 0.33045786524604165, 0.35639627863027074, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27389997], dtype=float32), 0.46617618]. 
=============================================
[2019-04-08 15:04:17,648] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[10.629539 ]
 [10.711248 ]
 [10.6347475]
 [10.31858  ]
 [10.54576  ]], R is [[11.36451435]
 [12.25086975]
 [12.17576122]
 [13.05400372]
 [13.92346382]].
[2019-04-08 15:04:18,031] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.01833636 0.07351196 0.1287845  0.42153874 0.04256115 0.04516479
 0.12548183 0.0222015  0.00490082 0.0031648  0.11435368], sum to 1.0000
[2019-04-08 15:04:18,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3554
[2019-04-08 15:04:18,057] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.183333333333333, 87.5, 0.0, 0.0, 19.0, 24.07393248125073, -0.00315490208912946, 0.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [6.183333333333333, 30.0], 
sim time this is 539400.0000, 
sim time next is 540000.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 19.0, 24.00452911773306, -0.02279717022232276, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.49307479224376743, 0.88, 0.0, 0.0, 0.08333333333333333, 0.5003774264777551, 0.49240094325922573, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0952278], dtype=float32), -1.5598487]. 
=============================================
[2019-04-08 15:04:18,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[7.9862304]
 [7.9052243]
 [7.9896502]
 [7.9614463]
 [7.759885 ]], R is [[ 9.04670811]
 [ 9.95624065]
 [10.85667801]
 [11.74811172]
 [12.63063049]].
[2019-04-08 15:04:18,358] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.01755914 0.06386195 0.15899782 0.3321703  0.06079887 0.04639841
 0.10844426 0.0268758  0.00766949 0.00374137 0.17348263], sum to 1.0000
[2019-04-08 15:04:18,358] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8984
[2019-04-08 15:04:18,378] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 19.0, 21.88786861424164, -0.4277414075755905, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [-0.6, 85.0, 77.0, 141.0, 19.0, 21.83104140304205, -0.4176149763906845, 0.0, 1.0, 65.0, 201461.3204580906], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.85, 0.25666666666666665, 0.1558011049723757, 0.08333333333333333, 0.3192534502535042, 0.36079500786977187, 0.0, 1.0, 1.0, 0.9593396212290028], 
reward next is 0.0407, 
noisyNet noise sample is [array([-0.92386353], dtype=float32), -0.022220472]. 
=============================================
[2019-04-08 15:04:18,669] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.02245493 0.09164734 0.14442228 0.3474934  0.05012954 0.05387945
 0.10627522 0.02987227 0.01049342 0.00521923 0.1381129 ], sum to 1.0000
[2019-04-08 15:04:18,673] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9464
[2019-04-08 15:04:18,685] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 17.5, 54.5, 19.0, 20.93823855675253, -0.6433132357548802, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 547200.0000, 
sim time next is 547800.0000, 
raw observation next is [0.4166666666666667, 91.83333333333334, 23.0, 71.00000000000001, 19.0, 20.94984881496829, -0.6318765244833322, 0.0, 1.0, 65.0, 200493.454398748], 
processed observation next is [0.0, 0.34782608695652173, 0.47414589104339805, 0.9183333333333334, 0.07666666666666666, 0.07845303867403317, 0.08333333333333333, 0.24582073458069095, 0.28937449183888925, 0.0, 1.0, 1.0, 0.9547307352321334], 
reward next is 0.0453, 
noisyNet noise sample is [array([-0.5849428], dtype=float32), 1.9293926]. 
=============================================
[2019-04-08 15:04:18,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00635675 0.044553   0.22048779 0.47092006 0.02685216 0.03348132
 0.0930009  0.01416021 0.00366183 0.00277059 0.08375551], sum to 1.0000
[2019-04-08 15:04:18,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7520
[2019-04-08 15:04:18,724] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-0.7, 82.0, 89.0, 135.0, 19.0, 21.89913987382322, -0.4377945990818697, 0.0, 1.0, 65.0, 201773.5574222659], 
current ob forecast is [], 
actual action is [4.3, 35.0], 
sim time this is 559800.0000, 
sim time next is 560400.0000, 
raw observation next is [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 19.0, 21.85128417912436, -0.4281499458842318, 0.0, 1.0, 35.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.44228993536472766, 0.8166666666666668, 0.3194444444444444, 0.19723756906077347, 0.08333333333333333, 0.3209403482603633, 0.35728335137192274, 0.0, 1.0, 0.4, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1797616], dtype=float32), 0.9037953]. 
=============================================
[2019-04-08 15:04:18,943] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00564339 0.07452437 0.14809379 0.5474136  0.02452396 0.01751332
 0.09037941 0.00917335 0.00206489 0.00148882 0.07918115], sum to 1.0000
[2019-04-08 15:04:18,956] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8227
[2019-04-08 15:04:18,977] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.883333333333334, 88.83333333333334, 0.0, 0.0, 19.0, 22.15061385995294, -0.4110014820563495, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [9.883333333333333, 45.0], 
sim time this is 522600.0000, 
sim time next is 523200.0000, 
raw observation next is [4.766666666666667, 88.66666666666667, 0.0, 0.0, 19.0, 22.08677310168291, -0.4065934386747995, 0.0, 1.0, 45.0, 192603.8022087645], 
processed observation next is [0.0, 0.043478260869565216, 0.5946445060018468, 0.8866666666666667, 0.0, 0.0, 0.08333333333333333, 0.3405644251402424, 0.3644688537750668, 0.0, 1.0, 0.6, 0.9171609628988786], 
reward next is 0.0828, 
noisyNet noise sample is [array([0.64405066], dtype=float32), 0.7480396]. 
=============================================
[2019-04-08 15:04:19,203] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.01216018 0.10760249 0.13827136 0.42146227 0.0280866  0.03791291
 0.11030136 0.02125253 0.00662497 0.00369407 0.11263127], sum to 1.0000
[2019-04-08 15:04:19,206] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5606
[2019-04-08 15:04:19,215] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.7, 82.0, 0.0, 0.0, 19.0, 22.88061778963383, -0.2847973550642488, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [7.7, 30.0], 
sim time this is 532800.0000, 
sim time next is 533400.0000, 
raw observation next is [2.516666666666667, 82.50000000000001, 0.0, 0.0, 19.0, 22.78298492096686, -0.3006775663246979, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.5323176361957526, 0.8250000000000002, 0.0, 0.0, 0.08333333333333333, 0.3985820767472384, 0.399774144558434, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.84799004], dtype=float32), -0.23285955]. 
=============================================
[2019-04-08 15:04:20,096] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00836371 0.06978601 0.22243969 0.30914414 0.02447105 0.03954765
 0.12672049 0.01400615 0.00370418 0.00247237 0.17934461], sum to 1.0000
[2019-04-08 15:04:20,099] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9868
[2019-04-08 15:04:20,114] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 80.0, 134.0, 495.5, 19.0, 21.16025436117137, -0.4964288076468188, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 565200.0000, 
sim time next is 565800.0000, 
raw observation next is [-1.2, 80.0, 135.3333333333333, 528.6666666666666, 19.0, 21.24268902333635, -0.467979713190305, 0.0, 1.0, 65.0, 200760.9133074967], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.45111111111111096, 0.5841620626151013, 0.08333333333333333, 0.27022408527802916, 0.34400676226989835, 0.0, 1.0, 1.0, 0.9560043490833176], 
reward next is 0.0440, 
noisyNet noise sample is [array([-0.266177], dtype=float32), -1.8552252]. 
=============================================
[2019-04-08 15:04:20,454] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 15088: loss 8.1672
[2019-04-08 15:04:20,456] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 15088: learning rate 0.0000
[2019-04-08 15:04:20,566] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15151: loss 14.1103
[2019-04-08 15:04:20,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15151: learning rate 0.0000
[2019-04-08 15:04:20,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00888694 0.05485459 0.13508059 0.587993   0.01897117 0.03215887
 0.06052282 0.00718841 0.0024281  0.00061899 0.09129661], sum to 1.0000
[2019-04-08 15:04:20,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1979
[2019-04-08 15:04:20,595] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 15169: loss 21.7160
[2019-04-08 15:04:20,596] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 15169: learning rate 0.0000
[2019-04-08 15:04:20,608] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.8666666666666667, 80.83333333333333, 116.3333333333333, 309.0000000000001, 19.0, 22.24334527613566, -0.3393950470116289, 0.0, 1.0, 40.0, 0.0], 
current ob forecast is [], 
actual action is [4.133333333333333, 25.0], 
sim time this is 562200.0000, 
sim time next is 562800.0000, 
raw observation next is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 19.0, 22.28039879169501, -0.3406746981898152, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4367497691597415, 0.8066666666666668, 0.4105555555555557, 0.38950276243093923, 0.08333333333333333, 0.3566998993079175, 0.3864417672700616, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3170599], dtype=float32), -1.0115916]. 
=============================================
[2019-04-08 15:04:21,023] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00434574 0.06342532 0.18982825 0.43870163 0.01778017 0.02803798
 0.08052048 0.01170683 0.00214254 0.00085201 0.16265903], sum to 1.0000
[2019-04-08 15:04:21,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1519
[2019-04-08 15:04:21,040] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.8666666666666667, 80.83333333333333, 116.3333333333333, 309.0000000000001, 19.0, 22.0368300443909, -0.3626855134420999, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [4.133333333333333, 25.0], 
sim time this is 562200.0000, 
sim time next is 562800.0000, 
raw observation next is [-0.9333333333333333, 80.66666666666667, 123.1666666666667, 352.5, 19.0, 21.94704468895242, -0.3772797181719252, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4367497691597415, 0.8066666666666668, 0.4105555555555557, 0.38950276243093923, 0.08333333333333333, 0.328920390746035, 0.3742400939426916, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.044562], dtype=float32), -0.9108757]. 
=============================================
[2019-04-08 15:04:21,092] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15443: loss 13.8796
[2019-04-08 15:04:21,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15443: learning rate 0.0000
[2019-04-08 15:04:21,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8066258e-03 3.2688621e-02 1.1057640e-01 5.6355304e-01 2.1545624e-02
 1.8160952e-02 1.0286803e-01 9.6088303e-03 2.0190424e-03 4.7085783e-04
 1.3570192e-01], sum to 1.0000
[2019-04-08 15:04:21,235] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2810
[2019-04-08 15:04:21,242] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1000, global step 15528: loss 19.6147
[2019-04-08 15:04:21,244] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1000, global step 15529: learning rate 0.0000
[2019-04-08 15:04:21,253] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.8, 87.0, 0.0, 0.0, 19.0, 22.66572854748477, -0.2576320519123046, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.2, 30.0], 
sim time this is 580200.0000, 
sim time next is 580800.0000, 
raw observation next is [-1.9, 87.0, 0.0, 0.0, 19.0, 22.67826381234133, -0.2682369192545656, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.4099722991689751, 0.87, 0.0, 0.0, 0.08333333333333333, 0.38985531769511095, 0.4105876935818114, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2572272], dtype=float32), -0.25621524]. 
=============================================
[2019-04-08 15:04:21,342] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15588: loss 41.4073
[2019-04-08 15:04:21,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15588: learning rate 0.0000
[2019-04-08 15:04:21,450] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [0.00487975 0.03225029 0.24600902 0.5450092  0.01054935 0.01530274
 0.05055751 0.00541352 0.00150658 0.00110531 0.08741672], sum to 1.0000
[2019-04-08 15:04:21,452] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8485
[2019-04-08 15:04:21,469] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-3.4, 85.0, 0.0, 0.0, 19.0, 20.67883997247694, -0.7285624886205851, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 40.0], 
sim time this is 603000.0000, 
sim time next is 603600.0000, 
raw observation next is [-3.4, 85.66666666666667, 0.0, 0.0, 19.0, 20.62226151663083, -0.742284918501447, 0.0, 1.0, 40.0, 0.0], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.21852179305256905, 0.252571693832851, 0.0, 1.0, 0.5, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6263269], dtype=float32), 0.59397763]. 
=============================================
[2019-04-08 15:04:21,653] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00264975 0.04232594 0.16940531 0.56316876 0.02172519 0.02311616
 0.06707554 0.01056342 0.00215368 0.00077989 0.09703632], sum to 1.0000
[2019-04-08 15:04:21,663] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0635
[2019-04-08 15:04:21,676] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.3, 87.0, 0.0, 0.0, 19.0, 21.64004467420819, -0.4152529369945883, 0.0, 1.0, 65.0, 200092.682106501], 
current ob forecast is [], 
actual action is [2.7, 25.0], 
sim time this is 583200.0000, 
sim time next is 583800.0000, 
raw observation next is [-2.383333333333333, 87.0, 0.0, 0.0, 19.0, 21.65028815241846, -0.4007602298861412, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.3965835641735919, 0.87, 0.0, 0.0, 0.08333333333333333, 0.30419067936820515, 0.3664132567046196, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7171388], dtype=float32), -0.08568822]. 
=============================================
[2019-04-08 15:04:21,701] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 15788: loss 10.0743
[2019-04-08 15:04:21,701] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 15788: learning rate 0.0000
[2019-04-08 15:04:22,004] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00831846 0.05097959 0.17241359 0.3890867  0.02902569 0.03755374
 0.13756995 0.01233261 0.00218435 0.00151934 0.15901595], sum to 1.0000
[2019-04-08 15:04:22,006] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1378
[2019-04-08 15:04:22,028] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.816666666666666, 86.16666666666667, 0.0, 0.0, 19.0, 20.06252267860795, -0.8170506619743284, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.183333333333334, 25.0], 
sim time this is 607800.0000, 
sim time next is 608400.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 20.05076350882691, -0.8268379393825173, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.17089695906890925, 0.22438735353916092, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6169209], dtype=float32), 0.28001836]. 
=============================================
[2019-04-08 15:04:22,086] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16015: loss 35.5389
[2019-04-08 15:04:22,087] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16015: learning rate 0.0000
[2019-04-08 15:04:22,386] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16190: loss 21.8928
[2019-04-08 15:04:22,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16190: learning rate 0.0000
[2019-04-08 15:04:22,462] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16220: loss 29.6466
[2019-04-08 15:04:22,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16224: learning rate 0.0000
[2019-04-08 15:04:22,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00638562 0.0761601  0.13541669 0.4464512  0.01840452 0.0365621
 0.07064993 0.01072755 0.00272193 0.00126704 0.19525328], sum to 1.0000
[2019-04-08 15:04:22,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6841
[2019-04-08 15:04:22,530] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16267: loss 27.4297
[2019-04-08 15:04:22,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16267: learning rate 0.0000
[2019-04-08 15:04:22,541] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 20.66834988877152, -0.6660177345407144, 0.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [2.2, 40.0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 19.0, 20.65712014768011, -0.6740086428541167, 0.0, 1.0, 40.0, 43079.1901946133], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.08333333333333333, 0.22142667897334256, 0.27533045238196113, 0.0, 1.0, 0.5, 0.20513900092673001], 
reward next is 0.7949, 
noisyNet noise sample is [array([-0.3043509], dtype=float32), -0.13955864]. 
=============================================
[2019-04-08 15:04:22,550] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[10.038241]
 [10.414824]
 [10.724126]
 [10.470806]
 [10.606585]], R is [[10.89701843]
 [11.78804874]
 [12.54272842]
 [13.41730118]
 [13.33974075]].
[2019-04-08 15:04:22,681] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16349: loss 39.5215
[2019-04-08 15:04:22,683] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16349: learning rate 0.0000
[2019-04-08 15:04:22,997] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16516: loss 13.5236
[2019-04-08 15:04:22,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16517: learning rate 0.0000
[2019-04-08 15:04:23,103] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16582: loss 14.9495
[2019-04-08 15:04:23,106] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16582: learning rate 0.0000
[2019-04-08 15:04:23,416] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16754: loss 16.6425
[2019-04-08 15:04:23,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16756: learning rate 0.0000
[2019-04-08 15:04:23,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00687288 0.07344649 0.22125135 0.38046083 0.04993976 0.02435697
 0.12941884 0.01923024 0.00458274 0.00286007 0.08757981], sum to 1.0000
[2019-04-08 15:04:23,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3564
[2019-04-08 15:04:23,821] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.1, 73.66666666666666, 38.33333333333333, 8.499999999999998, 19.0, 19.8399690525972, -0.881836728229468, 0.0, 1.0, 65.0, 201618.6284748243], 
current ob forecast is [], 
actual action is [0.9000000000000004, 30.0], 
sim time this is 636000.0000, 
sim time next is 636600.0000, 
raw observation next is [-4.0, 72.33333333333334, 57.66666666666666, 17.0, 19.0, 19.91777067992463, -0.8572682063161491, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.7233333333333334, 0.19222222222222218, 0.01878453038674033, 0.08333333333333333, 0.15981422332705245, 0.2142439312279503, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1913884], dtype=float32), -0.445714]. 
=============================================
[2019-04-08 15:04:24,235] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1000, global step 17220: loss 14.7850
[2019-04-08 15:04:24,237] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1000, global step 17222: learning rate 0.0000
[2019-04-08 15:04:24,238] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [0.00226177 0.04661608 0.2434454  0.43191355 0.01976482 0.01973743
 0.10838395 0.008173   0.00122912 0.00109332 0.1173816 ], sum to 1.0000
[2019-04-08 15:04:24,242] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6811
[2019-04-08 15:04:24,255] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.75, 59.5, 182.0, 93.0, 19.0, 19.22013547734262, -0.9539243554732404, 0.0, 1.0, 45.0, 30859.25601437002], 
current ob forecast is [], 
actual action is [3.25, 30.0], 
sim time this is 653400.0000, 
sim time next is 654000.0000, 
raw observation next is [-1.566666666666667, 59.66666666666667, 165.1666666666667, 86.83333333333333, 19.0, 19.42017908173042, -0.9387378595326071, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.4192059095106187, 0.5966666666666667, 0.5505555555555557, 0.09594843462246777, 0.08333333333333333, 0.1183482568108684, 0.18708738015579762, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5008613], dtype=float32), -0.80957794]. 
=============================================
[2019-04-08 15:04:24,267] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[10.447371]
 [10.185947]
 [ 9.835846]
 [10.099193]
 [10.04081 ]], R is [[11.51850224]
 [12.25636864]
 [12.18334103]
 [13.06150818]
 [13.93089294]].
[2019-04-08 15:04:24,765] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.00359349 0.06345861 0.20333889 0.44226712 0.0191446  0.04326442
 0.11713708 0.00626403 0.00261308 0.00118786 0.09773079], sum to 1.0000
[2019-04-08 15:04:24,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6677
[2019-04-08 15:04:24,805] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.05, 63.0, 89.0, 38.0, 19.0, 19.09537764028876, -1.063596499181098, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.9500000000000002, 30.0], 
sim time this is 646200.0000, 
sim time next is 646800.0000, 
raw observation next is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 19.0, 19.06960091297374, -1.072324543235906, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.38134810710988, 0.6233333333333333, 0.30944444444444447, 0.05340699815837937, 0.08333333333333333, 0.08913340941447838, 0.1425584855880313, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07767471], dtype=float32), 0.73851115]. 
=============================================
[2019-04-08 15:04:25,276] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7901949e-03 4.2722315e-02 2.0016095e-01 5.7640994e-01 2.3958733e-02
 2.6104340e-02 7.5669698e-02 7.3938421e-03 1.6194492e-03 5.2343583e-04
 4.1647092e-02], sum to 1.0000
[2019-04-08 15:04:25,277] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7836
[2019-04-08 15:04:25,288] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 19.0, 18.8283470813377, -1.168128277999105, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 30.0], 
sim time this is 701400.0000, 
sim time next is 702000.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 19.0, 18.82052955254456, -1.175794693487495, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.368421052631579, 0.75, 0.0, 0.0, 0.08333333333333333, 0.06837746271204666, 0.10806843550416834, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.8712, 
noisyNet noise sample is [array([-1.7119005], dtype=float32), -0.27951512]. 
=============================================
[2019-04-08 15:04:25,295] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[11.825465]
 [11.902383]
 [12.076855]
 [12.051251]
 [12.360667]], R is [[12.66978645]
 [13.42523003]
 [14.29097748]
 [15.12928963]
 [15.78471088]].
[2019-04-08 15:04:26,103] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6868092e-03 3.5714764e-02 1.1702818e-01 5.2854639e-01 1.5600485e-02
 1.0860576e-02 1.0811493e-01 6.2306440e-03 1.5516288e-03 3.3591018e-04
 1.7232978e-01], sum to 1.0000
[2019-04-08 15:04:26,103] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0473
[2019-04-08 15:04:26,114] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.4, 74.0, 0.0, 0.0, 19.0, 18.18863549691463, -1.308801096309422, 0.0, 1.0, 45.0, 137662.5877305616], 
current ob forecast is [], 
actual action is [1.6, 25.0], 
sim time this is 697200.0000, 
sim time next is 697800.0000, 
raw observation next is [-3.4, 74.5, 0.0, 0.0, 19.0, 18.17874995479101, -1.301800923438927, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.745, 0.0, 0.0, 0.08333333333333333, 0.014895829565917396, 0.06606635885369101, 0.0, 1.0, 0.2, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6429155], dtype=float32), 1.7868813]. 
=============================================
[2019-04-08 15:04:26,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.00298823 0.0348846  0.2991632  0.48194027 0.01489826 0.0256741
 0.05051178 0.00723476 0.00122662 0.00069048 0.08078767], sum to 1.0000
[2019-04-08 15:04:26,174] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4854
[2019-04-08 15:04:26,200] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 55.00000000000001, 36.33333333333333, 18.83333333333333, 19.0, 19.13689997146703, -1.031998116820395, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.2, 30.0], 
sim time this is 663600.0000, 
sim time next is 664200.0000, 
raw observation next is [-0.8999999999999999, 55.5, 27.0, 15.0, 19.0, 19.13411400262555, -1.042491715555327, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.43767313019390586, 0.555, 0.09, 0.016574585635359115, 0.08333333333333333, 0.09450950021879596, 0.15250276148155764, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05206744], dtype=float32), -0.43527237]. 
=============================================
[2019-04-08 15:04:26,231] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6444925e-03 3.6554150e-02 1.4332193e-01 6.2391907e-01 1.4355035e-02
 3.3377815e-02 8.3265401e-02 4.4911993e-03 6.5809581e-04 3.6720323e-04
 5.7045698e-02], sum to 1.0000
[2019-04-08 15:04:26,231] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4760
[2019-04-08 15:04:26,241] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.3, 75.0, 0.0, 0.0, 19.0, 17.7803735900947, -1.382403331858232, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.7000000000000002, 30.0], 
sim time this is 702600.0000, 
sim time next is 703200.0000, 
raw observation next is [-3.2, 75.0, 0.0, 0.0, 19.0, 17.81560550191528, -1.377363709564675, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.37396121883656513, 0.75, 0.0, 0.0, 0.08333333333333333, -0.015366208173726767, 0.040878763478441695, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.3523, 
noisyNet noise sample is [array([1.2468532], dtype=float32), -0.10985993]. 
=============================================
[2019-04-08 15:04:26,936] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5861263e-03 3.0037249e-02 1.7068662e-01 6.1020303e-01 1.1238463e-02
 5.7919598e-03 8.3896577e-02 3.8754472e-03 4.4761982e-04 2.6623710e-04
 8.1970684e-02], sum to 1.0000
[2019-04-08 15:04:26,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4668
[2019-04-08 15:04:26,946] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [0.00194687 0.03857001 0.2092029  0.47109544 0.01172187 0.01139938
 0.13188247 0.00258313 0.00114404 0.00047952 0.11997431], sum to 1.0000
[2019-04-08 15:04:26,949] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3636
[2019-04-08 15:04:26,951] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.03935613340386, -1.368942296058315, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [2.7, 30.0], 
sim time this is 715200.0000, 
sim time next is 715800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.04755369535174, -1.384780000257124, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.0039628079459784376, 0.03840666658095867, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.0820, 
noisyNet noise sample is [array([0.47762156], dtype=float32), -0.20079817]. 
=============================================
[2019-04-08 15:04:26,961] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 54.0, 55.0, 26.5, 19.0, 19.42002082122724, -0.9817995184726117, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.4, 30.0], 
sim time this is 662400.0000, 
sim time next is 663000.0000, 
raw observation next is [-0.7, 54.5, 45.66666666666666, 22.66666666666666, 19.0, 19.46827416330059, -0.9857561912018125, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.443213296398892, 0.545, 0.1522222222222222, 0.02504604051565377, 0.08333333333333333, 0.12235618027504902, 0.17141460293272917, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33873475], dtype=float32), -2.1747026]. 
=============================================
[2019-04-08 15:04:26,979] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[12.613561]
 [12.567929]
 [12.882622]
 [12.624242]
 [12.931722]], R is [[13.72174549]
 [14.58452797]
 [14.98368454]
 [15.833848  ]
 [15.73120213]].
[2019-04-08 15:04:27,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [0.00220756 0.04889863 0.37553877 0.36529967 0.01763752 0.0218127
 0.07119542 0.00549875 0.00072627 0.00073131 0.09045339], sum to 1.0000
[2019-04-08 15:04:27,161] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5507
[2019-04-08 15:04:27,173] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 19.0, 19.83069957345148, -0.972981783145185, 0.0, 1.0, 20.0, 0.0], 
current ob forecast is [], 
actual action is [1.9, 30.0], 
sim time this is 678600.0000, 
sim time next is 679200.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 19.7545760977944, -0.9913448865660417, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.14621467481619987, 0.1695517044779861, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6717303], dtype=float32), 0.18184103]. 
=============================================
[2019-04-08 15:04:28,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.87074102e-03 3.79769839e-02 2.38403842e-01 4.94026184e-01
 1.11962119e-02 1.80256423e-02 9.05298889e-02 4.83055599e-03
 7.86080956e-04 1.99094429e-04 1.02154754e-01], sum to 1.0000
[2019-04-08 15:04:28,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4326
[2019-04-08 15:04:28,171] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 73.0, 0.0, 0.0, 19.0, 18.44889295710972, -1.221766220387684, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 30.0], 
sim time this is 696000.0000, 
sim time next is 696600.0000, 
raw observation next is [-3.4, 73.5, 0.0, 0.0, 19.0, 18.45392594219887, -1.222160761694664, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.735, 0.0, 0.0, 0.08333333333333333, 0.03782716184990598, 0.09261307943511203, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.0503, 
noisyNet noise sample is [array([0.9592622], dtype=float32), -1.8083684]. 
=============================================
[2019-04-08 15:04:28,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2923319e-05 4.1316682e-03 2.5584480e-01 6.7462337e-01 2.0138149e-03
 2.3234009e-03 4.1718006e-02 2.2732934e-04 1.2346704e-05 2.4075812e-06
 1.9059882e-02], sum to 1.0000
[2019-04-08 15:04:28,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1935
[2019-04-08 15:04:28,579] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 48.33333333333334, 96.0, 719.0, 22.5, 22.62384743330146, -0.3061809418926816, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.5, 25.0], 
sim time this is 739200.0000, 
sim time next is 739800.0000, 
raw observation next is [0.5, 47.5, 89.0, 773.0, 22.5, 22.78831820997857, -0.280064189452461, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4764542936288089, 0.475, 0.2966666666666667, 0.8541436464088398, 0.375, 0.39902651749821416, 0.406645270182513, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47285965], dtype=float32), -1.0574107]. 
=============================================
[2019-04-08 15:04:29,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1138909e-05 7.4230069e-03 1.4441518e-01 7.6793808e-01 1.7237845e-03
 2.7382653e-03 2.7285948e-02 9.6337819e-05 5.8164328e-06 1.6985402e-06
 4.8350811e-02], sum to 1.0000
[2019-04-08 15:04:29,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6448
[2019-04-08 15:04:29,196] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.08333333333333331, 46.66666666666667, 81.66666666666667, 486.3333333333334, 22.5, 22.83455641932384, -0.3115064656789038, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.083333333333333, 30.0], 
sim time this is 744600.0000, 
sim time next is 745200.0000, 
raw observation next is [0.0, 47.0, 82.5, 372.5, 22.5, 22.62205361279783, -0.3478486341345897, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.275, 0.4116022099447514, 0.375, 0.38517113439981926, 0.3840504552884701, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08721469], dtype=float32), -0.86428505]. 
=============================================
[2019-04-08 15:04:30,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.5950834e-05 1.1591276e-02 1.3712870e-01 7.5638121e-01 2.0015261e-03
 2.8152138e-03 5.6856740e-02 1.1167293e-04 1.8048881e-05 3.1332027e-06
 3.3046510e-02], sum to 1.0000
[2019-04-08 15:04:30,146] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4635
[2019-04-08 15:04:30,184] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 67.33333333333334, 132.6666666666667, 64.83333333333334, 22.5, 21.20401517152533, -0.7644948295216342, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.666666666666667, 30.0], 
sim time this is 728400.0000, 
sim time next is 729000.0000, 
raw observation next is [-1.15, 67.0, 139.0, 68.0, 22.5, 21.25133003023797, -0.7535153628610877, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4307479224376732, 0.67, 0.4633333333333333, 0.07513812154696133, 0.375, 0.2709441691864975, 0.24882821237963745, 1.0, 1.0, 0.3, 0.0], 
reward next is 0.2745, 
noisyNet noise sample is [array([0.0389225], dtype=float32), 0.07803487]. 
=============================================
[2019-04-08 15:04:30,191] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[20.305996]
 [20.00823 ]
 [19.233105]
 [18.98469 ]
 [19.098768]], R is [[20.31271553]
 [20.33994484]
 [20.29462814]
 [20.53989029]
 [20.73096085]].
[2019-04-08 15:04:31,009] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4652482e-05 1.7267495e-02 5.1885378e-01 3.9173317e-01 2.6491170e-03
 1.4643443e-03 1.3362059e-02 9.0146881e-05 1.0791253e-05 3.3078736e-06
 5.4551121e-02], sum to 1.0000
[2019-04-08 15:04:31,009] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8290
[2019-04-08 15:04:31,047] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.9666666666666666, 46.5, 73.66666666666666, 12.33333333333333, 22.5, 22.11322715752436, -0.5045590662083818, 1.0, 1.0, 30.0, 18916.63941657638], 
current ob forecast is [], 
actual action is [4.033333333333333, 30.0], 
sim time this is 749400.0000, 
sim time next is 750000.0000, 
raw observation next is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 22.5, 22.16125563295621, -0.5760050651529007, 1.0, 1.0, 30.0, 19255.36241180034], 
processed observation next is [1.0, 0.6956521739130435, 0.42566943674976926, 0.48, 0.23611111111111113, 0.008471454880294658, 0.375, 0.34677130274635076, 0.30799831161569974, 1.0, 1.0, 0.3, 0.091692201960954], 
reward next is 0.4003, 
noisyNet noise sample is [array([-2.1794736], dtype=float32), -1.569158]. 
=============================================
[2019-04-08 15:04:31,060] A3C_AGENT_WORKER-Thread-7 INFO:Evaluating...
[2019-04-08 15:04:31,065] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[22.330112]
 [22.27524 ]
 [22.37915 ]
 [22.38632 ]
 [22.468294]], R is [[22.40516853]
 [23.09514618]
 [23.7905159 ]
 [24.37887192]
 [25.0642662 ]].
[2019-04-08 15:04:31,068] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:04:31,068] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:04:31,069] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:04:31,070] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:04:31,071] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:04:31,070] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:04:31,074] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run2
[2019-04-08 15:04:31,074] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run2
[2019-04-08 15:04:31,109] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run2
[2019-04-08 15:06:25,257] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6963.8168 212455248.9148 416.9164
[2019-04-08 15:06:25,290] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:06:25,290] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:06:25,402] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:06:25,402] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:06:40,683] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6724.3352 225341211.3582 -334.4429
[2019-04-08 15:06:40,704] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:06:40,704] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:06:40,834] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:06:40,834] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:06:44,160] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6479.6674 232618936.3134 -862.3499
[2019-04-08 15:06:44,180] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:06:44,180] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:06:44,295] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:06:44,295] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:06:45,182] A3C_AGENT_WORKER-Thread-7 INFO:Global step: 20000, evaluation results [20000.0, 6724.335227626312, 225341211.3582443, -334.4428833555324, 6963.816752750078, 212455248.91481417, 416.9163707503649, 6479.667424490219, 232618936.31342414, -862.3499289932174]
[2019-04-08 15:06:45,723] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4615521e-05 5.6198062e-03 2.5130826e-01 6.8450886e-01 1.7019600e-03
 9.3317230e-04 3.0994374e-02 3.0035205e-04 2.9680767e-05 4.1724015e-06
 2.4564698e-02], sum to 1.0000
[2019-04-08 15:06:45,727] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4983
[2019-04-08 15:06:45,745] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 54.5, 0.0, 0.0, 22.5, 20.13488569795277, -0.8465227936376482, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [1.1, 30.0], 
sim time this is 757800.0000, 
sim time next is 758400.0000, 
raw observation next is [-3.899999999999999, 54.0, 0.0, 0.0, 22.5, 20.05745328673415, -0.8511948501248457, 1.0, 1.0, 30.0, 40339.38597159616], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.54, 0.0, 0.0, 0.375, 0.1714544405611793, 0.21626838329171807, 1.0, 1.0, 0.3, 0.1920923141504579], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48949546], dtype=float32), -2.6008604]. 
=============================================
[2019-04-08 15:06:45,864] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6258702e-05 1.0873411e-02 2.5410599e-01 6.4621705e-01 1.3461956e-03
 6.5927143e-04 3.6395270e-02 6.4132502e-05 1.4771084e-05 9.8175333e-06
 5.0287805e-02], sum to 1.0000
[2019-04-08 15:06:45,864] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3246
[2019-04-08 15:06:45,899] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.2, 59.0, 0.0, 0.0, 19.0, 20.31282504149552, -0.8017823571213154, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-0.20000000000000018, 30.0], 
sim time this is 764400.0000, 
sim time next is 765000.0000, 
raw observation next is [-5.3, 59.5, 0.0, 0.0, 19.0, 20.20226079270591, -0.8215523902924143, 0.0, 1.0, 30.0, 32890.52015247337], 
processed observation next is [1.0, 0.8695652173913043, 0.31578947368421056, 0.595, 0.0, 0.0, 0.08333333333333333, 0.18352173272549255, 0.22614920323586188, 0.0, 1.0, 0.3, 0.15662152453558748], 
reward next is 0.8434, 
noisyNet noise sample is [array([-1.599954], dtype=float32), -0.1314549]. 
=============================================
[2019-04-08 15:06:45,910] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[21.004429]
 [21.451511]
 [21.429308]
 [21.337414]
 [21.795609]], R is [[21.47179413]
 [22.25707626]
 [22.03450584]
 [21.8141613 ]
 [21.59601974]].
[2019-04-08 15:06:45,964] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.40162320e-05 1.55329574e-02 3.98556054e-01 4.44353282e-01
 1.25040347e-03 1.33441051e-03 3.73780988e-02 2.67194322e-04
 1.82861313e-05 1.81907799e-05 1.01207085e-01], sum to 1.0000
[2019-04-08 15:06:45,964] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9588
[2019-04-08 15:06:45,989] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.7, 61.5, 0.0, 0.0, 19.0, 19.93829927967436, -0.8814119632927159, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [-0.7000000000000002, 65.0], 
sim time this is 767400.0000, 
sim time next is 768000.0000, 
raw observation next is [-5.8, 62.0, 0.0, 0.0, 19.0, 19.89492526073985, -0.8800766234368725, 0.0, 1.0, 65.0, 196736.2523619935], 
processed observation next is [1.0, 0.9130434782608695, 0.30193905817174516, 0.62, 0.0, 0.0, 0.08333333333333333, 0.1579104383949875, 0.2066411255210425, 0.0, 1.0, 1.0, 0.9368392969618738], 
reward next is 0.0632, 
noisyNet noise sample is [array([0.9463152], dtype=float32), -1.1436101]. 
=============================================
[2019-04-08 15:06:46,003] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[19.888092]
 [20.286463]
 [20.114866]
 [20.474474]
 [20.55443 ]], R is [[20.07720566]
 [20.7874794 ]
 [21.5796051 ]
 [22.36380959]
 [23.14017105]].
[2019-04-08 15:06:46,299] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2728564e-03 1.1087249e-02 1.5715393e-01 7.0318687e-01 5.0909668e-03
 7.3706242e-03 7.4671790e-02 1.4808706e-03 3.7487617e-04 7.6878699e-05
 3.8233027e-02], sum to 1.0000
[2019-04-08 15:06:46,308] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1882
[2019-04-08 15:06:46,318] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.300000000000001, 71.0, 0.0, 0.0, 19.0, 18.22804726688226, -1.236290446884291, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-2.3000000000000007, 30.0], 
sim time this is 796800.0000, 
sim time next is 797400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 18.24186386758793, -1.237822620473666, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.020155322298994232, 0.08739245984211134, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.1382, 
noisyNet noise sample is [array([-0.05576807], dtype=float32), -0.4743997]. 
=============================================
[2019-04-08 15:06:46,501] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.9891111e-05 3.6683688e-03 3.4431246e-01 5.8632171e-01 7.3280249e-04
 1.8390924e-03 2.1248570e-02 6.4670305e-05 9.0278700e-06 1.1483858e-06
 4.1762304e-02], sum to 1.0000
[2019-04-08 15:06:46,505] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3653
[2019-04-08 15:06:46,556] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 56.0, 0.0, 0.0, 22.5, 21.65222002285632, -0.5660425697372705, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [1.1, 30.0], 
sim time this is 756000.0000, 
sim time next is 756600.0000, 
raw observation next is [-3.9, 55.5, 0.0, 0.0, 22.5, 21.57244851609143, -0.6871297107294994, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.555, 0.0, 0.0, 0.375, 0.2977040430076192, 0.27095676309016686, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25152776], dtype=float32), -0.59602636]. 
=============================================
[2019-04-08 15:06:46,768] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7992677e-04 1.3319689e-02 2.1060298e-01 6.3240153e-01 4.3361764e-03
 1.0093914e-02 7.8836076e-02 1.8312752e-03 1.9561358e-04 9.9694633e-05
 4.7603197e-02], sum to 1.0000
[2019-04-08 15:06:46,768] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9876
[2019-04-08 15:06:46,791] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.300000000000001, 73.66666666666667, 0.0, 0.0, 19.0, 18.50076419207864, -1.218510032926842, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-2.3000000000000007, 65.0], 
sim time this is 793200.0000, 
sim time next is 793800.0000, 
raw observation next is [-7.3, 73.0, 0.0, 0.0, 19.0, 18.46258138493484, -1.208304676007036, 0.0, 1.0, 65.0, 199053.3197902328], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.73, 0.0, 0.0, 0.08333333333333333, 0.038548448744569974, 0.09723177466432135, 0.0, 1.0, 1.0, 0.947872951382061], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1157382], dtype=float32), 2.8918571]. 
=============================================
[2019-04-08 15:06:47,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1476503e-04 1.1834353e-02 1.8945867e-01 6.6531116e-01 6.6024568e-03
 4.4776229e-03 5.7633284e-02 1.2733325e-03 9.7231394e-05 3.9654369e-05
 6.2957481e-02], sum to 1.0000
[2019-04-08 15:06:47,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4848
[2019-04-08 15:06:47,896] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.55, 74.5, 0.0, 0.0, 19.0, 18.96962629494292, -1.121679768816838, 0.0, 1.0, 45.0, 94675.24356727947], 
current ob forecast is [], 
actual action is [-2.55, 30.0], 
sim time this is 790200.0000, 
sim time next is 790800.0000, 
raw observation next is [-7.466666666666667, 74.66666666666667, 0.0, 0.0, 19.0, 18.95090003578719, -1.127850405109568, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.25577100646352724, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.07924166964893242, 0.12404986496347736, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.9904, 
noisyNet noise sample is [array([-0.5020647], dtype=float32), 1.9831315]. 
=============================================
[2019-04-08 15:06:48,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6019682e-05 1.0575895e-02 2.2452804e-01 7.0210803e-01 3.2595939e-03
 1.5460509e-03 4.2053677e-02 1.6735004e-04 1.3241994e-05 1.5766288e-06
 1.5680531e-02], sum to 1.0000
[2019-04-08 15:06:48,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2337
[2019-04-08 15:06:48,261] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 22.5, 20.1753745862055, -0.9678816719162366, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-0.06666666666666643, 25.0], 
sim time this is 816000.0000, 
sim time next is 816600.0000, 
raw observation next is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 22.5, 20.28143028989079, -0.9527820940975071, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3301015697137581, 0.7166666666666667, 0.31555555555555553, 0.0, 0.375, 0.19011919082423243, 0.18240596863416428, 1.0, 1.0, 0.2, 0.0], 
reward next is 0.3775, 
noisyNet noise sample is [array([-0.15155329], dtype=float32), 1.0272704]. 
=============================================
[2019-04-08 15:06:48,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6832736e-04 1.4952798e-02 3.1996244e-01 5.3958404e-01 5.4594339e-03
 1.0481471e-02 6.5678097e-02 1.7662657e-03 1.4420942e-04 7.1188624e-05
 4.1631732e-02], sum to 1.0000
[2019-04-08 15:06:48,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8837
[2019-04-08 15:06:48,642] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5709538e-05 5.5856081e-03 1.7919500e-01 7.1255571e-01 1.4144526e-03
 8.5956597e-04 2.2406917e-02 8.3401108e-05 2.7128624e-06 2.7219198e-06
 7.7878207e-02], sum to 1.0000
[2019-04-08 15:06:48,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6189
[2019-04-08 15:06:48,648] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 5, 
current raw observation is [-6.7, 71.0, 0.0, 0.0, 22.5, 18.06104141581998, -1.308956663726781, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 40.0], 
sim time this is 804600.0000, 
sim time next is 805200.0000, 
raw observation next is [-6.700000000000001, 72.33333333333334, 0.0, 0.0, 22.5, 18.13550428763831, -1.291181799785087, 1.0, 1.0, 40.0, 126439.6481369082], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.7233333333333334, 0.0, 0.0, 0.375, 0.011292023969859066, 0.06960606673830434, 1.0, 1.0, 0.5, 0.6020935625567058], 
reward next is 0.4444, 
noisyNet noise sample is [array([1.3263981], dtype=float32), 0.18086195]. 
=============================================
[2019-04-08 15:06:48,684] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 71.0, 106.3333333333333, 0.0, 22.5, 20.7943198890668, -0.8363649326536992, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 820200.0000, 
sim time next is 820800.0000, 
raw observation next is [-4.5, 71.0, 104.5, 0.0, 22.5, 20.89291810017436, -0.8324195115460679, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.71, 0.34833333333333333, 0.0, 0.375, 0.2410765083478633, 0.22252682948464406, 1.0, 1.0, 0.3, 0.0], 
reward next is 0.0986, 
noisyNet noise sample is [array([0.27657595], dtype=float32), 1.2779939]. 
=============================================
[2019-04-08 15:06:49,321] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7779983e-04 4.3169321e-03 3.0492350e-01 6.0662681e-01 4.2170896e-03
 1.9603958e-03 4.7579493e-02 3.0548012e-04 2.4294024e-05 2.6355878e-05
 2.9841764e-02], sum to 1.0000
[2019-04-08 15:06:49,321] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8764
[2019-04-08 15:06:49,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5466057e-05 5.0748554e-03 2.4746236e-01 6.8626136e-01 1.6005294e-03
 1.2083118e-03 3.0203087e-02 4.9934188e-05 1.1520713e-05 1.6255850e-06
 2.8111057e-02], sum to 1.0000
[2019-04-08 15:06:49,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2954
[2019-04-08 15:06:49,341] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.9, 62.5, 0.0, 0.0, 19.0, 19.64156578859114, -0.9336732059997046, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-0.9000000000000004, 30.0], 
sim time this is 768600.0000, 
sim time next is 769200.0000, 
raw observation next is [-6.0, 63.0, 0.0, 0.0, 19.0, 19.56537013091457, -0.9487757102472529, 0.0, 1.0, 30.0, 31493.76464079057], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.63, 0.0, 0.0, 0.08333333333333333, 0.13044751090954745, 0.18374142991758236, 0.0, 1.0, 0.3, 0.14997030781328843], 
reward next is 0.8500, 
noisyNet noise sample is [array([-0.04109179], dtype=float32), -0.59635955]. 
=============================================
[2019-04-08 15:06:49,342] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.299999999999999, 71.0, 0.0, 0.0, 19.0, 19.28216893614747, -1.030261549877817, 0.0, 1.0, 30.0, 26374.61723636681], 
current ob forecast is [], 
actual action is [-2.299999999999999, 30.0], 
sim time this is 780600.0000, 
sim time next is 781200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 19.20755886776202, -1.04210021616113, 0.0, 1.0, 30.0, 24154.74875574116], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.10062990564683509, 0.15263326127962337, 0.0, 1.0, 0.3, 0.11502261312257694], 
reward next is 0.8850, 
noisyNet noise sample is [array([1.2593372], dtype=float32), 1.2214905]. 
=============================================
[2019-04-08 15:06:50,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1018369e-05 1.9736355e-03 3.4243390e-01 6.0111719e-01 8.8077789e-04
 4.5223950e-04 2.8422495e-02 7.1219663e-05 7.5697958e-06 1.3170400e-06
 2.4628578e-02], sum to 1.0000
[2019-04-08 15:06:50,187] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8456
[2019-04-08 15:06:50,259] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 22.5, 21.04832047656887, -0.8277615801406092, 1.0, 1.0, 45.00000000000004, 196436.4629431123], 
current ob forecast is [], 
actual action is [1.100000000000001, 30.0], 
sim time this is 837600.0000, 
sim time next is 838200.0000, 
raw observation next is [-3.9, 85.33333333333334, 19.33333333333334, 0.0, 22.5, 20.88656562389546, -0.7624892089471399, 1.0, 1.0, 30.00000000000004, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8533333333333334, 0.06444444444444447, 0.0, 0.375, 0.2405471353246217, 0.24583693035095336, 1.0, 1.0, 0.30000000000000077, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2529338], dtype=float32), 0.08223096]. 
=============================================
[2019-04-08 15:06:51,100] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6839068e-05 3.0698923e-03 1.9210567e-01 6.8514210e-01 2.3640816e-03
 1.0055897e-03 4.1995734e-02 1.5015197e-04 8.6197851e-06 1.5039745e-06
 7.4109770e-02], sum to 1.0000
[2019-04-08 15:06:51,100] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2055
[2019-04-08 15:06:51,149] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 75.0, 99.0, 0.0, 22.5, 20.5608983128831, -0.9245977742807332, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 22.5, 20.4728598615196, -0.9289818368212508, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333333, 0.32555555555555554, 0.0, 0.375, 0.2060716551266332, 0.19033938772624973, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1407284], dtype=float32), 0.85273665]. 
=============================================
[2019-04-08 15:06:51,365] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8694922e-05 3.1249356e-03 3.1532526e-01 6.0445845e-01 4.5389796e-04
 1.0316138e-03 5.6082781e-02 3.4057219e-05 1.1689849e-06 2.7104426e-07
 1.9458879e-02], sum to 1.0000
[2019-04-08 15:06:51,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7439
[2019-04-08 15:06:51,409] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 84.66666666666667, 0.0, 0.0, 22.5, 22.08251326159099, -0.5947059505923146, 1.0, 1.0, 29.99999999999999, 0.0], 
current ob forecast is [], 
actual action is [1.1, 30.0], 
sim time this is 840000.0000, 
sim time next is 840600.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 22.5, 21.56925958713368, -0.6622345283163348, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.84, 0.0, 0.0, 0.375, 0.2974382989278066, 0.2792551572278884, 1.0, 1.0, 0.3, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3051142], dtype=float32), -2.157948]. 
=============================================
[2019-04-08 15:06:51,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1844829e-04 1.2125821e-02 1.7868279e-01 7.2351748e-01 4.6275225e-03
 4.2294604e-03 3.0869765e-02 3.5984933e-04 4.1575298e-05 3.3178756e-05
 4.5394219e-02], sum to 1.0000
[2019-04-08 15:06:51,862] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3461
[2019-04-08 15:06:51,928] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.45, 75.0, 32.0, 0.0, 22.5, 18.96574318467621, -1.17247324075662, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.4500000000000002, 30.0], 
sim time this is 808200.0000, 
sim time next is 808800.0000, 
raw observation next is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 22.5, 19.21470016174735, -1.142905284473928, 1.0, 1.0, 30.00000000000001, 18680.41167023054], 
processed observation next is [1.0, 0.34782608695652173, 0.28624192059095105, 0.75, 0.12277777777777776, 0.0, 0.375, 0.1012250134789457, 0.119031571842024, 1.0, 1.0, 0.3000000000000002, 0.0889543412868121], 
reward next is 0.7392, 
noisyNet noise sample is [array([-0.10141226], dtype=float32), 2.622557]. 
=============================================
[2019-04-08 15:06:53,582] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0400750e-05 6.5409997e-03 1.0475611e-01 8.0727029e-01 9.4927737e-04
 2.6374464e-03 3.9124180e-02 6.5981418e-05 4.0821583e-06 4.2212517e-07
 3.8630836e-02], sum to 1.0000
[2019-04-08 15:06:53,582] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9241
[2019-04-08 15:06:53,696] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 22.5, 21.28562424047608, -0.7561103680838984, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.5, 45.0], 
sim time this is 823200.0000, 
sim time next is 823800.0000, 
raw observation next is [-4.5, 77.66666666666667, 96.33333333333334, 0.0, 22.5, 21.23427064956018, -0.8043853321000011, 1.0, 1.0, 45.00000000000007, 187986.0534285427], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7766666666666667, 0.3211111111111111, 0.0, 0.375, 0.269522554130015, 0.2318715559666663, 1.0, 1.0, 0.6000000000000014, 0.8951716829930605], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6928091], dtype=float32), 1.1552572]. 
=============================================
[2019-04-08 15:06:53,723] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9181085e-04 2.1877388e-02 3.5774368e-01 4.6695799e-01 5.9586954e-03
 2.4776557e-03 6.7106619e-02 8.2293223e-04 1.0741666e-04 6.9334492e-05
 7.6686449e-02], sum to 1.0000
[2019-04-08 15:06:53,725] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.46221007e-05 2.00334601e-02 1.46784186e-01 6.99503303e-01
 1.95650826e-03 7.47064478e-04 3.02547719e-02 1.47765430e-04
 2.72147167e-06 1.06535333e-06 1.00544505e-01], sum to 1.0000
[2019-04-08 15:06:53,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8388
[2019-04-08 15:06:53,734] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9466
[2019-04-08 15:06:53,775] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 17.83299642057759, -1.41818770102492, 1.0, 1.0, 30.0, 25979.55881047992], 
current ob forecast is [], 
actual action is [5.0, 30.0], 
sim time this is 891600.0000, 
sim time next is 892200.0000, 
raw observation next is [0.0, 72.0, 9.666666666666664, 0.0, 22.5, 17.78672878141202, -1.420301818161201, 1.0, 1.0, 30.0, 23883.38261578174], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.032222222222222215, 0.0, 0.375, -0.017772601548998363, 0.026566060612932985, 1.0, 1.0, 0.3, 0.11373039340848447], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.882364], dtype=float32), 1.3322222]. 
=============================================
[2019-04-08 15:06:53,801] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 83.33333333333334, 32.33333333333333, 0.0, 22.5, 21.27477751438176, -0.7434955946617005, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 836400.0000, 
sim time next is 837000.0000, 
raw observation next is [-3.9, 84.0, 29.0, 0.0, 22.5, 21.28037683775722, -0.7225276764142303, 1.0, 1.0, 65.0, 197103.8449421317], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.84, 0.09666666666666666, 0.0, 0.375, 0.2733647364797684, 0.2591574411952566, 1.0, 1.0, 1.0, 0.9385897378196747], 
reward next is 0.5242, 
noisyNet noise sample is [array([0.50532377], dtype=float32), 0.59598017]. 
=============================================
[2019-04-08 15:06:53,807] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[26.02498 ]
 [25.781034]
 [26.036463]
 [25.641703]
 [25.51505 ]], R is [[26.5217495 ]
 [26.28557587]
 [26.02272034]
 [25.76249313]
 [25.91057396]].
[2019-04-08 15:06:54,246] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 23402: loss 8.5114
[2019-04-08 15:06:54,249] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 23403: learning rate 0.0000
[2019-04-08 15:06:54,328] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 23432: loss 7.3824
[2019-04-08 15:06:54,329] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 23433: learning rate 0.0000
[2019-04-08 15:06:54,464] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23493: loss 6.8084
[2019-04-08 15:06:54,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23493: learning rate 0.0000
[2019-04-08 15:06:54,579] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23533: loss 27.3055
[2019-04-08 15:06:54,580] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23533: learning rate 0.0000
[2019-04-08 15:06:54,730] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23576: loss 1.3885
[2019-04-08 15:06:54,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23576: learning rate 0.0000
[2019-04-08 15:06:54,742] A3C_AGENT_WORKER-Thread-8 INFO:Local step 1500, global step 23583: loss 3.8011
[2019-04-08 15:06:54,754] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 1500, global step 23584: learning rate 0.0000
[2019-04-08 15:06:54,940] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.9903266e-05 1.8649684e-02 3.8853857e-01 4.1619077e-01 1.1458985e-03
 1.7837379e-03 5.4586519e-02 1.1371692e-04 9.0135463e-06 2.0216301e-06
 1.1896004e-01], sum to 1.0000
[2019-04-08 15:06:54,946] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0193
[2019-04-08 15:06:54,994] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.1, 84.0, 72.16666666666667, 0.0, 22.5, 20.10214462931452, -0.9949922149105621, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 25.0], 
sim time this is 901200.0000, 
sim time next is 901800.0000, 
raw observation next is [1.1, 84.0, 77.0, 0.0, 22.5, 20.18976243547574, -0.9916344916937323, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.25666666666666665, 0.0, 0.375, 0.1824802029563116, 0.16945516943542258, 1.0, 1.0, 0.2, 0.0], 
reward next is 0.0839, 
noisyNet noise sample is [array([-0.3636963], dtype=float32), 1.3088018]. 
=============================================
[2019-04-08 15:06:55,188] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 23716: loss 5.2492
[2019-04-08 15:06:55,204] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 23716: learning rate 0.0000
[2019-04-08 15:06:55,610] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23835: loss 17.4638
[2019-04-08 15:06:55,611] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23835: learning rate 0.0000
[2019-04-08 15:06:55,821] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23896: loss 32.0990
[2019-04-08 15:06:55,822] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23896: learning rate 0.0000
[2019-04-08 15:06:56,177] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 23996: loss -0.4076
[2019-04-08 15:06:56,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 23996: learning rate 0.0000
[2019-04-08 15:06:56,255] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24019: loss 4.0374
[2019-04-08 15:06:56,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24019: learning rate 0.0000
[2019-04-08 15:06:56,336] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24040: loss 3.8475
[2019-04-08 15:06:56,337] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24040: learning rate 0.0000
[2019-04-08 15:06:56,473] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.7724919e-05 4.6206568e-03 1.1288384e-01 8.0138844e-01 2.8194361e-03
 1.3180530e-03 3.2655831e-02 4.1045679e-04 5.5462660e-05 1.1845565e-05
 4.3778323e-02], sum to 1.0000
[2019-04-08 15:06:56,482] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9037
[2019-04-08 15:06:56,501] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 18.60763189786524, -1.185396105057526, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.3, 30.0], 
sim time this is 871200.0000, 
sim time next is 871800.0000, 
raw observation next is [-1.7, 79.00000000000001, 0.0, 0.0, 19.0, 18.69059292016148, -1.17820801504564, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.7900000000000001, 0.0, 0.0, 0.08333333333333333, 0.057549410013456814, 0.10726399498478667, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72945386], dtype=float32), 0.18192066]. 
=============================================
[2019-04-08 15:06:56,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7356602e-06 1.1742527e-03 1.5456872e-01 7.9856765e-01 5.4689357e-04
 6.2714517e-04 1.2452145e-02 1.3815297e-04 2.0432255e-06 6.6478981e-07
 3.1917680e-02], sum to 1.0000
[2019-04-08 15:06:56,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3738
[2019-04-08 15:06:56,818] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.1, 81.0, 0.0, 0.0, 19.0, 19.6423581517806, -0.9808726460282214, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.9, 30.0], 
sim time this is 858600.0000, 
sim time next is 859200.0000, 
raw observation next is [-3.0, 80.33333333333334, 0.0, 0.0, 19.0, 19.58533543915726, -0.9891827217839824, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.13211128659643823, 0.17027242607200588, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10371844], dtype=float32), -0.41821855]. 
=============================================
[2019-04-08 15:06:57,043] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24258: loss 10.7720
[2019-04-08 15:06:57,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24258: learning rate 0.0000
[2019-04-08 15:06:57,951] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24559: loss 27.9020
[2019-04-08 15:06:57,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24559: learning rate 0.0000
[2019-04-08 15:06:58,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.9545523e-07 1.1422337e-03 9.6085839e-02 8.2990181e-01 1.6436208e-04
 1.2228945e-04 5.4031380e-02 3.7096054e-06 2.1132088e-07 1.8642217e-08
 1.8547198e-02], sum to 1.0000
[2019-04-08 15:06:58,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8375
[2019-04-08 15:06:58,136] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.50039909e-06 5.18522086e-03 2.36020401e-01 6.01010561e-01
 3.35500896e-04 1.02315018e-04 3.25477384e-02 8.97637619e-06
 1.73360604e-07 6.89895607e-09 1.24787584e-01], sum to 1.0000
[2019-04-08 15:06:58,136] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5158
[2019-04-08 15:06:58,154] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.4, 93.0, 60.0, 0.0, 22.5, 22.4208474006595, -0.432785250073566, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [9.4, 30.0], 
sim time this is 919200.0000, 
sim time next is 919800.0000, 
raw observation next is [4.4, 93.0, 54.0, 0.0, 22.5, 22.53558653342561, -0.5042523775249123, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.18, 0.0, 0.375, 0.3779655444521343, 0.3319158741583626, 1.0, 1.0, 0.3, 0.0], 
reward next is 0.9982, 
noisyNet noise sample is [array([-0.54336053], dtype=float32), 1.7616262]. 
=============================================
[2019-04-08 15:06:58,183] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 92.0, 9.0, 0.0, 22.5, 23.80780728751297, -0.2312673379958844, 1.0, 1.0, 30.00000000000009, 0.0], 
current ob forecast is [], 
actual action is [10.0, 30.0], 
sim time this is 925200.0000, 
sim time next is 925800.0000, 
raw observation next is [4.9, 92.66666666666667, 0.0, 0.0, 22.5, 23.42303424616161, -0.1711944880660402, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5983379501385043, 0.9266666666666667, 0.0, 0.0, 0.375, 0.45191952051346745, 0.4429351706446532, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9050584], dtype=float32), -0.35665125]. 
=============================================
[2019-04-08 15:06:58,729] A3C_AGENT_WORKER-Thread-7 INFO:Local step 1500, global step 24868: loss 3.7021
[2019-04-08 15:06:58,730] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 1500, global step 24868: learning rate 0.0000
[2019-04-08 15:06:59,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7397416e-05 3.3312191e-03 2.0286256e-01 5.7336205e-01 1.0631671e-03
 1.1388531e-03 1.2923767e-01 4.0193668e-04 6.9433891e-06 1.6860452e-06
 8.8566445e-02], sum to 1.0000
[2019-04-08 15:06:59,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3681
[2019-04-08 15:06:59,133] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.0, 83.0, 0.0, 0.0, 19.0, 20.29597141576661, -0.765032497841997, 0.0, 1.0, 45.0, 196827.5122954909], 
current ob forecast is [], 
actual action is [14.0, 30.0], 
sim time this is 972600.0000, 
sim time next is 973200.0000, 
raw observation next is [9.200000000000001, 83.0, 0.0, 0.0, 19.0, 20.36748158895467, -0.7537516360625024, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7174515235457064, 0.83, 0.0, 0.0, 0.08333333333333333, 0.19729013241288929, 0.24874945464583254, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76239413], dtype=float32), -0.7886657]. 
=============================================
[2019-04-08 15:06:59,222] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7971790e-07 7.0736702e-03 5.6925118e-01 3.6629015e-01 2.7931106e-04
 2.3839055e-04 2.9228872e-02 1.6133041e-05 4.6066415e-08 3.8572519e-09
 2.7621910e-02], sum to 1.0000
[2019-04-08 15:06:59,224] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1916
[2019-04-08 15:06:59,252] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 19.0, 22.01822634642093, -0.4103857555535534, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [10.0, 25.0], 
sim time this is 937200.0000, 
sim time next is 937800.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 19.0, 21.94745096468201, -0.4258840306602855, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.08333333333333333, 0.32895424705683407, 0.35803865644657146, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.02382744], dtype=float32), -0.49928707]. 
=============================================
[2019-04-08 15:06:59,397] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9405016e-06 4.5882969e-04 1.3726699e-01 7.8843546e-01 7.3499512e-04
 4.4403432e-04 2.9044425e-02 1.6487282e-04 1.5528700e-06 2.7051206e-07
 4.3445621e-02], sum to 1.0000
[2019-04-08 15:06:59,400] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6131
[2019-04-08 15:06:59,420] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 19.0, 21.48502949542441, -0.484756552497123, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [13.8, 30.0], 
sim time this is 972000.0000, 
sim time next is 972600.0000, 
raw observation next is [9.0, 83.0, 0.0, 0.0, 19.0, 21.78591203541496, -0.4658277675947236, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7119113573407203, 0.83, 0.0, 0.0, 0.08333333333333333, 0.3154926696179133, 0.3447240774684255, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4526587], dtype=float32), 0.33318916]. 
=============================================
[2019-04-08 15:06:59,425] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 25196: loss 2.0498
[2019-04-08 15:06:59,426] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 25196: learning rate 0.0000
[2019-04-08 15:06:59,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8728443e-06 9.4618869e-04 9.6948549e-02 8.5042679e-01 3.4816752e-04
 5.5954175e-04 2.1344827e-02 2.2686036e-05 3.1485328e-07 1.0005888e-07
 2.9400909e-02], sum to 1.0000
[2019-04-08 15:06:59,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5201
[2019-04-08 15:06:59,525] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 19.89546963847059, -0.8528569349019114, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [10.0, 30.0], 
sim time this is 946800.0000, 
sim time next is 947400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 19.86009822251071, -0.8623445801548156, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.1550081852092259, 0.21255180661506146, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9547677], dtype=float32), 0.20267814]. 
=============================================
[2019-04-08 15:07:00,843] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.8082788e-06 3.7027523e-03 2.6652026e-01 6.5172827e-01 1.0224261e-03
 9.5194764e-04 3.0261807e-02 8.8393346e-05 1.6259094e-06 1.0946781e-06
 4.5712728e-02], sum to 1.0000
[2019-04-08 15:07:00,851] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1708
[2019-04-08 15:07:00,875] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [9.6, 83.0, 0.0, 0.0, 19.0, 21.03345043942193, -0.6318417038771147, 0.0, 1.0, 30.0, 18946.06722134101], 
current ob forecast is [], 
actual action is [14.6, 25.0], 
sim time this is 974400.0000, 
sim time next is 975000.0000, 
raw observation next is [9.8, 83.0, 0.0, 0.0, 19.0, 20.94088480347042, -0.6359963496155133, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7340720221606649, 0.83, 0.0, 0.0, 0.08333333333333333, 0.24507373362253512, 0.2880012167948289, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.69615215], dtype=float32), 1.0175275]. 
=============================================
[2019-04-08 15:07:00,886] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[27.321356]
 [26.992992]
 [27.04295 ]
 [26.989119]
 [26.853067]], R is [[28.09597015]
 [28.72479057]
 [29.43754387]
 [30.1431694 ]
 [30.84173775]].
[2019-04-08 15:07:00,961] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.14166426e-07 2.14852975e-03 6.42635047e-01 3.15302283e-01
 1.54962268e-04 8.27879776e-05 2.18548924e-02 2.11863448e-06
 6.44883471e-08 4.08069312e-09 1.78193115e-02], sum to 1.0000
[2019-04-08 15:07:00,961] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7222
[2019-04-08 15:07:01,009] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 81.0, 98.16666666666667, 0.0, 22.5, 23.05671210127961, -0.1991956473467663, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [19.4, 30.0], 
sim time this is 1002000.0000, 
sim time next is 1002600.0000, 
raw observation next is [14.4, 81.0, 94.0, 0.0, 22.5, 23.11591989859977, -0.3952716059343734, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.31333333333333335, 0.0, 0.375, 0.42632665821664756, 0.3682427980218755, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.01218055], dtype=float32), -0.7787588]. 
=============================================
[2019-04-08 15:07:01,715] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5208799e-07 8.0995681e-03 1.0713708e-01 8.4548682e-01 1.5780443e-04
 1.3384434e-05 8.8128168e-03 1.6495343e-06 5.6459437e-09 7.0507461e-10
 3.0290795e-02], sum to 1.0000
[2019-04-08 15:07:01,716] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-08 15:07:01,733] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 75.0, 41.0, 0.0, 22.5, 23.7752524958712, 0.03721280134827513, 1.0, 1.0, 25.00000000000002, 0.0], 
current ob forecast is [], 
actual action is [20.5, 30.0], 
sim time this is 1008000.0000, 
sim time next is 1008600.0000, 
raw observation next is [15.5, 75.5, 35.66666666666666, 0.0, 22.5, 24.12428357415827, 0.07923319305647801, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.755, 0.11888888888888886, 0.0, 0.375, 0.5103569645131891, 0.5264110643521593, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.72553116], dtype=float32), 0.5828635]. 
=============================================
[2019-04-08 15:07:03,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7448779e-07 1.2574391e-03 1.0585296e-01 8.1847519e-01 1.7091526e-04
 8.7700970e-04 7.7306912e-03 5.5445980e-06 6.4046993e-08 7.9341422e-09
 6.5629713e-02], sum to 1.0000
[2019-04-08 15:07:03,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4193
[2019-04-08 15:07:03,104] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 19.0, 20.77152778644348, -0.635994500399435, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [10.0, 30.0], 
sim time this is 949800.0000, 
sim time next is 950400.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 19.0, 20.78751821749592, -0.6425288154456296, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.08333333333333333, 0.23229318479132655, 0.2858237281847901, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19202569], dtype=float32), 1.3371278]. 
=============================================
[2019-04-08 15:07:03,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8924917e-09 3.0208426e-04 6.4525515e-02 9.0915632e-01 6.6243615e-06
 6.9985549e-06 1.1349431e-02 2.6080221e-07 1.5689486e-09 3.6357226e-10
 1.4652840e-02], sum to 1.0000
[2019-04-08 15:07:03,304] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9783
[2019-04-08 15:07:03,350] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8242960e-08 3.9153403e-04 7.7668792e-01 1.9020799e-01 3.2951406e-04
 4.8849543e-06 2.1439467e-02 2.7337348e-06 3.7106298e-09 6.6876266e-10
 1.0935905e-02], sum to 1.0000
[2019-04-08 15:07:03,350] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4108
[2019-04-08 15:07:03,357] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.5, 75.0, 41.0, 0.0, 22.5, 23.8852924709387, 0.06418867407018881, 1.0, 1.0, 30.00000000000002, 18680.41167023055], 
current ob forecast is [], 
actual action is [20.5, 45.0], 
sim time this is 1008000.0000, 
sim time next is 1008600.0000, 
raw observation next is [15.5, 75.5, 35.66666666666666, 0.0, 22.5, 24.23474643362251, 0.1401774754745922, 1.0, 1.0, 45.0, 196764.5955322811], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.755, 0.11888888888888886, 0.0, 0.375, 0.5195622028018759, 0.5467258251581973, 1.0, 1.0, 0.6, 0.9369742644394338], 
reward next is 0.0630, 
noisyNet noise sample is [array([-0.7274792], dtype=float32), -0.7193813]. 
=============================================
[2019-04-08 15:07:03,359] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 23.37373955974669, 0.01530568030927881, 0.0, 1.0, 65.0, 197718.645613186], 
current ob forecast is [], 
actual action is [19.4, 25.0], 
sim time this is 1026000.0000, 
sim time next is 1026600.0000, 
raw observation next is [14.4, 76.66666666666667, 0.0, 0.0, 19.0, 23.38564601223887, 0.02927951005594832, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7666666666666667, 0.0, 0.0, 0.08333333333333333, 0.44880383435323906, 0.5097598366853161, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12470693], dtype=float32), -0.04994461]. 
=============================================
[2019-04-08 15:07:04,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1666583e-09 4.8836333e-05 9.1897912e-02 8.8387764e-01 3.1337192e-06
 4.3057426e-06 6.2716664e-03 4.0554906e-07 5.1556581e-10 4.5755923e-11
 1.7896073e-02], sum to 1.0000
[2019-04-08 15:07:04,435] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6995
[2019-04-08 15:07:04,450] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 22.74197565855785, -0.1372911652830591, 0.0, 1.0, 45.0, 196871.0918312596], 
current ob forecast is [], 
actual action is [19.4, 30.0], 
sim time this is 1024800.0000, 
sim time next is 1025400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 22.74312547624215, -0.1219571228218501, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.3952604563535124, 0.45934762572605, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2536566], dtype=float32), -1.9503547]. 
=============================================
[2019-04-08 15:07:04,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5388480e-08 7.2933064e-04 1.3251519e-01 8.4838033e-01 1.2016438e-04
 9.0329459e-06 8.4433891e-03 1.8226433e-06 1.2774612e-09 2.6523512e-09
 9.8006921e-03], sum to 1.0000
[2019-04-08 15:07:04,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7939
[2019-04-08 15:07:04,708] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [12.75, 81.5, 100.0, 234.0, 22.5, 23.95923382697148, 0.1066179835231816, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [17.75, 30.0], 
sim time this is 1071000.0000, 
sim time next is 1071600.0000, 
raw observation next is [12.93333333333333, 81.0, 102.3333333333333, 195.0, 22.5, 24.07249891788515, 0.1293947878775811, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.391304347826087, 0.8208679593721145, 0.81, 0.341111111111111, 0.2154696132596685, 0.375, 0.506041576490429, 0.5431315959591937, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.308927], dtype=float32), 0.34712297]. 
=============================================
[2019-04-08 15:07:04,987] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2838832e-08 6.0578325e-04 2.2550194e-01 7.5404537e-01 2.5211711e-04
 3.6759608e-05 8.9251557e-03 3.0052158e-06 2.5579519e-08 2.8805982e-09
 1.0629860e-02], sum to 1.0000
[2019-04-08 15:07:04,994] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-08 15:07:05,004] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [12.2, 83.0, 15.0, 48.33333333333334, 22.5, 22.43100399448596, -0.2060127503308213, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [17.2, 25.0], 
sim time this is 1066200.0000, 
sim time next is 1066800.0000, 
raw observation next is [12.2, 83.0, 18.5, 58.66666666666666, 22.5, 22.47309842989733, -0.1675955682831065, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.06166666666666667, 0.06482504604051564, 0.375, 0.3727582024914442, 0.4441348105722978, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.339273], dtype=float32), 0.5267286]. 
=============================================
[2019-04-08 15:07:05,054] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9048986e-08 6.3128374e-04 8.5568748e-02 9.0419602e-01 2.1458949e-05
 5.3086619e-06 7.7408231e-03 9.0425709e-07 5.0759206e-09 1.5051258e-09
 1.8354182e-03], sum to 1.0000
[2019-04-08 15:07:05,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6569
[2019-04-08 15:07:05,074] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.3, 75.5, 0.0, 0.0, 19.0, 22.82187451140181, -0.1372866462290413, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [19.3, 30.0], 
sim time this is 1041000.0000, 
sim time next is 1041600.0000, 
raw observation next is [14.2, 76.0, 0.0, 0.0, 19.0, 22.84501945311332, -0.137778618940111, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.043478260869565216, 0.8559556786703602, 0.76, 0.0, 0.0, 0.08333333333333333, 0.4037516210927767, 0.45407379368662965, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([2.121017], dtype=float32), 0.98335946]. 
=============================================
[2019-04-08 15:07:05,290] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7824534e-09 1.1684357e-03 3.4786835e-01 6.2378198e-01 3.9248494e-05
 3.6985477e-06 6.2939660e-03 4.0431703e-07 3.9308721e-09 5.6869415e-11
 2.0843975e-02], sum to 1.0000
[2019-04-08 15:07:05,294] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5229
[2019-04-08 15:07:05,301] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.11666666666667, 80.5, 104.6666666666667, 156.0, 22.5, 23.87932919019241, 0.07554186902177273, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [18.11666666666667, 30.0], 
sim time this is 1072200.0000, 
sim time next is 1072800.0000, 
raw observation next is [13.3, 80.0, 107.0, 117.0, 22.5, 24.00622688275489, 0.093131470262125, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.8310249307479226, 0.8, 0.3566666666666667, 0.1292817679558011, 0.375, 0.5005189068962409, 0.5310438234207083, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([2.1280906], dtype=float32), 1.0506204]. 
=============================================
[2019-04-08 15:07:05,462] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8364283e-07 1.1206667e-03 2.6562130e-01 6.9427752e-01 1.1502498e-04
 1.9910171e-05 2.5968490e-02 5.5256924e-06 3.4053159e-08 7.7493461e-09
 1.2871307e-02], sum to 1.0000
[2019-04-08 15:07:05,465] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5913
[2019-04-08 15:07:05,476] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.38333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 22.70600772830531, -0.159705537212228, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [18.38333333333333, 30.0], 
sim time this is 1057800.0000, 
sim time next is 1058400.0000, 
raw observation next is [13.3, 80.0, 0.0, 0.0, 19.0, 22.6922494901353, -0.1676013084747572, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.2608695652173913, 0.8310249307479226, 0.8, 0.0, 0.0, 0.08333333333333333, 0.39102079084460833, 0.4441328971750809, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.6715447], dtype=float32), 0.06559852]. 
=============================================
[2019-04-08 15:07:06,150] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5610522e-10 4.4585735e-04 9.9973798e-02 8.8187331e-01 4.6655668e-06
 7.9595566e-07 1.7934403e-03 7.7806220e-08 2.7637848e-10 6.8019063e-11
 1.5907984e-02], sum to 1.0000
[2019-04-08 15:07:06,155] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3529
[2019-04-08 15:07:06,168] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.4, 75.0, 114.0, 0.0, 22.5, 24.7786046028707, 0.2497387676867577, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [19.4, 30.0], 
sim time this is 1074600.0000, 
sim time next is 1075200.0000, 
raw observation next is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 22.5, 24.83087339848657, 0.2688143603691275, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8716528162511544, 0.7333333333333334, 0.4577777777777776, 0.03959484346224677, 0.375, 0.5692394498738809, 0.5896047867897092, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18941733], dtype=float32), 0.51409924]. 
=============================================
[2019-04-08 15:07:06,330] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.2414547e-11 1.1178803e-05 3.2084953e-02 9.6191633e-01 1.0298903e-06
 1.2655170e-07 3.2641382e-03 8.9285335e-09 3.8431442e-11 1.6495197e-12
 2.7221434e-03], sum to 1.0000
[2019-04-08 15:07:06,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0406
[2019-04-08 15:07:06,341] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.45, 60.5, 181.0, 317.0, 22.5, 25.28925321144145, 0.4222026575743361, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [22.45, 30.0], 
sim time this is 1081800.0000, 
sim time next is 1082400.0000, 
raw observation next is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 22.5, 25.38420567427235, 0.4357179620597535, 1.0, 1.0, 30.00000000000001, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.9538319482917822, 0.59, 0.5977777777777776, 0.2918968692449356, 0.375, 0.6153504728560293, 0.6452393206865845, 1.0, 1.0, 0.3000000000000002, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.0563262], dtype=float32), 0.99796283]. 
=============================================
[2019-04-08 15:07:06,393] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.31846834e-08 9.87198000e-05 1.16793901e-01 8.60039294e-01
 9.84906364e-05 1.42943700e-05 5.29438863e-03 4.20246897e-06
 1.08938325e-08 7.21237958e-10 1.76566560e-02], sum to 1.0000
[2019-04-08 15:07:06,398] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0649
[2019-04-08 15:07:06,412] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 22.55418153492852, -0.1915596692028152, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [19.4, 25.0], 
sim time this is 1047600.0000, 
sim time next is 1048200.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 22.61024579994223, -0.1827240432757925, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.3841871499951859, 0.4390919855747358, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06224776], dtype=float32), -3.034749]. 
=============================================
[2019-04-08 15:07:06,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6680105e-10 2.8212494e-04 1.5483879e-01 8.4002060e-01 3.5466562e-06
 2.0268378e-06 8.4195699e-04 2.8579397e-08 2.9341366e-11 9.9874675e-12
 4.0109586e-03], sum to 1.0000
[2019-04-08 15:07:06,508] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6768
[2019-04-08 15:07:06,538] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [16.23333333333333, 66.66666666666666, 241.8333333333333, 232.0, 22.5, 24.96678196501519, 0.3647023863964224, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [21.23333333333333, 30.0], 
sim time this is 1078800.0000, 
sim time next is 1079400.0000, 
raw observation next is [16.41666666666667, 65.83333333333334, 229.6666666666667, 249.0, 22.5, 24.5868283331937, 0.07587665923421008, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.4782608695652174, 0.9173591874422903, 0.6583333333333334, 0.7655555555555558, 0.2751381215469613, 0.375, 0.5489023610994751, 0.5252922197447367, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.5289481], dtype=float32), -0.6759841]. 
=============================================
[2019-04-08 15:07:06,576] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5412707e-10 7.2343471e-05 3.5057396e-01 6.3987601e-01 1.8362884e-05
 2.5849749e-07 1.1105102e-03 2.0389713e-07 2.0389954e-10 3.2145486e-12
 8.3482740e-03], sum to 1.0000
[2019-04-08 15:07:06,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4256
[2019-04-08 15:07:06,592] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.55, 61.0, 0.0, 0.0, 19.0, 24.33811728094666, 0.2793139006028962, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [18.55, 30.0], 
sim time this is 1110600.0000, 
sim time next is 1111200.0000, 
raw observation next is [13.46666666666667, 61.33333333333333, 0.0, 0.0, 19.0, 24.27103772365162, 0.2665743323398506, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.8356417359187445, 0.6133333333333333, 0.0, 0.0, 0.08333333333333333, 0.5225864769709684, 0.5888581107799502, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7538463], dtype=float32), -0.9872776]. 
=============================================
[2019-04-08 15:07:07,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.4356074e-07 3.1378611e-03 1.8386346e-01 7.6569384e-01 3.1333786e-04
 4.6172376e-05 1.5460893e-02 1.5986647e-05 1.2544352e-07 4.3900219e-08
 3.1467315e-02], sum to 1.0000
[2019-04-08 15:07:07,296] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7250
[2019-04-08 15:07:07,304] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 23.29763665207386, 0.05766005834160912, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [16.1, 30.0], 
sim time this is 1134000.0000, 
sim time next is 1134600.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 19.0, 23.26355967475136, 0.05076063196093909, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.08333333333333333, 0.43862997289594663, 0.5169202106536464, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.47218156], dtype=float32), 0.55516064]. 
=============================================
[2019-04-08 15:07:07,313] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0056378e-11 1.4852052e-05 1.8777891e-01 8.0051440e-01 2.2373581e-06
 3.2490004e-08 3.6342433e-04 6.2558994e-09 1.3172074e-11 4.1368609e-13
 1.1326190e-02], sum to 1.0000
[2019-04-08 15:07:07,315] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6746
[2019-04-08 15:07:07,334] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 50.0, 18.0, 1.5, 22.5, 26.57997254436066, 0.5666940509677604, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [22.7, 30.0], 
sim time this is 1098000.0000, 
sim time next is 1098600.0000, 
raw observation next is [17.43333333333333, 50.5, 0.0, 0.0, 22.5, 26.09736362464766, 0.6169784149704344, 1.0, 1.0, 30.0, 9340.205835115268], 
processed observation next is [1.0, 0.7391304347826086, 0.9455216989843028, 0.505, 0.0, 0.0, 0.375, 0.6747803020539717, 0.7056594716568115, 1.0, 1.0, 0.3, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([0.90421844], dtype=float32), -1.577016]. 
=============================================
[2019-04-08 15:07:07,569] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3071990e-11 2.7983422e-06 1.0277482e-01 8.8945854e-01 6.2583572e-06
 4.6053049e-07 9.6936146e-04 4.9158394e-10 9.1370921e-12 2.1611360e-13
 6.7877583e-03], sum to 1.0000
[2019-04-08 15:07:07,572] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6523
[2019-04-08 15:07:07,580] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [19.4, 49.0, 63.5, 0.0, 22.5, 25.87261183237164, 0.5394038925599456, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [24.4, 30.0], 
sim time this is 1094400.0000, 
sim time next is 1095000.0000, 
raw observation next is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 22.5, 26.03962737773323, 0.5616281166719371, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9921514312096033, 0.4916666666666667, 0.18, 0.0, 0.375, 0.6699689481444357, 0.687209372223979, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12182157], dtype=float32), -0.5670689]. 
=============================================
[2019-04-08 15:07:07,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9946497e-09 1.2846148e-04 1.8577222e-02 9.7013330e-01 4.8995198e-06
 3.5172689e-06 1.8339397e-03 6.5136936e-07 6.5153738e-10 1.5512990e-10
 9.3179345e-03], sum to 1.0000
[2019-04-08 15:07:07,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9010
[2019-04-08 15:07:07,598] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.78102 ]
 [54.828518]
 [54.8254  ]
 [54.907265]
 [54.91301 ]], R is [[55.14760208]
 [55.59612656]
 [56.01051331]
 [56.42499161]
 [56.82515717]].
[2019-04-08 15:07:07,600] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.23333333333333, 73.0, 0.0, 0.0, 19.0, 23.72074182178186, 0.1677685263373934, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [16.23333333333333, 30.0], 
sim time this is 1124400.0000, 
sim time next is 1125000.0000, 
raw observation next is [11.05, 74.0, 0.0, 0.0, 19.0, 23.70368557635985, 0.162928419443063, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.0, 0.7686980609418284, 0.74, 0.0, 0.0, 0.08333333333333333, 0.47530713136332076, 0.5543094731476876, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95709515], dtype=float32), -0.18296601]. 
=============================================
[2019-04-08 15:07:07,626] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[45.081135]
 [45.92288 ]
 [46.57957 ]
 [47.307056]
 [47.2505  ]], R is [[44.81508636]
 [45.2779808 ]
 [45.73624802]
 [46.18992996]
 [46.72803116]].
[2019-04-08 15:07:08,225] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9953530e-08 1.5681643e-04 4.8512660e-02 9.1660416e-01 4.6989307e-04
 1.0128466e-05 2.7231660e-02 1.1828322e-06 9.3235872e-09 9.0644325e-10
 7.0134457e-03], sum to 1.0000
[2019-04-08 15:07:08,229] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7619
[2019-04-08 15:07:08,243] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [14.56666666666667, 78.0, 39.66666666666666, 0.0, 19.0, 24.16713067882912, 0.217401157105072, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [19.56666666666667, 30.0], 
sim time this is 1154400.0000, 
sim time next is 1155000.0000, 
raw observation next is [15.03333333333333, 76.5, 48.33333333333333, 0.0, 19.0, 24.15684361846144, 0.2125697220244344, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.879039704524469, 0.765, 0.1611111111111111, 0.0, 0.08333333333333333, 0.5130703015384533, 0.5708565740081448, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0652348], dtype=float32), 0.35803834]. 
=============================================
[2019-04-08 15:07:08,260] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[37.357693]
 [37.160267]
 [36.785686]
 [37.005444]
 [36.790737]], R is [[38.14691162]
 [38.76544189]
 [39.37778854]
 [39.9840126 ]
 [40.5841713 ]].
[2019-04-08 15:07:08,443] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6924531e-06 3.2968044e-03 1.9927143e-01 6.2441063e-01 7.7072915e-04
 5.6160463e-04 2.1900628e-02 2.8733308e-05 3.1383411e-07 3.3529442e-08
 1.4975639e-01], sum to 1.0000
[2019-04-08 15:07:08,447] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1454
[2019-04-08 15:07:08,454] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [11.6, 79.0, 0.0, 0.0, 19.0, 22.70268205732053, -0.07443674349285664, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [16.6, 25.0], 
sim time this is 1142400.0000, 
sim time next is 1143000.0000, 
raw observation next is [11.6, 80.0, 0.0, 0.0, 19.0, 22.6805001589879, -0.0809915700929253, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.8, 0.0, 0.0, 0.08333333333333333, 0.39004167991565836, 0.4730028099690249, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5096071], dtype=float32), -1.3695264]. 
=============================================
[2019-04-08 15:07:08,475] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[33.544655]
 [33.79079 ]
 [33.60501 ]
 [33.5139  ]
 [33.170235]], R is [[34.44946289]
 [35.0160141 ]
 [35.57690048]
 [36.13217545]
 [36.68190002]].
[2019-04-08 15:07:09,121] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.2036149e-09 4.4229490e-04 2.4597660e-01 7.3745883e-01 4.1054882e-05
 2.3778480e-06 1.8095151e-03 1.0336699e-06 4.2471804e-09 2.3270574e-10
 1.4268252e-02], sum to 1.0000
[2019-04-08 15:07:09,126] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7780
[2019-04-08 15:07:09,139] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.46666666666667, 64.33333333333334, 88.0, 0.0, 19.0, 24.58652176182411, 0.3472915444304765, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [23.46666666666667, 30.0], 
sim time this is 1178400.0000, 
sim time next is 1179000.0000, 
raw observation next is [18.55, 64.0, 80.0, 0.0, 19.0, 24.55961000847385, 0.3417865394575259, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.976454293628809, 0.64, 0.26666666666666666, 0.0, 0.08333333333333333, 0.5466341673728209, 0.613928846485842, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37540054], dtype=float32), -0.2558631]. 
=============================================
[2019-04-08 15:07:09,155] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[41.984825]
 [41.88501 ]
 [41.832207]
 [41.85978 ]
 [41.787773]], R is [[42.46440887]
 [43.0397644 ]
 [43.60936737]
 [44.17327499]
 [44.73154449]].
[2019-04-08 15:07:09,183] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9157334e-08 1.1087742e-03 1.2661868e-01 8.5677719e-01 8.1674596e-05
 5.3489262e-06 3.2925759e-03 2.0585792e-06 6.0057768e-09 3.7342443e-10
 1.2113733e-02], sum to 1.0000
[2019-04-08 15:07:09,190] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7961
[2019-04-08 15:07:09,194] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.63333333333333, 63.66666666666667, 71.5, 0.0, 19.0, 24.5261516787552, 0.3415582769934056, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [23.63333333333333, 30.0], 
sim time this is 1179600.0000, 
sim time next is 1180200.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 63.00000000000001, 0.0, 19.0, 24.54067900851927, 0.3399019240474201, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.981071098799631, 0.6333333333333333, 0.21000000000000002, 0.0, 0.08333333333333333, 0.5450565840432725, 0.6133006413491401, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25202096], dtype=float32), 1.213935]. 
=============================================
[2019-04-08 15:07:09,972] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0633336e-08 7.3584481e-05 4.8722589e-01 5.0144672e-01 8.4053907e-05
 1.0266341e-05 1.3421271e-03 1.1821271e-06 1.8002203e-09 7.7686232e-11
 9.8161362e-03], sum to 1.0000
[2019-04-08 15:07:09,975] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6393
[2019-04-08 15:07:09,982] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2000, global step 30928: loss 8.5606
[2019-04-08 15:07:09,983] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2000, global step 30928: learning rate 0.0000
[2019-04-08 15:07:09,987] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.71666666666667, 63.33333333333333, 167.3333333333333, 0.0, 19.0, 24.11065236151185, 0.2682779402733086, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [23.71666666666667, 30.0], 
sim time this is 1167000.0000, 
sim time next is 1167600.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 169.1666666666667, 0.0, 19.0, 24.17295941549396, 0.2757790483836878, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.9787626962142197, 0.6366666666666667, 0.563888888888889, 0.0, 0.08333333333333333, 0.5144132846244966, 0.5919263494612292, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.89692026], dtype=float32), -0.2270407]. 
=============================================
[2019-04-08 15:07:10,041] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 30966: loss 11.5901
[2019-04-08 15:07:10,043] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 30966: learning rate 0.0000
[2019-04-08 15:07:10,360] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.72652786e-09 1.35411543e-03 1.24959856e-01 8.31096590e-01
 2.28968001e-05 5.79338212e-06 1.01746048e-03 4.14433146e-07
 3.01584735e-09 3.10634879e-10 4.15428616e-02], sum to 1.0000
[2019-04-08 15:07:10,370] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8205
[2019-04-08 15:07:10,383] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 23.90754278280504, 0.1994752740482095, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [23.3, 30.0], 
sim time this is 1172400.0000, 
sim time next is 1173000.0000, 
raw observation next is [18.3, 65.0, 148.6666666666667, 0.0, 19.0, 23.92900465586325, 0.2014237511332038, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.4955555555555557, 0.0, 0.08333333333333333, 0.4940837213219374, 0.5671412503777346, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4234296], dtype=float32), -1.6092354]. 
=============================================
[2019-04-08 15:07:10,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[44.482494]
 [44.241535]
 [44.35934 ]
 [44.032345]
 [43.899197]], R is [[45.09090042]
 [45.63999176]
 [46.18359375]
 [46.7217598 ]
 [47.2545433 ]].
[2019-04-08 15:07:10,403] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31200: loss 13.7893
[2019-04-08 15:07:10,404] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31200: learning rate 0.0000
[2019-04-08 15:07:10,420] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.5618922e-10 1.2914149e-05 3.6856982e-01 6.2917513e-01 9.1515273e-07
 1.8531382e-07 3.0605303e-04 4.4621316e-09 1.1421922e-10 2.5522369e-12
 1.9350094e-03], sum to 1.0000
[2019-04-08 15:07:10,423] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5291
[2019-04-08 15:07:10,434] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [11.41666666666667, 72.0, 0.0, 0.0, 19.0, 23.65175796798572, 0.1484971538001607, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [16.41666666666667, 30.0], 
sim time this is 1123800.0000, 
sim time next is 1124400.0000, 
raw observation next is [11.23333333333333, 73.0, 0.0, 0.0, 19.0, 23.60317680225924, 0.1397880090524674, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.0, 0.7737765466297323, 0.73, 0.0, 0.0, 0.08333333333333333, 0.4669314001882701, 0.5465960030174891, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.46723577], dtype=float32), 1.1128415]. 
=============================================
[2019-04-08 15:07:10,444] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7030510e-09 5.3838972e-04 2.4493417e-01 7.4191403e-01 2.5550618e-05
 6.0715020e-06 2.1081595e-03 1.8983967e-07 4.6278872e-10 1.5829063e-10
 1.0473501e-02], sum to 1.0000
[2019-04-08 15:07:10,450] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5500
[2019-04-08 15:07:10,456] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 24.27691733186562, 0.2850939834634266, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [22.7, 30.0], 
sim time this is 1192200.0000, 
sim time next is 1192800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 24.26389245692697, 0.2809959884021358, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.5219910380772476, 0.5936653294673786, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.0169568], dtype=float32), 0.7645705]. 
=============================================
[2019-04-08 15:07:10,753] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31423: loss 10.1264
[2019-04-08 15:07:10,756] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31423: learning rate 0.0000
[2019-04-08 15:07:10,775] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2425397e-07 2.9366505e-03 1.5288013e-01 7.6563478e-01 3.4179166e-04
 9.6042777e-06 2.6131198e-02 1.4521753e-06 2.8431153e-08 1.0910266e-09
 5.2064177e-02], sum to 1.0000
[2019-04-08 15:07:10,780] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8467
[2019-04-08 15:07:10,788] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 24.16010483715885, 0.2716770011334372, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [20.5, 30.0], 
sim time this is 1220400.0000, 
sim time next is 1221000.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 24.13428056821724, 0.2718580768382017, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.5111900473514366, 0.5906193589460672, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38568032], dtype=float32), -0.06591643]. 
=============================================
[2019-04-08 15:07:10,801] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[40.924934]
 [41.159386]
 [41.35902 ]
 [41.725716]
 [41.942055]], R is [[41.44285965]
 [41.93947601]
 [42.43112564]
 [43.00681305]
 [43.57674408]].
[2019-04-08 15:07:10,815] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31465: loss 8.1350
[2019-04-08 15:07:10,816] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31465: learning rate 0.0000
[2019-04-08 15:07:10,881] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6119695e-08 1.4986813e-03 1.8835926e-01 7.9407555e-01 1.6411611e-04
 3.2414607e-06 9.0876389e-03 7.0365138e-07 3.0187797e-09 4.3006840e-10
 6.8108556e-03], sum to 1.0000
[2019-04-08 15:07:10,887] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6420
[2019-04-08 15:07:10,899] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 24.56255266683358, 0.3529176053657863, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [20.5, 30.0], 
sim time this is 1221000.0000, 
sim time next is 1221600.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 24.55818164586183, 0.3474789747173959, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.5465151371551524, 0.6158263249057986, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4575591], dtype=float32), -0.33234802]. 
=============================================
[2019-04-08 15:07:11,318] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31800: loss 4.3418
[2019-04-08 15:07:11,320] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31800: learning rate 0.0000
[2019-04-08 15:07:11,343] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 31817: loss 10.7126
[2019-04-08 15:07:11,344] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 31817: learning rate 0.0000
[2019-04-08 15:07:11,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3090369e-10 2.8156177e-05 1.1847215e-01 8.7314373e-01 3.2360338e-05
 1.0160492e-06 2.5153658e-03 1.3906359e-07 1.9651668e-10 1.3938569e-11
 5.8070598e-03], sum to 1.0000
[2019-04-08 15:07:11,403] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7064
[2019-04-08 15:07:11,410] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 24.42405955759723, 0.3150842741130191, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [22.7, 30.0], 
sim time this is 1198200.0000, 
sim time next is 1198800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 24.43100631814155, 0.310380832425545, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.5359171931784624, 0.6034602774751817, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.86901945], dtype=float32), -0.72976047]. 
=============================================
[2019-04-08 15:07:11,443] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.01909436e-10 4.05446699e-05 9.10417512e-02 8.97345841e-01
 3.14667250e-06 2.19281310e-06 1.21245300e-03 2.62239688e-08
 2.64330346e-11 1.34526045e-11 1.03540579e-02], sum to 1.0000
[2019-04-08 15:07:11,447] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8670
[2019-04-08 15:07:11,459] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [17.9, 65.66666666666667, 0.0, 0.0, 19.0, 25.47960805974338, 0.5269160584645425, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [22.9, 30.0], 
sim time this is 1190400.0000, 
sim time next is 1191000.0000, 
raw observation next is [17.8, 66.33333333333333, 0.0, 0.0, 19.0, 25.46562331289133, 0.5226051081788338, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.955678670360111, 0.6633333333333333, 0.0, 0.0, 0.08333333333333333, 0.6221352760742777, 0.674201702726278, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.71825975], dtype=float32), -1.439472]. 
=============================================
[2019-04-08 15:07:11,461] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31897: loss 4.6143
[2019-04-08 15:07:11,463] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31897: learning rate 0.0000
[2019-04-08 15:07:11,472] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[49.857914]
 [50.311226]
 [50.210854]
 [50.385532]
 [50.389034]], R is [[50.37350464]
 [50.86977005]
 [51.36107254]
 [51.8474617 ]
 [52.32898712]].
[2019-04-08 15:07:11,474] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31903: loss 14.9469
[2019-04-08 15:07:11,476] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31903: learning rate 0.0000
[2019-04-08 15:07:11,477] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31903: loss 14.7287
[2019-04-08 15:07:11,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31903: learning rate 0.0000
[2019-04-08 15:07:11,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.68823888e-08 2.41508038e-04 6.73870072e-02 9.19728696e-01
 3.25189358e-05 1.04935625e-05 4.02556593e-03 5.09700214e-07
 1.19844517e-08 7.43343831e-10 8.57375003e-03], sum to 1.0000
[2019-04-08 15:07:11,558] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2960
[2019-04-08 15:07:11,565] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.6, 91.33333333333333, 0.0, 0.0, 19.0, 23.92769532265841, 0.218334969014175, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [20.6, 30.0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 23.91715539295182, 0.2139867360560902, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.493096282745985, 0.5713289120186967, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.52760184], dtype=float32), 0.7790448]. 
=============================================
[2019-04-08 15:07:11,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2772977e-08 6.1341375e-04 1.6957246e-01 8.1276178e-01 2.8460478e-05
 1.6187652e-06 6.6654626e-03 1.7562206e-07 2.5630398e-09 1.3577370e-10
 1.0356497e-02], sum to 1.0000
[2019-04-08 15:07:11,588] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0604
[2019-04-08 15:07:11,595] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 19.0, 24.34735252415025, 0.2782816340081355, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [21.6, 25.0], 
sim time this is 1202400.0000, 
sim time next is 1203000.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 19.0, 24.32260372169646, 0.2724471039374591, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.08333333333333333, 0.526883643474705, 0.5908157013124864, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8689003], dtype=float32), 0.03845195]. 
=============================================
[2019-04-08 15:07:11,607] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[48.494427]
 [48.419117]
 [48.379795]
 [48.44409 ]
 [48.378445]], R is [[49.30845642]
 [49.81537247]
 [50.31721878]
 [50.81404877]
 [51.3059082 ]].
[2019-04-08 15:07:11,647] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31999: loss 10.1734
[2019-04-08 15:07:11,649] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32000: learning rate 0.0000
[2019-04-08 15:07:11,901] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32169: loss 15.6578
[2019-04-08 15:07:11,903] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32170: learning rate 0.0000
[2019-04-08 15:07:12,066] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32279: loss 12.3521
[2019-04-08 15:07:12,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32279: learning rate 0.0000
[2019-04-08 15:07:12,350] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.0682176e-08 9.6480863e-04 9.5119148e-02 8.6145139e-01 2.2558866e-04
 1.8351486e-05 2.4110661e-03 1.0752460e-06 5.4031148e-09 3.2999217e-10
 3.9808553e-02], sum to 1.0000
[2019-04-08 15:07:12,355] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7898
[2019-04-08 15:07:12,363] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.41666666666667, 93.5, 0.0, 0.0, 19.0, 23.73054730237443, 0.1827167932064148, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [20.41666666666667, 30.0], 
sim time this is 1224600.0000, 
sim time next is 1225200.0000, 
raw observation next is [15.33333333333333, 94.0, 0.0, 0.0, 19.0, 23.7174690535999, 0.1809785630386912, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.17391304347826086, 0.8873499538319484, 0.94, 0.0, 0.0, 0.08333333333333333, 0.4764557544666583, 0.5603261876795638, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.9689995], dtype=float32), 1.1058574]. 
=============================================
[2019-04-08 15:07:12,667] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3171355e-09 8.6745786e-05 2.5868827e-02 9.6686327e-01 5.1788275e-06
 1.6892611e-07 8.2414120e-04 4.0927866e-08 6.6934303e-10 6.2896917e-11
 6.3517471e-03], sum to 1.0000
[2019-04-08 15:07:12,675] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1269
[2019-04-08 15:07:12,682] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 23.79771799988989, 0.2029392940816813, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [20.0, 30.0], 
sim time this is 1228800.0000, 
sim time next is 1229400.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 23.78590067518394, 0.2060695028431683, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.4821583895986616, 0.5686898342810561, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.0626216], dtype=float32), 1.2241452]. 
=============================================
[2019-04-08 15:07:12,771] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3523004e-09 3.2535489e-04 2.8597188e-01 6.8687701e-01 4.1179006e-05
 1.2636021e-06 1.7389560e-02 4.5990311e-07 9.5763808e-10 2.5589952e-10
 9.3932282e-03], sum to 1.0000
[2019-04-08 15:07:12,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2802
[2019-04-08 15:07:12,788] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [15.7, 89.66666666666667, 0.0, 0.0, 19.0, 25.26007350543994, 0.5081257837239158, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [20.7, 25.0], 
sim time this is 1219200.0000, 
sim time next is 1219800.0000, 
raw observation next is [15.6, 91.33333333333333, 0.0, 0.0, 19.0, 25.22470151526279, 0.5021863334270339, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.8947368421052633, 0.9133333333333333, 0.0, 0.0, 0.08333333333333333, 0.6020584596052325, 0.667395444475678, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2865014], dtype=float32), -0.69937766]. 
=============================================
[2019-04-08 15:07:13,057] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32911: loss 4.3521
[2019-04-08 15:07:13,059] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32911: learning rate 0.0000
[2019-04-08 15:07:13,191] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3615562e-10 2.2818671e-04 3.6218017e-01 5.5851877e-01 2.8937377e-05
 4.1360472e-06 2.1291745e-03 1.4188480e-07 2.5597380e-10 1.3737237e-11
 7.6910488e-02], sum to 1.0000
[2019-04-08 15:07:13,196] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0050
[2019-04-08 15:07:13,206] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [13.8, 100.0, 96.0, 0.0, 19.0, 23.70636410840629, 0.2246007795755739, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [18.8, 25.0], 
sim time this is 1257600.0000, 
sim time next is 1258200.0000, 
raw observation next is [13.8, 100.0, 95.0, 0.0, 19.0, 23.70019590634705, 0.2257124054169503, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.31666666666666665, 0.0, 0.08333333333333333, 0.475016325528921, 0.5752374684723168, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32859007], dtype=float32), -0.26183242]. 
=============================================
[2019-04-08 15:07:13,540] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2000, global step 33202: loss 9.3733
[2019-04-08 15:07:13,542] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2000, global step 33203: learning rate 0.0000
[2019-04-08 15:07:13,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.9191782e-10 1.6222538e-04 1.1950525e-01 8.5864341e-01 8.2544290e-07
 1.7601630e-07 5.1270833e-04 1.3037498e-08 1.9123424e-10 1.3193349e-11
 2.1175418e-02], sum to 1.0000
[2019-04-08 15:07:13,930] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4562
[2019-04-08 15:07:13,937] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [18.3, 65.0, 120.0, 0.0, 19.0, 23.7191667895863, 0.1481023038408846, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [23.3, 30.0], 
sim time this is 1176000.0000, 
sim time next is 1176600.0000, 
raw observation next is [18.3, 65.0, 112.0, 0.0, 19.0, 23.72215473296342, 0.149347756243187, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.37333333333333335, 0.0, 0.08333333333333333, 0.4768462277469518, 0.5497825854143957, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.4772557], dtype=float32), -0.054879945]. 
=============================================
[2019-04-08 15:07:14,091] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2763975e-10 1.1799191e-05 1.3714141e-01 8.4171927e-01 6.9018716e-07
 7.6824570e-08 8.7185466e-04 3.5277643e-09 5.2602898e-11 1.9972949e-13
 2.0254899e-02], sum to 1.0000
[2019-04-08 15:07:14,097] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2213
[2019-04-08 15:07:14,103] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.8, 100.0, 51.0, 0.0, 19.0, 23.67688467367104, 0.215191244640337, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [18.8, 30.0], 
sim time this is 1265400.0000, 
sim time next is 1266000.0000, 
raw observation next is [13.8, 100.0, 45.66666666666666, 0.0, 19.0, 23.669245826009, 0.2116174812294218, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.6521739130434783, 0.844875346260388, 1.0, 0.1522222222222222, 0.0, 0.08333333333333333, 0.47243715216741666, 0.5705391604098072, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.787422], dtype=float32), 0.18917517]. 
=============================================
[2019-04-08 15:07:14,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[55.42807 ]
 [55.38446 ]
 [55.290375]
 [55.187332]
 [55.069733]], R is [[55.84121704]
 [56.2828064 ]
 [56.63102341]
 [57.06471252]
 [57.40510941]].
[2019-04-08 15:07:14,894] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4838537e-10 5.8935158e-05 6.3038632e-02 9.3458951e-01 1.4264995e-06
 4.9560196e-08 7.9545251e-04 1.7108674e-08 7.2290741e-12 7.0427198e-14
 1.5161032e-03], sum to 1.0000
[2019-04-08 15:07:14,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8373
[2019-04-08 15:07:14,901] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 34080: loss 5.4974
[2019-04-08 15:07:14,907] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 22.81083990961289, 0.02407044374397694, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [12.2, 30.0], 
sim time this is 1278000.0000, 
sim time next is 1278600.0000, 
raw observation next is [7.016666666666667, 96.0, 0.0, 0.0, 19.0, 22.76146779662475, 0.01403658673427246, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.8260869565217391, 0.656971375807941, 0.96, 0.0, 0.0, 0.08333333333333333, 0.3967889830520625, 0.5046788622447574, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.9436152], dtype=float32), 0.7515232]. 
=============================================
[2019-04-08 15:07:14,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 34080: learning rate 0.0000
[2019-04-08 15:07:15,190] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1147832e-11 1.5673881e-05 1.6768213e-02 9.8131210e-01 7.7934710e-06
 4.0567325e-08 4.4130097e-04 3.6105334e-09 8.1244317e-12 4.1624649e-13
 1.4549079e-03], sum to 1.0000
[2019-04-08 15:07:15,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6713
[2019-04-08 15:07:15,203] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 19.0, 22.22960470149056, -0.1026991404764293, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [10.5, 30.0], 
sim time this is 1288200.0000, 
sim time next is 1288800.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 22.20230550887629, -0.1102182369617202, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.3501921257396908, 0.4632605876794266, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.6438107], dtype=float32), 0.27079326]. 
=============================================
[2019-04-08 15:07:15,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1503281e-10 1.3445347e-05 5.9450604e-02 9.2394382e-01 8.6457712e-06
 2.7827858e-07 1.3849855e-02 4.9317446e-08 2.0015314e-10 4.6886713e-13
 2.7332949e-03], sum to 1.0000
[2019-04-08 15:07:15,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0762
[2019-04-08 15:07:15,715] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.8, 93.0, 0.0, 0.0, 19.0, 22.03868870149451, -0.1613811303784576, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [8.8, 30.0], 
sim time this is 1299600.0000, 
sim time next is 1300200.0000, 
raw observation next is [3.716666666666666, 92.83333333333333, 0.0, 0.0, 19.0, 22.074751225269, -0.1573965206055711, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.043478260869565216, 0.5655586334256695, 0.9283333333333332, 0.0, 0.0, 0.08333333333333333, 0.33956260210575007, 0.44753449313147625, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.6372526], dtype=float32), -0.40517431]. 
=============================================
[2019-04-08 15:07:15,894] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2745748e-10 3.0793031e-05 6.0025901e-02 9.3892008e-01 3.1355407e-06
 2.6238365e-07 5.3364568e-04 5.4604410e-09 5.5034801e-12 3.6380968e-13
 4.8625894e-04], sum to 1.0000
[2019-04-08 15:07:15,898] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0745
[2019-04-08 15:07:15,910] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.7, 92.0, 0.0, 0.0, 19.0, 21.81424716556172, -0.2375016981293091, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [7.7, 30.0], 
sim time this is 1306800.0000, 
sim time next is 1307400.0000, 
raw observation next is [2.616666666666667, 92.0, 0.0, 0.0, 19.0, 21.81913889466192, -0.2446251266087718, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.13043478260869565, 0.5350877192982457, 0.92, 0.0, 0.0, 0.08333333333333333, 0.31826157455515985, 0.4184582911304094, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.50713897], dtype=float32), 2.1945522]. 
=============================================
[2019-04-08 15:07:16,081] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4788037e-11 2.7054975e-06 2.9634872e-02 9.5661676e-01 3.0162698e-06
 4.1676795e-08 1.8254929e-03 2.8554501e-08 5.9473307e-12 2.1865395e-13
 1.1917033e-02], sum to 1.0000
[2019-04-08 15:07:16,088] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8751
[2019-04-08 15:07:16,095] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.283333333333333, 92.0, 0.0, 0.0, 19.0, 21.67986075450798, -0.2840760956176553, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [7.283333333333333, 30.0], 
sim time this is 1309800.0000, 
sim time next is 1310400.0000, 
raw observation next is [2.2, 92.0, 0.0, 0.0, 19.0, 21.62526159612911, -0.2906616220624841, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.92, 0.0, 0.0, 0.08333333333333333, 0.3021051330107592, 0.40311279264583866, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.33041254], dtype=float32), -0.80782956]. 
=============================================
[2019-04-08 15:07:16,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5537330e-10 4.9365103e-06 7.4089199e-02 9.1767365e-01 2.9382584e-06
 2.7297654e-07 6.4875847e-03 5.5657718e-08 5.6903744e-12 2.8014013e-12
 1.7412649e-03], sum to 1.0000
[2019-04-08 15:07:16,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9180
[2019-04-08 15:07:16,211] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.1, 92.0, 0.0, 0.0, 19.0, 21.37618037462426, -0.3461553845032024, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [7.1, 30.0], 
sim time this is 1311000.0000, 
sim time next is 1311600.0000, 
raw observation next is [2.0, 92.0, 0.0, 0.0, 19.0, 21.3577628197654, -0.35165164876648, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.17391304347826086, 0.518005540166205, 0.92, 0.0, 0.0, 0.08333333333333333, 0.27981356831378346, 0.38278278374450664, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.5590057], dtype=float32), -1.4140025]. 
=============================================
[2019-04-08 15:07:16,269] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8541077e-09 3.4371412e-06 6.6656411e-02 9.3069744e-01 3.3362846e-06
 5.7415258e-07 9.0437604e-04 1.1046671e-07 4.1137913e-11 1.0433278e-12
 1.7343075e-03], sum to 1.0000
[2019-04-08 15:07:16,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3399
[2019-04-08 15:07:16,281] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 21.64823434540796, -0.3148887471260109, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.6, 30.0], 
sim time this is 1315200.0000, 
sim time next is 1315800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 21.60523488300687, -0.3258393241221171, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.3004362402505724, 0.39138689195929427, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.19732988], dtype=float32), 0.7252901]. 
=============================================
[2019-04-08 15:07:16,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0752647e-10 8.4247644e-05 1.5758300e-01 8.1137782e-01 3.9268471e-06
 2.6314109e-07 2.8275246e-02 1.4905654e-08 8.8368250e-11 9.8915375e-13
 2.6754383e-03], sum to 1.0000
[2019-04-08 15:07:16,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9330
[2019-04-08 15:07:16,752] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 21.49878636755899, -0.3475122620119043, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.6, 30.0], 
sim time this is 1315200.0000, 
sim time next is 1315800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 21.4628935390115, -0.3575267222350192, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.2885744615842916, 0.38082442592166027, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.4118956], dtype=float32), 1.1551645]. 
=============================================
[2019-04-08 15:07:17,041] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.13008866e-13 1.21834000e-06 3.41698416e-02 9.64438260e-01
 2.35577232e-07 3.36998707e-10 7.24057754e-05 1.51671019e-11
 2.70794146e-15 6.24452806e-17 1.31798873e-03], sum to 1.0000
[2019-04-08 15:07:17,044] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6166
[2019-04-08 15:07:17,057] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 22.65676550206741, -0.06103820394578455, 1.0, 1.0, 30.00000000000001, 19496.75616670693], 
current ob forecast is [], 
actual action is [5.5, 30.0], 
sim time this is 1359600.0000, 
sim time next is 1360200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 22.61441016156964, -0.05685249872055902, 1.0, 1.0, 30.0, 19529.21332899471], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.38453418013080337, 0.481049167093147, 1.0, 1.0, 0.3, 0.09299625394759385], 
reward next is 0.9070, 
noisyNet noise sample is [array([-0.03742864], dtype=float32), 1.3374978]. 
=============================================
[2019-04-08 15:07:17,284] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9332564e-15 1.1395176e-08 7.3967832e-03 9.9254310e-01 1.8419368e-09
 1.5882653e-11 1.1318700e-06 2.1179202e-11 1.6356148e-16 3.3824852e-18
 5.8985512e-05], sum to 1.0000
[2019-04-08 15:07:17,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3908
[2019-04-08 15:07:17,305] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 127.0, 0.0, 22.5, 23.30419482137193, -0.06625970052421476, 1.0, 1.0, 30.0, 19383.21309230512], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1337400.0000, 
sim time next is 1338000.0000, 
raw observation next is [1.1, 92.0, 124.6666666666667, 0.0, 22.5, 23.3461785776233, -0.06608761377975832, 1.0, 1.0, 30.0, 19233.33836325569], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.4155555555555557, 0.0, 0.375, 0.44551488146860824, 0.4779707954067472, 1.0, 1.0, 0.3, 0.09158732553931281], 
reward next is 0.9084, 
noisyNet noise sample is [array([-0.9993346], dtype=float32), 0.24792963]. 
=============================================
[2019-04-08 15:07:17,318] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.393265]
 [75.01284 ]
 [74.46207 ]
 [73.86535 ]
 [73.53383 ]], R is [[75.76623535]
 [75.91627502]
 [76.06395721]
 [76.20919037]
 [76.35147095]].
[2019-04-08 15:07:17,460] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5484326e-15 3.9305272e-07 7.5535685e-02 9.1679418e-01 3.3163587e-08
 2.1644500e-09 3.5092700e-04 2.2500944e-11 2.7757476e-14 4.0922173e-17
 7.3187430e-03], sum to 1.0000
[2019-04-08 15:07:17,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1442
[2019-04-08 15:07:17,475] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 66.5, 0.0, 22.5, 23.71626012841488, 0.02557261383933546, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1348800.0000, 
sim time next is 1349400.0000, 
raw observation next is [1.1, 92.0, 62.0, 0.0, 22.5, 23.70794449145131, 0.02021676143505577, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.20666666666666667, 0.0, 0.375, 0.47566204095427594, 0.5067389204783519, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.88601583], dtype=float32), -0.5776877]. 
=============================================
[2019-04-08 15:07:17,662] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.46873988e-11 1.06396565e-05 4.36996482e-02 9.49413657e-01
 1.34412903e-06 1.70907288e-08 1.63492397e-03 7.16550952e-09
 5.44106470e-12 9.59606133e-14 5.23983082e-03], sum to 1.0000
[2019-04-08 15:07:17,664] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4494
[2019-04-08 15:07:17,685] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.55, 92.5, 0.0, 0.0, 19.0, 21.92390617788901, -0.2164428707749937, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [8.55, 30.0], 
sim time this is 1301400.0000, 
sim time next is 1302000.0000, 
raw observation next is [3.466666666666667, 92.33333333333333, 0.0, 0.0, 19.0, 21.87191216779516, -0.2304794638992486, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.043478260869565216, 0.5586334256694367, 0.9233333333333333, 0.0, 0.0, 0.08333333333333333, 0.3226593473162633, 0.4231735120335838, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.3118886], dtype=float32), 0.113033235]. 
=============================================
[2019-04-08 15:07:17,706] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[60.747547]
 [60.749775]
 [61.07609 ]
 [61.162083]
 [61.22674 ]], R is [[60.57804108]
 [60.8833046 ]
 [61.18551636]
 [61.48470688]
 [61.78090668]].
[2019-04-08 15:07:17,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9448981e-14 2.3270188e-05 4.0393475e-02 9.5822948e-01 2.3511650e-08
 7.2417039e-10 5.1453611e-05 1.7812238e-11 3.2831774e-15 4.8613851e-18
 1.3023824e-03], sum to 1.0000
[2019-04-08 15:07:17,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1285
[2019-04-08 15:07:17,847] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 71.0, 0.0, 22.5, 23.63250401010702, 0.008942803715905381, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1348200.0000, 
sim time next is 1348800.0000, 
raw observation next is [1.1, 92.0, 66.5, 0.0, 22.5, 23.66669056422514, 0.007586686887936636, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22166666666666668, 0.0, 0.375, 0.4722242136854282, 0.5025288956293122, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.02473613], dtype=float32), 0.3513756]. 
=============================================
[2019-04-08 15:07:18,283] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.2184564e-11 1.8088884e-06 3.2474004e-02 9.2281216e-01 7.8733740e-07
 5.3808797e-08 1.3090207e-03 4.8748845e-09 1.8484615e-11 3.8024626e-13
 4.3402132e-02], sum to 1.0000
[2019-04-08 15:07:18,285] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9294
[2019-04-08 15:07:18,295] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 21.44213761186263, -0.3725781624951477, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.6, 30.0], 
sim time this is 1317600.0000, 
sim time next is 1318200.0000, 
raw observation next is [1.516666666666667, 92.0, 0.0, 0.0, 19.0, 21.36696931904942, -0.3890190450421819, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.2608695652173913, 0.5046168051708219, 0.92, 0.0, 0.0, 0.08333333333333333, 0.28058077658745173, 0.3703269849859394, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.5935798], dtype=float32), -2.0529578]. 
=============================================
[2019-04-08 15:07:18,529] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.9422569e-10 1.5610725e-05 6.5571576e-02 9.3276691e-01 1.8830917e-07
 5.0172262e-09 2.7280883e-04 4.2242272e-09 1.3323531e-11 2.5774939e-13
 1.3729004e-03], sum to 1.0000
[2019-04-08 15:07:18,532] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7287
[2019-04-08 15:07:18,542] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 20.56250964052863, -0.5923437157849251, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [5.0, 30.0], 
sim time this is 1390800.0000, 
sim time next is 1391400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 20.52718238676782, -0.5920812184777938, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.2105985322306516, 0.3026395938407354, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.8279142], dtype=float32), -0.01689771]. 
=============================================
[2019-04-08 15:07:18,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3285266e-15 1.9648835e-06 1.7444400e-02 9.8249108e-01 9.3276864e-10
 1.4786629e-11 2.2712795e-05 2.2015365e-12 9.1170074e-16 6.4739730e-19
 3.9778002e-05], sum to 1.0000
[2019-04-08 15:07:18,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0243
[2019-04-08 15:07:18,636] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 22.75974488973666, -0.1520862152021448, 1.0, 1.0, 29.99999999999999, 18680.41167023053], 
current ob forecast is [], 
actual action is [5.5, 30.0], 
sim time this is 1359000.0000, 
sim time next is 1359600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 22.55897178527195, -0.09959973351481537, 1.0, 1.0, 30.00000000000001, 18680.41167023054], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.3799143154393292, 0.4668000888283949, 1.0, 1.0, 0.3000000000000002, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.38654995], dtype=float32), -0.64480186]. 
=============================================
[2019-04-08 15:07:19,123] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.3160237e-12 2.0861139e-06 9.8910555e-02 8.9970881e-01 1.7547483e-06
 5.4735412e-09 5.0153804e-04 2.8910827e-10 2.5906081e-12 1.2071339e-13
 8.7515137e-04], sum to 1.0000
[2019-04-08 15:07:19,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6285
[2019-04-08 15:07:19,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 92.0, 0.0, 0.0, 19.0, 21.64916274233016, -0.2988619376059216, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [7.0, 30.0], 
sim time this is 1311600.0000, 
sim time next is 1312200.0000, 
raw observation next is [1.9, 92.0, 0.0, 0.0, 19.0, 21.61380071421677, -0.3061225649347708, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.17391304347826086, 0.515235457063712, 0.92, 0.0, 0.0, 0.08333333333333333, 0.30115005951806406, 0.39795914502174307, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.10552859], dtype=float32), 0.6259587]. 
=============================================
[2019-04-08 15:07:19,429] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5330256e-14 1.3601989e-06 8.9079583e-01 1.0001165e-01 3.6987085e-08
 2.2439055e-09 2.9968889e-04 4.9952828e-12 7.1526444e-15 2.6924252e-16
 8.8915173e-03], sum to 1.0000
[2019-04-08 15:07:19,432] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5296
[2019-04-08 15:07:19,459] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.1, 92.0, 117.6666666666667, 0.0, 22.5, 23.28789338549397, -0.07932396006301208, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [6.1, 25.0], 
sim time this is 1339800.0000, 
sim time next is 1340400.0000, 
raw observation next is [1.1, 92.0, 115.3333333333333, 0.0, 22.5, 23.28576120409262, -0.0900298785945269, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.92, 0.3844444444444443, 0.0, 0.375, 0.4404801003410516, 0.4699900404684911, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47673064], dtype=float32), 0.62956005]. 
=============================================
[2019-04-08 15:07:19,619] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0943977e-10 6.9587559e-06 6.7041546e-02 9.3016785e-01 4.3519299e-06
 7.3652842e-08 3.6827094e-04 1.4906333e-07 8.5899083e-12 3.3206085e-13
 2.4108661e-03], sum to 1.0000
[2019-04-08 15:07:19,619] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5653
[2019-04-08 15:07:19,635] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 20.53737919962412, -0.6221085337775839, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [4.4, 30.0], 
sim time this is 1396800.0000, 
sim time next is 1397400.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 20.53056914092755, -0.6279984361358122, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.17391304347826086, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.21088076174396253, 0.2906671879547293, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.00023872], dtype=float32), -1.030477]. 
=============================================
[2019-04-08 15:07:19,663] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7505001e-11 1.3472784e-05 3.1118646e-01 6.8184328e-01 3.2714709e-06
 2.2660394e-07 4.7384491e-03 8.1701872e-09 6.5800134e-12 1.1620042e-12
 2.2148921e-03], sum to 1.0000
[2019-04-08 15:07:19,669] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6592
[2019-04-08 15:07:19,693] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 9.0, 0.0, 22.5, 20.17062107522561, -0.7282136337630695, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [4.4, 30.0], 
sim time this is 1411200.0000, 
sim time next is 1411800.0000, 
raw observation next is [-0.6, 100.0, 12.0, 0.0, 22.5, 20.16510298711968, -0.7302643061357083, 1.0, 1.0, 30.0, 19025.02112614247], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.04, 0.0, 0.375, 0.1804252489266401, 0.2565785646214306, 1.0, 1.0, 0.3, 0.09059533869591653], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7025017], dtype=float32), -1.014799]. 
=============================================
[2019-04-08 15:07:19,831] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7370814e-12 9.3638857e-07 7.4450754e-02 9.2492181e-01 1.2142820e-07
 9.1965475e-09 1.3748658e-04 4.3731199e-10 4.4478210e-13 4.3928399e-14
 4.8880716e-04], sum to 1.0000
[2019-04-08 15:07:19,834] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8314
[2019-04-08 15:07:19,857] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 22.66666666666666, 0.0, 22.5, 20.79027091536593, -0.6068538728465978, 1.0, 1.0, 30.00000000000001, 20376.28869984811], 
current ob forecast is [], 
actual action is [4.4, 30.0], 
sim time this is 1413600.0000, 
sim time next is 1414200.0000, 
raw observation next is [-0.6, 100.0, 27.33333333333333, 0.0, 22.5, 21.06501487351408, -0.5702668244601921, 1.0, 1.0, 30.0, 20218.74310926658], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.0911111111111111, 0.0, 0.375, 0.2554179061261734, 0.30991105851326933, 1.0, 1.0, 0.3, 0.09627972909174562], 
reward next is 0.9714, 
noisyNet noise sample is [array([0.24617532], dtype=float32), -0.37011844]. 
=============================================
[2019-04-08 15:07:19,905] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9983301e-11 9.4432980e-06 5.5199862e-02 9.3974590e-01 1.5554465e-07
 3.6941501e-08 3.5736554e-03 2.1066483e-10 8.0900569e-13 1.0037459e-13
 1.4708623e-03], sum to 1.0000
[2019-04-08 15:07:19,914] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5388
[2019-04-08 15:07:19,946] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 22.5, 21.62027025097866, -0.4852173394748188, 1.0, 1.0, 30.0, 25910.29819008838], 
current ob forecast is [], 
actual action is [4.5, 30.0], 
sim time this is 1415400.0000, 
sim time next is 1416000.0000, 
raw observation next is [-0.4, 98.33333333333334, 41.33333333333334, 0.0, 22.5, 21.65433174054435, -0.4679985477943636, 1.0, 1.0, 30.0, 22285.30494074587], 
processed observation next is [1.0, 0.391304347826087, 0.45152354570637127, 0.9833333333333334, 0.1377777777777778, 0.0, 0.375, 0.3045276450453625, 0.3440004840685455, 1.0, 1.0, 0.3, 0.10612049971783748], 
reward next is 0.8939, 
noisyNet noise sample is [array([0.4993451], dtype=float32), -1.130788]. 
=============================================
[2019-04-08 15:07:19,957] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[68.723404]
 [67.58086 ]
 [67.38112 ]
 [65.8855  ]
 [65.75748 ]], R is [[69.70476532]
 [69.88433838]
 [70.04750061]
 [70.34702301]
 [70.58824158]].
[2019-04-08 15:07:20,963] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.3872685e-15 6.3096643e-09 9.8141534e-03 9.9011034e-01 2.2619009e-09
 2.7537200e-12 2.4768600e-05 1.1374055e-12 8.3168217e-16 3.0326149e-18
 5.0802726e-05], sum to 1.0000
[2019-04-08 15:07:20,965] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7669
[2019-04-08 15:07:20,975] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 21.79049363712242, -0.2769022227805402, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [5.5, 30.0], 
sim time this is 1369800.0000, 
sim time next is 1370400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 21.74697294924437, -0.2841459780715619, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.3122477457703641, 0.40528467397614604, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.01924489], dtype=float32), 1.1813626]. 
=============================================
[2019-04-08 15:07:22,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8550262e-14 1.8175152e-07 5.7269341e-01 4.2448950e-01 8.8455280e-08
 4.1266660e-11 6.4454274e-05 2.6532597e-11 7.0444677e-15 8.3859524e-17
 2.7524105e-03], sum to 1.0000
[2019-04-08 15:07:22,412] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5663
[2019-04-08 15:07:22,442] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [0.5, 92.0, 93.0, 0.0, 22.5, 21.93559385176924, -0.3575670424755095, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [5.5, 25.0], 
sim time this is 1429200.0000, 
sim time next is 1429800.0000, 
raw observation next is [0.6000000000000001, 92.0, 92.0, 0.0, 22.5, 22.17888350748414, -0.3505116926077611, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.479224376731302, 0.92, 0.30666666666666664, 0.0, 0.375, 0.3482402922903451, 0.38316276913074626, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01069982], dtype=float32), 1.2567912]. 
=============================================
[2019-04-08 15:07:22,689] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8480249e-15 7.4588691e-08 2.1051154e-02 9.7861868e-01 4.3276788e-08
 4.4226781e-10 1.2866594e-04 3.8233660e-13 1.3433293e-16 1.0764214e-18
 2.0131413e-04], sum to 1.0000
[2019-04-08 15:07:22,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9062
[2019-04-08 15:07:22,722] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 21.9365530604, -0.3446424672941171, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1451400.0000, 
sim time next is 1452000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 21.90090896986548, -0.3542236172529536, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.32507574748879, 0.38192546091568214, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.72710526], dtype=float32), 1.3449672]. 
=============================================
[2019-04-08 15:07:22,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.707016]
 [84.69528 ]
 [84.71972 ]
 [84.74514 ]
 [84.756226]], R is [[84.88685608]
 [84.94903564]
 [85.01059723]
 [85.07154083]
 [85.13187408]].
[2019-04-08 15:07:23,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3878771e-15 2.2679738e-07 2.9320238e-02 9.7046971e-01 2.1581254e-08
 5.0128481e-11 4.7463647e-05 1.5265616e-12 2.9648279e-16 1.7724446e-19
 1.6229632e-04], sum to 1.0000
[2019-04-08 15:07:23,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5519
[2019-04-08 15:07:23,233] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.516666666666667, 89.5, 0.0, 0.0, 19.0, 21.39588114585208, -0.4115526300032465, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.5166666666666675, 30.0], 
sim time this is 1457400.0000, 
sim time next is 1458000.0000, 
raw observation next is [1.6, 89.0, 0.0, 0.0, 19.0, 21.37738200983794, -0.4149013422904031, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.89, 0.0, 0.0, 0.08333333333333333, 0.2814485008198284, 0.36169955256986563, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.00867925], dtype=float32), -0.53846353]. 
=============================================
[2019-04-08 15:07:23,239] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.68079 ]
 [84.41479 ]
 [85.108376]
 [85.64083 ]
 [86.428   ]], R is [[82.80184937]
 [82.88488007]
 [82.96707916]
 [83.04845428]
 [83.12902069]].
[2019-04-08 15:07:23,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.3122904e-15 1.2880110e-07 3.3111311e-02 9.6354175e-01 1.7159989e-08
 3.9186104e-10 1.8137255e-04 8.2427538e-13 1.4527695e-13 7.9936121e-18
 3.1654281e-03], sum to 1.0000
[2019-04-08 15:07:23,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4498
[2019-04-08 15:07:23,419] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.35, 90.5, 0.0, 0.0, 19.0, 21.33916113074209, -0.4222796140075206, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.35, 30.0], 
sim time this is 1459800.0000, 
sim time next is 1460400.0000, 
raw observation next is [1.266666666666667, 91.0, 0.0, 0.0, 19.0, 21.31978909702303, -0.4275209541661605, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.9130434782608695, 0.4976915974145891, 0.91, 0.0, 0.0, 0.08333333333333333, 0.2766490914185858, 0.3574930152779465, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.2819982], dtype=float32), 0.006625356]. 
=============================================
[2019-04-08 15:07:23,484] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0771896e-11 6.5312124e-06 2.8027141e-02 9.7064406e-01 1.0368766e-06
 1.0711065e-08 2.6831694e-04 5.0543592e-09 1.0470388e-12 1.4323161e-14
 1.0528330e-03], sum to 1.0000
[2019-04-08 15:07:23,489] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8341
[2019-04-08 15:07:23,502] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 20.28220514112111, -0.6813006164746822, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [4.4, 30.0], 
sim time this is 1402200.0000, 
sim time next is 1402800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 20.22478754392701, -0.6896585280979105, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.1853989619939176, 0.27011382396736316, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.9055024], dtype=float32), 0.6971355]. 
=============================================
[2019-04-08 15:07:24,156] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7145091e-11 7.0734473e-07 3.4974407e-02 9.6290678e-01 3.2793707e-08
 6.8151751e-09 4.7456531e-04 1.6439708e-09 1.6321615e-12 2.3510453e-14
 1.6435182e-03], sum to 1.0000
[2019-04-08 15:07:24,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4898
[2019-04-08 15:07:24,192] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.466666666666667, 98.66666666666666, 0.0, 0.0, 19.0, 20.01029815413492, -0.7936751388693001, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.466666666666667, 30.0], 
sim time this is 1492800.0000, 
sim time next is 1493400.0000, 
raw observation next is [1.283333333333333, 99.33333333333334, 0.0, 0.0, 19.0, 20.10875245153373, -0.8124931838971431, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.2608695652173913, 0.4981532779316713, 0.9933333333333334, 0.0, 0.0, 0.08333333333333333, 0.17572937096114405, 0.2291689387009523, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.0638129], dtype=float32), 1.0011145]. 
=============================================
[2019-04-08 15:07:24,853] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39179: loss 6.1643
[2019-04-08 15:07:24,853] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39179: learning rate 0.0000
[2019-04-08 15:07:24,922] A3C_AGENT_WORKER-Thread-8 INFO:Local step 2500, global step 39204: loss 6.2803
[2019-04-08 15:07:24,922] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 2500, global step 39204: learning rate 0.0000
[2019-04-08 15:07:25,053] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 39261: loss 6.9877
[2019-04-08 15:07:25,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 39261: learning rate 0.0000
[2019-04-08 15:07:25,625] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3071083e-12 1.1483105e-06 4.3553777e-02 9.5561880e-01 7.1270171e-07
 6.5708491e-08 1.1511977e-04 1.7268663e-09 1.3137887e-12 9.5731223e-15
 7.1038888e-04], sum to 1.0000
[2019-04-08 15:07:25,628] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8170
[2019-04-08 15:07:25,657] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 22.5, 20.11510777777637, -0.779174945087321, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1496400.0000, 
sim time next is 1497000.0000, 
raw observation next is [1.1, 100.0, 5.999999999999998, 0.0, 22.5, 20.08807518206906, -0.7794172598409435, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.019999999999999993, 0.0, 0.375, 0.17400626517242154, 0.2401942467196855, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7867894], dtype=float32), 1.038473]. 
=============================================
[2019-04-08 15:07:25,667] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[65.43613 ]
 [64.653885]
 [64.31415 ]
 [63.7471  ]
 [63.50398 ]], R is [[65.41353607]
 [64.77099609]
 [65.03433228]
 [64.75098419]
 [65.01452637]].
[2019-04-08 15:07:25,837] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0080356e-11 1.5108275e-06 2.3139097e-02 9.6959186e-01 4.0890356e-07
 2.2959732e-09 1.2978439e-03 2.4908011e-09 2.9767248e-12 9.5498682e-14
 5.9693987e-03], sum to 1.0000
[2019-04-08 15:07:25,838] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0968
[2019-04-08 15:07:25,847] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 93.0, 0.0, 0.0, 19.0, 20.51158958806168, -0.678070191546722, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [7.2, 30.0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [2.2, 93.33333333333334, 0.0, 0.0, 19.0, 20.46315982734258, -0.6899006804255913, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.20526331894521496, 0.2700331065248029, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.76615834], dtype=float32), -0.30172002]. 
=============================================
[2019-04-08 15:07:25,936] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39647: loss 6.1993
[2019-04-08 15:07:25,937] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39647: learning rate 0.0000
[2019-04-08 15:07:26,000] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39673: loss 6.1047
[2019-04-08 15:07:26,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39673: learning rate 0.0000
[2019-04-08 15:07:26,229] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39768: loss 8.0965
[2019-04-08 15:07:26,232] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39768: learning rate 0.0000
[2019-04-08 15:07:26,347] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39813: loss 6.1939
[2019-04-08 15:07:26,352] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39813: learning rate 0.0000
[2019-04-08 15:07:26,590] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39909: loss 6.3975
[2019-04-08 15:07:26,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39909: learning rate 0.0000
[2019-04-08 15:07:26,610] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39917: loss 6.1567
[2019-04-08 15:07:26,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39918: learning rate 0.0000
[2019-04-08 15:07:26,696] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39944: loss 6.8096
[2019-04-08 15:07:26,697] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39945: learning rate 0.0000
[2019-04-08 15:07:26,805] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2223944e-16 2.1648863e-09 4.7050372e-02 9.5230228e-01 1.1014092e-09
 1.8553462e-12 4.0267787e-07 3.5382880e-14 5.2474469e-17 1.7073552e-19
 6.4692163e-04], sum to 1.0000
[2019-04-08 15:07:26,807] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4505
[2019-04-08 15:07:26,824] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.033333333333333, 94.0, 90.0, 706.6666666666666, 22.5, 22.63182477138878, -0.2557653750810569, 1.0, 1.0, 29.99999999999998, 18680.41167023053], 
current ob forecast is [], 
actual action is [9.033333333333333, 30.0], 
sim time this is 1510800.0000, 
sim time next is 1511400.0000, 
raw observation next is [4.216666666666667, 93.5, 92.0, 705.3333333333334, 22.5, 22.51989578303278, -0.2582307430474313, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.4782608695652174, 0.5794090489381348, 0.935, 0.30666666666666664, 0.7793738489871087, 0.375, 0.3766579819193983, 0.4139230856508562, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.7158556], dtype=float32), 0.049939476]. 
=============================================
[2019-04-08 15:07:26,860] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-08 15:07:26,860] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:07:26,861] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:07:26,861] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:07:26,861] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:07:26,861] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:07:26,862] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:07:26,866] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run3
[2019-04-08 15:07:26,893] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run3
[2019-04-08 15:07:26,920] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run3
[2019-04-08 15:08:36,361] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.054890666]
[2019-04-08 15:08:36,361] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [-2.25, 40.66666666666667, 0.0, 0.0, 19.0, 19.87056112845719, -0.8689340134225806, 0.0, 1.0, 30.0, 18680.41167023054]
[2019-04-08 15:08:36,361] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:08:36,362] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [7.7936498e-11 1.9981784e-05 6.9739059e-02 9.2705351e-01 1.4620679e-06
 5.3787165e-08 7.8159739e-04 5.8339742e-09 2.8360994e-11 6.1994035e-13
 2.4043564e-03], sampled 0.2841002902978902
[2019-04-08 15:09:07,727] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 5665.6489 161975775.5911 -1285.9730
[2019-04-08 15:09:07,748] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:07,748] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:07,748] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:07,866] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:07,866] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:07,866] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:22,597] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 4872.6112 172922154.3701 -2438.1336
[2019-04-08 15:09:22,618] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:22,618] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:22,618] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:22,723] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:22,723] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:22,723] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:27,370] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 4131.5500 177263274.8366 -2910.4132
[2019-04-08 15:09:27,399] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:27,399] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:27,399] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:09:27,515] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:27,515] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:27,515] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:09:28,401] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 40000, evaluation results [40000.0, 4872.611158702275, 172922154.37009028, -2438.133593739112, 5665.648874701864, 161975775.59108892, -1285.9730049944737, 4131.549981936056, 177263274.8365835, -2910.4132023225984]
[2019-04-08 15:09:28,469] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.8548568e-11 2.6651751e-06 2.1080600e-02 9.7431606e-01 1.6807711e-07
 5.2659637e-09 4.2910865e-03 2.1340998e-09 2.3986011e-12 1.2730051e-13
 3.0945631e-04], sum to 1.0000
[2019-04-08 15:09:28,473] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40044: loss 6.1324
[2019-04-08 15:09:28,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6192
[2019-04-08 15:09:28,475] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40044: learning rate 0.0000
[2019-04-08 15:09:28,484] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 19.0, 20.30516656484657, -0.720839052002422, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [7.2, 30.0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 19.0, 20.23899227787112, -0.731985230241039, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 0.08333333333333333, 0.1865826898225933, 0.256004923252987, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.9706744], dtype=float32), 0.7850746]. 
=============================================
[2019-04-08 15:09:28,698] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40144: loss 6.3080
[2019-04-08 15:09:28,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40144: learning rate 0.0000
[2019-04-08 15:09:28,800] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40201: loss 6.7425
[2019-04-08 15:09:28,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40202: learning rate 0.0000
[2019-04-08 15:09:28,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.2771035e-18 2.1498337e-08 1.3601378e-02 9.8636508e-01 1.6451710e-09
 1.6742911e-13 3.1127995e-05 7.4117808e-16 6.2851573e-18 1.3512311e-19
 2.4001458e-06], sum to 1.0000
[2019-04-08 15:09:28,877] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6877
[2019-04-08 15:09:28,886] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [9.7, 60.5, 0.0, 0.0, 22.5, 23.9698557262393, 0.09262180529337886, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [14.7, 30.0], 
sim time this is 1535400.0000, 
sim time next is 1536000.0000, 
raw observation next is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 22.5, 23.94273955647513, 0.09797482542101214, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.7285318559556788, 0.6066666666666667, 0.0, 0.0, 0.375, 0.49522829637292737, 0.5326582751403374, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.3418379], dtype=float32), 0.8014334]. 
=============================================
[2019-04-08 15:09:28,897] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[91.43269]
 [91.54966]
 [91.64321]
 [91.78011]
 [91.93834]], R is [[91.40180206]
 [91.39883423]
 [91.39589691]
 [91.39299011]
 [91.3901062 ]].
[2019-04-08 15:09:29,554] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40582: loss 6.3947
[2019-04-08 15:09:29,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40582: learning rate 0.0000
[2019-04-08 15:09:30,037] A3C_AGENT_WORKER-Thread-7 INFO:Local step 2500, global step 40837: loss 6.4055
[2019-04-08 15:09:30,038] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 2500, global step 40838: learning rate 0.0000
[2019-04-08 15:09:30,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6206130e-18 1.7889722e-08 3.5859845e-04 9.9959892e-01 1.8998005e-09
 1.8249035e-12 3.5850528e-05 1.8619642e-13 1.5955511e-16 7.3839887e-20
 6.6897323e-06], sum to 1.0000
[2019-04-08 15:09:30,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3955
[2019-04-08 15:09:30,730] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.16666666666667, 59.33333333333334, 0.0, 0.0, 22.5, 24.15203327995697, 0.1044675014532423, 1.0, 1.0, 30.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [15.16666666666667, 30.0], 
sim time this is 1532400.0000, 
sim time next is 1533000.0000, 
raw observation next is [10.08333333333333, 59.66666666666666, 0.0, 0.0, 22.5, 24.08177204387667, 0.1046585978612223, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.7391304347826086, 0.7419205909510619, 0.5966666666666666, 0.0, 0.0, 0.375, 0.5068143369897223, 0.534886199287074, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.3885176], dtype=float32), 0.17508411]. 
=============================================
[2019-04-08 15:09:30,756] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[93.76763 ]
 [93.97507 ]
 [94.106186]
 [94.25122 ]
 [94.41232 ]], R is [[93.49979401]
 [93.52031708]
 [93.58511353]
 [93.6047821 ]
 [93.57978058]].
[2019-04-08 15:09:30,975] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0075405e-16 1.2000566e-08 1.4521699e-02 9.8545617e-01 1.2702320e-09
 4.3676572e-13 9.3194473e-07 4.0815772e-13 5.8186457e-16 2.0161680e-18
 2.1200165e-05], sum to 1.0000
[2019-04-08 15:09:30,976] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1368
[2019-04-08 15:09:30,986] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.416666666666667, 77.0, 0.0, 0.0, 19.0, 23.00921833904399, -0.1033780414212712, 0.0, 1.0, 30.0, 9340.205835115268], 
current ob forecast is [], 
actual action is [11.416666666666668, 30.0], 
sim time this is 1548600.0000, 
sim time next is 1549200.0000, 
raw observation next is [6.233333333333333, 78.0, 0.0, 0.0, 19.0, 22.95193351107547, -0.1190094833585158, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.9565217391304348, 0.6352723915050786, 0.78, 0.0, 0.0, 0.08333333333333333, 0.41266112592295584, 0.46033017221382805, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.8113713], dtype=float32), 0.1656756]. 
=============================================
[2019-04-08 15:09:31,262] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 41532: loss 7.5439
[2019-04-08 15:09:31,264] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 41533: learning rate 0.0000
[2019-04-08 15:09:31,285] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.10239684e-16 1.29068765e-08 1.96810128e-04 9.99800503e-01
 1.82493829e-11 1.53815518e-13 2.38862981e-07 3.51113156e-15
 1.61254073e-18 5.01624356e-22 2.54747329e-06], sum to 1.0000
[2019-04-08 15:09:31,289] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7052
[2019-04-08 15:09:31,305] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [10.78333333333333, 56.66666666666667, 58.66666666666667, 20.66666666666667, 22.5, 23.69915472199597, 0.0633740254173434, 1.0, 1.0, 29.99999999999996, 12453.60778015367], 
current ob forecast is [], 
actual action is [15.78333333333333, 30.0], 
sim time this is 1529400.0000, 
sim time next is 1530000.0000, 
raw observation next is [10.5, 58.0, 44.5, 17.0, 22.5, 24.12845186201991, 0.09596603227522398, 1.0, 1.0, 30.0, 9340.205835115268], 
processed observation next is [1.0, 0.7391304347826086, 0.7534626038781165, 0.58, 0.14833333333333334, 0.01878453038674033, 0.375, 0.5107043218349926, 0.5319886774250747, 1.0, 1.0, 0.3, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([-0.49103153], dtype=float32), -0.7155238]. 
=============================================
[2019-04-08 15:09:31,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[93.565926]
 [93.69942 ]
 [93.79673 ]
 [93.85748 ]
 [93.78058 ]], R is [[93.49034119]
 [93.4961319 ]
 [93.51669312]
 [93.51036072]
 [93.57525635]].
[2019-04-08 15:09:31,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2556232e-14 2.1479497e-08 1.4385525e-03 9.9834871e-01 2.3085445e-08
 4.3385515e-10 1.6676912e-04 6.7077163e-11 4.0400135e-13 9.4995338e-15
 4.5909866e-05], sum to 1.0000
[2019-04-08 15:09:31,341] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0414
[2019-04-08 15:09:31,350] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.9, 80.16666666666667, 0.0, 0.0, 19.0, 21.94491838872061, -0.3251338614902242, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [9.9, 30.0], 
sim time this is 1563000.0000, 
sim time next is 1563600.0000, 
raw observation next is [4.800000000000001, 81.33333333333334, 0.0, 0.0, 19.0, 21.93401337800877, -0.3309963371585847, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.08695652173913043, 0.5955678670360112, 0.8133333333333335, 0.0, 0.0, 0.08333333333333333, 0.3278344481673976, 0.3896678876138051, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-2.1283886], dtype=float32), 0.29282838]. 
=============================================
[2019-04-08 15:09:31,433] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1275381e-12 4.4890464e-07 2.6021351e-03 9.9645644e-01 1.3547171e-07
 4.4747209e-10 6.0572405e-05 1.0024716e-10 1.9153296e-13 2.2331882e-15
 8.8024436e-04], sum to 1.0000
[2019-04-08 15:09:31,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1169
[2019-04-08 15:09:31,448] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.6, 85.0, 0.0, 0.0, 19.0, 21.8158996024615, -0.3545017366034453, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [9.6, 30.0], 
sim time this is 1569600.0000, 
sim time next is 1570200.0000, 
raw observation next is [4.616666666666666, 84.83333333333334, 0.0, 0.0, 19.0, 21.848628349184, -0.3607554611498265, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.17391304347826086, 0.5904893813481072, 0.8483333333333334, 0.0, 0.0, 0.08333333333333333, 0.32071902909866673, 0.3797481796167245, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.0779486], dtype=float32), 0.8701268]. 
=============================================
[2019-04-08 15:09:31,876] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.7391794e-15 5.2499840e-06 2.2605208e-01 7.1891427e-01 4.7971572e-08
 1.1470735e-09 1.7177711e-03 1.3468780e-10 1.4386329e-15 6.0792256e-17
 5.3310681e-02], sum to 1.0000
[2019-04-08 15:09:31,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7613
[2019-04-08 15:09:31,895] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 19.0, 23.17837883174334, 0.01202966681896258, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [12.616666666666667, 30.0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 19.0, 23.19203606904563, 0.03043510894951513, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6759002770083103, 0.74, 0.0, 0.0, 0.08333333333333333, 0.4326696724204693, 0.5101450363165051, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4484893], dtype=float32), -0.33949456]. 
=============================================
[2019-04-08 15:09:32,002] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7591736e-13 2.1432689e-07 6.8675457e-03 9.9281138e-01 1.5921461e-08
 2.0080972e-09 2.4172445e-04 1.8547851e-10 7.0778060e-14 1.2293905e-16
 7.9021673e-05], sum to 1.0000
[2019-04-08 15:09:32,004] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0074
[2019-04-08 15:09:32,014] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.5, 79.0, 0.0, 0.0, 19.0, 21.60607907127741, -0.4332247928576944, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [10.5, 30.0], 
sim time this is 1580400.0000, 
sim time next is 1581000.0000, 
raw observation next is [5.416666666666667, 79.50000000000001, 0.0, 0.0, 22.5, 21.56850912286666, -0.4435282263243006, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.30434782608695654, 0.6126500461680519, 0.7950000000000002, 0.0, 0.0, 0.375, 0.29737576023888845, 0.35215725789189983, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.02219636], dtype=float32), -2.7290194]. 
=============================================
[2019-04-08 15:09:32,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.22819 ]
 [68.16964 ]
 [68.13706 ]
 [68.085014]
 [68.06507 ]], R is [[68.65891266]
 [68.88336945]
 [69.10558319]
 [69.32557678]
 [69.54337311]].
[2019-04-08 15:09:32,280] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.60737726e-15 1.36769387e-07 1.97809130e-01 8.02075684e-01
 1.71369248e-08 3.92707707e-11 3.31301544e-06 1.34958722e-12
 1.42959341e-14 1.36366700e-18 1.11815294e-04], sum to 1.0000
[2019-04-08 15:09:32,283] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8128
[2019-04-08 15:09:32,303] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.15, 72.0, 115.0, 136.0, 22.5, 22.89628430557045, -0.1736221183261216, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [12.15, 30.0], 
sim time this is 1589400.0000, 
sim time next is 1590000.0000, 
raw observation next is [7.333333333333333, 70.66666666666666, 129.1666666666667, 128.0, 22.5, 23.13644056669488, -0.1454853195710363, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.391304347826087, 0.6657433056325024, 0.7066666666666666, 0.4305555555555557, 0.1414364640883978, 0.375, 0.42803671389124, 0.4515048934763212, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.4203031], dtype=float32), -1.8985474]. 
=============================================
[2019-04-08 15:09:32,331] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.0655  ]
 [80.908554]
 [80.146805]
 [79.36586 ]
 [78.396904]], R is [[83.00799561]
 [83.08896637]
 [83.16912842]
 [83.24848938]
 [83.32705688]].
[2019-04-08 15:09:33,415] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7962569e-20 2.9982769e-10 1.3967413e-04 9.9985826e-01 2.2817932e-12
 4.2905404e-15 1.3012622e-09 1.0339289e-16 1.4306990e-19 1.5286321e-22
 2.0506718e-06], sum to 1.0000
[2019-04-08 15:09:33,420] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1026
[2019-04-08 15:09:33,432] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.38333333333333, 50.66666666666666, 68.66666666666667, 12.33333333333333, 22.5, 25.23362081465898, 0.3173664978507382, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [18.38333333333333, 30.0], 
sim time this is 1612200.0000, 
sim time next is 1612800.0000, 
raw observation next is [13.3, 51.0, 64.0, 18.5, 22.5, 25.30691527482658, 0.3311814520883473, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6956521739130435, 0.8310249307479226, 0.51, 0.21333333333333335, 0.020441988950276244, 0.375, 0.6089096062355482, 0.6103938173627824, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.34398845], dtype=float32), 0.01336623]. 
=============================================
[2019-04-08 15:09:33,482] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5040815e-20 2.8330224e-10 1.3745108e-04 9.9986064e-01 2.1761117e-12
 4.0279551e-15 1.2431051e-09 9.6497712e-17 1.2840055e-19 1.3486098e-22
 1.9554784e-06], sum to 1.0000
[2019-04-08 15:09:33,487] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6744
[2019-04-08 15:09:33,509] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 22.5, 25.36612440723051, 0.3419381067275136, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [18.11666666666667, 30.0], 
sim time this is 1613400.0000, 
sim time next is 1614000.0000, 
raw observation next is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 22.5, 25.41607707594438, 0.2524713813443155, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6956521739130435, 0.8208679593721148, 0.52, 0.18222222222222223, 0.03406998158379374, 0.375, 0.6180064229953649, 0.5841571271147719, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.34398845], dtype=float32), 0.01336623]. 
=============================================
[2019-04-08 15:09:33,514] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[96.95058 ]
 [96.88984 ]
 [96.729904]
 [96.63512 ]
 [96.59633 ]], R is [[96.92797089]
 [96.95869446]
 [96.90015411]
 [96.84220123]
 [96.78482819]].
[2019-04-08 15:09:33,801] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.8879598e-12 1.5401253e-06 4.4352310e-03 9.9511504e-01 2.6018631e-07
 1.8743251e-09 9.2253824e-05 1.3486094e-10 3.4146053e-13 7.3785015e-15
 3.5567002e-04], sum to 1.0000
[2019-04-08 15:09:33,806] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7491
[2019-04-08 15:09:33,815] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.633333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 21.91226552114567, -0.3506952410641644, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [9.633333333333333, 30.0], 
sim time this is 1570800.0000, 
sim time next is 1571400.0000, 
raw observation next is [4.65, 84.5, 0.0, 0.0, 19.0, 21.88335127426855, -0.3641172531957337, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.17391304347826086, 0.5914127423822716, 0.845, 0.0, 0.0, 0.08333333333333333, 0.3236126061890457, 0.37862758226808874, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.4403204], dtype=float32), 0.90689856]. 
=============================================
[2019-04-08 15:09:34,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.38417229e-12 1.72821876e-06 1.67165999e-03 9.98163879e-01
 1.18555164e-07 2.93345681e-09 1.32176392e-05 1.44176379e-10
 3.32246810e-14 1.92778007e-15 1.49388681e-04], sum to 1.0000
[2019-04-08 15:09:34,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7621
[2019-04-08 15:09:34,844] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.616666666666666, 84.83333333333334, 0.0, 0.0, 19.0, 21.86285142826978, -0.3600026632793404, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [9.616666666666667, 30.0], 
sim time this is 1570200.0000, 
sim time next is 1570800.0000, 
raw observation next is [4.633333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 21.82739725052009, -0.3690052957276309, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.17391304347826086, 0.5909510618651893, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.31894977087667414, 0.37699823475745636, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.3015058], dtype=float32), -0.5176348]. 
=============================================
[2019-04-08 15:09:35,758] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0184639e-12 1.6613939e-07 1.8084276e-01 8.1833243e-01 5.9548086e-08
 8.4055174e-10 1.6256201e-05 1.9040317e-10 1.8496096e-13 1.3380242e-14
 8.0827530e-04], sum to 1.0000
[2019-04-08 15:09:35,758] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9455
[2019-04-08 15:09:35,768] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.416666666666667, 96.16666666666667, 0.0, 0.0, 19.0, 22.2413272464757, -0.2396213083963855, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [10.416666666666668, 30.0], 
sim time this is 1663800.0000, 
sim time next is 1664400.0000, 
raw observation next is [5.333333333333334, 95.33333333333334, 0.0, 0.0, 19.0, 22.24763816720994, -0.2396812456195946, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.35396984726749486, 0.4201062514601351, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.7394335], dtype=float32), -0.28265214]. 
=============================================
[2019-04-08 15:09:36,131] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.7907540e-19 7.4570583e-10 9.3366991e-04 9.9905568e-01 9.0240350e-11
 7.7190554e-13 1.2334330e-07 3.0513809e-15 3.2186077e-18 1.2710454e-21
 1.0522851e-05], sum to 1.0000
[2019-04-08 15:09:36,133] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8536
[2019-04-08 15:09:36,146] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 80.0, 0.0, 22.5, 23.3093096557788, -0.07143225581642879, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1693800.0000, 
sim time next is 1694400.0000, 
raw observation next is [1.1, 88.0, 75.5, 0.0, 22.5, 23.3872116601692, -0.06112167046717818, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.25166666666666665, 0.0, 0.375, 0.4489343050141, 0.47962610984427395, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.8117298], dtype=float32), 2.3095238]. 
=============================================
[2019-04-08 15:09:36,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0212323e-13 5.4244208e-07 4.2693345e-03 9.9310935e-01 2.3560915e-07
 2.1538986e-09 2.2353954e-04 5.8010294e-11 4.4324112e-13 2.5196763e-16
 2.3969819e-03], sum to 1.0000
[2019-04-08 15:09:36,473] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0476
[2019-04-08 15:09:36,483] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.433333333333334, 92.0, 0.0, 0.0, 22.5, 22.14172654021797, -0.2746663330686951, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [9.433333333333334, 30.0], 
sim time this is 1668000.0000, 
sim time next is 1668600.0000, 
raw observation next is [4.15, 92.0, 0.0, 0.0, 22.5, 22.12453720612199, -0.2679911515062076, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.30434782608695654, 0.5775623268698062, 0.92, 0.0, 0.0, 0.375, 0.34371143384349906, 0.41066961616459746, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.9120627], dtype=float32), -0.50088876]. 
=============================================
[2019-04-08 15:09:36,603] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4871028e-16 7.1631433e-07 3.4800121e-01 6.5163046e-01 1.2542349e-09
 4.6222995e-10 1.9755473e-04 1.2976033e-12 3.3148751e-15 8.3160246e-17
 1.7009230e-04], sum to 1.0000
[2019-04-08 15:09:36,611] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3309
[2019-04-08 15:09:36,618] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [1.85, 92.0, 53.0, 0.0, 22.5, 22.94632262480806, -0.1421674373688059, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.85, 25.0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 22.5, 23.04370639524852, -0.1290885218355068, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5106186518928902, 0.92, 0.1838888888888889, 0.0, 0.375, 0.42030886627071, 0.45697049272149776, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6816912], dtype=float32), 1.0274384]. 
=============================================
[2019-04-08 15:09:36,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.25314619e-14 6.19212813e-07 1.27737755e-02 9.79260266e-01
 1.90812372e-07 1.97604211e-10 5.27398370e-05 2.39064157e-10
 2.27253803e-13 3.25381693e-15 7.91241787e-03], sum to 1.0000
[2019-04-08 15:09:36,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4118
[2019-04-08 15:09:36,785] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 96.33333333333333, 0.0, 0.0, 19.0, 22.84586803116032, -0.09940411430726594, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [12.0, 30.0], 
sim time this is 1650000.0000, 
sim time next is 1650600.0000, 
raw observation next is [6.9, 96.5, 0.0, 0.0, 19.0, 22.88644353097859, -0.09681684914808246, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.08695652173913043, 0.6537396121883658, 0.965, 0.0, 0.0, 0.08333333333333333, 0.4072036275815491, 0.46772771695063914, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.1461554], dtype=float32), -0.86671805]. 
=============================================
[2019-04-08 15:09:36,865] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0325762e-14 2.7991203e-07 1.7745897e-01 8.2224303e-01 1.5152977e-09
 7.5120736e-11 9.3719151e-05 8.4408408e-13 2.9933083e-16 9.1402848e-19
 2.0395279e-04], sum to 1.0000
[2019-04-08 15:09:36,874] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6582
[2019-04-08 15:09:36,884] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 86.0, 107.0, 0.0, 22.5, 23.48660461411975, -0.05740822684444926, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1686600.0000, 
sim time next is 1687200.0000, 
raw observation next is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 22.5, 23.41352647177593, -0.07284418016655775, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8666666666666667, 0.3527777777777777, 0.0, 0.375, 0.4511272059813276, 0.4757186066111474, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([2.0382168], dtype=float32), -1.6524209]. 
=============================================
[2019-04-08 15:09:37,000] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6070334e-15 3.2429607e-08 1.7205828e-01 8.2728845e-01 1.6718305e-09
 2.0920364e-12 6.1939968e-06 8.4607196e-13 2.9572353e-16 7.7179665e-18
 6.4708630e-04], sum to 1.0000
[2019-04-08 15:09:37,002] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5690
[2019-04-08 15:09:37,012] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.00000000000001, 102.3333333333333, 0.0, 22.5, 22.69167060166887, -0.1610504085350458, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1689000.0000, 
sim time next is 1689600.0000, 
raw observation next is [1.1, 88.0, 101.1666666666667, 0.0, 22.5, 22.76332670248106, -0.1297290852654735, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3372222222222223, 0.0, 0.375, 0.3969438918734216, 0.45675697157817546, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.53757274], dtype=float32), 2.2450955]. 
=============================================
[2019-04-08 15:09:37,277] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6440884e-14 8.2870697e-09 5.8960952e-03 9.9402583e-01 6.8907346e-09
 1.2885883e-10 3.9055917e-06 4.2148295e-11 4.2332869e-15 1.2060144e-16
 7.4204116e-05], sum to 1.0000
[2019-04-08 15:09:37,282] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-08 15:09:37,290] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.866666666666667, 97.0, 0.0, 0.0, 19.0, 22.31438645987951, -0.236550594384932, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [10.866666666666667, 30.0], 
sim time this is 1662000.0000, 
sim time next is 1662600.0000, 
raw observation next is [5.683333333333334, 97.0, 0.0, 0.0, 19.0, 22.22971151567895, -0.2430106244200674, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.21739130434782608, 0.6200369344413666, 0.97, 0.0, 0.0, 0.08333333333333333, 0.3524759596399125, 0.4189964585266442, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.328759], dtype=float32), -0.84551054]. 
=============================================
[2019-04-08 15:09:38,186] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4413070e-17 2.1165278e-08 3.0131156e-03 9.9692279e-01 1.1662004e-09
 2.2092340e-13 1.4461801e-07 3.0020748e-13 4.0320194e-17 1.0627207e-17
 6.3892192e-05], sum to 1.0000
[2019-04-08 15:09:38,187] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6496
[2019-04-08 15:09:38,187] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5998800e-17 1.0391438e-09 1.2458274e-04 9.9981982e-01 6.3729955e-10
 7.5905731e-13 3.3449960e-05 6.9446043e-14 5.8316469e-18 5.0989975e-20
 2.2193955e-05], sum to 1.0000
[2019-04-08 15:09:38,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0435
[2019-04-08 15:09:38,196] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 22.5, 23.07303408172334, -0.1058078220165907, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [6.433333333333333, 30.0], 
sim time this is 1698000.0000, 
sim time next is 1698600.0000, 
raw observation next is [1.516666666666667, 82.16666666666666, 45.33333333333334, 0.0, 22.5, 23.19601160818911, -0.09276313458983261, 1.0, 1.0, 30.0, 34309.40164724125], 
processed observation next is [1.0, 0.6521739130434783, 0.5046168051708219, 0.8216666666666665, 0.15111111111111114, 0.0, 0.375, 0.4330009673490925, 0.4690789551367225, 1.0, 1.0, 0.3, 0.16337810308210118], 
reward next is 0.8366, 
noisyNet noise sample is [array([0.8347563], dtype=float32), 1.7054092]. 
=============================================
[2019-04-08 15:09:38,205] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 85.33333333333334, 103.0, 0.0, 22.5, 23.54680723540245, -0.05351846430282248, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1686000.0000, 
sim time next is 1686600.0000, 
raw observation next is [1.1, 86.0, 107.0, 0.0, 22.5, 23.48449305512324, -0.06546489303502874, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.86, 0.3566666666666667, 0.0, 0.375, 0.4570410879269368, 0.4781783689883237, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.5444385], dtype=float32), 1.2827388]. 
=============================================
[2019-04-08 15:09:38,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2777504e-16 3.7095983e-08 3.3143165e-03 9.9668413e-01 1.1412046e-09
 3.6387653e-11 3.8806860e-07 1.0474048e-15 4.5506170e-17 1.6023281e-19
 1.1433742e-06], sum to 1.0000
[2019-04-08 15:09:38,377] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5718
[2019-04-08 15:09:38,393] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.1, 88.0, 101.1666666666667, 0.0, 22.5, 23.21571345537876, -0.07933718557318374, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [6.1, 30.0], 
sim time this is 1689600.0000, 
sim time next is 1690200.0000, 
raw observation next is [1.1, 88.0, 100.0, 0.0, 22.5, 23.27409360232832, -0.07246244237126802, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.3333333333333333, 0.0, 0.375, 0.43950780019402674, 0.4758458525429106, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.19622648], dtype=float32), -0.61976045]. 
=============================================
[2019-04-08 15:09:39,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9286383e-11 7.2566736e-06 7.1976408e-02 9.2574316e-01 2.1625843e-07
 3.3846320e-08 1.0098706e-04 1.1315078e-09 1.2281369e-13 5.3579565e-15
 2.1718997e-03], sum to 1.0000
[2019-04-08 15:09:39,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3772
[2019-04-08 15:09:39,934] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 20.43856573175861, -0.6172416774769164, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [5.5, 30.0], 
sim time this is 1730400.0000, 
sim time next is 1731000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 20.44655050978956, -0.6246243821992714, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.20387920914913002, 0.29179187260024286, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.47865185], dtype=float32), -0.3905155]. 
=============================================
[2019-04-08 15:09:39,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[67.51857 ]
 [68.98615 ]
 [70.34802 ]
 [71.744415]
 [72.900055]], R is [[66.42620087]
 [66.67298889]
 [66.91730499]
 [67.15917969]
 [67.39863586]].
[2019-04-08 15:09:40,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0624943e-10 6.0147695e-06 1.9155439e-02 9.7557271e-01 6.3960119e-06
 2.3631132e-07 1.9607525e-03 5.9800733e-08 2.8790531e-10 5.2878875e-11
 3.2983262e-03], sum to 1.0000
[2019-04-08 15:09:40,083] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0450
[2019-04-08 15:09:40,092] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8999999999999999, 85.0, 0.0, 0.0, 19.0, 19.55222376926451, -0.8538703930246081, 0.0, 1.0, 30.0, 20626.09476741194], 
current ob forecast is [], 
actual action is [4.1, 30.0], 
sim time this is 1747800.0000, 
sim time next is 1748400.0000, 
raw observation next is [-1.0, 85.66666666666667, 0.0, 0.0, 19.0, 19.52572719264519, -0.8640992808425408, 0.0, 1.0, 30.0, 20704.7027432552], 
processed observation next is [0.0, 0.21739130434782608, 0.4349030470914128, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.12714393272043237, 0.21196690638581972, 0.0, 1.0, 0.3, 0.09859382258692952], 
reward next is 0.9014, 
noisyNet noise sample is [array([0.23671487], dtype=float32), -0.35258487]. 
=============================================
[2019-04-08 15:09:40,095] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4951400e-09 9.6625947e-05 2.6755658e-01 7.2749186e-01 3.0033414e-06
 8.2178622e-07 1.9304355e-03 4.2497653e-08 1.2652421e-10 2.4301521e-11
 2.9206600e-03], sum to 1.0000
[2019-04-08 15:09:40,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0311
[2019-04-08 15:09:40,105] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.7, 87.0, 9.33333333333333, 0.0, 19.0, 19.11204724917565, -0.9613383569384865, 0.0, 1.0, 30.0, 21267.7455329869], 
current ob forecast is [], 
actual action is [3.3, 25.0], 
sim time this is 1756200.0000, 
sim time next is 1756800.0000, 
raw observation next is [-1.7, 87.0, 13.5, 0.0, 19.0, 19.07572443004637, -0.9717717005623401, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4155124653739613, 0.87, 0.045, 0.0, 0.08333333333333333, 0.08964370250386426, 0.17607609981255332, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6719515], dtype=float32), -0.4391506]. 
=============================================
[2019-04-08 15:09:40,893] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3406549e-09 1.7092624e-04 7.2433907e-03 9.8689055e-01 4.3920077e-06
 3.3602720e-07 3.4940038e-03 1.6094894e-07 6.2220995e-10 3.8321745e-11
 2.1962670e-03], sum to 1.0000
[2019-04-08 15:09:40,896] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4737
[2019-04-08 15:09:40,914] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.533333333333333, 87.0, 0.0, 0.0, 19.0, 19.29106635428461, -0.9135071833244552, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [3.466666666666667, 30.0], 
sim time this is 1752000.0000, 
sim time next is 1752600.0000, 
raw observation next is [-1.616666666666667, 87.0, 0.0, 0.0, 19.0, 19.25304705240733, -0.9230962352854614, 0.0, 1.0, 30.0, 29441.63876579254], 
processed observation next is [0.0, 0.2608695652173913, 0.4178208679593721, 0.87, 0.0, 0.0, 0.08333333333333333, 0.10442058770061073, 0.1923012549048462, 0.0, 1.0, 0.3, 0.14019827983710734], 
reward next is 0.8598, 
noisyNet noise sample is [array([1.8236866], dtype=float32), 1.6684159]. 
=============================================
[2019-04-08 15:09:41,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.0015915e-10 1.2233292e-05 6.8565451e-02 9.0304780e-01 3.2473461e-05
 2.3076375e-07 2.5135966e-03 5.6174315e-07 3.4257521e-09 4.2467266e-11
 2.5827564e-02], sum to 1.0000
[2019-04-08 15:09:41,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8642
[2019-04-08 15:09:41,105] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 65.0, 0.0, 19.0, 18.72193438764346, -1.042875792972094, 0.0, 1.0, 30.0, 21455.96537465048], 
current ob forecast is [], 
actual action is [3.0, 30.0], 
sim time this is 1762200.0000, 
sim time next is 1762800.0000, 
raw observation next is [-2.1, 85.66666666666667, 70.33333333333334, 0.0, 19.0, 18.69176089401879, -1.040003829439307, 0.0, 1.0, 30.0, 21455.85907374936], 
processed observation next is [0.0, 0.391304347826087, 0.404432132963989, 0.8566666666666667, 0.23444444444444448, 0.0, 0.08333333333333333, 0.05764674116823235, 0.15333205685356432, 0.0, 1.0, 0.3, 0.10217075749404458], 
reward next is 0.5808, 
noisyNet noise sample is [array([1.2250121], dtype=float32), 0.3316324]. 
=============================================
[2019-04-08 15:09:41,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.31672761e-10 1.06432935e-05 2.25672629e-02 9.74809051e-01
 7.93094296e-06 6.17779961e-07 3.38348647e-04 2.33373516e-08
 1.94738295e-10 1.39266315e-11 2.26610107e-03], sum to 1.0000
[2019-04-08 15:09:41,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8106
[2019-04-08 15:09:41,117] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 19.0, 18.54432362718931, -1.083298116961404, 0.0, 1.0, 30.0, 21478.87757165405], 
current ob forecast is [], 
actual action is [2.366666666666667, 30.0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [-2.716666666666667, 83.0, 123.6666666666667, 0.0, 19.0, 18.56062148953357, -1.086979699222275, 0.0, 1.0, 30.0, 21465.41569598319], 
processed observation next is [0.0, 0.5217391304347826, 0.3873499538319483, 0.83, 0.4122222222222223, 0.0, 0.08333333333333333, 0.04671845746113087, 0.13767343359257497, 0.0, 1.0, 0.3, 0.10221626521896757], 
reward next is 0.3586, 
noisyNet noise sample is [array([-1.9206109], dtype=float32), -0.082293585]. 
=============================================
[2019-04-08 15:09:41,692] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 47382: loss 3.9618
[2019-04-08 15:09:41,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 47382: learning rate 0.0000
[2019-04-08 15:09:41,712] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3000, global step 47393: loss 1.1404
[2019-04-08 15:09:41,714] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3000, global step 47393: learning rate 0.0000
[2019-04-08 15:09:41,786] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47423: loss -3.5051
[2019-04-08 15:09:41,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47424: learning rate 0.0000
[2019-04-08 15:09:42,463] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47708: loss 3.7462
[2019-04-08 15:09:42,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47708: learning rate 0.0000
[2019-04-08 15:09:42,661] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47796: loss -0.5637
[2019-04-08 15:09:42,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47796: learning rate 0.0000
[2019-04-08 15:09:42,746] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47842: loss -2.0140
[2019-04-08 15:09:42,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47842: learning rate 0.0000
[2019-04-08 15:09:42,753] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47847: loss 1.1462
[2019-04-08 15:09:42,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47847: learning rate 0.0000
[2019-04-08 15:09:42,885] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47916: loss 3.3010
[2019-04-08 15:09:42,889] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47916: learning rate 0.0000
[2019-04-08 15:09:43,055] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48012: loss -5.8258
[2019-04-08 15:09:43,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48013: learning rate 0.0000
[2019-04-08 15:09:43,099] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48031: loss 2.4088
[2019-04-08 15:09:43,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48031: learning rate 0.0000
[2019-04-08 15:09:43,110] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48041: loss 3.3458
[2019-04-08 15:09:43,121] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48041: learning rate 0.0000
[2019-04-08 15:09:43,229] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.8696534e-11 6.5575932e-06 9.5985159e-03 9.8917854e-01 3.0797639e-06
 6.4496987e-08 7.3748204e-04 9.1931831e-09 5.9349248e-11 2.8445560e-12
 4.7576384e-04], sum to 1.0000
[2019-04-08 15:09:43,233] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9758
[2019-04-08 15:09:43,258] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 83.66666666666667, 121.3333333333333, 0.0, 19.0, 18.60631613308822, -1.068196879984393, 0.0, 1.0, 30.0, 21414.34361861778], 
current ob forecast is [], 
actual action is [2.7, 30.0], 
sim time this is 1770600.0000, 
sim time next is 1771200.0000, 
raw observation next is [-2.3, 83.0, 122.5, 0.0, 19.0, 18.5909024266966, -1.071780344593858, 0.0, 1.0, 30.0, 21401.78015938362], 
processed observation next is [0.0, 0.5217391304347826, 0.3988919667590028, 0.83, 0.4083333333333333, 0.0, 0.08333333333333333, 0.049241868891383454, 0.14273988513538063, 0.0, 1.0, 0.3, 0.10191323885420771], 
reward next is 0.3120, 
noisyNet noise sample is [array([-1.156612], dtype=float32), -0.54800785]. 
=============================================
[2019-04-08 15:09:43,618] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48333: loss 3.2636
[2019-04-08 15:09:43,619] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48333: loss -1.9736
[2019-04-08 15:09:43,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48333: learning rate 0.0000
[2019-04-08 15:09:43,621] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48333: learning rate 0.0000
[2019-04-08 15:09:44,161] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48647: loss -1.9279
[2019-04-08 15:09:44,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48648: learning rate 0.0000
[2019-04-08 15:09:44,481] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7234248e-08 7.1209110e-04 1.7447068e-01 7.4445075e-01 1.1330858e-04
 7.4876793e-06 1.9072536e-02 4.4673129e-06 6.4924734e-08 1.8904917e-08
 6.1168585e-02], sum to 1.0000
[2019-04-08 15:09:44,482] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3602
[2019-04-08 15:09:44,518] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 75.0, 152.0, 66.0, 19.0, 16.42958793496033, -1.643500966428556, 0.0, 1.0, 30.0, 29385.54412296768], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 1854000.0000, 
sim time next is 1854600.0000, 
raw observation next is [-5.5, 74.33333333333333, 162.6666666666667, 71.0, 19.0, 16.45081373644906, -1.636847324114385, 0.0, 1.0, 30.0, 28559.65148916932], 
processed observation next is [0.0, 0.4782608695652174, 0.3102493074792244, 0.7433333333333333, 0.5422222222222224, 0.07845303867403315, 0.08333333333333333, -0.12909885529591167, -0.04561577470479502, 0.0, 1.0, 0.3, 0.1359983404246158], 
reward next is 0.2123, 
noisyNet noise sample is [array([0.40074915], dtype=float32), -0.13567649]. 
=============================================
[2019-04-08 15:09:44,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8666900e-07 4.6233227e-04 9.1756426e-02 8.8491625e-01 2.3304009e-04
 2.5800955e-05 1.1263003e-02 9.3325443e-06 6.4420533e-07 5.4642314e-08
 1.1332756e-02], sum to 1.0000
[2019-04-08 15:09:44,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6566
[2019-04-08 15:09:44,927] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 19.0, 16.136981941755, -1.707342218335545, 0.0, 1.0, 30.0, 38842.99507798285], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 30.0], 
sim time this is 1846200.0000, 
sim time next is 1846800.0000, 
raw observation next is [-6.7, 78.0, 87.5, 47.0, 19.0, 16.15573003490223, -1.699218502512361, 0.0, 1.0, 30.0, 33560.55231930586], 
processed observation next is [0.0, 0.391304347826087, 0.2770083102493075, 0.78, 0.2916666666666667, 0.051933701657458566, 0.08333333333333333, -0.15368916375814745, -0.06640616750412032, 0.0, 1.0, 0.3, 0.15981215390145648], 
reward next is 0.1875, 
noisyNet noise sample is [array([0.38151994], dtype=float32), -0.5387388]. 
=============================================
[2019-04-08 15:09:45,017] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3000, global step 49042: loss 1.4979
[2019-04-08 15:09:45,018] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3000, global step 49042: learning rate 0.0000
[2019-04-08 15:09:45,610] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 49266: loss -3.6601
[2019-04-08 15:09:45,630] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 49266: learning rate 0.0000
[2019-04-08 15:09:45,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6228448e-07 1.1314434e-03 1.6516219e-01 7.5497222e-01 4.1487033e-04
 6.1831357e-05 2.0117400e-02 1.2223320e-05 2.1855811e-07 8.4099927e-09
 5.8127139e-02], sum to 1.0000
[2019-04-08 15:09:45,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4374
[2019-04-08 15:09:45,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3907607e-08 7.4534459e-05 4.3712795e-01 5.3377628e-01 1.2067554e-04
 4.3492440e-05 2.5512453e-03 1.9671459e-06 1.1087567e-08 3.9084366e-10
 2.6303824e-02], sum to 1.0000
[2019-04-08 15:09:45,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3987
[2019-04-08 15:09:45,978] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.333333333333334, 78.0, 127.8333333333333, 78.33333333333334, 19.0, 16.25668474884516, -1.679499530071187, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-1.333333333333334, 30.0], 
sim time this is 1848000.0000, 
sim time next is 1848600.0000, 
raw observation next is [-6.15, 78.0, 148.0, 94.0, 19.0, 16.26714399037249, -1.672744193294499, 0.0, 1.0, 30.0, 41987.53149957749], 
processed observation next is [0.0, 0.391304347826087, 0.29224376731301943, 0.78, 0.49333333333333335, 0.10386740331491713, 0.08333333333333333, -0.14440466746895927, -0.05758139776483303, 0.0, 1.0, 0.3, 0.19994062618846423], 
reward next is 0.1046, 
noisyNet noise sample is [array([1.2611183], dtype=float32), -0.16021995]. 
=============================================
[2019-04-08 15:09:46,007] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0819461e-08 3.5652418e-05 7.7670641e-02 9.1274375e-01 2.4504248e-05
 8.1351955e-06 3.8949004e-03 6.1561855e-07 4.6020801e-09 1.1090547e-09
 5.6217546e-03], sum to 1.0000
[2019-04-08 15:09:46,007] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5936
[2019-04-08 15:09:46,030] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.916666666666667, 71.0, 141.3333333333333, 26.99999999999999, 19.0, 16.73018383911145, -1.551795490278677, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.08333333333333304, 25.0], 
sim time this is 1858200.0000, 
sim time next is 1858800.0000, 
raw observation next is [-4.833333333333334, 71.0, 130.6666666666667, 13.5, 19.0, 16.90905843426361, -1.542427141513221, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.32871652816251157, 0.71, 0.4355555555555557, 0.014917127071823204, 0.08333333333333333, -0.09091179714469917, -0.01414238050440697, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0349141], dtype=float32), -0.99907756]. 
=============================================
[2019-04-08 15:09:46,046] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 76.33333333333333, 43.33333333333333, 0.0, 19.0, 17.01056705856232, -1.564706931510172, 0.0, 1.0, 30.0, 26504.18187659333], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 1872600.0000, 
sim time next is 1873200.0000, 
raw observation next is [-4.5, 77.66666666666667, 36.16666666666666, 0.0, 19.0, 16.99800717808556, -1.563846692930128, 0.0, 1.0, 30.0, 26522.6954739979], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.7766666666666667, 0.12055555555555553, 0.0, 0.08333333333333333, -0.08349940182620319, -0.021282230976709338, 0.0, 1.0, 0.3, 0.12629854987618047], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6115087], dtype=float32), -1.0718353]. 
=============================================
[2019-04-08 15:09:46,518] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.63572815e-07 3.40011320e-04 1.13045335e-01 8.61380935e-01
 8.88735012e-05 2.90545177e-05 5.21864090e-03 1.07399399e-06
 3.31690657e-08 2.61964739e-09 1.98958274e-02], sum to 1.0000
[2019-04-08 15:09:46,518] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8631
[2019-04-08 15:09:46,550] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666667, 71.0, 128.3333333333333, 6.666666666666665, 19.0, 16.62775752846418, -1.619932454825245, 0.0, 1.0, 30.0, 27895.08846954314], 
current ob forecast is [], 
actual action is [0.33333333333333304, 30.0], 
sim time this is 1860000.0000, 
sim time next is 1860600.0000, 
raw observation next is [-4.583333333333333, 71.0, 136.6666666666667, 13.33333333333333, 19.0, 16.63923399763642, -1.615931597084504, 0.0, 1.0, 30.0, 27645.55861058804], 
processed observation next is [0.0, 0.5217391304347826, 0.3356417359187443, 0.71, 0.4555555555555557, 0.0147329650092081, 0.08333333333333333, -0.11339716686363162, -0.03864386569483469, 0.0, 1.0, 0.3, 0.13164551719327638], 
reward next is 0.1148, 
noisyNet noise sample is [array([-0.2869127], dtype=float32), 0.6425021]. 
=============================================
[2019-04-08 15:09:47,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.6685715e-08 9.3626638e-04 7.6419555e-02 9.0075809e-01 1.7702173e-04
 2.9041723e-05 1.1170767e-02 2.5717557e-06 2.4119236e-08 2.0141913e-09
 1.0506577e-02], sum to 1.0000
[2019-04-08 15:09:47,726] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0491
[2019-04-08 15:09:47,760] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 19.0, 16.99176265679413, -1.576342739200611, 0.0, 1.0, 30.0, 26479.96404928851], 
current ob forecast is [], 
actual action is [0.33333333333333304, 25.0], 
sim time this is 1876800.0000, 
sim time next is 1877400.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 19.0, 17.00502820342628, -1.58116295585658, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.3310249307479225, 0.845, 0.0, 0.0, 0.08333333333333333, -0.08291431638114322, -0.027054318618860023, 0.0, 1.0, 0.2, 0.0], 
reward next is 0.1327, 
noisyNet noise sample is [array([-0.3527957], dtype=float32), -0.15307906]. 
=============================================
[2019-04-08 15:09:48,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3586776e-08 1.5813712e-04 1.3649738e-01 8.5013449e-01 4.0968018e-05
 1.2994309e-05 5.0082821e-03 7.7620575e-07 2.7735299e-08 1.6138184e-09
 8.1468457e-03], sum to 1.0000
[2019-04-08 15:09:48,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3754
[2019-04-08 15:09:48,326] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.866666666666667, 84.0, 0.0, 0.0, 19.0, 16.87100479311481, -1.615403552517551, 0.0, 1.0, 30.0, 24379.64839256187], 
current ob forecast is [], 
actual action is [0.13333333333333286, 30.0], 
sim time this is 1884000.0000, 
sim time next is 1884600.0000, 
raw observation next is [-5.05, 84.5, 0.0, 0.0, 19.0, 16.83829870080368, -1.621101505957783, 0.0, 1.0, 30.0, 22786.73658275772], 
processed observation next is [0.0, 0.8260869565217391, 0.32271468144044324, 0.845, 0.0, 0.0, 0.08333333333333333, -0.09680844159969333, -0.0403671686525943, 0.0, 1.0, 0.3, 0.10850826944170343], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.59463847], dtype=float32), -0.08447192]. 
=============================================
[2019-04-08 15:09:48,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8945348e-08 2.3964327e-05 2.0764157e-02 9.7666776e-01 1.6494254e-05
 1.8320518e-06 6.2677835e-04 1.8778412e-07 6.4123049e-09 2.7255789e-10
 1.8987560e-03], sum to 1.0000
[2019-04-08 15:09:48,602] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4527
[2019-04-08 15:09:48,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.3001274e-08 1.5749554e-04 2.9945079e-02 9.4840550e-01 9.2458649e-05
 3.9008206e-05 5.1382156e-03 1.9147101e-06 5.4811363e-08 2.2678999e-09
 1.6220091e-02], sum to 1.0000
[2019-04-08 15:09:48,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1926
[2019-04-08 15:09:48,630] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 84.0, 0.0, 0.0, 19.0, 16.6537124080853, -1.661763683950513, 0.0, 1.0, 30.0, 23457.81742446946], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 1888800.0000, 
sim time next is 1889400.0000, 
raw observation next is [-5.6, 83.5, 0.0, 0.0, 19.0, 16.61533277905186, -1.669524910115377, 0.0, 1.0, 30.0, 23524.56726469762], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.835, 0.0, 0.0, 0.08333333333333333, -0.11538893507901153, -0.05650830337179236, 0.0, 1.0, 0.3, 0.11202174887951248], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9230225], dtype=float32), 0.43827394]. 
=============================================
[2019-04-08 15:09:48,635] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 19.0, 17.09836406238812, -1.553846641475243, 0.0, 1.0, 30.0, 26454.31840425759], 
current ob forecast is [], 
actual action is [0.25, 30.0], 
sim time this is 1877400.0000, 
sim time next is 1878000.0000, 
raw observation next is [-4.833333333333333, 85.0, 0.0, 0.0, 19.0, 17.0863598078127, -1.558498238990438, 0.0, 1.0, 30.0, 26450.76063393309], 
processed observation next is [0.0, 0.7391304347826086, 0.3287165281625116, 0.85, 0.0, 0.0, 0.08333333333333333, -0.07613668268227514, -0.01949941299681268, 0.0, 1.0, 0.3, 0.125956003018729], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0657176], dtype=float32), -0.4094037]. 
=============================================
[2019-04-08 15:09:48,642] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[41.986855]
 [42.13436 ]
 [42.21717 ]
 [42.27265 ]
 [42.320454]], R is [[41.44077301]
 [41.16067505]
 [40.74906921]
 [40.34157944]
 [39.93816376]].
[2019-04-08 15:09:48,965] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2529964e-08 8.4161898e-04 6.7327529e-02 9.0644860e-01 6.8308553e-04
 9.5030868e-05 6.7404783e-03 6.3624284e-06 3.4085581e-08 3.5792771e-09
 1.7857246e-02], sum to 1.0000
[2019-04-08 15:09:48,966] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8869
[2019-04-08 15:09:49,001] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 81.66666666666667, 19.66666666666667, 0.0, 19.0, 16.98660792047946, -1.575562523344427, 0.0, 1.0, 30.0, 26572.11977531875], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 1875000.0000, 
sim time next is 1875600.0000, 
raw observation next is [-4.5, 83.0, 15.0, 0.0, 19.0, 16.97388224679178, -1.57816663215594, 0.0, 1.0, 30.0, 26573.62307286818], 
processed observation next is [0.0, 0.7391304347826086, 0.3379501385041552, 0.83, 0.05, 0.0, 0.08333333333333333, -0.08550981276735155, -0.02605554405198, 0.0, 1.0, 0.3, 0.12654106225175324], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.838169], dtype=float32), -0.410515]. 
=============================================
[2019-04-08 15:09:50,488] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.6164630e-12 4.7015787e-06 8.0883680e-03 9.8790133e-01 2.8321310e-07
 2.8293476e-09 6.1470683e-04 5.3677424e-10 1.1670702e-13 3.1632520e-14
 3.3905746e-03], sum to 1.0000
[2019-04-08 15:09:50,490] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2998
[2019-04-08 15:09:50,525] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.266666666666667, 65.0, 212.0, 3.333333333333333, 22.5, 18.48921418654145, -1.372761314981111, 1.0, 1.0, 30.0, 24355.56988541451], 
current ob forecast is [], 
actual action is [0.7333333333333334, 30.0], 
sim time this is 1946400.0000, 
sim time next is 1947000.0000, 
raw observation next is [-4.083333333333333, 65.0, 197.0, 2.666666666666667, 22.5, 18.50066102851925, -1.36769149508933, 1.0, 1.0, 30.0, 24314.70358216783], 
processed observation next is [1.0, 0.5217391304347826, 0.34949215143120965, 0.65, 0.6566666666666666, 0.002946593001841621, 0.375, 0.04172175237660417, 0.04410283497022335, 1.0, 1.0, 0.3, 0.11578430277222776], 
reward next is 0.1267, 
noisyNet noise sample is [array([0.80049396], dtype=float32), -0.33395588]. 
=============================================
[2019-04-08 15:09:50,549] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[64.26547 ]
 [64.04761 ]
 [63.68962 ]
 [63.23133 ]
 [62.927765]], R is [[64.0838623 ]
 [63.51817703]
 [63.02005005]
 [62.38985062]
 [62.02575302]].
[2019-04-08 15:09:50,604] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3577119e-10 6.8918882e-05 5.2608006e-02 9.4469500e-01 3.6874719e-06
 3.3629678e-06 6.6906249e-04 6.0494749e-08 1.0788736e-11 3.1228934e-13
 1.9518480e-03], sum to 1.0000
[2019-04-08 15:09:50,604] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0616
[2019-04-08 15:09:50,641] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 22.5, 17.35631514752602, -1.605817204206619, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-2.5666666666666664, 30.0], 
sim time this is 1936200.0000, 
sim time next is 1936800.0000, 
raw observation next is [-7.3, 79.0, 128.0, 392.5, 22.5, 17.43184423943227, -1.581993858610627, 1.0, 1.0, 30.0, 35664.31178996382], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.79, 0.4266666666666667, 0.43370165745856354, 0.375, -0.04734631338064421, -0.027331286203542342, 1.0, 1.0, 0.3, 0.16983005614268484], 
reward next is 0.5956, 
noisyNet noise sample is [array([0.14060171], dtype=float32), 0.126054]. 
=============================================
[2019-04-08 15:09:50,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3881353e-11 4.1017170e-06 4.7199223e-02 9.3850172e-01 1.0544643e-06
 7.6320157e-07 3.2577217e-03 1.2509307e-09 2.3122971e-12 3.2056424e-13
 1.1035465e-02], sum to 1.0000
[2019-04-08 15:09:50,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5964
[2019-04-08 15:09:50,997] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.816666666666666, 65.0, 228.6666666666667, 6.0, 22.5, 18.48800661521035, -1.385801680294308, 1.0, 1.0, 30.0, 30766.32683087876], 
current ob forecast is [], 
actual action is [0.18333333333333357, 30.0], 
sim time this is 1944600.0000, 
sim time next is 1945200.0000, 
raw observation next is [-4.633333333333334, 65.0, 227.8333333333333, 5.0, 22.5, 18.52738825402025, -1.395211728169293, 1.0, 1.0, 30.0, 27182.69479376129], 
processed observation next is [1.0, 0.5217391304347826, 0.3342566943674977, 0.65, 0.7594444444444443, 0.0055248618784530384, 0.375, 0.04394902116835405, 0.03492942394356898, 1.0, 1.0, 0.3, 0.12944140377981567], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7292521], dtype=float32), 0.5583145]. 
=============================================
[2019-04-08 15:09:51,366] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.5639939e-13 4.5594916e-07 5.1153529e-01 4.8806104e-01 7.0323080e-07
 1.9047208e-08 2.3462980e-04 5.1547783e-11 7.0555026e-13 5.5732259e-16
 1.6782450e-04], sum to 1.0000
[2019-04-08 15:09:51,366] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2350
[2019-04-08 15:09:51,407] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 62.0, 80.33333333333334, 0.0, 22.5, 19.34906994951256, -1.20824416301727, 1.0, 1.0, 30.0, 32733.35039498567], 
current ob forecast is [], 
actual action is [2.2, 25.0], 
sim time this is 1956000.0000, 
sim time next is 1956600.0000, 
raw observation next is [-2.8, 62.0, 74.0, 0.0, 22.5, 19.38870497870959, -1.212221039527657, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.24666666666666667, 0.0, 0.375, 0.11572541489246586, 0.09592632015744769, 1.0, 1.0, 0.2, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14065886], dtype=float32), 0.40056154]. 
=============================================
[2019-04-08 15:09:52,287] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0816011e-07 3.5017438e-04 2.0046778e-01 7.9083174e-01 2.8426858e-04
 5.9428985e-05 2.2383744e-03 4.0839643e-07 4.9798153e-08 2.8895968e-09
 5.7676765e-03], sum to 1.0000
[2019-04-08 15:09:52,294] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5698
[2019-04-08 15:09:52,319] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.0, 83.50000000000001, 0.0, 0.0, 19.0, 15.23760190877318, -1.991050145194097, 0.0, 1.0, 30.0, 24737.90084651916], 
current ob forecast is [], 
actual action is [-4.0, 30.0], 
sim time this is 1923000.0000, 
sim time next is 1923600.0000, 
raw observation next is [-9.100000000000001, 85.0, 0.0, 0.0, 19.0, 15.21094936474406, -2.001617638813062, 0.0, 1.0, 30.0, 24690.52107982631], 
processed observation next is [1.0, 0.2608695652173913, 0.21052631578947364, 0.85, 0.0, 0.0, 0.08333333333333333, -0.23242088627132831, -0.16720587960435399, 0.0, 1.0, 0.3, 0.1175739099039348], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24465618], dtype=float32), -1.0055006]. 
=============================================
[2019-04-08 15:09:54,272] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1234386e-08 2.9200289e-04 3.2579582e-02 9.5687705e-01 8.5487496e-05
 8.4405801e-06 7.7943818e-04 5.8684975e-07 1.5756797e-09 4.6455190e-10
 9.3774712e-03], sum to 1.0000
[2019-04-08 15:09:54,272] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3413
[2019-04-08 15:09:54,303] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0101837e-12 1.1259849e-06 1.6390661e-02 9.8285884e-01 1.1090158e-06
 5.2406328e-09 6.6057883e-06 1.4543832e-09 1.3878700e-12 2.0623013e-14
 7.4153178e-04], sum to 1.0000
[2019-04-08 15:09:54,303] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8705
[2019-04-08 15:09:54,330] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.8, 84.33333333333334, 0.0, 0.0, 19.0, 16.54416062970402, -1.668083332694808, 0.0, 1.0, 30.0, 31542.62582003757], 
current ob forecast is [], 
actual action is [-0.7999999999999998, 30.0], 
sim time this is 1988400.0000, 
sim time next is 1989000.0000, 
raw observation next is [-5.9, 85.0, 0.0, 0.0, 19.0, 16.5028901666956, -1.673024020444462, 0.0, 1.0, 30.0, 28368.55163069979], 
processed observation next is [1.0, 0.0, 0.2991689750692521, 0.85, 0.0, 0.0, 0.08333333333333333, -0.12475915277536664, -0.057674673481487325, 0.0, 1.0, 0.3, 0.13508834109857043], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.0218089], dtype=float32), -0.6379802]. 
=============================================
[2019-04-08 15:09:54,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[46.455853]
 [47.249695]
 [47.805344]
 [48.24991 ]
 [48.223404]], R is [[45.58917236]
 [45.13328171]
 [44.68194962]
 [44.23513031]
 [43.79277802]].
[2019-04-08 15:09:54,359] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.416666666666667, 81.0, 0.0, 0.0, 22.5, 18.12370823793927, -1.347326982256419, 0.0, 1.0, 30.0, 25126.07723455723], 
current ob forecast is [], 
actual action is [-0.41666666666666696, 30.0], 
sim time this is 1972200.0000, 
sim time next is 1972800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 22.5, 18.02916690545154, -1.35912494682877, 0.0, 1.0, 30.0, 25198.94307843578], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.375, 0.0024305754542949622, 0.04695835105707665, 0.0, 1.0, 0.3, 0.11999496704017037], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.04347175], dtype=float32), 0.0789032]. 
=============================================
[2019-04-08 15:09:55,090] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5549097e-11 1.7256423e-06 1.2473587e-01 8.6968511e-01 4.0190166e-06
 2.9111479e-08 5.9138017e-04 2.6813725e-09 2.0388428e-12 2.0301458e-13
 4.9818228e-03], sum to 1.0000
[2019-04-08 15:09:55,097] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8381
[2019-04-08 15:09:55,143] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 22.5, 18.03842600837031, -1.35830490682089, 0.0, 1.0, 30.0, 25166.61441844979], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 1972800.0000, 
sim time next is 1973400.0000, 
raw observation next is [-5.600000000000001, 82.16666666666667, 0.0, 0.0, 19.0, 17.95526130805508, -1.368035959277488, 0.0, 1.0, 30.0, 25241.01632866107], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.8216666666666668, 0.0, 0.0, 0.08333333333333333, -0.0037282243287434604, 0.04398801357417067, 0.0, 1.0, 0.3, 0.120195315850767], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8555428], dtype=float32), 1.3396306]. 
=============================================
[2019-04-08 15:09:56,450] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1327264e-07 9.3584793e-05 1.6426327e-02 9.7849756e-01 6.2300583e-06
 5.2001687e-06 3.7418928e-03 9.1922988e-07 1.0273730e-08 9.8847996e-10
 1.2281543e-03], sum to 1.0000
[2019-04-08 15:09:56,457] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5746
[2019-04-08 15:09:56,473] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 16.46625352779424, -1.699987148227522, 0.0, 1.0, 30.0, 21692.10277489337], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 16.48389416119507, -1.702123354682205, 0.0, 1.0, 30.0, 21670.85354449605], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, -0.1263421532337441, -0.067374451560735, 0.0, 1.0, 0.3, 0.10319454068807643], 
reward next is 0.1764, 
noisyNet noise sample is [array([0.52755195], dtype=float32), -1.2787753]. 
=============================================
[2019-04-08 15:09:56,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5116155e-08 2.3744973e-04 2.3388761e-01 7.4897230e-01 1.4570236e-04
 2.2139639e-05 9.5949341e-03 3.8178041e-06 1.7034168e-08 1.1411636e-09
 7.1360334e-03], sum to 1.0000
[2019-04-08 15:09:56,761] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3489
[2019-04-08 15:09:56,780] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 16.40104471863226, -1.751070092686484, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 30.0], 
sim time this is 2008200.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 16.36451149562009, -1.754282933113431, 0.0, 1.0, 30.0, 19146.64801495575], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, -0.1362907086983259, -0.08476097770447703, 0.0, 1.0, 0.3, 0.09117451435693215], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2507142], dtype=float32), 0.55675006]. 
=============================================
[2019-04-08 15:09:57,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4535147e-08 1.9263387e-04 6.8704501e-02 9.1868818e-01 1.3211016e-04
 1.1156361e-05 8.7002581e-03 4.7980370e-07 2.9225358e-09 2.2561260e-09
 3.5706416e-03], sum to 1.0000
[2019-04-08 15:09:57,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9979
[2019-04-08 15:09:57,678] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.166666666666667, 86.83333333333333, 19.66666666666667, 0.0, 22.5, 16.18839916414739, -1.799122518629858, 1.0, 1.0, 30.0, 26784.82401811852], 
current ob forecast is [], 
actual action is [-1.166666666666667, 65.0], 
sim time this is 2016600.0000, 
sim time next is 2017200.0000, 
raw observation next is [-6.133333333333334, 86.66666666666667, 24.33333333333334, 0.0, 22.5, 16.19180870103804, -1.694286102287923, 1.0, 1.0, 65.0, 196382.9773167053], 
processed observation next is [1.0, 0.34782608695652173, 0.2927054478301016, 0.8666666666666667, 0.08111111111111113, 0.0, 0.375, -0.15068260824683014, -0.06476203409597432, 1.0, 1.0, 1.0, 0.9351570348414538], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39288238], dtype=float32), -0.6620398]. 
=============================================
[2019-04-08 15:09:58,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1670247e-11 6.3698462e-05 1.1941642e-02 9.8276776e-01 1.7713695e-05
 6.0011763e-07 7.4447546e-04 3.0195022e-08 2.2012548e-11 4.8018187e-13
 4.4640340e-03], sum to 1.0000
[2019-04-08 15:09:58,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1439
[2019-04-08 15:09:58,242] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.75, 75.0, 0.0, 0.0, 22.5, 18.8901757984131, -1.188675878594735, 1.0, 1.0, 29.99999999999999, 0.0], 
current ob forecast is [], 
actual action is [0.25, 30.0], 
sim time this is 1967400.0000, 
sim time next is 1968000.0000, 
raw observation next is [-4.666666666666667, 73.66666666666666, 0.0, 0.0, 22.5, 18.89458018337335, -1.18776405183312, 1.0, 1.0, 30.00000000000001, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3333333333333333, 0.7366666666666666, 0.0, 0.0, 0.375, 0.07454834861444581, 0.10407864938895998, 1.0, 1.0, 0.3000000000000002, 0.0], 
reward next is 0.0228, 
noisyNet noise sample is [array([0.63588697], dtype=float32), 0.48875755]. 
=============================================
[2019-04-08 15:09:58,252] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[58.421722]
 [58.214645]
 [57.62288 ]
 [57.752464]
 [57.69568 ]], R is [[57.50885773]
 [57.19174576]
 [56.77090836]
 [56.20320129]
 [55.79731369]].
[2019-04-08 15:09:58,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2297486e-09 3.2077744e-05 2.3154478e-01 6.2678033e-01 2.0444259e-05
 1.5424579e-06 3.6759523e-03 1.3093459e-07 1.5605601e-09 5.2696012e-12
 1.3794467e-01], sum to 1.0000
[2019-04-08 15:09:58,471] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8269
[2019-04-08 15:09:58,546] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 102.0, 0.0, 22.5, 17.96073550139712, -1.498899140200553, 1.0, 1.0, 30.0, 29245.75899384424], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 2025000.0000, 
sim time next is 2025600.0000, 
raw observation next is [-5.6, 83.0, 109.5, 0.0, 22.5, 18.02584175655742, -1.489650853842571, 1.0, 1.0, 30.0, 27373.89038543714], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.365, 0.0, 0.375, 0.0021534797131182635, 0.003449715385809663, 1.0, 1.0, 0.3, 0.1303518589782721], 
reward next is 0.2312, 
noisyNet noise sample is [array([0.1410097], dtype=float32), 0.3341184]. 
=============================================
[2019-04-08 15:09:58,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3683568e-09 1.9255075e-04 9.5356055e-02 8.2041782e-01 1.0883342e-04
 3.3474127e-05 3.8845040e-02 1.7898621e-07 1.4393937e-09 7.7519650e-11
 4.5046039e-02], sum to 1.0000
[2019-04-08 15:09:58,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1756
[2019-04-08 15:09:59,014] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.866666666666666, 85.0, 62.33333333333333, 0.0, 22.5, 17.83137863501419, -1.476065640019443, 1.0, 1.0, 29.99999999999999, 0.0], 
current ob forecast is [], 
actual action is [-0.8666666666666663, 30.0], 
sim time this is 2020800.0000, 
sim time next is 2021400.0000, 
raw observation next is [-5.8, 84.5, 69.0, 0.0, 22.5, 18.06205424641246, -1.445455607584991, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30193905817174516, 0.845, 0.23, 0.0, 0.375, 0.005171187201038189, 0.01818146413833636, 1.0, 1.0, 0.3, 0.0], 
reward next is 0.7653, 
noisyNet noise sample is [array([-0.5576273], dtype=float32), 0.17484206]. 
=============================================
[2019-04-08 15:09:59,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8027347e-09 1.0037117e-04 8.3786003e-02 8.8405949e-01 7.5556978e-05
 6.7293058e-06 9.2120888e-03 3.1862334e-07 5.5582117e-10 8.7769403e-11
 2.2759482e-02], sum to 1.0000
[2019-04-08 15:09:59,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6366
[2019-04-08 15:09:59,565] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 85.5, 0.0, 22.5, 17.97206591131205, -1.498698765407536, 1.0, 1.0, 30.0, 26300.42752045584], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 2023200.0000, 
sim time next is 2023800.0000, 
raw observation next is [-5.600000000000001, 83.0, 91.00000000000001, 0.0, 22.5, 18.00094233641219, -1.484470162428705, 1.0, 1.0, 30.0, 26297.37594183289], 
processed observation next is [1.0, 0.43478260869565216, 0.3074792243767313, 0.83, 0.3033333333333334, 0.0, 0.375, 7.85280343492308e-05, 0.005176612523764963, 1.0, 1.0, 0.3, 0.12522559972301375], 
reward next is 0.3557, 
noisyNet noise sample is [array([1.1372154], dtype=float32), 0.8684313]. 
=============================================
[2019-04-08 15:09:59,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0557800e-09 3.5959976e-05 3.0597158e-02 9.6416152e-01 1.3736187e-05
 9.4943116e-06 6.7253166e-04 2.2399674e-07 7.9730139e-10 2.3507519e-11
 4.5094839e-03], sum to 1.0000
[2019-04-08 15:09:59,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2775
[2019-04-08 15:09:59,620] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.7, 83.0, 0.0, 0.0, 19.0, 17.20447051936095, -1.527047352643862, 0.0, 1.0, 30.0, 21910.06449106735], 
current ob forecast is [], 
actual action is [-0.7000000000000002, 30.0], 
sim time this is 1983000.0000, 
sim time next is 1983600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 17.1360831280095, -1.53905623701399, 0.0, 1.0, 30.0, 21973.62195087754], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, -0.07199307266587507, -0.01301874567132999, 0.0, 1.0, 0.3, 0.10463629500417876], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6591196], dtype=float32), -0.5528315]. 
=============================================
[2019-04-08 15:09:59,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7431611e-11 1.5685702e-05 5.0734155e-02 9.4768196e-01 3.3203828e-06
 1.5764020e-06 5.9931219e-04 9.5809218e-09 1.0567335e-12 3.1110805e-13
 9.6404902e-04], sum to 1.0000
[2019-04-08 15:09:59,730] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7257
[2019-04-08 15:09:59,792] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 22.5, 18.77428646915652, -1.303302052105671, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 2033400.0000, 
sim time next is 2034000.0000, 
raw observation next is [-4.5, 79.0, 152.0, 0.0, 22.5, 18.83417719872447, -1.298629682726975, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.5066666666666667, 0.0, 0.375, 0.06951476656037237, 0.06712343909100833, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.1168, 
noisyNet noise sample is [array([-0.23495765], dtype=float32), -0.45904976]. 
=============================================
[2019-04-08 15:09:59,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[59.127506]
 [58.802113]
 [58.89623 ]
 [58.878475]
 [58.55313 ]], R is [[58.6945076 ]
 [58.23902893]
 [57.6566391 ]
 [57.82115555]
 [57.24294281]].
[2019-04-08 15:10:00,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2599040e-07 8.6293358e-04 8.6878657e-02 8.7974751e-01 2.0530696e-03
 7.3420386e-05 1.1010144e-02 8.5093752e-06 2.5807250e-08 5.7327103e-09
 1.9365273e-02], sum to 1.0000
[2019-04-08 15:10:00,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7801
[2019-04-08 15:10:00,951] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 16.36649237451532, -1.740780806668212, 0.0, 1.0, 30.0, 21608.09442308335], 
current ob forecast is [], 
actual action is [-0.7000000000000002, 30.0], 
sim time this is 2002200.0000, 
sim time next is 2002800.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 19.0, 16.28517650707038, -1.728137449383363, 0.0, 1.0, 30.0, 21723.92963593543], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, -0.14290195774413514, -0.076045816461121, 0.0, 1.0, 0.3, 0.1034472839806449], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.59379655], dtype=float32), -1.6640409]. 
=============================================
[2019-04-08 15:10:01,113] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4310310e-07 1.8113859e-03 1.3844027e-01 7.9380685e-01 5.1860610e-04
 2.9964824e-04 8.9309867e-03 6.8216209e-06 2.9120002e-07 7.7346245e-09
 5.6184776e-02], sum to 1.0000
[2019-04-08 15:10:01,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1117
[2019-04-08 15:10:01,164] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-6.1, 86.33333333333333, 0.0, 0.0, 19.0, 16.37450635385251, -1.739033043862031, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-1.0999999999999996, 25.0], 
sim time this is 2004600.0000, 
sim time next is 2005200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 16.36125871486619, -1.741649364541406, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, -0.13656177376115095, -0.08054978818046865, 0.0, 1.0, 0.2, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.00857546], dtype=float32), -0.5136738]. 
=============================================
[2019-04-08 15:10:02,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3131122e-09 1.9779720e-05 3.8003225e-02 9.0477073e-01 1.2482167e-04
 1.2973193e-05 5.5929245e-03 1.0031329e-06 1.3934034e-09 1.3727056e-10
 5.1474586e-02], sum to 1.0000
[2019-04-08 15:10:02,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6592
[2019-04-08 15:10:02,932] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 19.0, 17.67535215604633, -1.384136236662615, 0.0, 1.0, 30.0, 20911.38341532956], 
current ob forecast is [], 
actual action is [1.1, 30.0], 
sim time this is 2065800.0000, 
sim time next is 2066400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 17.65004523564208, -1.388658234653472, 0.0, 1.0, 30.0, 20884.68395200159], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, -0.029162897029826773, 0.03711392178217602, 0.0, 1.0, 0.3, 0.09945087596191234], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2558513], dtype=float32), 0.031904463]. 
=============================================
[2019-04-08 15:10:04,233] A3C_AGENT_WORKER-Thread-8 INFO:Local step 3500, global step 55287: loss -3.4727
[2019-04-08 15:10:04,234] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 3500, global step 55288: learning rate 0.0000
[2019-04-08 15:10:04,236] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0060724e-06 6.2541064e-04 1.9953929e-01 7.2138959e-01 5.7793583e-04
 1.3607214e-04 3.3949107e-02 3.2415566e-05 7.9506194e-08 1.7356468e-08
 4.3749064e-02], sum to 1.0000
[2019-04-08 15:10:04,236] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3807
[2019-04-08 15:10:04,258] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 80.5, 0.0, 0.0, 19.0, 16.27981050865583, -1.751520252355141, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 30.0], 
sim time this is 2097000.0000, 
sim time next is 2097600.0000, 
raw observation next is [-6.700000000000001, 79.66666666666667, 0.0, 0.0, 19.0, 16.20838564716223, -1.764589624478411, 0.0, 1.0, 30.0, 19865.89426932568], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, -0.14930119606981407, -0.08819654149280365, 0.0, 1.0, 0.3, 0.09459949652059847], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18387967], dtype=float32), -0.15458763]. 
=============================================
[2019-04-08 15:10:04,438] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55364: loss 0.3440
[2019-04-08 15:10:04,450] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55364: learning rate 0.0000
[2019-04-08 15:10:04,910] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55524: loss 0.5886
[2019-04-08 15:10:04,911] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55525: learning rate 0.0000
[2019-04-08 15:10:05,143] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55611: loss -0.1906
[2019-04-08 15:10:05,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55612: learning rate 0.0000
[2019-04-08 15:10:05,168] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55619: loss 9.0411
[2019-04-08 15:10:05,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55619: learning rate 0.0000
[2019-04-08 15:10:05,510] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55716: loss 7.1732
[2019-04-08 15:10:05,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55716: learning rate 0.0000
[2019-04-08 15:10:05,582] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 55736: loss 8.2606
[2019-04-08 15:10:05,583] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 55736: learning rate 0.0000
[2019-04-08 15:10:05,808] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55803: loss 0.1227
[2019-04-08 15:10:05,808] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55803: learning rate 0.0000
[2019-04-08 15:10:05,951] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55836: loss 0.3678
[2019-04-08 15:10:05,952] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55836: learning rate 0.0000
[2019-04-08 15:10:05,966] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55841: loss 0.8632
[2019-04-08 15:10:05,972] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55841: learning rate 0.0000
[2019-04-08 15:10:05,986] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55847: loss -0.9221
[2019-04-08 15:10:05,987] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55847: learning rate 0.0000
[2019-04-08 15:10:06,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6115917e-09 2.5317297e-04 1.4376956e-01 8.1584954e-01 2.1181916e-04
 2.0753183e-05 1.0353012e-02 2.1845299e-06 4.3432508e-10 6.9307282e-11
 2.9539898e-02], sum to 1.0000
[2019-04-08 15:10:06,921] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8218
[2019-04-08 15:10:06,975] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.716666666666667, 80.83333333333334, 196.6666666666667, 79.33333333333333, 22.5, 18.87345309683175, -1.293027439944516, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-2.716666666666667, 30.0], 
sim time this is 2110200.0000, 
sim time next is 2110800.0000, 
raw observation next is [-7.633333333333333, 79.66666666666667, 202.3333333333333, 69.66666666666666, 22.5, 18.87031443969357, -1.287330181426158, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.43478260869565216, 0.2511542012927055, 0.7966666666666667, 0.6744444444444443, 0.07697974217311233, 0.375, 0.07252620330779742, 0.07088993952461402, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.1424, 
noisyNet noise sample is [array([-0.77919465], dtype=float32), 0.19572322]. 
=============================================
[2019-04-08 15:10:06,976] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56104: loss 12.3089
[2019-04-08 15:10:06,977] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56104: learning rate 0.0000
[2019-04-08 15:10:07,306] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56182: loss 0.2351
[2019-04-08 15:10:07,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56182: learning rate 0.0000
[2019-04-08 15:10:07,375] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56196: loss 9.0484
[2019-04-08 15:10:07,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56196: learning rate 0.0000
[2019-04-08 15:10:08,439] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8033345e-10 1.4754123e-04 1.7183594e-02 9.7754294e-01 5.8294390e-06
 2.1953661e-06 1.0102257e-03 2.4876778e-08 3.8887334e-11 4.5615881e-12
 4.1075964e-03], sum to 1.0000
[2019-04-08 15:10:08,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7177
[2019-04-08 15:10:08,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7196148e-10 1.0887248e-04 1.5374720e-01 7.4900436e-01 1.2784931e-04
 2.4921907e-05 1.3010303e-02 1.1593792e-06 2.6889127e-10 1.9766289e-12
 8.3975315e-02], sum to 1.0000
[2019-04-08 15:10:08,445] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5632
[2019-04-08 15:10:08,499] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.2, 64.0, 150.0, 67.0, 22.5, 18.76146478003752, -1.294297484412853, 1.0, 1.0, 30.0, 23366.23785605645], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 30.0], 
sim time this is 2120400.0000, 
sim time next is 2121000.0000, 
raw observation next is [-6.100000000000001, 64.66666666666667, 149.6666666666667, 44.66666666666666, 22.5, 18.735967438979, -1.301536158427375, 1.0, 1.0, 30.0, 23369.18246067204], 
processed observation next is [1.0, 0.5652173913043478, 0.2936288088642659, 0.6466666666666667, 0.49888888888888905, 0.049355432780847135, 0.375, 0.06133061991491662, 0.06615461385754166, 1.0, 1.0, 0.3, 0.11128182124129543], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08960762], dtype=float32), 1.3040019]. 
=============================================
[2019-04-08 15:10:08,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[50.409523]
 [50.091835]
 [50.0918  ]
 [50.130985]
 [50.19554 ]], R is [[49.75789642]
 [49.26031876]
 [48.96302795]
 [48.61561966]
 [48.1294632 ]].
[2019-04-08 15:10:08,514] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.100000000000001, 64.66666666666667, 149.6666666666667, 44.66666666666666, 22.5, 20.12256765254239, -0.9542589392777945, 1.0, 1.0, 65.0, 198949.026331255], 
current ob forecast is [], 
actual action is [-1.1000000000000014, 30.0], 
sim time this is 2121000.0000, 
sim time next is 2121600.0000, 
raw observation next is [-6.0, 65.33333333333334, 149.3333333333333, 22.33333333333333, 22.5, 20.22895205814594, -0.9007908033015491, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.296398891966759, 0.6533333333333334, 0.4977777777777776, 0.024677716390423567, 0.375, 0.18574600484549494, 0.1997363988994836, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44779566], dtype=float32), -0.8575797]. 
=============================================
[2019-04-08 15:10:08,551] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8977642e-09 1.7261544e-04 1.5539867e-01 6.8712586e-01 1.1862441e-04
 1.5947420e-05 2.7373282e-02 1.3863949e-07 1.8766698e-10 1.8307604e-11
 1.2979476e-01], sum to 1.0000
[2019-04-08 15:10:08,551] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5589
[2019-04-08 15:10:08,639] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 68.0, 105.5, 0.0, 22.5, 20.460961303122, -0.8701026886843789, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [0.0, 30.0], 
sim time this is 2127600.0000, 
sim time next is 2128200.0000, 
raw observation next is [-4.916666666666667, 67.5, 99.0, 0.0, 22.5, 21.00539491637551, -0.8237511765521565, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.32640812557710064, 0.675, 0.33, 0.0, 0.375, 0.25044957636462595, 0.2254162744826145, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3518218], dtype=float32), 0.015399814]. 
=============================================
[2019-04-08 15:10:08,736] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7087384e-11 1.2216072e-04 1.0880308e-02 9.8180628e-01 1.8689678e-07
 1.8422581e-06 2.8405685e-03 7.5879232e-09 7.2344901e-11 1.0222747e-12
 4.3485942e-03], sum to 1.0000
[2019-04-08 15:10:08,740] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5445
[2019-04-08 15:10:08,771] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 65.33333333333334, 149.3333333333333, 22.33333333333333, 22.5, 18.76586100157295, -1.255457463391593, 1.0, 1.0, 30.0, 23048.36050425611], 
current ob forecast is [], 
actual action is [-1.0, 30.0], 
sim time this is 2121600.0000, 
sim time next is 2122200.0000, 
raw observation next is [-5.9, 66.0, 149.0, 0.0, 22.5, 18.89558381357266, -1.241117146308774, 1.0, 1.0, 30.0, 23048.81646910364], 
processed observation next is [1.0, 0.5652173913043478, 0.2991689750692521, 0.66, 0.49666666666666665, 0.0, 0.375, 0.07463198446438835, 0.086294284563742, 1.0, 1.0, 0.3, 0.10975626890049352], 
reward next is 0.3585, 
noisyNet noise sample is [array([-2.2438624], dtype=float32), 0.06379951]. 
=============================================
[2019-04-08 15:10:09,155] A3C_AGENT_WORKER-Thread-7 INFO:Local step 3500, global step 56623: loss -3.2972
[2019-04-08 15:10:09,157] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 3500, global step 56623: learning rate 0.0000
[2019-04-08 15:10:09,889] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56801: loss -0.5738
[2019-04-08 15:10:09,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56801: learning rate 0.0000
[2019-04-08 15:10:10,399] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4545928e-07 6.7154877e-04 4.8801754e-02 8.7004429e-01 1.0188118e-04
 2.2100980e-04 6.3334438e-03 1.0989382e-06 1.0353833e-07 4.9643636e-09
 7.3824517e-02], sum to 1.0000
[2019-04-08 15:10:10,399] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0197
[2019-04-08 15:10:10,483] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.299999999999999, 82.0, 0.0, 0.0, 19.0, 17.04177982461464, -1.545080879291582, 0.0, 1.0, 30.0, 27364.29208918732], 
current ob forecast is [], 
actual action is [-2.299999999999999, 25.0], 
sim time this is 2159400.0000, 
sim time next is 2160000.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 16.97551631459882, -1.559609054115824, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, -0.08537364045009828, -0.019869684705274688, 0.0, 1.0, 0.2, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8957973], dtype=float32), 1.398028]. 
=============================================
[2019-04-08 15:10:10,508] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[38.881313]
 [38.85603 ]
 [38.856026]
 [39.27604 ]
 [39.349   ]], R is [[37.33059311]
 [36.95728683]
 [36.58771515]
 [36.2218399 ]
 [35.85962296]].
[2019-04-08 15:10:11,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6205372e-10 3.8344304e-05 1.9372776e-01 6.7120099e-01 2.4289961e-04
 4.0315513e-06 1.5962085e-02 6.9311488e-07 3.5547040e-10 2.9066749e-11
 1.1882320e-01], sum to 1.0000
[2019-04-08 15:10:11,047] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8938
[2019-04-08 15:10:11,150] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 73.5, 0.0, 0.0, 22.5, 19.12472644384989, -1.144684158062803, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2141400.0000, 
sim time next is 2142000.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 22.5, 19.10109473175041, -1.103069654722007, 1.0, 1.0, 65.0, 197012.7891658071], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.74, 0.0, 0.0, 0.375, 0.09175789431253427, 0.13231011509266435, 1.0, 1.0, 1.0, 0.9381561388847958], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9261406], dtype=float32), -0.9392097]. 
=============================================
[2019-04-08 15:10:11,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[49.745426]
 [50.048115]
 [49.693554]
 [49.541092]
 [50.31149 ]], R is [[51.14245605]
 [50.63103104]
 [50.12472153]
 [49.62347412]
 [49.12723923]].
[2019-04-08 15:10:11,629] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.5189077e-07 3.7235950e-04 3.6483634e-02 8.9898616e-01 1.7315094e-04
 4.8321116e-04 2.7503857e-02 6.8989830e-06 2.7906728e-08 6.3270984e-09
 3.5989910e-02], sum to 1.0000
[2019-04-08 15:10:11,658] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9853
[2019-04-08 15:10:11,772] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 19.0, 17.33927420187813, -1.483455049352307, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.9000000000000004, 65.0], 
sim time this is 2166000.0000, 
sim time next is 2166600.0000, 
raw observation next is [-6.800000000000001, 78.16666666666666, 0.0, 0.0, 19.0, 17.34557385761355, -1.466986467434322, 0.0, 1.0, 65.0, 198881.7813057901], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.7816666666666666, 0.0, 0.0, 0.08333333333333333, -0.054535511865537366, 0.011004510855225988, 0.0, 1.0, 1.0, 0.9470561014561433], 
reward next is 0.0630, 
noisyNet noise sample is [array([-0.18169603], dtype=float32), 1.2285978]. 
=============================================
[2019-04-08 15:10:12,249] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6879403e-08 2.5201743e-04 5.1498756e-02 9.3350214e-01 1.8307244e-04
 4.5759643e-05 3.9513041e-03 9.4600495e-07 1.6327029e-09 1.8310489e-10
 1.0565876e-02], sum to 1.0000
[2019-04-08 15:10:12,250] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3055
[2019-04-08 15:10:12,267] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.299999999999999, 82.0, 0.0, 0.0, 19.0, 17.71023746653734, -1.399651335264396, 0.0, 1.0, 30.0, 22152.90311035534], 
current ob forecast is [], 
actual action is [-2.299999999999999, 30.0], 
sim time this is 2159400.0000, 
sim time next is 2160000.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 17.64819848419723, -1.412553232610922, 0.0, 1.0, 30.0, 21485.11914354205], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, -0.029316792983564117, 0.02914892246302599, 0.0, 1.0, 0.3, 0.10231009115972406], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.26917452], dtype=float32), 1.4433421]. 
=============================================
[2019-04-08 15:10:12,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[39.804375]
 [39.96209 ]
 [40.063416]
 [40.144005]
 [40.30982 ]], R is [[38.97950363]
 [38.58971024]
 [38.20381165]
 [37.82177353]
 [37.44355774]].
[2019-04-08 15:10:12,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.41149260e-08 5.82064327e-04 4.26486492e-01 2.40354717e-01
 3.64140258e-04 8.07393299e-05 1.09402746e-01 1.01123933e-05
 1.15063505e-08 1.64248048e-09 2.22718924e-01], sum to 1.0000
[2019-04-08 15:10:12,295] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4939
[2019-04-08 15:10:12,337] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 19.0, 18.62034139701196, -1.17547575880353, 0.0, 1.0, 45.0, 90564.64421963796], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 30.0], 
sim time this is 2152800.0000, 
sim time next is 2153400.0000, 
raw observation next is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 19.0, 18.63037721371597, -1.216879093487764, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.2742382271468144, 0.8283333333333335, 0.0, 0.0, 0.08333333333333333, 0.05253143447633087, 0.09437363550407867, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.5277, 
noisyNet noise sample is [array([-0.88172513], dtype=float32), -0.16575418]. 
=============================================
[2019-04-08 15:10:12,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4280743e-09 3.2732580e-04 5.5662159e-02 9.1134667e-01 5.7362915e-05
 6.5130284e-06 1.9285684e-02 4.2091895e-07 7.6399959e-10 2.6498812e-11
 1.3313888e-02], sum to 1.0000
[2019-04-08 15:10:12,645] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3061
[2019-04-08 15:10:12,733] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 22.5, 18.46541459943439, -1.213513486043353, 0.0, 1.0, 30.00000000000002, 0.0], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 2145600.0000, 
sim time next is 2146200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 18.50320021484725, -1.215523202896433, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.3074792243767313, 0.83, 0.0, 0.0, 0.08333333333333333, 0.0419333512372709, 0.09482559903452237, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.3882, 
noisyNet noise sample is [array([-0.20818073], dtype=float32), -0.76715904]. 
=============================================
[2019-04-08 15:10:12,884] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7533002e-07 3.1789337e-04 1.5331326e-01 6.2228101e-01 9.9060088e-03
 1.8429867e-03 2.3791196e-02 1.4048395e-05 8.7123290e-08 1.4811687e-08
 1.8853322e-01], sum to 1.0000
[2019-04-08 15:10:12,885] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0407
[2019-04-08 15:10:12,909] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-7.300000000000001, 82.0, 0.0, 0.0, 19.0, 18.03000282918478, -1.360352165368691, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-2.3000000000000007, 25.0], 
sim time this is 2157600.0000, 
sim time next is 2158200.0000, 
raw observation next is [-7.3, 82.0, 0.0, 0.0, 19.0, 17.91879854051921, -1.38166056091378, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 1.0, 0.26038781163434904, 0.82, 0.0, 0.0, 0.08333333333333333, -0.006766788290065939, 0.039446479695406676, 0.0, 1.0, 0.2, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.61848545], dtype=float32), 0.21838379]. 
=============================================
[2019-04-08 15:10:13,405] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0864552e-06 7.4545533e-04 9.3407288e-02 8.1824255e-01 8.6024037e-04
 1.5409278e-04 3.3769570e-02 2.3396895e-05 3.6188845e-07 8.2952390e-08
 5.2795913e-02], sum to 1.0000
[2019-04-08 15:10:13,405] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6831
[2019-04-08 15:10:13,418] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 17.85962006975772, -1.391711806436832, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.700000000000001, 30.0], 
sim time this is 2169600.0000, 
sim time next is 2170200.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 17.93913752058269, -1.393025674741054, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, -0.0050718732847758, 0.035658108419648636, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.3186593], dtype=float32), 1.0468291]. 
=============================================
[2019-04-08 15:10:14,782] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3493983e-07 5.9444399e-04 2.2121847e-02 9.5708477e-01 8.4755570e-04
 2.1953962e-04 9.0923999e-03 3.2607728e-05 8.0275832e-08 2.1347878e-08
 1.0006233e-02], sum to 1.0000
[2019-04-08 15:10:14,782] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2187
[2019-04-08 15:10:14,924] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 22.5, 16.48249300527439, -1.709129456254542, 1.0, 1.0, 29.99999999999999, 20247.83139788237], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 30.0], 
sim time this is 2186400.0000, 
sim time next is 2187000.0000, 
raw observation next is [-5.6, 75.0, 0.0, 0.0, 22.5, 16.46248181917003, -1.701075453178413, 1.0, 1.0, 30.0, 25024.47064739631], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.0, 0.0, 0.375, -0.12812651506916417, -0.06702515105947097, 1.0, 1.0, 0.3, 0.11916414593998242], 
reward next is 0.2014, 
noisyNet noise sample is [array([0.77414864], dtype=float32), -2.158948]. 
=============================================
[2019-04-08 15:10:14,983] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[31.62467 ]
 [31.961191]
 [31.827627]
 [32.066578]
 [32.20397 ]], R is [[31.9296093 ]
 [31.61031342]
 [31.29421043]
 [30.98126793]
 [30.67145538]].
[2019-04-08 15:10:16,439] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5473779e-08 5.0404191e-04 2.3211062e-01 5.7592583e-01 5.6461617e-05
 3.8056856e-05 4.5135129e-02 8.7571385e-07 2.9363161e-09 1.9133295e-10
 1.4622888e-01], sum to 1.0000
[2019-04-08 15:10:16,440] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9774
[2019-04-08 15:10:16,544] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.916666666666667, 71.0, 112.0, 150.3333333333333, 22.5, 19.11469231829404, -1.213292767967919, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [0.08333333333333304, 65.0], 
sim time this is 2196600.0000, 
sim time next is 2197200.0000, 
raw observation next is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 22.5, 19.25856866630126, -1.184008932014784, 1.0, 1.0, 65.0, 197567.6133219278], 
processed observation next is [1.0, 0.43478260869565216, 0.32871652816251157, 0.71, 0.38166666666666665, 0.08305709023941066, 0.375, 0.1048807221917718, 0.10533035599507201, 1.0, 1.0, 1.0, 0.9407981586758467], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.17460676], dtype=float32), 0.8768821]. 
=============================================
[2019-04-08 15:10:18,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5338263e-08 1.5681716e-04 2.7392459e-01 6.3291264e-01 5.7025863e-05
 5.0487673e-05 1.3409855e-03 7.9362906e-07 1.4560415e-09 7.1626989e-11
 9.1556571e-02], sum to 1.0000
[2019-04-08 15:10:18,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1840
[2019-04-08 15:10:18,827] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-5.0, 69.5, 0.0, 0.0, 19.0, 18.16003019551497, -1.300069989642707, 0.0, 1.0, 30.0, 22918.32213215958], 
current ob forecast is [], 
actual action is [0.0, 25.0], 
sim time this is 2233800.0000, 
sim time next is 2234400.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 19.0, 18.09610505040898, -1.312176358948757, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.69, 0.0, 0.0, 0.08333333333333333, 0.008008754200748314, 0.06260788035041435, 0.0, 1.0, 0.2, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24248832], dtype=float32), -0.45463356]. 
=============================================
[2019-04-08 15:10:19,001] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9835730e-10 6.4225739e-04 2.8634626e-01 5.7497758e-01 1.0835458e-04
 7.9413818e-05 4.4785380e-02 9.0635831e-07 8.9782751e-11 1.4097161e-11
 9.3059927e-02], sum to 1.0000
[2019-04-08 15:10:19,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5114
[2019-04-08 15:10:19,059] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.65, 68.0, 144.0, 0.0, 22.5, 19.43936370435277, -1.093673501523874, 1.0, 1.0, 30.0, 43091.64938040377], 
current ob forecast is [], 
actual action is [1.35, 30.0], 
sim time this is 2208600.0000, 
sim time next is 2209200.0000, 
raw observation next is [-3.733333333333333, 69.0, 140.0, 0.0, 22.5, 19.58749815052204, -1.070610903916091, 1.0, 1.0, 30.0, 33554.7540987159], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.69, 0.4666666666666667, 0.0, 0.375, 0.13229151254350344, 0.14312969869463635, 1.0, 1.0, 0.3, 0.1597845433272186], 
reward next is 0.5766, 
noisyNet noise sample is [array([-0.31350893], dtype=float32), 0.80748415]. 
=============================================
[2019-04-08 15:10:20,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2317450e-09 1.0996176e-04 1.7303862e-01 7.4771041e-01 1.0925149e-04
 1.0938164e-05 1.7703443e-03 1.9175575e-07 6.0629913e-12 3.8082753e-12
 7.7250279e-02], sum to 1.0000
[2019-04-08 15:10:20,891] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8045
[2019-04-08 15:10:20,954] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 71.0, 19.0, 0.0, 22.5, 22.49468735273252, -0.4989198755094996, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 2221200.0000, 
sim time next is 2221800.0000, 
raw observation next is [-4.5, 70.5, 13.66666666666666, 0.0, 22.5, 22.47047292372829, -0.5810439527138443, 1.0, 1.0, 29.99999999999999, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.705, 0.04555555555555554, 0.0, 0.375, 0.37253941031069093, 0.30631868242871857, 1.0, 1.0, 0.29999999999999977, 0.0], 
reward next is 0.3432, 
noisyNet noise sample is [array([1.610169], dtype=float32), 0.18081748]. 
=============================================
[2019-04-08 15:10:21,644] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2844253e-10 8.1048063e-05 2.6926088e-01 5.5176449e-01 3.7453268e-04
 7.3890269e-06 1.7842466e-02 3.1971092e-07 1.2552141e-10 1.3587297e-11
 1.6066881e-01], sum to 1.0000
[2019-04-08 15:10:21,644] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9386
[2019-04-08 15:10:21,733] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.6, 70.0, 0.0, 0.0, 22.5, 21.18531516290175, -0.6613567225636876, 0.0, 1.0, 65.0, 197970.6641346286], 
current ob forecast is [], 
actual action is [0.40000000000000036, 65.0], 
sim time this is 2228400.0000, 
sim time next is 2229000.0000, 
raw observation next is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 22.5, 21.1139109411853, -0.6050372163128247, 1.0, 1.0, 65.0, 199124.4654945082], 
processed observation next is [1.0, 0.8260869565217391, 0.33333333333333337, 0.7016666666666667, 0.0, 0.0, 0.375, 0.25949257843210827, 0.2983209278957251, 1.0, 1.0, 1.0, 0.948211740450039], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7729681], dtype=float32), -0.934493]. 
=============================================
[2019-04-08 15:10:21,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[52.72436 ]
 [52.309315]
 [52.40945 ]
 [52.875404]
 [52.978092]], R is [[53.85266113]
 [53.371418  ]
 [52.8377037 ]
 [52.30932617]
 [52.01572418]].
[2019-04-08 15:10:23,292] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-08 15:10:23,294] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:10:23,296] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:10:23,298] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run4
[2019-04-08 15:10:23,315] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:10:23,318] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:10:23,319] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:10:23,320] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:10:23,321] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1175329e-06 1.9536989e-03 7.9758368e-02 6.4377242e-01 3.1344857e-04
 9.2940591e-04 5.7496250e-02 1.6285017e-05 2.8284262e-07 4.2649571e-08
 2.1575874e-01], sum to 1.0000
[2019-04-08 15:10:23,322] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9522
[2019-04-08 15:10:23,324] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run4
[2019-04-08 15:10:23,343] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.4, 91.0, 0.0, 0.0, 19.0, 17.50751797768053, -1.466749315750662, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-4.4, 65.0], 
sim time this is 2271000.0000, 
sim time next is 2271600.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 19.0, 17.44230873567828, -1.466364045194242, 0.0, 1.0, 65.0, 198965.1437144226], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.08333333333333333, -0.04647427202680987, 0.011211984935252698, 0.0, 1.0, 1.0, 0.9474530653067742], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.42751107], dtype=float32), 0.4417808]. 
=============================================
[2019-04-08 15:10:23,345] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run4
[2019-04-08 15:10:45,236] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.043761604]
[2019-04-08 15:10:45,236] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [-10.23333333333333, 60.66666666666667, 170.5, 326.3333333333333, 22.5, 24.37446105285807, 0.09848595158608316, 1.0, 1.0, 30.0, 0.0]
[2019-04-08 15:10:45,236] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:10:45,237] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [1.5458316e-09 4.5825189e-04 1.3160484e-01 5.8752483e-01 1.1469973e-04
 1.6552847e-05 1.0091344e-02 2.7339848e-07 1.7008163e-10 8.1819065e-12
 2.7018914e-01], sampled 0.4087973942725541
[2019-04-08 15:12:02,051] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 7064.7716 295263352.2881 2467.4013
[2019-04-08 15:12:02,082] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:02,082] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:02,082] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:02,082] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:02,227] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:02,227] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:02,227] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:02,227] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:16,783] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6973.7572 318585103.3597 2125.1914
[2019-04-08 15:12:16,804] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:16,804] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:16,804] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:16,804] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:16,939] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:16,939] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:16,939] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:16,939] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:17,355] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6911.8889 325927818.1345 1557.7004
[2019-04-08 15:12:17,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:17,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:17,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:17,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:12:17,488] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:17,488] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:17,488] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:17,488] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:12:18,377] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 60000, evaluation results [60000.0, 6973.757168618495, 318585103.3597176, 2125.1913913167987, 7064.77156640041, 295263352.28808427, 2467.401339626776, 6911.888898649131, 325927818.1344917, 1557.7004366190044]
[2019-04-08 15:12:18,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0084272e-09 2.7010596e-04 6.2416438e-02 6.7270970e-01 5.1320961e-04
 2.6405412e-05 1.6076231e-02 1.7520493e-06 7.2734435e-10 7.0550976e-10
 2.4798614e-01], sum to 1.0000
[2019-04-08 15:12:18,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3653
[2019-04-08 15:12:18,909] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 21.83304473929593, -0.4929870233222116, 0.0, 1.0, 65.0, 199922.1491523942], 
current ob forecast is [], 
actual action is [-2.3, 30.0], 
sim time this is 2253600.0000, 
sim time next is 2254200.0000, 
raw observation next is [-7.383333333333333, 82.66666666666667, 0.0, 0.0, 19.0, 21.72004588850653, -0.4928392508835018, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.25807940904893817, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.3100038240422108, 0.3357202497054994, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4554611], dtype=float32), 0.4772763]. 
=============================================
[2019-04-08 15:12:19,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8535069e-07 1.1821152e-03 5.6560330e-02 6.4698207e-01 5.2144466e-04
 7.2104573e-05 5.9062745e-02 1.9645351e-05 5.2859237e-09 1.8455308e-09
 2.3559861e-01], sum to 1.0000
[2019-04-08 15:12:19,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5572
[2019-04-08 15:12:19,233] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.383333333333333, 82.66666666666667, 0.0, 0.0, 19.0, 20.64073963313652, -0.7363481774757346, 0.0, 1.0, 65.0, 200950.1507152248], 
current ob forecast is [], 
actual action is [-2.383333333333333, 30.0], 
sim time this is 2254200.0000, 
sim time next is 2254800.0000, 
raw observation next is [-7.466666666666667, 83.33333333333334, 0.0, 0.0, 19.0, 20.65164529421523, -0.7074160105149184, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.25577100646352724, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.22097044118460238, 0.2641946631616939, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21688735], dtype=float32), -0.71232325]. 
=============================================
[2019-04-08 15:12:21,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0495513e-11 2.4732098e-04 4.0411528e-02 7.2321749e-01 6.0909431e-05
 2.7121957e-06 3.1664755e-02 3.8391050e-08 6.0878989e-11 4.9283863e-13
 2.0439537e-01], sum to 1.0000
[2019-04-08 15:12:21,612] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4506
[2019-04-08 15:12:21,675] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 22.5, 22.92317280777519, -0.3355743687094517, 1.0, 1.0, 65.0, 199114.6069960625], 
current ob forecast is [], 
actual action is [-1.1333333333333346, 30.0], 
sim time this is 2283600.0000, 
sim time next is 2284200.0000, 
raw observation next is [-5.85, 73.0, 178.0, 50.0, 22.5, 23.01033522750594, -0.3044964993652283, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.30055401662049863, 0.73, 0.5933333333333334, 0.055248618784530384, 0.375, 0.41752793562549506, 0.3985011668782572, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.76747084], dtype=float32), 1.1592526]. 
=============================================
[2019-04-08 15:12:21,771] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5124821e-08 3.2458207e-04 6.7827404e-02 8.5392773e-01 5.8666083e-05
 4.4882334e-05 4.9192547e-03 1.8458492e-06 1.6231663e-09 1.2568044e-10
 7.2895706e-02], sum to 1.0000
[2019-04-08 15:12:21,771] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1898
[2019-04-08 15:12:21,799] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 19.0, 21.42119753360835, -0.5623051685155721, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-2.0, 30.0], 
sim time this is 2251800.0000, 
sim time next is 2252400.0000, 
raw observation next is [-7.1, 79.66666666666667, 0.0, 0.0, 19.0, 21.41207118568524, -0.5829080919465243, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.2659279778393352, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.2843392654737699, 0.3056973026844919, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3165394], dtype=float32), -0.44657677]. 
=============================================
[2019-04-08 15:12:21,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.4816467e-07 2.4739250e-03 1.9851524e-01 7.3438632e-01 9.4262941e-04
 4.2800679e-05 1.3767578e-02 8.9563619e-06 1.8500070e-07 1.8129212e-08
 4.9861506e-02], sum to 1.0000
[2019-04-08 15:12:21,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6178
[2019-04-08 15:12:21,995] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.716666666666667, 85.33333333333334, 0.0, 0.0, 19.0, 20.25122475371585, -0.9181504162773019, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-2.716666666666667, 30.0], 
sim time this is 2256600.0000, 
sim time next is 2257200.0000, 
raw observation next is [-7.8, 86.0, 0.0, 0.0, 19.0, 19.9922761375962, -0.9421192118665308, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.86, 0.0, 0.0, 0.08333333333333333, 0.16602301146634998, 0.1859602627111564, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0725923], dtype=float32), -1.1078545]. 
=============================================
[2019-04-08 15:12:22,209] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6683049e-11 1.9168253e-06 2.0680247e-01 7.0463240e-01 1.6448294e-05
 5.5184382e-06 1.0564185e-02 1.0868522e-08 2.1333713e-11 6.8522153e-14
 7.7976987e-02], sum to 1.0000
[2019-04-08 15:12:22,209] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4175
[2019-04-08 15:12:22,282] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 44.0, 89.5, 21.0, 22.5, 25.37415532867715, 0.1377579830033101, 1.0, 1.0, 65.0, 204101.9313580287], 
current ob forecast is [], 
actual action is [5.0, 30.0], 
sim time this is 2304000.0000, 
sim time next is 2304600.0000, 
raw observation next is [-0.09999999999999999, 44.83333333333334, 73.33333333333333, 14.0, 22.5, 25.43026850642433, 0.1696223240895105, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4598337950138504, 0.4483333333333334, 0.24444444444444444, 0.015469613259668509, 0.375, 0.6191890422020275, 0.5565407746965035, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1639154], dtype=float32), -2.3304741]. 
=============================================
[2019-04-08 15:12:24,161] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.3970788e-09 2.8004061e-04 1.1830840e-01 4.6226233e-01 4.7280046e-05
 4.6464145e-05 3.6246225e-02 6.7125154e-07 1.2529016e-09 3.0250610e-10
 3.8280851e-01], sum to 1.0000
[2019-04-08 15:12:24,161] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1770
[2019-04-08 15:12:24,182] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 19.0, 23.83572908101793, -0.05255028347421164, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.7, 30.0], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [-2.3, 63.5, 0.0, 0.0, 19.0, 23.89878105622834, -0.05666215875670561, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.635, 0.0, 0.0, 0.08333333333333333, 0.4915650880190284, 0.4811126137477648, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1658938], dtype=float32), -0.34214756]. 
=============================================
[2019-04-08 15:12:24,534] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3252044e-12 4.7226087e-05 2.0340562e-02 2.1956164e-01 8.6013870e-06
 1.3780743e-07 3.6063862e-03 1.6045820e-10 8.9447418e-14 1.3582027e-15
 7.5643545e-01], sum to 1.0000
[2019-04-08 15:12:24,534] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5384
[2019-04-08 15:12:24,617] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.09999999999999999, 44.83333333333334, 73.33333333333333, 14.0, 22.5, 26.90053046434335, 0.5481960305380017, 1.0, 1.0, 65.0, 83129.00111668049], 
current ob forecast is [], 
actual action is [4.9, 65.0], 
sim time this is 2304600.0000, 
sim time next is 2305200.0000, 
raw observation next is [-0.2, 45.66666666666667, 57.16666666666667, 6.999999999999998, 22.5, 27.03615010823932, 0.5596550915450064, 1.0, 1.0, 65.0, 64580.62339667141], 
processed observation next is [1.0, 0.6956521739130435, 0.4570637119113574, 0.4566666666666667, 0.19055555555555556, 0.007734806629834252, 0.375, 0.7530125090199432, 0.6865516971816689, 1.0, 1.0, 1.0, 0.30752677807938766], 
reward next is 0.6925, 
noisyNet noise sample is [array([1.0454838], dtype=float32), 0.07592325]. 
=============================================
[2019-04-08 15:12:24,833] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8084925e-10 5.6095494e-05 2.2060633e-01 7.3115802e-01 3.5694300e-04
 3.4576408e-07 2.9936878e-03 1.2804404e-07 4.1787643e-11 1.7796334e-13
 4.4828471e-02], sum to 1.0000
[2019-04-08 15:12:24,834] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9475
[2019-04-08 15:12:24,856] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7, 49.5, 17.0, 0.0, 22.5, 23.58408814775744, -0.1538065583973042, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.3, 30.0], 
sim time this is 2308200.0000, 
sim time next is 2308800.0000, 
raw observation next is [-0.8, 50.0, 11.0, 0.0, 22.5, 23.31545132194607, -0.2069736894627242, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4404432132963989, 0.5, 0.03666666666666667, 0.0, 0.375, 0.44295427682883926, 0.4310087701790919, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9030291], dtype=float32), 0.21358606]. 
=============================================
[2019-04-08 15:12:25,247] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4076843e-09 3.1662546e-04 9.9496938e-02 8.5900122e-01 1.7128357e-05
 1.0187182e-06 2.2419256e-03 3.9115028e-07 7.5397509e-12 1.0263247e-11
 3.8924787e-02], sum to 1.0000
[2019-04-08 15:12:25,247] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2098
[2019-04-08 15:12:25,270] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 64.0, 0.0, 0.0, 19.0, 24.27705942175232, 0.0147608861992526, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.7, 30.0], 
sim time this is 2334000.0000, 
sim time next is 2334600.0000, 
raw observation next is [-2.3, 63.5, 0.0, 0.0, 19.0, 24.13144563341281, -0.01222695247357282, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.0, 0.3988919667590028, 0.635, 0.0, 0.0, 0.08333333333333333, 0.5109538027844008, 0.49592434917547573, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60025567], dtype=float32), -0.74206585]. 
=============================================
[2019-04-08 15:12:25,391] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9018096e-10 7.2253015e-06 5.3992737e-02 9.0399832e-01 2.1121255e-05
 6.2537765e-06 8.0856252e-03 4.2534394e-09 5.8607598e-12 8.4585334e-14
 3.3888802e-02], sum to 1.0000
[2019-04-08 15:12:25,391] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4819
[2019-04-08 15:12:25,435] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 50.0, 11.0, 0.0, 22.5, 25.98723216104018, 0.309166317292504, 1.0, 1.0, 29.99999999999999, 0.0], 
current ob forecast is [], 
actual action is [4.2, 30.0], 
sim time this is 2308800.0000, 
sim time next is 2309400.0000, 
raw observation next is [-0.8999999999999999, 50.5, 0.0, 0.0, 22.5, 25.49925761824007, 0.2333019719882458, 1.0, 1.0, 29.99999999999999, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.43767313019390586, 0.505, 0.0, 0.0, 0.375, 0.624938134853339, 0.577767323996082, 1.0, 1.0, 0.29999999999999977, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3755395], dtype=float32), -0.84512717]. 
=============================================
[2019-04-08 15:12:26,295] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.05586894e-07 1.37817522e-03 1.17526032e-01 6.75635159e-01
 8.57396983e-04 1.01174715e-04 2.24109255e-02 4.45715477e-06
 4.09212468e-08 2.86594148e-09 1.82086378e-01], sum to 1.0000
[2019-04-08 15:12:26,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6982
[2019-04-08 15:12:26,320] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 19.0, 21.75560909296731, -0.5531579150707143, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 30.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 19.0, 21.59724270942257, -0.5872338770744785, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.08333333333333333, 0.2997702257852142, 0.3042553743085072, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12195527], dtype=float32), 0.73453367]. 
=============================================
[2019-04-08 15:12:27,683] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4000, global step 62736: loss 4.0386
[2019-04-08 15:12:27,695] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4000, global step 62740: learning rate 0.0000
[2019-04-08 15:12:27,782] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0968942e-06 2.0933698e-03 1.1067343e-01 7.0542908e-01 1.8634972e-03
 2.5285390e-04 3.9691534e-02 3.3266602e-05 7.0563516e-08 2.0689070e-08
 1.3996178e-01], sum to 1.0000
[2019-04-08 15:12:27,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9076
[2019-04-08 15:12:27,808] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 31.16666666666667, 0.0, 19.0, 21.20981144335933, -0.6078314394309207, 0.0, 1.0, 65.0, 199390.8958160908], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 2362800.0000, 
sim time next is 2363400.0000, 
raw observation next is [-3.4, 69.0, 37.0, 0.0, 19.0, 21.08744806995655, -0.5931189050615464, 0.0, 1.0, 65.0, 200720.5692925988], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.12333333333333334, 0.0, 0.08333333333333333, 0.2572873391630459, 0.3022936983128179, 0.0, 1.0, 1.0, 0.955812234726661], 
reward next is 0.0442, 
noisyNet noise sample is [array([-0.22541824], dtype=float32), 0.6751804]. 
=============================================
[2019-04-08 15:12:28,112] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 62964: loss 4.8015
[2019-04-08 15:12:28,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 62964: learning rate 0.0000
[2019-04-08 15:12:28,270] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.13299711e-06 2.37260363e-03 1.01212457e-01 7.07657516e-01
 4.22622630e-04 1.19054850e-04 8.13209862e-02 1.78229820e-05
 2.18539668e-07 8.00756297e-08 1.06873535e-01], sum to 1.0000
[2019-04-08 15:12:28,274] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0512
[2019-04-08 15:12:28,288] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-4.666666666666667, 43.0, 0.0, 0.0, 19.0, 19.77509590018104, -0.9414444887353194, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [0.33333333333333304, 25.0], 
sim time this is 2413200.0000, 
sim time next is 2413800.0000, 
raw observation next is [-4.75, 42.5, 0.0, 0.0, 19.0, 19.66064822462572, -0.964922477022543, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.3310249307479225, 0.425, 0.0, 0.0, 0.08333333333333333, 0.1383873520521434, 0.17835917432581902, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9518759], dtype=float32), -0.52333367]. 
=============================================
[2019-04-08 15:12:28,438] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.3254661e-11 5.8084552e-05 2.2973940e-02 9.5637202e-01 1.0524973e-05
 6.6828996e-08 7.0147123e-03 1.0243828e-08 1.0799413e-12 8.5428660e-15
 1.3570663e-02], sum to 1.0000
[2019-04-08 15:12:28,438] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0167
[2019-04-08 15:12:28,480] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-1.283333333333333, 54.33333333333333, 0.0, 0.0, 22.5, 25.65306118141425, 0.3735980133376799, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.716666666666667, 25.0], 
sim time this is 2315400.0000, 
sim time next is 2316000.0000, 
raw observation next is [-1.366666666666667, 54.66666666666667, 0.0, 0.0, 22.5, 25.53472832923762, 0.3391191977914818, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.42474607571560485, 0.5466666666666667, 0.0, 0.0, 0.375, 0.6278940274364683, 0.6130397325971606, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5216331], dtype=float32), -0.34973258]. 
=============================================
[2019-04-08 15:12:28,490] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[66.114334]
 [66.25136 ]
 [65.983475]
 [66.203125]
 [66.395905]], R is [[66.23537445]
 [66.57302094]
 [66.9072876 ]
 [67.23821259]
 [67.56583405]].
[2019-04-08 15:12:28,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.91739113e-08 1.00158935e-03 2.35312596e-01 5.07957816e-01
 2.69756187e-04 4.59262774e-05 2.51033846e-02 3.55634597e-06
 1.37687985e-08 1.68528219e-10 2.30305269e-01], sum to 1.0000
[2019-04-08 15:12:28,744] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5599
[2019-04-08 15:12:28,751] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 66.33333333333333, 124.0, 375.0, 19.0, 23.24960497733651, -0.07610810317953345, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.0, 30.0], 
sim time this is 2367600.0000, 
sim time next is 2368200.0000, 
raw observation next is [-2.9, 65.66666666666667, 127.0, 390.0, 19.0, 23.17597184456846, -0.08848490564387694, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.38227146814404434, 0.6566666666666667, 0.42333333333333334, 0.430939226519337, 0.08333333333333333, 0.43133098704737155, 0.470505031452041, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08856215], dtype=float32), -0.6882904]. 
=============================================
[2019-04-08 15:12:28,921] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 63433: loss 0.0119
[2019-04-08 15:12:28,924] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 63433: learning rate 0.0000
[2019-04-08 15:12:29,216] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63610: loss 3.0187
[2019-04-08 15:12:29,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63622: learning rate 0.0000
[2019-04-08 15:12:29,348] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 63692: loss 0.2678
[2019-04-08 15:12:29,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 63693: learning rate 0.0000
[2019-04-08 15:12:29,361] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63703: loss -0.1276
[2019-04-08 15:12:29,362] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63703: learning rate 0.0000
[2019-04-08 15:12:29,438] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63748: loss 7.2613
[2019-04-08 15:12:29,440] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63748: learning rate 0.0000
[2019-04-08 15:12:29,467] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4124423e-10 1.3110858e-04 9.2741571e-02 8.5004234e-01 7.4955184e-05
 3.1838772e-06 2.8905254e-02 1.7592104e-07 3.5800499e-10 7.7863219e-12
 2.8101373e-02], sum to 1.0000
[2019-04-08 15:12:29,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1310
[2019-04-08 15:12:29,482] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.3, 62.0, 153.0, 378.0, 19.0, 24.57082095348449, 0.1846591654528027, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.7, 30.0], 
sim time this is 2372400.0000, 
sim time next is 2373000.0000, 
raw observation next is [-2.116666666666667, 59.50000000000001, 157.6666666666667, 354.0, 19.0, 24.48446050584964, 0.1740250322457728, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.40397045244690677, 0.5950000000000001, 0.5255555555555557, 0.3911602209944751, 0.08333333333333333, 0.5403717088208033, 0.5580083440819242, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9479417], dtype=float32), -1.610274]. 
=============================================
[2019-04-08 15:12:29,487] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[50.832035]
 [50.754063]
 [50.57097 ]
 [50.33575 ]
 [49.953114]], R is [[51.14384842]
 [51.63241196]
 [52.11608887]
 [52.59492874]
 [53.06898117]].
[2019-04-08 15:12:29,493] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63779: loss 1.7943
[2019-04-08 15:12:29,495] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63781: learning rate 0.0000
[2019-04-08 15:12:29,798] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63957: loss -3.6812
[2019-04-08 15:12:29,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63965: learning rate 0.0000
[2019-04-08 15:12:30,044] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64104: loss 8.7335
[2019-04-08 15:12:30,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64104: learning rate 0.0000
[2019-04-08 15:12:30,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.3581669e-08 9.1840420e-04 1.4005022e-01 8.2201272e-01 7.1434962e-04
 1.1823640e-05 1.9643446e-03 1.7794868e-06 5.3638125e-09 1.2939320e-09
 3.4326229e-02], sum to 1.0000
[2019-04-08 15:12:30,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5203
[2019-04-08 15:12:30,089] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 19.0, 21.52115477056369, -0.5731935914844231, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.6, 30.0], 
sim time this is 2406600.0000, 
sim time next is 2407200.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 19.0, 21.43006581548884, -0.5951490473857725, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.08333333333333333, 0.28583881795740346, 0.30161698420474253, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2501919], dtype=float32), -0.41285524]. 
=============================================
[2019-04-08 15:12:30,120] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 64150: loss 4.6421
[2019-04-08 15:12:30,124] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 64154: learning rate 0.0000
[2019-04-08 15:12:30,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8315748e-09 1.5850499e-04 1.8232727e-01 5.5453974e-01 9.6658423e-06
 2.9304529e-06 5.6240957e-02 8.1062575e-07 6.7666428e-10 4.4048896e-12
 2.0672014e-01], sum to 1.0000
[2019-04-08 15:12:30,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7523
[2019-04-08 15:12:30,163] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.00000000000001, 110.6666666666667, 227.3333333333334, 19.0, 24.88185982923558, 0.2387911923924998, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.0, 45.0], 
sim time this is 2387400.0000, 
sim time next is 2388000.0000, 
raw observation next is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 19.0, 24.85148628654309, 0.2323637328064475, 0.0, 1.0, 45.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.3277777777777778, 0.31399631675874773, 0.08333333333333333, 0.5709571905452574, 0.5774545776021491, 0.0, 1.0, 0.6, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6101734], dtype=float32), 0.21757366]. 
=============================================
[2019-04-08 15:12:30,166] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64172: loss 4.4277
[2019-04-08 15:12:30,169] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64174: learning rate 0.0000
[2019-04-08 15:12:30,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[53.012817]
 [53.277004]
 [52.940277]
 [53.477562]
 [53.49289 ]], R is [[54.14634705]
 [54.6048851 ]
 [55.05883789]
 [55.50825119]
 [55.95317078]].
[2019-04-08 15:12:30,319] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6312951e-07 1.8863572e-03 1.8092147e-01 6.1544192e-01 7.3686219e-04
 2.0068306e-04 6.3884638e-02 2.4855690e-05 5.8395724e-08 1.3659583e-09
 1.3690293e-01], sum to 1.0000
[2019-04-08 15:12:30,322] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2304
[2019-04-08 15:12:30,341] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 46.33333333333334, 0.0, 0.0, 19.0, 22.29699903081745, -0.3784518157475427, 0.0, 1.0, 35.0, 0.0], 
current ob forecast is [], 
actual action is [-1.0, 30.0], 
sim time this is 2421600.0000, 
sim time next is 2422200.0000, 
raw observation next is [-6.1, 47.16666666666666, 0.0, 0.0, 19.0, 22.2039760238022, -0.4040144562501843, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.0, 0.29362880886426596, 0.47166666666666657, 0.0, 0.0, 0.08333333333333333, 0.35033133531684985, 0.36532851458327187, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31372833], dtype=float32), -1.4764686]. 
=============================================
[2019-04-08 15:12:30,343] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0466598e-08 5.2715145e-04 4.6033565e-02 8.6219817e-01 3.1391729e-04
 3.0581265e-05 2.6626755e-02 6.6431983e-07 4.6658322e-10 1.7760783e-10
 6.4269267e-02], sum to 1.0000
[2019-04-08 15:12:30,343] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5189
[2019-04-08 15:12:30,362] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.5, 42.66666666666667, 0.0, 0.0, 19.0, 22.08173705867301, -0.4168356939864106, 0.0, 1.0, 65.0, 203981.6176019923], 
current ob forecast is [], 
actual action is [-0.5, 30.0], 
sim time this is 2418600.0000, 
sim time next is 2419200.0000, 
raw observation next is [-5.6, 43.0, 0.0, 0.0, 19.0, 22.1277452019498, -0.404506810983769, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.0, 0.30747922437673136, 0.43, 0.0, 0.0, 0.08333333333333333, 0.34397876682915, 0.36516439633874365, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20989613], dtype=float32), -0.5904588]. 
=============================================
[2019-04-08 15:12:30,605] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64438: loss 0.7755
[2019-04-08 15:12:30,605] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64438: learning rate 0.0000
[2019-04-08 15:12:30,707] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64502: loss 1.8826
[2019-04-08 15:12:30,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64502: learning rate 0.0000
[2019-04-08 15:12:30,832] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9795540e-07 4.6005487e-04 4.5345873e-02 6.2636387e-01 1.5548968e-03
 8.8504814e-05 1.4616303e-02 2.9103505e-06 1.0214040e-08 3.4194090e-09
 3.1156716e-01], sum to 1.0000
[2019-04-08 15:12:30,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3969
[2019-04-08 15:12:30,855] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 19.0, 21.74170223057055, -0.4717870001824175, 0.0, 1.0, 45.0, 112148.0956304512], 
current ob forecast is [], 
actual action is [1.6, 30.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 19.0, 21.71138150052792, -0.4830481799831188, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.08333333333333333, 0.30928179171066006, 0.3389839400056271, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.83642787], dtype=float32), -1.0389993]. 
=============================================
[2019-04-08 15:12:30,899] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6732453e-07 2.1392975e-03 2.1945320e-01 6.0524476e-01 2.9472979e-03
 1.9207004e-03 3.8851496e-02 7.9394613e-06 1.0446929e-07 2.1487844e-08
 1.2943473e-01], sum to 1.0000
[2019-04-08 15:12:30,900] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9598
[2019-04-08 15:12:30,913] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 60.5, 0.0, 0.0, 19.0, 20.04318181591685, -0.8569789822189158, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 2445000.0000, 
sim time next is 2445600.0000, 
raw observation next is [-9.5, 60.0, 0.0, 0.0, 19.0, 20.00115671444074, -0.8553999131172331, 0.0, 1.0, 65.0, 200103.8922449021], 
processed observation next is [0.0, 0.30434782608695654, 0.1994459833795014, 0.6, 0.0, 0.0, 0.08333333333333333, 0.1667630595367283, 0.21486669562758898, 0.0, 1.0, 1.0, 0.9528756773566767], 
reward next is 0.0471, 
noisyNet noise sample is [array([2.8914564], dtype=float32), -2.026778]. 
=============================================
[2019-04-08 15:12:31,419] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8196420e-08 3.6631769e-04 4.3109417e-02 8.8761830e-01 9.2163638e-05
 4.8666489e-06 4.9428516e-03 6.7183475e-07 5.5372240e-09 4.3036907e-10
 6.3865468e-02], sum to 1.0000
[2019-04-08 15:12:31,419] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6133
[2019-04-08 15:12:31,434] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.95, 39.5, 76.0, 777.0, 19.0, 20.80180133211611, -0.6367131765452759, 0.0, 1.0, 65.0, 200659.2131773955], 
current ob forecast is [], 
actual action is [1.0499999999999998, 30.0], 
sim time this is 2457000.0000, 
sim time next is 2457600.0000, 
raw observation next is [-3.4, 38.33333333333334, 77.66666666666667, 785.6666666666666, 19.0, 20.88473065406672, -0.6079144961557911, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.368421052631579, 0.3833333333333334, 0.2588888888888889, 0.8681399631675875, 0.08333333333333333, 0.24039422117222653, 0.2973618346147363, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60450757], dtype=float32), -2.3840883]. 
=============================================
[2019-04-08 15:12:31,640] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9700732e-09 1.9303622e-04 2.3502782e-02 8.7607491e-01 4.6199548e-05
 2.5297295e-06 2.9921953e-02 3.7842494e-07 2.0457369e-09 2.1270462e-10
 7.0258170e-02], sum to 1.0000
[2019-04-08 15:12:31,643] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7972
[2019-04-08 15:12:31,655] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 49.5, 160.0, 0.0, 19.0, 22.45787081744414, -0.2352522739382089, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.0, 30.0], 
sim time this is 2385000.0000, 
sim time next is 2385600.0000, 
raw observation next is [0.0, 48.66666666666667, 147.6666666666667, 56.83333333333332, 19.0, 22.53107309382559, -0.2270474040953897, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.4866666666666667, 0.4922222222222224, 0.06279926335174953, 0.08333333333333333, 0.37758942448546584, 0.4243175319682034, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-7.6296565e-05], dtype=float32), 0.111064315]. 
=============================================
[2019-04-08 15:12:31,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9981131e-06 1.7585097e-02 1.1415937e-01 4.8782140e-01 8.3594961e-04
 1.6038258e-04 5.7243604e-02 1.4526805e-05 1.0460127e-06 3.6639356e-08
 3.2217559e-01], sum to 1.0000
[2019-04-08 15:12:31,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6393
[2019-04-08 15:12:31,700] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.933333333333334, 51.33333333333333, 0.0, 0.0, 19.0, 20.06922151521265, -0.8330508560028113, 0.0, 1.0, 30.0, 19920.75756382935], 
current ob forecast is [], 
actual action is [-1.9333333333333336, 45.0], 
sim time this is 2425200.0000, 
sim time next is 2425800.0000, 
raw observation next is [-7.116666666666666, 52.16666666666667, 0.0, 0.0, 19.0, 19.98495713789959, -0.8367503665885444, 0.0, 1.0, 45.0, 196487.0171346621], 
processed observation next is [0.0, 0.043478260869565216, 0.265466297322253, 0.5216666666666667, 0.0, 0.0, 0.08333333333333333, 0.1654130948249657, 0.22108321113715187, 0.0, 1.0, 0.6, 0.93565246254601], 
reward next is 0.0643, 
noisyNet noise sample is [array([-0.25204077], dtype=float32), -1.1094849]. 
=============================================
[2019-04-08 15:12:31,706] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9244983e-09 1.6399725e-04 2.1680612e-02 8.8860208e-01 3.9589995e-05
 2.5534018e-06 2.8672881e-02 4.0675479e-07 2.0086568e-09 2.1142799e-10
 6.0837906e-02], sum to 1.0000
[2019-04-08 15:12:31,707] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4916
[2019-04-08 15:12:31,721] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 47.0, 123.0, 170.5, 19.0, 22.59937974938497, -0.2232634481964235, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.0, 30.0], 
sim time this is 2386800.0000, 
sim time next is 2387400.0000, 
raw observation next is [0.0, 47.00000000000001, 110.6666666666667, 227.3333333333334, 19.0, 22.60445335252555, -0.2274187042716407, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.4700000000000001, 0.368888888888889, 0.2511970534069982, 0.08333333333333333, 0.3837044460437958, 0.42419376524278646, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-7.6296565e-05], dtype=float32), 0.111064315]. 
=============================================
[2019-04-08 15:12:31,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9602782e-09 1.5145118e-04 4.5489836e-02 5.0018328e-01 1.4800808e-05
 6.9659563e-06 3.5030204e-03 4.1133529e-08 3.3443903e-10 1.7349533e-11
 4.5065054e-01], sum to 1.0000
[2019-04-08 15:12:31,838] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2793
[2019-04-08 15:12:31,856] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 19.0, 21.98152520654606, -0.4346090605813919, 0.0, 1.0, 65.0, 179696.0650853455], 
current ob forecast is [], 
actual action is [3.266666666666667, 30.0], 
sim time this is 2460000.0000, 
sim time next is 2460600.0000, 
raw observation next is [-1.45, 33.5, 86.0, 829.0, 19.0, 22.02246554863122, -0.410144533952589, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.422437673130194, 0.335, 0.2866666666666667, 0.9160220994475138, 0.08333333333333333, 0.33520546238593507, 0.36328515534913697, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7072706], dtype=float32), -0.7125486]. 
=============================================
[2019-04-08 15:12:32,136] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.02695369e-09 7.42668461e-04 3.37221660e-02 8.70551288e-01
 1.53379937e-04 1.41655419e-05 2.39027813e-02 1.83225256e-06
 1.06736024e-08 1.52009705e-09 7.09117204e-02], sum to 1.0000
[2019-04-08 15:12:32,140] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2099
[2019-04-08 15:12:32,149] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1333333333333333, 30.0, 89.33333333333333, 842.3333333333334, 19.0, 19.80394882003395, -0.8785573900504069, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.133333333333333, 30.0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 90.0, 845.0, 19.0, 19.81544483865957, -0.8763310365271598, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4764542936288089, 0.295, 0.3, 0.9337016574585635, 0.08333333333333333, 0.15128706988829746, 0.20788965449094673, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12035942], dtype=float32), 0.14532168]. 
=============================================
[2019-04-08 15:12:32,204] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4000, global step 65386: loss 0.4529
[2019-04-08 15:12:32,205] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4000, global step 65387: learning rate 0.0000
[2019-04-08 15:12:32,483] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 65562: loss 1.1844
[2019-04-08 15:12:32,485] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 65564: learning rate 0.0000
[2019-04-08 15:12:33,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1726903e-08 1.6461473e-04 2.4542205e-02 9.5711201e-01 5.3905056e-05
 2.1453636e-06 1.3637524e-03 4.3401749e-07 4.2737218e-09 2.0014368e-10
 1.6760955e-02], sum to 1.0000
[2019-04-08 15:12:33,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4456
[2019-04-08 15:12:33,561] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.2, 34.33333333333334, 0.0, 0.0, 19.0, 20.63573003807574, -0.7232903921313265, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.8, 30.0], 
sim time this is 2497200.0000, 
sim time next is 2497800.0000, 
raw observation next is [-1.2, 33.66666666666666, 0.0, 0.0, 19.0, 20.66819942347679, -0.7320740467104198, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.33666666666666656, 0.0, 0.0, 0.08333333333333333, 0.2223499519563991, 0.25597531776319343, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54296607], dtype=float32), 0.7070849]. 
=============================================
[2019-04-08 15:12:33,774] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2466812e-09 2.8202517e-04 1.2768674e-01 7.9169470e-01 3.2069167e-04
 7.1226964e-06 9.4770035e-03 2.9232965e-06 1.6726769e-08 8.0873602e-10
 7.0528731e-02], sum to 1.0000
[2019-04-08 15:12:33,780] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0313
[2019-04-08 15:12:33,790] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-0.9666666666666667, 36.0, 0.0, 0.0, 19.0, 19.90623632011336, -0.9409191143674139, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.033333333333333, 25.0], 
sim time this is 2503200.0000, 
sim time next is 2503800.0000, 
raw observation next is [-1.15, 36.5, 0.0, 0.0, 19.0, 19.85251923887673, -0.955630953856376, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 1.0, 0.4307479224376732, 0.365, 0.0, 0.0, 0.08333333333333333, 0.15437660323972757, 0.18145634871454133, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4185834], dtype=float32), -0.61667776]. 
=============================================
[2019-04-08 15:12:34,045] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4293476e-08 7.3994359e-04 6.8376750e-02 8.5511458e-01 2.7370997e-04
 5.3566505e-06 4.4542186e-02 2.1590533e-07 5.2829181e-09 8.8766960e-10
 3.0947182e-02], sum to 1.0000
[2019-04-08 15:12:34,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5354
[2019-04-08 15:12:34,059] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.7, 34.66666666666666, 0.0, 0.0, 19.0, 20.49098211384384, -0.810231315970484, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [4.3, 30.0], 
sim time this is 2501400.0000, 
sim time next is 2502000.0000, 
raw observation next is [-0.6, 35.0, 0.0, 0.0, 19.0, 20.4290335423648, -0.8248901649778807, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 1.0, 0.44598337950138506, 0.35, 0.0, 0.0, 0.08333333333333333, 0.20241946186373325, 0.22503661167403977, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6257478], dtype=float32), 0.07409918]. 
=============================================
[2019-04-08 15:12:34,067] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[45.029198]
 [45.18509 ]
 [45.063465]
 [45.117523]
 [45.22431 ]], R is [[45.65858459]
 [46.20199966]
 [46.73997879]
 [47.27257919]
 [47.79985428]].
[2019-04-08 15:12:34,107] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8754594e-09 2.0760948e-04 2.6867988e-02 9.3615860e-01 2.8678103e-04
 2.0697748e-06 3.2820380e-03 6.9727935e-07 4.6597934e-09 4.5276111e-10
 3.3194274e-02], sum to 1.0000
[2019-04-08 15:12:34,116] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2530
[2019-04-08 15:12:34,136] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 39.33333333333334, 0.0, 0.0, 19.0, 20.28082591551578, -0.8698068228001614, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.3, 30.0], 
sim time this is 2510400.0000, 
sim time next is 2511000.0000, 
raw observation next is [-1.7, 39.0, 0.0, 0.0, 19.0, 20.28143738725456, -0.8682290802775988, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.39, 0.0, 0.0, 0.08333333333333333, 0.1901197822712133, 0.21059030657413372, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6847783], dtype=float32), -0.35924757]. 
=============================================
[2019-04-08 15:12:34,151] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[45.898643]
 [45.343647]
 [45.26723 ]
 [45.212135]
 [45.095192]], R is [[46.87049866]
 [47.40179443]
 [47.92777634]
 [48.44849777]
 [48.96401215]].
[2019-04-08 15:12:34,283] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4959815e-08 2.0521415e-04 1.4923149e-01 6.8786532e-01 4.2998817e-04
 7.2566313e-06 3.2513227e-02 3.0091314e-06 3.9821078e-08 3.9190979e-09
 1.2974425e-01], sum to 1.0000
[2019-04-08 15:12:34,284] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5228
[2019-04-08 15:12:34,294] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 39.33333333333333, 0.0, 0.0, 19.0, 19.28674231071654, -1.080846510419348, 0.0, 1.0, 30.00000000000001, 0.0], 
current ob forecast is [], 
actual action is [3.3, 30.0], 
sim time this is 2508000.0000, 
sim time next is 2508600.0000, 
raw observation next is [-1.7, 39.66666666666667, 0.0, 0.0, 19.0, 19.36067587783423, -1.085051473092182, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3966666666666667, 0.0, 0.0, 0.08333333333333333, 0.11338965648618575, 0.13831617563593931, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.050243], dtype=float32), 0.31840885]. 
=============================================
[2019-04-08 15:12:36,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7812675e-10 8.0328471e-05 7.9632588e-02 8.2263303e-01 6.3663306e-06
 3.5421479e-07 2.2582572e-03 6.8075843e-09 9.8425313e-12 6.3007221e-12
 9.5389023e-02], sum to 1.0000
[2019-04-08 15:12:36,503] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0239
[2019-04-08 15:12:36,558] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-2.8, 55.66666666666667, 79.33333333333333, 23.0, 22.5, 20.86676301842459, -0.792887610383992, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [2.2, 25.0], 
sim time this is 2537400.0000, 
sim time next is 2538000.0000, 
raw observation next is [-2.8, 56.0, 93.5, 25.5, 22.5, 20.92968737976708, -0.7734994479830832, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.38504155124653744, 0.56, 0.31166666666666665, 0.0281767955801105, 0.375, 0.24414061498059011, 0.2421668506723056, 1.0, 1.0, 0.2, 0.0], 
reward next is 0.4847, 
noisyNet noise sample is [array([1.6866622], dtype=float32), 2.0078604]. 
=============================================
[2019-04-08 15:12:36,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[61.524036]
 [60.557793]
 [60.087406]
 [59.103756]
 [58.604706]], R is [[62.16298294]
 [61.98331833]
 [61.96524429]
 [62.27381134]
 [62.65107346]].
[2019-04-08 15:12:36,813] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6694998e-14 1.0060280e-05 8.9594238e-03 9.8414767e-01 1.5875302e-06
 1.2139821e-10 5.4252410e-04 5.4526966e-11 9.9922681e-15 2.0293933e-16
 6.3386527e-03], sum to 1.0000
[2019-04-08 15:12:36,814] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6348
[2019-04-08 15:12:36,846] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5333333333333334, 41.66666666666667, 229.6666666666667, 62.83333333333334, 22.5, 22.19462076899609, -0.5299564774652813, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [5.533333333333333, 30.0], 
sim time this is 2547600.0000, 
sim time next is 2548200.0000, 
raw observation next is [0.8166666666666668, 40.33333333333333, 227.3333333333333, 54.66666666666667, 22.5, 22.23422771644807, -0.5242132127870155, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.4782608695652174, 0.4852262234533703, 0.40333333333333327, 0.7577777777777777, 0.060405156537753225, 0.375, 0.35285230970400594, 0.32526226240432815, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9087, 
noisyNet noise sample is [array([0.16118485], dtype=float32), -0.26942685]. 
=============================================
[2019-04-08 15:12:37,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0988960e-09 2.9267672e-05 6.4715073e-03 8.2577962e-01 1.3095493e-05
 4.8906918e-07 7.9419594e-03 1.6415225e-08 6.2728439e-10 7.3647631e-13
 1.5976408e-01], sum to 1.0000
[2019-04-08 15:12:37,201] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6157
[2019-04-08 15:12:37,226] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.933333333333334, 25.5, 20.0, 121.6666666666667, 19.0, 21.30176667872686, -0.6222445760233516, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [7.933333333333334, 30.0], 
sim time this is 2481000.0000, 
sim time next is 2481600.0000, 
raw observation next is [2.566666666666667, 26.0, 13.0, 82.33333333333333, 19.0, 21.34729197108597, -0.630318978034692, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5337026777469991, 0.26, 0.043333333333333335, 0.09097605893186003, 0.08333333333333333, 0.27894099759049745, 0.289893673988436, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1053553], dtype=float32), -0.010352314]. 
=============================================
[2019-04-08 15:12:38,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2982260e-10 5.9823185e-05 2.9572053e-02 9.5540315e-01 6.1826919e-05
 1.1352229e-06 2.0283165e-03 3.9096836e-08 1.3881542e-10 1.4443682e-11
 1.2873696e-02], sum to 1.0000
[2019-04-08 15:12:38,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4064
[2019-04-08 15:12:38,663] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 19.0, 20.22714852436867, -0.9056147620919214, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.3, 30.0], 
sim time this is 2511000.0000, 
sim time next is 2511600.0000, 
raw observation next is [-1.7, 38.66666666666667, 0.0, 0.0, 19.0, 20.19960750156062, -0.9179885686501889, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3866666666666667, 0.0, 0.0, 0.08333333333333333, 0.18330062513005174, 0.19400381044993706, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34812117], dtype=float32), 0.18081445]. 
=============================================
[2019-04-08 15:12:39,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5693545e-15 4.2513034e-06 3.7144555e-04 9.9906605e-01 4.9329241e-08
 6.8577249e-10 1.2802429e-05 1.3618726e-12 5.5180375e-17 7.5512372e-19
 5.4532872e-04], sum to 1.0000
[2019-04-08 15:12:39,720] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2927
[2019-04-08 15:12:39,775] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8666666666666667, 34.0, 0.0, 0.0, 22.5, 22.72299313326099, -0.2741807050972984, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [5.866666666666667, 30.0], 
sim time this is 2569800.0000, 
sim time next is 2570400.0000, 
raw observation next is [0.5, 35.0, 0.0, 0.0, 22.5, 22.82305898486, -0.2775567559576032, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.35, 0.0, 0.0, 0.375, 0.4019215820716668, 0.4074810813474656, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.34335774], dtype=float32), -0.43377405]. 
=============================================
[2019-04-08 15:12:40,283] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.4947495e-09 2.8712256e-04 2.7593896e-01 4.8105702e-01 3.9314433e-05
 2.2452508e-07 7.1632178e-03 4.4780823e-08 2.9658882e-09 1.7603915e-12
 2.3551403e-01], sum to 1.0000
[2019-04-08 15:12:40,284] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6664
[2019-04-08 15:12:40,313] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.633333333333333, 55.0, 0.0, 0.0, 19.0, 19.82795895904076, -0.9668953801027134, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.366666666666667, 30.0], 
sim time this is 2529600.0000, 
sim time next is 2530200.0000, 
raw observation next is [-2.716666666666667, 54.5, 0.0, 0.0, 19.0, 19.96501907304607, -0.9854658319982114, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3873499538319483, 0.545, 0.0, 0.0, 0.08333333333333333, 0.16375158942050572, 0.17151138933392954, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04898749], dtype=float32), -0.04148298]. 
=============================================
[2019-04-08 15:12:40,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4316103e-11 3.4164561e-05 2.7435968e-02 9.6114147e-01 9.3931976e-07
 1.4791414e-07 2.1884739e-03 6.1745159e-10 2.3623705e-12 1.6272328e-14
 9.1989646e-03], sum to 1.0000
[2019-04-08 15:12:40,702] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8254
[2019-04-08 15:12:40,732] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.35, 57.5, 0.0, 0.0, 19.0, 21.8739118291472, -0.4420753463714751, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [1.65, 30.0], 
sim time this is 2586600.0000, 
sim time next is 2587200.0000, 
raw observation next is [-3.533333333333333, 58.0, 0.0, 0.0, 19.0, 21.71016295596395, -0.4646092957141576, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.9565217391304348, 0.36472760849492153, 0.58, 0.0, 0.0, 0.08333333333333333, 0.30918024633032903, 0.34513023476194743, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.7383436], dtype=float32), 0.7578733]. 
=============================================
[2019-04-08 15:12:42,071] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1920642e-11 3.3880133e-06 2.3152772e-02 9.6154249e-01 1.0454468e-06
 3.8019142e-08 1.1809670e-02 2.1275848e-09 2.1553695e-11 4.4296405e-14
 3.4905956e-03], sum to 1.0000
[2019-04-08 15:12:42,071] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7303
[2019-04-08 15:12:42,119] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.3, 79.0, 42.0, 4.0, 22.5, 19.17331076956527, -1.011564697646985, 1.0, 1.0, 30.00000000000002, 0.0], 
current ob forecast is [], 
actual action is [-2.3, 30.0], 
sim time this is 2620800.0000, 
sim time next is 2621400.0000, 
raw observation next is [-7.199999999999999, 78.33333333333334, 53.66666666666667, 2.666666666666666, 22.5, 19.32293313023396, -1.00198539850153, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.26315789473684215, 0.7833333333333334, 0.1788888888888889, 0.00294659300184162, 0.375, 0.11024442751949663, 0.1660048671661567, 1.0, 1.0, 0.3, 0.0], 
reward next is 0.2395, 
noisyNet noise sample is [array([-0.44483027], dtype=float32), -1.7818512]. 
=============================================
[2019-04-08 15:12:45,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0161892e-15 5.0136759e-06 8.9929113e-03 8.8550270e-01 1.3429364e-07
 2.1327495e-10 1.3408123e-03 6.8258459e-12 3.5268057e-15 5.6301138e-19
 1.0415851e-01], sum to 1.0000
[2019-04-08 15:12:45,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8857
[2019-04-08 15:12:45,678] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1333333333333333, 44.33333333333333, 172.6666666666667, 197.5, 22.5, 22.77806433995267, -0.2445224145259721, 1.0, 1.0, 30.00000000000002, 0.0], 
current ob forecast is [], 
actual action is [5.133333333333333, 30.0], 
sim time this is 2641200.0000, 
sim time next is 2641800.0000, 
raw observation next is [0.3166666666666667, 43.66666666666666, 181.3333333333333, 184.0, 22.5, 23.22513063986846, -0.205912753163684, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.47137580794090495, 0.4366666666666666, 0.6044444444444443, 0.20331491712707184, 0.375, 0.4354275533223717, 0.43136241561210537, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.59122044], dtype=float32), -1.5596106]. 
=============================================
[2019-04-08 15:12:47,323] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71032: loss 2.7973
[2019-04-08 15:12:47,324] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71033: learning rate 0.0000
[2019-04-08 15:12:47,418] A3C_AGENT_WORKER-Thread-8 INFO:Local step 4500, global step 71064: loss 1.4754
[2019-04-08 15:12:47,419] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 4500, global step 71064: learning rate 0.0000
[2019-04-08 15:12:47,609] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71124: loss 4.4258
[2019-04-08 15:12:47,610] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71124: learning rate 0.0000
[2019-04-08 15:12:47,736] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71160: loss -4.1416
[2019-04-08 15:12:47,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71169: learning rate 0.0000
[2019-04-08 15:12:48,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3104290e-10 2.0434872e-04 3.4410939e-02 8.8175166e-01 2.6379263e-05
 3.6137401e-06 1.3829722e-02 4.3759584e-08 8.6565637e-11 6.8557465e-11
 6.9773354e-02], sum to 1.0000
[2019-04-08 15:12:48,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7249
[2019-04-08 15:12:48,182] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 19.0, 18.46432927628101, -1.185481233311633, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [-10.0, 30.0], 
sim time this is 2696400.0000, 
sim time next is 2697000.0000, 
raw observation next is [-15.16666666666667, 83.0, 0.0, 0.0, 19.0, 18.42049284336118, -1.200359001463753, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.21739130434782608, 0.04247460757156039, 0.83, 0.0, 0.0, 0.08333333333333333, 0.03504107028009843, 0.09988033284541564, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42295277], dtype=float32), 0.01718152]. 
=============================================
[2019-04-08 15:12:48,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[51.156208]
 [51.259   ]
 [51.35461 ]
 [51.536674]
 [51.657013]], R is [[49.43916321]
 [48.94477081]
 [48.67982864]
 [48.56401443]
 [48.68280411]].
[2019-04-08 15:12:48,327] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71346: loss -0.0799
[2019-04-08 15:12:48,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71346: learning rate 0.0000
[2019-04-08 15:12:48,418] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71366: loss -2.0254
[2019-04-08 15:12:48,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71366: learning rate 0.0000
[2019-04-08 15:12:48,836] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5567287e-13 5.4170964e-06 1.8551683e-02 8.7740409e-01 5.1135071e-06
 1.1563582e-07 2.0401110e-04 2.0146645e-10 5.0645057e-15 1.5869927e-15
 1.0382953e-01], sum to 1.0000
[2019-04-08 15:12:48,836] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1584
[2019-04-08 15:12:48,866] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.783333333333333, 68.33333333333333, 0.0, 0.0, 19.0, 22.60993650064346, -0.2526530112032307, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.216666666666667, 30.0], 
sim time this is 2670600.0000, 
sim time next is 2671200.0000, 
raw observation next is [-3.1, 69.0, 0.0, 0.0, 19.0, 22.55526551519206, -0.2713189797182438, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.37673130193905824, 0.69, 0.0, 0.0, 0.08333333333333333, 0.37960545959933817, 0.40956034009391873, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5168725], dtype=float32), -0.11925452]. 
=============================================
[2019-04-08 15:12:49,310] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.74849879e-13 1.93309774e-07 1.58302824e-03 9.39775169e-01
 2.81972007e-07 1.52744828e-09 2.12323954e-04 3.65375577e-11
 1.11567114e-13 1.40610942e-14 5.84290065e-02], sum to 1.0000
[2019-04-08 15:12:49,310] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6921
[2019-04-08 15:12:49,378] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 72.0, 0.0, 0.0, 19.0, 21.94101237656327, -0.3771571958702374, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [-2.0, 30.0], 
sim time this is 2678400.0000, 
sim time next is 2679000.0000, 
raw observation next is [-7.333333333333334, 71.5, 0.0, 0.0, 19.0, 21.86796101075444, -0.39734895428267, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.0, 0.2594644506001847, 0.715, 0.0, 0.0, 0.08333333333333333, 0.3223300842295365, 0.36755034857244334, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.5165825], dtype=float32), -0.6641056]. 
=============================================
[2019-04-08 15:12:49,383] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[67.65681 ]
 [68.349915]
 [68.520004]
 [68.679634]
 [68.87851 ]], R is [[67.20874023]
 [67.4477005 ]
 [67.68427277]
 [67.91847992]
 [68.15034485]].
[2019-04-08 15:12:49,819] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71799: loss 1.9252
[2019-04-08 15:12:49,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71799: learning rate 0.0000
[2019-04-08 15:12:50,157] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71887: loss 2.4277
[2019-04-08 15:12:50,158] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71887: learning rate 0.0000
[2019-04-08 15:12:50,180] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3738123e-11 2.4728383e-06 2.0452836e-03 9.9157929e-01 2.5939355e-06
 7.0473966e-08 4.2079805e-04 2.0402529e-09 7.1820080e-12 4.4091358e-13
 5.9493934e-03], sum to 1.0000
[2019-04-08 15:12:50,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8958
[2019-04-08 15:12:50,221] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.9, 83.0, 0.0, 0.0, 19.0, 20.34735467747388, -0.7465843735787457, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-7.9, 30.0], 
sim time this is 2689200.0000, 
sim time next is 2689800.0000, 
raw observation next is [-13.25, 84.33333333333334, 0.0, 0.0, 19.0, 20.29321326686649, -0.7640146288948819, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.09556786703601107, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.1911011055722076, 0.24532845703503936, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57781285], dtype=float32), 1.4585327]. 
=============================================
[2019-04-08 15:12:50,443] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71973: loss 3.8174
[2019-04-08 15:12:50,444] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71973: learning rate 0.0000
[2019-04-08 15:12:50,600] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 72023: loss 0.0692
[2019-04-08 15:12:50,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 72025: learning rate 0.0000
[2019-04-08 15:12:50,696] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1392089e-14 7.8665318e-05 1.2536623e-02 9.3047601e-01 9.2480022e-08
 2.7655192e-10 7.7856175e-04 5.2086582e-12 1.0017990e-15 1.1349644e-18
 5.6130134e-02], sum to 1.0000
[2019-04-08 15:12:50,697] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3047
[2019-04-08 15:12:50,720] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.1333333333333333, 44.33333333333333, 172.6666666666667, 197.5, 22.5, 24.32926936121855, 0.07161443270099138, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.133333333333333, 30.0], 
sim time this is 2641200.0000, 
sim time next is 2641800.0000, 
raw observation next is [0.3166666666666667, 43.66666666666666, 181.3333333333333, 184.0, 22.5, 24.41174377159315, -0.01257727891345352, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.47137580794090495, 0.4366666666666666, 0.6044444444444443, 0.20331491712707184, 0.375, 0.534311980966096, 0.4958075736955155, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27398202], dtype=float32), -0.97699624]. 
=============================================
[2019-04-08 15:12:50,751] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 72067: loss 1.9850
[2019-04-08 15:12:50,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 72067: learning rate 0.0000
[2019-04-08 15:12:50,903] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5861006e-10 1.5613268e-04 2.3039952e-02 7.6097864e-01 3.2612596e-05
 1.0162295e-06 8.6906198e-03 6.7558652e-08 9.3889774e-10 1.2483235e-11
 2.0710088e-01], sum to 1.0000
[2019-04-08 15:12:50,907] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6693
[2019-04-08 15:12:50,970] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.5, 83.0, 0.0, 0.0, 19.0, 19.44565060821507, -0.9553714672162931, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-10.5, 30.0], 
sim time this is 2701800.0000, 
sim time next is 2702400.0000, 
raw observation next is [-15.33333333333333, 83.0, 0.0, 0.0, 19.0, 19.40090704180992, -0.9896726751213237, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.03785780240073876, 0.83, 0.0, 0.0, 0.08333333333333333, 0.11674225348415994, 0.17010910829289208, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43470088], dtype=float32), 0.34875193]. 
=============================================
[2019-04-08 15:12:51,005] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72132: loss 1.2757
[2019-04-08 15:12:51,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72132: learning rate 0.0000
[2019-04-08 15:12:51,540] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72255: loss 0.6000
[2019-04-08 15:12:51,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72255: learning rate 0.0000
[2019-04-08 15:12:51,549] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72256: loss -0.0013
[2019-04-08 15:12:51,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72256: learning rate 0.0000
[2019-04-08 15:12:54,462] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.2246986e-10 3.7709331e-05 5.5183895e-02 7.0102274e-01 1.0965651e-05
 2.0479050e-07 3.4483720e-03 1.3700411e-07 3.2221675e-10 5.3957311e-11
 2.4029593e-01], sum to 1.0000
[2019-04-08 15:12:54,463] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1902
[2019-04-08 15:12:54,478] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-15.0, 89.66666666666667, 0.0, 0.0, 19.0, 19.53115934319806, -0.8891769502441426, 0.0, 1.0, 65.0, 196710.9496090159], 
current ob forecast is [], 
actual action is [-10.0, 30.0], 
sim time this is 2693400.0000, 
sim time next is 2694000.0000, 
raw observation next is [-15.0, 88.33333333333334, 0.0, 0.0, 19.0, 19.47559504046929, -0.8900044409173332, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.12296625337244087, 0.2033318530275556, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05410692], dtype=float32), 0.026645456]. 
=============================================
[2019-04-08 15:12:54,518] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[54.297047]
 [53.622295]
 [53.784454]
 [53.964283]
 [54.948208]], R is [[54.77833176]
 [54.29383087]
 [54.66193771]
 [55.1153183 ]
 [55.56416702]].
[2019-04-08 15:12:54,932] A3C_AGENT_WORKER-Thread-7 INFO:Local step 4500, global step 73052: loss 1.2654
[2019-04-08 15:12:54,952] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 4500, global step 73052: learning rate 0.0000
[2019-04-08 15:12:55,588] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1248666e-09 8.6775079e-05 1.0034434e-01 8.6329532e-01 1.4128104e-04
 1.0712176e-06 5.8133556e-03 4.9147271e-08 2.1461074e-11 9.1562270e-12
 3.0317828e-02], sum to 1.0000
[2019-04-08 15:12:55,589] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9301
[2019-04-08 15:12:55,609] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 19.60172529782435, -0.9837152275345771, 0.0, 1.0, 45.0, 76372.16952049955], 
current ob forecast is [], 
actual action is [-1.0, 30.0], 
sim time this is 2778600.0000, 
sim time next is 2779200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 19.59332486870787, -0.9861376320239806, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.1327770723923226, 0.17128745599200645, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8405216], dtype=float32), -0.1596319]. 
=============================================
[2019-04-08 15:12:56,147] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 73339: loss 0.0888
[2019-04-08 15:12:56,157] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 73339: learning rate 0.0000
[2019-04-08 15:12:57,787] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6745386e-14 1.2961361e-06 7.3882155e-02 8.9559084e-01 1.0725444e-06
 4.0884265e-09 8.5348607e-04 4.3895454e-10 1.6914046e-13 5.7043678e-18
 2.9671203e-02], sum to 1.0000
[2019-04-08 15:12:57,787] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1032
[2019-04-08 15:12:57,847] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 22.5, 22.07712573856772, -0.365307372023892, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-2.666666666666667, 30.0], 
sim time this is 2722200.0000, 
sim time next is 2722800.0000, 
raw observation next is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 22.5, 22.64347072346466, -0.4449320650058667, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2594644506001847, 0.6233333333333334, 0.376111111111111, 0.8795580110497238, 0.375, 0.38695589362205496, 0.3516893116647111, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8618402], dtype=float32), -1.0454957]. 
=============================================
[2019-04-08 15:12:58,501] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5134645e-14 6.4100791e-06 2.4604376e-03 9.6840423e-01 6.6838072e-07
 1.1313926e-09 5.1234581e-04 1.3677681e-10 8.7549210e-14 8.1433370e-17
 2.8615888e-02], sum to 1.0000
[2019-04-08 15:12:58,502] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8670
[2019-04-08 15:12:58,562] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 22.5, 22.58579880756428, -0.3972297928993713, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.666666666666667, 30.0], 
sim time this is 2803800.0000, 
sim time next is 2804400.0000, 
raw observation next is [-1.0, 50.0, 149.5, 635.5, 22.5, 22.55824150601543, -0.4061398207396278, 1.0, 1.0, 30.00000000000002, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.5, 0.49833333333333335, 0.7022099447513812, 0.375, 0.37985345883461924, 0.3646200597534574, 1.0, 1.0, 0.30000000000000043, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15425025], dtype=float32), -0.75843096]. 
=============================================
[2019-04-08 15:12:58,608] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1458055e-09 5.2099739e-04 1.2219090e-02 7.1491200e-01 3.4636618e-05
 4.8440387e-07 1.4906945e-03 1.3895360e-08 6.2091464e-11 2.0055521e-11
 2.7082208e-01], sum to 1.0000
[2019-04-08 15:12:58,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7088
[2019-04-08 15:12:58,672] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 19.52605583469212, -1.004096029704457, 0.0, 1.0, 65.0, 196871.0581972766], 
current ob forecast is [], 
actual action is [-2.0, 30.0], 
sim time this is 2788800.0000, 
sim time next is 2789400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 19.3687588600599, -1.023531089040775, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.11406323833832488, 0.15882297031974169, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1796241], dtype=float32), -1.0474355]. 
=============================================
[2019-04-08 15:12:59,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.23608067e-13 2.09243422e-06 1.10320458e-02 9.87367392e-01
 1.75196703e-07 7.30999483e-10 1.77991227e-04 1.09691395e-10
 3.35775041e-14 3.75283135e-16 1.42031070e-03], sum to 1.0000
[2019-04-08 15:12:59,432] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3242
[2019-04-08 15:12:59,484] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.5, 56.5, 159.3333333333333, 324.6666666666666, 22.5, 21.24100567237335, -0.663697058614293, 1.0, 1.0, 30.0, 26339.14437746002], 
current ob forecast is [], 
actual action is [1.5, 30.0], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [-3.0, 55.0, 163.0, 370.5, 22.5, 21.39005666511465, -0.628347890543825, 1.0, 1.0, 30.0, 25463.46455367427], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.55, 0.5433333333333333, 0.4093922651933702, 0.375, 0.2825047220928874, 0.29055070315205833, 1.0, 1.0, 0.3, 0.12125459311273461], 
reward next is 0.8837, 
noisyNet noise sample is [array([-0.55964994], dtype=float32), 1.2655636]. 
=============================================
[2019-04-08 15:13:02,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2420839e-15 1.3210776e-07 1.5456155e-02 9.2620307e-01 4.8730300e-07
 1.0138852e-09 1.2972640e-03 4.2468556e-11 3.3776365e-16 6.3461426e-19
 5.7042971e-02], sum to 1.0000
[2019-04-08 15:13:02,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7862
[2019-04-08 15:13:02,332] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.333333333333333, 38.0, 189.8333333333333, 599.6666666666667, 22.5, 24.14363464736012, -0.001084998716619036, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [8.333333333333332, 30.0], 
sim time this is 2810400.0000, 
sim time next is 2811000.0000, 
raw observation next is [3.666666666666667, 36.5, 201.6666666666667, 514.3333333333334, 22.5, 23.68024885642242, -0.03729602846361676, 1.0, 1.0, 29.99999999999996, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.564173591874423, 0.365, 0.6722222222222224, 0.5683241252302026, 0.375, 0.47335407136853497, 0.48756799051212774, 1.0, 1.0, 0.2999999999999992, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38642526], dtype=float32), -0.6679972]. 
=============================================
[2019-04-08 15:13:02,378] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[92.52594]
 [91.52582]
 [90.87109]
 [90.25365]
 [89.68631]], R is [[93.09025574]
 [93.15935516]
 [93.22776031]
 [93.29548645]
 [93.36253357]].
[2019-04-08 15:13:02,817] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2959164e-13 1.6566606e-04 1.3037333e-01 7.3249203e-01 1.4066815e-04
 1.1147884e-08 1.5446355e-03 3.5748113e-10 1.3436428e-12 1.0495065e-15
 1.3528371e-01], sum to 1.0000
[2019-04-08 15:13:02,817] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9451
[2019-04-08 15:13:02,856] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 22.5, 22.74510242004205, -0.3239015180661743, 1.0, 1.0, 30.0, 29435.38904251022], 
current ob forecast is [], 
actual action is [10.666666666666666, 65.0], 
sim time this is 2814600.0000, 
sim time next is 2815200.0000, 
raw observation next is [6.0, 30.0, 183.5, 86.5, 22.5, 22.801527279132, -0.2961718635583089, 1.0, 1.0, 65.0, 196283.8563884479], 
processed observation next is [1.0, 0.6086956521739131, 0.6288088642659281, 0.3, 0.6116666666666667, 0.09558011049723757, 0.375, 0.40012727326099995, 0.4012760454805637, 1.0, 1.0, 1.0, 0.9346850304211805], 
reward next is 0.0653, 
noisyNet noise sample is [array([-0.24991979], dtype=float32), -0.9233518]. 
=============================================
[2019-04-08 15:13:02,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0023516e-12 3.1113852e-06 1.2086280e-02 6.5648389e-01 1.8695482e-05
 8.0831552e-07 8.9066970e-04 1.4074778e-09 8.9676096e-13 2.5009893e-15
 3.3051661e-01], sum to 1.0000
[2019-04-08 15:13:02,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-08 15:13:02,969] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.666666666666667, 65.33333333333334, 0.0, 0.0, 19.0, 23.16949019545016, -0.1943595468469554, 0.0, 1.0, 65.0, 203189.9163058002], 
current ob forecast is [], 
actual action is [6.666666666666667, 30.0], 
sim time this is 2848800.0000, 
sim time next is 2849400.0000, 
raw observation next is [1.5, 67.0, 0.0, 0.0, 19.0, 23.15988973516892, -0.1868804096676574, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 1.0, 0.5041551246537397, 0.67, 0.0, 0.0, 0.08333333333333333, 0.42999081126407673, 0.4377065301107808, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83674306], dtype=float32), 1.1736379]. 
=============================================
[2019-04-08 15:13:03,304] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4895235e-14 4.3651505e-07 2.4520046e-01 6.9430703e-01 2.0065495e-07
 1.7264173e-10 1.5786717e-03 4.9428235e-12 4.1430846e-15 7.7028845e-17
 5.8913130e-02], sum to 1.0000
[2019-04-08 15:13:03,304] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4318
[2019-04-08 15:13:03,389] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 44.0, 151.5, 724.0, 22.5, 23.26371870846042, -0.2416759463603439, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [7.0, 30.0], 
sim time this is 2808000.0000, 
sim time next is 2808600.0000, 
raw observation next is [2.333333333333333, 42.50000000000001, 160.3333333333333, 711.0, 22.5, 23.29650173028471, -0.2320201635682783, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5272391505078486, 0.42500000000000004, 0.5344444444444443, 0.7856353591160221, 0.375, 0.44137514419039253, 0.42265994547724056, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6396172], dtype=float32), 0.56673527]. 
=============================================
[2019-04-08 15:13:03,538] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.9340584e-10 1.8156142e-04 3.5286810e-02 8.8058251e-01 3.1945394e-05
 3.5667040e-07 7.3352903e-03 9.2347090e-09 1.3634133e-10 1.6411920e-12
 7.6581478e-02], sum to 1.0000
[2019-04-08 15:13:03,539] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3883
[2019-04-08 15:13:03,590] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 21.21391643619232, -0.5678341719791455, 0.0, 1.0, 30.0, 19868.8200774153], 
current ob forecast is [], 
actual action is [-1.0, 30.0], 
sim time this is 2768400.0000, 
sim time next is 2769000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 21.15102943567634, -0.5724188288304254, 0.0, 1.0, 30.0, 19749.23599718157], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.2625857863063616, 0.30919372372319154, 0.0, 1.0, 0.3, 0.09404398093895987], 
reward next is 0.9060, 
noisyNet noise sample is [array([-1.2330422], dtype=float32), -0.119829744]. 
=============================================
[2019-04-08 15:13:03,622] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[58.110897]
 [58.766136]
 [59.24813 ]
 [60.039463]
 [60.67428 ]], R is [[57.81118393]
 [58.13846207]
 [58.46197891]
 [58.78149796]
 [59.09704208]].
[2019-04-08 15:13:03,952] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8179958e-16 9.5504083e-07 3.0884540e-02 9.2482096e-01 8.2202405e-09
 1.2220155e-10 1.3373311e-03 5.2547951e-13 2.5403757e-16 1.9587480e-17
 4.2956170e-02], sum to 1.0000
[2019-04-08 15:13:03,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1411
[2019-04-08 15:13:03,988] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [6.5, 25.5, 79.0, 50.66666666666667, 22.5, 24.18180545462748, 0.02811873435635906, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [11.5, 30.0], 
sim time this is 2823000.0000, 
sim time next is 2823600.0000, 
raw observation next is [6.4, 26.0, 75.0, 63.33333333333334, 22.5, 24.45367265195072, 0.04444367250945144, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.6956521739130435, 0.6398891966759004, 0.26, 0.25, 0.0699815837937385, 0.375, 0.5378060543292266, 0.5148145575031505, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.5567802], dtype=float32), 0.29907995]. 
=============================================
[2019-04-08 15:13:04,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.3828106e-16 1.7206111e-06 1.9582135e-03 1.1377238e-01 2.0107569e-07
 2.6843297e-10 4.8003485e-03 2.0663117e-11 8.4507540e-16 2.4201529e-19
 8.7946713e-01], sum to 1.0000
[2019-04-08 15:13:04,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0676
[2019-04-08 15:13:04,175] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.333333333333333, 29.33333333333334, 0.0, 0.0, 22.5, 24.07521950725187, 0.09083426772958215, 1.0, 1.0, 64.99999999999991, 201086.6453738647], 
current ob forecast is [], 
actual action is [10.333333333333332, 30.0], 
sim time this is 2828400.0000, 
sim time next is 2829000.0000, 
raw observation next is [5.166666666666667, 29.66666666666666, 0.0, 0.0, 22.5, 24.16733087922092, 0.1338944084329714, 1.0, 1.0, 29.99999999999996, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6057248384118191, 0.29666666666666663, 0.0, 0.0, 0.375, 0.5139442399350767, 0.5446314694776572, 1.0, 1.0, 0.2999999999999992, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6640842], dtype=float32), 0.0437373]. 
=============================================
[2019-04-08 15:13:04,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.03971]
 [80.45831]
 [80.05049]
 [79.06072]
 [78.41966]], R is [[80.98327637]
 [80.21588898]
 [79.46377563]
 [78.72701263]
 [78.00432587]].
[2019-04-08 15:13:04,712] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.4773974e-14 2.5391397e-05 1.7260841e-01 1.3132583e-01 1.9550632e-06
 3.7102934e-09 1.2443625e-02 6.4137654e-11 7.5433604e-16 5.9544648e-16
 6.8359482e-01], sum to 1.0000
[2019-04-08 15:13:04,712] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0165
[2019-04-08 15:13:04,786] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8481344e-14 7.2237824e-05 1.0415783e-01 8.7446636e-01 1.8045802e-07
 2.2785851e-09 2.8642692e-04 3.2023784e-10 6.0566998e-15 1.0308208e-16
 2.1016965e-02], sum to 1.0000
[2019-04-08 15:13:04,786] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3103
[2019-04-08 15:13:04,789] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 22.5, 23.77849873635984, 0.03695926791735674, 1.0, 1.0, 65.0, 199886.3581494728], 
current ob forecast is [], 
actual action is [7.833333333333333, 25.0], 
sim time this is 2833800.0000, 
sim time next is 2834400.0000, 
raw observation next is [2.666666666666667, 39.33333333333334, 0.0, 0.0, 22.5, 23.99370542531046, 0.04608901805725211, 0.0, 1.0, 24.99999999999999, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.3933333333333334, 0.0, 0.0, 0.375, 0.4994754521092049, 0.515363006019084, 0.0, 1.0, 0.1999999999999998, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9731783], dtype=float32), 1.1571908]. 
=============================================
[2019-04-08 15:13:04,802] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [5.0, 30.0, 0.0, 0.0, 22.5, 23.77631762218983, -0.06673742773059886, 1.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [10.0, 30.0], 
sim time this is 2829600.0000, 
sim time next is 2830200.0000, 
raw observation next is [4.666666666666667, 31.16666666666666, 0.0, 0.0, 22.5, 23.78059902900241, -0.1090306825039878, 1.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.5918744228993538, 0.3116666666666666, 0.0, 0.0, 0.375, 0.4817165857502008, 0.46365643916533744, 1.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.3942944], dtype=float32), 0.41694814]. 
=============================================
[2019-04-08 15:13:04,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4028984e-11 5.6495675e-05 1.0621341e-01 1.4627483e-01 2.5474848e-04
 5.6687048e-07 2.6450865e-03 7.2341253e-09 1.8898401e-12 2.4306083e-13
 7.4455488e-01], sum to 1.0000
[2019-04-08 15:13:04,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9807
[2019-04-08 15:13:04,910] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 95.33333333333334, 0.0, 0.0, 19.0, 21.09431916450979, -0.5421973808548973, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [1.0, 96.5, 0.0, 0.0, 19.0, 21.45739492048784, -0.5034261168187901, 0.0, 1.0, 65.0, 201041.8927792541], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.965, 0.0, 0.0, 0.08333333333333333, 0.2881162433739866, 0.3321912943937366, 0.0, 1.0, 1.0, 0.9573423465678766], 
reward next is 0.0427, 
noisyNet noise sample is [array([-1.6308439], dtype=float32), 0.30783775]. 
=============================================
[2019-04-08 15:13:04,950] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.54567 ]
 [65.29072 ]
 [64.54195 ]
 [63.574093]
 [62.664577]], R is [[66.092453  ]
 [66.43152618]
 [65.81596375]
 [65.21463776]
 [64.62536621]].
[2019-04-08 15:13:06,837] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.6247856e-14 2.7336673e-06 2.1538995e-02 9.6503055e-01 8.4720119e-07
 3.4694372e-09 2.8278378e-03 6.3775596e-13 9.2331483e-16 8.1225252e-17
 1.0599039e-02], sum to 1.0000
[2019-04-08 15:13:06,848] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7912
[2019-04-08 15:13:06,913] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 22.5, 22.64218608931532, -0.3618804356994887, 1.0, 1.0, 30.0, 24742.77670538444], 
current ob forecast is [], 
actual action is [5.666666666666667, 30.0], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 81.0, 0.0, 22.5, 22.69066312806373, -0.352903047235837, 1.0, 1.0, 30.0, 24285.58804817586], 
processed observation next is [1.0, 0.43478260869565216, 0.4856879039704525, 0.9416666666666665, 0.27, 0.0, 0.375, 0.3908885940053108, 0.3823656509213877, 1.0, 1.0, 0.3, 0.11564565737226601], 
reward next is 0.8844, 
noisyNet noise sample is [array([2.3831108], dtype=float32), -1.1160046]. 
=============================================
[2019-04-08 15:13:07,073] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6439102e-15 1.6959909e-06 1.6241944e-03 9.3768960e-01 1.6703723e-06
 2.0808119e-11 4.1204607e-03 1.1250581e-11 2.3999074e-14 4.2242876e-17
 5.6562379e-02], sum to 1.0000
[2019-04-08 15:13:07,077] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7361
[2019-04-08 15:13:07,145] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 23.65712406807847, 0.06326286335576776, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [7.0, 30.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 23.85436228327856, 0.06801537788475115, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.4878635236065468, 0.5226717926282504, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3225632], dtype=float32), -0.28308615]. 
=============================================
[2019-04-08 15:13:10,020] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.01237701e-13 1.28270485e-05 6.63224995e-01 3.01768064e-01
 1.48679765e-05 4.15636769e-09 1.67897390e-03 3.25715205e-10
 1.03133444e-14 6.30143650e-16 3.33002470e-02], sum to 1.0000
[2019-04-08 15:13:10,021] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6451
[2019-04-08 15:13:10,083] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [4.666666666666667, 31.16666666666666, 0.0, 0.0, 22.5, 24.07961894873269, -0.05042434146647479, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [9.666666666666668, 25.0], 
sim time this is 2830200.0000, 
sim time next is 2830800.0000, 
raw observation next is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 22.5, 23.8947636809434, -0.08674784590632771, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 0.3233333333333334, 0.0, 0.0, 0.375, 0.49123030674528323, 0.4710840513645574, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6612346], dtype=float32), -1.3148621]. 
=============================================
[2019-04-08 15:13:10,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1261953e-16 4.4977380e-08 4.0241433e-03 9.8452133e-01 2.3858803e-08
 2.0892668e-11 3.1938544e-04 9.9302836e-13 3.7481961e-18 8.1651283e-20
 1.1135116e-02], sum to 1.0000
[2019-04-08 15:13:10,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8798
[2019-04-08 15:13:10,247] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 100.0, 86.66666666666666, 0.0, 22.5, 22.95895699720963, -0.2375132658368748, 1.0, 1.0, 29.99999999999999, 18680.41167023054], 
current ob forecast is [], 
actual action is [7.0, 30.0], 
sim time this is 2905800.0000, 
sim time next is 2906400.0000, 
raw observation next is [2.0, 100.0, 85.83333333333334, 0.0, 22.5, 22.68425804328616, -0.2482380716668584, 1.0, 1.0, 30.00000000000001, 18680.41167023054], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.28611111111111115, 0.0, 0.375, 0.39035483694051337, 0.4172539761110472, 1.0, 1.0, 0.3000000000000002, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.22760485], dtype=float32), 1.2899073]. 
=============================================
[2019-04-08 15:13:12,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1239279e-15 2.1146585e-07 1.3384647e-02 9.8169625e-01 1.0808650e-08
 1.3585137e-09 1.3981253e-03 6.5211942e-13 4.1057642e-17 2.0117572e-18
 3.5207206e-03], sum to 1.0000
[2019-04-08 15:13:12,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1251
[2019-04-08 15:13:12,267] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 93.0, 82.5, 0.0, 22.5, 23.84022018378319, -0.1171350060062532, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [6.0, 30.0], 
sim time this is 2890800.0000, 
sim time next is 2891400.0000, 
raw observation next is [1.0, 94.16666666666666, 84.0, 0.0, 22.5, 23.89280224105237, -0.1195092328726845, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.9416666666666665, 0.28, 0.0, 0.375, 0.49106685342103074, 0.4601635890424385, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31577998], dtype=float32), 0.6733868]. 
=============================================
[2019-04-08 15:13:12,459] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4246062e-11 7.7441291e-06 6.1459695e-03 9.7828305e-01 3.6590691e-06
 1.6085063e-07 1.7076004e-03 1.3640057e-08 2.5018768e-11 1.2343716e-13
 1.3851847e-02], sum to 1.0000
[2019-04-08 15:13:12,459] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7304
[2019-04-08 15:13:12,470] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 19.0, 20.64439064246009, -0.6332704932962311, 0.0, 1.0, 30.0, 19847.06499817003], 
current ob forecast is [], 
actual action is [3.0, 30.0], 
sim time this is 2941800.0000, 
sim time next is 2942400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 20.58553816408508, -0.6473237098126358, 0.0, 1.0, 30.0, 19913.78556030672], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.21546151367375663, 0.28422543006245476, 0.0, 1.0, 0.3, 0.09482755028717485], 
reward next is 0.9052, 
noisyNet noise sample is [array([0.99171823], dtype=float32), 0.58609074]. 
=============================================
[2019-04-08 15:13:13,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5326330e-10 2.9483415e-06 1.5224369e-02 9.5258498e-01 5.7842248e-05
 1.4838494e-07 6.4804410e-03 6.8106991e-09 3.5022818e-11 2.7264443e-12
 2.5649395e-02], sum to 1.0000
[2019-04-08 15:13:13,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.83219115e-09 4.79235168e-05 7.87276868e-03 2.55851090e-01
 1.06137675e-04 4.70922669e-06 1.16625037e-02 5.73444140e-07
 2.21798815e-10 8.81882501e-12 7.24454284e-01], sum to 1.0000
[2019-04-08 15:13:13,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6267
[2019-04-08 15:13:13,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7058
[2019-04-08 15:13:13,480] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 19.72397736555365, -0.8052185181729675, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 2960400.0000, 
sim time next is 2961000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 19.80807088374885, -0.7877884314348628, 0.0, 1.0, 65.0, 200727.2807666545], 
processed observation next is [0.0, 0.2608695652173913, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.15067257364573758, 0.2374038561883791, 0.0, 1.0, 1.0, 0.9558441941269262], 
reward next is 0.0442, 
noisyNet noise sample is [array([-0.8005105], dtype=float32), -0.7471886]. 
=============================================
[2019-04-08 15:13:13,499] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 20.82234056603381, -0.628713084483944, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.0, 30.0], 
sim time this is 2941200.0000, 
sim time next is 2941800.0000, 
raw observation next is [-2.0, 85.00000000000001, 0.0, 0.0, 19.0, 20.72242605398644, -0.6503703471105637, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.8500000000000001, 0.0, 0.0, 0.08333333333333333, 0.22686883783220324, 0.28320988429647875, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.6325776], dtype=float32), -0.46915066]. 
=============================================
[2019-04-08 15:13:13,516] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[50.21422 ]
 [50.13841 ]
 [49.281754]
 [48.73812 ]
 [48.00616 ]], R is [[50.47327423]
 [50.96854019]
 [50.5085907 ]
 [50.18288803]
 [49.74430466]].
[2019-04-08 15:13:14,106] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9891512e-15 1.5097432e-06 4.1621299e-03 4.1566798e-01 5.9081873e-07
 7.0810253e-11 3.2150777e-04 5.5369305e-13 3.7030034e-17 1.9226974e-18
 5.7984626e-01], sum to 1.0000
[2019-04-08 15:13:14,109] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0435
[2019-04-08 15:13:14,159] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 83.83333333333334, 0.0, 0.0, 19.0, 25.25191854256806, 0.3928599396052337, 0.0, 1.0, 65.0, 126655.0944941337], 
current ob forecast is [], 
actual action is [4.0, 30.0], 
sim time this is 2927400.0000, 
sim time next is 2928000.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 19.0, 25.339412268365, 0.3993641396697736, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6116176890304166, 0.6331213798899246, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.600845], dtype=float32), 1.1564006]. 
=============================================
[2019-04-08 15:13:14,184] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[89.93944 ]
 [90.57063 ]
 [90.467735]
 [93.05493 ]
 [93.18701 ]], R is [[90.45760345]
 [89.94991302]
 [89.13150024]
 [89.2401886 ]
 [88.38050079]].
[2019-04-08 15:13:14,616] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.04475314e-09 2.50843324e-04 1.10443309e-01 6.59569860e-01
 4.12381523e-05 4.81525240e-06 1.07885554e-01 6.47237641e-07
 4.26041113e-09 4.67975991e-10 1.21803723e-01], sum to 1.0000
[2019-04-08 15:13:14,616] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0997
[2019-04-08 15:13:14,621] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 78423: loss 5.2084
[2019-04-08 15:13:14,625] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 19.92696771646941, -0.7725567290104992, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [2.0, 25.0], 
sim time this is 2950200.0000, 
sim time next is 2950800.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 19.86456328777568, -0.7892150389036395, 0.0, 1.0, 25.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.15538027398130674, 0.2369283203654535, 0.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09235267], dtype=float32), 1.2682173]. 
=============================================
[2019-04-08 15:13:14,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 78423: learning rate 0.0000
[2019-04-08 15:13:15,977] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3693598e-14 7.2813748e-07 2.2060262e-01 6.7567712e-01 7.8962138e-08
 1.1275596e-09 1.6807859e-04 1.5946256e-11 3.1418565e-15 8.3860527e-19
 1.0355137e-01], sum to 1.0000
[2019-04-08 15:13:15,980] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0349
[2019-04-08 15:13:15,991] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [2.0, 100.0, 127.0, 0.0, 22.5, 22.72948384426766, -0.2658400143622259, 1.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [7.0, 25.0], 
sim time this is 2901600.0000, 
sim time next is 2902200.0000, 
raw observation next is [2.0, 100.0, 114.6666666666667, 0.0, 22.5, 22.93875407623374, -0.2552193222195072, 1.0, 1.0, 25.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.38222222222222235, 0.0, 0.375, 0.4115628396861449, 0.41492689259349763, 1.0, 1.0, 0.2, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1300586], dtype=float32), 1.4367296]. 
=============================================
[2019-04-08 15:13:16,171] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5000, global step 78926: loss 4.5911
[2019-04-08 15:13:16,171] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5000, global step 78926: learning rate 0.0000
[2019-04-08 15:13:16,705] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79069: loss -1.3881
[2019-04-08 15:13:16,706] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79069: learning rate 0.0000
[2019-04-08 15:13:17,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9505646e-14 9.8934424e-07 1.3286186e-03 9.9202567e-01 3.4590484e-09
 1.1915066e-09 5.2673044e-04 1.3884769e-12 3.1014776e-15 1.7392189e-17
 6.1179791e-03], sum to 1.0000
[2019-04-08 15:13:17,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5662
[2019-04-08 15:13:17,101] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 79.16666666666666, 0.0, 0.0, 19.0, 23.02402327760432, -0.07315160389208397, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.0, 30.0], 
sim time this is 2929800.0000, 
sim time next is 2930400.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 19.0, 22.97461040871363, -0.08669096529218144, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.78, 0.0, 0.0, 0.08333333333333333, 0.4145508673928025, 0.47110301156927287, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28996435], dtype=float32), 0.89147335]. 
=============================================
[2019-04-08 15:13:17,332] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 79283: loss 0.2132
[2019-04-08 15:13:17,333] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 79283: learning rate 0.0000
[2019-04-08 15:13:18,012] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 79520: loss 0.1127
[2019-04-08 15:13:18,013] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 79520: learning rate 0.0000
[2019-04-08 15:13:18,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5054865e-12 2.0931131e-05 9.1718040e-02 6.4291918e-01 2.4649510e-06
 1.2313993e-07 4.4747186e-03 5.0261351e-10 1.7185485e-13 9.0757192e-15
 2.6086459e-01], sum to 1.0000
[2019-04-08 15:13:18,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6269
[2019-04-08 15:13:18,151] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 19.0, 25.37401870513776, 0.3526078060661036, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.0, 30.0], 
sim time this is 2941800.0000, 
sim time next is 2942400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 25.23919796508553, 0.3237050342783548, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6032664970904609, 0.607901678092785, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50226367], dtype=float32), 0.6683387]. 
=============================================
[2019-04-08 15:13:18,166] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79543: loss 0.0470
[2019-04-08 15:13:18,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79543: learning rate 0.0000
[2019-04-08 15:13:18,293] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79555: loss -0.0423
[2019-04-08 15:13:18,295] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79555: learning rate 0.0000
[2019-04-08 15:13:18,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3188323e-11 8.6770897e-06 1.5767833e-02 8.8017362e-01 1.3904053e-06
 2.1928125e-07 4.7375867e-03 2.4292899e-09 3.8777111e-12 4.7210910e-15
 9.9310622e-02], sum to 1.0000
[2019-04-08 15:13:18,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7101
[2019-04-08 15:13:18,544] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.666666666666667, 63.33333333333334, 121.8333333333333, 781.8333333333334, 19.0, 21.4094279770578, -0.4549604934731477, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.333333333333333, 30.0], 
sim time this is 2985600.0000, 
sim time next is 2986200.0000, 
raw observation next is [-2.5, 62.5, 110.0, 800.0, 19.0, 21.38542360918515, -0.4620560982835105, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.39335180055401664, 0.625, 0.36666666666666664, 0.8839779005524862, 0.08333333333333333, 0.2821186340987625, 0.3459813005721632, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7308252], dtype=float32), 0.30706376]. 
=============================================
[2019-04-08 15:13:18,711] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.8416910e-09 1.4454035e-04 2.4156235e-02 9.2438364e-01 1.2435572e-04
 3.6727624e-07 1.9705922e-03 8.1659408e-08 1.7946191e-09 7.6138601e-12
 4.9220055e-02], sum to 1.0000
[2019-04-08 15:13:18,712] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2077
[2019-04-08 15:13:18,734] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 19.0, 19.75861141724351, -0.9399245319159587, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.3333333333333339, 30.0], 
sim time this is 3026400.0000, 
sim time next is 3027000.0000, 
raw observation next is [-4.833333333333334, 70.0, 0.0, 0.0, 19.0, 19.64903271131726, -0.960995771290403, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.0, 0.32871652816251157, 0.7, 0.0, 0.0, 0.08333333333333333, 0.13741939260977176, 0.17966807623653233, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0763927], dtype=float32), -0.4260484]. 
=============================================
[2019-04-08 15:13:18,772] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[50.000504]
 [50.71377 ]
 [51.276924]
 [52.976643]
 [53.573586]], R is [[50.35014725]
 [50.84664536]
 [51.33818054]
 [51.82479858]
 [52.30655289]].
[2019-04-08 15:13:19,009] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.2511335e-08 4.3105686e-04 2.1376973e-02 6.6744620e-01 1.2472353e-04
 1.2256354e-05 1.4464611e-02 1.0750856e-06 1.0311809e-08 3.1225933e-09
 2.9614300e-01], sum to 1.0000
[2019-04-08 15:13:19,010] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4504
[2019-04-08 15:13:19,018] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79814: loss -1.0783
[2019-04-08 15:13:19,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79814: learning rate 0.0000
[2019-04-08 15:13:19,158] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.333333333333334, 73.0, 0.0, 0.0, 19.0, 18.78919133047887, -1.099682593851841, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-0.3333333333333339, 30.0], 
sim time this is 3032400.0000, 
sim time next is 3033000.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 19.0, 18.76778431637185, -1.108569594973083, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.3102493074792244, 0.74, 0.0, 0.0, 0.08333333333333333, 0.06398202636432071, 0.13047680167563902, 0.0, 1.0, 0.3, 0.0], 
reward next is 0.7843, 
noisyNet noise sample is [array([-1.0934799], dtype=float32), 1.6544156]. 
=============================================
[2019-04-08 15:13:19,190] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[44.334373]
 [44.730843]
 [44.185463]
 [44.6914  ]
 [46.205215]], R is [[44.11231232]
 [44.49342728]
 [44.30743408]
 [44.76455307]
 [45.22795486]].
[2019-04-08 15:13:19,506] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79968: loss 10.9673
[2019-04-08 15:13:19,507] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79969: learning rate 0.0000
[2019-04-08 15:13:19,696] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-08 15:13:19,697] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:13:19,698] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:13:19,698] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:13:19,700] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:13:19,700] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:13:19,720] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run5
[2019-04-08 15:13:19,720] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:13:19,751] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run5
[2019-04-08 15:13:19,771] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 80000: loss -1.6926
[2019-04-08 15:13:19,773] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run5
[2019-04-08 15:13:19,794] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 80000: learning rate 0.0000
[2019-04-08 15:15:12,023] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 7067.4313 227349107.0741 682.0270
[2019-04-08 15:15:12,044] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:12,044] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:12,044] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:12,044] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:12,044] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:12,148] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:12,148] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:12,148] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:12,148] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:12,148] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:33,570] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6653.7761 234588528.6771 -183.9443
[2019-04-08 15:15:33,603] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:33,603] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:33,603] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:33,603] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:33,603] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:33,782] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:33,782] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:33,782] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:33,782] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:33,782] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:36,741] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6485.6710 238345575.5725 -795.1771
[2019-04-08 15:15:36,777] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:36,777] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:36,777] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:36,777] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:36,777] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:15:36,933] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:36,933] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:36,933] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:36,933] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:36,933] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:15:37,780] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 80000, evaluation results [80000.0, 6653.776127669027, 234588528.67713654, -183.94427143188537, 7067.431317861789, 227349107.07408455, 682.0269941462133, 6485.670984728158, 238345575.57254726, -795.1771384437674]
[2019-04-08 15:15:37,894] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 80060: loss 0.2191
[2019-04-08 15:15:37,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 80060: learning rate 0.0000
[2019-04-08 15:15:38,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7237807e-11 2.9328234e-05 1.7897165e-03 9.4769549e-01 2.8317791e-06
 6.1557273e-08 2.2366062e-02 5.0829909e-09 1.9280440e-11 3.4647052e-13
 2.8116535e-02], sum to 1.0000
[2019-04-08 15:15:38,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8970
[2019-04-08 15:15:38,266] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 21.31477277054622, -0.5735791999136378, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.0, 30.0], 
sim time this is 3022200.0000, 
sim time next is 3022800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 21.34801031821292, -0.5818992417494341, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.27900085985107675, 0.306033586083522, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5827837], dtype=float32), -0.14580767]. 
=============================================
[2019-04-08 15:15:38,425] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5896629e-16 9.9956708e-08 4.0100459e-03 9.2763811e-01 3.7568299e-08
 3.4323536e-10 2.1056812e-02 1.4001522e-13 6.6371437e-17 6.8753546e-20
 4.7294945e-02], sum to 1.0000
[2019-04-08 15:15:38,425] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4205
[2019-04-08 15:15:38,503] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.0, 92.5, 0.0, 0.0, 22.5, 25.48537196898852, 0.3275040436053747, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.0, 30.0], 
sim time this is 2917800.0000, 
sim time next is 2918400.0000, 
raw observation next is [-0.3333333333333333, 92.33333333333333, 0.0, 0.0, 22.5, 25.29090263727418, 0.3059403575795532, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4533702677747, 0.9233333333333333, 0.0, 0.0, 0.375, 0.6075752197728482, 0.6019801191931844, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-7.68455e-05], dtype=float32), -2.0359247]. 
=============================================
[2019-04-08 15:15:38,506] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80223: loss 2.5374
[2019-04-08 15:15:38,507] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80223: learning rate 0.0000
[2019-04-08 15:15:40,066] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80763: loss -3.1600
[2019-04-08 15:15:40,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80765: learning rate 0.0000
[2019-04-08 15:15:40,423] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6687049e-10 3.3247475e-06 1.5162389e-02 9.5556247e-01 8.2641245e-06
 1.5128451e-07 9.0220873e-04 1.5598864e-08 3.7754327e-11 4.2611119e-12
 2.8361104e-02], sum to 1.0000
[2019-04-08 15:15:40,423] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9212
[2019-04-08 15:15:40,451] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 21.8275314794247, -0.4564489745541608, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.0, 30.0], 
sim time this is 3038400.0000, 
sim time next is 3039000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 21.82682235994399, -0.4682225976637948, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.31890186332866577, 0.3439258007787351, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3134353], dtype=float32), 1.0846263]. 
=============================================
[2019-04-08 15:15:40,481] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[54.112988]
 [54.032925]
 [53.61127 ]
 [54.21037 ]
 [54.09245 ]], R is [[54.04590607]
 [54.50544739]
 [54.960392  ]
 [55.41078949]
 [55.85668182]].
[2019-04-08 15:15:40,669] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.9306737e-10 4.3916963e-05 1.2661537e-02 5.9834886e-01 1.1861833e-06
 4.1058269e-07 1.8594923e-03 6.2051551e-09 5.8303518e-12 2.3348733e-13
 3.8708460e-01], sum to 1.0000
[2019-04-08 15:15:40,669] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7027
[2019-04-08 15:15:40,688] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666667, 39.16666666666666, 89.0, 707.0, 19.0, 20.35121011941901, -0.7132346991213808, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 3078600.0000, 
sim time next is 3079200.0000, 
raw observation next is [0.3333333333333333, 39.33333333333334, 86.5, 690.0, 19.0, 20.42354901986091, -0.6839724716955303, 0.0, 1.0, 65.0, 200192.0153280917], 
processed observation next is [0.0, 0.6521739130434783, 0.4718374884579871, 0.3933333333333334, 0.28833333333333333, 0.7624309392265194, 0.08333333333333333, 0.20196241832174255, 0.2720091761014899, 0.0, 1.0, 1.0, 0.9532953110861508], 
reward next is 0.0467, 
noisyNet noise sample is [array([-0.7417229], dtype=float32), 0.4752981]. 
=============================================
[2019-04-08 15:15:41,105] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 81120: loss -4.1286
[2019-04-08 15:15:41,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 81120: learning rate 0.0000
[2019-04-08 15:15:41,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5970995e-12 1.2907063e-05 1.8507084e-02 9.6559483e-01 1.4770084e-06
 3.6093354e-08 1.3947034e-04 7.6768067e-09 6.7143383e-12 7.9708355e-14
 1.5744250e-02], sum to 1.0000
[2019-04-08 15:15:41,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2246
[2019-04-08 15:15:41,522] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.666666666666667, 54.33333333333334, 110.1666666666667, 797.3333333333334, 19.0, 20.81335812316769, -0.6673721995356575, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.333333333333333, 30.0], 
sim time this is 3064800.0000, 
sim time next is 3065400.0000, 
raw observation next is [-3.5, 54.5, 111.0, 805.0, 19.0, 20.75451498207098, -0.6771658574011635, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.37, 0.8895027624309392, 0.08333333333333333, 0.22954291517258163, 0.2742780475329455, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19805823], dtype=float32), 0.28062612]. 
=============================================
[2019-04-08 15:15:41,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.60296272e-11 2.83151458e-05 1.21692605e-02 5.82691550e-01
 2.48498145e-05 3.17787432e-08 1.28535945e-02 3.39490858e-09
 3.54214639e-12 2.03043881e-13 3.92232388e-01], sum to 1.0000
[2019-04-08 15:15:41,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3822
[2019-04-08 15:15:41,697] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 54.0, 106.8333333333333, 766.6666666666666, 19.0, 21.52050918723324, -0.4070559520475794, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.0, 30.0], 
sim time this is 3062400.0000, 
sim time next is 3063000.0000, 
raw observation next is [-4.0, 54.0, 107.6666666666667, 774.3333333333333, 19.0, 21.59576195432824, -0.3920945208036195, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.358888888888889, 0.8556169429097605, 0.08333333333333333, 0.2996468295273533, 0.3693018263987935, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8356063], dtype=float32), 1.5620052]. 
=============================================
[2019-04-08 15:15:41,707] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[57.47775 ]
 [56.93771 ]
 [55.708576]
 [56.099876]
 [56.64497 ]], R is [[58.01631546]
 [58.43615341]
 [57.90634537]
 [58.32728195]
 [58.74401093]].
[2019-04-08 15:15:41,833] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.79905664e-11 4.75748766e-06 1.33000752e-02 9.40392613e-01
 5.16912633e-06 1.05847434e-07 1.79138989e-03 5.15796916e-09
 1.34889643e-11 1.11781526e-12 4.45058718e-02], sum to 1.0000
[2019-04-08 15:15:41,850] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9335
[2019-04-08 15:15:41,864] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 19.0, 20.26438332132467, -0.829252766896344, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.0, 30.0], 
sim time this is 3093000.0000, 
sim time next is 3093600.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 19.0, 20.1993141177788, -0.8445072224218476, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.08333333333333333, 0.18327617648156677, 0.2184975925260508, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9392368], dtype=float32), 2.15789]. 
=============================================
[2019-04-08 15:15:42,173] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2290091e-10 8.5396032e-06 1.1665683e-02 9.7637343e-01 2.2012691e-06
 2.4519503e-07 9.2845736e-04 1.0809755e-08 4.2301436e-12 3.5225905e-13
 1.1021509e-02], sum to 1.0000
[2019-04-08 15:15:42,173] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7934
[2019-04-08 15:15:42,197] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 62.33333333333334, 69.33333333333334, 310.8333333333334, 19.0, 22.15706920488972, -0.3526928219788281, 0.0, 1.0, 65.0, 203381.6584959418], 
current ob forecast is [], 
actual action is [-1.0, 30.0], 
sim time this is 3054000.0000, 
sim time next is 3054600.0000, 
raw observation next is [-6.0, 61.5, 83.0, 359.0, 19.0, 22.2172837177204, -0.3100857563495902, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.615, 0.27666666666666667, 0.3966850828729282, 0.08333333333333333, 0.3514403098100332, 0.3966380812168033, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19331184], dtype=float32), -1.008117]. 
=============================================
[2019-04-08 15:15:42,290] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5000, global step 81609: loss 0.0476
[2019-04-08 15:15:42,293] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5000, global step 81609: learning rate 0.0000
[2019-04-08 15:15:42,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.37778788e-11 2.18205041e-05 1.53614385e-02 7.74113357e-01
 4.78766333e-06 1.35732762e-06 1.89605507e-03 1.51668686e-08
 1.46514207e-11 1.88703588e-12 2.08601281e-01], sum to 1.0000
[2019-04-08 15:15:42,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9344
[2019-04-08 15:15:42,760] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.2, 75.33333333333334, 15.33333333333333, 154.3333333333333, 19.0, 21.15587725077484, -0.6144872333759371, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.8, 65.0], 
sim time this is 3086400.0000, 
sim time next is 3087000.0000, 
raw observation next is [-0.3, 77.0, 7.0, 88.0, 19.0, 21.08108789340721, -0.6192779349125224, 0.0, 1.0, 65.0, 197933.7221363404], 
processed observation next is [0.0, 0.7391304347826086, 0.4542936288088643, 0.77, 0.023333333333333334, 0.09723756906077348, 0.08333333333333333, 0.25675732445060095, 0.29357402169582586, 0.0, 1.0, 1.0, 0.9425415339825733], 
reward next is 0.0575, 
noisyNet noise sample is [array([-0.8129191], dtype=float32), -1.8495876]. 
=============================================
[2019-04-08 15:15:42,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[62.358395]
 [62.9196  ]
 [62.963474]
 [63.311817]
 [63.968323]], R is [[61.83434677]
 [62.21600342]
 [62.59384537]
 [62.96790695]
 [63.33823013]].
[2019-04-08 15:15:43,028] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3236841e-10 2.6265887e-05 1.4505181e-01 6.1694276e-01 3.3995187e-07
 1.9311035e-07 2.4728701e-04 1.5734361e-08 2.2262480e-11 2.3134329e-13
 2.3773129e-01], sum to 1.0000
[2019-04-08 15:15:43,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7477
[2019-04-08 15:15:43,045] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-0.8, 87.0, 0.0, 0.0, 19.0, 20.87860233667439, -0.6452170547370212, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [4.2, 30.0], 
sim time this is 3090600.0000, 
sim time next is 3091200.0000, 
raw observation next is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 19.0, 20.88495444975148, -0.6495220949843927, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.4385964912280702, 0.8866666666666666, 0.0, 0.0, 0.08333333333333333, 0.24041287081262327, 0.28349263500520244, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1267079], dtype=float32), -0.43309984]. 
=============================================
[2019-04-08 15:15:43,540] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5494489e-11 3.5756993e-06 6.7310855e-03 9.5887417e-01 1.6052497e-06
 6.6234080e-08 4.2101252e-03 2.0437187e-09 5.2253644e-12 2.0821667e-14
 3.0179378e-02], sum to 1.0000
[2019-04-08 15:15:43,541] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9517
[2019-04-08 15:15:43,561] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.733333333333333, 100.0, 0.0, 0.0, 19.0, 18.80639922658982, -1.146527214611747, 0.0, 1.0, 30.0, 26313.607352979], 
current ob forecast is [], 
actual action is [7.7333333333333325, 30.0], 
sim time this is 3126000.0000, 
sim time next is 3126600.0000, 
raw observation next is [2.8, 100.0, 0.0, 0.0, 19.0, 18.828740295416, -1.146550517480563, 0.0, 1.0, 30.0, 26261.31149552229], 
processed observation next is [1.0, 0.17391304347826086, 0.5401662049861496, 1.0, 0.0, 0.0, 0.08333333333333333, 0.06906169128466673, 0.11781649417314566, 0.0, 1.0, 0.3, 0.12505386426439186], 
reward next is 0.8770, 
noisyNet noise sample is [array([0.5620217], dtype=float32), 0.28957]. 
=============================================
[2019-04-08 15:15:44,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.03774824e-13 1.04457882e-07 8.50860476e-02 3.30673873e-01
 1.57470811e-06 3.70602060e-09 1.24610006e-03 4.46668091e-10
 1.47199798e-13 3.68469496e-15 5.82992315e-01], sum to 1.0000
[2019-04-08 15:15:44,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7970
[2019-04-08 15:15:44,212] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 98.66666666666667, 0.0, 0.0, 19.0, 24.84063895894676, 0.1863412445258151, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3099000.0000, 
sim time next is 3099600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 24.81582214104004, 0.1891304445186477, 0.0, 1.0, 65.0, 155571.6859138821], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5679851784200034, 0.5630434815062159, 0.0, 1.0, 1.0, 0.7408175519708672], 
reward next is 0.2592, 
noisyNet noise sample is [array([1.2511159], dtype=float32), -1.0571424]. 
=============================================
[2019-04-08 15:15:44,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9186709e-12 5.5018559e-06 3.8741806e-03 9.7642142e-01 2.4945350e-06
 7.1338498e-08 1.6081549e-03 4.3040633e-09 1.1030481e-12 1.1091440e-14
 1.8088242e-02], sum to 1.0000
[2019-04-08 15:15:44,581] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5705
[2019-04-08 15:15:44,598] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.166666666666667, 54.83333333333334, 112.0, 809.0, 19.0, 20.55595976316712, -0.6528163821891443, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.833333333333333, 30.0], 
sim time this is 3066600.0000, 
sim time next is 3067200.0000, 
raw observation next is [-3.0, 55.0, 112.5, 811.0, 19.0, 20.51153637737701, -0.6588682608389288, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.375, 0.8961325966850828, 0.08333333333333333, 0.20929469811475077, 0.2803772463870237, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9015265], dtype=float32), -0.708156]. 
=============================================
[2019-04-08 15:15:44,600] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 82513: loss 3.3934
[2019-04-08 15:15:44,608] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 82515: learning rate 0.0000
[2019-04-08 15:15:45,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5691611e-11 1.7742217e-05 5.1571187e-02 3.3533782e-01 8.6477976e-06
 2.2698515e-08 6.5043885e-03 5.2034350e-09 1.1987021e-11 2.1329287e-13
 6.0656023e-01], sum to 1.0000
[2019-04-08 15:15:45,387] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6791
[2019-04-08 15:15:45,409] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 39.5, 84.0, 673.0, 19.0, 20.93690143597616, -0.4897809287873264, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 3079800.0000, 
sim time next is 3080400.0000, 
raw observation next is [0.6666666666666666, 39.66666666666666, 79.5, 641.8333333333334, 19.0, 21.22069476258205, -0.4273375186487831, 0.0, 1.0, 65.0, 200121.5183257205], 
processed observation next is [0.0, 0.6521739130434783, 0.4810710987996307, 0.39666666666666656, 0.265, 0.7092081031307551, 0.08333333333333333, 0.26839123021517075, 0.3575541604504056, 0.0, 1.0, 1.0, 0.9529596110748596], 
reward next is 0.0470, 
noisyNet noise sample is [array([-0.9168601], dtype=float32), 0.51276726]. 
=============================================
[2019-04-08 15:15:46,015] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5034013e-14 1.9949704e-08 3.2723675e-04 9.9378186e-01 1.7841175e-07
 1.5647966e-10 3.8765105e-05 7.9055312e-13 1.2894296e-14 7.2366802e-17
 5.8519784e-03], sum to 1.0000
[2019-04-08 15:15:46,015] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1000
[2019-04-08 15:15:46,034] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 100.0, 0.0, 0.0, 19.0, 23.30454835716198, -0.1568253325686287, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [7.5, 30.0], 
sim time this is 3124200.0000, 
sim time next is 3124800.0000, 
raw observation next is [2.6, 100.0, 0.0, 0.0, 19.0, 23.1667405279492, -0.1775273673974302, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5346260387811635, 1.0, 0.0, 0.0, 0.08333333333333333, 0.4305617106624335, 0.44082421086752327, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13681753], dtype=float32), 0.07988282]. 
=============================================
[2019-04-08 15:15:46,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8232815e-12 8.2018587e-06 1.9818597e-02 4.5396751e-01 6.1327785e-07
 1.6216612e-08 7.0231170e-03 2.9960032e-09 1.2970027e-13 1.7446748e-14
 5.1918191e-01], sum to 1.0000
[2019-04-08 15:15:46,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8550
[2019-04-08 15:15:46,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 22.33806985334174, -0.3038807362043131, 0.0, 1.0, 65.0, 198991.4697930787], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 22.29539728399958, -0.2864382185310323, 0.0, 1.0, 65.0, 200386.310040901], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.3579497736666317, 0.4045205938229892, 0.0, 1.0, 1.0, 0.9542205240042905], 
reward next is 0.0458, 
noisyNet noise sample is [array([-0.91907984], dtype=float32), 0.9401393]. 
=============================================
[2019-04-08 15:15:46,446] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5138613e-13 3.0449364e-06 1.6226673e-02 9.5016813e-01 6.1331542e-07
 7.9815182e-10 2.3346627e-04 3.8501088e-11 3.1294532e-15 1.0363158e-15
 3.3367988e-02], sum to 1.0000
[2019-04-08 15:15:46,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3286
[2019-04-08 15:15:46,463] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 100.0, 0.0, 0.0, 19.0, 24.5080531433705, 0.05487600347612148, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [7.0, 30.0], 
sim time this is 3121200.0000, 
sim time next is 3121800.0000, 
raw observation next is [2.1, 100.0, 0.0, 0.0, 19.0, 24.4066259668691, 0.04163343815119133, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5207756232686982, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5338854972390917, 0.5138778127170638, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2880737], dtype=float32), 0.16658758]. 
=============================================
[2019-04-08 15:15:46,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9714369e-12 4.5493005e-05 3.4953479e-02 5.1204956e-01 2.3510468e-06
 3.5963981e-07 1.0730689e-03 1.1608994e-08 4.2829011e-11 7.7983654e-14
 4.5187569e-01], sum to 1.0000
[2019-04-08 15:15:46,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9855
[2019-04-08 15:15:46,552] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 96.0, 0.0, 0.0, 19.0, 21.10935442774554, -0.6270019088837344, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3097800.0000, 
sim time next is 3098400.0000, 
raw observation next is [-1.0, 97.33333333333333, 0.0, 0.0, 19.0, 21.01201471630549, -0.6271804758465702, 0.0, 1.0, 65.0, 196947.1029869543], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.9733333333333333, 0.0, 0.0, 0.08333333333333333, 0.25100122635879085, 0.2909398413844766, 0.0, 1.0, 1.0, 0.9378433475569252], 
reward next is 0.0622, 
noisyNet noise sample is [array([0.10584386], dtype=float32), 1.1696504]. 
=============================================
[2019-04-08 15:15:46,943] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4880751e-10 7.4685668e-05 3.0482376e-02 8.0341738e-01 8.1062053e-06
 1.1305536e-07 2.8047594e-03 8.1437470e-08 2.8891091e-11 4.7846384e-13
 1.6321248e-01], sum to 1.0000
[2019-04-08 15:15:46,943] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0410
[2019-04-08 15:15:46,956] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 23.26051307007235, -0.161160553377095, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.0, 30.0], 
sim time this is 3034800.0000, 
sim time next is 3035400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 23.19507437475044, -0.1758633612382613, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.4329228645625367, 0.44137887958724625, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3291465], dtype=float32), -0.7605778]. 
=============================================
[2019-04-08 15:15:48,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2721054e-20 3.6344290e-09 2.5700713e-03 9.9530542e-01 1.7592793e-09
 3.9595809e-12 1.7662787e-04 4.9275540e-15 5.3249151e-20 1.4870176e-22
 1.9478892e-03], sum to 1.0000
[2019-04-08 15:15:48,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4435
[2019-04-08 15:15:48,398] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 22.5, 25.71194500372926, 0.4616646377575016, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [12.833333333333334, 30.0], 
sim time this is 3153000.0000, 
sim time next is 3153600.0000, 
raw observation next is [8.0, 93.0, 113.5, 814.0, 22.5, 25.91511362858126, 0.4871945113840189, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6842105263157896, 0.93, 0.37833333333333335, 0.8994475138121547, 0.375, 0.6595928023817716, 0.6623981704613396, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9406132], dtype=float32), -1.0069236]. 
=============================================
[2019-04-08 15:15:48,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5806988e-19 1.9691365e-10 2.1082463e-04 9.9909949e-01 7.6303706e-11
 3.0365250e-14 8.0596519e-06 1.4109569e-15 1.9300901e-20 5.6008428e-26
 6.8156474e-04], sum to 1.0000
[2019-04-08 15:15:48,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8956
[2019-04-08 15:15:48,501] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.666666666666667, 95.33333333333334, 113.1666666666667, 820.0, 22.5, 25.23499226859872, 0.3256361080994802, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [12.666666666666668, 30.0], 
sim time this is 3154800.0000, 
sim time next is 3155400.0000, 
raw observation next is [7.5, 96.5, 113.0, 823.0, 22.5, 25.24413677356248, 0.330246865536618, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6703601108033241, 0.965, 0.37666666666666665, 0.9093922651933701, 0.375, 0.6036780644635401, 0.610082288512206, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.66951305], dtype=float32), 0.31262746]. 
=============================================
[2019-04-08 15:15:48,656] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.0438995e-13 8.0127967e-07 1.5130212e-02 9.1446304e-01 2.1961944e-06
 1.6366398e-08 4.6984659e-04 1.4034204e-09 1.7695955e-12 6.1608200e-13
 6.9933772e-02], sum to 1.0000
[2019-04-08 15:15:48,656] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8398
[2019-04-08 15:15:48,673] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.0, 56.5, 99.0, 635.0, 19.0, 22.32098526602882, -0.323524209088299, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.0, 30.0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 19.0, 22.28500147954474, -0.3304946085311626, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.08333333333333333, 0.35708345662872826, 0.38983513048961244, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4119903], dtype=float32), -0.15828648]. 
=============================================
[2019-04-08 15:15:48,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.63534867e-22 5.70267300e-09 1.82177007e-01 7.84164906e-01
 4.54558266e-11 1.08747648e-15 1.09441535e-05 7.59313028e-19
 1.98858910e-22 1.25442960e-25 3.36470790e-02], sum to 1.0000
[2019-04-08 15:15:48,792] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6235
[2019-04-08 15:15:48,827] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.0, 100.0, 92.5, 721.0, 22.5, 27.65490440797832, 0.9631381715816607, 1.0, 1.0, 65.0, 203516.4966869676], 
current ob forecast is [], 
actual action is [12.0, 30.0], 
sim time this is 3164400.0000, 
sim time next is 3165000.0000, 
raw observation next is [6.933333333333334, 99.83333333333334, 89.66666666666666, 707.0, 22.5, 27.7142871158748, 0.8584387991268688, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6546629732225301, 0.9983333333333334, 0.29888888888888887, 0.7812154696132597, 0.375, 0.8095239263229, 0.7861462663756229, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6260756], dtype=float32), -0.97547406]. 
=============================================
[2019-04-08 15:15:48,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[131.2478 ]
 [129.97557]
 [129.86143]
 [129.56877]
 [129.71498]], R is [[131.60191345]
 [130.3167572 ]
 [130.01359558]
 [129.7134552 ]
 [129.4163208 ]].
[2019-04-08 15:15:49,266] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.8108411e-19 6.3509303e-10 2.1333421e-02 5.6645852e-01 5.9747790e-10
 8.2015552e-13 2.1196288e-04 6.8868082e-15 2.6989140e-19 8.8257152e-23
 4.1199616e-01], sum to 1.0000
[2019-04-08 15:15:49,267] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5076
[2019-04-08 15:15:49,301] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [4.333333333333333, 100.0, 0.0, 0.0, 22.5, 26.05043192417073, 0.6251117231840616, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [9.333333333333332, 30.0], 
sim time this is 3178200.0000, 
sim time next is 3178800.0000, 
raw observation next is [4.0, 100.0, 0.0, 0.0, 22.5, 25.97010997972274, 0.6189836157789729, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5734072022160666, 1.0, 0.0, 0.0, 0.375, 0.6641758316435616, 0.7063278719263243, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23393813], dtype=float32), -0.6939685]. 
=============================================
[2019-04-08 15:15:49,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.58483863e-20 2.19405431e-11 1.56382608e-04 1.11150965e-02
 2.37816798e-12 2.06525098e-15 2.45863816e-07 5.92831478e-17
 1.67108776e-23 1.70246568e-26 9.88728285e-01], sum to 1.0000
[2019-04-08 15:15:49,332] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3870
[2019-04-08 15:15:49,342] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 100.0, 102.8333333333333, 770.1666666666667, 22.5, 26.03893820691271, 0.5765244291899059, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 3162000.0000, 
sim time next is 3162600.0000, 
raw observation next is [7.0, 100.0, 101.0, 763.0, 22.5, 26.00426885050215, 0.602639582625499, 1.0, 1.0, 65.0, 197844.8095781032], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.33666666666666667, 0.8430939226519337, 0.375, 0.6670224042085126, 0.7008798608751663, 1.0, 1.0, 1.0, 0.9421181408481104], 
reward next is 0.0579, 
noisyNet noise sample is [array([0.00175013], dtype=float32), -1.1568829]. 
=============================================
[2019-04-08 15:15:50,238] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5561907e-17 1.0192337e-09 1.2484540e-02 7.1781898e-01 1.1272909e-07
 1.0177214e-11 2.7491158e-04 1.8830505e-13 2.4782662e-16 8.6235865e-21
 2.6942149e-01], sum to 1.0000
[2019-04-08 15:15:50,240] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1867
[2019-04-08 15:15:50,279] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.5, 100.0, 0.0, 0.0, 19.0, 24.68840933725338, 0.3788564253583386, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [7.5, 30.0], 
sim time this is 3187800.0000, 
sim time next is 3188400.0000, 
raw observation next is [2.333333333333333, 100.0, 0.0, 0.0, 19.0, 24.66219154108625, 0.3681044969906779, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5272391505078486, 1.0, 0.0, 0.0, 0.08333333333333333, 0.5551826284238542, 0.6227014989968926, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31040725], dtype=float32), -0.35186502]. 
=============================================
[2019-04-08 15:15:50,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3246189e-14 1.6075035e-07 5.2413922e-03 9.9038976e-01 8.9181009e-09
 1.2499003e-09 2.0895666e-04 1.3881326e-11 8.5439205e-15 1.3747046e-16
 4.1597765e-03], sum to 1.0000
[2019-04-08 15:15:50,363] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3480
[2019-04-08 15:15:50,377] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 19.0, 23.75257883370147, -0.01751504951506158, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.333333333333333, 30.0], 
sim time this is 3084000.0000, 
sim time next is 3084600.0000, 
raw observation next is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 19.0, 23.65907400358245, -0.03896206066280325, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4672206832871654, 0.6666666666666667, 0.13444444444444448, 0.39042357274401485, 0.08333333333333333, 0.4715895002985375, 0.4870126464457323, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44908085], dtype=float32), 0.17252386]. 
=============================================
[2019-04-08 15:15:50,414] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3549016e-19 4.4357215e-10 7.5762864e-04 9.8040468e-01 1.9683422e-11
 4.6520371e-14 6.4133142e-06 2.9965641e-15 1.8711589e-21 2.2018799e-22
 1.8831281e-02], sum to 1.0000
[2019-04-08 15:15:50,418] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7293
[2019-04-08 15:15:50,438] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [7.333333333333333, 97.66666666666666, 112.8333333333333, 820.1666666666667, 22.5, 25.46200646962484, 0.4068471550758234, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [12.333333333333332, 30.0], 
sim time this is 3156000.0000, 
sim time next is 3156600.0000, 
raw observation next is [7.166666666666667, 98.83333333333334, 112.6666666666667, 817.3333333333334, 22.5, 25.43182410808464, 0.4117443602350097, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6611265004616806, 0.9883333333333334, 0.37555555555555564, 0.9031307550644567, 0.375, 0.61931867567372, 0.6372481200783365, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.81014735], dtype=float32), 0.5261799]. 
=============================================
[2019-04-08 15:15:51,552] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.73363231e-18 5.88285275e-07 1.17406612e-02 2.88826317e-01
 1.19195676e-08 2.43726653e-11 3.82388243e-03 8.55845817e-14
 2.13700602e-18 1.54343088e-19 6.95608556e-01], sum to 1.0000
[2019-04-08 15:15:51,579] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-08 15:15:51,637] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 26.04377537040433, 0.6806139092567715, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3183000.0000, 
sim time next is 3183600.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 19.0, 25.94562591178116, 0.6779132005425202, 0.0, 1.0, 65.0, 197448.3895419104], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6621354926484301, 0.7259710668475067, 0.0, 1.0, 1.0, 0.9402304263900496], 
reward next is 0.0598, 
noisyNet noise sample is [array([0.4850806], dtype=float32), -2.1781805]. 
=============================================
[2019-04-08 15:15:51,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2761611e-18 1.9724103e-11 1.3431233e-04 9.7887141e-01 4.3340792e-10
 8.4137219e-14 1.8948778e-05 2.4199856e-16 3.4164352e-18 1.5819460e-23
 2.0975359e-02], sum to 1.0000
[2019-04-08 15:15:51,745] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6446
[2019-04-08 15:15:51,756] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 26.96305141946289, 0.9531627091181242, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [8.0, 30.0], 
sim time this is 3184800.0000, 
sim time next is 3185400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 19.0, 26.8876079593914, 0.9369531568833295, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.08333333333333333, 0.74063399661595, 0.8123177189611098, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23749292], dtype=float32), 0.36077493]. 
=============================================
[2019-04-08 15:15:52,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3502676e-13 2.9538862e-07 1.8880164e-02 8.7594086e-01 1.2929432e-07
 4.8689688e-09 6.7737114e-05 2.4088057e-10 1.7139439e-14 1.3736030e-15
 1.0511085e-01], sum to 1.0000
[2019-04-08 15:15:52,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1771
[2019-04-08 15:15:52,189] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 100.0, 0.0, 0.0, 19.0, 22.72105044729243, -0.0626847063522195, 0.0, 1.0, 30.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [2.0, 30.0], 
sim time this is 3218400.0000, 
sim time next is 3219000.0000, 
raw observation next is [-3.0, 98.66666666666667, 0.0, 0.0, 19.0, 22.7061530069439, -0.07870961797029823, 0.0, 1.0, 30.0, 18680.41167023054], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9866666666666667, 0.0, 0.0, 0.08333333333333333, 0.392179417245325, 0.47376346067656727, 0.0, 1.0, 0.3, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.6734193], dtype=float32), 0.86748827]. 
=============================================
[2019-04-08 15:15:52,209] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.40042 ]
 [78.55424 ]
 [78.70744 ]
 [78.835075]
 [79.00904 ]], R is [[78.36042786]
 [78.48787689]
 [78.61404419]
 [78.73895264]
 [78.86260986]].
[2019-04-08 15:15:53,113] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9403348e-15 5.4292826e-08 4.6916301e-03 8.1866491e-01 2.4016260e-09
 9.5618263e-11 3.9792841e-04 3.3965777e-13 2.6145435e-17 3.3270222e-20
 1.7624547e-01], sum to 1.0000
[2019-04-08 15:15:53,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5440
[2019-04-08 15:15:53,175] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 26.08693485450737, 0.6583853543009586, 0.0, 1.0, 65.0, 127508.5639315918], 
current ob forecast is [], 
actual action is [7.0, 30.0], 
sim time this is 3193800.0000, 
sim time next is 3194400.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 19.0, 26.09972564165038, 0.659116656640525, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.08333333333333333, 0.6749771368041984, 0.7197055522135084, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25752148], dtype=float32), -0.99945194]. 
=============================================
[2019-04-08 15:15:53,252] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.7568850e-16 6.5343602e-07 2.1604109e-01 4.8109686e-01 1.7988575e-07
 8.4755909e-12 2.3082143e-04 1.8687271e-13 1.1134574e-16 5.2026781e-21
 3.0263042e-01], sum to 1.0000
[2019-04-08 15:15:53,253] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7132
[2019-04-08 15:15:53,278] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.9, 90.0, 106.3333333333333, 719.0, 22.5, 25.76321650345609, 0.578318136135036, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.1, 65.0], 
sim time this is 3233400.0000, 
sim time next is 3234000.0000, 
raw observation next is [-2.8, 88.0, 107.6666666666667, 735.5, 22.5, 26.00437353275385, 0.6257120524778209, 1.0, 1.0, 65.0, 201684.0817328943], 
processed observation next is [1.0, 0.43478260869565216, 0.38504155124653744, 0.88, 0.358888888888889, 0.812707182320442, 0.375, 0.6670311277294875, 0.7085706841592736, 1.0, 1.0, 1.0, 0.9604003892042585], 
reward next is 0.0396, 
noisyNet noise sample is [array([-0.05530105], dtype=float32), 0.35443363]. 
=============================================
[2019-04-08 15:15:53,281] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[104.68893]
 [103.25142]
 [101.30974]
 [ 99.67304]
 [ 98.42865]], R is [[105.76184082]
 [105.70422363]
 [104.6937561 ]
 [104.64682007]
 [103.65437317]].
[2019-04-08 15:15:53,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4615216e-14 1.2215112e-06 3.1498160e-02 2.7663147e-01 9.0434938e-08
 2.3018127e-09 9.3935913e-04 7.6505512e-12 5.3844827e-14 6.6683413e-18
 6.9092971e-01], sum to 1.0000
[2019-04-08 15:15:53,447] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1203
[2019-04-08 15:15:53,475] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-3.0, 96.0, 0.0, 0.0, 19.0, 24.50664801660811, 0.343657326375777, 0.0, 1.0, 65.0, 201564.9061611498], 
current ob forecast is [], 
actual action is [2.0, 30.0], 
sim time this is 3220200.0000, 
sim time next is 3220800.0000, 
raw observation next is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 24.47699388877319, 0.3461941496433405, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9466666666666665, 0.0, 0.0, 0.08333333333333333, 0.5397494907310992, 0.6153980498811135, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4697394], dtype=float32), -1.184339]. 
=============================================
[2019-04-08 15:15:53,503] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5750691e-14 1.1756879e-06 2.9156115e-02 2.4460287e-01 8.8968228e-08
 2.5286240e-09 8.6927024e-04 8.4462021e-12 5.8089662e-14 6.7267936e-18
 7.2537047e-01], sum to 1.0000
[2019-04-08 15:15:53,503] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-08 15:15:53,538] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 93.33333333333334, 0.0, 0.0, 19.0, 24.50193834311784, 0.3408596331542716, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3221400.0000, 
sim time next is 3222000.0000, 
raw observation next is [-3.0, 92.0, 0.0, 0.0, 19.0, 24.51776817432466, 0.3491881960319687, 0.0, 1.0, 65.0, 202342.6939655108], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.0, 0.0, 0.08333333333333333, 0.5431473478603884, 0.6163960653439896, 0.0, 1.0, 1.0, 0.9635366379310039], 
reward next is 0.0365, 
noisyNet noise sample is [array([-0.4697394], dtype=float32), -1.184339]. 
=============================================
[2019-04-08 15:15:53,562] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.801476]
 [85.967705]
 [86.084984]
 [85.42196 ]
 [85.494934]], R is [[85.76631165]
 [85.90864563]
 [86.04956055]
 [85.22923279]
 [85.37693787]].
[2019-04-08 15:15:53,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.12060733e-19 1.39280976e-10 1.09102484e-03 9.21718478e-01
 6.47584244e-08 7.40506103e-15 4.77526301e-05 4.60854124e-18
 5.03819515e-21 1.06359804e-23 7.71426708e-02], sum to 1.0000
[2019-04-08 15:15:53,817] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3069
[2019-04-08 15:15:53,842] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.8, 88.0, 107.6666666666667, 735.5, 22.5, 27.64664207308025, 0.966555997719885, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.2, 30.0], 
sim time this is 3234000.0000, 
sim time next is 3234600.0000, 
raw observation next is [-2.7, 86.0, 109.0, 752.0, 22.5, 27.67117380469322, 0.9749394821366556, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.38781163434903054, 0.86, 0.36333333333333334, 0.830939226519337, 0.375, 0.8059311503911015, 0.8249798273788852, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5225464], dtype=float32), 1.0076749]. 
=============================================
[2019-04-08 15:15:55,442] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.9687085e-20 2.5718927e-09 3.2111234e-04 6.6082084e-01 3.3394598e-10
 7.4884277e-15 2.9600121e-04 1.2664736e-15 2.1941984e-21 2.2474127e-23
 3.3856201e-01], sum to 1.0000
[2019-04-08 15:15:55,442] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5207
[2019-04-08 15:15:55,496] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.066666666666666, 72.5, 113.6666666666667, 815.0, 22.5, 27.46071953372335, 0.9852803643929432, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.933333333333334, 30.0], 
sim time this is 3239400.0000, 
sim time next is 3240000.0000, 
raw observation next is [-2.0, 71.0, 114.0, 817.0, 22.5, 27.66552967140636, 0.7060531889527275, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.71, 0.38, 0.9027624309392265, 0.375, 0.80546080595053, 0.7353510629842425, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9783553], dtype=float32), -0.6116331]. 
=============================================
[2019-04-08 15:15:55,540] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[117.03789 ]
 [117.19971 ]
 [116.83092 ]
 [115.460526]
 [115.10396 ]], R is [[116.72659302]
 [116.55932617]
 [116.39373779]
 [115.26634979]
 [115.11368561]].
[2019-04-08 15:15:55,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2404662e-17 5.8850297e-10 7.5257203e-04 9.2367107e-01 1.4889274e-11
 8.7013795e-13 3.0338936e-04 1.2117171e-13 2.5911255e-20 1.9346456e-21
 7.5272992e-02], sum to 1.0000
[2019-04-08 15:15:55,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7436
[2019-04-08 15:15:55,997] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.333333333333333, 78.5, 111.6666666666667, 791.3333333333334, 22.5, 26.49523204254088, 0.4744558147795237, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [2.666666666666667, 30.0], 
sim time this is 3237000.0000, 
sim time next is 3237600.0000, 
raw observation next is [-2.266666666666667, 77.0, 112.3333333333333, 801.1666666666666, 22.5, 26.57407651965065, 0.7586673438256079, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.39981532779316714, 0.77, 0.37444444444444436, 0.8852670349907918, 0.375, 0.7145063766375541, 0.752889114608536, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7137855], dtype=float32), 0.8322369]. 
=============================================
[2019-04-08 15:15:56,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2150943e-19 9.4071950e-10 2.5117779e-04 9.9320257e-01 1.0226324e-09
 3.8820463e-13 1.1418343e-04 6.8232915e-14 5.3659536e-20 8.9541453e-21
 6.4319819e-03], sum to 1.0000
[2019-04-08 15:15:56,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4686
[2019-04-08 15:15:56,393] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.30869146490633, 0.9064169507228903, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.0, 30.0], 
sim time this is 3264000.0000, 
sim time next is 3264600.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.20385922658066, 0.8980301106907193, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.375, 0.7669882688817218, 0.7993433702302397, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8660556], dtype=float32), -0.42259127]. 
=============================================
[2019-04-08 15:15:56,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1461003e-18 2.2376299e-09 4.6359380e-03 9.9522394e-01 3.9371431e-10
 8.4831070e-15 7.3571455e-06 7.5636764e-15 3.2693108e-20 2.3823047e-23
 1.3271759e-04], sum to 1.0000
[2019-04-08 15:15:56,680] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7645
[2019-04-08 15:15:56,700] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.15058897754157, 0.8487996553563101, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.0, 30.0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 22.5, 27.04422548457413, 0.8355030012925525, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.375, 0.7536854570478443, 0.7785010004308508, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.902734], dtype=float32), -2.0765703]. 
=============================================
[2019-04-08 15:15:56,793] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.3411722e-17 2.8774375e-08 4.8039074e-04 7.1750897e-01 1.2224685e-07
 2.5508306e-11 2.9297458e-05 7.2643606e-13 1.7000054e-17 1.2034057e-20
 2.8198132e-01], sum to 1.0000
[2019-04-08 15:15:56,793] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8939
[2019-04-08 15:15:56,815] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.26475584208983, 0.6441046820634551, 0.0, 1.0, 65.0, 125258.7367875676], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3277200.0000, 
sim time next is 3277800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.21406494420877, 0.643579870706823, 0.0, 1.0, 65.0, 92606.01899642411], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6845054120173973, 0.714526623568941, 0.0, 1.0, 1.0, 0.44098104284011486], 
reward next is 0.5590, 
noisyNet noise sample is [array([-0.6654212], dtype=float32), -0.22035578]. 
=============================================
[2019-04-08 15:15:56,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2095665e-18 3.4370970e-08 1.3571179e-04 8.5854298e-01 6.7275002e-10
 1.3856830e-12 1.0169196e-03 1.7784827e-15 2.2594550e-18 1.2363505e-20
 1.4030440e-01], sum to 1.0000
[2019-04-08 15:15:56,926] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1125
[2019-04-08 15:15:56,938] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 67.0, 0.0, 0.0, 22.5, 26.35511545635519, 0.6481314364179728, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.0, 30.0], 
sim time this is 3266400.0000, 
sim time next is 3267000.0000, 
raw observation next is [-4.0, 68.0, 0.0, 0.0, 22.5, 26.22634172515534, 0.6322981520453392, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.68, 0.0, 0.0, 0.375, 0.6855284770962783, 0.7107660506817798, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8903074], dtype=float32), 0.5252246]. 
=============================================
[2019-04-08 15:15:56,979] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[ 99.15746]
 [100.84257]
 [101.86636]
 [103.28307]
 [104.05161]], R is [[98.45610046]
 [98.47154236]
 [98.48683167]
 [98.50196075]
 [98.51694489]].
[2019-04-08 15:15:57,008] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 86841: loss 6.0732
[2019-04-08 15:15:57,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 86841: learning rate 0.0000
[2019-04-08 15:15:57,674] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87095: loss 0.2027
[2019-04-08 15:15:57,677] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87095: learning rate 0.0000
[2019-04-08 15:15:57,898] A3C_AGENT_WORKER-Thread-8 INFO:Local step 5500, global step 87189: loss 1.2247
[2019-04-08 15:15:57,899] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 5500, global step 87189: learning rate 0.0000
[2019-04-08 15:15:58,257] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.1995017e-15 9.6047768e-07 1.3898937e-02 5.7984114e-01 1.4588981e-06
 2.0642034e-10 2.5772976e-03 1.9790779e-12 2.7731454e-15 3.6661133e-19
 4.0368024e-01], sum to 1.0000
[2019-04-08 15:15:58,257] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9862
[2019-04-08 15:15:58,271] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.333333333333333, 89.33333333333334, 0.0, 0.0, 19.0, 26.26675608671118, 0.6601128302292433, 0.0, 1.0, 65.0, 105644.2854963548], 
current ob forecast is [], 
actual action is [-1.333333333333333, 65.0], 
sim time this is 3280800.0000, 
sim time next is 3281400.0000, 
raw observation next is [-6.5, 88.0, 0.0, 0.0, 19.0, 26.25311841177113, 0.6579585642396232, 0.0, 1.0, 65.0, 71581.93243253817], 
processed observation next is [1.0, 1.0, 0.28254847645429365, 0.88, 0.0, 0.0, 0.08333333333333333, 0.6877598676475941, 0.7193195214132078, 0.0, 1.0, 1.0, 0.3408663449168485], 
reward next is 0.6591, 
noisyNet noise sample is [array([-0.4777509], dtype=float32), -1.4644866]. 
=============================================
[2019-04-08 15:15:58,546] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 87420: loss 0.7539
[2019-04-08 15:15:58,546] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 87420: learning rate 0.0000
[2019-04-08 15:15:58,703] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87463: loss 2.1089
[2019-04-08 15:15:58,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87463: learning rate 0.0000
[2019-04-08 15:15:58,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1095942e-13 8.0093412e-07 3.8287931e-03 5.2034551e-01 4.8791202e-07
 2.5920089e-08 2.6187678e-03 5.3661433e-12 3.8228371e-15 7.6788382e-15
 4.7320560e-01], sum to 1.0000
[2019-04-08 15:15:58,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5461
[2019-04-08 15:15:58,786] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 26.32835524843816, 0.6615936090759705, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.166666666666667, 65.0], 
sim time this is 3207000.0000, 
sim time next is 3207600.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.23162709933476, 0.66211988782048, 0.0, 1.0, 65.0, 203754.3785703608], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.6859689249445632, 0.7207066292734933, 0.0, 1.0, 1.0, 0.9702589455731466], 
reward next is 0.0297, 
noisyNet noise sample is [array([0.7961471], dtype=float32), -1.2839848]. 
=============================================
[2019-04-08 15:15:58,841] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87509: loss 2.6432
[2019-04-08 15:15:58,851] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87509: learning rate 0.0000
[2019-04-08 15:15:59,187] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87621: loss 15.0581
[2019-04-08 15:15:59,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87621: learning rate 0.0000
[2019-04-08 15:15:59,394] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87683: loss 4.0632
[2019-04-08 15:15:59,395] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87683: learning rate 0.0000
[2019-04-08 15:15:59,485] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87712: loss 1.2161
[2019-04-08 15:15:59,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87712: learning rate 0.0000
[2019-04-08 15:15:59,786] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0521791e-13 2.7893205e-07 1.0692662e-02 6.8552834e-01 9.9812794e-08
 3.9490150e-08 1.4181648e-03 2.3772673e-11 4.7242218e-13 7.5989983e-16
 3.0236039e-01], sum to 1.0000
[2019-04-08 15:15:59,787] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0730
[2019-04-08 15:15:59,810] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 87809: loss 1.7355
[2019-04-08 15:15:59,812] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 87809: learning rate 0.0000
[2019-04-08 15:15:59,854] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-8.600000000000001, 77.0, 0.0, 0.0, 19.0, 24.90804660903825, 0.3295996720521482, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-3.6000000000000014, 30.0], 
sim time this is 3296400.0000, 
sim time next is 3297000.0000, 
raw observation next is [-8.75, 77.0, 0.0, 0.0, 19.0, 24.91700116680087, 0.3223420962602213, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.22022160664819945, 0.77, 0.0, 0.0, 0.08333333333333333, 0.5764167639000725, 0.6074473654200737, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23177075], dtype=float32), 0.51057315]. 
=============================================
[2019-04-08 15:15:59,871] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.12661 ]
 [73.80442 ]
 [74.03597 ]
 [73.999794]
 [74.409096]], R is [[74.11442566]
 [74.37328339]
 [74.62955475]
 [74.88326263]
 [75.13442993]].
[2019-04-08 15:15:59,963] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87860: loss 0.1907
[2019-04-08 15:15:59,964] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87860: learning rate 0.0000
[2019-04-08 15:15:59,966] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8741864e-14 8.2438612e-07 1.9537568e-02 8.8122912e-02 2.9168007e-07
 3.2268291e-10 2.1676310e-04 9.2854977e-13 1.9356937e-14 4.2346420e-18
 8.9212161e-01], sum to 1.0000
[2019-04-08 15:15:59,966] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3989
[2019-04-08 15:15:59,993] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.333333333333332, 78.16666666666667, 92.0, 469.0, 22.5, 25.37964674269516, 0.4031947415145882, 1.0, 1.0, 65.0, 87220.89494395193], 
current ob forecast is [], 
actual action is [-4.333333333333332, 65.0], 
sim time this is 3315000.0000, 
sim time next is 3315600.0000, 
raw observation next is [-9.0, 77.0, 95.0, 505.5, 22.5, 25.62935417987588, 0.4391117258173217, 1.0, 1.0, 65.0, 69786.87719984003], 
processed observation next is [1.0, 0.391304347826087, 0.21329639889196678, 0.77, 0.31666666666666665, 0.5585635359116022, 0.375, 0.6357795149896566, 0.6463705752724406, 1.0, 1.0, 1.0, 0.3323184628563811], 
reward next is 0.6677, 
noisyNet noise sample is [array([-1.7928697], dtype=float32), 1.48391]. 
=============================================
[2019-04-08 15:16:00,001] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87877: loss 1.8449
[2019-04-08 15:16:00,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87877: learning rate 0.0000
[2019-04-08 15:16:00,049] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9280892e-12 2.1162516e-06 2.3565115e-03 9.8347867e-01 1.1774389e-06
 1.9258181e-08 9.9791971e-04 1.2948055e-09 1.4008339e-12 3.0845188e-15
 1.3163511e-02], sum to 1.0000
[2019-04-08 15:16:00,049] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9921
[2019-04-08 15:16:00,095] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.633333333333333, 76.33333333333333, 0.0, 0.0, 19.0, 24.39084847362038, 0.1613282210694765, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-4.633333333333333, 65.0], 
sim time this is 3300000.0000, 
sim time next is 3300600.0000, 
raw observation next is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 19.0, 24.10031155888549, 0.1484850436736238, 0.0, 1.0, 65.0, 200726.5536409985], 
processed observation next is [1.0, 0.17391304347826086, 0.19067405355494, 0.7616666666666667, 0.0, 0.0, 0.08333333333333333, 0.5083592965737909, 0.5494950145578746, 0.0, 1.0, 1.0, 0.9558407316238023], 
reward next is 0.0442, 
noisyNet noise sample is [array([0.82664716], dtype=float32), -0.20708005]. 
=============================================
[2019-04-08 15:16:00,681] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88082: loss 1.9794
[2019-04-08 15:16:00,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88084: learning rate 0.0000
[2019-04-08 15:16:00,769] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.1561469e-16 1.6909736e-07 3.0951325e-02 5.3216952e-01 8.2936999e-09
 1.6983537e-10 3.7312016e-03 1.2178962e-13 7.4197009e-18 2.5637274e-20
 4.3314785e-01], sum to 1.0000
[2019-04-08 15:16:00,798] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-08 15:16:00,911] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.833333333333334, 62.33333333333334, 114.3333333333333, 778.6666666666667, 22.5, 27.15840276002348, 0.4031702180254268, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.833333333333334, 65.0], 
sim time this is 3323400.0000, 
sim time next is 3324000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666667, 115.1666666666667, 788.3333333333333, 22.5, 27.0820138692834, 0.7663567304435994, 1.0, 1.0, 65.0, 202217.6365932355], 
processed observation next is [1.0, 0.4782608695652174, 0.27793167128347185, 0.6066666666666667, 0.383888888888889, 0.871086556169429, 0.375, 0.7568344891069501, 0.7554522434811998, 1.0, 1.0, 1.0, 0.9629411266344549], 
reward next is 0.0371, 
noisyNet noise sample is [array([-0.47092217], dtype=float32), -0.39926112]. 
=============================================
[2019-04-08 15:16:00,932] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[105.11825 ]
 [104.48131 ]
 [104.776985]
 [104.30024 ]
 [102.2387  ]], R is [[104.94749451]
 [104.89801788]
 [104.84903717]
 [104.80054474]
 [103.83351898]].
[2019-04-08 15:16:01,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9116033e-17 4.8116899e-08 7.4773487e-03 6.8655413e-01 1.3071627e-10
 4.0444280e-12 1.5976713e-03 4.4541668e-13 1.4648882e-18 4.4401108e-21
 3.0437079e-01], sum to 1.0000
[2019-04-08 15:16:01,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9080
[2019-04-08 15:16:01,085] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 54.0, 116.0, 805.5, 22.5, 26.81983206384902, 0.751514359926452, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3330000.0000, 
sim time next is 3330600.0000, 
raw observation next is [-4.833333333333334, 53.33333333333333, 115.3333333333333, 803.6666666666666, 22.5, 26.91074188768914, 0.5895941719972811, 1.0, 1.0, 65.0, 109783.461000047], 
processed observation next is [1.0, 0.5652173913043478, 0.32871652816251157, 0.5333333333333333, 0.3844444444444443, 0.8880294659300184, 0.375, 0.7425618239740951, 0.6965313906657604, 1.0, 1.0, 1.0, 0.5227783857145095], 
reward next is 0.4772, 
noisyNet noise sample is [array([1.2258646], dtype=float32), 0.45748347]. 
=============================================
[2019-04-08 15:16:01,351] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8055134e-18 3.2614019e-11 7.5692311e-05 9.9722242e-01 1.7481050e-11
 8.1698380e-13 7.1168964e-05 3.0711691e-15 9.0271569e-21 1.5229326e-24
 2.6307870e-03], sum to 1.0000
[2019-04-08 15:16:01,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6666
[2019-04-08 15:16:01,441] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.0, 75.66666666666667, 114.6666666666667, 821.0, 22.5, 27.07842324481394, 0.908420996117078, 1.0, 1.0, 29.99999999999999, 0.0], 
current ob forecast is [], 
actual action is [3.0, 30.0], 
sim time this is 3241200.0000, 
sim time next is 3241800.0000, 
raw observation next is [-2.0, 78.0, 115.0, 823.0, 22.5, 27.37725549386973, 0.924564620621059, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.78, 0.38333333333333336, 0.9093922651933701, 0.375, 0.7814379578224774, 0.8081882068736864, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.668746], dtype=float32), -2.0256462]. 
=============================================
[2019-04-08 15:16:02,319] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0751407e-17 3.7217905e-08 2.7345619e-03 4.5549381e-01 4.6281559e-10
 5.5923465e-12 8.5005857e-05 1.8553989e-14 3.0832450e-18 6.5033883e-20
 5.4168659e-01], sum to 1.0000
[2019-04-08 15:16:02,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0670
[2019-04-08 15:16:02,375] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-2.833333333333333, 49.33333333333334, 93.33333333333334, 700.0, 22.5, 27.25998932957253, 0.7107202767417999, 1.0, 1.0, 65.0, 112007.3194686192], 
current ob forecast is [], 
actual action is [2.166666666666667, 30.0], 
sim time this is 3337800.0000, 
sim time next is 3338400.0000, 
raw observation next is [-2.666666666666667, 48.66666666666667, 90.16666666666666, 687.0, 22.5, 26.43151353783256, 0.7890033498872272, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.38873499538319484, 0.4866666666666667, 0.3005555555555555, 0.7591160220994475, 0.375, 0.7026261281527134, 0.7630011166290758, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8915524], dtype=float32), -1.0661219]. 
=============================================
[2019-04-08 15:16:03,524] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88988: loss 1.2368
[2019-04-08 15:16:03,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88988: learning rate 0.0000
[2019-04-08 15:16:03,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.3590799e-13 4.4498191e-07 4.3709665e-03 9.8191088e-01 4.0424862e-07
 1.9469756e-08 1.5717315e-03 3.3049596e-10 7.8816305e-13 6.2850836e-15
 1.2145569e-02], sum to 1.0000
[2019-04-08 15:16:03,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3719
[2019-04-08 15:16:03,606] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.816666666666666, 76.16666666666667, 0.0, 0.0, 19.0, 22.98284595636367, -0.09907830393401042, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-4.816666666666666, 30.0], 
sim time this is 3300600.0000, 
sim time next is 3301200.0000, 
raw observation next is [-10.0, 76.0, 0.0, 0.0, 19.0, 22.89764668181771, -0.1195335609308244, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.18559556786703602, 0.76, 0.0, 0.0, 0.08333333333333333, 0.4081372234848093, 0.4601554796897252, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56368375], dtype=float32), 2.0394645]. 
=============================================
[2019-04-08 15:16:04,729] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0078839e-12 2.5950092e-06 1.6867211e-01 3.8507938e-01 2.0890736e-06
 5.5870499e-09 5.5576395e-02 1.9580162e-10 3.1952163e-14 8.8172705e-16
 3.9066744e-01], sum to 1.0000
[2019-04-08 15:16:04,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2571
[2019-04-08 15:16:04,779] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.166666666666667, 66.0, 0.0, 0.0, 19.0, 26.50015402969439, 0.6311750107970467, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [0.833333333333333, 30.0], 
sim time this is 3363000.0000, 
sim time next is 3363600.0000, 
raw observation next is [-4.333333333333334, 67.0, 0.0, 0.0, 19.0, 26.45417313293794, 0.6371959852861798, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.3425669436749769, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7045144277448282, 0.71239866176206, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9150271], dtype=float32), -0.14047204]. 
=============================================
[2019-04-08 15:16:05,355] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6122669e-14 7.6265826e-07 4.5547052e-03 8.8146275e-01 4.5076145e-08
 9.2232021e-11 3.8458538e-06 1.3419513e-12 1.8803263e-15 6.7799347e-19
 1.1397792e-01], sum to 1.0000
[2019-04-08 15:16:05,356] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5061
[2019-04-08 15:16:05,405] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.5, 76.0, 0.0, 0.0, 19.0, 26.07803612079841, 0.6929032229791575, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [0.5, 30.0], 
sim time this is 3270600.0000, 
sim time next is 3271200.0000, 
raw observation next is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 19.0, 26.0150605927273, 0.6736452593311917, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.33333333333333337, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6679217160606082, 0.7245484197770639, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4633273], dtype=float32), 0.012250913]. 
=============================================
[2019-04-08 15:16:05,626] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 89775: loss 3.1229
[2019-04-08 15:16:05,627] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 89775: learning rate 0.0000
[2019-04-08 15:16:06,191] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6348475e-13 9.8328712e-07 7.0905234e-03 7.2950977e-01 9.5273344e-06
 1.2187888e-07 2.1279745e-03 2.7189095e-10 2.3597357e-13 1.3604415e-15
 2.6126105e-01], sum to 1.0000
[2019-04-08 15:16:06,196] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-08 15:16:06,214] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-4.0, 65.5, 0.0, 0.0, 19.0, 25.84444571835476, 0.480418817722037, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [1.0, 30.0], 
sim time this is 3389400.0000, 
sim time next is 3390000.0000, 
raw observation next is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 19.0, 25.99936207638542, 0.4678933896974757, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.3610341643582641, 0.6366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6666135063654517, 0.6559644632324919, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8927447], dtype=float32), -0.10839325]. 
=============================================
[2019-04-08 15:16:06,232] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[76.83041]
 [77.26652]
 [77.15189]
 [76.10881]
 [75.5183 ]], R is [[77.05599976]
 [77.28543854]
 [77.20040894]
 [77.05477905]
 [76.84185791]].
[2019-04-08 15:16:06,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9248790e-14 4.3276270e-07 6.7202086e-03 4.1654208e-01 5.8543772e-08
 1.0638551e-09 1.5593915e-02 1.2377793e-11 2.0438028e-15 9.3979159e-17
 5.6114334e-01], sum to 1.0000
[2019-04-08 15:16:06,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8927
[2019-04-08 15:16:06,572] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 60.0, 87.0, 422.0, 22.5, 25.33388229104978, 0.3653375316936796, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 3400200.0000, 
sim time next is 3400800.0000, 
raw observation next is [-1.333333333333333, 60.0, 89.0, 461.3333333333333, 22.5, 25.52204692050106, 0.4019797474262762, 1.0, 1.0, 65.00000000000003, 203660.3846865929], 
processed observation next is [1.0, 0.34782608695652173, 0.42566943674976926, 0.6, 0.2966666666666667, 0.5097605893186004, 0.375, 0.6268372433750885, 0.6339932491420921, 1.0, 1.0, 1.0000000000000007, 0.9698113556504424], 
reward next is 0.0302, 
noisyNet noise sample is [array([0.8023021], dtype=float32), 1.1970489]. 
=============================================
[2019-04-08 15:16:07,190] A3C_AGENT_WORKER-Thread-7 INFO:Local step 5500, global step 90427: loss 0.1018
[2019-04-08 15:16:07,192] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 5500, global step 90427: learning rate 0.0000
[2019-04-08 15:16:07,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7430261e-17 9.2623897e-10 4.2981203e-03 3.5250597e-03 2.2618263e-09
 1.8367773e-12 1.7887674e-05 3.7252920e-15 9.0757578e-21 8.7097320e-24
 9.9215901e-01], sum to 1.0000
[2019-04-08 15:16:07,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6441
[2019-04-08 15:16:07,510] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 48.33333333333334, 115.6666666666667, 813.3333333333334, 22.5, 27.54639061009475, 0.743800573449537, 1.0, 1.0, 65.0, 74112.09187059013], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3415800.0000, 
sim time next is 3416400.0000, 
raw observation next is [3.0, 49.0, 115.0, 811.5, 22.5, 26.78104055383989, 0.8537276059031468, 1.0, 1.0, 65.0, 80679.69037861406], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.38333333333333336, 0.8966850828729281, 0.375, 0.7317533794866575, 0.7845758686343823, 1.0, 1.0, 1.0, 0.3841890018029241], 
reward next is 0.6158, 
noisyNet noise sample is [array([1.3312005], dtype=float32), -1.1062478]. 
=============================================
[2019-04-08 15:16:07,703] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2926872e-17 6.2361258e-09 2.7157899e-04 9.8383510e-01 4.5046139e-10
 1.2100398e-13 1.1722761e-04 2.3823858e-15 1.0663171e-18 1.4615160e-22
 1.5776115e-02], sum to 1.0000
[2019-04-08 15:16:07,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2454
[2019-04-08 15:16:07,744] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 45.0, 116.0, 810.5, 22.5, 27.57930978313547, 0.874831520076553, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [8.0, 30.0], 
sim time this is 3412800.0000, 
sim time next is 3413400.0000, 
raw observation next is [3.0, 45.66666666666666, 116.3333333333333, 812.6666666666667, 22.5, 27.60260637814792, 0.8771913523631548, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.45666666666666655, 0.38777777777777767, 0.897974217311234, 0.375, 0.8002171981789935, 0.792397117454385, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.53734887], dtype=float32), -1.198237]. 
=============================================
[2019-04-08 15:16:07,996] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2448128e-13 7.2664818e-07 3.1438079e-02 9.4941884e-01 8.5855518e-08
 6.4547162e-10 5.9431157e-04 1.6300159e-11 2.9351994e-16 1.3737548e-16
 1.8547952e-02], sum to 1.0000
[2019-04-08 15:16:07,999] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7011
[2019-04-08 15:16:08,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-5.833333333333333, 76.0, 0.0, 0.0, 19.0, 26.20790673200782, 0.5888511074508008, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-0.833333333333333, 30.0], 
sim time this is 3369000.0000, 
sim time next is 3369600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.13575236426234, 0.5726916003767466, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6779793636885284, 0.6908972001255821, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3925034], dtype=float32), -0.35899365]. 
=============================================
[2019-04-08 15:16:09,329] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4775650e-15 1.0371788e-07 1.8503614e-03 1.4083804e-01 8.9218029e-09
 3.5794149e-11 3.2874894e-05 3.1452803e-13 5.4708531e-17 7.4823444e-22
 8.5727859e-01], sum to 1.0000
[2019-04-08 15:16:09,329] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7844
[2019-04-08 15:16:09,345] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 27.0002482353202, 0.844763420643897, 0.0, 1.0, 65.0, 42976.67623854572], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3445200.0000, 
sim time next is 3445800.0000, 
raw observation next is [1.0, 80.16666666666667, 0.0, 0.0, 19.0, 26.98930546447239, 0.8421622671405546, 0.0, 1.0, 65.0, 40928.44260711525], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.8016666666666667, 0.0, 0.0, 0.08333333333333333, 0.7491087887060326, 0.7807207557135182, 0.0, 1.0, 1.0, 0.19489734574816786], 
reward next is 0.8051, 
noisyNet noise sample is [array([-0.16943917], dtype=float32), -1.0997851]. 
=============================================
[2019-04-08 15:16:09,880] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3792452e-18 1.3620974e-07 2.0492815e-03 5.5462682e-01 2.0879587e-09
 3.3897760e-13 5.9149804e-04 6.1716073e-13 1.0424737e-18 6.2857838e-21
 4.4273221e-01], sum to 1.0000
[2019-04-08 15:16:09,882] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-08 15:16:09,903] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 22.5, 27.01209946969992, 0.8055245690247584, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [6.0, 30.0], 
sim time this is 3439200.0000, 
sim time next is 3439800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 22.5, 26.93731842942771, 0.8010140650804557, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.375, 0.7447765357856424, 0.7670046883601519, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18413019], dtype=float32), -0.8936011]. 
=============================================
[2019-04-08 15:16:10,570] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.1791988e-15 9.6107637e-09 3.0819583e-03 1.4157606e-02 1.1971761e-09
 8.6911710e-12 2.1631511e-05 1.6110407e-13 1.9574582e-18 6.9894328e-22
 9.8273879e-01], sum to 1.0000
[2019-04-08 15:16:10,571] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8757
[2019-04-08 15:16:10,591] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.92696883275812, 0.7691857107236237, 1.0, 1.0, 65.0, 40471.10990963276], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3354000.0000, 
sim time next is 3354600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.86216719588743, 0.7612588268971366, 1.0, 1.0, 65.0, 40669.37038596438], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7385139329906192, 0.7537529422990455, 1.0, 1.0, 1.0, 0.1936636685045923], 
reward next is 0.8063, 
noisyNet noise sample is [array([0.51953167], dtype=float32), 0.34531724]. 
=============================================
[2019-04-08 15:16:10,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2855849e-12 2.1317534e-05 1.6829170e-02 5.3931135e-01 5.3867002e-06
 3.7861492e-09 2.2242076e-03 2.3330371e-09 2.0418508e-13 7.1311820e-16
 4.4160855e-01], sum to 1.0000
[2019-04-08 15:16:10,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1310
[2019-04-08 15:16:10,675] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 69.5, 0.0, 0.0, 19.0, 26.69156410751854, 0.7170074649499149, 0.0, 1.0, 65.0, 69788.75702887145], 
current ob forecast is [], 
actual action is [5.5, 30.0], 
sim time this is 3472200.0000, 
sim time next is 3472800.0000, 
raw observation next is [0.3333333333333334, 70.33333333333334, 0.0, 0.0, 19.0, 26.67386163448959, 0.7143823247509884, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4718374884579871, 0.7033333333333335, 0.0, 0.0, 0.08333333333333333, 0.7228218028741326, 0.7381274415836628, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25640923], dtype=float32), 0.9229001]. 
=============================================
[2019-04-08 15:16:10,735] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.9320514e-15 8.6569374e-09 4.0106024e-03 3.7271404e-01 5.5070760e-08
 2.2468319e-11 8.6512606e-05 3.5245597e-13 1.5789285e-17 1.4133169e-20
 6.2318879e-01], sum to 1.0000
[2019-04-08 15:16:10,736] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3865
[2019-04-08 15:16:10,749] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.666666666666667, 61.66666666666666, 0.0, 0.0, 19.0, 26.74001731368402, 0.7437815424992035, 0.0, 1.0, 65.0, 42111.65350826662], 
current ob forecast is [], 
actual action is [1.333333333333333, 65.0], 
sim time this is 3357600.0000, 
sim time next is 3358200.0000, 
raw observation next is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 19.0, 26.75012661914651, 0.7372513554060877, 0.0, 1.0, 65.0, 41468.78938147877], 
processed observation next is [1.0, 0.8695652173913043, 0.3564173591874424, 0.6333333333333334, 0.0, 0.0, 0.08333333333333333, 0.7291772182622092, 0.7457504518020293, 0.0, 1.0, 1.0, 0.19747042562608938], 
reward next is 0.8025, 
noisyNet noise sample is [array([0.35225698], dtype=float32), 0.009366011]. 
=============================================
[2019-04-08 15:16:10,854] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.24641301e-13 8.20557034e-07 2.48935781e-02 3.28276217e-01
 9.67877796e-08 6.50441589e-10 1.08752438e-05 1.14019995e-10
 1.17235501e-14 1.91165526e-17 6.46818459e-01], sum to 1.0000
[2019-04-08 15:16:10,854] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2971
[2019-04-08 15:16:10,876] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [1.0, 67.83333333333333, 0.0, 0.0, 19.0, 26.79108906214165, 0.707690916693552, 0.0, 1.0, 65.0, 59539.68015335306], 
current ob forecast is [], 
actual action is [6.0, 30.0], 
sim time this is 3469800.0000, 
sim time next is 3470400.0000, 
raw observation next is [1.0, 67.0, 0.0, 0.0, 19.0, 26.75649036005014, 0.7011968554404686, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.67, 0.0, 0.0, 0.08333333333333333, 0.7297075300041783, 0.7337322851468229, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57170814], dtype=float32), -0.169]. 
=============================================
[2019-04-08 15:16:11,819] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.89618383e-14 1.15568746e-08 1.81730240e-04 8.33413098e-03
 6.52096617e-07 6.37531306e-10 2.33721526e-04 1.71597320e-11
 4.54792996e-14 1.01017885e-17 9.91249800e-01], sum to 1.0000
[2019-04-08 15:16:11,819] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5620
[2019-04-08 15:16:11,837] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8346513e-15 5.5191532e-07 7.3227548e-04 7.9666786e-02 1.5875723e-09
 3.0475879e-11 6.6336638e-06 1.4460958e-14 1.4165434e-19 6.8494303e-21
 9.1959375e-01], sum to 1.0000
[2019-04-08 15:16:11,838] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7840
[2019-04-08 15:16:11,839] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.66140938562457, 0.6954348400126126, 0.0, 1.0, 65.0, 77614.4305094397], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.64895982063058, 0.695292579069562, 0.0, 1.0, 65.0, 58055.84469383588], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.720746651719215, 0.7317641930231873, 0.0, 1.0, 1.0, 0.2764564033039804], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.1583743], dtype=float32), 0.5425729]. 
=============================================
[2019-04-08 15:16:11,859] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.6666666666666667, 65.33333333333334, 0.0, 0.0, 22.5, 27.5871528066336, 0.996835005986943, 0.0, 1.0, 65.0, 32854.91073332464], 
current ob forecast is [], 
actual action is [5.666666666666667, 65.0], 
sim time this is 3526800.0000, 
sim time next is 3527400.0000, 
raw observation next is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 22.5, 27.52032759125261, 0.9868675080381261, 0.0, 1.0, 65.0, 34268.40266439696], 
processed observation next is [1.0, 0.8260869565217391, 0.4718374884579871, 0.6866666666666665, 0.0, 0.0, 0.375, 0.7933606326043842, 0.8289558360127086, 0.0, 1.0, 1.0, 0.16318286983046174], 
reward next is 0.8368, 
noisyNet noise sample is [array([0.04698493], dtype=float32), -0.37393978]. 
=============================================
[2019-04-08 15:16:12,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4007697e-14 5.7427815e-07 5.1038142e-02 6.8062478e-01 1.1096326e-08
 4.0366746e-10 2.8032057e-03 3.7582193e-10 4.4270835e-17 1.0065569e-18
 2.6553333e-01], sum to 1.0000
[2019-04-08 15:16:12,526] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5340
[2019-04-08 15:16:12,549] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-1.0, 71.0, 91.66666666666666, 489.3333333333334, 22.5, 27.13408210055387, 0.8330200277706455, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [4.0, 30.0], 
sim time this is 3487800.0000, 
sim time next is 3488400.0000, 
raw observation next is [-1.0, 71.0, 93.5, 534.5, 22.5, 27.27229425048791, 0.8420646593090471, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.71, 0.31166666666666665, 0.5906077348066299, 0.375, 0.7726911875406591, 0.7806882197696824, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3342126], dtype=float32), -0.7122894]. 
=============================================
[2019-04-08 15:16:12,740] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.8165727e-15 3.4684604e-08 1.3488707e-03 6.6115606e-01 4.4474180e-07
 7.2413181e-10 3.2657495e-04 1.2843550e-12 2.3296320e-15 2.7734975e-18
 3.3716804e-01], sum to 1.0000
[2019-04-08 15:16:12,740] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6539
[2019-04-08 15:16:12,756] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.8333333333333334, 67.83333333333334, 0.0, 0.0, 19.0, 26.82343416283896, 0.7356190998412632, 0.0, 1.0, 65.0, 46199.85917009832], 
current ob forecast is [], 
actual action is [5.833333333333333, 30.0], 
sim time this is 3471000.0000, 
sim time next is 3471600.0000, 
raw observation next is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 19.0, 26.79672088272162, 0.7274192906675642, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4810710987996307, 0.6866666666666668, 0.0, 0.0, 0.08333333333333333, 0.7330600735601349, 0.7424730968891881, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7516384], dtype=float32), -0.39560273]. 
=============================================
[2019-04-08 15:16:12,971] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8562840e-19 4.4661056e-07 6.8183169e-03 1.0082991e-02 3.4340747e-10
 2.6119917e-11 4.9094386e-05 8.8771840e-15 1.7954820e-18 6.6596931e-23
 9.8304915e-01], sum to 1.0000
[2019-04-08 15:16:12,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5524
[2019-04-08 15:16:13,012] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 62.0, 525.0, 22.5, 28.7004760950585, 1.240432945427899, 1.0, 1.0, 65.0, 27765.4835905106], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3515400.0000, 
sim time next is 3516000.0000, 
raw observation next is [3.0, 49.0, 53.83333333333334, 462.6666666666667, 22.5, 28.37234953679158, 1.175271207997072, 1.0, 1.0, 64.99999999999994, 19500.20227106081], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.1794444444444445, 0.5112338858195212, 0.375, 0.8643624613992985, 0.8917570693323573, 1.0, 1.0, 0.9999999999999989, 0.09285810605267053], 
reward next is 0.9071, 
noisyNet noise sample is [array([0.7522117], dtype=float32), -0.36797982]. 
=============================================
[2019-04-08 15:16:13,029] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[106.077385]
 [106.27495 ]
 [105.972176]
 [106.17313 ]
 [105.96015 ]], R is [[105.72568512]
 [105.53620911]
 [105.39109802]
 [105.24742889]
 [105.10519409]].
[2019-04-08 15:16:13,821] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.4673822e-18 4.9089586e-09 5.7153270e-04 6.5929514e-01 1.3661997e-11
 1.1705945e-13 2.1177459e-05 3.4038979e-17 9.5170418e-19 1.9928845e-22
 3.4011218e-01], sum to 1.0000
[2019-04-08 15:16:13,828] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0919
[2019-04-08 15:16:13,835] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [3.0, 49.0, 102.0, 781.0, 22.5, 28.53374257384552, 1.166937830089169, 1.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [8.0, 30.0], 
sim time this is 3508200.0000, 
sim time next is 3508800.0000, 
raw observation next is [3.0, 49.0, 99.66666666666666, 765.3333333333334, 22.5, 28.58928597073066, 1.173881142245388, 1.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3322222222222222, 0.8456721915285451, 0.375, 0.8824404975608884, 0.891293714081796, 1.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.98563313], dtype=float32), -0.23283114]. 
=============================================
[2019-04-08 15:16:13,919] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8063851e-14 2.0833596e-07 8.9125399e-04 9.4057964e-03 2.6709802e-07
 1.7167551e-08 2.6411150e-04 3.8738637e-11 1.5408370e-15 1.3576781e-17
 9.8943835e-01], sum to 1.0000
[2019-04-08 15:16:13,920] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3893
[2019-04-08 15:16:13,939] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.66481925190673, 0.7132582437387726, 0.0, 1.0, 65.0, 96655.42504897043], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3475200.0000, 
sim time next is 3475800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.69165876556455, 0.7169013085837589, 0.0, 1.0, 65.0, 62581.30853089533], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7243048971303793, 0.738967102861253, 0.0, 1.0, 1.0, 0.2980062310995016], 
reward next is 0.7020, 
noisyNet noise sample is [array([-0.9133432], dtype=float32), -1.8231411]. 
=============================================
[2019-04-08 15:16:14,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.35181464e-15 6.61620163e-07 1.14833461e-02 3.57308500e-02
 5.50834223e-08 5.33935916e-11 1.98525423e-03 1.43440110e-12
 2.47135461e-17 1.33718395e-17 9.50799763e-01], sum to 1.0000
[2019-04-08 15:16:14,065] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2187
[2019-04-08 15:16:14,085] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.1666666666666667, 73.0, 0.0, 0.0, 19.0, 27.3994084502216, 0.969209179992298, 0.0, 1.0, 65.0, 35043.24674887516], 
current ob forecast is [], 
actual action is [4.833333333333333, 65.0], 
sim time this is 3532200.0000, 
sim time next is 3532800.0000, 
raw observation next is [-0.3333333333333333, 74.0, 0.0, 0.0, 19.0, 27.40424970783166, 0.9613345184625208, 0.0, 1.0, 65.0, 34846.49488521084], 
processed observation next is [1.0, 0.9130434782608695, 0.4533702677747, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7836874756526383, 0.8204448394875069, 0.0, 1.0, 1.0, 0.16593568992957544], 
reward next is 0.8341, 
noisyNet noise sample is [array([0.5224174], dtype=float32), -1.0244241]. 
=============================================
[2019-04-08 15:16:14,244] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.51756633e-16 1.19043015e-07 6.22951724e-02 1.60457015e-01
 1.45895840e-09 8.65832406e-12 1.24502962e-03 1.41655376e-14
 1.48213902e-18 1.82447763e-22 7.76002705e-01], sum to 1.0000
[2019-04-08 15:16:14,247] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5953
[2019-04-08 15:16:14,266] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 53.83333333333334, 462.6666666666667, 22.5, 28.76319106413082, 1.24283276738791, 1.0, 1.0, 65.0, 29472.66065512887], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3516000.0000, 
sim time next is 3516600.0000, 
raw observation next is [3.0, 49.0, 45.66666666666667, 400.3333333333334, 22.5, 28.35368757133702, 1.163652635082765, 1.0, 1.0, 64.9999999999999, 20418.28316642307], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.15222222222222223, 0.4423572744014734, 0.375, 0.8628072976114183, 0.887884211694255, 1.0, 1.0, 0.999999999999998, 0.09722991984010985], 
reward next is 0.9028, 
noisyNet noise sample is [array([0.30541682], dtype=float32), -0.5051805]. 
=============================================
[2019-04-08 15:16:14,307] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4242668e-13 2.1442355e-07 1.3860120e-01 4.7953433e-01 2.1514488e-06
 7.8291741e-08 4.2894264e-04 1.6527560e-09 6.8462765e-13 5.3829973e-17
 3.8143313e-01], sum to 1.0000
[2019-04-08 15:16:14,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9232
[2019-04-08 15:16:14,323] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666666, 71.16666666666666, 0.0, 0.0, 19.0, 26.71923142864181, 0.7195357953412834, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 3473400.0000, 
sim time next is 3474000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.68306511345675, 0.7150618567831689, 0.0, 1.0, 65.0, 93360.66491416305], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7235887594547291, 0.7383539522610563, 0.0, 1.0, 1.0, 0.44457459482934786], 
reward next is 0.5554, 
noisyNet noise sample is [array([0.38315699], dtype=float32), 0.2244255]. 
=============================================
[2019-04-08 15:16:14,332] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.20482 ]
 [77.78233 ]
 [76.9742  ]
 [77.88309 ]
 [78.825294]], R is [[77.27265167]
 [77.49992371]
 [77.28137207]
 [77.50856018]
 [77.73347473]].
[2019-04-08 15:16:14,666] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.53237793e-19 3.86978272e-09 3.64424748e-04 6.59301132e-02
 1.05303904e-10 6.13042196e-14 5.64013491e-04 7.43135263e-13
 8.05202488e-20 3.71213519e-23 9.33141410e-01], sum to 1.0000
[2019-04-08 15:16:14,670] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8585
[2019-04-08 15:16:14,679] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 49.5, 110.6666666666667, 797.6666666666666, 22.5, 28.51217493408469, 1.127940548351063, 1.0, 1.0, 65.0, 18846.88811004769], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 3505800.0000, 
sim time next is 3506400.0000, 
raw observation next is [3.0, 49.0, 108.5, 793.5, 22.5, 28.56187547663047, 1.140486713998846, 1.0, 1.0, 65.0, 18847.23604145094], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.49, 0.3616666666666667, 0.8767955801104972, 0.375, 0.8801562897192058, 0.8801622379996153, 1.0, 1.0, 1.0, 0.08974874305452828], 
reward next is 0.9103, 
noisyNet noise sample is [array([-1.1973969], dtype=float32), 1.5296828]. 
=============================================
[2019-04-08 15:16:14,709] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.9631089e-14 1.4880856e-06 6.3332572e-04 4.9861435e-02 1.5096660e-07
 1.8451227e-10 6.0192111e-04 4.6310240e-11 1.5279282e-14 6.6278645e-16
 9.4890171e-01], sum to 1.0000
[2019-04-08 15:16:14,710] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1494
[2019-04-08 15:16:14,728] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 65.5, 0.0, 0.0, 19.0, 26.82873395393603, 0.7932250365713562, 0.0, 1.0, 65.0, 48147.72154741792], 
current ob forecast is [], 
actual action is [2.5, 65.0], 
sim time this is 3547800.0000, 
sim time next is 3548400.0000, 
raw observation next is [-2.666666666666667, 67.33333333333333, 0.0, 0.0, 19.0, 26.81320559051109, 0.7880390672257466, 0.0, 1.0, 65.0, 46419.72940986379], 
processed observation next is [0.0, 0.043478260869565216, 0.38873499538319484, 0.6733333333333333, 0.0, 0.0, 0.08333333333333333, 0.7344337992092574, 0.7626796890752489, 0.0, 1.0, 1.0, 0.2210463305231609], 
reward next is 0.7790, 
noisyNet noise sample is [array([0.8229628], dtype=float32), -0.88343734]. 
=============================================
[2019-04-08 15:16:15,694] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 94284: loss -1.6810
[2019-04-08 15:16:15,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 94284: learning rate 0.0000
[2019-04-08 15:16:15,759] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5380821e-14 2.3221273e-06 5.4476488e-02 7.7349506e-02 5.7541456e-07
 5.5335914e-09 4.6250769e-03 4.1093048e-10 3.5871534e-14 2.2798834e-16
 8.6354601e-01], sum to 1.0000
[2019-04-08 15:16:15,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5954
[2019-04-08 15:16:15,782] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 41.66666666666667, 66.83333333333333, 545.6666666666666, 19.0, 27.36390090667775, 0.9324452337717969, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3601200.0000, 
sim time next is 3601800.0000, 
raw observation next is [0.0, 41.0, 63.0, 515.0, 19.0, 27.37016441261336, 0.9312169442837547, 0.0, 1.0, 65.0, 48679.30014288714], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.41, 0.21, 0.569060773480663, 0.08333333333333333, 0.7808470343844466, 0.8104056480945849, 0.0, 1.0, 1.0, 0.23180619115660545], 
reward next is 0.7682, 
noisyNet noise sample is [array([-1.951955], dtype=float32), 0.7878698]. 
=============================================
[2019-04-08 15:16:16,342] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4387567e-16 1.0476663e-07 3.0711507e-03 7.6086886e-02 2.5235165e-09
 2.5366351e-11 3.8967461e-05 1.3611201e-12 1.5657761e-16 9.7291639e-19
 9.2080289e-01], sum to 1.0000
[2019-04-08 15:16:16,343] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1440
[2019-04-08 15:16:16,366] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 37.5, 338.0, 22.5, 28.64923919409522, 1.181706352371607, 1.0, 1.0, 65.0, 26372.36088299167], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3517200.0000, 
sim time next is 3517800.0000, 
raw observation next is [2.833333333333333, 49.5, 29.33333333333333, 275.6666666666666, 22.5, 28.71301116528251, 1.181530133134156, 1.0, 1.0, 65.0, 19416.3372415722], 
processed observation next is [1.0, 0.7391304347826086, 0.541089566020314, 0.495, 0.09777777777777776, 0.30460405156537745, 0.375, 0.8927509304402091, 0.8938433777113852, 1.0, 1.0, 1.0, 0.09245874876939143], 
reward next is 0.9075, 
noisyNet noise sample is [array([0.3005433], dtype=float32), 0.34862745]. 
=============================================
[2019-04-08 15:16:16,578] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3414527e-12 1.0201743e-06 1.4248780e-03 7.3134000e-03 7.4067731e-07
 5.6400711e-09 7.0112619e-05 3.9801695e-10 2.0611120e-13 5.1747949e-15
 9.9118984e-01], sum to 1.0000
[2019-04-08 15:16:16,583] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0371
[2019-04-08 15:16:16,605] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 19.0, 26.41599670957507, 0.6640413813327568, 0.0, 1.0, 65.0, 58820.58926755004], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3565200.0000, 
sim time next is 3565800.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 19.0, 26.39214782786596, 0.6595676726724643, 0.0, 1.0, 65.0, 53251.71667518006], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6993456523221632, 0.7198558908908215, 0.0, 1.0, 1.0, 0.25357960321514317], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.82712775], dtype=float32), -0.23434238]. 
=============================================
[2019-04-08 15:16:17,002] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2257511e-14 6.7041078e-06 3.3702997e-03 1.2277318e-01 3.9861328e-07
 1.7594176e-09 3.5180806e-04 9.5946348e-11 9.0506377e-15 4.4264510e-16
 8.7349761e-01], sum to 1.0000
[2019-04-08 15:16:17,004] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8831
[2019-04-08 15:16:17,018] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 47.33333333333334, 114.6666666666667, 813.8333333333334, 19.0, 27.0813150888209, 0.8717260846039333, 0.0, 1.0, 65.0, 32798.57865199454], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 3590400.0000, 
sim time next is 3591000.0000, 
raw observation next is [-1.5, 46.0, 114.0, 812.0, 19.0, 27.09962037643741, 0.8770489482917133, 0.0, 1.0, 65.0, 32915.10403232411], 
processed observation next is [0.0, 0.5652173913043478, 0.4210526315789474, 0.46, 0.38, 0.8972375690607735, 0.08333333333333333, 0.7583016980364509, 0.7923496494305712, 0.0, 1.0, 1.0, 0.1567385906301148], 
reward next is 0.8433, 
noisyNet noise sample is [array([-1.7628392], dtype=float32), -0.005827852]. 
=============================================
[2019-04-08 15:16:17,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6445765e-11 5.8641734e-05 3.4433515e-03 6.0196584e-01 2.6671313e-07
 4.7149104e-08 1.3142086e-03 2.1245526e-08 3.7176625e-12 8.8706271e-15
 3.9321759e-01], sum to 1.0000
[2019-04-08 15:16:17,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6574
[2019-04-08 15:16:17,039] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.72079 ]
 [81.66732 ]
 [81.63459 ]
 [81.59334 ]
 [81.623795]], R is [[81.74803925]
 [81.77437592]
 [81.79404449]
 [81.80878448]
 [81.82115936]].
[2019-04-08 15:16:17,055] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 19.0, 26.37538136051934, 0.6427301968916564, 0.0, 1.0, 30.0, 0.0], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3565200.0000, 
sim time next is 3565800.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 19.0, 26.35188611815875, 0.6382457288516202, 0.0, 1.0, 65.0, 101938.1446677412], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6959905098465624, 0.7127485762838734, 0.0, 1.0, 1.0, 0.48541973651305337], 
reward next is 0.5146, 
noisyNet noise sample is [array([-1.2666365], dtype=float32), -0.36276677]. 
=============================================
[2019-04-08 15:16:17,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6183137e-15 1.6749222e-07 2.6329278e-03 9.7297691e-03 1.6789002e-08
 1.5352238e-09 1.2761967e-04 5.1767349e-12 2.7468111e-16 1.2612564e-17
 9.8750955e-01], sum to 1.0000
[2019-04-08 15:16:17,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7396
[2019-04-08 15:16:17,118] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.833333333333333, 48.66666666666667, 115.3333333333333, 815.6666666666666, 19.0, 27.04375298217747, 0.8706777247975402, 0.0, 1.0, 65.0, 43052.32632177205], 
current ob forecast is [], 
actual action is [3.166666666666667, 65.0], 
sim time this is 3589800.0000, 
sim time next is 3590400.0000, 
raw observation next is [-1.666666666666667, 47.33333333333334, 114.6666666666667, 813.8333333333334, 19.0, 27.10390670597202, 0.8730661844802093, 0.0, 1.0, 65.0, 35074.81948915571], 
processed observation next is [0.0, 0.5652173913043478, 0.4164358264081256, 0.47333333333333344, 0.38222222222222235, 0.8992633517495396, 0.08333333333333333, 0.7586588921643349, 0.7910220614934032, 0.0, 1.0, 1.0, 0.1670229499483605], 
reward next is 0.8330, 
noisyNet noise sample is [array([-0.00613872], dtype=float32), 0.67286015]. 
=============================================
[2019-04-08 15:16:17,518] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95208: loss 1.8672
[2019-04-08 15:16:17,518] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95208: learning rate 0.0000
[2019-04-08 15:16:17,561] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 95232: loss 0.0277
[2019-04-08 15:16:17,565] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 95233: learning rate 0.0000
[2019-04-08 15:16:17,621] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 95259: loss 2.3232
[2019-04-08 15:16:17,622] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 95259: learning rate 0.0000
[2019-04-08 15:16:17,716] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95308: loss 0.0204
[2019-04-08 15:16:17,718] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95309: learning rate 0.0000
[2019-04-08 15:16:17,794] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8285069e-14 1.2606290e-05 9.7716395e-03 1.7737938e-01 4.7910266e-06
 4.2457718e-10 4.5417840e-04 1.8467740e-10 4.9276992e-14 9.6223697e-17
 8.1237739e-01], sum to 1.0000
[2019-04-08 15:16:17,796] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-08 15:16:17,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3430645e-12 1.2717863e-04 8.4763039e-03 1.0965068e-01 1.7229722e-05
 2.8327433e-08 8.9697336e-04 1.6502993e-08 7.5574252e-12 7.0899287e-14
 8.8083154e-01], sum to 1.0000
[2019-04-08 15:16:17,810] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6661
[2019-04-08 15:16:17,819] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.1666666666666667, 39.5, 30.33333333333333, 266.3333333333333, 19.0, 27.40648347890065, 0.9161071326803044, 0.0, 1.0, 65.0, 49361.8688473171], 
current ob forecast is [], 
actual action is [4.833333333333333, 65.0], 
sim time this is 3604200.0000, 
sim time next is 3604800.0000, 
raw observation next is [-0.3333333333333333, 40.0, 22.16666666666666, 204.1666666666667, 19.0, 27.38615710080353, 0.9067118131439088, 0.0, 1.0, 65.0, 40313.02744035285], 
processed observation next is [0.0, 0.7391304347826086, 0.4533702677747, 0.4, 0.07388888888888887, 0.22559852670349914, 0.08333333333333333, 0.7821797584002942, 0.8022372710479696, 0.0, 1.0, 1.0, 0.1919667973350136], 
reward next is 0.8080, 
noisyNet noise sample is [array([0.3300468], dtype=float32), 1.5727241]. 
=============================================
[2019-04-08 15:16:17,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1762057e-15 2.1156287e-08 1.1993464e-04 6.3120009e-04 8.7461530e-08
 8.9524048e-11 3.5421755e-05 2.9132341e-12 7.6046014e-16 4.5939816e-18
 9.9921334e-01], sum to 1.0000
[2019-04-08 15:16:17,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5236
[2019-04-08 15:16:17,833] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 40.33333333333334, 54.83333333333334, 452.8333333333334, 19.0, 27.41680746008538, 0.9406711133167046, 0.0, 1.0, 65.0, 30417.5587474708], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3602400.0000, 
sim time next is 3603000.0000, 
raw observation next is [0.0, 39.66666666666666, 46.66666666666667, 390.6666666666667, 19.0, 27.41609859174746, 0.9341124717826865, 0.0, 1.0, 65.0, 30718.89976246273], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.39666666666666656, 0.15555555555555556, 0.43167587476979746, 0.08333333333333333, 0.7846748826456217, 0.8113708239275622, 0.0, 1.0, 1.0, 0.14628047505934633], 
reward next is 0.8537, 
noisyNet noise sample is [array([-0.35390258], dtype=float32), -0.9093246]. 
=============================================
[2019-04-08 15:16:17,837] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 19.0, 27.14766974098787, 0.7347967809954082, 0.0, 1.0, 65.0, 45127.92348157846], 
current ob forecast is [], 
actual action is [13.133333333333333, 65.0], 
sim time this is 3640800.0000, 
sim time next is 3641400.0000, 
raw observation next is [8.1, 28.0, 0.0, 0.0, 19.0, 27.12649608553128, 0.7359815625683829, 0.0, 1.0, 65.0, 38383.03460726667], 
processed observation next is [0.0, 0.13043478260869565, 0.6869806094182825, 0.28, 0.0, 0.0, 0.08333333333333333, 0.7605413404609399, 0.7453271875227943, 0.0, 1.0, 1.0, 0.18277635527269845], 
reward next is 0.8172, 
noisyNet noise sample is [array([0.56825596], dtype=float32), 1.1216419]. 
=============================================
[2019-04-08 15:16:17,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.22109]
 [77.4762 ]
 [77.71115]
 [77.88903]
 [78.06666]], R is [[76.98175049]
 [77.0670929 ]
 [77.15231323]
 [77.23631287]
 [77.31860352]].
[2019-04-08 15:16:18,045] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95475: loss 0.6571
[2019-04-08 15:16:18,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95475: learning rate 0.0000
[2019-04-08 15:16:18,057] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95481: loss 0.1355
[2019-04-08 15:16:18,058] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95481: learning rate 0.0000
[2019-04-08 15:16:18,098] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3439806e-13 1.5607431e-07 9.0378523e-04 2.8291708e-02 1.3063674e-07
 1.4944131e-09 7.5176256e-05 3.9803321e-12 9.2073552e-14 2.4685470e-16
 9.7072899e-01], sum to 1.0000
[2019-04-08 15:16:18,100] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4888
[2019-04-08 15:16:18,115] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.16078376140129, 0.8400492083676268, 0.0, 1.0, 65.0, 37085.24110148807], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3609600.0000, 
sim time next is 3610200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.1368203020992, 0.8442670684424245, 0.0, 1.0, 65.0, 36501.63493178973], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7614016918415999, 0.7814223561474748, 0.0, 1.0, 1.0, 0.17381730919899874], 
reward next is 0.8262, 
noisyNet noise sample is [array([-0.2683299], dtype=float32), 1.034214]. 
=============================================
[2019-04-08 15:16:18,461] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95697: loss 0.0356
[2019-04-08 15:16:18,463] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95697: learning rate 0.0000
[2019-04-08 15:16:18,563] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 95753: loss -2.3180
[2019-04-08 15:16:18,566] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 95755: learning rate 0.0000
[2019-04-08 15:16:18,699] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 95833: loss 0.0127
[2019-04-08 15:16:18,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 95834: learning rate 0.0000
[2019-04-08 15:16:18,901] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95930: loss -4.0787
[2019-04-08 15:16:18,903] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1716290e-13 6.8318337e-07 9.0432598e-04 4.1131001e-02 2.3191300e-08
 3.9971182e-09 3.2767918e-04 2.4637217e-10 1.2876100e-13 1.9797029e-16
 9.5763636e-01], sum to 1.0000
[2019-04-08 15:16:18,904] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95930: learning rate 0.0000
[2019-04-08 15:16:18,904] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3173
[2019-04-08 15:16:18,921] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 42.5, 0.0, 0.0, 19.0, 26.86268038049534, 0.7329860554250273, 0.0, 1.0, 65.0, 59899.69000021032], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3627000.0000, 
sim time next is 3627600.0000, 
raw observation next is [5.0, 36.66666666666667, 0.0, 0.0, 19.0, 26.90361077554739, 0.7389245048353269, 0.0, 1.0, 65.0, 52073.98626519262], 
processed observation next is [0.0, 1.0, 0.6011080332409973, 0.3666666666666667, 0.0, 0.0, 0.08333333333333333, 0.7419675646289491, 0.7463081682784423, 0.0, 1.0, 1.0, 0.24797136316758392], 
reward next is 0.7520, 
noisyNet noise sample is [array([-0.9677652], dtype=float32), -0.087436154]. 
=============================================
[2019-04-08 15:16:19,245] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7790055e-14 3.5542502e-08 2.5762749e-05 2.0369713e-03 5.4019576e-08
 3.0056627e-11 1.9937852e-05 1.4003580e-13 6.7413624e-15 1.7192849e-18
 9.9791723e-01], sum to 1.0000
[2019-04-08 15:16:19,246] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1690
[2019-04-08 15:16:19,262] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.0, 24.0, 113.5, 789.5, 19.0, 27.72814563587738, 0.9616218541283702, 0.0, 1.0, 65.0, 20850.63454434924], 
current ob forecast is [], 
actual action is [17.0, 65.0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [10.66666666666667, 27.5, 114.3333333333333, 798.3333333333334, 19.0, 27.75701562050862, 0.9671263799874797, 0.0, 1.0, 65.0, 20710.07187696398], 
processed observation next is [0.0, 0.4782608695652174, 0.7580794090489382, 0.275, 0.381111111111111, 0.8821362799263353, 0.08333333333333333, 0.8130846350423852, 0.8223754599958265, 0.0, 1.0, 1.0, 0.09861938989030467], 
reward next is 0.9014, 
noisyNet noise sample is [array([-0.8670048], dtype=float32), -0.92509836]. 
=============================================
[2019-04-08 15:16:19,266] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0996036e-13 2.3495379e-07 4.6006442e-04 1.1728777e-03 1.7726772e-07
 4.2838155e-10 4.7477544e-05 1.5114087e-11 6.1194749e-14 5.7547969e-16
 9.9831909e-01], sum to 1.0000
[2019-04-08 15:16:19,267] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8878
[2019-04-08 15:16:19,280] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.96503 ]
 [77.48566 ]
 [76.89012 ]
 [76.615845]
 [76.150345]], R is [[78.80407715]
 [78.91674805]
 [79.02684021]
 [79.13219452]
 [79.23299408]].
[2019-04-08 15:16:19,283] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 27.12362832555504, 0.7605382779176649, 0.0, 1.0, 65.0, 35768.68014914484], 
current ob forecast is [], 
actual action is [14.0, 65.0], 
sim time this is 3630600.0000, 
sim time next is 3631200.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 27.16080503145782, 0.7607700292287971, 0.0, 1.0, 65.0, 35331.67560864783], 
processed observation next is [0.0, 0.0, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.7634004192881516, 0.7535900097429323, 0.0, 1.0, 1.0, 0.16824607432689442], 
reward next is 0.8318, 
noisyNet noise sample is [array([-0.55271536], dtype=float32), -0.8971575]. 
=============================================
[2019-04-08 15:16:19,774] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6000, global step 96376: loss 0.0542
[2019-04-08 15:16:19,776] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6000, global step 96376: learning rate 0.0000
[2019-04-08 15:16:20,139] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0702710e-12 1.2463891e-04 5.4705967e-03 5.8031704e-02 5.6255021e-07
 1.1484223e-08 6.0232254e-03 2.8642650e-09 4.9969506e-14 1.5424216e-14
 9.3034917e-01], sum to 1.0000
[2019-04-08 15:16:20,140] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5237
[2019-04-08 15:16:20,162] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.333333333333334, 26.33333333333334, 0.0, 0.0, 19.0, 27.15169068878737, 0.7403532157263868, 0.0, 1.0, 65.0, 34834.71157847101], 
current ob forecast is [], 
actual action is [14.333333333333334, 65.0], 
sim time this is 3652800.0000, 
sim time next is 3653400.0000, 
raw observation next is [9.166666666666666, 26.66666666666666, 0.0, 0.0, 19.0, 27.14972390489594, 0.7446728778209123, 0.0, 1.0, 65.0, 34862.128740582], 
processed observation next is [0.0, 0.2608695652173913, 0.7165281625115422, 0.2666666666666666, 0.0, 0.0, 0.08333333333333333, 0.7624769920746616, 0.7482242926069708, 0.0, 1.0, 1.0, 0.16601013685991428], 
reward next is 0.8340, 
noisyNet noise sample is [array([-2.0900192], dtype=float32), -0.42795625]. 
=============================================
[2019-04-08 15:16:20,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4217074e-12 1.0948598e-06 7.7037499e-03 1.2539709e-02 2.4599158e-06
 1.7054342e-08 8.1739342e-03 3.3723264e-09 3.0624446e-12 4.1096389e-14
 9.7157907e-01], sum to 1.0000
[2019-04-08 15:16:20,182] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7101
[2019-04-08 15:16:20,196] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.1, 28.0, 0.0, 0.0, 19.0, 27.13197113518731, 0.7339535370602445, 0.0, 1.0, 65.0, 34672.79211350471], 
current ob forecast is [], 
actual action is [13.1, 65.0], 
sim time this is 3641400.0000, 
sim time next is 3642000.0000, 
raw observation next is [8.066666666666666, 28.33333333333334, 0.0, 0.0, 19.0, 27.15814798480359, 0.7318278902613242, 0.0, 1.0, 65.0, 35288.07114713977], 
processed observation next is [0.0, 0.13043478260869565, 0.6860572483841183, 0.2833333333333334, 0.0, 0.0, 0.08333333333333333, 0.7631789987336326, 0.743942630087108, 0.0, 1.0, 1.0, 0.16803843403399893], 
reward next is 0.8320, 
noisyNet noise sample is [array([-0.86417633], dtype=float32), 1.2040693]. 
=============================================
[2019-04-08 15:16:20,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.09513 ]
 [66.075745]
 [66.00961 ]
 [66.00299 ]
 [66.16847 ]], R is [[66.31813812]
 [66.48985291]
 [66.66152954]
 [66.8208313 ]
 [66.97927094]].
[2019-04-08 15:16:20,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.07052995e-11 2.69002310e-04 2.01886776e-03 1.31401867e-01
 9.48712204e-06 2.33820359e-07 1.19748572e-03 4.98272108e-08
 2.98928872e-12 7.58383072e-15 8.65103006e-01], sum to 1.0000
[2019-04-08 15:16:20,290] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5521
[2019-04-08 15:16:20,316] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.866666666666667, 25.33333333333334, 0.0, 0.0, 19.0, 27.12885142567708, 0.7463419896677341, 0.0, 1.0, 65.0, 35664.01449045721], 
current ob forecast is [], 
actual action is [13.866666666666667, 65.0], 
sim time this is 3636600.0000, 
sim time next is 3637200.0000, 
raw observation next is [8.733333333333334, 25.66666666666667, 0.0, 0.0, 19.0, 27.13690549163384, 0.7461430701101112, 0.0, 1.0, 65.0, 35672.38893976402], 
processed observation next is [0.0, 0.08695652173913043, 0.7045244690674055, 0.2566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7614087909694867, 0.7487143567033704, 0.0, 1.0, 1.0, 0.16986851876078107], 
reward next is 0.8301, 
noisyNet noise sample is [array([-1.1942722], dtype=float32), 0.8041592]. 
=============================================
[2019-04-08 15:16:20,604] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3362990e-11 2.6785576e-05 6.0843775e-04 3.7430450e-02 3.4527427e-06
 1.8955092e-07 3.2161886e-03 1.5928029e-09 7.3854666e-14 5.1209372e-14
 9.5871449e-01], sum to 1.0000
[2019-04-08 15:16:20,607] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9543
[2019-04-08 15:16:20,635] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 19.0, 27.09819772535978, 0.7383354911249392, 0.0, 1.0, 65.0, 35381.55996435189], 
current ob forecast is [], 
actual action is [14.833333333333334, 65.0], 
sim time this is 3649800.0000, 
sim time next is 3650400.0000, 
raw observation next is [10.0, 25.0, 0.0, 0.0, 19.0, 27.13264108303216, 0.7441172986129388, 0.0, 1.0, 65.0, 34739.35093218495], 
processed observation next is [0.0, 0.2608695652173913, 0.739612188365651, 0.25, 0.0, 0.0, 0.08333333333333333, 0.7610534235860132, 0.7480390995376464, 0.0, 1.0, 1.0, 0.16542548062945214], 
reward next is 0.8346, 
noisyNet noise sample is [array([-1.1109833], dtype=float32), -1.0711094]. 
=============================================
[2019-04-08 15:16:20,640] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96827: loss -7.7521
[2019-04-08 15:16:20,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96828: learning rate 0.0000
[2019-04-08 15:16:21,087] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 97067: loss -0.2270
[2019-04-08 15:16:21,087] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 97067: learning rate 0.0000
[2019-04-08 15:16:22,590] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.11089786e-14 2.08297280e-07 1.36695089e-04 4.97619202e-03
 1.65695246e-09 4.16253365e-10 9.87064413e-05 1.15471877e-11
 9.60023543e-16 1.17568411e-16 9.94788170e-01], sum to 1.0000
[2019-04-08 15:16:22,594] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2344
[2019-04-08 15:16:22,608] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 62.83333333333333, 0.0, 0.0, 19.0, 27.60645637522759, 0.9342098724775592, 0.0, 1.0, 65.0, 33328.34547429586], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 3701400.0000, 
sim time next is 3702000.0000, 
raw observation next is [2.666666666666667, 62.66666666666667, 0.0, 0.0, 19.0, 27.58557028989076, 0.9334223578006382, 0.0, 1.0, 65.0, 33654.68380629096], 
processed observation next is [0.0, 0.8695652173913043, 0.5364727608494922, 0.6266666666666667, 0.0, 0.0, 0.08333333333333333, 0.7987975241575634, 0.8111407859335461, 0.0, 1.0, 1.0, 0.160260399077576], 
reward next is 0.8397, 
noisyNet noise sample is [array([0.21091816], dtype=float32), -0.4122674]. 
=============================================
[2019-04-08 15:16:22,615] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.88856]
 [77.9686 ]
 [78.07463]
 [78.22031]
 [78.32942]], R is [[77.90568542]
 [77.96792603]
 [78.03120422]
 [78.09555054]
 [78.16124725]].
[2019-04-08 15:16:22,849] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 98044: loss -0.8672
[2019-04-08 15:16:22,851] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 98045: learning rate 0.0000
[2019-04-08 15:16:23,184] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7783006e-14 1.7665440e-07 1.4679200e-03 3.5438887e-03 1.2016285e-07
 6.8812939e-10 4.8839800e-05 1.6877357e-11 5.8832977e-15 2.0764075e-17
 9.9493909e-01], sum to 1.0000
[2019-04-08 15:16:23,188] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5857
[2019-04-08 15:16:23,208] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 19.0, 27.11866539935883, 0.8143067349677198, 0.0, 1.0, 65.0, 41677.59678671407], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3713400.0000, 
sim time next is 3714000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 19.0, 27.098138730595, 0.8097472781147491, 0.0, 1.0, 65.0, 41822.98934121174], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7581782275495833, 0.7699157593715831, 0.0, 1.0, 1.0, 0.19915709210100827], 
reward next is 0.8008, 
noisyNet noise sample is [array([-0.3646536], dtype=float32), -0.2138689]. 
=============================================
[2019-04-08 15:16:23,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.35223 ]
 [80.25577 ]
 [80.42746 ]
 [80.564835]
 [80.64009 ]], R is [[80.31380463]
 [80.31220245]
 [80.31244659]
 [80.31506348]
 [80.32086182]].
[2019-04-08 15:16:23,717] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6000, global step 98529: loss 0.1490
[2019-04-08 15:16:23,718] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6000, global step 98530: learning rate 0.0000
[2019-04-08 15:16:24,066] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0431760e-14 1.2451645e-07 2.0778035e-04 6.4155245e-03 5.7358662e-08
 2.0877713e-09 5.0621042e-03 2.2877139e-12 2.1860735e-15 3.0020564e-18
 9.8831439e-01], sum to 1.0000
[2019-04-08 15:16:24,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8159
[2019-04-08 15:16:24,085] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.02678890028197, 0.7302713952257243, 0.0, 1.0, 65.0, 42373.49890691397], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3729000.0000, 
sim time next is 3729600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.99076425257612, 0.7336589508479191, 0.0, 1.0, 65.0, 43909.60189561622], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7492303543813433, 0.7445529836159731, 0.0, 1.0, 1.0, 0.20909334236007726], 
reward next is 0.7909, 
noisyNet noise sample is [array([0.5117184], dtype=float32), 1.97945]. 
=============================================
[2019-04-08 15:16:24,251] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.8236223e-15 3.5724136e-07 6.9909956e-04 6.9821705e-05 1.5560817e-08
 9.4854770e-11 1.9816502e-05 1.0176271e-11 3.2762845e-16 2.2946860e-17
 9.9921083e-01], sum to 1.0000
[2019-04-08 15:16:24,254] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6756
[2019-04-08 15:16:24,274] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 19.0, 27.12941498056933, 0.8185605085837565, 0.0, 1.0, 65.0, 41589.64485412056], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3713400.0000, 
sim time next is 3714000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 19.0, 27.10850167596226, 0.8138721301048099, 0.0, 1.0, 65.0, 41684.27023408251], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7590418063301883, 0.7712907100349367, 0.0, 1.0, 1.0, 0.19849652492420244], 
reward next is 0.8015, 
noisyNet noise sample is [array([0.06860933], dtype=float32), 2.0533726]. 
=============================================
[2019-04-08 15:16:24,283] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.22977 ]
 [80.31779 ]
 [80.372986]
 [80.46892 ]
 [80.55844 ]], R is [[80.20359802]
 [80.20352173]
 [80.20517731]
 [80.20952606]
 [80.21703339]].
[2019-04-08 15:16:24,366] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2660168e-13 5.3451906e-08 1.4135738e-04 1.4240970e-02 7.1593433e-08
 2.8371175e-10 1.2333343e-03 2.8823259e-12 4.1518516e-14 1.6508285e-17
 9.8438418e-01], sum to 1.0000
[2019-04-08 15:16:24,371] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9212
[2019-04-08 15:16:24,385] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.97602590743648, 0.7406148480367755, 0.0, 1.0, 65.0, 44132.24706243666], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3725400.0000, 
sim time next is 3726000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.95290386267679, 0.7341743962199971, 0.0, 1.0, 65.0, 44440.1447584109], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7460753218897326, 0.7447247987399991, 0.0, 1.0, 1.0, 0.21161973694481379], 
reward next is 0.7884, 
noisyNet noise sample is [array([-1.0208902], dtype=float32), -0.6213772]. 
=============================================
[2019-04-08 15:16:24,402] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.429535]
 [81.13003 ]
 [80.936806]
 [80.68969 ]
 [80.72436 ]], R is [[81.77461243]
 [81.74671173]
 [81.72310638]
 [81.69714355]
 [81.66026306]].
[2019-04-08 15:16:25,408] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.16526161e-19 1.22069056e-11 1.58455876e-07 1.60426714e-06
 1.33244818e-10 1.54157053e-15 1.11209156e-04 1.67640119e-14
 2.25385036e-20 3.21766846e-22 9.99886990e-01], sum to 1.0000
[2019-04-08 15:16:25,409] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5669
[2019-04-08 15:16:25,432] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 22.5, 27.93075260918617, 0.9299859182864227, 1.0, 1.0, 65.0, 23840.10860197116], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3751800.0000, 
sim time next is 3752400.0000, 
raw observation next is [-3.0, 75.0, 109.1666666666667, 753.3333333333334, 22.5, 27.98613506755115, 0.9471814772057261, 1.0, 1.0, 65.0, 23260.24725643301], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.75, 0.363888888888889, 0.8324125230202579, 0.375, 0.8321779222959291, 0.8157271590685754, 1.0, 1.0, 1.0, 0.11076308217349053], 
reward next is 0.8892, 
noisyNet noise sample is [array([1.2129356], dtype=float32), -1.1749711]. 
=============================================
[2019-04-08 15:16:26,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0165273e-18 8.7539725e-10 3.7244749e-06 1.7955882e-04 1.3990775e-10
 5.6130291e-12 1.0447759e-05 2.4391652e-15 1.3745106e-21 3.5696540e-24
 9.9980634e-01], sum to 1.0000
[2019-04-08 15:16:26,215] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9770
[2019-04-08 15:16:26,228] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 60.00000000000001, 90.16666666666666, 721.8333333333333, 22.5, 28.68958980805536, 1.176065753578597, 1.0, 1.0, 65.0, 18848.36914744735], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3770400.0000, 
sim time next is 3771000.0000, 
raw observation next is [0.0, 60.0, 87.0, 711.0, 22.5, 28.74273306857086, 1.188807842057253, 1.0, 1.0, 65.0, 18848.68923086274], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.29, 0.7856353591160221, 0.375, 0.8952277557142384, 0.896269280685751, 1.0, 1.0, 1.0, 0.08975566300410828], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.1450075], dtype=float32), 0.5493189]. 
=============================================
[2019-04-08 15:16:26,241] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[112.25194 ]
 [112.28078 ]
 [112.283844]
 [112.336365]
 [112.37417 ]], R is [[111.96894073]
 [111.7594986 ]
 [111.55216217]
 [111.31928253]
 [110.95960999]].
[2019-04-08 15:16:26,352] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.7952468e-18 4.2470187e-09 2.6674559e-06 2.2794062e-04 2.0603292e-11
 8.3381869e-14 6.4074214e-07 1.7666037e-15 1.2443425e-21 5.8058250e-25
 9.9976879e-01], sum to 1.0000
[2019-04-08 15:16:26,352] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7160
[2019-04-08 15:16:26,384] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 75.0, 109.1666666666667, 753.3333333333334, 22.5, 28.00134827917686, 0.9540796491264808, 1.0, 1.0, 65.0, 22935.57606059803], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3752400.0000, 
sim time next is 3753000.0000, 
raw observation next is [-3.0, 74.0, 111.0, 769.0, 22.5, 28.05684692873619, 0.969480251044924, 1.0, 1.0, 65.0, 22236.36805321692], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.74, 0.37, 0.8497237569060774, 0.375, 0.8380705773946824, 0.8231600836816413, 1.0, 1.0, 1.0, 0.10588746692008057], 
reward next is 0.8941, 
noisyNet noise sample is [array([-2.3599803], dtype=float32), 0.6667589]. 
=============================================
[2019-04-08 15:16:26,391] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[105.62849]
 [104.56355]
 [103.76598]
 [102.88996]
 [101.41366]], R is [[105.49111938]
 [105.32699585]
 [105.16207886]
 [104.99534607]
 [104.82183075]].
[2019-04-08 15:16:26,502] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-08 15:16:26,505] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:16:26,505] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:16:26,507] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run6
[2019-04-08 15:16:26,521] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:16:26,522] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:16:26,523] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:16:26,523] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:16:26,525] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run6
[2019-04-08 15:16:26,539] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run6
[2019-04-08 15:17:04,113] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.057041347]
[2019-04-08 15:17:04,113] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [15.1, 59.0, 198.5, 344.0, 19.0, 28.2741061365537, 1.265943340124577, 0.0, 1.0, 65.0, 18848.97494224384]
[2019-04-08 15:17:04,113] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:17:04,114] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [2.9592750e-17 4.5877626e-09 7.8184406e-05 7.4845128e-04 1.4783846e-09
 2.8338566e-12 8.7385979e-06 6.7972585e-14 2.6992740e-18 9.8117927e-21
 9.9916458e-01], sampled 0.7781624165403952
[2019-04-08 15:17:31,923] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.057041347]
[2019-04-08 15:17:31,923] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-1.130891233, 57.08662486, 17.59145249, 583.61143275, 22.5, 27.96372933300879, 0.8629420208148256, 1.0, 1.0, 65.0, 39916.97134850226]
[2019-04-08 15:17:31,924] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:17:31,925] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [9.8793258e-18 3.5216712e-09 9.3990173e-05 7.5456389e-04 7.5905515e-10
 1.3780760e-12 7.0423557e-06 1.9415554e-14 6.7750481e-19 1.9648470e-21
 9.9914443e-01], sampled 0.40251831443334984
[2019-04-08 15:17:54,482] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6991.2206 316003677.1565 2957.6257
[2019-04-08 15:17:54,502] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:17:54,502] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:17:54,502] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:17:54,502] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:17:54,502] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:17:54,502] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:17:54,618] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:17:54,618] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:17:54,618] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:17:54,618] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:17:54,618] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:17:54,618] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:00,696] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.9982 355740383.0588 2370.0720
[2019-04-08 15:18:00,729] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:00,729] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:00,729] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:00,729] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:00,729] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:00,729] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:00,907] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:00,907] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:00,907] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:00,907] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:00,907] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:00,907] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:06,634] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6864.0495 342709594.5792 2767.6160
[2019-04-08 15:18:06,669] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:06,669] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:06,669] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:06,669] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:06,669] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:06,669] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:18:06,823] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:06,823] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:06,823] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:06,823] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:06,823] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:06,823] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:18:07,670] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 100000, evaluation results [100000.0, 6864.049549622886, 342709594.5792011, 2767.6160148732374, 6991.220584969171, 316003677.1564768, 2957.6256599671915, 6801.998175910536, 355740383.0587898, 2370.0719546249416]
[2019-04-08 15:18:08,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0830716e-16 5.1307860e-09 6.0074080e-06 1.0234673e-02 1.2020728e-08
 3.4471391e-12 4.1356702e-07 4.2601879e-13 1.7915364e-17 4.3538171e-21
 9.8975885e-01], sum to 1.0000
[2019-04-08 15:18:08,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1368
[2019-04-08 15:18:09,040] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 73.0, 0.0, 0.0, 19.0, 27.01465484432028, 0.8429514419048377, 0.0, 1.0, 65.0, 45377.26307099214], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3796800.0000, 
sim time next is 3797400.0000, 
raw observation next is [-3.0, 72.0, 0.0, 0.0, 19.0, 26.97872333788408, 0.8354637171294031, 0.0, 1.0, 65.0, 45376.88227438505], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7482269448236734, 0.7784879057098011, 0.0, 1.0, 1.0, 0.21608039178278593], 
reward next is 0.7839, 
noisyNet noise sample is [array([-0.45885253], dtype=float32), -1.2303519]. 
=============================================
[2019-04-08 15:18:09,806] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3702508e-18 1.4253767e-09 7.5754528e-05 1.8309054e-04 1.2228037e-09
 7.7761080e-15 4.7312886e-07 4.0194258e-17 8.2379744e-20 1.8596135e-23
 9.9974066e-01], sum to 1.0000
[2019-04-08 15:18:09,809] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2702
[2019-04-08 15:18:09,837] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 22.5, 28.00010660499864, 1.044612969646295, 1.0, 1.0, 65.0, 24389.90130134791], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3782400.0000, 
sim time next is 3783000.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 22.5, 27.93387820002748, 1.040130035740639, 1.0, 1.0, 65.0, 27631.18206978318], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.375, 0.8278231833356232, 0.8467100119135463, 1.0, 1.0, 1.0, 0.131577057475158], 
reward next is 0.8684, 
noisyNet noise sample is [array([0.30252078], dtype=float32), -2.0235646]. 
=============================================
[2019-04-08 15:18:09,841] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[ 98.69343]
 [ 99.17101]
 [ 99.57638]
 [100.0897 ]
 [100.60862]], R is [[98.24452209]
 [98.14593506]
 [98.06046295]
 [97.93830872]
 [97.67657471]].
[2019-04-08 15:18:09,884] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.4629918e-20 1.7934029e-11 6.3232490e-04 2.2015387e-04 3.8915665e-10
 1.3903311e-13 4.6897139e-06 5.3625714e-16 4.8970110e-20 4.6775867e-23
 9.9914277e-01], sum to 1.0000
[2019-04-08 15:18:09,888] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4911
[2019-04-08 15:18:09,906] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3333333333333334, 60.0, 113.5, 811.0, 22.5, 28.33838454592948, 1.114599952562022, 1.0, 1.0, 65.0, 18844.13202615846], 
current ob forecast is [], 
actual action is [4.666666666666667, 65.0], 
sim time this is 3764400.0000, 
sim time next is 3765000.0000, 
raw observation next is [-0.1666666666666666, 60.0, 112.0, 804.0, 22.5, 28.43579617678519, 1.128404592925557, 1.0, 1.0, 65.0, 18844.19188497565], 
processed observation next is [1.0, 0.5652173913043478, 0.4579870729455217, 0.6, 0.37333333333333335, 0.8883977900552487, 0.375, 0.8696496813987658, 0.8761348643085191, 1.0, 1.0, 1.0, 0.08973424707131263], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.0269976], dtype=float32), -1.9519976]. 
=============================================
[2019-04-08 15:18:09,912] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[104.665146]
 [104.62909 ]
 [104.62662 ]
 [104.585335]
 [104.5574  ]], R is [[104.54259491]
 [104.40743256]
 [104.26917267]
 [104.10197449]
 [103.94139862]].
[2019-04-08 15:18:10,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2907035e-14 3.7760464e-07 8.4368931e-04 4.3989155e-03 9.8173327e-09
 1.7531080e-09 2.1610291e-05 1.9468177e-11 3.0945398e-16 9.9704041e-18
 9.9473542e-01], sum to 1.0000
[2019-04-08 15:18:10,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7532
[2019-04-08 15:18:10,131] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.1959274326581, 0.8346336910633485, 0.0, 1.0, 65.0, 40103.48205376553], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3711600.0000, 
sim time next is 3712200.0000, 
raw observation next is [-3.0, 66.0, 0.0, 0.0, 19.0, 27.1859460821462, 0.829187442699094, 0.0, 1.0, 65.0, 40584.12597400082], 
processed observation next is [0.0, 1.0, 0.3795013850415513, 0.66, 0.0, 0.0, 0.08333333333333333, 0.7654955068455166, 0.7763958142330313, 0.0, 1.0, 1.0, 0.19325774273333723], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.9087766], dtype=float32), -0.9082683]. 
=============================================
[2019-04-08 15:18:10,382] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2233071e-17 3.6921559e-09 3.8080472e-03 2.1243116e-03 1.7597181e-10
 4.8562313e-12 2.5705125e-05 4.9077001e-14 1.0912724e-20 2.7937263e-22
 9.9404198e-01], sum to 1.0000
[2019-04-08 15:18:10,383] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2452
[2019-04-08 15:18:10,403] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 60.00000000000001, 90.16666666666666, 721.8333333333333, 22.5, 28.69290890167454, 1.176958209278167, 1.0, 1.0, 65.0, 18848.40207819029], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3770400.0000, 
sim time next is 3771000.0000, 
raw observation next is [0.0, 60.0, 87.0, 711.0, 22.5, 28.74616342368258, 1.189652546080259, 1.0, 1.0, 65.0, 18848.72081206903], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.29, 0.7856353591160221, 0.375, 0.8955136186402152, 0.8965508486934196, 1.0, 1.0, 1.0, 0.0897558133908049], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.95352125], dtype=float32), 1.2381358]. 
=============================================
[2019-04-08 15:18:10,413] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[106.04019]
 [106.06593]
 [106.07344]
 [106.12686]
 [106.16636]], R is [[105.82291412]
 [105.67493439]
 [105.52844238]
 [105.35604858]
 [105.05680084]].
[2019-04-08 15:18:10,922] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3926961e-17 1.8057211e-09 9.5882478e-06 3.5891784e-03 5.7464939e-11
 1.7587993e-12 2.3843137e-07 7.4802095e-13 5.6177449e-19 6.8052747e-21
 9.9640095e-01], sum to 1.0000
[2019-04-08 15:18:10,923] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5947
[2019-04-08 15:18:10,939] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 26.91656089781191, 0.8230716191947228, 0.0, 1.0, 65.0, 46401.77834171284], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3799200.0000, 
sim time next is 3799800.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 26.92281960139604, 0.8165016133701091, 0.0, 1.0, 65.0, 46049.32618811681], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7435683001163366, 0.7721672044567031, 0.0, 1.0, 1.0, 0.21928250565769908], 
reward next is 0.7807, 
noisyNet noise sample is [array([-1.2432729], dtype=float32), -2.299615]. 
=============================================
[2019-04-08 15:18:11,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.28238799e-15 4.32584741e-08 9.04161643e-05 2.68632866e-04
 1.17099352e-09 2.08471435e-11 1.15913945e-05 1.19018326e-12
 2.65560314e-16 7.32910043e-19 9.99629259e-01], sum to 1.0000
[2019-04-08 15:18:11,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9690
[2019-04-08 15:18:11,309] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.5903038378719, 0.6870961085290476, 0.0, 1.0, 65.0, 51796.68569733625], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3818400.0000, 
sim time next is 3819000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.57004190022782, 0.6865901478510456, 0.0, 1.0, 65.0, 51799.0132655209], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7141701583523185, 0.7288633826170151, 0.0, 1.0, 1.0, 0.2466619679310519], 
reward next is 0.7533, 
noisyNet noise sample is [array([0.5806203], dtype=float32), -0.19776045]. 
=============================================
[2019-04-08 15:18:11,328] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.09899 ]
 [78.117485]
 [78.17324 ]
 [78.18487 ]
 [78.21024 ]], R is [[78.03377533]
 [78.00679016]
 [77.98416901]
 [77.96601868]
 [77.94915009]].
[2019-04-08 15:18:12,022] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5306131e-14 7.4327112e-08 1.6907146e-04 1.4104069e-03 7.6715727e-09
 4.8305759e-11 8.6907603e-05 1.0427643e-11 2.3955263e-15 1.7846218e-17
 9.9833351e-01], sum to 1.0000
[2019-04-08 15:18:12,027] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7066
[2019-04-08 15:18:12,075] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.66907827936926, 0.6996866163178505, 0.0, 1.0, 65.0, 49917.19158289069], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3817200.0000, 
sim time next is 3817800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.63729278688466, 0.6936888477994968, 0.0, 1.0, 65.0, 50808.6415270207], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7197743989070551, 0.7312296159331656, 0.0, 1.0, 1.0, 0.2419459120334319], 
reward next is 0.7581, 
noisyNet noise sample is [array([0.206026], dtype=float32), 0.4806279]. 
=============================================
[2019-04-08 15:18:12,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3983016e-15 1.0975458e-08 1.3298850e-04 1.6392978e-03 3.1571237e-09
 6.6489600e-12 5.5559080e-06 1.2864612e-13 2.1447950e-16 1.0960549e-18
 9.9822217e-01], sum to 1.0000
[2019-04-08 15:18:12,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8529
[2019-04-08 15:18:12,136] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 77.0, 33.66666666666666, 248.3333333333333, 22.5, 26.46083033964913, 0.6629890434276157, 1.0, 1.0, 65.0, 50319.12702412492], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3829800.0000, 
sim time next is 3830400.0000, 
raw observation next is [-5.0, 77.0, 48.0, 298.0, 22.5, 26.45255158945097, 0.6675369404835942, 1.0, 1.0, 65.0, 50006.99583389772], 
processed observation next is [1.0, 0.34782608695652173, 0.32409972299168976, 0.77, 0.16, 0.3292817679558011, 0.375, 0.7043792991209141, 0.7225123134945314, 1.0, 1.0, 1.0, 0.23812855158998916], 
reward next is 0.7619, 
noisyNet noise sample is [array([0.06166532], dtype=float32), 1.673633]. 
=============================================
[2019-04-08 15:18:13,103] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.7320969e-14 2.0766283e-08 5.3248857e-04 9.6230106e-03 1.5540150e-08
 4.2908913e-10 2.8483053e-05 3.7319002e-12 2.9804830e-15 4.4174509e-17
 9.8981595e-01], sum to 1.0000
[2019-04-08 15:18:13,106] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5631
[2019-04-08 15:18:13,141] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.74106323321398, 0.7328203913992487, 0.0, 1.0, 65.0, 48410.23191846972], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3814200.0000, 
sim time next is 3814800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.70961009111945, 0.7255605020292001, 0.0, 1.0, 65.0, 48848.44551118393], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7258008409266209, 0.7418535006764001, 0.0, 1.0, 1.0, 0.23261164529135203], 
reward next is 0.7674, 
noisyNet noise sample is [array([1.6809396], dtype=float32), -0.4613923]. 
=============================================
[2019-04-08 15:18:13,509] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0673993e-19 9.5437314e-10 3.6778703e-05 4.6772879e-04 2.2781856e-11
 2.7310770e-14 1.8306682e-05 3.0134000e-15 5.4367506e-19 1.5764777e-21
 9.9947721e-01], sum to 1.0000
[2019-04-08 15:18:13,521] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0984
[2019-04-08 15:18:13,554] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1670325e-17 1.2317416e-09 5.5992317e-05 1.9062577e-04 3.3456019e-10
 7.5265261e-14 7.9882767e-07 6.0242878e-16 2.0940098e-19 5.7991406e-23
 9.9975258e-01], sum to 1.0000
[2019-04-08 15:18:13,556] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8670
[2019-04-08 15:18:13,592] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.5, 101.0, 680.0, 22.5, 27.48636722462225, 0.8738085491017241, 1.0, 1.0, 65.0, 28121.54316062725], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3835800.0000, 
sim time next is 3836400.0000, 
raw observation next is [-2.666666666666667, 63.66666666666667, 102.5, 695.8333333333334, 22.5, 27.6310238491594, 0.8945728725352126, 1.0, 1.0, 65.0, 26163.69960545336], 
processed observation next is [1.0, 0.391304347826087, 0.38873499538319484, 0.6366666666666667, 0.3416666666666667, 0.768876611418048, 0.375, 0.8025853207632835, 0.7981909575117375, 1.0, 1.0, 1.0, 0.12458904574025409], 
reward next is 0.8754, 
noisyNet noise sample is [array([-0.9131417], dtype=float32), -0.9547564]. 
=============================================
[2019-04-08 15:18:13,594] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 48.0, 96.5, 749.5, 22.5, 27.95435363624992, 1.175975512902814, 1.0, 1.0, 65.0, 41921.34879177424], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3855600.0000, 
sim time next is 3856200.0000, 
raw observation next is [2.166666666666667, 47.5, 93.33333333333334, 738.6666666666667, 22.5, 27.47611678199467, 1.090052334457953, 1.0, 1.0, 64.99999999999999, 25361.89471715084], 
processed observation next is [1.0, 0.6521739130434783, 0.5226223453370269, 0.475, 0.3111111111111111, 0.816206261510129, 0.375, 0.7896763984995557, 0.8633507781526509, 1.0, 1.0, 0.9999999999999997, 0.12077092722452781], 
reward next is 0.8792, 
noisyNet noise sample is [array([-0.28362224], dtype=float32), 1.6201893]. 
=============================================
[2019-04-08 15:18:13,990] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4820667e-18 9.8335589e-09 2.5186532e-03 1.0065763e-03 2.9185709e-10
 1.2002447e-13 9.6009353e-06 1.8983931e-14 3.9914120e-18 3.5975240e-22
 9.9646509e-01], sum to 1.0000
[2019-04-08 15:18:13,990] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4960
[2019-04-08 15:18:13,999] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 22.5, 28.69485353608972, 1.193990636662223, 1.0, 1.0, 65.0, 18848.70422587762], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 3858600.0000, 
sim time next is 3859200.0000, 
raw observation next is [3.0, 45.0, 75.5, 634.0, 22.5, 28.74075268830795, 1.199370790626776, 1.0, 1.0, 65.0, 18848.48640153544], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.25166666666666665, 0.7005524861878453, 0.375, 0.8950627240256624, 0.8997902635422587, 1.0, 1.0, 1.0, 0.08975469715016876], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.8739341], dtype=float32), 0.3077066]. 
=============================================
[2019-04-08 15:18:14,035] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 102308: loss 0.0160
[2019-04-08 15:18:14,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 102308: learning rate 0.0000
[2019-04-08 15:18:15,408] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8502567e-13 4.2166232e-07 1.2145097e-04 7.3087625e-03 1.8568426e-09
 3.4938116e-10 2.1629596e-04 1.9357509e-10 8.3690291e-14 2.0019633e-16
 9.9235314e-01], sum to 1.0000
[2019-04-08 15:18:15,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6479
[2019-04-08 15:18:15,456] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.57898769387246, 0.689345265700947, 0.0, 1.0, 65.0, 51688.03848378204], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3819000.0000, 
sim time next is 3819600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.58752526778755, 0.6974009774303861, 0.0, 1.0, 65.0, 50930.85140128425], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7156271056489626, 0.7324669924767954, 0.0, 1.0, 1.0, 0.2425278638156393], 
reward next is 0.7575, 
noisyNet noise sample is [array([0.5496411], dtype=float32), -1.0613565]. 
=============================================
[2019-04-08 15:18:15,705] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.3753778e-18 4.8240660e-09 3.9436469e-05 5.3487584e-04 3.5642860e-11
 2.9154051e-12 5.3858730e-06 6.1021397e-15 9.3694528e-20 1.2053072e-20
 9.9942029e-01], sum to 1.0000
[2019-04-08 15:18:15,705] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8067
[2019-04-08 15:18:15,789] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3333333333333334, 57.00000000000001, 117.0, 832.8333333333334, 22.5, 28.01339127011191, 1.07284816951814, 1.0, 1.0, 65.0, 40765.74429512399], 
current ob forecast is [], 
actual action is [4.666666666666667, 65.0], 
sim time this is 3846000.0000, 
sim time next is 3846600.0000, 
raw observation next is [0.0, 55.5, 117.0, 835.0, 22.5, 27.58697874922985, 1.018532373010308, 1.0, 1.0, 64.99999999999993, 28270.76321905695], 
processed observation next is [1.0, 0.5217391304347826, 0.46260387811634357, 0.555, 0.39, 0.9226519337016574, 0.375, 0.7989148957691542, 0.839510791003436, 1.0, 1.0, 0.9999999999999986, 0.13462268199550928], 
reward next is 0.8654, 
noisyNet noise sample is [array([0.98045677], dtype=float32), 0.574543]. 
=============================================
[2019-04-08 15:18:16,198] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.01447761e-18 5.55761703e-09 7.56131485e-05 1.74194272e-03
 1.33390882e-10 1.55918559e-13 2.11103179e-05 1.36373753e-14
 5.74760824e-18 1.31360256e-20 9.98161376e-01], sum to 1.0000
[2019-04-08 15:18:16,198] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7457
[2019-04-08 15:18:16,257] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.666666666666667, 49.0, 0.0, 0.0, 22.5, 27.53284412676063, 1.081424350814695, 1.0, 1.0, 65.0, 56798.71923806923], 
current ob forecast is [], 
actual action is [6.666666666666667, 65.0], 
sim time this is 3867600.0000, 
sim time next is 3868200.0000, 
raw observation next is [1.5, 49.5, 0.0, 0.0, 22.5, 26.82355004265317, 0.9802834824335424, 1.0, 1.0, 64.99999999999999, 38692.71900245041], 
processed observation next is [1.0, 0.782608695652174, 0.5041551246537397, 0.495, 0.0, 0.0, 0.375, 0.7352958368877642, 0.8267611608111808, 1.0, 1.0, 0.9999999999999997, 0.18425104286881147], 
reward next is 0.8157, 
noisyNet noise sample is [array([-1.1502728], dtype=float32), 0.005423703]. 
=============================================
[2019-04-08 15:18:16,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8679590e-14 1.4483624e-06 2.1597750e-04 4.3351684e-02 1.4019935e-06
 3.3056151e-09 4.8140294e-04 1.2559869e-09 2.5063021e-14 7.2919431e-16
 9.5594811e-01], sum to 1.0000
[2019-04-08 15:18:16,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8863
[2019-04-08 15:18:16,415] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.9465356435143, 0.8091455140030134, 0.0, 1.0, 65.0, 41584.33586425404], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3891600.0000, 
sim time next is 3892200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.97248443438192, 0.8146591186745816, 0.0, 1.0, 65.0, 40925.93293633437], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7477070361984932, 0.7715530395581939, 0.0, 1.0, 1.0, 0.19488539493492557], 
reward next is 0.8051, 
noisyNet noise sample is [array([-1.500864], dtype=float32), -0.44802642]. 
=============================================
[2019-04-08 15:18:16,416] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 103093: loss 0.0423
[2019-04-08 15:18:16,417] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 103093: learning rate 0.0000
[2019-04-08 15:18:16,520] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7755941e-14 1.6106625e-06 2.3314693e-04 4.0665194e-02 1.5245839e-06
 3.4088115e-09 4.6238149e-04 1.3475985e-09 3.3881572e-14 9.1417545e-16
 9.5863616e-01], sum to 1.0000
[2019-04-08 15:18:16,521] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7365
[2019-04-08 15:18:16,552] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 27.01682239374165, 0.7974278680204336, 0.0, 1.0, 65.0, 40028.38610410202], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3893400.0000, 
sim time next is 3894000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.97400919396912, 0.792881361119273, 0.0, 1.0, 65.0, 41897.77697131439], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7478340994974267, 0.7642937870397577, 0.0, 1.0, 1.0, 0.19951322367292568], 
reward next is 0.8005, 
noisyNet noise sample is [array([-1.500864], dtype=float32), -0.44802642]. 
=============================================
[2019-04-08 15:18:16,557] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.92751 ]
 [77.90495 ]
 [78.1074  ]
 [78.408806]
 [78.71292 ]], R is [[77.55503082]
 [77.58886719]
 [77.62645721]
 [77.65531158]
 [77.68074036]].
[2019-04-08 15:18:16,825] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0046705e-14 3.6160134e-07 3.8753398e-04 1.7919177e-02 1.6223980e-07
 8.8049645e-10 2.1052312e-05 3.8190850e-11 8.0962014e-16 9.9554165e-17
 9.8167175e-01], sum to 1.0000
[2019-04-08 15:18:16,853] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103248: loss 0.0527
[2019-04-08 15:18:16,854] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103248: learning rate 0.0000
[2019-04-08 15:18:16,857] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9789
[2019-04-08 15:18:16,909] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.94350840602711, 0.8085320957945372, 0.0, 1.0, 65.0, 41688.40255766093], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3891600.0000, 
sim time next is 3892200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.96846156830196, 0.8141185224959497, 0.0, 1.0, 65.0, 41012.94645394367], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7473717973584968, 0.7713728408319832, 0.0, 1.0, 1.0, 0.19529974501877936], 
reward next is 0.8047, 
noisyNet noise sample is [array([-1.0636599], dtype=float32), 0.23423147]. 
=============================================
[2019-04-08 15:18:17,060] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103326: loss 0.0370
[2019-04-08 15:18:17,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103326: learning rate 0.0000
[2019-04-08 15:18:17,374] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103435: loss -1.3682
[2019-04-08 15:18:17,376] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103436: learning rate 0.0000
[2019-04-08 15:18:17,493] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 103465: loss 0.0538
[2019-04-08 15:18:17,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6360329e-14 1.4745908e-07 1.7720795e-03 1.3204113e-03 1.9333378e-07
 3.2068187e-10 8.0479571e-05 1.1897120e-11 4.2971243e-15 3.5848903e-17
 9.9682671e-01], sum to 1.0000
[2019-04-08 15:18:17,530] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8296
[2019-04-08 15:18:17,541] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 103481: learning rate 0.0000
[2019-04-08 15:18:17,566] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 68.0, 0.0, 0.0, 19.0, 26.752972022601, 0.702546493748271, 0.0, 1.0, 65.0, 46707.38861386045], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3907800.0000, 
sim time next is 3908400.0000, 
raw observation next is [-5.333333333333334, 65.0, 0.0, 0.0, 19.0, 26.67524469582704, 0.6996852818867417, 0.0, 1.0, 65.0, 49092.90269922149], 
processed observation next is [1.0, 0.21739130434782608, 0.31486611265004616, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7229370579855866, 0.7332284272955806, 0.0, 1.0, 1.0, 0.23377572713914996], 
reward next is 0.7662, 
noisyNet noise sample is [array([-0.37349778], dtype=float32), -0.052224483]. 
=============================================
[2019-04-08 15:18:17,602] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103502: loss 0.0572
[2019-04-08 15:18:17,622] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103505: learning rate 0.0000
[2019-04-08 15:18:17,725] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103535: loss 0.0599
[2019-04-08 15:18:17,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103535: learning rate 0.0000
[2019-04-08 15:18:17,941] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0030793e-15 4.5608317e-08 8.8428074e-05 1.2564084e-03 4.0927738e-08
 2.8332650e-10 2.4036062e-05 7.6662114e-12 4.7309855e-16 1.5326651e-17
 9.9863106e-01], sum to 1.0000
[2019-04-08 15:18:17,976] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5195
[2019-04-08 15:18:17,993] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.97275864633392, 0.8187425320055116, 0.0, 1.0, 65.0, 41548.02437005637], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3889200.0000, 
sim time next is 3889800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.95923535755481, 0.8143312673173365, 0.0, 1.0, 65.0, 41726.34188102037], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7466029464629008, 0.7714437557724455, 0.0, 1.0, 1.0, 0.198696866100097], 
reward next is 0.8013, 
noisyNet noise sample is [array([0.24277], dtype=float32), -0.019418525]. 
=============================================
[2019-04-08 15:18:18,111] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4794927e-14 1.4839414e-07 3.3143529e-04 1.6182293e-03 2.4646909e-08
 1.3500819e-10 1.9832414e-06 7.0455443e-12 1.8084914e-14 1.1904491e-16
 9.9804819e-01], sum to 1.0000
[2019-04-08 15:18:18,114] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3493
[2019-04-08 15:18:18,154] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.97449784166562, 0.7930255026613983, 0.0, 1.0, 65.0, 41887.54150936782], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3894000.0000, 
sim time next is 3894600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.93261018193014, 0.7881057607870702, 0.0, 1.0, 65.0, 42459.92199447926], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7443841818275118, 0.7627019202623567, 0.0, 1.0, 1.0, 0.2021901047356155], 
reward next is 0.7978, 
noisyNet noise sample is [array([-0.12796797], dtype=float32), 0.26742768]. 
=============================================
[2019-04-08 15:18:18,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7078020e-14 1.0094460e-08 6.6112161e-06 1.0353016e-03 2.9062022e-08
 1.7833096e-11 1.9318677e-05 6.5866176e-13 5.6208823e-15 1.6250708e-17
 9.9893874e-01], sum to 1.0000
[2019-04-08 15:18:18,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1122
[2019-04-08 15:18:18,241] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.58218870869674, 0.6895982988275692, 0.0, 1.0, 65.0, 49749.2521491725], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3909600.0000, 
sim time next is 3910200.0000, 
raw observation next is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 19.0, 26.60204716928427, 0.6925430890746883, 0.0, 1.0, 65.0, 49102.78122251669], 
processed observation next is [1.0, 0.2608695652173913, 0.29178208679593726, 0.5983333333333334, 0.0, 0.0, 0.08333333333333333, 0.7168372641070224, 0.7308476963582294, 0.0, 1.0, 1.0, 0.23382276772626998], 
reward next is 0.7662, 
noisyNet noise sample is [array([-0.3800487], dtype=float32), 1.1857759]. 
=============================================
[2019-04-08 15:18:18,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4858629e-17 1.0040048e-07 5.4777041e-04 1.0446756e-04 6.6813233e-10
 5.5596115e-13 3.7247810e-05 8.3997479e-14 1.6071816e-19 3.1169560e-22
 9.9931037e-01], sum to 1.0000
[2019-04-08 15:18:18,420] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6627
[2019-04-08 15:18:18,429] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 22.5, 28.69725276888985, 1.187163536839613, 1.0, 1.0, 65.0, 18849.3143247338], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 3858000.0000, 
sim time next is 3858600.0000, 
raw observation next is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 22.5, 28.70712559066008, 1.197178307220179, 1.0, 1.0, 65.0, 18848.86777714703], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.2644444444444445, 0.7311233885819522, 0.375, 0.8922604658883401, 0.8990594357400598, 1.0, 1.0, 1.0, 0.08975651322450966], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.0234712], dtype=float32), 0.3778286]. 
=============================================
[2019-04-08 15:18:18,489] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 103819: loss 0.0400
[2019-04-08 15:18:18,490] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 103819: learning rate 0.0000
[2019-04-08 15:18:18,696] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103897: loss 0.0616
[2019-04-08 15:18:18,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103898: learning rate 0.0000
[2019-04-08 15:18:18,726] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 103904: loss 0.0941
[2019-04-08 15:18:18,726] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 103904: learning rate 0.0000
[2019-04-08 15:18:19,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7137928e-17 2.3652977e-09 8.6705443e-03 1.5335737e-04 4.3858237e-09
 1.2278296e-11 2.3211942e-06 3.3836307e-13 2.9128520e-17 4.7916227e-20
 9.9117380e-01], sum to 1.0000
[2019-04-08 15:18:19,523] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-08 15:18:19,545] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 22.5, 27.84776842156068, 1.034285675252587, 1.0, 1.0, 65.0, 25220.4977983187], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3870600.0000, 
sim time next is 3871200.0000, 
raw observation next is [1.0, 51.00000000000001, 0.0, 0.0, 22.5, 27.84538152230962, 1.028487050124433, 1.0, 1.0, 65.0, 26008.07974178328], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.5100000000000001, 0.0, 0.0, 0.375, 0.8204484601924683, 0.8428290167081443, 1.0, 1.0, 1.0, 0.12384799877039657], 
reward next is 0.8762, 
noisyNet noise sample is [array([-1.7444568], dtype=float32), -0.27536196]. 
=============================================
[2019-04-08 15:18:19,994] A3C_AGENT_WORKER-Thread-8 INFO:Local step 6500, global step 104326: loss 0.0282
[2019-04-08 15:18:19,995] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 6500, global step 104326: learning rate 0.0000
[2019-04-08 15:18:21,737] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104917: loss 0.0686
[2019-04-08 15:18:21,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104917: learning rate 0.0000
[2019-04-08 15:18:21,744] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104919: loss 0.0951
[2019-04-08 15:18:21,745] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104919: learning rate 0.0000
[2019-04-08 15:18:22,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.97493253e-14 1.93042774e-06 1.05052786e-04 2.24645220e-04
 1.39695643e-07 8.65572414e-09 6.17478072e-05 1.41078302e-10
 3.32237260e-15 2.51213577e-17 9.99606550e-01], sum to 1.0000
[2019-04-08 15:18:22,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7098
[2019-04-08 15:18:22,921] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 19.0, 26.61942337184299, 0.7244020881848993, 0.0, 1.0, 65.0, 50892.91122169569], 
current ob forecast is [], 
actual action is [-3.666666666666668, 65.0], 
sim time this is 3969600.0000, 
sim time next is 3970200.0000, 
raw observation next is [-8.833333333333332, 52.33333333333334, 0.0, 0.0, 19.0, 26.57545694690956, 0.7150753724797713, 0.0, 1.0, 65.0, 51168.742775596], 
processed observation next is [1.0, 0.9565217391304348, 0.2179132040627886, 0.5233333333333334, 0.0, 0.0, 0.08333333333333333, 0.7146214122424634, 0.738358457493257, 0.0, 1.0, 1.0, 0.24366067988379048], 
reward next is 0.7563, 
noisyNet noise sample is [array([2.2487593], dtype=float32), -0.78630185]. 
=============================================
[2019-04-08 15:18:23,770] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3556455e-13 5.6711212e-08 9.7938260e-05 7.6308486e-04 4.4151999e-08
 2.1079072e-09 5.4052018e-04 6.0077610e-11 1.6077814e-14 5.6856547e-16
 9.9859840e-01], sum to 1.0000
[2019-04-08 15:18:23,770] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9687
[2019-04-08 15:18:23,786] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.83333333333333, 62.16666666666666, 0.0, 0.0, 19.0, 26.28075582352124, 0.6097919301762446, 0.0, 1.0, 65.0, 55677.49056437887], 
current ob forecast is [], 
actual action is [-6.83333333333333, 65.0], 
sim time this is 3981000.0000, 
sim time next is 3981600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.2252346838481, 0.6127609115700643, 0.0, 1.0, 65.0, 56191.98948375461], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.6854362236540084, 0.7042536371900215, 0.0, 1.0, 1.0, 0.2675809023035934], 
reward next is 0.7324, 
noisyNet noise sample is [array([-0.96402603], dtype=float32), 0.571102]. 
=============================================
[2019-04-08 15:18:24,249] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 105845: loss 0.1965
[2019-04-08 15:18:24,250] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 105846: learning rate 0.0000
[2019-04-08 15:18:24,367] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7374896e-13 2.2054760e-06 1.8747592e-04 6.0567474e-03 1.1480732e-07
 5.7901595e-10 1.2620408e-04 4.5258684e-12 2.5267796e-14 4.4762700e-17
 9.9362731e-01], sum to 1.0000
[2019-04-08 15:18:24,371] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7079
[2019-04-08 15:18:24,389] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.66666666666667, 61.33333333333333, 62.00000000000001, 296.0000000000001, 22.5, 25.92737743889442, 0.5128533162924751, 1.0, 1.0, 65.0, 53481.55656065894], 
current ob forecast is [], 
actual action is [-7.66666666666667, 65.0], 
sim time this is 4003800.0000, 
sim time next is 4004400.0000, 
raw observation next is [-12.33333333333333, 59.66666666666667, 77.5, 370.0, 22.5, 25.98571620404283, 0.5558625041506923, 1.0, 1.0, 64.99999999999997, 51044.3646014152], 
processed observation next is [1.0, 0.34782608695652173, 0.12096029547553101, 0.5966666666666667, 0.25833333333333336, 0.4088397790055249, 0.375, 0.6654763503369026, 0.6852875013835641, 1.0, 1.0, 0.9999999999999994, 0.2430684028638819], 
reward next is 0.7569, 
noisyNet noise sample is [array([-1.254743], dtype=float32), -2.0535378]. 
=============================================
[2019-04-08 15:18:25,525] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5267656e-16 7.7901785e-10 6.4658966e-06 1.2632867e-04 1.5370713e-08
 1.3276598e-11 1.6770339e-06 3.3663179e-13 4.7857616e-17 6.1327125e-20
 9.9986553e-01], sum to 1.0000
[2019-04-08 15:18:25,529] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0217
[2019-04-08 15:18:25,563] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.833333333333333, 40.5, 0.0, 0.0, 22.5, 27.624713767976, 0.963342882758991, 1.0, 1.0, 65.0, 26878.5911433378], 
current ob forecast is [], 
actual action is [-0.833333333333333, 65.0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 22.5, 27.72379582548852, 0.9554397486528163, 1.0, 1.0, 65.0, 27676.67009411385], 
processed observation next is [1.0, 0.782608695652174, 0.296398891966759, 0.41, 0.0, 0.0, 0.375, 0.81031631879071, 0.8184799162176054, 1.0, 1.0, 1.0, 0.13179366711482787], 
reward next is 0.8682, 
noisyNet noise sample is [array([-0.13163358], dtype=float32), 1.2265092]. 
=============================================
[2019-04-08 15:18:25,861] A3C_AGENT_WORKER-Thread-7 INFO:Local step 6500, global step 106427: loss 0.1506
[2019-04-08 15:18:25,864] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 6500, global step 106429: learning rate 0.0000
[2019-04-08 15:18:26,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2005681e-17 1.9312507e-08 4.7161939e-05 1.2512340e-03 3.9754969e-10
 1.4037918e-12 3.8591878e-07 8.4471436e-15 8.7267488e-18 2.3547092e-20
 9.9870121e-01], sum to 1.0000
[2019-04-08 15:18:26,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4733
[2019-04-08 15:18:26,115] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 38.5, 119.0, 816.0, 22.5, 26.61093510006242, 0.7752489603208206, 1.0, 1.0, 64.99999999999999, 38051.75989092143], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 4015800.0000, 
sim time next is 4016400.0000, 
raw observation next is [-6.666666666666667, 38.0, 118.8333333333333, 820.1666666666666, 22.5, 27.1460514409674, 0.5039649297503034, 1.0, 1.0, 65.00000000000003, 31030.27713458639], 
processed observation next is [1.0, 0.4782608695652174, 0.27793167128347185, 0.38, 0.396111111111111, 0.9062615101289134, 0.375, 0.7621709534139501, 0.6679883099167677, 1.0, 1.0, 1.0000000000000007, 0.14776322445041137], 
reward next is 0.8522, 
noisyNet noise sample is [array([1.3238839], dtype=float32), 0.11000022]. 
=============================================
[2019-04-08 15:18:26,400] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0959637e-19 1.9854769e-09 1.8715693e-05 7.8345044e-04 2.5238764e-09
 1.3088216e-11 2.6512805e-07 4.1780718e-15 1.5027199e-18 3.4506912e-20
 9.9919754e-01], sum to 1.0000
[2019-04-08 15:18:26,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4819
[2019-04-08 15:18:26,477] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.333333333333334, 41.33333333333333, 113.1666666666667, 786.8333333333334, 22.5, 27.3207046423082, 0.4718021284374838, 1.0, 1.0, 65.0, 33574.82771278307], 
current ob forecast is [], 
actual action is [-3.333333333333334, 65.0], 
sim time this is 4012800.0000, 
sim time next is 4013400.0000, 
raw observation next is [-8.166666666666666, 40.66666666666667, 114.3333333333333, 792.6666666666667, 22.5, 27.12028220859111, 0.8223241622294495, 1.0, 1.0, 65.0, 49468.37883503316], 
processed observation next is [1.0, 0.43478260869565216, 0.23638042474607576, 0.40666666666666673, 0.381111111111111, 0.8758747697974218, 0.375, 0.7600235173825926, 0.7741080540764832, 1.0, 1.0, 1.0, 0.23556370873825314], 
reward next is 0.7644, 
noisyNet noise sample is [array([0.06267786], dtype=float32), 0.35680035]. 
=============================================
[2019-04-08 15:18:26,598] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7449646e-16 8.7523553e-09 1.2823049e-05 1.6354346e-04 1.3661582e-08
 1.5924279e-12 3.5750637e-07 2.2642408e-13 1.6546211e-17 4.2377448e-19
 9.9982327e-01], sum to 1.0000
[2019-04-08 15:18:26,599] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4501
[2019-04-08 15:18:26,610] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 23.33333333333334, 59.16666666666667, 488.8333333333334, 22.5, 27.72682560867214, 1.004475069582488, 1.0, 1.0, 65.0, 21498.04571205391], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 4034400.0000, 
sim time next is 4035000.0000, 
raw observation next is [-1.833333333333333, 23.66666666666666, 51.33333333333334, 429.6666666666667, 22.5, 28.15707633951683, 1.016392470355053, 1.0, 1.0, 65.0, 18846.25293921772], 
processed observation next is [1.0, 0.6956521739130435, 0.41181902123730385, 0.2366666666666666, 0.17111111111111113, 0.47476979742173114, 0.375, 0.8464230282930693, 0.8387974901183511, 1.0, 1.0, 1.0, 0.08974406161532247], 
reward next is 0.9103, 
noisyNet noise sample is [array([1.738909], dtype=float32), 0.12748536]. 
=============================================
[2019-04-08 15:18:26,624] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.227295]
 [86.518906]
 [86.71211 ]
 [86.850006]
 [87.0045  ]], R is [[86.10250092]
 [86.13910675]
 [86.1452179 ]
 [86.1940155 ]
 [86.2423172 ]].
[2019-04-08 15:18:26,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4288640e-15 5.2615614e-09 1.3432468e-03 1.6757360e-03 1.6648577e-09
 3.6005759e-10 4.7488804e-05 2.4405246e-12 1.2856269e-16 8.1891869e-20
 9.9693346e-01], sum to 1.0000
[2019-04-08 15:18:26,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4065
[2019-04-08 15:18:26,920] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.333333333333333, 37.5, 118.6666666666667, 824.3333333333334, 22.5, 27.18325556562929, 0.8540312050675788, 1.0, 1.0, 65.0, 44724.08562407516], 
current ob forecast is [], 
actual action is [-1.333333333333333, 65.0], 
sim time this is 4017000.0000, 
sim time next is 4017600.0000, 
raw observation next is [-6.0, 37.0, 118.5, 828.5, 22.5, 26.83839523572885, 0.8063902025006234, 1.0, 1.0, 65.00000000000018, 33428.95734641989], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.37, 0.395, 0.9154696132596685, 0.375, 0.7365329363107375, 0.7687967341668744, 1.0, 1.0, 1.0000000000000038, 0.15918551117342805], 
reward next is 0.8408, 
noisyNet noise sample is [array([0.10505562], dtype=float32), -0.14997151]. 
=============================================
[2019-04-08 15:18:26,930] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.2602001e-13 1.0552722e-07 3.8871981e-04 6.4184662e-04 1.0643646e-07
 3.5436734e-10 1.2642663e-04 5.0198595e-10 2.0860448e-14 4.2095841e-16
 9.9884272e-01], sum to 1.0000
[2019-04-08 15:18:26,932] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8707
[2019-04-08 15:18:26,959] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.28146447870837, 0.5923126758294232, 0.0, 1.0, 65.0, 52249.44869992598], 
current ob forecast is [], 
actual action is [-7.0, 65.0], 
sim time this is 3985800.0000, 
sim time next is 3986400.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 19.0, 26.31967500286253, 0.5797015124934689, 0.0, 1.0, 65.0, 51962.46499345094], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.08333333333333333, 0.6933062502385443, 0.693233837497823, 0.0, 1.0, 1.0, 0.24744030949262355], 
reward next is 0.7526, 
noisyNet noise sample is [array([-1.0154333], dtype=float32), -0.39310756]. 
=============================================
[2019-04-08 15:18:26,963] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6593116e-14 5.4088428e-08 3.6133244e-04 3.5136889e-04 4.9018567e-08
 5.7708266e-10 1.4147774e-05 2.6208211e-10 2.7030335e-15 8.9760578e-18
 9.9927300e-01], sum to 1.0000
[2019-04-08 15:18:26,969] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7169
[2019-04-08 15:18:27,025] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.166666666666666, 53.83333333333334, 0.0, 0.0, 19.0, 26.50901588210187, 0.7004374038195073, 0.0, 1.0, 65.0, 52360.18736683182], 
current ob forecast is [], 
actual action is [-4.166666666666666, 65.0], 
sim time this is 3971400.0000, 
sim time next is 3972000.0000, 
raw observation next is [-9.333333333333334, 54.66666666666667, 0.0, 0.0, 19.0, 26.47975141371262, 0.6933761916036266, 0.0, 1.0, 65.0, 52794.25943269581], 
processed observation next is [1.0, 1.0, 0.20406278855032317, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.7066459511427183, 0.7311253972012088, 0.0, 1.0, 1.0, 0.2514012353937896], 
reward next is 0.7486, 
noisyNet noise sample is [array([2.418414], dtype=float32), 0.016489418]. 
=============================================
[2019-04-08 15:18:27,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.720825]
 [70.81538 ]
 [70.806694]
 [70.86132 ]
 [71.09811 ]], R is [[70.70500946]
 [70.74862671]
 [70.79462433]
 [70.84315491]
 [70.89252472]].
[2019-04-08 15:18:27,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6663216e-17 4.4204583e-08 2.1132223e-06 1.0909530e-04 7.6454965e-10
 2.6087256e-11 1.9009409e-05 1.4670038e-11 3.1540252e-18 1.8949632e-19
 9.9986970e-01], sum to 1.0000
[2019-04-08 15:18:27,371] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-08 15:18:27,412] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.333333333333333, 37.5, 118.6666666666667, 824.3333333333334, 22.5, 27.18539189932547, 0.8546646809564429, 1.0, 1.0, 65.0, 44708.95167958894], 
current ob forecast is [], 
actual action is [-1.333333333333333, 65.0], 
sim time this is 4017000.0000, 
sim time next is 4017600.0000, 
raw observation next is [-6.0, 37.0, 118.5, 828.5, 22.5, 26.8404616329727, 0.8070491764356275, 1.0, 1.0, 65.00000000000018, 33405.12131219581], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.37, 0.395, 0.9154696132596685, 0.375, 0.7367051360810585, 0.7690163921452092, 1.0, 1.0, 1.0000000000000038, 0.1590720062485515], 
reward next is 0.8409, 
noisyNet noise sample is [array([-0.473407], dtype=float32), -0.96254677]. 
=============================================
[2019-04-08 15:18:28,002] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1000326e-17 6.6841084e-09 1.3780796e-04 3.0493273e-05 8.4380336e-10
 9.1533163e-11 8.1322951e-06 2.7375958e-13 1.9421681e-17 3.2610749e-20
 9.9982363e-01], sum to 1.0000
[2019-04-08 15:18:28,003] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6911
[2019-04-08 15:18:28,085] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 29.0, 116.0, 835.5, 22.5, 27.54493380945174, 0.8811997208262979, 1.0, 1.0, 65.0, 23159.09020607006], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 4021200.0000, 
sim time next is 4021800.0000, 
raw observation next is [-3.833333333333333, 28.5, 115.3333333333333, 833.6666666666666, 22.5, 27.59470502690105, 0.724508930821837, 1.0, 1.0, 65.0, 38194.88772958997], 
processed observation next is [1.0, 0.5652173913043478, 0.3564173591874424, 0.285, 0.3844444444444443, 0.9211786372007366, 0.375, 0.7995587522417541, 0.7415029769406124, 1.0, 1.0, 1.0, 0.18188041775995226], 
reward next is 0.8181, 
noisyNet noise sample is [array([-0.53808326], dtype=float32), 0.9890836]. 
=============================================
[2019-04-08 15:18:28,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0545455e-14 8.7826180e-08 1.5773047e-02 8.5490039e-03 1.8541972e-08
 3.6597586e-11 2.1939750e-05 1.4952619e-11 3.8358819e-16 3.4388131e-18
 9.7565585e-01], sum to 1.0000
[2019-04-08 15:18:28,380] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0281
[2019-04-08 15:18:28,404] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 25.0, 20.0, 193.0, 22.5, 27.83344392003431, 0.9409941848091387, 1.0, 1.0, 65.0, 24888.93846583408], 
current ob forecast is [], 
actual action is [2.5, 65.0], 
sim time this is 4037400.0000, 
sim time next is 4038000.0000, 
raw observation next is [-2.666666666666667, 25.33333333333333, 16.66666666666667, 160.8333333333333, 22.5, 27.79712560298507, 0.9673437627279808, 1.0, 1.0, 65.0, 25247.34854725012], 
processed observation next is [1.0, 0.7391304347826086, 0.38873499538319484, 0.2533333333333333, 0.05555555555555557, 0.17771639042357268, 0.375, 0.8164271335820891, 0.822447920909327, 1.0, 1.0, 1.0, 0.12022546927261962], 
reward next is 0.8798, 
noisyNet noise sample is [array([0.9489455], dtype=float32), 0.61786246]. 
=============================================
[2019-04-08 15:18:28,522] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[84.65117]
 [85.09286]
 [85.49223]
 [85.90947]
 [86.185  ]], R is [[84.2335968 ]
 [84.27274323]
 [84.30171204]
 [84.29792786]
 [84.36503601]].
[2019-04-08 15:18:28,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6867282e-16 4.7197112e-08 4.7809741e-04 1.4448256e-03 6.6463799e-09
 6.3215100e-10 1.3706292e-04 1.8046068e-12 1.0658811e-16 2.9609150e-19
 9.9793988e-01], sum to 1.0000
[2019-04-08 15:18:28,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9584
[2019-04-08 15:18:28,922] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.333333333333333, 21.33333333333334, 85.33333333333334, 684.6666666666667, 22.5, 28.27551900466272, 1.003300067029252, 1.0, 1.0, 65.0, 18849.08091145698], 
current ob forecast is [], 
actual action is [3.666666666666667, 65.0], 
sim time this is 4030800.0000, 
sim time next is 4031400.0000, 
raw observation next is [-1.166666666666667, 21.66666666666667, 81.66666666666667, 657.3333333333334, 22.5, 28.28342606388641, 1.025874373255979, 1.0, 1.0, 65.0, 18848.41424190583], 
processed observation next is [1.0, 0.6521739130434783, 0.43028624192059095, 0.2166666666666667, 0.27222222222222225, 0.7263351749539595, 0.375, 0.8569521719905341, 0.8419581244186597, 1.0, 1.0, 1.0, 0.0897543535328849], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.24062218], dtype=float32), 0.91053283]. 
=============================================
[2019-04-08 15:18:29,011] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4086292e-16 2.2793849e-08 1.3133966e-03 3.4140795e-04 1.5815887e-10
 4.0848297e-11 3.1947789e-06 4.6243964e-14 2.0258874e-17 1.2199290e-20
 9.9834192e-01], sum to 1.0000
[2019-04-08 15:18:29,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1959
[2019-04-08 15:18:29,068] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 33.5, 114.0, 769.0, 22.5, 27.7393180569727, 0.8867178579766005, 1.0, 1.0, 65.0, 21339.65365683143], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 4098600.0000, 
sim time next is 4099200.0000, 
raw observation next is [-1.333333333333333, 33.0, 115.1666666666667, 776.8333333333334, 22.5, 27.77895555835373, 0.9001416320181491, 1.0, 1.0, 65.0, 19734.93688836192], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.33, 0.383888888888889, 0.8583793738489871, 0.375, 0.8149129631961441, 0.8000472106727163, 1.0, 1.0, 1.0, 0.09397588994458057], 
reward next is 0.9060, 
noisyNet noise sample is [array([0.88388383], dtype=float32), -0.5051708]. 
=============================================
[2019-04-08 15:18:30,373] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5498364e-13 2.2291859e-07 7.8953984e-05 1.7610558e-04 1.2121144e-07
 1.6139292e-09 1.6747478e-04 2.9462613e-10 4.2741761e-15 3.4260486e-15
 9.9957711e-01], sum to 1.0000
[2019-04-08 15:18:30,373] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6057
[2019-04-08 15:18:30,417] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 39.5, 0.0, 0.0, 19.0, 26.53132363366524, 0.6114422486494255, 0.0, 1.0, 65.0, 48373.22544552924], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 4073400.0000, 
sim time next is 4074000.0000, 
raw observation next is [-5.0, 40.0, 0.0, 0.0, 19.0, 26.49677293129538, 0.609475195275749, 0.0, 1.0, 65.0, 49145.53207294943], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7080644109412816, 0.7031583984252497, 0.0, 1.0, 1.0, 0.2340263432045211], 
reward next is 0.7660, 
noisyNet noise sample is [array([0.6425139], dtype=float32), 0.39524218]. 
=============================================
[2019-04-08 15:18:30,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[64.72249 ]
 [64.702805]
 [64.63116 ]
 [64.513306]
 [64.55811 ]], R is [[64.89858246]
 [65.01924896]
 [65.1471405 ]
 [65.2726059 ]
 [65.39357758]].
[2019-04-08 15:18:30,817] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6175444e-13 9.9465758e-08 6.1062537e-04 1.7147801e-03 2.4561660e-07
 4.3794741e-09 9.2844610e-05 9.8952480e-10 4.7957735e-14 6.5566677e-15
 9.9758136e-01], sum to 1.0000
[2019-04-08 15:18:30,817] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5830
[2019-04-08 15:18:30,846] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.5, 39.5, 0.0, 0.0, 19.0, 26.63660323001487, 0.638960017635865, 0.0, 1.0, 65.0, 45722.49793360545], 
current ob forecast is [], 
actual action is [-0.5, 65.0], 
sim time this is 4069800.0000, 
sim time next is 4070400.0000, 
raw observation next is [-5.333333333333333, 39.0, 0.0, 0.0, 19.0, 26.63464776737194, 0.6348956070258159, 0.0, 1.0, 65.0, 46494.29470239639], 
processed observation next is [1.0, 0.08695652173913043, 0.3148661126500462, 0.39, 0.0, 0.0, 0.08333333333333333, 0.7195539806143284, 0.7116318690086053, 0.0, 1.0, 1.0, 0.22140140334474473], 
reward next is 0.7786, 
noisyNet noise sample is [array([-1.7612243], dtype=float32), -1.793864]. 
=============================================
[2019-04-08 15:18:31,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2807401e-13 2.5576009e-07 4.1420219e-04 3.0009041e-04 1.5388596e-07
 1.7012485e-09 3.8277532e-04 1.2750385e-10 7.3803836e-15 3.3808297e-15
 9.9890256e-01], sum to 1.0000
[2019-04-08 15:18:31,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4904
[2019-04-08 15:18:32,093] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 26.63072396868571, 0.6422337621766766, 0.0, 1.0, 65.0, 47142.43322656822], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [-5.833333333333333, 40.50000000000001, 0.0, 0.0, 19.0, 26.59432949938642, 0.6472197879818903, 0.0, 1.0, 65.0, 47658.29769223405], 
processed observation next is [1.0, 0.08695652173913043, 0.30101569713758086, 0.4050000000000001, 0.0, 0.0, 0.08333333333333333, 0.7161941249488682, 0.7157399293272967, 0.0, 1.0, 1.0, 0.22694427472492407], 
reward next is 0.7731, 
noisyNet noise sample is [array([0.76207525], dtype=float32), 0.16234411]. 
=============================================
[2019-04-08 15:18:32,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4088385e-12 9.4378470e-07 1.0256955e-04 2.4035627e-03 4.6778780e-07
 1.3489668e-09 1.1685627e-04 2.9715844e-10 3.3678578e-13 2.8786682e-15
 9.9737561e-01], sum to 1.0000
[2019-04-08 15:18:32,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0003
[2019-04-08 15:18:33,023] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-12.0, 63.00000000000001, 0.0, 0.0, 19.0, 26.32239142255449, 0.5805774975240079, 0.0, 1.0, 65.0, 51929.71780306577], 
current ob forecast is [], 
actual action is [-7.0, 30.0], 
sim time this is 3986400.0000, 
sim time next is 3987000.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.26595279396058, 0.5787145159214412, 0.0, 1.0, 30.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.688829399496715, 0.6929048386404805, 0.0, 1.0, 0.3, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1245618], dtype=float32), -1.9382834]. 
=============================================
[2019-04-08 15:18:33,049] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[68.727066]
 [68.68861 ]
 [68.59876 ]
 [68.67154 ]
 [68.69285 ]], R is [[68.28504181]
 [68.3549118 ]
 [68.4227066 ]
 [68.47900391]
 [68.5258255 ]].
[2019-04-08 15:18:33,185] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5052258e-12 6.1782948e-06 2.5130610e-04 8.2251821e-03 2.8139723e-07
 1.6662441e-09 6.8714136e-05 5.7889010e-10 5.2689939e-14 7.6668205e-15
 9.9144840e-01], sum to 1.0000
[2019-04-08 15:18:33,186] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5831
[2019-04-08 15:18:33,222] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 26.6746586703488, 0.6480794253693766, 0.0, 1.0, 65.0, 46820.85727732887], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4067400.0000, 
sim time next is 4068000.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 26.63406457213982, 0.6432759446156056, 0.0, 1.0, 65.0, 47454.05063340173], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7195053810116517, 0.7144253148718684, 0.0, 1.0, 1.0, 0.22597166968286536], 
reward next is 0.7740, 
noisyNet noise sample is [array([-0.06734404], dtype=float32), -1.0463887]. 
=============================================
[2019-04-08 15:18:33,407] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[65.6028  ]
 [65.94435 ]
 [66.25469 ]
 [65.808464]
 [65.86545 ]], R is [[65.60588837]
 [65.72687531]
 [65.83532715]
 [65.89422607]
 [65.90934753]].
[2019-04-08 15:18:45,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.72747929e-17 3.28567373e-09 1.02355414e-04 1.65038473e-05
 4.35951934e-11 6.48152430e-13 2.68573785e-07 4.51133000e-13
 3.86857871e-19 1.42473485e-19 9.99880910e-01], sum to 1.0000
[2019-04-08 15:18:45,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9677
[2019-04-08 15:18:45,656] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 40.66666666666667, 0.0, 0.0, 19.0, 27.62658802114266, 1.001099879045457, 0.0, 1.0, 65.0, 27338.54191244375], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4134000.0000, 
sim time next is 4134600.0000, 
raw observation next is [1.0, 39.5, 0.0, 0.0, 19.0, 27.60324249172426, 0.9949734667426071, 0.0, 1.0, 65.0, 27499.75828062931], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.395, 0.0, 0.0, 0.08333333333333333, 0.8002702076436883, 0.8316578222475357, 0.0, 1.0, 1.0, 0.13095122990775862], 
reward next is 0.8690, 
noisyNet noise sample is [array([0.43454802], dtype=float32), -2.2914896]. 
=============================================
[2019-04-08 15:18:46,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1569871e-16 4.9933615e-08 1.2412935e-04 4.8341323e-04 1.8614279e-09
 6.5313110e-10 2.6307458e-05 5.0112383e-13 3.5506880e-17 4.6446941e-19
 9.9936610e-01], sum to 1.0000
[2019-04-08 15:18:46,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5599
[2019-04-08 15:18:46,084] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 41.83333333333334, 0.0, 0.0, 19.0, 27.66276643532082, 1.0060728519038, 1.0, 1.0, 65.0, 26842.81855768833], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4133400.0000, 
sim time next is 4134000.0000, 
raw observation next is [1.0, 40.66666666666667, 0.0, 0.0, 19.0, 27.62686303015768, 1.001171350357971, 0.0, 1.0, 65.0, 27334.53152347155], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.40666666666666673, 0.0, 0.0, 0.08333333333333333, 0.8022385858464732, 0.833723783452657, 0.0, 1.0, 1.0, 0.130164435826055], 
reward next is 0.8698, 
noisyNet noise sample is [array([0.2605349], dtype=float32), 0.86887294]. 
=============================================
[2019-04-08 15:18:46,088] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.30488 ]
 [81.83121 ]
 [82.42973 ]
 [82.555374]
 [83.36597 ]], R is [[81.08538818]
 [81.14671326]
 [81.21019745]
 [81.2749176 ]
 [81.34086609]].
[2019-04-08 15:18:46,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9426304e-14 6.0673628e-08 1.3002829e-04 1.5883391e-03 8.1989029e-09
 2.4577726e-10 9.0667727e-06 2.7271742e-12 1.2622380e-15 4.9559989e-18
 9.9827254e-01], sum to 1.0000
[2019-04-08 15:18:46,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0617
[2019-04-08 15:18:46,807] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 40.66666666666667, 196.0, 282.3333333333333, 19.0, 27.48071616800364, 0.9216180871211602, 0.0, 1.0, 65.0, 27340.48312661997], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4198200.0000, 
sim time next is 4198800.0000, 
raw observation next is [2.0, 41.33333333333334, 191.5, 185.6666666666666, 19.0, 27.48692800381174, 0.9139442044608348, 0.0, 1.0, 64.99999999999997, 27913.68956361985], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.41333333333333344, 0.6383333333333333, 0.20515653775322276, 0.08333333333333333, 0.7905773336509784, 0.8046480681536116, 0.0, 1.0, 0.9999999999999994, 0.13292233125533262], 
reward next is 0.8671, 
noisyNet noise sample is [array([1.4105895], dtype=float32), -1.0646172]. 
=============================================
[2019-04-08 15:18:46,969] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 110174: loss 0.0083
[2019-04-08 15:18:46,970] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 110174: learning rate 0.0000
[2019-04-08 15:18:48,261] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8948542e-11 1.7988272e-06 4.3831975e-03 5.1804841e-03 1.3132016e-06
 4.2524725e-08 1.6680288e-03 1.6494878e-10 8.2025592e-13 6.7201539e-13
 9.8876512e-01], sum to 1.0000
[2019-04-08 15:18:48,261] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2886
[2019-04-08 15:18:48,274] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.77062547785928, 0.7300907408450978, 0.0, 1.0, 65.0, 44371.10115381191], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 4161600.0000, 
sim time next is 4162200.0000, 
raw observation next is [-3.166666666666667, 50.66666666666667, 0.0, 0.0, 19.0, 26.77355685177853, 0.7224472197238706, 0.0, 1.0, 65.0, 43752.69653093933], 
processed observation next is [0.0, 0.17391304347826086, 0.3748845798707295, 0.5066666666666667, 0.0, 0.0, 0.08333333333333333, 0.7311297376482108, 0.7408157399079568, 0.0, 1.0, 1.0, 0.20834617395685395], 
reward next is 0.7917, 
noisyNet noise sample is [array([-1.4038591], dtype=float32), 0.18204015]. 
=============================================
[2019-04-08 15:18:49,475] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3655232e-15 3.0020825e-08 6.8836198e-05 1.1002045e-03 2.8380677e-08
 7.8124790e-10 6.6099233e-06 1.9345044e-12 3.9300132e-16 7.0793648e-19
 9.9882430e-01], sum to 1.0000
[2019-04-08 15:18:49,475] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0054
[2019-04-08 15:18:49,503] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 35.0, 114.0, 774.0, 19.0, 26.92546096571967, 0.7989556527548353, 0.0, 1.0, 65.0, 33937.76072231697], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 4185000.0000, 
sim time next is 4185600.0000, 
raw observation next is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 19.0, 26.9626567320031, 0.8080619602850515, 0.0, 1.0, 65.0, 33794.15381440238], 
processed observation next is [0.0, 0.43478260869565216, 0.42566943674976926, 0.35, 0.38277777777777766, 0.8640883977900552, 0.08333333333333333, 0.7468880610002584, 0.7693539867616838, 0.0, 1.0, 1.0, 0.16092454197334466], 
reward next is 0.8391, 
noisyNet noise sample is [array([0.9471324], dtype=float32), -1.6419855]. 
=============================================
[2019-04-08 15:18:49,947] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 111231: loss 0.0375
[2019-04-08 15:18:49,948] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 111231: learning rate 0.0000
[2019-04-08 15:18:50,077] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111284: loss 0.0089
[2019-04-08 15:18:50,080] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111285: learning rate 0.0000
[2019-04-08 15:18:50,120] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111301: loss 0.0147
[2019-04-08 15:18:50,120] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111301: learning rate 0.0000
[2019-04-08 15:18:50,519] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111447: loss 0.0111
[2019-04-08 15:18:50,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111447: learning rate 0.0000
[2019-04-08 15:18:50,606] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111473: loss 0.0367
[2019-04-08 15:18:50,610] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111473: learning rate 0.0000
[2019-04-08 15:18:50,704] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7701371e-14 4.5548763e-08 2.6429270e-04 2.2840954e-04 1.5842618e-08
 5.0407762e-09 3.0157809e-05 3.4752509e-10 5.1758670e-14 1.2755498e-15
 9.9947709e-01], sum to 1.0000
[2019-04-08 15:18:50,704] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9849
[2019-04-08 15:18:50,725] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 48.33333333333334, 0.0, 0.0, 19.0, 26.93057954765052, 0.6986205290585557, 0.0, 1.0, 65.0, 42193.83008746623], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4254600.0000, 
sim time next is 4255200.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.0, 26.92443957567109, 0.6968544878992421, 0.0, 1.0, 65.0, 42253.7875300681], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7437032979725909, 0.7322848292997474, 0.0, 1.0, 1.0, 0.20120851204794332], 
reward next is 0.7988, 
noisyNet noise sample is [array([-0.37277418], dtype=float32), 0.40099677]. 
=============================================
[2019-04-08 15:18:50,841] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111565: loss 0.0391
[2019-04-08 15:18:50,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111569: learning rate 0.0000
[2019-04-08 15:18:51,037] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 111644: loss 0.0219
[2019-04-08 15:18:51,041] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 111646: learning rate 0.0000
[2019-04-08 15:18:51,170] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111688: loss 0.0684
[2019-04-08 15:18:51,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111688: learning rate 0.0000
[2019-04-08 15:18:51,487] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111799: loss 0.0309
[2019-04-08 15:18:51,488] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111799: learning rate 0.0000
[2019-04-08 15:18:51,592] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 111844: loss 0.0275
[2019-04-08 15:18:51,596] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 111846: learning rate 0.0000
[2019-04-08 15:18:51,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8078370e-15 1.4889250e-08 6.8756810e-05 7.4543124e-05 8.3885721e-10
 6.9848488e-11 1.0657264e-05 1.3256241e-12 5.5284183e-16 8.9630272e-19
 9.9984598e-01], sum to 1.0000
[2019-04-08 15:18:51,713] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8262
[2019-04-08 15:18:51,731] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.85, 40.5, 24.0, 163.0, 19.0, 27.59411023244272, 0.9268047912709657, 0.0, 1.0, 65.0, 29544.29957888335], 
current ob forecast is [], 
actual action is [6.85, 65.0], 
sim time this is 4210200.0000, 
sim time next is 4210800.0000, 
raw observation next is [1.8, 40.66666666666667, 20.0, 135.8333333333333, 19.0, 27.55206151481748, 0.9180257197871273, 0.0, 1.0, 65.0, 30440.61556440621], 
processed observation next is [0.0, 0.7391304347826086, 0.5124653739612189, 0.40666666666666673, 0.06666666666666667, 0.1500920810313075, 0.08333333333333333, 0.7960051262347901, 0.8060085732623757, 0.0, 1.0, 1.0, 0.14495531221145813], 
reward next is 0.8550, 
noisyNet noise sample is [array([-0.8792385], dtype=float32), -1.2832866]. 
=============================================
[2019-04-08 15:18:51,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7767017e-13 6.9964130e-08 1.7550135e-04 1.5481244e-03 1.0556248e-06
 2.2528777e-10 6.0411062e-06 6.2763900e-12 1.8427229e-14 2.7095414e-16
 9.9826920e-01], sum to 1.0000
[2019-04-08 15:18:51,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1318
[2019-04-08 15:18:51,826] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.2, 42.5, 0.0, 0.0, 19.0, 27.34453376665451, 0.8541613502827539, 0.0, 1.0, 65.0, 34713.98795376601], 
current ob forecast is [], 
actual action is [6.2, 65.0], 
sim time this is 4217400.0000, 
sim time next is 4218000.0000, 
raw observation next is [1.133333333333333, 42.66666666666666, 0.0, 0.0, 19.0, 27.34229794959424, 0.8455368445774063, 0.0, 1.0, 65.0, 34445.78051955807], 
processed observation next is [0.0, 0.8260869565217391, 0.49399815327793173, 0.4266666666666666, 0.0, 0.0, 0.08333333333333333, 0.7785248291328534, 0.7818456148591354, 0.0, 1.0, 1.0, 0.16402752628360986], 
reward next is 0.8360, 
noisyNet noise sample is [array([-0.4337535], dtype=float32), -0.17252462]. 
=============================================
[2019-04-08 15:18:51,838] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.28774 ]
 [70.518135]
 [70.729836]
 [71.03545 ]
 [71.36529 ]], R is [[70.33109283]
 [70.46247864]
 [70.58834839]
 [70.71797943]
 [70.84825134]].
[2019-04-08 15:18:51,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1909446e-14 4.0094591e-08 2.5800839e-05 6.9705864e-05 2.8983621e-08
 1.4156573e-10 1.6720545e-06 8.1066349e-13 9.5510789e-16 4.0105498e-17
 9.9990273e-01], sum to 1.0000
[2019-04-08 15:18:51,866] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2300
[2019-04-08 15:18:51,897] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.666666666666667, 47.66666666666667, 0.0, 0.0, 19.0, 27.06736757824373, 0.7600497248550591, 0.0, 1.0, 65.0, 40302.72230710762], 
current ob forecast is [], 
actual action is [6.666666666666667, 65.0], 
sim time this is 4232400.0000, 
sim time next is 4233000.0000, 
raw observation next is [1.833333333333333, 47.83333333333333, 0.0, 0.0, 19.0, 27.06527190697109, 0.7580313111657778, 0.0, 1.0, 65.0, 40378.96890624587], 
processed observation next is [0.0, 1.0, 0.5133887349953832, 0.4783333333333333, 0.0, 0.0, 0.08333333333333333, 0.7554393255809243, 0.752677103721926, 0.0, 1.0, 1.0, 0.19228080431545652], 
reward next is 0.8077, 
noisyNet noise sample is [array([-0.47578585], dtype=float32), -0.06952779]. 
=============================================
[2019-04-08 15:18:51,922] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[69.962685]
 [69.8885  ]
 [69.855446]
 [69.78135 ]
 [69.74299 ]], R is [[70.13460541]
 [70.24134064]
 [70.34747314]
 [70.4526825 ]
 [70.55752563]].
[2019-04-08 15:18:52,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.2528925e-13 3.2240149e-07 3.3692323e-04 3.3808728e-03 3.1047921e-07
 1.7659750e-10 2.0520887e-04 1.3036194e-10 1.9637667e-13 1.4963025e-16
 9.9607629e-01], sum to 1.0000
[2019-04-08 15:18:52,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6631
[2019-04-08 15:18:52,174] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 19.0, 27.20358659438008, 0.8148597397239007, 0.0, 1.0, 65.0, 37752.23541920222], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4222200.0000, 
sim time next is 4222800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 19.0, 27.17654958021436, 0.8096152297868632, 0.0, 1.0, 65.0, 37863.4837777745], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.43, 0.0, 0.0, 0.08333333333333333, 0.7647124650178633, 0.7698717432622878, 0.0, 1.0, 1.0, 0.18030230370368808], 
reward next is 0.8197, 
noisyNet noise sample is [array([1.0729607], dtype=float32), -0.57777387]. 
=============================================
[2019-04-08 15:18:52,498] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.0993620e-15 1.7914923e-08 1.2968516e-04 4.8932678e-04 9.6148804e-09
 1.3852756e-10 7.5587785e-05 1.7228226e-11 1.1316919e-15 2.1295915e-17
 9.9930537e-01], sum to 1.0000
[2019-04-08 15:18:52,499] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-08 15:18:52,537] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 35.0, 113.0, 755.0, 19.0, 26.85172133699989, 0.7927015072840473, 0.0, 1.0, 65.0, 34769.75721259788], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 4184400.0000, 
sim time next is 4185000.0000, 
raw observation next is [-1.5, 35.0, 114.0, 774.0, 19.0, 26.92549560188636, 0.7989670337171345, 0.0, 1.0, 65.0, 33937.36336260921], 
processed observation next is [0.0, 0.43478260869565216, 0.4210526315789474, 0.35, 0.38, 0.8552486187845304, 0.08333333333333333, 0.7437913001571967, 0.7663223445723782, 0.0, 1.0, 1.0, 0.16160649220290102], 
reward next is 0.8384, 
noisyNet noise sample is [array([-0.07444198], dtype=float32), -1.4120148]. 
=============================================
[2019-04-08 15:18:52,542] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[74.759674]
 [74.364784]
 [73.99605 ]
 [73.54393 ]
 [73.04196 ]], R is [[75.16916656]
 [75.25189972]
 [75.325737  ]
 [75.3945694 ]
 [75.45806885]].
[2019-04-08 15:18:53,278] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.14215305e-13 4.03546210e-08 3.40933948e-05 1.29432196e-03
 3.29775958e-08 1.58989899e-09 1.85794743e-05 1.10433815e-10
 3.24201861e-14 5.38701738e-17 9.98652935e-01], sum to 1.0000
[2019-04-08 15:18:53,278] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7714
[2019-04-08 15:18:53,295] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 27.05381474794433, 0.7468969607833585, 0.0, 1.0, 65.0, 40936.88149744217], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4236000.0000, 
sim time next is 4236600.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 27.04552339095507, 0.7436079015865978, 0.0, 1.0, 65.0, 41187.44725143925], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7537936159129224, 0.7478693005288659, 0.0, 1.0, 1.0, 0.19613070119732975], 
reward next is 0.8039, 
noisyNet noise sample is [array([1.098965], dtype=float32), 0.77017117]. 
=============================================
[2019-04-08 15:18:53,885] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7000, global step 112633: loss 0.0566
[2019-04-08 15:18:53,886] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7000, global step 112633: learning rate 0.0000
[2019-04-08 15:18:53,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.8033133e-13 1.8848122e-07 5.7293952e-04 4.8738602e-03 7.6036503e-07
 8.0370499e-10 3.0159786e-06 3.4193589e-11 4.7501099e-14 1.5998631e-15
 9.9454921e-01], sum to 1.0000
[2019-04-08 15:18:53,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6829
[2019-04-08 15:18:53,990] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 46.33333333333334, 0.0, 0.0, 19.0, 26.95301107887868, 0.7017744386031396, 0.0, 1.0, 65.0, 42025.96464299474], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4252800.0000, 
sim time next is 4253400.0000, 
raw observation next is [3.0, 47.0, 0.0, 0.0, 19.0, 26.93839129389077, 0.700614410529036, 0.0, 1.0, 65.0, 42677.55699520446], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.47, 0.0, 0.0, 0.08333333333333333, 0.7448659411575642, 0.733538136843012, 0.0, 1.0, 1.0, 0.203226461881926], 
reward next is 0.7968, 
noisyNet noise sample is [array([-0.5622748], dtype=float32), 1.4313821]. 
=============================================
[2019-04-08 15:18:54,457] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112818: loss 0.1163
[2019-04-08 15:18:54,459] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112818: learning rate 0.0000
[2019-04-08 15:18:54,772] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112926: loss 0.0639
[2019-04-08 15:18:54,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112927: learning rate 0.0000
[2019-04-08 15:18:55,071] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.2655238e-16 6.8963457e-09 4.2260050e-05 5.5274868e-04 1.2633786e-08
 1.2212698e-10 3.4139732e-06 3.6510517e-12 9.5988348e-17 1.3159737e-18
 9.9940157e-01], sum to 1.0000
[2019-04-08 15:18:55,075] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8839
[2019-04-08 15:18:55,110] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 43.0, 0.0, 0.0, 19.0, 27.17184475665765, 0.8707321427412177, 0.0, 1.0, 65.0, 37718.18267564334], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4143600.0000, 
sim time next is 4144200.0000, 
raw observation next is [-0.1666666666666667, 42.83333333333334, 0.0, 0.0, 19.0, 27.16091915825259, 0.8658199292851037, 0.0, 1.0, 65.0, 38127.07642700438], 
processed observation next is [1.0, 1.0, 0.4579870729455217, 0.42833333333333345, 0.0, 0.0, 0.08333333333333333, 0.7634099298543825, 0.7886066430950346, 0.0, 1.0, 1.0, 0.18155750679525895], 
reward next is 0.8184, 
noisyNet noise sample is [array([-1.1635575], dtype=float32), -0.09905419]. 
=============================================
[2019-04-08 15:18:55,269] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0393562e-15 1.3231241e-08 3.4294120e-05 1.3839001e-05 1.1309080e-08
 8.0046296e-11 1.1975812e-05 4.2134313e-11 2.3815160e-16 2.4307819e-19
 9.9993992e-01], sum to 1.0000
[2019-04-08 15:18:55,304] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3989
[2019-04-08 15:18:55,349] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8333333333333334, 42.16666666666666, 0.0, 0.0, 19.0, 27.07200009071028, 0.8411753803108435, 0.0, 1.0, 65.0, 39451.23444811287], 
current ob forecast is [], 
actual action is [4.166666666666667, 65.0], 
sim time this is 4146600.0000, 
sim time next is 4147200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.03800703083269, 0.834407469794855, 0.0, 1.0, 65.0, 39800.05981337612], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.753167252569391, 0.7781358232649517, 0.0, 1.0, 1.0, 0.18952409434941012], 
reward next is 0.8105, 
noisyNet noise sample is [array([0.4444332], dtype=float32), 1.2794204]. 
=============================================
[2019-04-08 15:18:55,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1655612e-14 3.2392649e-07 7.2424431e-05 9.3452272e-04 1.6973406e-08
 5.4603683e-11 1.2624818e-04 1.8373931e-11 3.4846711e-15 1.1258937e-18
 9.9886644e-01], sum to 1.0000
[2019-04-08 15:18:55,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5559
[2019-04-08 15:18:55,668] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.6, 41.33333333333334, 0.0, 0.0, 19.0, 27.46616509814622, 0.8869922183331048, 0.0, 1.0, 65.0, 32637.34737250673], 
current ob forecast is [], 
actual action is [6.6, 65.0], 
sim time this is 4213200.0000, 
sim time next is 4213800.0000, 
raw observation next is [1.55, 41.5, 0.0, 0.0, 19.0, 27.4465503860957, 0.8792709804068011, 0.0, 1.0, 65.0, 33185.59671566263], 
processed observation next is [0.0, 0.782608695652174, 0.5055401662049862, 0.415, 0.0, 0.0, 0.08333333333333333, 0.7872125321746418, 0.793090326802267, 0.0, 1.0, 1.0, 0.1580266510269649], 
reward next is 0.8420, 
noisyNet noise sample is [array([-2.2648938], dtype=float32), 0.18278655]. 
=============================================
[2019-04-08 15:18:56,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0922592e-17 9.1695151e-10 1.4268165e-05 2.8453392e-04 2.8694427e-11
 4.1488853e-13 9.3661022e-08 2.9945935e-14 8.6890526e-19 4.3656937e-21
 9.9970108e-01], sum to 1.0000
[2019-04-08 15:18:56,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9808
[2019-04-08 15:18:56,093] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 52.0, 154.0, 782.0, 19.0, 27.69608126947994, 0.9711399914303782, 0.0, 1.0, 65.0, 24084.67566809656], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 4280400.0000, 
sim time next is 4281000.0000, 
raw observation next is [7.0, 52.83333333333334, 165.3333333333333, 760.3333333333333, 19.0, 27.70427975877761, 0.9778296124336542, 0.0, 1.0, 65.0, 24892.9375521898], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5283333333333334, 0.551111111111111, 0.840147329650092, 0.08333333333333333, 0.8086899798981341, 0.8259432041445515, 0.0, 1.0, 1.0, 0.11853779786757047], 
reward next is 0.8815, 
noisyNet noise sample is [array([0.68327165], dtype=float32), 0.42257288]. 
=============================================
[2019-04-08 15:18:56,106] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1966420e-14 4.1800883e-09 7.9443620e-05 5.0811900e-04 2.7993021e-08
 9.1256169e-11 5.8444516e-07 3.4361423e-11 5.6920690e-15 2.5996123e-17
 9.9941182e-01], sum to 1.0000
[2019-04-08 15:18:56,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4212
[2019-04-08 15:18:56,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[88.63815]
 [88.48007]
 [88.35125]
 [88.18957]
 [88.06052]], R is [[88.79512787]
 [88.7924881 ]
 [88.78946686]
 [88.78031921]
 [88.77056122]].
[2019-04-08 15:18:56,139] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 19.0, 27.15582183459871, 0.8021340802363802, 0.0, 1.0, 65.0, 38167.65746571639], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4224000.0000, 
sim time next is 4224600.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 19.0, 27.15919044199559, 0.7996075031921127, 0.0, 1.0, 65.0, 38305.01234937115], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7632658701662992, 0.7665358343973709, 0.0, 1.0, 1.0, 0.1824048207112912], 
reward next is 0.8176, 
noisyNet noise sample is [array([0.7631243], dtype=float32), -0.6642481]. 
=============================================
[2019-04-08 15:18:56,160] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9347515e-15 8.8273815e-09 7.0388101e-05 3.6946134e-04 3.4731318e-08
 3.2257361e-10 7.3830238e-06 7.5557963e-11 1.5090479e-15 8.0350598e-17
 9.9955267e-01], sum to 1.0000
[2019-04-08 15:18:56,165] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8061
[2019-04-08 15:18:56,195] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 52.33333333333334, 134.0, 79.0, 19.0, 26.93820174644309, 0.7257198512453428, 0.0, 1.0, 65.0, 40219.89107328861], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4265400.0000, 
sim time next is 4266000.0000, 
raw observation next is [3.0, 53.0, 146.0, 92.0, 19.0, 26.96150543677095, 0.7304359143028929, 0.0, 1.0, 65.0, 39872.20141108742], 
processed observation next is [0.0, 0.391304347826087, 0.5457063711911359, 0.53, 0.4866666666666667, 0.10165745856353592, 0.08333333333333333, 0.7467921197309124, 0.7434786381009643, 0.0, 1.0, 1.0, 0.18986762576708296], 
reward next is 0.8101, 
noisyNet noise sample is [array([0.5110178], dtype=float32), -0.33265033]. 
=============================================
[2019-04-08 15:18:56,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[73.95106 ]
 [73.46557 ]
 [72.982925]
 [72.56494 ]
 [72.146126]], R is [[74.55826569]
 [74.62116241]
 [74.68249512]
 [74.73858643]
 [74.79259491]].
[2019-04-08 15:18:57,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4384045e-17 1.1413587e-09 3.6103051e-04 1.9216619e-04 1.5662150e-09
 4.4811333e-13 7.5885976e-07 7.5725109e-13 1.4991783e-16 1.2240906e-20
 9.9944597e-01], sum to 1.0000
[2019-04-08 15:18:57,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0062
[2019-04-08 15:18:57,470] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 19.0, 27.99600593803088, 1.029628627035842, 0.0, 1.0, 65.0, 25363.58644679427], 
current ob forecast is [], 
actual action is [11.333333333333332, 65.0], 
sim time this is 4297200.0000, 
sim time next is 4297800.0000, 
raw observation next is [6.266666666666667, 63.33333333333334, 16.0, 152.0, 19.0, 27.97404792871028, 1.020708207594207, 0.0, 1.0, 65.0, 25907.90050279263], 
processed observation next is [0.0, 0.7391304347826086, 0.6361957525392429, 0.6333333333333334, 0.05333333333333334, 0.16795580110497238, 0.08333333333333333, 0.8311706607258568, 0.840236069198069, 0.0, 1.0, 1.0, 0.12337095477520299], 
reward next is 0.8766, 
noisyNet noise sample is [array([-1.7586485], dtype=float32), 0.81111926]. 
=============================================
[2019-04-08 15:18:58,000] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 114021: loss 0.1802
[2019-04-08 15:18:58,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 114021: learning rate 0.0000
[2019-04-08 15:18:58,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4095001e-20 1.9043250e-11 2.5371035e-06 8.2523884e-06 7.4540062e-11
 5.1001463e-15 1.5949997e-08 1.0864670e-17 5.0400179e-22 1.8719186e-25
 9.9998915e-01], sum to 1.0000
[2019-04-08 15:18:58,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8762
[2019-04-08 15:18:58,216] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.73333333333333, 36.66666666666666, 115.8333333333333, 788.0, 22.5, 28.88580182079835, 1.188661361151829, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.73333333333333, 65.0], 
sim time this is 4358400.0000, 
sim time next is 4359000.0000, 
raw observation next is [12.16666666666667, 35.33333333333334, 116.6666666666667, 796.0, 22.5, 28.93454961036947, 1.208005227686827, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7996306555863344, 0.35333333333333344, 0.388888888888889, 0.8795580110497238, 0.375, 0.9112124675307891, 0.9026684092289422, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.91648996], dtype=float32), 0.38814428]. 
=============================================
[2019-04-08 15:18:58,227] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[104.12072]
 [104.02273]
 [103.83849]
 [103.24943]
 [102.75541]], R is [[104.30560303]
 [104.26255035]
 [104.21992493]
 [104.13285065]
 [104.04664612]].
[2019-04-08 15:18:58,285] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1769135e-13 3.3365769e-08 5.5772831e-05 6.4576522e-04 8.0659461e-08
 2.2488422e-09 1.4320465e-05 5.7433915e-11 3.7014777e-15 1.1675323e-16
 9.9928409e-01], sum to 1.0000
[2019-04-08 15:18:58,285] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5444
[2019-04-08 15:18:58,308] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.99164185293426, 0.723411517153442, 0.0, 1.0, 65.0, 41949.44553390743], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4244400.0000, 
sim time next is 4245000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.99558991911917, 0.7220759204274438, 0.0, 1.0, 65.0, 41686.64372259766], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7496324932599308, 0.7406919734758146, 0.0, 1.0, 1.0, 0.19850782725046506], 
reward next is 0.8015, 
noisyNet noise sample is [array([0.93811864], dtype=float32), 1.1659861]. 
=============================================
[2019-04-08 15:18:58,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.19798]
 [73.21548]
 [73.18879]
 [73.17353]
 [73.06211]], R is [[73.39048004]
 [73.45681763]
 [73.52264404]
 [73.58834076]
 [73.65331268]].
[2019-04-08 15:18:58,754] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4002121e-15 1.2480607e-08 5.5336668e-06 1.9108073e-04 2.2437279e-08
 1.8582041e-12 2.5022100e-06 4.3427445e-12 9.1169438e-17 3.8501610e-19
 9.9980086e-01], sum to 1.0000
[2019-04-08 15:18:58,770] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0185
[2019-04-08 15:18:58,793] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.05, 73.5, 0.0, 0.0, 19.0, 27.57930903972446, 0.9208353077985331, 0.0, 1.0, 65.0, 33180.02292033105], 
current ob forecast is [], 
actual action is [10.05, 65.0], 
sim time this is 4309800.0000, 
sim time next is 4310400.0000, 
raw observation next is [5.0, 74.0, 0.0, 0.0, 19.0, 27.56302310411224, 0.9187863350424951, 0.0, 1.0, 65.0, 33431.43522688928], 
processed observation next is [0.0, 0.9130434782608695, 0.6011080332409973, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7969185920093533, 0.8062621116808316, 0.0, 1.0, 1.0, 0.15919731060423467], 
reward next is 0.8408, 
noisyNet noise sample is [array([-0.3794457], dtype=float32), 1.4210517]. 
=============================================
[2019-04-08 15:18:59,836] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7000, global step 114569: loss 0.1439
[2019-04-08 15:18:59,838] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7000, global step 114570: learning rate 0.0000
[2019-04-08 15:19:00,858] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8428782e-20 5.7532997e-12 5.3102369e-08 2.6777134e-06 2.6579469e-10
 4.9312757e-14 1.5216484e-07 2.1043371e-15 2.2956475e-20 3.9676068e-23
 9.9999714e-01], sum to 1.0000
[2019-04-08 15:19:00,872] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7049
[2019-04-08 15:19:00,884] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.5, 43.0, 6.000000000000001, 0.0, 22.5, 29.34058856955069, 1.410080257157008, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [17.5, 65.0], 
sim time this is 4384200.0000, 
sim time next is 4384800.0000, 
raw observation next is [12.4, 44.0, 0.0, 0.0, 22.5, 29.36628641015039, 1.400333368509644, 1.0, 1.0, 65.0, 18849.43849605016], 
processed observation next is [1.0, 0.782608695652174, 0.806094182825485, 0.44, 0.0, 0.0, 0.375, 0.9471905341791992, 0.9667777895032147, 1.0, 1.0, 1.0, 0.08975923093357219], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.36279047], dtype=float32), 0.89115584]. 
=============================================
[2019-04-08 15:19:02,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6392469e-18 2.4625843e-10 6.7603992e-06 8.5942935e-05 3.8007408e-10
 3.7266251e-13 9.1588959e-07 1.9922386e-14 6.5641941e-19 3.7194857e-22
 9.9990642e-01], sum to 1.0000
[2019-04-08 15:19:02,362] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9159
[2019-04-08 15:19:02,400] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 19.0, 28.43616824924142, 1.252535056542204, 0.0, 1.0, 65.0, 18848.81532229035], 
current ob forecast is [], 
actual action is [14.933333333333334, 65.0], 
sim time this is 4396800.0000, 
sim time next is 4397400.0000, 
raw observation next is [9.8, 60.0, 0.0, 0.0, 19.0, 28.41293284400172, 1.247579604807805, 0.0, 1.0, 65.0, 18849.02538353501], 
processed observation next is [1.0, 0.9130434782608695, 0.7340720221606649, 0.6, 0.0, 0.0, 0.08333333333333333, 0.8677444036668099, 0.9158598682692682, 0.0, 1.0, 1.0, 0.08975726373111909], 
reward next is 0.9102, 
noisyNet noise sample is [array([2.1991436], dtype=float32), 1.8678185]. 
=============================================
[2019-04-08 15:19:02,979] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.04205174e-18 2.65132066e-11 1.13882897e-05 1.66951816e-06
 4.59210924e-12 7.26616982e-14 1.19048025e-07 1.55468034e-17
 1.01295342e-20 2.03205426e-23 9.99986768e-01], sum to 1.0000
[2019-04-08 15:19:02,980] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7463
[2019-04-08 15:19:02,992] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 22.5, 28.08864933866301, 1.012618425235919, 1.0, 1.0, 65.0, 18844.40519588906], 
current ob forecast is [], 
actual action is [12.533333333333333, 65.0], 
sim time this is 4353600.0000, 
sim time next is 4354200.0000, 
raw observation next is [8.15, 49.5, 107.0, 677.0, 22.5, 28.27688129070329, 1.044503755749896, 1.0, 1.0, 64.99999999999997, 18845.71905301585], 
processed observation next is [1.0, 0.391304347826087, 0.6883656509695293, 0.495, 0.3566666666666667, 0.7480662983425415, 0.375, 0.8564067742252742, 0.8481679185832988, 1.0, 1.0, 0.9999999999999994, 0.08974151930007548], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.5324366], dtype=float32), 1.3280202]. 
=============================================
[2019-04-08 15:19:04,568] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.7897359e-21 9.0369612e-10 1.7684463e-06 7.6368900e-07 4.4097052e-12
 6.3567282e-15 5.2836804e-06 2.9366958e-16 1.4581482e-21 1.4189997e-23
 9.9999213e-01], sum to 1.0000
[2019-04-08 15:19:04,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2595
[2019-04-08 15:19:04,586] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 22.5, 28.08866267615645, 1.012621213243752, 1.0, 1.0, 65.0, 18844.4052463483], 
current ob forecast is [], 
actual action is [12.533333333333333, 65.0], 
sim time this is 4353600.0000, 
sim time next is 4354200.0000, 
raw observation next is [8.15, 49.5, 107.0, 677.0, 22.5, 28.27689474514757, 1.044506583061357, 1.0, 1.0, 64.99999999999997, 18845.71913531793], 
processed observation next is [1.0, 0.391304347826087, 0.6883656509695293, 0.495, 0.3566666666666667, 0.7480662983425415, 0.375, 0.8564078954289641, 0.8481688610204522, 1.0, 1.0, 0.9999999999999994, 0.08974151969199014], 
reward next is 0.9103, 
noisyNet noise sample is [array([-0.6282691], dtype=float32), -1.6048245]. 
=============================================
[2019-04-08 15:19:06,726] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8525802e-23 2.7716149e-11 2.0652038e-08 2.5742303e-08 8.0835783e-14
 8.3119773e-17 9.0170673e-08 3.0425370e-19 1.5290227e-23 1.6697689e-27
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:19:06,726] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5119
[2019-04-08 15:19:06,739] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.75, 32.5, 133.6666666666667, 209.6666666666666, 22.5, 29.59112899771009, 1.474193059291668, 1.0, 1.0, 64.99999999999999, 0.0], 
current ob forecast is [], 
actual action is [18.75, 65.0], 
sim time this is 4374600.0000, 
sim time next is 4375200.0000, 
raw observation next is [13.6, 33.0, 118.3333333333333, 104.8333333333333, 22.5, 29.84062506542635, 1.313616348818232, 1.0, 1.0, 65.00000000000001, 9424.628810411323], 
processed observation next is [1.0, 0.6521739130434783, 0.8393351800554018, 0.33, 0.3944444444444443, 0.11583793738489867, 0.375, 0.986718755452196, 0.9378721162727439, 1.0, 1.0, 1.0000000000000002, 0.04487918481148249], 
reward next is 0.9551, 
noisyNet noise sample is [array([0.47978032], dtype=float32), -0.8667666]. 
=============================================
[2019-04-08 15:19:07,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.4907190e-21 1.7641521e-12 1.5776531e-07 3.9015926e-08 2.1245902e-13
 1.0225238e-15 4.9610727e-10 1.4308046e-17 2.8326674e-20 1.5972686e-24
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:19:07,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8813
[2019-04-08 15:19:07,450] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 19.0, 27.84659048168979, 1.017902197440479, 0.0, 1.0, 65.0, 24061.75380492421], 
current ob forecast is [], 
actual action is [11.933333333333334, 65.0], 
sim time this is 4285200.0000, 
sim time next is 4285800.0000, 
raw observation next is [6.9, 58.5, 229.0, 385.0, 19.0, 27.85413424584846, 1.018991138144648, 0.0, 1.0, 65.0, 24181.46484059921], 
processed observation next is [0.0, 0.6086956521739131, 0.6537396121883658, 0.585, 0.7633333333333333, 0.425414364640884, 0.08333333333333333, 0.821177853820705, 0.8396637127148826, 0.0, 1.0, 1.0, 0.11514983257428195], 
reward next is 0.8849, 
noisyNet noise sample is [array([0.29365063], dtype=float32), -0.21011105]. 
=============================================
[2019-04-08 15:19:07,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1221795e-18 1.7945864e-10 5.0649473e-06 8.6967782e-05 1.2138077e-10
 2.6903227e-13 1.5567049e-06 1.1093407e-14 7.8549929e-19 1.6719690e-22
 9.9990642e-01], sum to 1.0000
[2019-04-08 15:19:07,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2388
[2019-04-08 15:19:07,632] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.4, 61.0, 0.0, 0.0, 19.0, 28.33304591008202, 1.232379015543568, 0.0, 1.0, 65.0, 18848.53060253423], 
current ob forecast is [], 
actual action is [14.4, 65.0], 
sim time this is 4399200.0000, 
sim time next is 4399800.0000, 
raw observation next is [9.25, 61.16666666666667, 0.0, 0.0, 19.0, 28.31038503168595, 1.182724353347348, 0.0, 1.0, 65.0, 18848.0584147371], 
processed observation next is [1.0, 0.9565217391304348, 0.718836565096953, 0.6116666666666667, 0.0, 0.0, 0.08333333333333333, 0.8591987526404958, 0.8942414511157827, 0.0, 1.0, 1.0, 0.08975265911779572], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.14990738], dtype=float32), -0.4257455]. 
=============================================
[2019-04-08 15:19:08,554] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2971802e-19 1.1016875e-09 2.0874880e-07 2.5312818e-05 1.4591697e-10
 4.6942797e-12 9.7933582e-07 1.0263857e-14 1.4680571e-19 1.1769819e-21
 9.9997354e-01], sum to 1.0000
[2019-04-08 15:19:08,557] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8365
[2019-04-08 15:19:08,574] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 57.0, 208.5, 551.0, 19.0, 27.80950688411016, 1.009509791403542, 0.0, 1.0, 65.0, 23839.16053733557], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 4284000.0000, 
sim time next is 4284600.0000, 
raw observation next is [6.966666666666667, 57.5, 215.3333333333333, 495.6666666666666, 19.0, 27.82445958369681, 1.014611866358674, 0.0, 1.0, 65.0, 23893.30033378063], 
processed observation next is [0.0, 0.6086956521739131, 0.6555863342566944, 0.575, 0.7177777777777776, 0.5476979742173111, 0.08333333333333333, 0.8187049653080676, 0.8382039554528914, 0.0, 1.0, 1.0, 0.11377762063705062], 
reward next is 0.8862, 
noisyNet noise sample is [array([-0.28850257], dtype=float32), -0.59597677]. 
=============================================
[2019-04-08 15:19:10,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8241601e-14 4.5425588e-09 6.0720777e-04 5.9981691e-03 2.2090632e-08
 1.3724877e-10 5.8124356e-06 1.8493613e-11 5.3173514e-15 1.6320631e-17
 9.9338877e-01], sum to 1.0000
[2019-04-08 15:19:10,451] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9692
[2019-04-08 15:19:10,474] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.11483232792936, 0.8366384857875452, 0.0, 1.0, 65.0, 42003.63403831513], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 4497600.0000, 
sim time next is 4498200.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.04357829295056, 0.8313300955690229, 0.0, 1.0, 65.0, 44350.42828182254], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7536315244125467, 0.777110031856341, 0.0, 1.0, 1.0, 0.21119251562772637], 
reward next is 0.7888, 
noisyNet noise sample is [array([1.0016315], dtype=float32), 0.32643184]. 
=============================================
[2019-04-08 15:19:10,516] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9829235e-22 1.6708708e-12 4.3429544e-08 2.5283248e-07 3.1982934e-13
 3.2390273e-16 4.8957563e-08 8.8991606e-18 6.1562599e-24 1.0235726e-24
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:19:10,529] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8878
[2019-04-08 15:19:10,544] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 88.5, 85.0, 0.0, 22.5, 28.37858846136444, 1.137336831740631, 1.0, 1.0, 65.0, 23818.3269437478], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4458600.0000, 
sim time next is 4459200.0000, 
raw observation next is [0.0, 87.33333333333334, 82.66666666666667, 0.0, 22.5, 28.4016219344513, 0.8754805646827095, 1.0, 1.0, 65.0, 30049.5737557725], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.8733333333333334, 0.27555555555555555, 0.0, 0.375, 0.8668018278709416, 0.7918268548942365, 1.0, 1.0, 1.0, 0.14309320836082143], 
reward next is 0.8569, 
noisyNet noise sample is [array([-0.19453757], dtype=float32), 1.0640327]. 
=============================================
[2019-04-08 15:19:10,572] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 117943: loss 0.8249
[2019-04-08 15:19:10,581] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 117944: learning rate 0.0000
[2019-04-08 15:19:11,433] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5845471e-19 6.6706396e-10 3.2657679e-06 1.7655356e-06 4.5413322e-11
 2.3190098e-13 1.5495910e-07 7.2981706e-15 3.3928167e-20 4.4405351e-22
 9.9999475e-01], sum to 1.0000
[2019-04-08 15:19:11,436] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7638
[2019-04-08 15:19:11,464] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.89640824502982, 1.040241351285154, 1.0, 1.0, 65.0, 27395.50350190076], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4471800.0000, 
sim time next is 4472400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.8532695000775, 1.037048536398735, 1.0, 1.0, 65.0, 31276.02483685227], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.8211057916731249, 0.845682845466245, 1.0, 1.0, 1.0, 0.14893345160405844], 
reward next is 0.8511, 
noisyNet noise sample is [array([-0.9917509], dtype=float32), -0.99355686]. 
=============================================
[2019-04-08 15:19:11,894] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3961272e-17 2.5226128e-09 5.9544574e-05 4.0182110e-04 2.0116500e-10
 1.7293629e-12 1.6232305e-05 5.0561324e-13 3.8483000e-17 5.2594127e-20
 9.9952245e-01], sum to 1.0000
[2019-04-08 15:19:11,894] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6547
[2019-04-08 15:19:11,917] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.21858592742303, 0.9437677598496613, 0.0, 1.0, 65.0, 38527.55149275807], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4482600.0000, 
sim time next is 4483200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.20469426012577, 0.940608104217164, 0.0, 1.0, 65.0, 38713.37431477677], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7670578550104808, 0.8135360347390547, 0.0, 1.0, 1.0, 0.184349401498937], 
reward next is 0.8157, 
noisyNet noise sample is [array([0.34943604], dtype=float32), -1.0721635]. 
=============================================
[2019-04-08 15:19:12,803] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.9722733e-17 2.8046945e-11 4.2415107e-07 2.3695622e-05 5.5047822e-10
 1.0471096e-13 1.1718565e-07 4.3130520e-15 8.4710165e-19 1.3722817e-20
 9.9997580e-01], sum to 1.0000
[2019-04-08 15:19:12,805] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1433
[2019-04-08 15:19:12,820] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.05, 72.0, 0.0, 0.0, 19.0, 27.14429404843169, 0.882311807751973, 0.0, 1.0, 65.0, 40646.92675481901], 
current ob forecast is [], 
actual action is [4.95, 65.0], 
sim time this is 4486200.0000, 
sim time next is 4486800.0000, 
raw observation next is [-0.09999999999999999, 72.0, 0.0, 0.0, 19.0, 27.12729090110815, 0.8836075490211478, 0.0, 1.0, 65.0, 45089.15585896776], 
processed observation next is [1.0, 0.9565217391304348, 0.4598337950138504, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7606075750923459, 0.7945358496737159, 0.0, 1.0, 1.0, 0.21471026599508455], 
reward next is 0.7853, 
noisyNet noise sample is [array([-1.8040962], dtype=float32), -2.0465431]. 
=============================================
[2019-04-08 15:19:12,893] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 119099: loss 0.3312
[2019-04-08 15:19:12,897] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 119100: learning rate 0.0000
[2019-04-08 15:19:12,982] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119149: loss 0.4468
[2019-04-08 15:19:12,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119151: learning rate 0.0000
[2019-04-08 15:19:13,068] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6401135e-18 2.4701324e-08 3.8067778e-06 2.1258325e-04 1.4812189e-09
 4.8346145e-12 1.1045272e-06 1.6954615e-13 5.8713842e-18 6.1734351e-22
 9.9978250e-01], sum to 1.0000
[2019-04-08 15:19:13,071] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0973
[2019-04-08 15:19:13,083] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 47.0, 264.0, 113.0, 22.5, 28.11556201987737, 1.063681177203487, 1.0, 1.0, 65.0, 22219.0147182957], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4545000.0000, 
sim time next is 4545600.0000, 
raw observation next is [3.0, 46.33333333333334, 245.5, 96.16666666666667, 22.5, 28.1919612769373, 1.070704814521082, 1.0, 1.0, 65.0, 20037.154679342], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.46333333333333343, 0.8183333333333334, 0.10626151012891345, 0.375, 0.8493301064114416, 0.8569016048403606, 1.0, 1.0, 1.0, 0.09541502228258095], 
reward next is 0.9046, 
noisyNet noise sample is [array([-0.10729501], dtype=float32), 1.161417]. 
=============================================
[2019-04-08 15:19:13,309] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119323: loss -5.4128
[2019-04-08 15:19:13,310] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119324: learning rate 0.0000
[2019-04-08 15:19:13,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3374468e-16 1.5122250e-08 2.5056817e-05 8.4751409e-05 2.7225552e-10
 3.0893016e-12 1.9406589e-05 3.1508498e-13 4.8108265e-17 3.3418026e-20
 9.9987078e-01], sum to 1.0000
[2019-04-08 15:19:13,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1913
[2019-04-08 15:19:13,384] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.06666666666667, 59.33333333333334, 0.0, 0.0, 19.0, 28.46687157581692, 1.257379231642486, 0.0, 1.0, 65.0, 18848.35848066443], 
current ob forecast is [], 
actual action is [15.06666666666667, 65.0], 
sim time this is 4396200.0000, 
sim time next is 4396800.0000, 
raw observation next is [9.933333333333334, 59.66666666666667, 0.0, 0.0, 19.0, 28.43703532640996, 1.252802602101396, 0.0, 1.0, 65.0, 18848.84607208128], 
processed observation next is [1.0, 0.9130434782608695, 0.7377654662973223, 0.5966666666666667, 0.0, 0.0, 0.08333333333333333, 0.8697529438674966, 0.9176008673671321, 0.0, 1.0, 1.0, 0.08975640986705372], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.57445675], dtype=float32), -0.38246825]. 
=============================================
[2019-04-08 15:19:13,556] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119447: loss 0.4255
[2019-04-08 15:19:13,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119448: learning rate 0.0000
[2019-04-08 15:19:13,580] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119459: loss 0.3826
[2019-04-08 15:19:13,581] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119459: learning rate 0.0000
[2019-04-08 15:19:13,985] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119636: loss 0.3206
[2019-04-08 15:19:13,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119636: learning rate 0.0000
[2019-04-08 15:19:14,023] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 119657: loss 0.2981
[2019-04-08 15:19:14,024] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 119657: learning rate 0.0000
[2019-04-08 15:19:14,089] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119686: loss 0.3457
[2019-04-08 15:19:14,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119688: learning rate 0.0000
[2019-04-08 15:19:14,104] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119700: loss 0.3400
[2019-04-08 15:19:14,106] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119700: learning rate 0.0000
[2019-04-08 15:19:14,422] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 119859: loss 0.3528
[2019-04-08 15:19:14,424] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 119860: learning rate 0.0000
[2019-04-08 15:19:14,710] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-08 15:19:14,711] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:19:14,713] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:19:14,713] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:19:14,713] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:19:14,714] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:19:14,714] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:19:14,719] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run7
[2019-04-08 15:19:14,742] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run7
[2019-04-08 15:19:14,742] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run7
[2019-04-08 15:19:49,660] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.055661853]
[2019-04-08 15:19:49,661] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [4.7, 80.0, 0.0, 0.0, 22.5, 26.77332333860475, 0.7909732976128784, 0.0, 1.0, 65.0, 59669.07979978202]
[2019-04-08 15:19:49,661] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:19:49,661] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [1.22387045e-14 2.30255068e-08 6.26130786e-05 2.19923721e-04
 1.46204178e-08 1.31891997e-10 1.58279254e-05 5.64368951e-12
 8.59882119e-16 1.31502932e-17 9.99701560e-01], sampled 0.20451281646595876
[2019-04-08 15:20:10,972] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.055661853]
[2019-04-08 15:20:10,973] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-5.0, 59.0, 0.0, 0.0, 22.5, 26.66509050811357, 0.7139743735777432, 0.0, 1.0, 65.0, 66846.50899748591]
[2019-04-08 15:20:10,973] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:20:10,974] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [5.4046411e-15 2.3171319e-08 4.7788733e-05 1.2561031e-04 7.6058146e-09
 7.5953972e-11 7.5156099e-06 2.2036797e-12 3.4916006e-16 3.6688905e-18
 9.9981910e-01], sampled 0.8129286191648499
[2019-04-08 15:20:43,519] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.8610 316079198.4444 2957.8912
[2019-04-08 15:20:43,552] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:43,552] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:43,552] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:43,552] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:43,552] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:43,552] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:43,552] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:43,671] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:43,671] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:43,671] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:43,671] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:43,671] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:43,671] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:43,671] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:49,030] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.1044 355928068.0522 2370.4700
[2019-04-08 15:20:49,050] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:49,050] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:49,050] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:49,050] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:49,050] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:49,050] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:49,050] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:49,164] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:49,164] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:49,164] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:49,164] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:49,164] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:49,164] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:49,164] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:53,156] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.6776 342787704.4394 2767.8268
[2019-04-08 15:20:53,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:53,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:53,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:53,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:53,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:53,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:53,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:20:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:53,285] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:20:54,178] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 120000, evaluation results [120000.0, 6863.677597907764, 342787704.4393716, 2767.8268011239306, 6990.860959788622, 316079198.4443895, 2957.891224127164, 6801.104437846777, 355928068.0521759, 2370.469993354614]
[2019-04-08 15:20:54,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5402920e-16 3.6148646e-09 2.0192156e-04 7.0920010e-05 3.0097097e-10
 9.7501607e-13 1.7261697e-06 6.8034344e-14 3.7496031e-18 6.4631224e-21
 9.9972540e-01], sum to 1.0000
[2019-04-08 15:20:54,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3849
[2019-04-08 15:20:54,536] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 187.0, 24.0, 22.5, 26.9985837465825, 0.962485442812644, 1.0, 1.0, 65.0, 59450.23381498924], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4539600.0000, 
sim time next is 4540200.0000, 
raw observation next is [2.166666666666667, 51.5, 207.0, 32.0, 22.5, 26.52738068407835, 0.8873710276373746, 1.0, 1.0, 65.0, 31148.52048401106], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.69, 0.03535911602209945, 0.375, 0.710615057006529, 0.7957903425457915, 1.0, 1.0, 1.0, 0.14832628801910028], 
reward next is 0.8517, 
noisyNet noise sample is [array([-0.25910228], dtype=float32), -1.3007395]. 
=============================================
[2019-04-08 15:20:55,280] A3C_AGENT_WORKER-Thread-8 INFO:Local step 7500, global step 120604: loss 0.3916
[2019-04-08 15:20:55,282] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 7500, global step 120604: learning rate 0.0000
[2019-04-08 15:20:55,592] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120776: loss 0.3718
[2019-04-08 15:20:55,593] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120776: learning rate 0.0000
[2019-04-08 15:20:55,860] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0851905e-17 1.9618744e-08 1.1276503e-04 9.2421951e-06 2.3109629e-10
 1.5292727e-13 8.4382748e-08 3.1025047e-15 4.1222666e-18 1.5882965e-22
 9.9987793e-01], sum to 1.0000
[2019-04-08 15:20:55,864] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5349
[2019-04-08 15:20:55,900] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.66666666666666, 227.0, 79.33333333333334, 22.5, 28.26431693626046, 0.9542856740863498, 1.0, 1.0, 65.0, 34712.99613126436], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4546200.0000, 
sim time next is 4546800.0000, 
raw observation next is [3.0, 45.0, 208.5, 62.5, 22.5, 27.65962081048228, 1.064863788206551, 1.0, 1.0, 65.0, 50907.72017540806], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.695, 0.06906077348066299, 0.375, 0.8049684008735234, 0.8549545960688504, 1.0, 1.0, 1.0, 0.24241771512099075], 
reward next is 0.7576, 
noisyNet noise sample is [array([2.6116385], dtype=float32), 0.042570163]. 
=============================================
[2019-04-08 15:20:56,029] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 121004: loss 0.3510
[2019-04-08 15:20:56,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 121005: learning rate 0.0000
[2019-04-08 15:20:56,124] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9145567e-16 4.4901860e-08 3.9466395e-05 1.5485928e-04 8.0623535e-09
 2.1036619e-11 1.4956201e-03 5.8773732e-13 2.3679646e-18 1.7663783e-20
 9.9830997e-01], sum to 1.0000
[2019-04-08 15:20:56,126] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-08 15:20:56,159] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 50.66666666666666, 95.50000000000001, 61.33333333333334, 22.5, 28.27993960424909, 0.9343191810813809, 1.0, 1.0, 65.0, 34892.99339296864], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4552800.0000, 
sim time next is 4553400.0000, 
raw observation next is [2.0, 51.33333333333334, 82.0, 54.66666666666667, 22.5, 27.62663543317915, 1.045071196236367, 1.0, 1.0, 65.0, 53181.46118207266], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.5133333333333334, 0.2733333333333333, 0.060405156537753225, 0.375, 0.8022196194315958, 0.8483570654121223, 1.0, 1.0, 1.0, 0.25324505324796504], 
reward next is 0.7468, 
noisyNet noise sample is [array([0.10282694], dtype=float32), 1.5699922]. 
=============================================
[2019-04-08 15:20:56,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.4069568e-18 5.9370184e-09 3.9602779e-07 1.4889929e-05 3.3925966e-11
 8.7939963e-13 1.7259643e-07 1.1301959e-14 2.9266596e-19 4.2246570e-20
 9.9998450e-01], sum to 1.0000
[2019-04-08 15:20:56,767] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6416
[2019-04-08 15:20:56,779] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 23.33333333333334, 23.33333333333334, 22.5, 27.97997070617789, 1.036921512638748, 1.0, 1.0, 65.0, 26029.18301460467], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4556400.0000, 
sim time next is 4557000.0000, 
raw observation next is [2.0, 52.0, 18.66666666666667, 18.66666666666667, 22.5, 27.89506012556462, 1.035636340117091, 1.0, 1.0, 65.0, 23612.91410974552], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.06222222222222224, 0.02062615101289135, 0.375, 0.8245883437970516, 0.8452121133723637, 1.0, 1.0, 1.0, 0.11244244814164533], 
reward next is 0.8876, 
noisyNet noise sample is [array([1.0020951], dtype=float32), 0.871292]. 
=============================================
[2019-04-08 15:20:56,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7139738e-15 3.9198667e-09 3.9839928e-05 2.1946154e-04 6.3346445e-10
 8.2722586e-11 1.1481627e-05 1.1567410e-12 9.4605487e-17 7.6690719e-18
 9.9972922e-01], sum to 1.0000
[2019-04-08 15:20:56,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0197
[2019-04-08 15:20:56,811] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.79384 ]
 [85.1324  ]
 [85.56344 ]
 [85.882675]
 [86.21797 ]], R is [[84.4568634 ]
 [84.48834991]
 [84.5239563 ]
 [84.5812912 ]
 [84.64574432]].
[2019-04-08 15:20:56,812] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 19.0, 27.04476987098785, 0.8545692125178196, 0.0, 1.0, 65.0, 37569.56256820791], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4577400.0000, 
sim time next is 4578000.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 19.0, 27.01642722559978, 0.8533538335478151, 0.0, 1.0, 65.0, 37656.84242000956], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.08333333333333333, 0.7513689354666484, 0.7844512778492717, 0.0, 1.0, 1.0, 0.17931829723814077], 
reward next is 0.8207, 
noisyNet noise sample is [array([0.68010736], dtype=float32), 2.474901]. 
=============================================
[2019-04-08 15:20:56,825] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.63288]
 [76.35527]
 [76.16586]
 [76.46234]
 [76.40665]], R is [[76.74259186]
 [76.79626465]
 [76.85042572]
 [76.90533447]
 [76.95895386]].
[2019-04-08 15:20:56,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.16122934e-15 3.02351069e-08 8.71910015e-06 1.20321805e-04
 1.26227473e-09 1.44424014e-10 2.59870177e-07 7.22776277e-13
 2.13228540e-16 8.79471249e-19 9.99870658e-01], sum to 1.0000
[2019-04-08 15:20:57,003] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5892
[2019-04-08 15:20:57,031] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 19.0, 27.3141778662291, 0.9433463864833119, 0.0, 1.0, 65.0, 31936.31659025482], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4567800.0000, 
sim time next is 4568400.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 19.0, 27.30065829061587, 0.9444579593145548, 0.0, 1.0, 65.0, 31818.20377884043], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.57, 0.0, 0.0, 0.08333333333333333, 0.7750548575513226, 0.8148193197715182, 0.0, 1.0, 1.0, 0.15151525608971633], 
reward next is 0.8485, 
noisyNet noise sample is [array([1.7579516], dtype=float32), 0.355657]. 
=============================================
[2019-04-08 15:20:57,066] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0091508e-19 6.4638538e-11 1.0267629e-06 1.2330478e-06 6.2189273e-12
 9.6459028e-13 3.9505136e-07 6.7482252e-16 1.5504318e-18 7.4575060e-23
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:20:57,072] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6350
[2019-04-08 15:20:57,106] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 49.5, 121.0, 0.0, 22.5, 27.92611147053599, 0.9766179984509934, 1.0, 1.0, 65.0, 26856.65452624842], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4535400.0000, 
sim time next is 4536000.0000, 
raw observation next is [2.0, 48.0, 122.5, 0.0, 22.5, 27.94595033437476, 0.9829178195478229, 1.0, 1.0, 65.0, 26045.0909212496], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.48, 0.4083333333333333, 0.0, 0.375, 0.82882919453123, 0.8276392731826077, 1.0, 1.0, 1.0, 0.12402424248214096], 
reward next is 0.8760, 
noisyNet noise sample is [array([1.3703728], dtype=float32), -1.0232575]. 
=============================================
[2019-04-08 15:20:57,127] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[88.450676]
 [88.61153 ]
 [88.75073 ]
 [89.21222 ]
 [89.209625]], R is [[88.25243378]
 [88.24201965]
 [88.23861694]
 [88.23945618]
 [88.23986053]].
[2019-04-08 15:20:57,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2463801e-14 1.2336552e-09 5.5095425e-05 1.5556980e-04 1.4056555e-08
 1.3591259e-10 1.4091380e-05 5.3687377e-13 3.8498921e-16 5.3173594e-19
 9.9977523e-01], sum to 1.0000
[2019-04-08 15:20:57,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0359
[2019-04-08 15:20:57,642] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.05, 72.0, 0.0, 0.0, 19.0, 27.14445664055282, 0.8823623179219381, 0.0, 1.0, 65.0, 40645.00094853], 
current ob forecast is [], 
actual action is [4.95, 65.0], 
sim time this is 4486200.0000, 
sim time next is 4486800.0000, 
raw observation next is [-0.09999999999999999, 72.0, 0.0, 0.0, 19.0, 27.12745146385631, 0.8836577080303734, 0.0, 1.0, 65.0, 45087.29478209831], 
processed observation next is [1.0, 0.9565217391304348, 0.4598337950138504, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7606209553213592, 0.7945525693434577, 0.0, 1.0, 1.0, 0.21470140372427768], 
reward next is 0.7853, 
noisyNet noise sample is [array([4.208165], dtype=float32), 1.1799412]. 
=============================================
[2019-04-08 15:20:57,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6229111e-17 1.3201835e-08 5.1846448e-04 2.9317976e-04 1.6491478e-09
 9.3656187e-11 2.1462552e-05 5.8527602e-13 4.7074289e-17 6.6890639e-20
 9.9916685e-01], sum to 1.0000
[2019-04-08 15:20:57,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4846
[2019-04-08 15:20:57,873] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 66.5, 123.0, 0.0, 22.5, 27.64772856479532, 0.8875244084858881, 1.0, 1.0, 65.0, 29771.41104722271], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 4527000.0000, 
sim time next is 4527600.0000, 
raw observation next is [0.6666666666666666, 64.66666666666667, 139.3333333333333, 2.999999999999999, 22.5, 27.62287534061691, 0.899803795310902, 1.0, 1.0, 65.0, 30243.34088468965], 
processed observation next is [1.0, 0.391304347826087, 0.4810710987996307, 0.6466666666666667, 0.46444444444444427, 0.0033149171270718224, 0.375, 0.8019062783847426, 0.7999345984369673, 1.0, 1.0, 1.0, 0.14401590897471261], 
reward next is 0.8560, 
noisyNet noise sample is [array([-0.3337919], dtype=float32), 1.9497551]. 
=============================================
[2019-04-08 15:20:58,722] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 122020: loss 0.1495
[2019-04-08 15:20:58,747] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 122023: learning rate 0.0000
[2019-04-08 15:20:59,436] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.1153678e-16 1.1531022e-09 7.0854141e-05 9.3410472e-06 5.1320981e-10
 2.4318946e-11 1.1880034e-06 3.9336474e-14 2.5354020e-18 3.6096789e-20
 9.9991858e-01], sum to 1.0000
[2019-04-08 15:20:59,437] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4319
[2019-04-08 15:20:59,463] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.55569288931301, 0.9837881193743824, 1.0, 1.0, 65.0, 30162.4456813346], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4561200.0000, 
sim time next is 4561800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 27.56186422669193, 0.9814321671937777, 1.0, 1.0, 65.0, 29103.83586786307], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.7968220188909942, 0.8271440557312593, 1.0, 1.0, 1.0, 0.13858969460887174], 
reward next is 0.8614, 
noisyNet noise sample is [array([-1.8555286], dtype=float32), 0.4895332]. 
=============================================
[2019-04-08 15:20:59,749] A3C_AGENT_WORKER-Thread-7 INFO:Local step 7500, global step 122372: loss 0.2336
[2019-04-08 15:20:59,750] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 7500, global step 122372: learning rate 0.0000
[2019-04-08 15:20:59,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1793823e-13 1.5247144e-08 2.2767970e-05 1.0595084e-04 6.6573640e-09
 7.8027862e-11 1.3529419e-04 1.4486768e-11 3.4553116e-16 1.7987286e-17
 9.9973601e-01], sum to 1.0000
[2019-04-08 15:20:59,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8726
[2019-04-08 15:20:59,844] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.733333333333333, 75.0, 0.0, 0.0, 19.0, 26.82740406097327, 0.7361540056066397, 0.0, 1.0, 65.0, 40154.51677848685], 
current ob forecast is [], 
actual action is [2.266666666666667, 65.0], 
sim time this is 4602000.0000, 
sim time next is 4602600.0000, 
raw observation next is [-2.8, 75.5, 0.0, 0.0, 19.0, 26.75826388817841, 0.7321487349264206, 0.0, 1.0, 65.0, 42103.91971851923], 
processed observation next is [1.0, 0.2608695652173913, 0.38504155124653744, 0.755, 0.0, 0.0, 0.08333333333333333, 0.7298553240148674, 0.7440495783088069, 0.0, 1.0, 1.0, 0.20049485580247253], 
reward next is 0.7995, 
noisyNet noise sample is [array([-0.72062993], dtype=float32), 0.78856087]. 
=============================================
[2019-04-08 15:21:01,119] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1829417e-17 2.0037794e-10 2.4836024e-06 4.0737916e-05 5.6505961e-10
 3.9523224e-13 3.4178939e-07 1.7232026e-14 6.9581322e-19 5.3483905e-20
 9.9995649e-01], sum to 1.0000
[2019-04-08 15:21:01,119] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4984
[2019-04-08 15:21:01,194] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 71.0, 136.6666666666667, 283.6666666666666, 22.5, 27.5034067103886, 0.8635536331142348, 1.0, 1.0, 65.0, 25000.41854807893], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 4611000.0000, 
sim time next is 4611600.0000, 
raw observation next is [-2.0, 71.0, 143.5, 340.0, 22.5, 27.49939859732701, 0.8749553195004901, 1.0, 1.0, 65.0, 26917.0027732729], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.71, 0.47833333333333333, 0.3756906077348066, 0.375, 0.7916165497772507, 0.79165177316683, 1.0, 1.0, 1.0, 0.1281762036822519], 
reward next is 0.8718, 
noisyNet noise sample is [array([0.817311], dtype=float32), -0.125404]. 
=============================================
[2019-04-08 15:21:02,849] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3429040e-17 4.5756537e-11 2.1229098e-06 7.3677015e-06 4.7034937e-10
 1.1023219e-12 6.5659901e-07 1.1730424e-15 3.4001872e-19 2.2656499e-21
 9.9998987e-01], sum to 1.0000
[2019-04-08 15:21:02,849] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6028
[2019-04-08 15:21:02,911] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 50.66666666666666, 95.50000000000001, 61.33333333333334, 22.5, 28.28019870848788, 0.9343933786532755, 1.0, 1.0, 65.0, 34879.72797781017], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4552800.0000, 
sim time next is 4553400.0000, 
raw observation next is [2.0, 51.33333333333334, 82.0, 54.66666666666667, 22.5, 27.62687918658945, 1.045143689171832, 1.0, 1.0, 65.0, 53174.08801526274], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.5133333333333334, 0.2733333333333333, 0.060405156537753225, 0.375, 0.8022399322157874, 0.848381229723944, 1.0, 1.0, 1.0, 0.2532099429298226], 
reward next is 0.7468, 
noisyNet noise sample is [array([0.96725625], dtype=float32), -1.3754486]. 
=============================================
[2019-04-08 15:21:02,917] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.07605624e-12 3.95586198e-07 6.05302863e-04 1.98714071e-04
 1.36281614e-07 1.10264364e-09 1.83088763e-04 2.01213785e-11
 7.71120425e-16 1.07171315e-17 9.99012351e-01], sum to 1.0000
[2019-04-08 15:21:02,934] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3931
[2019-04-08 15:21:02,950] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.65, 66.0, 0.0, 0.0, 19.0, 27.11047516045842, 0.8268040378657692, 0.0, 1.0, 65.0, 34825.67669538955], 
current ob forecast is [], 
actual action is [4.35, 65.0], 
sim time this is 4588200.0000, 
sim time next is 4588800.0000, 
raw observation next is [-0.8, 66.33333333333333, 0.0, 0.0, 19.0, 27.12171212019405, 0.8066993924419071, 0.0, 1.0, 65.0, 34936.96413554347], 
processed observation next is [1.0, 0.08695652173913043, 0.4404432132963989, 0.6633333333333333, 0.0, 0.0, 0.08333333333333333, 0.7601426766828375, 0.7688997974806356, 0.0, 1.0, 1.0, 0.16636649588354036], 
reward next is 0.8336, 
noisyNet noise sample is [array([-0.8515043], dtype=float32), -0.3949073]. 
=============================================
[2019-04-08 15:21:05,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0569506e-16 2.4822439e-10 1.9154140e-05 6.4259434e-06 1.2235336e-09
 6.8442300e-12 1.4084768e-06 5.4246304e-12 1.3845693e-16 5.3687215e-18
 9.9997306e-01], sum to 1.0000
[2019-04-08 15:21:05,810] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6324
[2019-04-08 15:21:05,897] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 55.33333333333334, 0.0, 0.0, 19.0, 27.48626138087845, 0.9487766074196132, 0.0, 1.0, 65.0, 31163.77807453994], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4670400.0000, 
sim time next is 4671000.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 19.0, 27.47719900376172, 0.9398518162415703, 0.0, 1.0, 65.0, 31809.81659469643], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.57, 0.0, 0.0, 0.08333333333333333, 0.7897665836468098, 0.8132839387471901, 0.0, 1.0, 1.0, 0.15147531711760204], 
reward next is 0.8485, 
noisyNet noise sample is [array([-0.5506729], dtype=float32), 1.4673809]. 
=============================================
[2019-04-08 15:21:05,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.672104]
 [75.80696 ]
 [76.03366 ]
 [76.24993 ]
 [76.28561 ]], R is [[75.88608551]
 [75.97882843]
 [76.07077789]
 [76.15753937]
 [76.2350235 ]].
[2019-04-08 15:21:07,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.9750529e-18 1.3011909e-09 3.4945083e-06 2.5102221e-05 3.0515357e-10
 1.2756396e-12 7.2556929e-08 9.5776811e-15 2.8845611e-17 7.4714670e-22
 9.9997127e-01], sum to 1.0000
[2019-04-08 15:21:07,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8998
[2019-04-08 15:21:07,919] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.666666666666667, 50.33333333333334, 0.0, 0.0, 22.5, 28.69297926012921, 1.227689314502485, 1.0, 1.0, 65.0, 18849.39629266786], 
current ob forecast is [], 
actual action is [8.666666666666668, 65.0], 
sim time this is 4645200.0000, 
sim time next is 4645800.0000, 
raw observation next is [3.5, 51.0, 0.0, 0.0, 22.5, 28.62015151669508, 1.215527242813233, 1.0, 1.0, 65.0, 18849.25951199635], 
processed observation next is [1.0, 0.782608695652174, 0.5595567867036012, 0.51, 0.0, 0.0, 0.375, 0.8850126263912568, 0.9051757476044111, 1.0, 1.0, 1.0, 0.08975837862855406], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.37142634], dtype=float32), 0.9779725]. 
=============================================
[2019-04-08 15:21:09,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1080883e-17 5.4248508e-09 2.4112519e-03 2.2092185e-05 1.8801136e-09
 6.3422466e-12 3.2911925e-05 2.0851566e-13 8.2403609e-18 8.2393464e-20
 9.9753368e-01], sum to 1.0000
[2019-04-08 15:21:09,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4723
[2019-04-08 15:21:09,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9841098e-17 1.8000885e-07 3.0952555e-04 7.2576790e-03 7.3612597e-08
 4.8174767e-12 4.3274285e-06 1.7609541e-14 7.6132718e-17 3.1632106e-21
 9.9242824e-01], sum to 1.0000
[2019-04-08 15:21:09,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5031
[2019-04-08 15:21:09,141] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.6, 47.5, 40.0, 145.0, 22.5, 29.02583601140428, 1.232117510746645, 1.0, 1.0, 65.0, 18846.54435271909], 
current ob forecast is [], 
actual action is [9.6, 65.0], 
sim time this is 4642200.0000, 
sim time next is 4642800.0000, 
raw observation next is [4.4, 48.0, 33.33333333333334, 120.8333333333333, 22.5, 28.69280396005787, 1.287187081666259, 1.0, 1.0, 65.0, 18846.78696821007], 
processed observation next is [1.0, 0.7391304347826086, 0.5844875346260389, 0.48, 0.11111111111111115, 0.1335174953959484, 0.375, 0.8910669966714891, 0.9290623605554197, 1.0, 1.0, 1.0, 0.08974660461052415], 
reward next is 0.9103, 
noisyNet noise sample is [array([1.4952798], dtype=float32), 0.8491641]. 
=============================================
[2019-04-08 15:21:09,175] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 121.5, 0.0, 22.5, 26.99191448886452, 0.9581469853585999, 1.0, 1.0, 65.0, 79674.46362673311], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4712400.0000, 
sim time next is 4713000.0000, 
raw observation next is [1.166666666666667, 83.83333333333334, 126.0, 0.0, 22.5, 26.34112509847746, 0.8604471118201715, 1.0, 1.0, 64.99999999999999, 64690.04674130609], 
processed observation next is [1.0, 0.5652173913043478, 0.49492151431209613, 0.8383333333333334, 0.42, 0.0, 0.375, 0.6950937582064549, 0.7868157039400572, 1.0, 1.0, 0.9999999999999997, 0.3080478416252671], 
reward next is 0.6920, 
noisyNet noise sample is [array([0.7804603], dtype=float32), -0.6310561]. 
=============================================
[2019-04-08 15:21:09,244] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[99.41081 ]
 [99.5269  ]
 [99.65932 ]
 [99.77897 ]
 [99.963806]], R is [[98.93174744]
 [98.56302643]
 [98.28813171]
 [98.0901947 ]
 [97.90372467]].
[2019-04-08 15:21:09,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0209985e-16 4.6199631e-08 1.4178917e-05 3.5204564e-04 8.6132274e-10
 1.6379391e-13 7.4206389e-07 3.8979910e-15 4.1271093e-18 1.6994885e-20
 9.9963295e-01], sum to 1.0000
[2019-04-08 15:21:09,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3849
[2019-04-08 15:21:09,430] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 92.0, 106.3333333333333, 0.0, 22.5, 27.69916933838794, 0.9320594034584341, 1.0, 1.0, 65.0, 25201.44304577901], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4699200.0000, 
sim time next is 4699800.0000, 
raw observation next is [0.0, 92.0, 115.0, 0.0, 22.5, 27.80805495985298, 0.9436241019911558, 1.0, 1.0, 65.0, 23147.29604326015], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.38333333333333336, 0.0, 0.375, 0.8173379133210817, 0.8145413673303853, 1.0, 1.0, 1.0, 0.11022521925361976], 
reward next is 0.8898, 
noisyNet noise sample is [array([-0.82047254], dtype=float32), -0.21992199]. 
=============================================
[2019-04-08 15:21:10,008] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0823688e-19 1.1451256e-11 6.0509370e-07 3.8948221e-07 7.3691552e-13
 8.9734189e-14 1.3026681e-08 2.1573267e-16 2.0444663e-22 2.1580434e-23
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:21:10,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6597
[2019-04-08 15:21:10,081] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.6666666666666666, 88.0, 195.5, 5.0, 22.5, 28.02140790695938, 1.00298836260488, 1.0, 1.0, 65.0, 35553.20253925584], 
current ob forecast is [], 
actual action is [5.666666666666667, 65.0], 
sim time this is 4707600.0000, 
sim time next is 4708200.0000, 
raw observation next is [0.8333333333333334, 87.0, 178.0, 4.0, 22.5, 28.03739790352277, 1.005869481612855, 1.0, 1.0, 65.0, 40189.24110349007], 
processed observation next is [1.0, 0.4782608695652174, 0.4856879039704525, 0.87, 0.5933333333333334, 0.004419889502762431, 0.375, 0.8364498252935642, 0.835289827204285, 1.0, 1.0, 1.0, 0.19137733858804795], 
reward next is 0.8086, 
noisyNet noise sample is [array([-0.6184548], dtype=float32), -0.5917156]. 
=============================================
[2019-04-08 15:21:10,125] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.0144024e-18 1.0467058e-09 6.7000648e-05 5.2293562e-06 4.3674717e-11
 5.3202126e-14 1.5169181e-07 6.9096967e-16 6.2314340e-20 1.4526730e-23
 9.9992764e-01], sum to 1.0000
[2019-04-08 15:21:10,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7779
[2019-04-08 15:21:10,178] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666667, 91.0, 211.3333333333333, 6.0, 22.5, 28.07146452839975, 1.005830752411905, 1.0, 1.0, 65.0, 22308.91351269588], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 4705800.0000, 
sim time next is 4706400.0000, 
raw observation next is [0.3333333333333333, 90.0, 212.1666666666667, 6.0, 22.5, 28.02419691224802, 1.013406855436541, 1.0, 1.0, 65.0, 23707.4116854088], 
processed observation next is [1.0, 0.4782608695652174, 0.4718374884579871, 0.9, 0.7072222222222224, 0.0066298342541436465, 0.375, 0.835349742687335, 0.8378022851455137, 1.0, 1.0, 1.0, 0.11289243659718476], 
reward next is 0.8871, 
noisyNet noise sample is [array([0.28312916], dtype=float32), -1.4440019]. 
=============================================
[2019-04-08 15:21:10,228] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9680381e-20 1.8444760e-12 1.0199118e-07 2.6045132e-07 2.9431313e-13
 8.8833978e-16 7.6806062e-10 7.6883725e-18 2.6952042e-22 1.1828396e-24
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:21:10,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8547
[2019-04-08 15:21:10,264] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 131.3333333333333, 3.666666666666666, 22.5, 28.11167343348168, 1.036647675459968, 1.0, 1.0, 65.0, 27433.95229002765], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4722600.0000, 
sim time next is 4723200.0000, 
raw observation next is [1.0, 72.0, 123.5, 5.5, 22.5, 28.13502738670716, 1.038827964322591, 1.0, 1.0, 65.0, 29147.06581119412], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.4116666666666667, 0.0060773480662983425, 0.375, 0.8445856155589301, 0.8462759881075304, 1.0, 1.0, 1.0, 0.13879555148187675], 
reward next is 0.8612, 
noisyNet noise sample is [array([0.39286593], dtype=float32), -0.9866001]. 
=============================================
[2019-04-08 15:21:10,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0756617e-18 4.9162462e-11 4.6186305e-08 1.4504057e-05 3.2359261e-11
 3.8456922e-14 6.5755508e-08 2.5672346e-16 1.0507911e-20 2.4295433e-23
 9.9998534e-01], sum to 1.0000
[2019-04-08 15:21:10,301] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9953
[2019-04-08 15:21:10,323] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 92.0, 209.6666666666667, 6.0, 22.5, 28.0884178329053, 1.008261510792333, 1.0, 1.0, 65.0, 21075.15219546755], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4704600.0000, 
sim time next is 4705200.0000, 
raw observation next is [0.0, 92.0, 210.5, 6.0, 22.5, 28.10075225586333, 1.008041580563706, 1.0, 1.0, 65.0, 21264.00090006027], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.92, 0.7016666666666667, 0.0066298342541436465, 0.375, 0.8417293546552775, 0.836013860187902, 1.0, 1.0, 1.0, 0.10125714714314414], 
reward next is 0.8987, 
noisyNet noise sample is [array([1.1296257], dtype=float32), 1.4069433]. 
=============================================
[2019-04-08 15:21:10,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.3783652e-19 1.3153550e-09 2.1553337e-06 1.2099761e-06 5.5235232e-14
 7.2437862e-13 1.6403028e-08 2.9132077e-18 3.5769503e-21 3.7727072e-24
 9.9999666e-01], sum to 1.0000
[2019-04-08 15:21:10,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9775
[2019-04-08 15:21:10,508] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.333333333333333, 49.0, 120.3333333333333, 854.6666666666667, 22.5, 28.52803050420279, 0.9319141008719475, 1.0, 1.0, 65.0, 18847.68194892915], 
current ob forecast is [], 
actual action is [8.333333333333332, 65.0], 
sim time this is 4623600.0000, 
sim time next is 4624200.0000, 
raw observation next is [3.5, 49.0, 120.0, 859.0, 22.5, 28.37886418892869, 1.176586142373213, 1.0, 1.0, 65.0, 24043.61870764755], 
processed observation next is [1.0, 0.5217391304347826, 0.5595567867036012, 0.49, 0.4, 0.949171270718232, 0.375, 0.8649053490773909, 0.8921953807910711, 1.0, 1.0, 1.0, 0.11449342241736929], 
reward next is 0.8855, 
noisyNet noise sample is [array([0.972492], dtype=float32), -0.9910139]. 
=============================================
[2019-04-08 15:21:11,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.09280630e-18 1.16661944e-10 1.45095910e-05 1.44668957e-05
 9.02751415e-11 1.24271516e-14 2.58412758e-08 2.24910567e-16
 6.09336459e-20 2.75453662e-24 9.99971032e-01], sum to 1.0000
[2019-04-08 15:21:11,715] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9778
[2019-04-08 15:21:11,781] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 171.5, 3.0, 22.5, 28.12869939508484, 0.90676613871931, 1.0, 1.0, 65.0, 47969.65066186483], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4719600.0000, 
sim time next is 4720200.0000, 
raw observation next is [1.0, 72.0, 163.3333333333333, 2.0, 22.5, 27.44553411097501, 1.044706415958576, 1.0, 1.0, 65.0, 64580.9266643178], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.5444444444444443, 0.0022099447513812156, 0.375, 0.7871278425812509, 0.8482354719861919, 1.0, 1.0, 1.0, 0.30752822221103715], 
reward next is 0.6925, 
noisyNet noise sample is [array([0.8809696], dtype=float32), -0.32237476]. 
=============================================
[2019-04-08 15:21:12,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8922301e-16 2.8702738e-09 3.5036646e-05 5.3510239e-06 7.0374240e-10
 1.2949842e-11 2.4455262e-06 3.9325690e-14 8.3323123e-19 1.8359175e-20
 9.9995720e-01], sum to 1.0000
[2019-04-08 15:21:12,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3559
[2019-04-08 15:21:12,061] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 19.0, 26.97456994851247, 0.8807450114729579, 0.0, 1.0, 65.0, 40414.09801958316], 
current ob forecast is [], 
actual action is [2.333333333333333, 65.0], 
sim time this is 4743600.0000, 
sim time next is 4744200.0000, 
raw observation next is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 19.0, 26.95684514249794, 0.8765886819399832, 0.0, 1.0, 65.0, 40589.95629210344], 
processed observation next is [1.0, 0.9130434782608695, 0.3841181902123731, 0.8416666666666666, 0.0, 0.0, 0.08333333333333333, 0.7464037618748284, 0.7921962273133277, 0.0, 1.0, 1.0, 0.19328550615287352], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.34431648], dtype=float32), 1.6241488]. 
=============================================
[2019-04-08 15:21:12,678] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 125913: loss 0.0076
[2019-04-08 15:21:12,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 125913: learning rate 0.0000
[2019-04-08 15:21:13,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4384557e-14 7.8520593e-08 5.0063379e-04 1.1050204e-03 1.5516335e-08
 2.2559142e-11 8.1567960e-06 9.4983776e-13 2.4248628e-15 1.9687962e-18
 9.9838614e-01], sum to 1.0000
[2019-04-08 15:21:13,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7114
[2019-04-08 15:21:13,184] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 19.0, 27.04580876551372, 0.9036300174625401, 0.0, 1.0, 65.0, 39585.07703292324], 
current ob forecast is [], 
actual action is [3.166666666666667, 65.0], 
sim time this is 4740600.0000, 
sim time next is 4741200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 27.05628022710904, 0.8982913441556826, 0.0, 1.0, 65.0, 39017.62659749332], 
processed observation next is [1.0, 0.9130434782608695, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7546900189257532, 0.7994304480518942, 0.0, 1.0, 1.0, 0.18579822189282533], 
reward next is 0.8142, 
noisyNet noise sample is [array([1.4900393], dtype=float32), 0.6314292]. 
=============================================
[2019-04-08 15:21:13,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7003006e-15 1.8617277e-08 1.4161698e-04 1.1133510e-04 1.2592029e-09
 1.5178279e-10 2.7896493e-05 8.3234613e-13 3.3890945e-16 1.6292588e-17
 9.9971908e-01], sum to 1.0000
[2019-04-08 15:21:13,595] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0279
[2019-04-08 15:21:13,627] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 36.0, 84.5, 578.6666666666666, 19.0, 27.6166848483987, 1.017286336359082, 0.0, 1.0, 65.0, 26268.39391325101], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4810800.0000, 
sim time next is 4811400.0000, 
raw observation next is [3.0, 35.5, 82.0, 549.0, 19.0, 27.63612206151939, 1.017194715158332, 0.0, 1.0, 65.0, 25967.75360187775], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.355, 0.2733333333333333, 0.6066298342541436, 0.08333333333333333, 0.8030101717932826, 0.8390649050527773, 0.0, 1.0, 1.0, 0.1236559695327512], 
reward next is 0.8763, 
noisyNet noise sample is [array([0.7647788], dtype=float32), 1.5317745]. 
=============================================
[2019-04-08 15:21:13,729] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6844363e-14 1.1115756e-09 2.3058055e-05 7.1699877e-05 7.5313640e-09
 8.4027932e-12 9.8313171e-07 2.8215581e-13 2.4770795e-17 8.8968438e-20
 9.9990427e-01], sum to 1.0000
[2019-04-08 15:21:13,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8701
[2019-04-08 15:21:13,749] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.166666666666667, 78.16666666666667, 0.0, 0.0, 19.0, 26.72996985606084, 0.8016464541301683, 0.0, 1.0, 65.0, 46705.88474852558], 
current ob forecast is [], 
actual action is [1.833333333333333, 65.0], 
sim time this is 4749000.0000, 
sim time next is 4749600.0000, 
raw observation next is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 19.0, 26.71045237626208, 0.7958843308434743, 0.0, 1.0, 65.0, 46902.30283646317], 
processed observation next is [1.0, 1.0, 0.37026777469990774, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.7258710313551733, 0.7652947769478248, 0.0, 1.0, 1.0, 0.22334429922125318], 
reward next is 0.7767, 
noisyNet noise sample is [array([-0.67782634], dtype=float32), -0.37685242]. 
=============================================
[2019-04-08 15:21:14,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5539668e-16 1.4227504e-09 4.5763365e-08 1.0688805e-05 6.5891259e-10
 1.2784816e-10 4.6767570e-07 9.9130360e-14 8.4601882e-17 6.5924682e-18
 9.9998879e-01], sum to 1.0000
[2019-04-08 15:21:14,694] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5426
[2019-04-08 15:21:14,709] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3333333333333333, 94.66666666666667, 0.0, 0.0, 19.0, 27.40809515000521, 0.885370348886341, 0.0, 1.0, 65.0, 30896.94497639032], 
current ob forecast is [], 
actual action is [4.666666666666667, 65.0], 
sim time this is 4681200.0000, 
sim time next is 4681800.0000, 
raw observation next is [-0.5, 96.0, 0.0, 0.0, 19.0, 27.34254152501781, 0.8869844286595937, 0.0, 1.0, 65.0, 34017.95883304554], 
processed observation next is [1.0, 0.17391304347826086, 0.44875346260387816, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7785451270848176, 0.7956614762198645, 0.0, 1.0, 1.0, 0.1619902801573597], 
reward next is 0.8380, 
noisyNet noise sample is [array([-1.6001968], dtype=float32), 0.7321675]. 
=============================================
[2019-04-08 15:21:14,712] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [9.0504540e-19 8.2304184e-11 4.2881351e-05 2.4697196e-05 1.1774243e-11
 8.4952309e-14 1.1531500e-07 3.7976320e-16 5.5648157e-19 1.4435719e-21
 9.9993229e-01], sum to 1.0000
[2019-04-08 15:21:14,721] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1668
[2019-04-08 15:21:14,753] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 155.1666666666667, 0.9999999999999998, 22.5, 26.92834996612832, 0.9451940940840041, 1.0, 1.0, 64.99999999999999, 45436.21150723867], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4720800.0000, 
sim time next is 4721400.0000, 
raw observation next is [1.0, 72.0, 147.0, 0.0, 22.5, 27.61944966506351, 1.005692386748795, 1.0, 1.0, 65.0, 33217.66843158035], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.49, 0.0, 0.375, 0.8016208054219591, 0.8352307955829317, 1.0, 1.0, 1.0, 0.15817937348371597], 
reward next is 0.8418, 
noisyNet noise sample is [array([-1.712538], dtype=float32), 0.41028228]. 
=============================================
[2019-04-08 15:21:15,510] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2191169e-16 1.5953694e-09 4.2602969e-06 4.2703265e-05 4.0339179e-10
 3.6622611e-11 4.9564858e-07 3.6724161e-12 5.8823851e-17 5.1570407e-17
 9.9995255e-01], sum to 1.0000
[2019-04-08 15:21:15,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5350
[2019-04-08 15:21:15,525] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.333333333333333, 82.0, 132.3333333333333, 419.3333333333334, 19.0, 26.31974325020377, 0.689414605016882, 0.0, 1.0, 65.0, 48082.50824869886], 
current ob forecast is [], 
actual action is [-0.33333333333333304, 65.0], 
sim time this is 4783200.0000, 
sim time next is 4783800.0000, 
raw observation next is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 19.0, 26.33668251067389, 0.6957625475598667, 0.0, 1.0, 65.0, 47543.89070371511], 
processed observation next is [0.0, 0.34782608695652173, 0.31948291782086796, 0.795, 0.468888888888889, 0.4637200736648251, 0.08333333333333333, 0.6947235425561574, 0.7319208491866221, 0.0, 1.0, 1.0, 0.22639947954150053], 
reward next is 0.7736, 
noisyNet noise sample is [array([-0.4403801], dtype=float32), 0.5014129]. 
=============================================
[2019-04-08 15:21:15,713] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0783228e-15 3.5315324e-07 4.6730186e-03 1.3794164e-04 4.3272294e-07
 5.9231890e-11 3.0067179e-03 1.7036812e-10 1.1627238e-16 2.5289083e-18
 9.9218154e-01], sum to 1.0000
[2019-04-08 15:21:15,713] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4641
[2019-04-08 15:21:15,729] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.166666666666667, 42.5, 154.0, 804.3333333333334, 19.0, 27.01970249741138, 0.8733704337643656, 0.0, 1.0, 65.0, 32792.73019196386], 
current ob forecast is [], 
actual action is [6.166666666666667, 65.0], 
sim time this is 4795800.0000, 
sim time next is 4796400.0000, 
raw observation next is [1.333333333333333, 42.0, 162.0, 795.6666666666667, 19.0, 27.05931197179956, 0.8825533120997417, 0.0, 1.0, 65.0, 32170.59802089549], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.42, 0.54, 0.8791896869244936, 0.08333333333333333, 0.7549426643166299, 0.7941844373665806, 0.0, 1.0, 1.0, 0.15319332390902615], 
reward next is 0.8468, 
noisyNet noise sample is [array([-0.08653446], dtype=float32), -0.46291003]. 
=============================================
[2019-04-08 15:21:16,071] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 127140: loss 0.0004
[2019-04-08 15:21:16,075] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 127140: learning rate 0.0000
[2019-04-08 15:21:16,343] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9736434e-16 1.1045536e-08 8.6727887e-06 1.1984839e-05 8.1561069e-10
 9.7511374e-11 2.9275353e-07 9.0267171e-13 1.9322785e-17 4.0506293e-19
 9.9997902e-01], sum to 1.0000
[2019-04-08 15:21:16,343] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0580
[2019-04-08 15:21:16,362] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 37.0, 139.5, 739.5, 19.0, 27.39880437812391, 0.9742501973821573, 0.0, 1.0, 65.0, 28131.92794082493], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4804800.0000, 
sim time next is 4805400.0000, 
raw observation next is [3.0, 37.0, 131.0, 737.0, 19.0, 27.41375807648086, 0.9791436222856252, 0.0, 1.0, 65.0, 28163.58393099791], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.43666666666666665, 0.8143646408839779, 0.08333333333333333, 0.7844798397067384, 0.8263812074285418, 0.0, 1.0, 1.0, 0.13411230443332336], 
reward next is 0.8659, 
noisyNet noise sample is [array([-1.0857463], dtype=float32), -0.76187336]. 
=============================================
[2019-04-08 15:21:16,393] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127269: loss 0.0014
[2019-04-08 15:21:16,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127269: learning rate 0.0000
[2019-04-08 15:21:16,487] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127312: loss 0.0033
[2019-04-08 15:21:16,492] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127313: learning rate 0.0000
[2019-04-08 15:21:16,510] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127325: loss 0.0170
[2019-04-08 15:21:16,511] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127325: learning rate 0.0000
[2019-04-08 15:21:16,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.90758674e-18 1.23490038e-10 2.99098724e-06 1.98726912e-06
 3.14125288e-11 6.03650159e-13 4.97289641e-08 2.87344111e-15
 4.12724545e-20 1.12088986e-23 9.99994993e-01], sum to 1.0000
[2019-04-08 15:21:16,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4014
[2019-04-08 15:21:16,674] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 160.5, 3.0, 22.5, 28.04367069711371, 1.006382670928708, 1.0, 1.0, 65.0, 41666.20573985497], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4708800.0000, 
sim time next is 4709400.0000, 
raw observation next is [1.0, 86.0, 143.0, 2.0, 22.5, 28.04320420011403, 1.006888712620265, 1.0, 1.0, 65.0, 42068.32767232952], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.4766666666666667, 0.0022099447513812156, 0.375, 0.8369336833428358, 0.8356295708734217, 1.0, 1.0, 1.0, 0.20032536986823582], 
reward next is 0.7997, 
noisyNet noise sample is [array([-0.7395498], dtype=float32), 0.06570911]. 
=============================================
[2019-04-08 15:21:17,305] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127609: loss 0.0064
[2019-04-08 15:21:17,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127609: learning rate 0.0000
[2019-04-08 15:21:17,314] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127611: loss 0.0088
[2019-04-08 15:21:17,319] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127611: learning rate 0.0000
[2019-04-08 15:21:17,528] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127690: loss 0.0060
[2019-04-08 15:21:17,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127692: learning rate 0.0000
[2019-04-08 15:21:17,786] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127780: loss 0.0075
[2019-04-08 15:21:17,787] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127780: learning rate 0.0000
[2019-04-08 15:21:17,894] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.0601078e-14 6.5249573e-08 1.0269508e-05 1.0250511e-04 2.7208371e-08
 5.1287358e-10 1.7292179e-05 1.4480916e-11 5.5347806e-14 7.2829567e-17
 9.9986982e-01], sum to 1.0000
[2019-04-08 15:21:17,896] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0435
[2019-04-08 15:21:17,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1564507e-14 1.4219685e-08 9.1722555e-05 3.8910235e-04 3.5300143e-08
 4.1155793e-10 1.5603508e-05 5.2741214e-12 4.7474169e-16 7.0841859e-17
 9.9950349e-01], sum to 1.0000
[2019-04-08 15:21:17,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5019
[2019-04-08 15:21:17,919] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.36450959758281, 0.6767936890202834, 0.0, 1.0, 65.0, 50037.88769063889], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4767600.0000, 
sim time next is 4768200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.35171905599446, 0.6842396143535211, 0.0, 1.0, 65.0, 50343.85431324485], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6959765879995384, 0.7280798714511737, 0.0, 1.0, 1.0, 0.23973263958688024], 
reward next is 0.7603, 
noisyNet noise sample is [array([-0.82417744], dtype=float32), -0.36907944]. 
=============================================
[2019-04-08 15:21:17,922] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 127839: loss 0.0008
[2019-04-08 15:21:17,924] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 127839: learning rate 0.0000
[2019-04-08 15:21:17,938] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.55427686911652, 0.7402906517320907, 0.0, 1.0, 65.0, 48103.26832436136], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 4759200.0000, 
sim time next is 4759800.0000, 
raw observation next is [-4.333333333333334, 74.5, 0.0, 0.0, 19.0, 26.56771013504872, 0.7356020625690801, 0.0, 1.0, 65.0, 47082.42745711064], 
processed observation next is [0.0, 0.08695652173913043, 0.3425669436749769, 0.745, 0.0, 0.0, 0.08333333333333333, 0.7139758445873934, 0.7452006875230267, 0.0, 1.0, 1.0, 0.2242020355100507], 
reward next is 0.7758, 
noisyNet noise sample is [array([1.1105024], dtype=float32), -1.5264697]. 
=============================================
[2019-04-08 15:21:17,997] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 127866: loss 0.0019
[2019-04-08 15:21:17,997] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 127866: learning rate 0.0000
[2019-04-08 15:21:18,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.12636293e-15 1.88451764e-07 4.19532444e-05 1.66683720e-04
 8.29029290e-08 9.93181752e-12 8.61171611e-06 1.19001675e-11
 1.74579883e-16 1.68472268e-18 9.99782503e-01], sum to 1.0000
[2019-04-08 15:21:18,220] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3523
[2019-04-08 15:21:18,232] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 35.0, 49.33333333333333, 306.3333333333333, 19.0, 27.64209843884002, 0.990943590805597, 0.0, 1.0, 65.0, 27852.8140219893], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 4813800.0000, 
sim time next is 4814400.0000, 
raw observation next is [2.666666666666667, 36.0, 41.16666666666666, 245.6666666666667, 19.0, 27.62039238416006, 0.9882872594864421, 0.0, 1.0, 65.0, 27973.90720446167], 
processed observation next is [0.0, 0.7391304347826086, 0.5364727608494922, 0.36, 0.1372222222222222, 0.27145488029465936, 0.08333333333333333, 0.8016993653466716, 0.8294290864954807, 0.0, 1.0, 1.0, 0.13320908192600794], 
reward next is 0.8668, 
noisyNet noise sample is [array([0.7047482], dtype=float32), -0.8762238]. 
=============================================
[2019-04-08 15:21:18,838] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.24092290e-14 1.13459635e-07 3.89779598e-04 4.08258842e-04
 3.92570705e-08 3.52275674e-11 4.49730607e-04 3.99199424e-12
 2.60515469e-14 4.42169992e-17 9.98752117e-01], sum to 1.0000
[2019-04-08 15:21:18,846] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8734
[2019-04-08 15:21:18,864] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 36.5, 87.0, 608.3333333333334, 19.0, 27.60095233260576, 1.015721396361275, 0.0, 1.0, 65.0, 26286.70764056283], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 19.0, 27.61674012327046, 1.017310176288046, 0.0, 1.0, 65.0, 26267.52126917802], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 0.08333333333333333, 0.8013950102725383, 0.8391033920960154, 0.0, 1.0, 1.0, 0.12508343461513344], 
reward next is 0.8749, 
noisyNet noise sample is [array([0.99882716], dtype=float32), -0.08585469]. 
=============================================
[2019-04-08 15:21:18,869] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6557993e-14 6.3933087e-10 1.1583404e-05 4.8941271e-05 8.4482188e-10
 5.8742983e-10 4.8820715e-07 1.5240468e-12 8.2082538e-16 5.1673257e-18
 9.9993896e-01], sum to 1.0000
[2019-04-08 15:21:18,870] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1861
[2019-04-08 15:21:18,900] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.36710262556804, 0.6657168038511064, 0.0, 1.0, 65.0, 51053.23665164762], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4770000.0000, 
sim time next is 4770600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.32910113588676, 0.6668538824996714, 0.0, 1.0, 65.0, 51806.77624796872], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6940917613238966, 0.7222846274998904, 0.0, 1.0, 1.0, 0.24669893451413677], 
reward next is 0.7533, 
noisyNet noise sample is [array([-1.5701807], dtype=float32), -0.050672933]. 
=============================================
[2019-04-08 15:21:19,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7120807e-15 5.1753721e-09 5.3195722e-06 1.7337145e-05 1.2729890e-08
 7.8318720e-11 3.3882175e-06 3.3522479e-12 1.8712927e-15 2.9625774e-16
 9.9997389e-01], sum to 1.0000
[2019-04-08 15:21:19,198] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4922
[2019-04-08 15:21:19,214] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.033333333333333, 92.16666666666667, 0.0, 0.0, 19.0, 26.25008929952776, 0.6405823257936537, 0.0, 1.0, 65.0, 52544.52744021092], 
current ob forecast is [], 
actual action is [-1.0333333333333332, 65.0], 
sim time this is 4774200.0000, 
sim time next is 4774800.0000, 
raw observation next is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 19.0, 26.23493693200866, 0.6370684672607628, 0.0, 1.0, 65.0, 52758.20388041496], 
processed observation next is [0.0, 0.2608695652173913, 0.2945521698984303, 0.9233333333333335, 0.0, 0.0, 0.08333333333333333, 0.6862447443340551, 0.7123561557535876, 0.0, 1.0, 1.0, 0.2512295422876903], 
reward next is 0.7488, 
noisyNet noise sample is [array([0.03475583], dtype=float32), 1.557249]. 
=============================================
[2019-04-08 15:21:20,222] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.6969215e-15 1.1365128e-09 3.2385826e-06 1.4077392e-04 1.6422044e-08
 6.4264917e-12 1.5167384e-06 1.2117518e-12 6.0642650e-15 7.5954363e-18
 9.9985445e-01], sum to 1.0000
[2019-04-08 15:21:20,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6001085e-14 3.3940830e-07 9.7269850e-04 2.1896102e-04 6.4523156e-09
 8.5369106e-10 2.3619491e-06 1.1883898e-10 2.8016272e-14 9.7630129e-16
 9.9880564e-01], sum to 1.0000
[2019-04-08 15:21:20,225] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7029
[2019-04-08 15:21:20,225] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1318
[2019-04-08 15:21:20,240] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 65.5, 0.0, 0.0, 19.0, 26.60030666634292, 0.646421424134601, 0.0, 1.0, 65.0, 46251.12896636974], 
current ob forecast is [], 
actual action is [1.5, 65.0], 
sim time this is 4858200.0000, 
sim time next is 4858800.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 19.0, 26.57330344800105, 0.644426272175617, 0.0, 1.0, 65.0, 46995.28772848026], 
processed observation next is [0.0, 0.21739130434782608, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7144419540000874, 0.7148087573918723, 0.0, 1.0, 1.0, 0.22378708442133458], 
reward next is 0.7762, 
noisyNet noise sample is [array([0.9044466], dtype=float32), -0.19489786]. 
=============================================
[2019-04-08 15:21:20,241] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 27.01768957879522, 0.7769438170949862, 0.0, 1.0, 65.0, 40474.33255278722], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 4837200.0000, 
sim time next is 4837800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 19.0, 27.01781412925061, 0.7850001223650355, 0.0, 1.0, 65.0, 40807.81728415577], 
processed observation next is [0.0, 1.0, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.08333333333333333, 0.7514845107708842, 0.7616667074550119, 0.0, 1.0, 1.0, 0.1943229394483608], 
reward next is 0.8057, 
noisyNet noise sample is [array([-0.1338016], dtype=float32), 1.892092]. 
=============================================
[2019-04-08 15:21:20,322] A3C_AGENT_WORKER-Thread-8 INFO:Local step 8000, global step 128710: loss 0.0133
[2019-04-08 15:21:20,322] A3C_AGENT_WORKER-Thread-8 DEBUG:Local step 8000, global step 128710: learning rate 0.0000
[2019-04-08 15:21:20,605] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0670097e-14 5.6047433e-08 5.3299623e-06 6.6687277e-04 1.7911136e-08
 5.2377665e-11 4.8404972e-06 2.8947438e-11 2.0997517e-15 1.8481164e-17
 9.9932289e-01], sum to 1.0000
[2019-04-08 15:21:20,606] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-08 15:21:20,674] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 19.0, 27.01765187650063, 0.7769333681676289, 0.0, 1.0, 65.0, 40474.72430520404], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 4837200.0000, 
sim time next is 4837800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 19.0, 27.01777587656836, 0.7849897888497366, 0.0, 1.0, 65.0, 40808.20493466454], 
processed observation next is [0.0, 1.0, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.08333333333333333, 0.7514813230473635, 0.7616632629499122, 0.0, 1.0, 1.0, 0.1943247854031645], 
reward next is 0.8057, 
noisyNet noise sample is [array([0.03994732], dtype=float32), -1.3807899]. 
=============================================
[2019-04-08 15:21:20,893] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128940: loss 0.0009
[2019-04-08 15:21:20,897] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128941: learning rate 0.0000
[2019-04-08 15:21:21,133] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.36502748e-13 1.04432885e-07 2.69124776e-05 5.95161328e-05
 2.05686828e-08 2.14521678e-10 3.89596516e-06 5.71647113e-12
 2.58854810e-14 1.99571524e-16 9.99909520e-01], sum to 1.0000
[2019-04-08 15:21:21,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6180
[2019-04-08 15:21:21,160] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 26.52706231600817, 0.6328874830197304, 0.0, 1.0, 65.0, 47103.42634534786], 
current ob forecast is [], 
actual action is [1.666666666666667, 65.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 19.0, 26.51589072488164, 0.6314671176829517, 0.0, 1.0, 65.0, 47240.88352059157], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.08333333333333333, 0.7096575604068033, 0.7104890392276505, 0.0, 1.0, 1.0, 0.22495658819329317], 
reward next is 0.7750, 
noisyNet noise sample is [array([0.12640472], dtype=float32), -0.94305086]. 
=============================================
[2019-04-08 15:21:21,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3246046e-13 4.5365712e-07 8.8327308e-04 8.4841211e-04 9.5353776e-08
 3.6319712e-09 5.6992164e-03 4.8621773e-11 6.2955211e-15 1.2675063e-15
 9.9256855e-01], sum to 1.0000
[2019-04-08 15:21:21,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1050
[2019-04-08 15:21:21,230] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 37.5, 0.0, 0.0, 19.0, 27.15056872814394, 0.7758107834721047, 0.0, 1.0, 65.0, 37033.81773436316], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 4919400.0000, 
sim time next is 4920000.0000, 
raw observation next is [0.3333333333333334, 38.0, 0.0, 0.0, 19.0, 27.12170207352388, 0.7675361219349793, 0.0, 1.0, 65.0, 37558.32595696], 
processed observation next is [0.0, 0.9565217391304348, 0.4718374884579871, 0.38, 0.0, 0.0, 0.08333333333333333, 0.7601418394603234, 0.7558453739783264, 0.0, 1.0, 1.0, 0.17884917122361904], 
reward next is 0.8212, 
noisyNet noise sample is [array([0.09497765], dtype=float32), 1.3131682]. 
=============================================
[2019-04-08 15:21:21,254] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.50901]
 [69.61494]
 [69.68188]
 [69.77463]
 [69.86295]], R is [[69.56821442]
 [69.69618225]
 [69.81639099]
 [69.93562317]
 [70.05700684]].
[2019-04-08 15:21:21,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7454324e-15 2.1376250e-09 2.0886950e-05 3.8185073e-05 5.8250045e-09
 1.7844223e-12 2.6547864e-06 3.9622680e-12 7.8829312e-16 8.9302394e-18
 9.9993825e-01], sum to 1.0000
[2019-04-08 15:21:21,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6075
[2019-04-08 15:21:21,521] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.33637603915684, 0.6667456832990158, 0.0, 1.0, 65.0, 50806.64869809579], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4771200.0000, 
sim time next is 4771800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.36447619552001, 0.6580184768974428, 0.0, 1.0, 65.0, 49712.66931830641], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6970396829600007, 0.7193394922991475, 0.0, 1.0, 1.0, 0.23672699675384007], 
reward next is 0.7633, 
noisyNet noise sample is [array([-0.762497], dtype=float32), -0.18478428]. 
=============================================
[2019-04-08 15:21:21,540] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.0863642e-15 3.6064191e-07 2.0962521e-05 7.9850382e-05 1.1135446e-09
 5.4672600e-11 2.4913925e-06 2.2204202e-11 1.2489803e-15 9.1409030e-17
 9.9989629e-01], sum to 1.0000
[2019-04-08 15:21:21,540] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3398
[2019-04-08 15:21:21,577] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 129200: loss 0.0020
[2019-04-08 15:21:21,579] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 129200: learning rate 0.0000
[2019-04-08 15:21:21,609] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.5, 41.5, 0.0, 0.0, 19.0, 27.45985253446819, 0.9218757711918283, 0.0, 1.0, 65.0, 31793.42572527129], 
current ob forecast is [], 
actual action is [6.5, 65.0], 
sim time this is 4818600.0000, 
sim time next is 4819200.0000, 
raw observation next is [1.333333333333333, 42.0, 0.0, 0.0, 19.0, 27.43147223060627, 0.9132809089443971, 0.0, 1.0, 65.0, 32275.52261344176], 
processed observation next is [0.0, 0.782608695652174, 0.4995383194829178, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7859560192171893, 0.8044269696481323, 0.0, 1.0, 1.0, 0.15369296482591316], 
reward next is 0.8463, 
noisyNet noise sample is [array([-0.06889838], dtype=float32), -2.083954]. 
=============================================
[2019-04-08 15:21:22,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7478009e-13 2.6330522e-08 9.0423782e-05 7.5730984e-04 1.7469395e-08
 1.4849054e-09 4.7698544e-05 2.0432489e-11 7.6368863e-15 1.6016999e-16
 9.9910444e-01], sum to 1.0000
[2019-04-08 15:21:22,227] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9477
[2019-04-08 15:21:22,247] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 19.0, 26.55806605162847, 0.6416337652331419, 0.0, 1.0, 65.0, 47059.27557757569], 
current ob forecast is [], 
actual action is [1.833333333333333, 65.0], 
sim time this is 4859400.0000, 
sim time next is 4860000.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 19.0, 26.54479173012246, 0.639005616453116, 0.0, 1.0, 65.0, 46986.39771377046], 
processed observation next is [0.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7120659775102048, 0.7130018721510387, 0.0, 1.0, 1.0, 0.22374475101795455], 
reward next is 0.7763, 
noisyNet noise sample is [array([0.6823353], dtype=float32), 0.018933332]. 
=============================================
[2019-04-08 15:21:22,286] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[71.40739 ]
 [71.41888 ]
 [71.4056  ]
 [71.4112  ]
 [71.363106]], R is [[71.46853638]
 [71.52976227]
 [71.59068298]
 [71.65453339]
 [71.71898651]].
[2019-04-08 15:21:23,482] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 129953: loss 0.0072
[2019-04-08 15:21:23,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 129953: learning rate 0.0000
[2019-04-08 15:21:24,132] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.7210838e-16 2.4526365e-09 7.8519524e-06 2.1276519e-06 4.2315889e-09
 3.9309302e-12 5.4187768e-07 3.5119746e-12 4.5232363e-17 4.0135568e-19
 9.9998951e-01], sum to 1.0000
[2019-04-08 15:21:24,138] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6907
[2019-04-08 15:21:24,185] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 42.33333333333333, 0.0, 0.0, 19.0, 27.29385969624361, 0.8375545238928619, 0.0, 1.0, 65.0, 35191.80387787605], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4909200.0000, 
sim time next is 4909800.0000, 
raw observation next is [1.0, 41.16666666666666, 0.0, 0.0, 19.0, 27.2565044290199, 0.8333983879971614, 0.0, 1.0, 65.0, 35926.5922413987], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.41166666666666657, 0.0, 0.0, 0.08333333333333333, 0.7713753690849915, 0.7777994626657204, 0.0, 1.0, 1.0, 0.17107901067332715], 
reward next is 0.8289, 
noisyNet noise sample is [array([-0.17900595], dtype=float32), -0.96964276]. 
=============================================
[2019-04-08 15:21:24,349] A3C_AGENT_WORKER-Thread-7 INFO:Local step 8000, global step 130304: loss 0.0067
[2019-04-08 15:21:24,384] A3C_AGENT_WORKER-Thread-7 DEBUG:Local step 8000, global step 130308: learning rate 0.0000
[2019-04-08 15:21:24,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.9087680e-14 1.0458450e-08 1.5731986e-05 5.3444295e-04 4.8127593e-08
 1.0137557e-10 9.4821435e-06 4.1716548e-12 1.0895194e-14 6.1112243e-17
 9.9944025e-01], sum to 1.0000
[2019-04-08 15:21:24,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3364
[2019-04-08 15:21:24,827] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 37.5, 0.0, 0.0, 19.0, 27.15058475359296, 0.7758162438445279, 0.0, 1.0, 65.0, 37033.70656489045], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 4919400.0000, 
sim time next is 4920000.0000, 
raw observation next is [0.3333333333333334, 38.0, 0.0, 0.0, 19.0, 27.12171736503125, 0.7675416103886107, 0.0, 1.0, 65.0, 37558.17388474227], 
processed observation next is [0.0, 0.9565217391304348, 0.4718374884579871, 0.38, 0.0, 0.0, 0.08333333333333333, 0.7601431137526043, 0.7558472034628703, 0.0, 1.0, 1.0, 0.17884844707020128], 
reward next is 0.8212, 
noisyNet noise sample is [array([0.31633314], dtype=float32), -0.38111746]. 
=============================================
[2019-04-08 15:21:24,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.69159 ]
 [71.80221 ]
 [71.869896]
 [71.96439 ]
 [72.056366]], R is [[71.72668457]
 [71.83306122]
 [71.93190002]
 [72.02998352]
 [72.1304245 ]].
[2019-04-08 15:21:26,387] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2974247e-14 2.6089398e-07 1.2893611e-04 8.7143789e-04 3.5833143e-08
 9.6878827e-10 7.3228170e-05 7.1007488e-12 9.9742255e-15 1.2212123e-18
 9.9892610e-01], sum to 1.0000
[2019-04-08 15:21:26,388] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0288
[2019-04-08 15:21:26,412] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 50.0, 0.0, 0.0, 19.0, 26.88857925059416, 0.6775079959317433, 0.0, 1.0, 65.0, 41372.81438572406], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 4937400.0000, 
sim time next is 4938000.0000, 
raw observation next is [-1.666666666666667, 50.0, 0.0, 0.0, 19.0, 26.84828928640756, 0.6822449484562081, 0.0, 1.0, 65.0, 42401.63840864289], 
processed observation next is [1.0, 0.13043478260869565, 0.4164358264081256, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7373574405339633, 0.7274149828187361, 0.0, 1.0, 1.0, 0.20191256385068043], 
reward next is 0.7981, 
noisyNet noise sample is [array([0.5798806], dtype=float32), -1.4885538]. 
=============================================
[2019-04-08 15:21:26,419] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.45844 ]
 [76.428185]
 [76.35145 ]
 [75.7985  ]
 [75.335945]], R is [[76.54392242]
 [76.58146667]
 [76.6190033 ]
 [76.6577301 ]
 [76.7000885 ]].
[2019-04-08 15:21:26,444] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4066317e-15 2.8202161e-08 7.6426120e-05 2.1439007e-05 4.5914214e-10
 4.7179687e-11 5.0544818e-06 1.4465256e-13 2.2521587e-16 5.7340341e-20
 9.9989700e-01], sum to 1.0000
[2019-04-08 15:21:26,444] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8114
[2019-04-08 15:21:26,459] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.133333333333333, 46.33333333333334, 281.3333333333334, 376.3333333333333, 19.0, 27.12961760209948, 0.8415768399191371, 0.0, 1.0, 65.0, 32014.64194335276], 
current ob forecast is [], 
actual action is [6.133333333333333, 65.0], 
sim time this is 4882800.0000, 
sim time next is 4883400.0000, 
raw observation next is [1.2, 46.0, 281.0, 390.0, 19.0, 27.17412784086365, 0.8427375336590067, 0.0, 1.0, 65.0, 30818.81479266441], 
processed observation next is [0.0, 0.5217391304347826, 0.4958448753462604, 0.46, 0.9366666666666666, 0.430939226519337, 0.08333333333333333, 0.7645106534053042, 0.780912511219669, 0.0, 1.0, 1.0, 0.14675626091744956], 
reward next is 0.8532, 
noisyNet noise sample is [array([-0.6966624], dtype=float32), 1.8859392]. 
=============================================
[2019-04-08 15:21:26,982] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.23409131e-14 1.10031495e-08 1.48074891e-04 2.47232820e-04
 6.03423658e-08 8.51210769e-11 2.74959984e-05 8.45885601e-12
 2.04089963e-15 1.90235593e-16 9.99577105e-01], sum to 1.0000
[2019-04-08 15:21:26,983] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8700
[2019-04-08 15:21:27,008] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 39.83333333333334, 0.0, 0.0, 19.0, 27.01820242911555, 0.7471820145311288, 0.0, 1.0, 65.0, 40889.988136706], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 4924200.0000, 
sim time next is 4924800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 27.03477710196879, 0.7427020775871945, 0.0, 1.0, 65.0, 39457.81824324778], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7528980918307324, 0.7475673591957315, 0.0, 1.0, 1.0, 0.18789437258689418], 
reward next is 0.8121, 
noisyNet noise sample is [array([2.040445], dtype=float32), -2.1423588]. 
=============================================
[2019-04-08 15:21:27,370] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2662041e-13 8.6299270e-09 1.4080239e-06 1.5457092e-05 7.2717299e-09
 7.3608463e-12 7.8543934e-07 2.3319827e-12 2.1357230e-16 7.2248577e-18
 9.9998236e-01], sum to 1.0000
[2019-04-08 15:21:27,370] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7514
[2019-04-08 15:21:27,389] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 46.5, 0.0, 0.0, 19.0, 27.06958777359554, 0.7262537791116781, 0.0, 1.0, 65.0, 39417.54156058478], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 4930200.0000, 
sim time next is 4930800.0000, 
raw observation next is [-0.6666666666666666, 47.66666666666666, 0.0, 0.0, 19.0, 27.04907371833167, 0.7213328537307041, 0.0, 1.0, 65.0, 38892.1550097061], 
processed observation next is [1.0, 0.043478260869565216, 0.44413665743305636, 0.47666666666666657, 0.0, 0.0, 0.08333333333333333, 0.7540894765276391, 0.7404442845769014, 0.0, 1.0, 1.0, 0.1852007381414576], 
reward next is 0.8148, 
noisyNet noise sample is [array([-0.65179527], dtype=float32), -0.74950504]. 
=============================================
[2019-04-08 15:21:28,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2620735e-19 1.4587125e-10 4.6806454e-05 8.6748085e-05 8.8282587e-10
 6.9404833e-12 7.9817401e-06 7.4395141e-14 3.9471994e-19 9.7511589e-21
 9.9985850e-01], sum to 1.0000
[2019-04-08 15:21:28,081] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4207
[2019-04-08 15:21:28,102] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 29.16666666666667, 118.6666666666667, 817.0, 22.5, 28.19519344061434, 0.9835724286666648, 1.0, 1.0, 65.0, 18845.70485671639], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 4963800.0000, 
sim time next is 4964400.0000, 
raw observation next is [3.0, 29.0, 119.5, 824.0, 22.5, 28.18187962701897, 0.997164994641128, 1.0, 1.0, 65.0, 18846.00950180993], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.29, 0.3983333333333333, 0.9104972375690608, 0.375, 0.8484899689182474, 0.8323883315470426, 1.0, 1.0, 1.0, 0.0897429023895711], 
reward next is 0.9103, 
noisyNet noise sample is [array([-0.14334498], dtype=float32), -0.26573002]. 
=============================================
[2019-04-08 15:21:28,380] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8256491e-15 8.6150731e-09 9.6401389e-05 2.4253826e-04 2.1683629e-08
 6.8787025e-11 4.6968395e-05 1.3002728e-12 2.8865586e-16 8.8205491e-17
 9.9961406e-01], sum to 1.0000
[2019-04-08 15:21:28,383] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0902
[2019-04-08 15:21:28,396] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.65018980824743, 0.6190204755316041, 0.0, 1.0, 65.0, 45506.18794182509], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 4950000.0000, 
sim time next is 4950600.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 22.5, 26.64288840393437, 0.6157320852560461, 0.0, 1.0, 65.0, 45038.77209382375], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.4933333333333334, 0.0, 0.0, 0.375, 0.7202407003278642, 0.7052440284186821, 0.0, 1.0, 1.0, 0.21447034330392262], 
reward next is 0.7855, 
noisyNet noise sample is [array([1.438235], dtype=float32), -1.0306093]. 
=============================================
[2019-04-08 15:21:28,574] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.8749649e-20 6.3826788e-10 2.1486047e-05 1.5555426e-05 3.4819575e-10
 4.4406573e-14 1.3571234e-08 3.7466223e-16 3.3042685e-21 4.8610631e-24
 9.9996293e-01], sum to 1.0000
[2019-04-08 15:21:28,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4212
[2019-04-08 15:21:28,610] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.833333333333334, 24.16666666666666, 121.0, 863.3333333333334, 22.5, 27.93362798031701, 1.121459110489136, 1.0, 1.0, 65.0, 36758.00858511082], 
current ob forecast is [], 
actual action is [11.833333333333334, 65.0], 
sim time this is 4971000.0000, 
sim time next is 4971600.0000, 
raw observation next is [7.0, 24.0, 120.0, 862.5, 22.5, 27.54218900249881, 1.074407423799754, 1.0, 1.0, 65.0, 21812.6338268645], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 0.24, 0.4, 0.9530386740331491, 0.375, 0.7951824168749008, 0.8581358079332514, 1.0, 1.0, 1.0, 0.10386968488983096], 
reward next is 0.8961, 
noisyNet noise sample is [array([0.20977211], dtype=float32), 0.036888897]. 
=============================================
[2019-04-08 15:21:29,505] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.58068557e-17 3.04013548e-09 1.60522075e-06 1.29289838e-05
 8.21132495e-09 5.95664522e-13 1.04188516e-07 5.52288114e-14
 7.30402301e-18 8.75614622e-20 9.99985337e-01], sum to 1.0000
[2019-04-08 15:21:29,505] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9116
[2019-04-08 15:21:29,521] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.8, 44.33333333333334, 266.0, 385.6666666666667, 19.0, 27.29508483160642, 0.8859350219622932, 0.0, 1.0, 65.0, 30908.97987467386], 
current ob forecast is [], 
actual action is [6.8, 65.0], 
sim time this is 4887600.0000, 
sim time next is 4888200.0000, 
raw observation next is [1.9, 44.16666666666667, 260.0, 383.3333333333333, 19.0, 27.31980523235091, 0.8886868400203044, 0.0, 1.0, 65.0, 30240.53867713463], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.4416666666666667, 0.8666666666666667, 0.42357274401473294, 0.08333333333333333, 0.7766504360292424, 0.7962289466734348, 0.0, 1.0, 1.0, 0.14400256512921253], 
reward next is 0.8560, 
noisyNet noise sample is [array([1.3282439], dtype=float32), 0.26925287]. 
=============================================
[2019-04-08 15:21:30,024] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6004901e-20 5.1472177e-12 2.2945263e-08 1.0835125e-04 3.7890997e-12
 6.8099115e-15 3.4438219e-08 3.2408510e-17 9.9681193e-21 6.4214994e-23
 9.9989164e-01], sum to 1.0000
[2019-04-08 15:21:30,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7679
[2019-04-08 15:21:30,087] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.5, 25.5, 72.0, 641.0, 22.5, 29.54376244330078, 1.26838492091313, 1.0, 1.0, 65.0, 22581.92794966706], 
current ob forecast is [], 
actual action is [13.5, 65.0], 
sim time this is 4984200.0000, 
sim time next is 4984800.0000, 
raw observation next is [8.333333333333334, 25.66666666666666, 65.66666666666667, 584.8333333333334, 22.5, 28.95591260951398, 1.359745590081457, 1.0, 1.0, 65.0, 32422.38034126342], 
processed observation next is [1.0, 0.6956521739130435, 0.6934441366574331, 0.2566666666666666, 0.2188888888888889, 0.6462246777163905, 0.375, 0.9129927174594984, 0.9532485300271523, 1.0, 1.0, 1.0, 0.15439228733934962], 
reward next is 0.8456, 
noisyNet noise sample is [array([-0.24091558], dtype=float32), -0.8384997]. 
=============================================
[2019-04-08 15:21:30,840] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4897250e-20 1.9122630e-10 4.6394318e-05 6.8101384e-05 2.5881702e-10
 9.5597510e-13 3.3235143e-08 6.2086901e-16 1.2187070e-18 8.4032003e-22
 9.9988544e-01], sum to 1.0000
[2019-04-08 15:21:30,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0226
[2019-04-08 15:21:30,848] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.16666666666667, 19.83333333333334, 113.3333333333333, 832.6666666666667, 22.5, 29.46875219940395, 1.376704933585212, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.16666666666667, 65.0], 
sim time this is 5062200.0000, 
sim time next is 5062800.0000, 
raw observation next is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 22.5, 29.62033583409081, 1.397831851210755, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7765466297322253, 0.1966666666666667, 0.373888888888889, 0.9125230202578268, 0.375, 0.9683613195075674, 0.965943950403585, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05214629], dtype=float32), -1.0802455]. 
=============================================
[2019-04-08 15:21:31,871] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.76654946e-18 2.43717158e-10 2.98682171e-05 1.17249459e-04
 1.29447814e-10 2.57840786e-13 6.47444176e-06 1.02146685e-14
 2.38037222e-19 4.40861972e-22 9.99846339e-01], sum to 1.0000
[2019-04-08 15:21:31,898] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9597
[2019-04-08 15:21:31,907] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.166666666666666, 24.83333333333334, 122.6666666666667, 858.3333333333334, 22.5, 28.5009580748662, 1.098272746087572, 1.0, 1.0, 65.0, 9424.696647641224], 
current ob forecast is [], 
actual action is [11.166666666666666, 65.0], 
sim time this is 4968600.0000, 
sim time next is 4969200.0000, 
raw observation next is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 22.5, 28.58688044072856, 1.099396095743725, 1.0, 1.0, 65.0, 18849.08964524937], 
processed observation next is [1.0, 0.5217391304347826, 0.6380424746075716, 0.2466666666666667, 0.40944444444444433, 0.9521178637200738, 0.375, 0.88224003672738, 0.8664653652479083, 1.0, 1.0, 1.0, 0.08975756973928271], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.17049745], dtype=float32), -0.5042694]. 
=============================================
[2019-04-08 15:21:32,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.8104236e-17 1.5969379e-10 2.4627329e-06 5.7772240e-06 1.3760690e-10
 1.6988601e-12 2.2784693e-07 9.4240669e-14 8.6051467e-18 1.8813354e-19
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:21:32,579] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1664
[2019-04-08 15:21:32,592] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.26667575888108, 0.8333655209576283, 0.0, 1.0, 65.0, 36271.05594505764], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 5023800.0000, 
sim time next is 5024400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.22019143438028, 0.8297107651718622, 0.0, 1.0, 65.0, 36328.10889457088], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.7683492861983566, 0.7765702550572874, 0.0, 1.0, 1.0, 0.17299099473605184], 
reward next is 0.8270, 
noisyNet noise sample is [array([-1.3259478], dtype=float32), 0.8221497]. 
=============================================
[2019-04-08 15:21:32,804] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.72559258e-16 3.19839337e-08 1.08460859e-04 1.19452794e-04
 2.01058947e-09 2.18911651e-12 8.89206331e-06 9.24486114e-14
 8.81119902e-18 4.09433401e-20 9.99763191e-01], sum to 1.0000
[2019-04-08 15:21:32,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9483
[2019-04-08 15:21:32,828] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 56.0, 97.0, 533.0, 22.5, 27.30475749257229, 0.8668558100878213, 1.0, 1.0, 65.0, 23480.13475581248], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 5041800.0000, 
sim time next is 5042400.0000, 
raw observation next is [-1.110223024625157e-16, 53.0, 99.5, 560.5, 22.5, 27.60048719750905, 0.9023931939003056, 1.0, 1.0, 65.0, 20425.44787600149], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.53, 0.33166666666666667, 0.6193370165745856, 0.375, 0.8000405997924208, 0.8007977313001019, 1.0, 1.0, 1.0, 0.097264037504769], 
reward next is 0.9027, 
noisyNet noise sample is [array([0.70450187], dtype=float32), -0.107434645]. 
=============================================
[2019-04-08 15:21:32,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.5420415e-20 2.3569008e-10 1.0686604e-07 3.8454553e-05 1.5549510e-11
 1.0509941e-13 2.2939237e-07 1.1678207e-15 5.1254851e-19 2.2452448e-22
 9.9996126e-01], sum to 1.0000
[2019-04-08 15:21:32,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4022
[2019-04-08 15:21:32,960] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:32,973] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 56.0, 97.0, 533.0, 22.5, 27.30473361131552, 0.8668441689258909, 1.0, 1.0, 65.0, 23476.51618515367], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 5041800.0000, 
sim time next is 5042400.0000, 
raw observation next is [-1.110223024625157e-16, 53.0, 99.5, 560.5, 22.5, 27.6006446299188, 0.9023562955469112, 1.0, 1.0, 65.0, 20420.20399606623], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.53, 0.33166666666666667, 0.6193370165745856, 0.375, 0.8000537191599001, 0.8007854318489703, 1.0, 1.0, 1.0, 0.09723906664793443], 
reward next is 0.9028, 
noisyNet noise sample is [array([-0.58549786], dtype=float32), -0.20546561]. 
=============================================
[2019-04-08 15:21:33,056] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1388562e-16 1.2428448e-08 1.6109036e-05 2.5959028e-04 8.2389884e-10
 1.8414285e-12 9.8018279e-07 1.0765113e-13 1.7837142e-17 1.3875466e-19
 9.9972326e-01], sum to 1.0000
[2019-04-08 15:21:33,056] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2153
[2019-04-08 15:21:33,069] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 53.33333333333334, 0.0, 0.0, 19.0, 27.07794917209009, 0.8082500485875278, 0.0, 1.0, 65.0, 38886.21219311826], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 5026800.0000, 
sim time next is 5027400.0000, 
raw observation next is [-1.0, 52.5, 0.0, 0.0, 19.0, 27.11762329743808, 0.7998105838558045, 0.0, 1.0, 65.0, 37597.6492485093], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.525, 0.0, 0.0, 0.08333333333333333, 0.7598019414531733, 0.7666035279519349, 0.0, 1.0, 1.0, 0.1790364249929014], 
reward next is 0.8210, 
noisyNet noise sample is [array([0.12548009], dtype=float32), 1.7359884]. 
=============================================
[2019-04-08 15:21:33,176] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:33,962] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:33,962] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:33,966] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res14/Eplus-env-sub_run2
[2019-04-08 15:21:34,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4650198e-18 8.7897772e-11 5.4087404e-06 3.3178272e-05 1.3097365e-12
 5.6643163e-14 2.6006868e-08 2.2559926e-16 1.5480171e-19 5.3787515e-23
 9.9996138e-01], sum to 1.0000
[2019-04-08 15:21:34,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5343
[2019-04-08 15:21:34,798] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.0, 17.0, 56.0, 438.5, 22.5, 30.30387437413603, 1.538124592585776, 1.0, 1.0, 65.00000000000009, 5384.607370600793], 
current ob forecast is [], 
actual action is [17.0, 65.0], 
sim time this is 5072400.0000, 
sim time next is 5073000.0000, 
raw observation next is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 22.5, 30.01769824431807, 1.504301602985807, 1.0, 1.0, 64.99999999999999, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7903970452446908, 0.17, 0.16444444444444442, 0.4305709023941067, 0.375, 1.0014748536931724, 1.0014338676619357, 1.0, 1.0, 0.9999999999999997, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7035844], dtype=float32), -2.0061054]. 
=============================================
[2019-04-08 15:21:34,820] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[97.71098]
 [98.06078]
 [98.37211]
 [98.54954]
 [98.77248]], R is [[97.41925812]
 [97.41942596]
 [97.40036011]
 [97.42636108]
 [97.45209503]].
[2019-04-08 15:21:35,236] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.32037845e-17 3.56736807e-10 3.85418161e-06 2.37460583e-04
 4.05673134e-10 1.31274809e-12 3.18039906e-06 1.40362562e-13
 5.39707846e-18 7.96191713e-20 9.99755561e-01], sum to 1.0000
[2019-04-08 15:21:35,238] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4343
[2019-04-08 15:21:35,292] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.666666666666668, 19.0, 0.0, 0.0, 19.0, 28.58758800981216, 1.249915320453451, 0.0, 1.0, 65.0, 18849.36522884578], 
current ob forecast is [], 
actual action is [14.666666666666668, 65.0], 
sim time this is 5084400.0000, 
sim time next is 5085000.0000, 
raw observation next is [9.5, 19.0, 0.0, 0.0, 19.0, 28.54665058115055, 1.242173234256976, 0.0, 1.0, 65.0, 18849.43086230444], 
processed observation next is [1.0, 0.8695652173913043, 0.7257617728531857, 0.19, 0.0, 0.0, 0.08333333333333333, 0.8788875484292126, 0.9140577447523253, 0.0, 1.0, 1.0, 0.0897591945824021], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.9378266], dtype=float32), -0.07276901]. 
=============================================
[2019-04-08 15:21:35,299] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[88.61706 ]
 [88.69252 ]
 [90.06515 ]
 [90.673225]
 [91.31912 ]], R is [[87.06304932]
 [87.10266113]
 [87.14187622]
 [87.18070221]
 [87.30889893]].
[2019-04-08 15:21:35,379] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0805268e-21 5.2130508e-11 2.5085808e-07 1.3884828e-06 2.0374017e-12
 5.1640858e-14 1.1999558e-07 1.7377704e-15 3.1285664e-20 2.6292998e-25
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:21:35,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3548
[2019-04-08 15:21:35,416] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 22.5, 28.2245881098273, 1.017408854291658, 1.0, 1.0, 65.0, 18847.98645133073], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [3.0, 41.0, 114.0, 753.5, 22.5, 28.23282786798917, 1.041318874007424, 1.0, 1.0, 65.0, 18849.17975330701], 
processed observation next is [1.0, 0.43478260869565216, 0.5457063711911359, 0.41, 0.38, 0.8325966850828729, 0.375, 0.852735655665764, 0.8471062913358081, 1.0, 1.0, 1.0, 0.08975799882527147], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.90410334], dtype=float32), 0.21946429]. 
=============================================
[2019-04-08 15:21:36,096] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:36,122] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2782664e-16 4.2954564e-09 3.7203874e-05 5.8967424e-05 4.2361350e-09
 3.0495634e-12 7.7071054e-06 2.8338603e-12 7.2500508e-17 2.1664941e-19
 9.9989605e-01], sum to 1.0000
[2019-04-08 15:21:36,122] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3986
[2019-04-08 15:21:36,132] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.55, 19.5, 0.0, 0.0, 19.0, 28.24119881293519, 1.133613981334233, 0.0, 1.0, 65.0, 18845.69376770112], 
current ob forecast is [], 
actual action is [13.55, 65.0], 
sim time this is 5092200.0000, 
sim time next is 5092800.0000, 
raw observation next is [8.5, 19.66666666666666, 0.0, 0.0, 19.0, 28.16327303091698, 1.123798796096559, 0.0, 1.0, 65.0, 18844.45182260134], 
processed observation next is [1.0, 0.9565217391304348, 0.698060941828255, 0.1966666666666666, 0.0, 0.0, 0.08333333333333333, 0.8469394192430816, 0.8745995986988531, 0.0, 1.0, 1.0, 0.08973548486953019], 
reward next is 0.9103, 
noisyNet noise sample is [array([-0.6608096], dtype=float32), 0.74662554]. 
=============================================
[2019-04-08 15:21:36,269] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:36,289] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:36,536] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:36,539] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:36,704] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:36,751] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:36,841] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:36,849] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7167546e-16 1.9592976e-09 1.8936299e-06 3.6564972e-05 9.9495401e-10
 2.9705763e-13 3.0844974e-06 8.5786635e-13 9.2241309e-19 2.9738270e-19
 9.9995840e-01], sum to 1.0000
[2019-04-08 15:21:36,849] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0585
[2019-04-08 15:21:36,868] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 65.0, 59.0, 101.0, 22.5, 26.86209233617039, 0.7613884747856098, 1.0, 1.0, 65.0, 40016.32180840218], 
current ob forecast is [], 
actual action is [2.5, 65.0], 
sim time this is 5038200.0000, 
sim time next is 5038800.0000, 
raw observation next is [-2.333333333333333, 65.0, 65.33333333333334, 173.0, 22.5, 26.89864085733695, 0.7623113628876007, 0.0, 1.0, 65.0, 38060.18929814803], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.65, 0.21777777777777782, 0.19116022099447513, 0.375, 0.7415534047780792, 0.7541037876292003, 0.0, 1.0, 1.0, 0.18123899665784773], 
reward next is 0.8188, 
noisyNet noise sample is [array([0.09488387], dtype=float32), 1.375375]. 
=============================================
[2019-04-08 15:21:36,989] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:37,047] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:37,050] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.7244518e-18 2.8007674e-10 5.8407735e-05 3.0275969e-05 7.2505457e-10
 4.4461421e-13 1.0704102e-07 4.7150754e-15 1.0253727e-18 1.8454866e-21
 9.9991119e-01], sum to 1.0000
[2019-04-08 15:21:37,054] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2133
[2019-04-08 15:21:37,060] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.333333333333334, 25.66666666666667, 94.83333333333334, 781.5, 22.5, 28.84031532661774, 1.260293376772926, 1.0, 1.0, 64.99999999999997, 6282.855152668538], 
current ob forecast is [], 
actual action is [13.333333333333334, 65.0], 
sim time this is 4980000.0000, 
sim time next is 4980600.0000, 
raw observation next is [8.5, 25.5, 92.0, 774.0, 22.5, 29.12629854008675, 1.289554338202007, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.698060941828255, 0.255, 0.30666666666666664, 0.8552486187845304, 0.375, 0.9271915450072292, 0.9298514460673356, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1225992], dtype=float32), -0.30275345]. 
=============================================
[2019-04-08 15:21:37,073] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:37,101] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:37,101] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:37,103] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res6/Eplus-env-sub_run2
[2019-04-08 15:21:37,280] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:37,280] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:37,282] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res15/Eplus-env-sub_run2
[2019-04-08 15:21:37,300] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:37,380] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:37,509] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:37,541] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:37,541] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:37,543] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res17/Eplus-env-sub_run2
[2019-04-08 15:21:37,670] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:37,737] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:37,758] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:37,760] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:37,765] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res10/Eplus-env-sub_run2
[2019-04-08 15:21:37,843] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:37,843] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:37,845] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res2/Eplus-env-sub_run2
[2019-04-08 15:21:38,049] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:38,049] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:38,051] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res16/Eplus-env-sub_run2
[2019-04-08 15:21:38,374] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:38,375] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:38,376] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res9/Eplus-env-sub_run2
[2019-04-08 15:21:38,521] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:38,521] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:38,523] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res13/Eplus-env-sub_run2
[2019-04-08 15:21:38,639] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:38,677] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:38,875] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:39,002] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:39,384] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.9062973e-16 9.8842357e-09 2.1940048e-05 2.0447046e-06 1.5017190e-09
 9.9649360e-12 1.1545175e-07 1.2615849e-12 1.2393021e-16 7.1436052e-19
 9.9997592e-01], sum to 1.0000
[2019-04-08 15:21:39,385] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7445
[2019-04-08 15:21:39,400] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 19.0, 27.30835726415015, 0.8742244334603918, 0.0, 1.0, 65.0, 36125.51118902557], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 5017200.0000, 
sim time next is 5017800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 27.27565929964318, 0.872175637089348, 0.0, 1.0, 65.0, 36704.45777707168], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7729716083035983, 0.790725212363116, 0.0, 1.0, 1.0, 0.1747831322717699], 
reward next is 0.8252, 
noisyNet noise sample is [array([1.0624899], dtype=float32), 0.3371399]. 
=============================================
[2019-04-08 15:21:39,653] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:39,653] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:39,655] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res5/Eplus-env-sub_run2
[2019-04-08 15:21:39,716] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:39,716] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:39,735] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res4/Eplus-env-sub_run2
[2019-04-08 15:21:40,500] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:40,674] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:40,843] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:40,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7191406e-19 1.2468632e-11 1.3891109e-06 2.8095340e-07 2.0542042e-12
 1.3952145e-15 4.9145832e-08 1.6016537e-16 3.2240247e-21 3.6844209e-23
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:21:40,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0897
[2019-04-08 15:21:40,948] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.0, 22.5, 118.0, 860.0, 22.5, 29.24654747106421, 1.248092537897232, 1.0, 1.0, 65.0, 15073.17497523088], 
current ob forecast is [], 
actual action is [15.0, 65.0], 
sim time this is 5059800.0000, 
sim time next is 5060400.0000, 
raw observation next is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 22.5, 28.86393993407945, 1.352578900639996, 1.0, 1.0, 65.0, 25278.6287183486], 
processed observation next is [1.0, 0.5652173913043478, 0.7488457987072946, 0.21666666666666662, 0.3894444444444443, 0.9427255985267036, 0.375, 0.9053283278399542, 0.9508596335466653, 1.0, 1.0, 1.0, 0.12037442246832666], 
reward next is 0.8796, 
noisyNet noise sample is [array([0.06700401], dtype=float32), -0.02665324]. 
=============================================
[2019-04-08 15:21:41,001] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:41,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1586607e-19 2.9499993e-11 2.1564772e-06 4.8977786e-06 1.4741532e-10
 1.5822966e-13 2.6549746e-08 5.8594235e-16 4.1981529e-20 4.2763794e-22
 9.9999297e-01], sum to 1.0000
[2019-04-08 15:21:41,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5726
[2019-04-08 15:21:41,020] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.16666666666667, 19.83333333333334, 113.3333333333333, 832.6666666666667, 22.5, 29.46873148969753, 1.376700516797877, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.16666666666667, 65.0], 
sim time this is 5062200.0000, 
sim time next is 5062800.0000, 
raw observation next is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 22.5, 29.62031376796963, 1.397827392189025, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7765466297322253, 0.1966666666666667, 0.373888888888889, 0.9125230202578268, 0.375, 0.9683594806641359, 0.9659424640630082, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7300591], dtype=float32), 0.5841143]. 
=============================================
[2019-04-08 15:21:41,329] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:41,493] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:41,501] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res12/Eplus-env-sub_run2
[2019-04-08 15:21:41,570] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:41,674] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:41,674] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:41,676] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res8/Eplus-env-sub_run2
[2019-04-08 15:21:41,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7591362e-15 4.3520512e-08 7.1036257e-04 1.2394326e-04 4.1972742e-10
 1.9309678e-12 1.0033583e-06 1.3562022e-13 4.4182311e-17 2.4182551e-20
 9.9916470e-01], sum to 1.0000
[2019-04-08 15:21:41,834] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5616
[2019-04-08 15:21:41,840] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 22.5, 29.13343896963856, 1.327849031274014, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [15.83333333333333, 65.0], 
sim time this is 5080200.0000, 
sim time next is 5080800.0000, 
raw observation next is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 22.5, 29.03486601258075, 1.308583506167629, 0.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7580794090489382, 0.17666666666666672, 0.0, 0.0, 0.375, 0.9195721677150624, 0.9361945020558764, 0.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3908301], dtype=float32), 0.5704842]. 
=============================================
[2019-04-08 15:21:41,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1112498e-17 9.8339037e-12 1.5775958e-06 4.9368209e-06 1.4204569e-10
 5.1907957e-13 7.3576579e-08 8.2603819e-14 2.0287270e-18 1.9555584e-20
 9.9999344e-01], sum to 1.0000
[2019-04-08 15:21:41,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5113
[2019-04-08 15:21:42,013] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.833333333333334, 19.0, 0.0, 0.0, 19.0, 28.63356537357681, 1.258045746260502, 0.0, 1.0, 65.0, 18849.18924994936], 
current ob forecast is [], 
actual action is [14.833333333333334, 65.0], 
sim time this is 5083800.0000, 
sim time next is 5084400.0000, 
raw observation next is [9.666666666666668, 19.0, 0.0, 0.0, 19.0, 28.58762228684386, 1.249925449853743, 0.0, 1.0, 65.0, 18849.36989044823], 
processed observation next is [1.0, 0.8695652173913043, 0.7303785780240075, 0.19, 0.0, 0.0, 0.08333333333333333, 0.8823018572369884, 0.9166418166179143, 0.0, 1.0, 1.0, 0.08975890424022967], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.48837212], dtype=float32), -1.4070388]. 
=============================================
[2019-04-08 15:21:42,329] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:42,329] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:42,331] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res3/Eplus-env-sub_run2
[2019-04-08 15:21:42,646] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:42,855] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:42,892] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:21:43,136] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:21:43,647] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:43,648] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:43,649] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res11/Eplus-env-sub_run2
[2019-04-08 15:21:43,893] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:21:43,893] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:21:43,895] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res7/Eplus-env-sub_run2
[2019-04-08 15:21:51,501] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6507961e-14 8.4075758e-08 6.2167797e-05 1.5519709e-04 3.7495660e-08
 1.5073220e-10 3.4260181e-06 3.3933780e-13 4.8152897e-16 6.5381684e-19
 9.9977905e-01], sum to 1.0000
[2019-04-08 15:21:51,501] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3563
[2019-04-08 15:21:51,520] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 24.35328770244382, 0.1581250225647415, 0.0, 1.0, 65.0, 56951.63691965153], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 17400.0000, 
sim time next is 18000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 24.39099467294451, 0.1687963805217122, 0.0, 1.0, 65.0, 56805.91228681617], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.5325828894120427, 0.5562654601739041, 0.0, 1.0, 1.0, 0.2705043442229342], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.6771734], dtype=float32), -0.8224373]. 
=============================================
[2019-04-08 15:21:51,574] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.0774  ]
 [78.008354]
 [78.03672 ]
 [78.22489 ]
 [78.374626]], R is [[78.13773346]
 [78.0851593 ]
 [78.03238678]
 [77.97937775]
 [77.92603302]].
[2019-04-08 15:21:53,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6812543e-17 2.0318776e-09 6.3794387e-06 1.6676006e-05 2.2230588e-09
 1.0306038e-12 2.9273579e-07 7.4289214e-14 1.2556052e-17 6.3011367e-19
 9.9997663e-01], sum to 1.0000
[2019-04-08 15:21:53,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6477
[2019-04-08 15:21:53,651] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 19.0, 26.30942188973735, 0.6405935507271351, 0.0, 1.0, 65.0, 49786.2223877006], 
current ob forecast is [], 
actual action is [8.8, 65.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 19.0, 26.30849674577412, 0.6423590235126654, 0.0, 1.0, 65.0, 49796.87896452782], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.08333333333333333, 0.6923747288145101, 0.7141196745042219, 0.0, 1.0, 1.0, 0.23712799506918011], 
reward next is 0.7629, 
noisyNet noise sample is [array([-0.25479156], dtype=float32), 1.8294277]. 
=============================================
[2019-04-08 15:21:53,703] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.0241  ]
 [82.04068 ]
 [82.072525]
 [82.09554 ]
 [82.18038 ]], R is [[81.94958496]
 [81.893013  ]
 [81.83708954]
 [81.78199768]
 [81.72776031]].
[2019-04-08 15:21:54,375] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9663808e-16 2.2790974e-09 4.9886216e-06 8.8962042e-05 2.4783325e-10
 1.6505176e-12 1.1853942e-06 5.2391895e-14 5.0172203e-17 7.8268902e-20
 9.9990487e-01], sum to 1.0000
[2019-04-08 15:21:54,378] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7905
[2019-04-08 15:21:54,393] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 70.0, 0.0, 19.0, 25.92583782081531, 0.5270523602025458, 0.0, 1.0, 65.0, 50803.38199756491], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 40200.0000, 
sim time next is 40800.0000, 
raw observation next is [7.7, 93.0, 72.5, 0.0, 19.0, 25.9410168880398, 0.5313366397087402, 0.0, 1.0, 65.0, 50592.62656654379], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.24166666666666667, 0.0, 0.08333333333333333, 0.66175140733665, 0.6771122132362467, 0.0, 1.0, 1.0, 0.24091726936449423], 
reward next is 0.7591, 
noisyNet noise sample is [array([-0.5207918], dtype=float32), 0.2163736]. 
=============================================
[2019-04-08 15:21:55,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7196879e-16 3.1515299e-10 1.9784050e-06 4.5019979e-06 3.7551349e-09
 2.1944592e-11 1.9562362e-06 2.0230701e-14 1.5060332e-17 4.1762025e-19
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:21:55,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3459
[2019-04-08 15:21:55,399] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 24.87701962573823, 0.2885705827964443, 0.0, 1.0, 65.0, 55872.40834064736], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 24000.0000, 
sim time next is 24600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 19.0, 24.91276266926749, 0.2978918593969613, 0.0, 1.0, 65.0, 55850.03224207821], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.5760635557722908, 0.5992972864656537, 0.0, 1.0, 1.0, 0.2659525344860867], 
reward next is 0.7340, 
noisyNet noise sample is [array([-0.64581233], dtype=float32), -0.89480793]. 
=============================================
[2019-04-08 15:21:55,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4198971e-17 6.5216175e-09 1.0995520e-06 6.3404668e-06 3.8813326e-09
 9.0341498e-13 9.6940516e-07 1.3702158e-13 3.7516293e-19 1.9150049e-19
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:21:55,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2438
[2019-04-08 15:21:55,429] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.433333333333334, 87.0, 0.0, 0.0, 19.0, 26.33255407319399, 0.6443063190691273, 0.0, 1.0, 65.0, 49314.98416164592], 
current ob forecast is [], 
actual action is [8.433333333333334, 65.0], 
sim time this is 69600.0000, 
sim time next is 70200.0000, 
raw observation next is [3.25, 87.5, 0.0, 0.0, 19.0, 26.32857346334387, 0.642911341792637, 0.0, 1.0, 65.0, 49441.84760333685], 
processed observation next is [0.0, 0.8260869565217391, 0.5526315789473685, 0.875, 0.0, 0.0, 0.08333333333333333, 0.694047788611989, 0.7143037805975457, 0.0, 1.0, 1.0, 0.2354373695396993], 
reward next is 0.7646, 
noisyNet noise sample is [array([-1.0762289], dtype=float32), -0.92696553]. 
=============================================
[2019-04-08 15:21:55,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9773840e-17 6.8039246e-10 1.0910502e-07 3.2822925e-05 2.7388822e-09
 1.5570089e-13 1.2826909e-07 4.0522115e-15 5.5586863e-19 4.4195101e-21
 9.9996698e-01], sum to 1.0000
[2019-04-08 15:21:55,624] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-08 15:21:55,667] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 75.0, 0.0, 19.0, 25.91662077195372, 0.5315757694435012, 0.0, 1.0, 65.0, 51607.80948778592], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 41400.0000, 
sim time next is 42000.0000, 
raw observation next is [7.699999999999999, 93.0, 78.5, 0.0, 19.0, 25.92829098064627, 0.537606691394034, 0.0, 1.0, 65.0, 51408.22641566627], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.26166666666666666, 0.0, 0.08333333333333333, 0.6606909150538559, 0.679202230464678, 0.0, 1.0, 1.0, 0.2448010781698394], 
reward next is 0.7552, 
noisyNet noise sample is [array([-0.9469575], dtype=float32), -0.4840409]. 
=============================================
[2019-04-08 15:21:55,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[86.688965]
 [86.54284 ]
 [86.34488 ]
 [86.17081 ]
 [85.982704]], R is [[86.76212311]
 [86.64875031]
 [86.53974152]
 [86.43132019]
 [86.31783295]].
[2019-04-08 15:21:56,107] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.11897049e-16 4.86036233e-09 4.67634800e-06 8.53928650e-05
 2.19749038e-10 2.62253023e-12 3.37123265e-06 4.02760854e-12
 5.90450892e-16 1.02522264e-19 9.99906540e-01], sum to 1.0000
[2019-04-08 15:21:56,109] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4686
[2019-04-08 15:21:56,129] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 19.0, 24.64332117034871, 0.2247206395810059, 0.0, 1.0, 65.0, 56235.99904448086], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 19800.0000, 
sim time next is 20400.0000, 
raw observation next is [7.699999999999999, 93.0, 0.0, 0.0, 19.0, 24.67395767516726, 0.2334616749333142, 0.0, 1.0, 65.0, 56164.74758799356], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.08333333333333333, 0.5561631395972716, 0.5778205583111048, 0.0, 1.0, 1.0, 0.2674511789904455], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.32205838], dtype=float32), -1.6440586]. 
=============================================
[2019-04-08 15:21:57,032] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2543197e-15 6.4758582e-10 8.9713285e-06 1.6450253e-05 6.6141209e-10
 2.7072814e-12 9.5215336e-07 5.1756689e-12 2.6340445e-17 9.8929529e-19
 9.9997365e-01], sum to 1.0000
[2019-04-08 15:21:57,035] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4623
[2019-04-08 15:21:57,052] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 10.5, 0.0, 19.0, 25.13171423877824, 0.3603621579778718, 0.0, 1.0, 65.0, 55077.92966591194], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 28800.0000, 
sim time next is 29400.0000, 
raw observation next is [7.7, 93.0, 14.0, 0.0, 19.0, 25.22573450027307, 0.3662049870541011, 0.0, 1.0, 65.0, 53739.3859116542], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.04666666666666667, 0.0, 0.08333333333333333, 0.6021445416894226, 0.6220683290180337, 0.0, 1.0, 1.0, 0.25590183767454383], 
reward next is 0.7441, 
noisyNet noise sample is [array([-2.3712974], dtype=float32), 0.24384807]. 
=============================================
[2019-04-08 15:21:57,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6762460e-18 2.0353771e-10 1.1051786e-05 3.7590456e-05 2.4065876e-09
 3.3534755e-12 4.4413312e-07 2.6461140e-14 8.2565802e-18 3.6538658e-20
 9.9995089e-01], sum to 1.0000
[2019-04-08 15:21:57,198] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8925
[2019-04-08 15:21:57,254] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 19.0, 26.25496701223689, 0.5955973088605866, 0.0, 1.0, 65.0, 52492.78681388821], 
current ob forecast is [], 
actual action is [2.566666666666667, 65.0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 19.0, 26.19396932952962, 0.6057436530479403, 0.0, 1.0, 65.0, 54944.52976497192], 
processed observation next is [1.0, 0.08695652173913043, 0.3901200369344414, 0.8766666666666666, 0.0, 0.0, 0.08333333333333333, 0.6828307774608016, 0.7019145510159802, 0.0, 1.0, 1.0, 0.26164061792843774], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.09618451], dtype=float32), 1.1586487]. 
=============================================
[2019-04-08 15:21:57,728] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.9739358e-16 1.6721248e-09 2.0392898e-05 2.0722813e-05 3.9909573e-09
 2.7448586e-12 2.3319934e-07 6.1725847e-13 3.3414912e-16 5.3512004e-19
 9.9995863e-01], sum to 1.0000
[2019-04-08 15:21:57,728] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7959
[2019-04-08 15:21:57,762] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.416666666666667, 82.66666666666667, 28.66666666666666, 0.0, 19.0, 26.39489279297279, 0.6497881337259529, 0.0, 1.0, 65.0, 46490.178789612], 
current ob forecast is [], 
actual action is [11.416666666666668, 65.0], 
sim time this is 58200.0000, 
sim time next is 58800.0000, 
raw observation next is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 19.0, 26.38327145732615, 0.6495576392803267, 0.0, 1.0, 65.0, 47398.86249707032], 
processed observation next is [0.0, 0.6956521739130435, 0.6352723915050786, 0.8333333333333335, 0.07777777777777777, 0.0, 0.08333333333333333, 0.6986059547771791, 0.7165192130934422, 0.0, 1.0, 1.0, 0.22570886903366819], 
reward next is 0.7743, 
noisyNet noise sample is [array([-0.5034941], dtype=float32), -0.33985195]. 
=============================================
[2019-04-08 15:21:57,912] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.6760467e-16 1.2767182e-08 7.3783503e-06 9.9455487e-05 2.2299936e-10
 9.4632054e-12 3.7547458e-08 4.0464943e-13 1.0723962e-17 1.2253532e-20
 9.9989307e-01], sum to 1.0000
[2019-04-08 15:21:57,912] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6136
[2019-04-08 15:21:57,947] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.0, 89.5, 96.0, 0.0, 19.0, 26.05936632387974, 0.5742008858807789, 0.0, 1.0, 65.0, 49617.29763937035], 
current ob forecast is [], 
actual action is [13.0, 65.0], 
sim time this is 45000.0000, 
sim time next is 45600.0000, 
raw observation next is [8.1, 88.33333333333333, 94.5, 0.0, 19.0, 26.08550256977507, 0.5808535238970584, 0.0, 1.0, 65.0, 49251.78077273544], 
processed observation next is [0.0, 0.5217391304347826, 0.6869806094182825, 0.8833333333333333, 0.315, 0.0, 0.08333333333333333, 0.6737918808145892, 0.6936178412990195, 0.0, 1.0, 1.0, 0.23453228939397827], 
reward next is 0.7655, 
noisyNet noise sample is [array([1.2349083], dtype=float32), 0.17458102]. 
=============================================
[2019-04-08 15:21:58,085] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0556090e-15 8.4264231e-09 5.4330008e-05 4.8248199e-05 1.9023811e-09
 6.2193903e-11 1.5832035e-06 2.7027921e-13 4.2763176e-17 3.2299014e-19
 9.9989581e-01], sum to 1.0000
[2019-04-08 15:21:58,086] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4482
[2019-04-08 15:21:58,144] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.883333333333334, 88.5, 0.0, 0.0, 19.0, 26.3308706868841, 0.6444463082814407, 0.0, 1.0, 65.0, 49559.59541154932], 
current ob forecast is [], 
actual action is [7.883333333333335, 65.0], 
sim time this is 71400.0000, 
sim time next is 72000.0000, 
raw observation next is [2.7, 89.0, 0.0, 0.0, 19.0, 26.32348839493836, 0.642953603509664, 0.0, 1.0, 65.0, 49652.74455725974], 
processed observation next is [0.0, 0.8695652173913043, 0.5373961218836566, 0.89, 0.0, 0.0, 0.08333333333333333, 0.6936240329115299, 0.7143178678365546, 0.0, 1.0, 1.0, 0.23644164074885593], 
reward next is 0.7636, 
noisyNet noise sample is [array([0.63977146], dtype=float32), -0.35783112]. 
=============================================
[2019-04-08 15:21:58,154] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.387634]
 [82.39613 ]
 [82.3815  ]
 [82.40466 ]
 [82.367004]], R is [[82.33946991]
 [82.28007507]
 [82.22050476]
 [82.16327667]
 [82.10723114]].
[2019-04-08 15:21:58,573] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4269738e-15 3.1917626e-08 5.9341823e-05 3.0185017e-04 1.1891696e-08
 2.6024952e-10 1.6705644e-05 1.3151262e-12 1.0325215e-17 6.0172646e-20
 9.9962199e-01], sum to 1.0000
[2019-04-08 15:21:58,574] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0820
[2019-04-08 15:21:58,620] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.516666666666667, 91.0, 0.0, 0.0, 19.0, 26.23672626050711, 0.6107673209603965, 0.0, 1.0, 65.0, 53031.63994770782], 
current ob forecast is [], 
actual action is [3.483333333333333, 65.0], 
sim time this is 93000.0000, 
sim time next is 93600.0000, 
raw observation next is [-1.7, 91.0, 0.0, 0.0, 19.0, 26.24757083512741, 0.6100299384339074, 0.0, 1.0, 65.0, 52150.98658455268], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.91, 0.0, 0.0, 0.08333333333333333, 0.6872975695939507, 0.7033433128113025, 0.0, 1.0, 1.0, 0.24833803135501276], 
reward next is 0.7517, 
noisyNet noise sample is [array([1.1471415], dtype=float32), -1.8787041]. 
=============================================
[2019-04-08 15:21:59,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.51525102e-15 6.26496632e-09 7.13855798e-06 3.42587242e-04
 8.04526668e-09 2.87345647e-10 1.47386045e-05 1.10829633e-13
 4.79941209e-17 1.07712002e-19 9.99635458e-01], sum to 1.0000
[2019-04-08 15:21:59,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6755
[2019-04-08 15:21:59,096] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 71.5, 0.0, 0.0, 19.0, 25.89145244290883, 0.5266374551051664, 0.0, 1.0, 65.0, 59865.45912642338], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 109800.0000, 
sim time next is 110400.0000, 
raw observation next is [-7.1, 70.33333333333333, 0.0, 0.0, 19.0, 25.90062451094707, 0.5187076626479986, 0.0, 1.0, 65.0, 59832.80013537656], 
processed observation next is [1.0, 0.2608695652173913, 0.2659279778393352, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.6583853759122557, 0.6729025542159995, 0.0, 1.0, 1.0, 0.2849180958827455], 
reward next is 0.7151, 
noisyNet noise sample is [array([-0.46450123], dtype=float32), 0.19285429]. 
=============================================
[2019-04-08 15:21:59,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2285406e-16 4.5031321e-09 1.5895146e-05 1.8134468e-05 2.7257233e-10
 1.8369071e-12 8.0775071e-06 3.3427306e-13 5.7116437e-17 4.3094957e-19
 9.9995792e-01], sum to 1.0000
[2019-04-08 15:21:59,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4883
[2019-04-08 15:21:59,427] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.733333333333333, 74.83333333333334, 0.0, 0.0, 19.0, 26.08085435183847, 0.5632280367218031, 0.0, 1.0, 65.0, 57713.95542034558], 
current ob forecast is [], 
actual action is [0.2666666666666666, 65.0], 
sim time this is 103800.0000, 
sim time next is 104400.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.05464467513568, 0.555024731086086, 0.0, 1.0, 65.0, 57688.93170075056], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.67122038959464, 0.685008243695362, 0.0, 1.0, 1.0, 0.27470919857500264], 
reward next is 0.7253, 
noisyNet noise sample is [array([0.3527516], dtype=float32), -0.9606312]. 
=============================================
[2019-04-08 15:22:00,361] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0456435e-16 2.0730113e-09 2.3666160e-05 7.2634562e-06 7.1795896e-11
 3.9086453e-11 2.2872071e-06 2.3458728e-13 7.6127086e-18 3.0296020e-20
 9.9996674e-01], sum to 1.0000
[2019-04-08 15:22:00,378] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5287
[2019-04-08 15:22:00,398] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 64.33333333333333, 0.0, 0.0, 22.5, 26.63428820144437, 0.6638366916083563, 1.0, 1.0, 65.0, 54523.29810346281], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 150000.0000, 
sim time next is 150600.0000, 
raw observation next is [-7.299999999999999, 62.66666666666666, 0.0, 0.0, 22.5, 26.48681996094088, 0.6537205619146755, 1.0, 1.0, 65.0, 56322.22393110475], 
processed observation next is [1.0, 0.7391304347826086, 0.2603878116343491, 0.6266666666666666, 0.0, 0.0, 0.375, 0.7072349967450734, 0.7179068539715585, 1.0, 1.0, 1.0, 0.2682010663385941], 
reward next is 0.7318, 
noisyNet noise sample is [array([1.319192], dtype=float32), 0.28109765]. 
=============================================
[2019-04-08 15:22:00,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5729024e-17 2.4607005e-09 3.9912843e-06 3.8317448e-05 5.3735211e-11
 5.1044286e-12 5.5373641e-07 5.1398088e-14 1.0992051e-17 1.4613777e-20
 9.9995708e-01], sum to 1.0000
[2019-04-08 15:22:00,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1432
[2019-04-08 15:22:00,619] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 84.33333333333334, 0.0, 0.0, 19.0, 26.25912144914588, 0.5976917974021253, 0.0, 1.0, 65.0, 53598.8599876985], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 98400.0000, 
sim time next is 99000.0000, 
raw observation next is [-3.1, 83.0, 0.0, 0.0, 19.0, 26.23847776308351, 0.5865424471163655, 0.0, 1.0, 65.0, 53490.57724076521], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6865398135902924, 0.6955141490387885, 0.0, 1.0, 1.0, 0.25471703447983435], 
reward next is 0.7453, 
noisyNet noise sample is [array([-0.06386529], dtype=float32), 1.3135273]. 
=============================================
[2019-04-08 15:22:00,644] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.72956]
 [81.83596]
 [81.40784]
 [81.14746]
 [80.80624]], R is [[81.50572968]
 [81.43544006]
 [81.36660767]
 [81.30510712]
 [81.23046875]].
[2019-04-08 15:22:01,034] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5577048e-15 2.0388902e-09 2.5992704e-04 6.5722573e-04 1.9126696e-09
 3.6530799e-11 2.8290566e-05 1.2363833e-12 4.0355578e-17 2.7447536e-19
 9.9905461e-01], sum to 1.0000
[2019-04-08 15:22:01,035] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0524
[2019-04-08 15:22:01,058] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.300000000000001, 68.0, 0.0, 0.0, 22.5, 25.79778799118332, 0.5063250940205523, 1.0, 1.0, 65.0, 61102.2518876567], 
current ob forecast is [], 
actual action is [-2.3000000000000007, 65.0], 
sim time this is 112800.0000, 
sim time next is 113400.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 22.5, 25.79589922888368, 0.5085233454705423, 1.0, 1.0, 65.0, 60431.05994980538], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.375, 0.64965826907364, 0.6695077818235141, 1.0, 1.0, 1.0, 0.2877669521419304], 
reward next is 0.7122, 
noisyNet noise sample is [array([-1.0299156], dtype=float32), 0.5386123]. 
=============================================
[2019-04-08 15:22:01,601] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.7725547e-16 1.5228246e-09 9.3231414e-05 1.7346401e-03 3.6202783e-09
 3.0089251e-11 8.6004780e-05 3.2956548e-12 1.7551788e-17 3.6705317e-18
 9.9808609e-01], sum to 1.0000
[2019-04-08 15:22:01,602] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8328
[2019-04-08 15:22:01,631] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 19.0, 26.25691036453848, 0.5961430499988946, 0.0, 1.0, 65.0, 52469.13155621672], 
current ob forecast is [], 
actual action is [2.566666666666667, 65.0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 19.0, 26.19588918367335, 0.6062877956715502, 0.0, 1.0, 65.0, 54920.96336012022], 
processed observation next is [1.0, 0.08695652173913043, 0.3901200369344414, 0.8766666666666666, 0.0, 0.0, 0.08333333333333333, 0.6829907653061126, 0.7020959318905167, 0.0, 1.0, 1.0, 0.26152839695295343], 
reward next is 0.7385, 
noisyNet noise sample is [array([-2.8727696], dtype=float32), 1.8954937]. 
=============================================
[2019-04-08 15:22:01,737] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.1144812e-16 1.3702399e-09 8.7307046e-05 1.7885623e-03 2.7630869e-09
 2.8326308e-11 7.7874749e-05 2.6510645e-12 1.3612187e-17 2.3850874e-18
 9.9804628e-01], sum to 1.0000
[2019-04-08 15:22:01,740] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5870
[2019-04-08 15:22:01,757] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.25535569536276, 0.5946470590670964, 0.0, 1.0, 65.0, 52032.46775064491], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 97200.0000, 
sim time next is 97800.0000, 
raw observation next is [-2.9, 85.66666666666667, 0.0, 0.0, 19.0, 26.22251839753574, 0.6018480786382655, 0.0, 1.0, 65.0, 53428.22934678585], 
processed observation next is [1.0, 0.13043478260869565, 0.38227146814404434, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.6852098664613117, 0.7006160262127552, 0.0, 1.0, 1.0, 0.25442013974659927], 
reward next is 0.7456, 
noisyNet noise sample is [array([-2.8727696], dtype=float32), 1.8954937]. 
=============================================
[2019-04-08 15:22:02,195] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-08 15:22:02,198] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:22:02,199] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:22:02,200] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:22:02,200] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:22:02,201] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:22:02,201] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:22:02,204] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run8
[2019-04-08 15:22:02,221] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run8
[2019-04-08 15:22:02,233] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run8
[2019-04-08 15:22:49,391] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.058557037]
[2019-04-08 15:22:49,391] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-0.05030999666666658, 60.44754095666667, 196.3480406166667, 0.0, 19.0, 26.80221096745976, 0.7248274191559713, 0.0, 1.0, 65.0, 42246.1938714953]
[2019-04-08 15:22:49,391] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:22:49,391] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [5.74355995e-16 5.11945863e-09 1.37243205e-05 3.58988400e-05
 1.97587791e-09 8.68412980e-12 1.43736747e-06 4.55204667e-13
 8.54205662e-17 3.66104607e-19 9.99948978e-01], sampled 0.25142622650455604
[2019-04-08 15:23:28,518] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.058557037]
[2019-04-08 15:23:28,518] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [2.604052611166667, 53.22821258666667, 134.8408976, 773.9730789666667, 22.5, 28.04553034791056, 1.048595115474244, 1.0, 1.0, 65.0, 18847.61167834224]
[2019-04-08 15:23:28,518] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:23:28,519] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [7.2306198e-18 7.1411094e-10 4.3501836e-06 9.2805594e-06 1.4898595e-10
 4.0007844e-13 2.3271257e-07 1.0337467e-14 6.9032668e-19 1.8808928e-21
 9.9998617e-01], sampled 0.43275877303291
[2019-04-08 15:23:30,972] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6989.6510 316333283.0942 2958.2603
[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:30,994] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:31,123] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,210] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6800.9551 355959435.6306 2370.5094
[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,231] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:36,345] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,271] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.4473 342836061.4741 2768.1066
[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,291] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:41,402] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:23:42,294] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 140000, evaluation results [140000.0, 6863.4473263139325, 342836061.474076, 2768.1066186944513, 6989.651032884548, 316333283.0942475, 2958.260328491998, 6800.955068425689, 355959435.6306067, 2370.5093880671925]
[2019-04-08 15:23:42,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.30903818e-15 3.36194672e-09 3.00581269e-06 9.29606813e-06
 5.87275739e-09 1.45148026e-11 7.15934675e-07 2.12090314e-13
 5.48173188e-17 1.09412873e-19 9.99987006e-01], sum to 1.0000
[2019-04-08 15:23:42,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2385
[2019-04-08 15:23:42,384] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 69.0, 0.0, 0.0, 19.0, 25.64060714444221, 0.4912477535095154, 0.0, 1.0, 65.0, 61103.32450796283], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 163200.0000, 
sim time next is 163800.0000, 
raw observation next is [-8.4, 69.5, 0.0, 0.0, 19.0, 25.6335787717136, 0.4879903823894054, 0.0, 1.0, 65.0, 61017.44751866919], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.695, 0.0, 0.0, 0.08333333333333333, 0.6361315643094668, 0.6626634607964684, 0.0, 1.0, 1.0, 0.29055927389842473], 
reward next is 0.7094, 
noisyNet noise sample is [array([-0.98056966], dtype=float32), -0.45196396]. 
=============================================
[2019-04-08 15:23:43,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.99462564e-16 1.09451981e-09 5.12520019e-05 1.13552145e-04
 2.51369991e-09 1.16023760e-10 1.32321725e-06 1.86815827e-13
 3.13226140e-16 1.33151325e-18 9.99833822e-01], sum to 1.0000
[2019-04-08 15:23:43,176] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4105
[2019-04-08 15:23:43,203] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 22.5, 25.14436836897175, 0.3248697713601252, 1.0, 1.0, 65.0, 61515.20622365382], 
current ob forecast is [], 
actual action is [-3.900000000000002, 65.0], 
sim time this is 199200.0000, 
sim time next is 199800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 22.5, 25.14152307492302, 0.3222714893102009, 0.0, 1.0, 65.0, 60749.38639049826], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.375, 0.5951269229102515, 0.607423829770067, 0.0, 1.0, 1.0, 0.289282792335706], 
reward next is 0.7107, 
noisyNet noise sample is [array([2.7317882], dtype=float32), 0.36382964]. 
=============================================
[2019-04-08 15:23:43,211] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3203890e-16 9.1907459e-10 4.2087318e-05 9.6402982e-05 2.2993865e-09
 9.6232938e-11 1.1105209e-06 1.4992016e-13 2.6493261e-16 1.0841779e-18
 9.9986041e-01], sum to 1.0000
[2019-04-08 15:23:43,214] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2125
[2019-04-08 15:23:43,242] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 22.5, 25.14152307492302, 0.3222714893102009, 0.0, 1.0, 65.0, 60749.38639049826], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 199800.0000, 
sim time next is 200400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 22.5, 25.1427748572133, 0.3187066107593702, 1.0, 1.0, 65.0, 60376.38150363033], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.375, 0.5952312381011083, 0.60623553691979, 1.0, 1.0, 1.0, 0.28750657858871587], 
reward next is 0.7125, 
noisyNet noise sample is [array([2.7317882], dtype=float32), 0.36382964]. 
=============================================
[2019-04-08 15:23:44,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1883384e-14 5.2113256e-09 1.0535906e-04 1.3917115e-03 3.4576424e-08
 4.7691895e-10 9.7199234e-07 5.3167807e-11 3.6666629e-15 2.3913624e-17
 9.9850190e-01], sum to 1.0000
[2019-04-08 15:23:44,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3503
[2019-04-08 15:23:44,281] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 25.30976578770643, 0.3639784767411258, 0.0, 1.0, 65.0, 60347.71226957715], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 189000.0000, 
sim time next is 189600.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 25.26838924668643, 0.3626139466623493, 0.0, 1.0, 65.0, 60662.91057996945], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6056991038905357, 0.6208713155541165, 0.0, 1.0, 1.0, 0.2888710027617593], 
reward next is 0.7111, 
noisyNet noise sample is [array([0.06598175], dtype=float32), -1.3845614]. 
=============================================
[2019-04-08 15:23:44,371] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1714483e-14 2.4388285e-09 9.4042131e-05 8.0932459e-06 3.6378314e-09
 9.1625388e-11 1.3194379e-05 4.2742234e-13 2.6899636e-15 1.4970631e-17
 9.9988461e-01], sum to 1.0000
[2019-04-08 15:23:44,376] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8479
[2019-04-08 15:23:44,395] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 25.50426870701992, 0.440022898401469, 0.0, 1.0, 65.0, 60085.54421195311], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 172200.0000, 
sim time next is 172800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 25.4896989975403, 0.4361245088098106, 0.0, 1.0, 65.0, 60060.92682136737], 
processed observation next is [1.0, 0.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6241415831283584, 0.6453748362699369, 0.0, 1.0, 1.0, 0.2860044134350827], 
reward next is 0.7140, 
noisyNet noise sample is [array([0.05628409], dtype=float32), -1.3782258]. 
=============================================
[2019-04-08 15:23:44,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.10448034e-16 1.99921524e-09 1.81542782e-05 4.01721882e-05
 1.30577091e-10 1.91673587e-12 1.14090916e-07 3.52384192e-13
 5.23054467e-18 1.25230040e-22 9.99941587e-01], sum to 1.0000
[2019-04-08 15:23:44,495] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2352
[2019-04-08 15:23:44,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3093410e-17 3.5845335e-10 1.3406450e-06 5.1472784e-07 4.7923674e-09
 1.6996640e-12 8.8938356e-07 8.6073801e-14 8.8160155e-19 3.4688590e-21
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:23:44,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3567
[2019-04-08 15:23:44,518] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 63.5, 49.66666666666667, 31.0, 22.5, 27.05804797793048, 0.768512510400687, 1.0, 1.0, 65.0, 44248.80098615105], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 143400.0000, 
sim time next is 144000.0000, 
raw observation next is [-6.7, 64.0, 44.0, 24.0, 22.5, 27.03584531117841, 0.7704620437514124, 1.0, 1.0, 65.0, 44501.83182993845], 
processed observation next is [1.0, 0.6956521739130435, 0.2770083102493075, 0.64, 0.14666666666666667, 0.026519337016574586, 0.375, 0.7529871092648674, 0.7568206812504709, 1.0, 1.0, 1.0, 0.2119134849044688], 
reward next is 0.7881, 
noisyNet noise sample is [array([0.2635755], dtype=float32), 0.4763994]. 
=============================================
[2019-04-08 15:23:44,543] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.916666666666667, 65.0, 137.0, 0.0, 22.5, 26.53429920467299, 0.5902083671327562, 1.0, 1.0, 65.0, 48797.59002771818], 
current ob forecast is [], 
actual action is [0.08333333333333304, 65.0], 
sim time this is 216600.0000, 
sim time next is 217200.0000, 
raw observation next is [-4.833333333333334, 65.0, 133.0, 0.0, 22.5, 26.5891170074153, 0.5877372560035922, 1.0, 1.0, 65.0, 46948.82440949326], 
processed observation next is [1.0, 0.5217391304347826, 0.32871652816251157, 0.65, 0.44333333333333336, 0.0, 0.375, 0.7157597506179417, 0.6959124186678641, 1.0, 1.0, 1.0, 0.22356583052139647], 
reward next is 0.7764, 
noisyNet noise sample is [array([-1.790061], dtype=float32), 2.222447]. 
=============================================
[2019-04-08 15:23:44,547] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[86.35828]
 [86.66809]
 [87.01094]
 [87.41762]
 [87.83061]], R is [[86.00362396]
 [85.93287659]
 [85.85777283]
 [85.79016113]
 [85.7301712 ]].
[2019-04-08 15:23:44,679] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5556945e-15 5.3695693e-08 1.8092993e-05 4.3236982e-06 5.8766636e-10
 1.0729009e-11 3.8024513e-07 6.7479607e-12 7.4868508e-17 1.5996153e-18
 9.9997711e-01], sum to 1.0000
[2019-04-08 15:23:44,683] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2611
[2019-04-08 15:23:44,702] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 25.59251273222337, 0.4633395865550949, 0.0, 1.0, 65.0, 60551.45385758572], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 168600.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 25.57214020381637, 0.4590789456294535, 0.0, 1.0, 65.0, 60525.80902745317], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6310116836513643, 0.6530263152098178, 0.0, 1.0, 1.0, 0.2882181382259675], 
reward next is 0.7118, 
noisyNet noise sample is [array([-0.02434182], dtype=float32), -1.7986388]. 
=============================================
[2019-04-08 15:23:45,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9894802e-16 2.0780302e-08 1.6097294e-05 2.1790099e-04 1.3490050e-09
 2.9584390e-11 3.9460548e-07 4.5346678e-14 5.3745561e-17 2.5841241e-19
 9.9976557e-01], sum to 1.0000
[2019-04-08 15:23:45,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9301
[2019-04-08 15:23:45,097] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.483333333333334, 78.0, 49.0, 197.3333333333333, 22.5, 25.79665608016836, 0.4289217802589306, 1.0, 1.0, 65.0, 54457.62028176462], 
current ob forecast is [], 
actual action is [-3.4833333333333343, 65.0], 
sim time this is 204600.0000, 
sim time next is 205200.0000, 
raw observation next is [-8.4, 78.0, 56.5, 148.0, 22.5, 25.81378426888568, 0.4434165803240331, 1.0, 1.0, 65.0, 56020.46735164755], 
processed observation next is [1.0, 0.391304347826087, 0.2299168975069252, 0.78, 0.18833333333333332, 0.16353591160220995, 0.375, 0.6511486890738066, 0.6478055267746777, 1.0, 1.0, 1.0, 0.2667641302459407], 
reward next is 0.7332, 
noisyNet noise sample is [array([1.9279171], dtype=float32), 1.6801515]. 
=============================================
[2019-04-08 15:23:45,569] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8790877e-14 3.7603836e-08 1.2052465e-03 4.4711443e-04 5.8585673e-08
 6.0982758e-10 2.1689244e-05 6.8600134e-12 2.3476878e-15 6.3597703e-18
 9.9832588e-01], sum to 1.0000
[2019-04-08 15:23:45,572] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8456
[2019-04-08 15:23:45,592] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 76.66666666666667, 0.0, 0.0, 19.0, 25.64718562524606, 0.410582364343262, 0.0, 1.0, 65.0, 57228.51462939465], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 182400.0000, 
sim time next is 183000.0000, 
raw observation next is [-8.9, 77.33333333333333, 0.0, 0.0, 19.0, 25.55352752779078, 0.4141932922469622, 0.0, 1.0, 65.0, 60310.45164515602], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.7733333333333333, 0.0, 0.0, 0.08333333333333333, 0.6294606273158984, 0.6380644307489874, 0.0, 1.0, 1.0, 0.28719262688169533], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.8307082], dtype=float32), -0.029141279]. 
=============================================
[2019-04-08 15:23:45,599] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[72.36557 ]
 [72.183655]
 [71.84286 ]
 [71.88425 ]
 [71.76625 ]], R is [[72.55876923]
 [72.56066895]
 [72.5655365 ]
 [72.55576324]
 [72.54830933]].
[2019-04-08 15:23:46,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6542699e-15 8.2471779e-08 3.5645200e-05 9.3773509e-05 2.1477917e-09
 1.8059141e-11 1.1096385e-05 3.6944048e-13 6.8411119e-17 6.9085556e-19
 9.9985933e-01], sum to 1.0000
[2019-04-08 15:23:46,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2415
[2019-04-08 15:23:46,356] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.916666666666667, 65.0, 137.0, 0.0, 22.5, 26.53423575573872, 0.5901794026239616, 1.0, 1.0, 65.0, 48798.30794893228], 
current ob forecast is [], 
actual action is [0.08333333333333304, 65.0], 
sim time this is 216600.0000, 
sim time next is 217200.0000, 
raw observation next is [-4.833333333333334, 65.0, 133.0, 0.0, 22.5, 26.58905433356109, 0.5877085730871744, 1.0, 1.0, 65.0, 46949.53074790509], 
processed observation next is [1.0, 0.5217391304347826, 0.32871652816251157, 0.65, 0.44333333333333336, 0.0, 0.375, 0.7157545277967575, 0.6959028576957248, 1.0, 1.0, 1.0, 0.22356919403764328], 
reward next is 0.7764, 
noisyNet noise sample is [array([0.30875894], dtype=float32), -0.5359737]. 
=============================================
[2019-04-08 15:23:47,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9310564e-17 9.7282925e-11 1.5362384e-06 1.8647817e-05 6.9514000e-10
 6.7130719e-12 3.6190852e-07 4.3597534e-13 3.3608098e-18 6.6685317e-20
 9.9997950e-01], sum to 1.0000
[2019-04-08 15:23:47,147] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1059
[2019-04-08 15:23:47,161] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 26.50689853368301, 0.5880795609006584, 1.0, 1.0, 65.0, 51923.16606841882], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 238800.0000, 
sim time next is 239400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 26.44933257499191, 0.5833291222859135, 1.0, 1.0, 65.0, 52663.7207241169], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.7041110479159925, 0.6944430407619712, 1.0, 1.0, 1.0, 0.2507796224957948], 
reward next is 0.7492, 
noisyNet noise sample is [array([1.2036276], dtype=float32), 0.0141881]. 
=============================================
[2019-04-08 15:23:47,316] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.7633034e-13 1.1742901e-06 2.0335603e-03 3.0684858e-03 3.8853841e-06
 1.0443192e-09 6.8163895e-04 2.0762854e-10 1.8164070e-13 1.2453612e-15
 9.9421126e-01], sum to 1.0000
[2019-04-08 15:23:47,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7558
[2019-04-08 15:23:47,338] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 24.8100943378437, 0.2257904754057944, 0.0, 1.0, 65.0, 65744.35036731443], 
current ob forecast is [], 
actual action is [-10.0, 65.0], 
sim time this is 354600.0000, 
sim time next is 355200.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 24.79082347343801, 0.2131652261416165, 0.0, 1.0, 65.0, 65836.21261662118], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5659019561198342, 0.5710550753805389, 0.0, 1.0, 1.0, 0.31350577436486277], 
reward next is 0.6865, 
noisyNet noise sample is [array([-0.5052897], dtype=float32), -0.5898517]. 
=============================================
[2019-04-08 15:23:47,352] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7403928e-12 6.3646844e-08 1.4211192e-04 2.7578796e-04 8.1309814e-08
 1.3269404e-09 3.7921185e-05 2.0080861e-10 3.1755022e-15 1.5070262e-16
 9.9954396e-01], sum to 1.0000
[2019-04-08 15:23:47,354] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3311
[2019-04-08 15:23:47,372] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 19.0, 25.23239783663711, 0.3395887860200795, 0.0, 1.0, 65.0, 60195.65767072444], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 25.20303322209153, 0.3405899432125174, 0.0, 1.0, 65.0, 60529.74071919876], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6002527685076275, 0.6135299810708391, 0.0, 1.0, 1.0, 0.28823686056761316], 
reward next is 0.7118, 
noisyNet noise sample is [array([0.29397357], dtype=float32), 1.6327579]. 
=============================================
[2019-04-08 15:23:47,882] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7111253e-15 2.8805115e-08 3.2740893e-06 6.8429345e-06 1.2074517e-09
 1.3550914e-12 1.3405886e-06 5.1343812e-12 2.3938829e-16 9.8147312e-18
 9.9998856e-01], sum to 1.0000
[2019-04-08 15:23:47,886] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8773
[2019-04-08 15:23:47,910] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 22.5, 26.28548598862898, 0.5798069840040884, 1.0, 1.0, 65.0, 50982.36214612095], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 237600.0000, 
sim time next is 238200.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 26.39621287624012, 0.605733049706057, 1.0, 1.0, 65.0, 50252.9170056586], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.6996844063533434, 0.7019110165686856, 1.0, 1.0, 1.0, 0.23929960478885048], 
reward next is 0.7607, 
noisyNet noise sample is [array([0.29023433], dtype=float32), -0.9633542]. 
=============================================
[2019-04-08 15:23:48,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2455243e-15 1.0837820e-09 2.5197847e-05 1.4644354e-05 2.6489652e-10
 4.3643724e-11 4.6851741e-07 1.8175461e-13 6.7155088e-17 2.5193461e-19
 9.9995971e-01], sum to 1.0000
[2019-04-08 15:23:48,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5078
[2019-04-08 15:23:48,393] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 76.16666666666667, 0.0, 0.0, 19.0, 25.8517748735825, 0.5063992815020506, 0.0, 1.0, 65.0, 58325.82107144898], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 252600.0000, 
sim time next is 253200.0000, 
raw observation next is [-3.9, 77.33333333333334, 0.0, 0.0, 19.0, 25.843756817406, 0.5040449642139814, 0.0, 1.0, 65.0, 58362.99125518952], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.6536464014505, 0.6680149880713272, 0.0, 1.0, 1.0, 0.27791900597709296], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.07134204], dtype=float32), 0.28928986]. 
=============================================
[2019-04-08 15:23:48,737] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.40059112e-13 2.96903721e-07 7.61544288e-05 1.33558102e-02
 2.27678818e-08 1.49421239e-10 1.04193205e-05 1.04805325e-12
 4.57467291e-15 4.17901572e-18 9.86557305e-01], sum to 1.0000
[2019-04-08 15:23:48,738] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5078
[2019-04-08 15:23:48,758] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 65.0, 139.0, 0.0, 22.5, 26.53951370295228, 0.6015222851507315, 1.0, 1.0, 65.0, 48452.85005453761], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 219600.0000, 
sim time next is 220200.0000, 
raw observation next is [-4.316666666666666, 64.5, 142.3333333333333, 0.0, 22.5, 26.57534351607431, 0.6032822023555258, 1.0, 1.0, 65.0, 48120.21941229458], 
processed observation next is [1.0, 0.5652173913043478, 0.34302862419205915, 0.645, 0.4744444444444443, 0.0, 0.375, 0.7146119596728591, 0.7010940674518419, 1.0, 1.0, 1.0, 0.22914390196330753], 
reward next is 0.7709, 
noisyNet noise sample is [array([1.4214336], dtype=float32), -0.09558124]. 
=============================================
[2019-04-08 15:23:48,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1564641e-14 9.6663717e-09 1.4348002e-06 2.1584244e-06 2.6497202e-09
 4.2493886e-12 1.6647839e-07 6.3644307e-12 6.6746858e-17 9.6154607e-19
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:23:48,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1404
[2019-04-08 15:23:48,968] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 19.0, 25.89045630261876, 0.4880127004375588, 0.0, 1.0, 65.0, 58602.81126491673], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 262800.0000, 
sim time next is 263400.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 19.0, 25.88278974730895, 0.483895543831222, 0.0, 1.0, 65.0, 59270.74801580373], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6568991456090792, 0.6612985146104073, 0.0, 1.0, 1.0, 0.282241657218113], 
reward next is 0.7178, 
noisyNet noise sample is [array([0.8021734], dtype=float32), 0.30589005]. 
=============================================
[2019-04-08 15:23:49,010] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.3796571e-15 5.6124055e-08 1.8269317e-05 8.9574482e-05 3.9738275e-09
 1.6858159e-11 4.3432055e-06 4.4004817e-12 4.6837518e-16 5.1457781e-18
 9.9988770e-01], sum to 1.0000
[2019-04-08 15:23:49,012] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7732
[2019-04-08 15:23:49,035] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.9, 59.5, 76.33333333333333, 0.0, 22.5, 26.85645978607474, 0.65839951565223, 1.0, 1.0, 65.0, 44650.53411845345], 
current ob forecast is [], 
actual action is [2.1, 65.0], 
sim time this is 227400.0000, 
sim time next is 228000.0000, 
raw observation next is [-3.0, 60.0, 66.16666666666667, 0.0, 22.5, 26.87815753139758, 0.6599497598049799, 1.0, 1.0, 65.0, 43499.38260242423], 
processed observation next is [1.0, 0.6521739130434783, 0.3795013850415513, 0.6, 0.22055555555555556, 0.0, 0.375, 0.7398464609497983, 0.7199832532683267, 1.0, 1.0, 1.0, 0.20713991715440108], 
reward next is 0.7929, 
noisyNet noise sample is [array([-0.57583743], dtype=float32), 1.2400225]. 
=============================================
[2019-04-08 15:23:49,046] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.09455 ]
 [78.273636]
 [78.448456]
 [78.586815]
 [78.66765 ]], R is [[77.91860962]
 [77.92680359]
 [77.94307709]
 [77.96691895]
 [77.99425507]].
[2019-04-08 15:23:49,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1523751e-14 1.1787040e-06 4.8294081e-05 1.9703572e-05 4.3010284e-09
 8.3007580e-11 2.9152984e-06 4.5985086e-12 4.0146804e-15 3.9084911e-16
 9.9992788e-01], sum to 1.0000
[2019-04-08 15:23:49,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4219
[2019-04-08 15:23:49,917] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.4, 69.5, 0.0, 0.0, 19.0, 25.59643717259231, 0.4219779977192529, 0.0, 1.0, 65.0, 61065.41945368107], 
current ob forecast is [], 
actual action is [-4.4, 65.0], 
sim time this is 273000.0000, 
sim time next is 273600.0000, 
raw observation next is [-9.5, 70.0, 0.0, 0.0, 19.0, 25.56427107225817, 0.4140624026288597, 0.0, 1.0, 65.0, 61864.68344403668], 
processed observation next is [1.0, 0.17391304347826086, 0.1994459833795014, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6303559226881807, 0.6380208008762865, 0.0, 1.0, 1.0, 0.29459373068588895], 
reward next is 0.7054, 
noisyNet noise sample is [array([-0.20586179], dtype=float32), -0.6768763]. 
=============================================
[2019-04-08 15:23:50,156] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9531863e-12 8.3717950e-08 1.0117473e-04 3.9082312e-04 5.4741292e-08
 9.8905062e-10 3.3871274e-06 1.6815639e-11 3.3992340e-13 5.4807616e-16
 9.9950445e-01], sum to 1.0000
[2019-04-08 15:23:50,159] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4864
[2019-04-08 15:23:50,178] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.633333333333333, 67.66666666666667, 0.0, 0.0, 19.0, 25.72787529694635, 0.456642611640969, 0.0, 1.0, 65.0, 60778.98523183892], 
current ob forecast is [], 
actual action is [-3.633333333333333, 65.0], 
sim time this is 269400.0000, 
sim time next is 270000.0000, 
raw observation next is [-8.9, 67.0, 0.0, 0.0, 19.0, 25.74067202366001, 0.4446593730508034, 0.0, 1.0, 65.0, 60468.07169320065], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.67, 0.0, 0.0, 0.08333333333333333, 0.6450560019716676, 0.6482197910169344, 0.0, 1.0, 1.0, 0.2879431985390507], 
reward next is 0.7121, 
noisyNet noise sample is [array([-0.10808556], dtype=float32), 0.31571573]. 
=============================================
[2019-04-08 15:23:50,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.09077 ]
 [68.091606]
 [68.10794 ]
 [67.95032 ]
 [68.14198 ]], R is [[68.01815796]
 [68.04855347]
 [68.08498383]
 [68.11738586]
 [68.14754486]].
[2019-04-08 15:23:50,345] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4056341e-13 1.0687028e-07 7.4442418e-05 4.0033236e-04 2.8120047e-07
 3.4788306e-10 1.3983849e-05 3.6770607e-11 7.1622348e-14 4.3057007e-17
 9.9951088e-01], sum to 1.0000
[2019-04-08 15:23:50,345] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2586
[2019-04-08 15:23:50,381] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.8, 69.5, 0.0, 0.0, 19.0, 25.22746203726076, 0.3465942836309213, 0.0, 1.0, 65.0, 64272.63426237371], 
current ob forecast is [], 
actual action is [-6.800000000000001, 65.0], 
sim time this is 281400.0000, 
sim time next is 282000.0000, 
raw observation next is [-11.9, 69.0, 0.0, 0.0, 19.0, 25.17547915987722, 0.340682542934553, 0.0, 1.0, 65.0, 64548.89871682475], 
processed observation next is [1.0, 0.2608695652173913, 0.13296398891966757, 0.69, 0.0, 0.0, 0.08333333333333333, 0.597956596656435, 0.613560847644851, 0.0, 1.0, 1.0, 0.3073757081753559], 
reward next is 0.6926, 
noisyNet noise sample is [array([0.8633354], dtype=float32), 0.35881877]. 
=============================================
[2019-04-08 15:23:50,388] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[66.68209 ]
 [66.75418 ]
 [66.8244  ]
 [66.903244]
 [66.9847  ]], R is [[66.66421509]
 [66.69151306]
 [66.71723938]
 [66.74449921]
 [66.77276611]].
[2019-04-08 15:23:50,388] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6949212e-14 3.2855880e-08 1.3826498e-05 2.6122126e-04 8.5379268e-09
 1.8463472e-09 3.2757438e-05 6.3188014e-12 1.1337087e-13 5.4567103e-17
 9.9969220e-01], sum to 1.0000
[2019-04-08 15:23:50,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1548
[2019-04-08 15:23:50,414] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.4, 69.5, 0.0, 0.0, 19.0, 25.59667051344228, 0.4220237053765485, 0.0, 1.0, 65.0, 61064.56509556761], 
current ob forecast is [], 
actual action is [-4.4, 65.0], 
sim time this is 273000.0000, 
sim time next is 273600.0000, 
raw observation next is [-9.5, 70.0, 0.0, 0.0, 19.0, 25.5645043894071, 0.414108079295415, 0.0, 1.0, 65.0, 61863.83029644001], 
processed observation next is [1.0, 0.17391304347826086, 0.1994459833795014, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6303753657839252, 0.638036026431805, 0.0, 1.0, 1.0, 0.2945896680782858], 
reward next is 0.7054, 
noisyNet noise sample is [array([-1.2917693], dtype=float32), -0.39996195]. 
=============================================
[2019-04-08 15:23:50,694] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5534611e-12 2.1285130e-07 8.4699313e-05 2.6621707e-04 1.2877744e-07
 2.2167919e-08 2.1724512e-05 1.4448000e-10 2.5690940e-14 6.7858594e-16
 9.9962699e-01], sum to 1.0000
[2019-04-08 15:23:50,694] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1098
[2019-04-08 15:23:50,712] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.41666666666667, 67.5, 0.0, 0.0, 19.0, 25.35759789415252, 0.3820715218300827, 0.0, 1.0, 65.0, 64334.08406579253], 
current ob forecast is [], 
actual action is [-5.41666666666667, 65.0], 
sim time this is 276600.0000, 
sim time next is 277200.0000, 
raw observation next is [-10.6, 67.0, 0.0, 0.0, 19.0, 25.32494113075914, 0.3783073096436755, 0.0, 1.0, 65.0, 64441.32613447921], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.67, 0.0, 0.0, 0.08333333333333333, 0.610411760896595, 0.6261024365478919, 0.0, 1.0, 1.0, 0.3068634577832343], 
reward next is 0.6931, 
noisyNet noise sample is [array([1.2157148], dtype=float32), -0.6399077]. 
=============================================
[2019-04-08 15:23:51,115] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2853387e-15 2.6455480e-08 2.2746937e-05 6.4623302e-05 7.4089983e-09
 1.5125378e-11 1.2518404e-06 4.9363318e-13 2.1961869e-16 1.0340188e-17
 9.9991131e-01], sum to 1.0000
[2019-04-08 15:23:51,115] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3284
[2019-04-08 15:23:51,154] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.7, 63.0, 91.0, 447.5, 22.5, 26.28005249781128, 0.5407665753013174, 1.0, 1.0, 65.0, 50173.34667282641], 
current ob forecast is [], 
actual action is [-6.699999999999999, 65.0], 
sim time this is 295200.0000, 
sim time next is 295800.0000, 
raw observation next is [-11.51666666666667, 62.5, 89.66666666666667, 469.0, 22.5, 26.32666366290895, 0.5529700897718643, 1.0, 1.0, 65.0, 49544.93633421732], 
processed observation next is [1.0, 0.43478260869565216, 0.14358264081255764, 0.625, 0.2988888888888889, 0.518232044198895, 0.375, 0.6938886385757458, 0.6843233632572882, 1.0, 1.0, 1.0, 0.2359282682581777], 
reward next is 0.7641, 
noisyNet noise sample is [array([-0.6144074], dtype=float32), 0.021162437]. 
=============================================
[2019-04-08 15:23:51,265] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2769616e-12 2.0476463e-07 2.2060133e-04 9.7164605e-04 3.1969620e-07
 3.0071440e-08 9.2419823e-06 2.1876684e-10 3.0837315e-13 3.8061536e-15
 9.9879789e-01], sum to 1.0000
[2019-04-08 15:23:51,268] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4692
[2019-04-08 15:23:51,291] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.1, 67.0, 0.0, 0.0, 19.0, 24.8966718522285, 0.2689158502261174, 0.0, 1.0, 65.0, 64285.44289333071], 
current ob forecast is [], 
actual action is [-9.1, 65.0], 
sim time this is 346800.0000, 
sim time next is 347400.0000, 
raw observation next is [-14.2, 67.5, 0.0, 0.0, 19.0, 24.86082831234149, 0.2605548401514527, 0.0, 1.0, 65.0, 64333.77552588737], 
processed observation next is [1.0, 0.0, 0.06925207756232687, 0.675, 0.0, 0.0, 0.08333333333333333, 0.5717356926951241, 0.5868516133838176, 0.0, 1.0, 1.0, 0.3063513120280351], 
reward next is 0.6936, 
noisyNet noise sample is [array([2.703732], dtype=float32), 0.50122255]. 
=============================================
[2019-04-08 15:23:51,297] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3601745e-15 1.5128570e-08 3.7956972e-05 4.8884140e-06 2.4070896e-08
 9.0187002e-10 9.7742313e-06 7.9463882e-13 1.2313734e-16 6.7572086e-17
 9.9994731e-01], sum to 1.0000
[2019-04-08 15:23:51,297] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9785
[2019-04-08 15:23:51,316] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.8, 70.0, 15.0, 205.5, 22.5, 25.13843864490848, 0.3359561311942967, 1.0, 1.0, 65.0, 62327.3187911783], 
current ob forecast is [], 
actual action is [-7.800000000000001, 65.0], 
sim time this is 288000.0000, 
sim time next is 288600.0000, 
raw observation next is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 22.5, 25.15044554987573, 0.3579704386347689, 1.0, 1.0, 65.0, 62639.24645419185], 
processed observation next is [1.0, 0.34782608695652173, 0.1103416435826407, 0.695, 0.06666666666666667, 0.29355432780847146, 0.375, 0.5958704624896441, 0.619323479544923, 1.0, 1.0, 1.0, 0.29828212597234216], 
reward next is 0.7017, 
noisyNet noise sample is [array([-0.0954087], dtype=float32), -0.19693731]. 
=============================================
[2019-04-08 15:23:51,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8237229e-14 2.4973716e-07 8.4804255e-05 1.2264251e-04 1.7177323e-08
 8.4219076e-10 1.0495618e-05 1.9726689e-11 9.1216585e-13 1.0553110e-16
 9.9978179e-01], sum to 1.0000
[2019-04-08 15:23:51,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3582
[2019-04-08 15:23:51,389] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.9, 77.83333333333334, 0.0, 0.0, 19.0, 25.3443391733899, 0.3876296363256826, 0.0, 1.0, 65.0, 64740.18196841035], 
current ob forecast is [], 
actual action is [-7.9, 65.0], 
sim time this is 335400.0000, 
sim time next is 336000.0000, 
raw observation next is [-13.0, 78.66666666666667, 0.0, 0.0, 19.0, 25.31433072250682, 0.381691948703794, 0.0, 1.0, 65.0, 64901.09148762017], 
processed observation next is [1.0, 0.9130434782608695, 0.10249307479224376, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.6095275602089018, 0.6272306495679313, 0.0, 1.0, 1.0, 0.3090528166077151], 
reward next is 0.6909, 
noisyNet noise sample is [array([-0.5911361], dtype=float32), 0.65777475]. 
=============================================
[2019-04-08 15:23:51,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.45482 ]
 [64.56183 ]
 [64.934944]
 [65.28755 ]
 [66.105225]], R is [[64.52812195]
 [64.57455444]
 [64.62225342]
 [64.66959381]
 [64.71615601]].
[2019-04-08 15:23:51,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.09388941e-12 5.36770131e-08 1.50954238e-05 1.99346949e-04
 1.00623625e-08 5.24264354e-10 1.84949204e-06 1.15528385e-10
 1.20138740e-14 1.84990175e-15 9.99783695e-01], sum to 1.0000
[2019-04-08 15:23:51,859] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1677
[2019-04-08 15:23:51,878] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 25.13950707832959, 0.32071319408697, 0.0, 1.0, 65.0, 61664.51195221766], 
current ob forecast is [], 
actual action is [-6.699999999999999, 65.0], 
sim time this is 428400.0000, 
sim time next is 429000.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 25.11190481946937, 0.3138587016469718, 0.0, 1.0, 65.0, 61500.00351704411], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.5926587349557808, 0.6046195672156572, 0.0, 1.0, 1.0, 0.29285715960497194], 
reward next is 0.7071, 
noisyNet noise sample is [array([-0.36788535], dtype=float32), 1.2723205]. 
=============================================
[2019-04-08 15:23:51,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0285113e-12 2.1687131e-06 1.4889244e-03 1.8916429e-04 3.0181482e-07
 3.6096658e-08 2.2699140e-05 5.8732252e-10 4.2996119e-12 2.0374149e-15
 9.9829668e-01], sum to 1.0000
[2019-04-08 15:23:51,900] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0207
[2019-04-08 15:23:51,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[60.608883]
 [60.332184]
 [60.169407]
 [59.958786]
 [60.06552 ]], R is [[61.01360321]
 [61.10982895]
 [61.20536804]
 [61.30080032]
 [61.39505768]].
[2019-04-08 15:23:51,919] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 19.0, 25.00100568912029, 0.2980168192900456, 0.0, 1.0, 65.0, 64151.76836422951], 
current ob forecast is [], 
actual action is [-8.9, 65.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 19.0, 24.96609050946184, 0.299385476439377, 0.0, 1.0, 65.0, 64161.50206133306], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.08333333333333333, 0.5805075424551532, 0.5997951588131257, 0.0, 1.0, 1.0, 0.30553096219682413], 
reward next is 0.6945, 
noisyNet noise sample is [array([1.2857594], dtype=float32), 0.80164355]. 
=============================================
[2019-04-08 15:23:52,431] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6072401e-15 9.1270689e-09 7.4314762e-06 2.1444146e-06 4.7880713e-09
 1.2648413e-10 1.0831667e-06 8.4929649e-13 8.0551187e-16 5.8775002e-20
 9.9998927e-01], sum to 1.0000
[2019-04-08 15:23:52,439] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8264
[2019-04-08 15:23:52,461] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 44.0, 95.0, 631.0, 22.5, 26.59106004124665, 0.6947779497683358, 1.0, 1.0, 65.0, 60243.77476057323], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 307800.0000, 
sim time next is 308400.0000, 
raw observation next is [-9.5, 44.0, 92.83333333333334, 629.6666666666667, 22.5, 26.28097380560818, 0.6521150898479459, 1.0, 1.0, 64.99999999999994, 52004.8271925623], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.30944444444444447, 0.6957642725598527, 0.375, 0.6900811504673484, 0.7173716966159819, 1.0, 1.0, 0.9999999999999989, 0.24764203425029666], 
reward next is 0.7524, 
noisyNet noise sample is [array([-0.04085605], dtype=float32), -0.31207862]. 
=============================================
[2019-04-08 15:23:52,517] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6706195e-13 1.2695236e-08 1.5537056e-05 9.7901793e-04 9.6951652e-08
 2.1291835e-09 2.1931405e-06 8.3062481e-12 2.1888117e-13 1.2926828e-15
 9.9900311e-01], sum to 1.0000
[2019-04-08 15:23:52,517] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6050
[2019-04-08 15:23:52,551] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.0, 78.66666666666667, 0.0, 0.0, 19.0, 25.3166244345095, 0.3822771315033183, 0.0, 1.0, 65.0, 64897.48831456619], 
current ob forecast is [], 
actual action is [-8.0, 65.0], 
sim time this is 336000.0000, 
sim time next is 336600.0000, 
raw observation next is [-13.1, 79.5, 0.0, 0.0, 19.0, 25.28887530769339, 0.375880799994388, 0.0, 1.0, 65.0, 64831.4845926181], 
processed observation next is [1.0, 0.9130434782608695, 0.0997229916897507, 0.795, 0.0, 0.0, 0.08333333333333333, 0.6074062756411159, 0.6252935999981294, 0.0, 1.0, 1.0, 0.30872135520294336], 
reward next is 0.6913, 
noisyNet noise sample is [array([-0.2467847], dtype=float32), -1.1224439]. 
=============================================
[2019-04-08 15:23:52,661] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4236635e-15 3.1622884e-09 2.3434102e-06 8.1819189e-06 8.8628499e-10
 3.6682610e-10 7.7171563e-07 4.9372225e-13 7.9509193e-16 1.3595669e-17
 9.9998868e-01], sum to 1.0000
[2019-04-08 15:23:52,670] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4538
[2019-04-08 15:23:52,690] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.41666666666667, 47.83333333333334, 16.0, 162.0, 22.5, 26.16248427881737, 0.6156121873762811, 1.0, 1.0, 65.0, 57392.13892060205], 
current ob forecast is [], 
actual action is [-5.41666666666667, 65.0], 
sim time this is 319800.0000, 
sim time next is 320400.0000, 
raw observation next is [-10.6, 49.0, 12.0, 123.0, 22.5, 26.67552619358292, 0.645809610667786, 1.0, 1.0, 65.0, 46580.01536635861], 
processed observation next is [1.0, 0.7391304347826086, 0.1689750692520776, 0.49, 0.04, 0.13591160220994475, 0.375, 0.7229605161319101, 0.7152698702225954, 1.0, 1.0, 1.0, 0.22180959698266003], 
reward next is 0.7782, 
noisyNet noise sample is [array([-0.34589392], dtype=float32), 0.15178505]. 
=============================================
[2019-04-08 15:23:53,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5855294e-12 1.8979684e-07 1.6187796e-04 2.5159097e-04 9.6791304e-08
 4.5162087e-09 2.8151213e-05 7.2312079e-10 2.2171269e-14 7.9736158e-15
 9.9955803e-01], sum to 1.0000
[2019-04-08 15:23:53,374] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9809
[2019-04-08 15:23:53,417] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 24.33530938577388, 0.1291108733406176, 0.0, 1.0, 65.0, 65675.45132572249], 
current ob forecast is [], 
actual action is [-10.6, 65.0], 
sim time this is 362400.0000, 
sim time next is 363000.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 24.33008003401091, 0.1458831874470958, 0.0, 1.0, 65.0, 66109.37163820499], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.5275066695009091, 0.5486277291490319, 0.0, 1.0, 1.0, 0.31480653161049993], 
reward next is 0.6852, 
noisyNet noise sample is [array([-0.34053585], dtype=float32), -1.6352707]. 
=============================================
[2019-04-08 15:23:53,431] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[64.5417 ]
 [64.56629]
 [64.63113]
 [64.6471 ]
 [64.67951]], R is [[64.53411102]
 [64.57603455]
 [64.61680603]
 [64.65674591]
 [64.69606018]].
[2019-04-08 15:23:53,697] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2517307e-12 3.1993848e-06 3.0323500e-03 1.5980306e-03 8.5505292e-07
 5.8783627e-09 6.5025859e-05 7.0012590e-10 1.7052352e-12 1.0689840e-14
 9.9530053e-01], sum to 1.0000
[2019-04-08 15:23:53,698] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0557
[2019-04-08 15:23:53,733] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.1, 41.5, 0.0, 0.0, 22.5, 24.87654806966547, 0.2074085403661588, 1.0, 1.0, 65.0, 61865.34932797399], 
current ob forecast is [], 
actual action is [-3.0999999999999996, 65.0], 
sim time this is 459000.0000, 
sim time next is 459600.0000, 
raw observation next is [-8.0, 41.0, 0.0, 0.0, 22.5, 24.84982789099305, 0.2119819656945897, 1.0, 1.0, 65.0, 61974.21090080788], 
processed observation next is [1.0, 0.30434782608695654, 0.24099722991689754, 0.41, 0.0, 0.0, 0.375, 0.5708189909160876, 0.57066065523153, 1.0, 1.0, 1.0, 0.295115290003847], 
reward next is 0.7049, 
noisyNet noise sample is [array([-0.29865333], dtype=float32), 0.13813579]. 
=============================================
[2019-04-08 15:23:53,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6464787e-13 2.2211376e-08 1.5541085e-04 2.7748451e-04 7.7107470e-10
 3.6135803e-11 1.2830507e-05 2.8991260e-12 5.5602594e-16 2.8981345e-17
 9.9955422e-01], sum to 1.0000
[2019-04-08 15:23:53,918] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9235
[2019-04-08 15:23:53,968] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 42.0, 70.0, 477.0, 22.5, 26.20822272745151, 0.6772343903854182, 1.0, 1.0, 65.0, 68387.48081235282], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 315000.0000, 
sim time next is 315600.0000, 
raw observation next is [-9.5, 42.0, 62.33333333333334, 437.5, 22.5, 25.5911908012, 0.5904772517346245, 1.0, 1.0, 65.0, 49250.60263854366], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.2077777777777778, 0.48342541436464087, 0.375, 0.6325992334333334, 0.6968257505782082, 1.0, 1.0, 1.0, 0.2345266792311603], 
reward next is 0.7655, 
noisyNet noise sample is [array([0.5420861], dtype=float32), 0.23874827]. 
=============================================
[2019-04-08 15:23:53,983] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1532163e-14 3.1681548e-08 4.6837613e-06 2.8143284e-05 1.3769681e-08
 7.0181708e-11 4.9920493e-07 3.5426009e-11 2.4937448e-14 5.9807587e-17
 9.9996662e-01], sum to 1.0000
[2019-04-08 15:23:53,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-08 15:23:54,004] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 33.0, 42.5, 0.0, 22.5, 25.63270749182033, 0.3408109733588631, 1.0, 1.0, 65.0, 56742.97013728349], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 65.0], 
sim time this is 464400.0000, 
sim time next is 465000.0000, 
raw observation next is [-5.916666666666667, 32.83333333333334, 49.0, 0.0, 22.5, 25.77888436173263, 0.3489335242648828, 1.0, 1.0, 65.0, 55007.61301967881], 
processed observation next is [1.0, 0.391304347826087, 0.2987072945521699, 0.3283333333333334, 0.16333333333333333, 0.0, 0.375, 0.6482403634777191, 0.6163111747549609, 1.0, 1.0, 1.0, 0.2619410143794229], 
reward next is 0.7381, 
noisyNet noise sample is [array([-0.11396232], dtype=float32), 1.7529708]. 
=============================================
[2019-04-08 15:23:54,008] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.106346]
 [67.24726 ]
 [66.61692 ]
 [66.53094 ]
 [66.200325]], R is [[67.86985779]
 [67.92095947]
 [67.97159576]
 [68.02658081]
 [68.07705688]].
[2019-04-08 15:23:54,027] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.5839164e-13 2.2104740e-07 4.0292330e-04 3.5941676e-04 6.8036918e-08
 2.4621125e-09 6.8209316e-05 6.4591901e-11 1.5805491e-14 8.8689479e-16
 9.9916923e-01], sum to 1.0000
[2019-04-08 15:23:54,029] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8890
[2019-04-08 15:23:54,050] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.71666666666667, 68.83333333333333, 0.0, 0.0, 22.5, 25.64492487817192, 0.4469726348915393, 1.0, 1.0, 65.0, 63323.38933617045], 
current ob forecast is [], 
actual action is [-7.71666666666667, 65.0], 
sim time this is 330600.0000, 
sim time next is 331200.0000, 
raw observation next is [-12.8, 70.0, 0.0, 0.0, 22.5, 25.60124147620911, 0.4367051478002795, 0.0, 1.0, 65.0, 63304.35574192894], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7, 0.0, 0.0, 0.375, 0.6334367896840926, 0.6455683826000932, 0.0, 1.0, 1.0, 0.3014493130568045], 
reward next is 0.6986, 
noisyNet noise sample is [array([-0.48888195], dtype=float32), -0.19780503]. 
=============================================
[2019-04-08 15:23:55,053] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1905264e-12 1.9076944e-08 1.1600735e-05 3.1529915e-05 2.2514950e-08
 8.1769697e-10 5.6786471e-06 2.3629883e-11 2.1593374e-14 8.7381131e-16
 9.9995112e-01], sum to 1.0000
[2019-04-08 15:23:55,054] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1103
[2019-04-08 15:23:55,073] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.66666666666667, 69.0, 0.0, 0.0, 19.0, 24.88317888858737, 0.2670795447326084, 0.0, 1.0, 65.0, 64516.72157588548], 
current ob forecast is [], 
actual action is [-9.66666666666667, 65.0], 
sim time this is 350400.0000, 
sim time next is 351000.0000, 
raw observation next is [-14.75, 69.0, 0.0, 0.0, 19.0, 24.95303607247023, 0.2641578002767357, 0.0, 1.0, 65.0, 64406.30444242458], 
processed observation next is [1.0, 0.043478260869565216, 0.05401662049861495, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5794196727058525, 0.5880526000922452, 0.0, 1.0, 1.0, 0.30669668782106946], 
reward next is 0.6933, 
noisyNet noise sample is [array([-0.2726148], dtype=float32), 1.664936]. 
=============================================
[2019-04-08 15:23:55,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[63.014595]
 [63.136227]
 [63.345486]
 [63.54463 ]
 [63.567696]], R is [[63.20627594]
 [63.26699066]
 [63.32738876]
 [63.38733292]
 [63.44678497]].
[2019-04-08 15:23:55,193] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3208382e-13 9.7934496e-08 1.3654407e-04 1.4674732e-05 4.6962403e-09
 4.5085391e-10 2.5299153e-06 1.3054713e-11 1.0344105e-14 1.1108521e-16
 9.9984610e-01], sum to 1.0000
[2019-04-08 15:23:55,194] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3611
[2019-04-08 15:23:55,209] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.8, 72.33333333333334, 0.0, 0.0, 19.0, 25.49985000400738, 0.4203004673952527, 0.0, 1.0, 65.0, 64279.74198702955], 
current ob forecast is [], 
actual action is [-7.800000000000001, 65.0], 
sim time this is 332400.0000, 
sim time next is 333000.0000, 
raw observation next is [-12.8, 73.5, 0.0, 0.0, 19.0, 25.45668611186641, 0.4125913924557509, 0.0, 1.0, 65.0, 64341.80181182495], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.735, 0.0, 0.0, 0.08333333333333333, 0.6213905093222009, 0.6375304641519169, 0.0, 1.0, 1.0, 0.3063895324372617], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.14472982], dtype=float32), 0.0124798]. 
=============================================
[2019-04-08 15:23:55,214] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[67.81963 ]
 [68.741104]
 [68.92903 ]
 [69.324356]
 [69.33971 ]], R is [[67.8022995 ]
 [67.8181839 ]
 [67.83573914]
 [67.85592651]
 [67.87582397]].
[2019-04-08 15:23:55,778] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1272706e-12 6.2672420e-08 1.4656785e-04 7.3790760e-04 3.0458534e-07
 1.3448916e-09 2.5133425e-04 8.5295626e-10 1.9487789e-13 3.3461082e-15
 9.9886382e-01], sum to 1.0000
[2019-04-08 15:23:55,782] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1532
[2019-04-08 15:23:55,797] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.58333333333333, 69.0, 0.0, 0.0, 19.0, 24.86603980604297, 0.2596920878638285, 0.0, 1.0, 65.0, 64457.8817299795], 
current ob forecast is [], 
actual action is [-9.58333333333333, 65.0], 
sim time this is 349800.0000, 
sim time next is 350400.0000, 
raw observation next is [-14.66666666666667, 69.0, 0.0, 0.0, 19.0, 24.8819290237654, 0.2667630165766576, 0.0, 1.0, 65.0, 64518.11288829037], 
processed observation next is [1.0, 0.043478260869565216, 0.05632502308402576, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5734940853137834, 0.5889210055255525, 0.0, 1.0, 1.0, 0.3072291089918589], 
reward next is 0.6928, 
noisyNet noise sample is [array([-1.4650009], dtype=float32), -1.865616]. 
=============================================
[2019-04-08 15:23:56,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.1801377e-15 3.5173753e-09 5.1888492e-07 2.6147579e-06 1.1240877e-09
 5.3672683e-12 2.2560901e-08 1.1807871e-13 1.1670199e-16 5.9161230e-20
 9.9999678e-01], sum to 1.0000
[2019-04-08 15:23:56,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0923
[2019-04-08 15:23:56,985] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.3, 58.5, 62.33333333333333, 752.3333333333333, 22.5, 25.91622855102157, 0.4684236867045282, 1.0, 1.0, 65.0, 62243.90587614027], 
current ob forecast is [], 
actual action is [-8.3, 65.0], 
sim time this is 385800.0000, 
sim time next is 386400.0000, 
raw observation next is [-13.2, 57.00000000000001, 60.16666666666666, 758.1666666666667, 22.5, 25.51648561700286, 0.4219778282321225, 1.0, 1.0, 65.0, 58072.2418205502], 
processed observation next is [1.0, 0.4782608695652174, 0.09695290858725764, 0.5700000000000001, 0.20055555555555551, 0.8377532228360959, 0.375, 0.626373801416905, 0.6406592760773742, 1.0, 1.0, 1.0, 0.2765344848597629], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.1232799], dtype=float32), 1.0459205]. 
=============================================
[2019-04-08 15:23:58,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1610524e-13 6.5328706e-08 6.1955150e-05 7.4552820e-04 8.2524799e-08
 5.2730154e-10 4.0375002e-05 2.9437219e-10 2.7882254e-13 8.5044093e-16
 9.9915195e-01], sum to 1.0000
[2019-04-08 15:23:58,049] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6220
[2019-04-08 15:23:58,083] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.683333333333334, 45.33333333333334, 0.0, 0.0, 19.0, 24.73880464754625, 0.1946095700692672, 0.0, 1.0, 65.0, 62974.3814194669], 
current ob forecast is [], 
actual action is [-4.683333333333334, 65.0], 
sim time this is 453000.0000, 
sim time next is 453600.0000, 
raw observation next is [-9.5, 44.0, 0.0, 0.0, 19.0, 24.72491430289186, 0.1975525342840226, 0.0, 1.0, 65.0, 62907.1052146017], 
processed observation next is [1.0, 0.2608695652173913, 0.1994459833795014, 0.44, 0.0, 0.0, 0.08333333333333333, 0.5604095252409884, 0.5658508447613408, 0.0, 1.0, 1.0, 0.2995576438790557], 
reward next is 0.7004, 
noisyNet noise sample is [array([-0.36559016], dtype=float32), -0.07032068]. 
=============================================
[2019-04-08 15:23:58,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2667803e-13 6.8324702e-08 6.3432592e-05 7.4955256e-04 8.5189747e-08
 5.4120103e-10 4.0595234e-05 3.0364300e-10 2.9263408e-13 8.9616880e-16
 9.9914622e-01], sum to 1.0000
[2019-04-08 15:23:58,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7426
[2019-04-08 15:23:58,272] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 19.0, 24.76990472950018, 0.1966312890558067, 0.0, 1.0, 65.0, 62713.26735591141], 
current ob forecast is [], 
actual action is [-4.133333333333333, 65.0], 
sim time this is 454800.0000, 
sim time next is 455400.0000, 
raw observation next is [-8.95, 43.5, 0.0, 0.0, 19.0, 24.74829079137067, 0.2144847775120092, 0.0, 1.0, 65.0, 63170.68712462627], 
processed observation next is [1.0, 0.2608695652173913, 0.21468144044321333, 0.435, 0.0, 0.0, 0.08333333333333333, 0.5623575659475559, 0.5714949258373364, 0.0, 1.0, 1.0, 0.3008127958315537], 
reward next is 0.6992, 
noisyNet noise sample is [array([-0.36559016], dtype=float32), -0.07032068]. 
=============================================
[2019-04-08 15:23:58,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6348802e-12 1.7817345e-06 1.6652484e-03 7.8600663e-04 9.2926904e-08
 2.0660224e-08 1.8176231e-04 5.7936339e-10 1.4361824e-13 8.7198095e-15
 9.9736506e-01], sum to 1.0000
[2019-04-08 15:23:58,743] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2710
[2019-04-08 15:23:58,768] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.7, 52.0, 0.0, 0.0, 19.0, 24.79903029739811, 0.2115329330977667, 0.0, 1.0, 65.0, 63089.18263814303], 
current ob forecast is [], 
actual action is [-5.699999999999999, 65.0], 
sim time this is 449400.0000, 
sim time next is 450000.0000, 
raw observation next is [-10.6, 52.0, 0.0, 0.0, 19.0, 24.7957572764859, 0.2092825311087364, 0.0, 1.0, 65.0, 63110.61120235217], 
processed observation next is [1.0, 0.21739130434782608, 0.1689750692520776, 0.52, 0.0, 0.0, 0.08333333333333333, 0.566313106373825, 0.5697608437029121, 0.0, 1.0, 1.0, 0.3005267200112008], 
reward next is 0.6995, 
noisyNet noise sample is [array([-0.14781815], dtype=float32), -1.3705655]. 
=============================================
[2019-04-08 15:23:58,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.164116]
 [63.225636]
 [63.270287]
 [63.353634]
 [63.402077]], R is [[63.1886673 ]
 [63.25635529]
 [63.32344055]
 [63.38986206]
 [63.45583344]].
[2019-04-08 15:23:59,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4262625e-14 2.3934783e-08 3.3919869e-06 3.4526143e-05 2.9245004e-09
 1.3238434e-11 1.6394881e-06 7.5605902e-12 1.0144650e-14 5.7799550e-17
 9.9996042e-01], sum to 1.0000
[2019-04-08 15:23:59,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6073
[2019-04-08 15:23:59,470] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 26.5, 122.0, 0.0, 22.5, 26.43028723670705, 0.4963985080077934, 1.0, 1.0, 65.0, 47603.51514661843], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 473400.0000, 
sim time next is 474000.0000, 
raw observation next is [-1.9, 26.0, 123.1666666666667, 0.0, 22.5, 26.47506448509344, 0.5034904292262298, 1.0, 1.0, 65.0, 46451.22946864952], 
processed observation next is [1.0, 0.4782608695652174, 0.4099722991689751, 0.26, 0.4105555555555557, 0.0, 0.375, 0.7062553737577867, 0.66783014307541, 1.0, 1.0, 1.0, 0.22119633080309298], 
reward next is 0.7788, 
noisyNet noise sample is [array([-2.5906062], dtype=float32), 0.17403188]. 
=============================================
[2019-04-08 15:23:59,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.28384 ]
 [69.53717 ]
 [69.37354 ]
 [68.98633 ]
 [68.888306]], R is [[69.51996613]
 [69.5980835 ]
 [69.68280029]
 [69.76557159]
 [69.84426117]].
[2019-04-08 15:23:59,587] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7295763e-13 1.5542081e-08 8.1457401e-05 2.4207631e-05 1.1184467e-08
 5.3150408e-11 4.3221408e-07 5.3318829e-12 2.7503769e-16 3.8311375e-18
 9.9989390e-01], sum to 1.0000
[2019-04-08 15:23:59,589] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9505
[2019-04-08 15:23:59,599] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2317771e-12 4.9978678e-07 2.5675385e-04 3.0730173e-04 1.6341254e-07
 7.9275244e-09 6.9575817e-06 4.1675371e-10 1.4386403e-13 5.1178023e-15
 9.9942833e-01], sum to 1.0000
[2019-04-08 15:23:59,603] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3483
[2019-04-08 15:23:59,612] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.366666666666667, 27.0, 127.3333333333333, 0.0, 22.5, 26.56800920281813, 0.5291597309130665, 1.0, 1.0, 65.0, 45430.44603535044], 
current ob forecast is [], 
actual action is [3.633333333333333, 65.0], 
sim time this is 477600.0000, 
sim time next is 478200.0000, 
raw observation next is [-1.283333333333333, 27.5, 125.6666666666667, 0.0, 22.5, 26.50800995151104, 0.5328101806018396, 1.0, 1.0, 65.0, 46296.77208788909], 
processed observation next is [1.0, 0.5217391304347826, 0.4270544783010157, 0.275, 0.418888888888889, 0.0, 0.375, 0.7090008292925866, 0.6776033935339466, 1.0, 1.0, 1.0, 0.22046081946613852], 
reward next is 0.7795, 
noisyNet noise sample is [array([-0.4755479], dtype=float32), 0.7042457]. 
=============================================
[2019-04-08 15:23:59,615] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1473483e-13 2.2803521e-08 4.9069033e-05 2.0910747e-04 1.7279678e-08
 1.3353305e-09 2.2878085e-06 5.9604932e-11 2.6845723e-14 3.0934370e-15
 9.9973947e-01], sum to 1.0000
[2019-04-08 15:23:59,619] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6844
[2019-04-08 15:23:59,642] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.6, 48.66666666666666, 0.0, 0.0, 19.0, 25.29108761850245, 0.3593165642779745, 0.0, 1.0, 65.0, 61736.35845178788], 
current ob forecast is [], 
actual action is [-5.6, 65.0], 
sim time this is 424200.0000, 
sim time next is 424800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 19.0, 25.27327254898938, 0.3555215402778262, 0.0, 1.0, 65.0, 61769.04363806525], 
processed observation next is [1.0, 0.9565217391304348, 0.1689750692520776, 0.49, 0.0, 0.0, 0.08333333333333333, 0.6061060457491149, 0.6185071800926087, 0.0, 1.0, 1.0, 0.294138303038406], 
reward next is 0.7059, 
noisyNet noise sample is [array([-1.1718534], dtype=float32), 1.8012848]. 
=============================================
[2019-04-08 15:23:59,660] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.4, 45.33333333333334, 0.0, 0.0, 19.0, 25.46412733677308, 0.394900974402129, 0.0, 1.0, 65.0, 62221.73369868712], 
current ob forecast is [], 
actual action is [-5.4, 65.0], 
sim time this is 420000.0000, 
sim time next is 420600.0000, 
raw observation next is [-10.5, 46.16666666666667, 0.0, 0.0, 19.0, 25.43143320365974, 0.3895122997451743, 0.0, 1.0, 65.0, 62511.59964844392], 
processed observation next is [1.0, 0.8695652173913043, 0.17174515235457063, 0.4616666666666667, 0.0, 0.0, 0.08333333333333333, 0.6192861003049783, 0.6298374332483915, 0.0, 1.0, 1.0, 0.2976742840402091], 
reward next is 0.7023, 
noisyNet noise sample is [array([-1.8704463], dtype=float32), 0.09369696]. 
=============================================
[2019-04-08 15:24:00,607] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3531069e-15 1.1016682e-09 5.3666454e-06 3.2677202e-05 2.5680754e-08
 3.4695259e-12 3.1313979e-07 2.9597175e-12 6.9748183e-15 7.5512051e-19
 9.9996161e-01], sum to 1.0000
[2019-04-08 15:24:00,607] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4360
[2019-04-08 15:24:00,628] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 43.0, 10.0, 0.0, 22.5, 27.02662161325859, 0.6463474518162029, 1.0, 1.0, 65.0, 38232.66309010929], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 493200.0000, 
sim time next is 493800.0000, 
raw observation next is [1.0, 49.83333333333334, 0.0, 0.0, 22.5, 27.05058140907954, 0.6465937146540329, 1.0, 1.0, 65.0, 37871.06511606306], 
processed observation next is [1.0, 0.7391304347826086, 0.4903047091412743, 0.4983333333333334, 0.0, 0.0, 0.375, 0.7542151174232951, 0.715531238218011, 1.0, 1.0, 1.0, 0.180338405314586], 
reward next is 0.8197, 
noisyNet noise sample is [array([2.0405257], dtype=float32), -0.11886041]. 
=============================================
[2019-04-08 15:24:01,031] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4962301e-14 8.9865800e-08 8.4705753e-05 6.6190201e-04 3.1645779e-08
 6.1945071e-10 2.1044994e-05 2.0587571e-10 1.3613827e-15 2.2789813e-16
 9.9923217e-01], sum to 1.0000
[2019-04-08 15:24:01,032] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-08 15:24:01,053] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.2, 27.5, 114.0, 0.0, 22.5, 26.44107954418237, 0.4851313303798506, 1.0, 1.0, 65.0, 46273.69382315017], 
current ob forecast is [], 
actual action is [2.8, 65.0], 
sim time this is 472200.0000, 
sim time next is 472800.0000, 
raw observation next is [-2.1, 27.0, 118.0, 0.0, 22.5, 26.47516697626676, 0.4830141764927212, 1.0, 1.0, 65.0, 46042.69009410465], 
processed observation next is [1.0, 0.4782608695652174, 0.404432132963989, 0.27, 0.3933333333333333, 0.0, 0.375, 0.7062639146888966, 0.6610047254975737, 1.0, 1.0, 1.0, 0.21925090521002213], 
reward next is 0.7807, 
noisyNet noise sample is [array([0.37616074], dtype=float32), 0.6207756]. 
=============================================
[2019-04-08 15:24:01,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6655690e-13 8.1383035e-08 4.9907033e-04 1.0763523e-03 2.0452191e-08
 1.8935389e-09 7.3385490e-05 5.7224000e-11 2.5650551e-15 2.4832433e-17
 9.9835110e-01], sum to 1.0000
[2019-04-08 15:24:01,562] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9913
[2019-04-08 15:24:01,617] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.427036291757, 0.6386009392002214, 0.0, 1.0, 65.0, 50942.44068388581], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 586800.0000, 
sim time next is 587400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.4126682909971, 0.638150576367298, 0.0, 1.0, 65.0, 51186.61831924709], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7010556909164251, 0.7127168587890993, 0.0, 1.0, 1.0, 0.24374580152022424], 
reward next is 0.7563, 
noisyNet noise sample is [array([0.6757036], dtype=float32), -0.39812303]. 
=============================================
[2019-04-08 15:24:01,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0968245e-16 7.3635147e-11 3.0124377e-06 5.5040531e-05 2.7009637e-09
 7.3650808e-13 1.6642744e-07 5.5521527e-13 2.6333363e-18 1.2665426e-19
 9.9994183e-01], sum to 1.0000
[2019-04-08 15:24:01,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3029
[2019-04-08 15:24:01,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6952313e-15 1.3254746e-07 1.8479883e-05 1.5416632e-04 3.9240039e-08
 1.8310074e-10 4.9239497e-06 1.3515785e-11 5.9037218e-15 4.1916987e-17
 9.9982232e-01], sum to 1.0000
[2019-04-08 15:24:01,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4863
[2019-04-08 15:24:01,947] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 94.0, 0.0, 0.0, 22.5, 26.34241736628451, 0.562492501075206, 0.0, 1.0, 65.0, 50200.96685003302], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 499800.0000, 
sim time next is 500400.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 22.5, 26.29942414887008, 0.5553620872469319, 0.0, 1.0, 65.0, 51019.06025421803], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.375, 0.6916186790725067, 0.6851206957489774, 0.0, 1.0, 1.0, 0.2429479059724668], 
reward next is 0.7571, 
noisyNet noise sample is [array([-0.09035879], dtype=float32), -1.0296539]. 
=============================================
[2019-04-08 15:24:01,956] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 83.66666666666667, 0.0, 0.0, 19.0, 26.37104699909084, 0.6097152952347286, 0.0, 1.0, 65.0, 51491.13334451033], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 593400.0000, 
sim time next is 594000.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 19.0, 26.34963594350321, 0.6070680780618075, 0.0, 1.0, 65.0, 52612.61450512935], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6958029952919341, 0.7023560260206025, 0.0, 1.0, 1.0, 0.250536259548235], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.2849631], dtype=float32), 1.485381]. 
=============================================
[2019-04-08 15:24:01,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.89619]
 [71.9571 ]
 [71.98765]
 [72.0474 ]
 [72.04805]], R is [[71.87275696]
 [71.90883636]
 [71.94625092]
 [71.97813416]
 [72.01079559]].
[2019-04-08 15:24:02,273] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.3845141e-13 4.3409255e-08 5.1749830e-05 3.0777050e-04 1.8123371e-08
 5.1543814e-10 1.1165806e-06 8.6285814e-12 1.1393752e-14 1.4802537e-15
 9.9963927e-01], sum to 1.0000
[2019-04-08 15:24:02,273] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1372
[2019-04-08 15:24:02,308] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.3, 42.5, 0.0, 0.0, 22.5, 24.82281761444107, 0.2124696666812899, 0.0, 1.0, 65.0, 62112.35267105296], 
current ob forecast is [], 
actual action is [-3.3000000000000007, 65.0], 
sim time this is 457800.0000, 
sim time next is 458400.0000, 
raw observation next is [-8.2, 42.0, 0.0, 0.0, 22.5, 24.87826566800851, 0.213297755662321, 1.0, 1.0, 65.0, 61877.82813395721], 
processed observation next is [1.0, 0.30434782608695654, 0.23545706371191139, 0.42, 0.0, 0.0, 0.375, 0.5731888056673758, 0.5710992518874404, 1.0, 1.0, 1.0, 0.2946563244474153], 
reward next is 0.7053, 
noisyNet noise sample is [array([0.05807711], dtype=float32), 0.5237202]. 
=============================================
[2019-04-08 15:24:03,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6717771e-15 3.8671363e-08 2.8803188e-05 3.6676131e-05 2.0868669e-09
 6.1756857e-11 1.6308433e-05 2.2817506e-13 1.4931169e-15 1.6188636e-18
 9.9991822e-01], sum to 1.0000
[2019-04-08 15:24:03,731] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1188
[2019-04-08 15:24:03,766] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 36.0, 94.0, 0.0, 22.5, 26.80237732116507, 0.5978029388148843, 1.0, 1.0, 65.0, 40126.95431789388], 
current ob forecast is [], 
actual action is [4.7, 65.0], 
sim time this is 484200.0000, 
sim time next is 484800.0000, 
raw observation next is [-0.2, 36.33333333333333, 87.66666666666667, 0.0, 22.5, 26.84480730899702, 0.6005053208528692, 1.0, 1.0, 65.0, 39618.14526933009], 
processed observation next is [1.0, 0.6086956521739131, 0.4570637119113574, 0.3633333333333333, 0.2922222222222222, 0.0, 0.375, 0.7370672757497516, 0.7001684402842897, 1.0, 1.0, 1.0, 0.18865783461585756], 
reward next is 0.8113, 
noisyNet noise sample is [array([0.75861543], dtype=float32), -2.000953]. 
=============================================
[2019-04-08 15:24:04,191] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.51605583e-16 2.41529996e-09 6.03567696e-06 1.06018124e-04
 7.23731575e-10 1.57121365e-12 6.45408761e-07 3.58572850e-12
 8.73761734e-17 1.72231091e-18 9.99887228e-01], sum to 1.0000
[2019-04-08 15:24:04,191] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7433
[2019-04-08 15:24:04,237] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8, 63.5, 0.0, 0.0, 22.5, 26.02528282672203, 0.5703285345916276, 1.0, 1.0, 65.0, 66586.47059204959], 
current ob forecast is [], 
actual action is [5.8, 65.0], 
sim time this is 495000.0000, 
sim time next is 495600.0000, 
raw observation next is [0.7000000000000001, 70.33333333333334, 0.0, 0.0, 22.5, 25.2341171405284, 0.495494886065527, 1.0, 1.0, 65.0, 53044.87950978822], 
processed observation next is [1.0, 0.7391304347826086, 0.4819944598337951, 0.7033333333333335, 0.0, 0.0, 0.375, 0.6028430950440334, 0.6651649620218424, 1.0, 1.0, 1.0, 0.25259466433232486], 
reward next is 0.7474, 
noisyNet noise sample is [array([2.1075], dtype=float32), 1.5507625]. 
=============================================
[2019-04-08 15:24:04,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9970379e-16 4.1187453e-09 1.8828168e-05 1.1692729e-05 2.4063307e-10
 8.6575533e-13 8.7042059e-07 3.4869565e-13 2.5476203e-17 4.7104262e-20
 9.9996865e-01], sum to 1.0000
[2019-04-08 15:24:04,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7929
[2019-04-08 15:24:04,623] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 19.0, 26.3019540066307, 0.5801798166449079, 0.0, 1.0, 65.0, 48178.2495577824], 
current ob forecast is [], 
actual action is [8.716666666666665, 65.0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [3.8, 97.0, 0.0, 0.0, 19.0, 26.29928682893553, 0.5795976587548773, 0.0, 1.0, 65.0, 48524.50834895007], 
processed observation next is [0.0, 0.0, 0.5678670360110805, 0.97, 0.0, 0.0, 0.08333333333333333, 0.6916072357446273, 0.6931992195849591, 0.0, 1.0, 1.0, 0.23106908737595272], 
reward next is 0.7689, 
noisyNet noise sample is [array([-1.9186015], dtype=float32), 0.45354477]. 
=============================================
[2019-04-08 15:24:05,321] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.1508047e-15 1.6079680e-07 4.7344267e-05 1.2829520e-04 3.6003996e-08
 4.1181780e-10 5.8954873e-04 2.7010800e-11 1.3804899e-15 5.9709549e-18
 9.9923468e-01], sum to 1.0000
[2019-04-08 15:24:05,323] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7118
[2019-04-08 15:24:05,342] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.05, 87.0, 0.0, 0.0, 19.0, 26.31495781818717, 0.5923628164156877, 0.0, 1.0, 65.0, 49324.47364378539], 
current ob forecast is [], 
actual action is [9.05, 65.0], 
sim time this is 527400.0000, 
sim time next is 528000.0000, 
raw observation next is [3.966666666666667, 86.66666666666666, 0.0, 0.0, 19.0, 26.31095498986028, 0.5922953430225429, 0.0, 1.0, 65.0, 49395.60252571418], 
processed observation next is [0.0, 0.08695652173913043, 0.5724838411819021, 0.8666666666666666, 0.0, 0.0, 0.08333333333333333, 0.6925795824883568, 0.6974317810075142, 0.0, 1.0, 1.0, 0.23521715488435324], 
reward next is 0.7648, 
noisyNet noise sample is [array([-0.5399227], dtype=float32), -0.9841415]. 
=============================================
[2019-04-08 15:24:05,347] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.4985  ]
 [76.32262 ]
 [76.536804]
 [76.6153  ]
 [76.84533 ]], R is [[76.30107117]
 [76.30318451]
 [76.30573273]
 [76.30876923]
 [76.31221008]].
[2019-04-08 15:24:05,384] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.4146277e-14 5.2770364e-08 4.9924975e-06 3.1541506e-04 3.1807795e-07
 2.0977641e-11 9.8227133e-07 1.3147211e-12 1.5668641e-15 4.1740683e-18
 9.9967825e-01], sum to 1.0000
[2019-04-08 15:24:05,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9584
[2019-04-08 15:24:05,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.66374337e-17 4.26926022e-10 1.19996503e-05 9.55695214e-06
 1.88621604e-10 3.34775922e-11 2.25066401e-07 1.02792314e-13
 2.72763039e-17 3.99951158e-19 9.99978185e-01], sum to 1.0000
[2019-04-08 15:24:05,399] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7643
[2019-04-08 15:24:05,402] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 80.0, 136.1666666666667, 573.6666666666667, 19.0, 26.57184685813825, 0.7180128195411438, 0.0, 1.0, 65.0, 45022.44679921083], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 567600.0000, 
sim time next is 568200.0000, 
raw observation next is [-1.2, 80.0, 134.3333333333333, 552.3333333333333, 19.0, 26.57931529366905, 0.7223907744234784, 0.0, 1.0, 65.0, 45328.45607401054], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.4477777777777776, 0.6103130755064455, 0.08333333333333333, 0.7149429411390876, 0.7407969248078262, 0.0, 1.0, 1.0, 0.2158497908286216], 
reward next is 0.7842, 
noisyNet noise sample is [array([-0.02659132], dtype=float32), -1.5188764]. 
=============================================
[2019-04-08 15:24:05,419] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.516666666666667, 96.0, 0.0, 0.0, 19.0, 26.18779579945879, 0.5558420947650677, 0.0, 1.0, 65.0, 51676.97621397596], 
current ob forecast is [], 
actual action is [6.5166666666666675, 65.0], 
sim time this is 507000.0000, 
sim time next is 507600.0000, 
raw observation next is [1.6, 96.0, 0.0, 0.0, 19.0, 26.17611964157142, 0.5558191248939561, 0.0, 1.0, 65.0, 52174.35277633867], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.96, 0.0, 0.0, 0.08333333333333333, 0.6813433034642852, 0.6852730416313187, 0.0, 1.0, 1.0, 0.24844929893494602], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.9505956], dtype=float32), 0.43283135]. 
=============================================
[2019-04-08 15:24:05,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4471355e-16 2.6556818e-10 1.9216190e-05 2.1598495e-04 1.1558816e-09
 2.5763007e-12 2.2209106e-06 3.1226484e-13 1.1368784e-17 1.1642328e-19
 9.9976259e-01], sum to 1.0000
[2019-04-08 15:24:05,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2699
[2019-04-08 15:24:05,613] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 93.33333333333333, 0.0, 0.0, 19.0, 26.17379019584926, 0.5607408235101132, 0.0, 1.0, 65.0, 51533.99646406268], 
current ob forecast is [], 
actual action is [7.333333333333333, 65.0], 
sim time this is 510000.0000, 
sim time next is 510600.0000, 
raw observation next is [2.516666666666667, 92.66666666666667, 0.0, 0.0, 19.0, 26.19018835196221, 0.5610673976685411, 0.0, 1.0, 65.0, 51075.68057019841], 
processed observation next is [1.0, 0.9130434782608695, 0.5323176361957526, 0.9266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6825156959968508, 0.6870224658895138, 0.0, 1.0, 1.0, 0.24321752652475434], 
reward next is 0.7568, 
noisyNet noise sample is [array([0.666387], dtype=float32), -1.0082667]. 
=============================================
[2019-04-08 15:24:05,796] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.6930610e-15 5.9931549e-09 9.1584159e-05 3.2903201e-05 6.8096306e-09
 1.7384573e-10 1.5433448e-06 1.5543013e-12 2.4576357e-16 1.5910161e-18
 9.9987400e-01], sum to 1.0000
[2019-04-08 15:24:05,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0652
[2019-04-08 15:24:05,831] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.783333333333333, 84.5, 0.0, 0.0, 19.0, 26.26859910326741, 0.5860974391836214, 0.0, 1.0, 65.0, 51055.19828089602], 
current ob forecast is [], 
actual action is [6.783333333333333, 65.0], 
sim time this is 535800.0000, 
sim time next is 536400.0000, 
raw observation next is [1.6, 85.0, 0.0, 0.0, 19.0, 26.25664030351796, 0.5846295768327193, 0.0, 1.0, 65.0, 51211.79003728947], 
processed observation next is [0.0, 0.21739130434782608, 0.5069252077562327, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6880533586264965, 0.6948765256109065, 0.0, 1.0, 1.0, 0.24386566684423558], 
reward next is 0.7561, 
noisyNet noise sample is [array([-0.7769329], dtype=float32), 0.9928591]. 
=============================================
[2019-04-08 15:24:05,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1526115e-14 5.5427289e-08 8.9702437e-05 9.4437173e-06 6.1661069e-08
 4.9952393e-11 1.0385029e-05 2.4111256e-11 6.5696629e-15 4.0230032e-17
 9.9989033e-01], sum to 1.0000
[2019-04-08 15:24:05,993] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2020
[2019-04-08 15:24:06,010] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.266666666666667, 87.0, 0.0, 0.0, 19.0, 26.2358768709527, 0.5916522514930588, 0.0, 1.0, 65.0, 51634.50273167298], 
current ob forecast is [], 
actual action is [6.2666666666666675, 65.0], 
sim time this is 538800.0000, 
sim time next is 539400.0000, 
raw observation next is [1.183333333333333, 87.5, 0.0, 0.0, 19.0, 26.29734742165353, 0.5870273000072768, 0.0, 1.0, 65.0, 49840.57900044487], 
processed observation next is [0.0, 0.21739130434782608, 0.49538319482917825, 0.875, 0.0, 0.0, 0.08333333333333333, 0.6914456184711275, 0.6956757666690923, 0.0, 1.0, 1.0, 0.23733609047830892], 
reward next is 0.7627, 
noisyNet noise sample is [array([-2.0961397], dtype=float32), 2.4610817]. 
=============================================
[2019-04-08 15:24:06,279] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.2271260e-16 9.4983355e-09 1.2417193e-04 4.1583222e-05 3.6929599e-08
 3.9651121e-12 4.3753911e-07 1.2188330e-13 5.3617751e-17 9.0928811e-20
 9.9983370e-01], sum to 1.0000
[2019-04-08 15:24:06,289] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7743
[2019-04-08 15:24:06,303] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.2, 95.33333333333333, 0.0, 0.0, 19.0, 26.21531308037321, 0.5681381542783704, 0.0, 1.0, 65.0, 50526.85209622909], 
current ob forecast is [], 
actual action is [8.2, 65.0], 
sim time this is 514200.0000, 
sim time next is 514800.0000, 
raw observation next is [3.3, 96.0, 0.0, 0.0, 19.0, 26.22162566998985, 0.5703278516079023, 0.0, 1.0, 65.0, 50311.1535022114], 
processed observation next is [1.0, 1.0, 0.554016620498615, 0.96, 0.0, 0.0, 0.08333333333333333, 0.6851354724991543, 0.6901092838693007, 0.0, 1.0, 1.0, 0.2395769214391019], 
reward next is 0.7604, 
noisyNet noise sample is [array([1.2090013], dtype=float32), 0.3500593]. 
=============================================
[2019-04-08 15:24:06,880] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0692155e-13 2.4609921e-09 2.9294126e-05 2.0912439e-05 4.4939590e-08
 9.2905432e-12 5.9318004e-06 2.3131150e-12 4.4123439e-16 4.3474658e-17
 9.9994385e-01], sum to 1.0000
[2019-04-08 15:24:06,882] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9065
[2019-04-08 15:24:06,922] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.883333333333334, 82.66666666666667, 0.0, 0.0, 19.0, 26.28737130159759, 0.5899004308158883, 0.0, 1.0, 65.0, 50162.26159465416], 
current ob forecast is [], 
actual action is [7.883333333333335, 65.0], 
sim time this is 532200.0000, 
sim time next is 532800.0000, 
raw observation next is [2.7, 82.0, 0.0, 0.0, 19.0, 26.28248657341691, 0.5891962306295339, 0.0, 1.0, 65.0, 50284.84386204194], 
processed observation next is [0.0, 0.17391304347826086, 0.5373961218836566, 0.82, 0.0, 0.0, 0.08333333333333333, 0.6902072144514092, 0.696398743543178, 0.0, 1.0, 1.0, 0.23945163743829495], 
reward next is 0.7605, 
noisyNet noise sample is [array([-0.7904173], dtype=float32), -1.756967]. 
=============================================
[2019-04-08 15:24:07,074] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7618185e-15 4.2754951e-09 1.4724901e-04 4.1731655e-05 1.8285126e-08
 1.4321136e-10 2.8629945e-06 2.7071637e-12 1.3454145e-15 2.4929558e-17
 9.9980813e-01], sum to 1.0000
[2019-04-08 15:24:07,074] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6230
[2019-04-08 15:24:07,116] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 83.0, 0.0, 0.0, 19.0, 26.29287435392854, 0.5907801327691676, 0.0, 1.0, 65.0, 49987.15236300028], 
current ob forecast is [], 
actual action is [7.333333333333333, 65.0], 
sim time this is 534000.0000, 
sim time next is 534600.0000, 
raw observation next is [2.15, 83.5, 0.0, 0.0, 19.0, 26.29513830037707, 0.5883790435219253, 0.0, 1.0, 65.0, 49977.2299933536], 
processed observation next is [0.0, 0.17391304347826086, 0.5221606648199446, 0.835, 0.0, 0.0, 0.08333333333333333, 0.6912615250314224, 0.6961263478406418, 0.0, 1.0, 1.0, 0.23798680949216], 
reward next is 0.7620, 
noisyNet noise sample is [array([0.77906895], dtype=float32), 0.4031216]. 
=============================================
[2019-04-08 15:24:08,031] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.4322172e-13 9.8127252e-08 6.7256675e-05 1.1006052e-05 9.3953680e-09
 1.6336038e-10 1.2833738e-05 1.7495441e-11 1.1352435e-14 1.0339373e-17
 9.9990880e-01], sum to 1.0000
[2019-04-08 15:24:08,038] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4964
[2019-04-08 15:24:08,068] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 19.0, 26.36183568605291, 0.594005729060361, 0.0, 1.0, 65.0, 48758.31408246284], 
current ob forecast is [], 
actual action is [9.416666666666668, 65.0], 
sim time this is 525000.0000, 
sim time next is 525600.0000, 
raw observation next is [4.3, 88.0, 0.0, 0.0, 19.0, 26.34128333745525, 0.5928668977586771, 0.0, 1.0, 65.0, 49035.94954626408], 
processed observation next is [0.0, 0.08695652173913043, 0.5817174515235458, 0.88, 0.0, 0.0, 0.08333333333333333, 0.6951069447879373, 0.6976222992528923, 0.0, 1.0, 1.0, 0.23350452164887656], 
reward next is 0.7665, 
noisyNet noise sample is [array([-0.8918218], dtype=float32), 0.6736304]. 
=============================================
[2019-04-08 15:24:08,356] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.4846834e-15 1.5159214e-08 7.3235999e-05 9.1407983e-06 3.3372094e-09
 7.5435639e-12 3.0976094e-06 7.8438064e-13 1.4667517e-17 2.9926913e-18
 9.9991453e-01], sum to 1.0000
[2019-04-08 15:24:08,356] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7793
[2019-04-08 15:24:08,393] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.616666666666667, 85.33333333333334, 0.0, 0.0, 19.0, 26.30288830531088, 0.5917064307397571, 0.0, 1.0, 65.0, 49639.4195514438], 
current ob forecast is [], 
actual action is [8.616666666666667, 65.0], 
sim time this is 529800.0000, 
sim time next is 530400.0000, 
raw observation next is [3.433333333333334, 84.66666666666667, 0.0, 0.0, 19.0, 26.29996442264326, 0.5912883480280207, 0.0, 1.0, 65.0, 49749.59951016408], 
processed observation next is [0.0, 0.13043478260869565, 0.5577100646352725, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6916637018869384, 0.6970961160093402, 0.0, 1.0, 1.0, 0.23690285481030515], 
reward next is 0.7631, 
noisyNet noise sample is [array([0.93174565], dtype=float32), -0.037646875]. 
=============================================
[2019-04-08 15:24:08,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4564993e-15 1.2686137e-08 1.2444177e-05 7.1581046e-05 6.2867564e-09
 1.4980380e-11 1.4166925e-06 3.5895900e-12 4.4496345e-17 5.9518788e-19
 9.9991453e-01], sum to 1.0000
[2019-04-08 15:24:08,395] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3511
[2019-04-08 15:24:08,424] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7666666666666667, 81.33333333333333, 102.6666666666667, 222.0, 19.0, 26.36901821123671, 0.6471083965317052, 0.0, 1.0, 65.0, 49278.4570702667], 
current ob forecast is [], 
actual action is [4.233333333333333, 65.0], 
sim time this is 561000.0000, 
sim time next is 561600.0000, 
raw observation next is [-0.8, 81.0, 109.5, 265.5, 19.0, 26.36733586522663, 0.6600516335920834, 0.0, 1.0, 65.0, 49023.27887605333], 
processed observation next is [0.0, 0.5217391304347826, 0.4404432132963989, 0.81, 0.365, 0.29337016574585634, 0.08333333333333333, 0.6972779887688857, 0.7200172111973612, 0.0, 1.0, 1.0, 0.23344418512406348], 
reward next is 0.7666, 
noisyNet noise sample is [array([-0.17056517], dtype=float32), -0.4750747]. 
=============================================
[2019-04-08 15:24:08,609] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7650249e-15 2.3026288e-08 1.9907288e-06 1.5294383e-04 2.2901240e-08
 2.2039391e-10 7.5072410e-07 3.2494219e-11 9.3147387e-16 2.9555873e-18
 9.9984419e-01], sum to 1.0000
[2019-04-08 15:24:08,612] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8739
[2019-04-08 15:24:08,623] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.37995931996971, 0.62931765899098, 0.0, 1.0, 65.0, 51777.48547010791], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 589800.0000, 
sim time next is 590400.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.39949697164819, 0.624271819920712, 0.0, 1.0, 65.0, 50990.86719556021], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6999580809706826, 0.7080906066402374, 0.0, 1.0, 1.0, 0.24281365331219149], 
reward next is 0.7572, 
noisyNet noise sample is [array([0.41976008], dtype=float32), -0.14052929]. 
=============================================
[2019-04-08 15:24:08,684] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.4423151e-14 1.0041498e-08 7.7095843e-05 2.4036261e-04 1.8520080e-08
 1.1190686e-10 3.5551400e-07 1.7447052e-12 1.7668618e-15 1.1338513e-16
 9.9968219e-01], sum to 1.0000
[2019-04-08 15:24:08,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6119
[2019-04-08 15:24:08,722] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.616666666666667, 86.33333333333333, 27.33333333333334, 30.0, 19.0, 26.59523734727676, 0.6952076677413453, 0.0, 1.0, 65.0, 47440.74136470605], 
current ob forecast is [], 
actual action is [3.383333333333333, 65.0], 
sim time this is 579000.0000, 
sim time next is 579600.0000, 
raw observation next is [-1.7, 87.0, 20.5, 22.5, 19.0, 26.61445097651831, 0.6883981664899913, 0.0, 1.0, 65.0, 46743.52783987088], 
processed observation next is [0.0, 0.7391304347826086, 0.4155124653739613, 0.87, 0.06833333333333333, 0.024861878453038673, 0.08333333333333333, 0.7178709147098591, 0.7294660554966638, 0.0, 1.0, 1.0, 0.22258822780890897], 
reward next is 0.7774, 
noisyNet noise sample is [array([0.64732623], dtype=float32), 1.3566601]. 
=============================================
[2019-04-08 15:24:08,946] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7439995e-14 7.5955837e-09 1.2750800e-05 1.1513260e-05 1.5890063e-08
 3.7504208e-11 3.3901192e-06 4.6100949e-12 2.1050476e-15 1.9333790e-17
 9.9997234e-01], sum to 1.0000
[2019-04-08 15:24:08,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5264086e-14 1.0167638e-08 8.6913424e-06 1.0817504e-04 9.8033697e-09
 1.5945531e-11 6.4264555e-07 8.7730463e-13 1.1808305e-16 2.3731425e-18
 9.9988246e-01], sum to 1.0000
[2019-04-08 15:24:08,950] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1457
[2019-04-08 15:24:08,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3154
[2019-04-08 15:24:08,980] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 19.0, 26.41749602282759, 0.6041882764692595, 0.0, 1.0, 65.0, 48411.78365965191], 
current ob forecast is [], 
actual action is [5.7, 65.0], 
sim time this is 542400.0000, 
sim time next is 543000.0000, 
raw observation next is [0.6, 91.33333333333334, 0.0, 0.0, 19.0, 26.42765283550205, 0.592755326876676, 0.0, 1.0, 65.0, 48173.42645935655], 
processed observation next is [0.0, 0.2608695652173913, 0.479224376731302, 0.9133333333333334, 0.0, 0.0, 0.08333333333333333, 0.7023044029585043, 0.6975851089588919, 0.0, 1.0, 1.0, 0.22939726885407882], 
reward next is 0.7706, 
noisyNet noise sample is [array([-1.2948633], dtype=float32), -0.62219465]. 
=============================================
[2019-04-08 15:24:08,996] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 81.0, 128.8333333333333, 488.3333333333333, 19.0, 26.61399058317866, 0.7306980086927152, 0.0, 1.0, 65.0, 44585.18245230333], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 570000.0000, 
sim time next is 570600.0000, 
raw observation next is [-1.2, 81.5, 127.0, 467.0, 19.0, 26.62309905366183, 0.7394617890744031, 0.0, 1.0, 65.0, 44496.66778410015], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.815, 0.42333333333333334, 0.5160220994475138, 0.08333333333333333, 0.7185915878051524, 0.7464872630248011, 0.0, 1.0, 1.0, 0.2118888942100007], 
reward next is 0.7881, 
noisyNet noise sample is [array([-0.4316968], dtype=float32), -0.08786494]. 
=============================================
[2019-04-08 15:24:09,023] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.60081]
 [72.57526]
 [72.59028]
 [72.57026]
 [72.60094]], R is [[72.66385651]
 [72.70668793]
 [72.7487793 ]
 [72.77662659]
 [72.80312347]].
[2019-04-08 15:24:09,271] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.96545197e-15 1.17145802e-07 4.00159224e-05 1.10603476e-04
 4.06270084e-09 1.63941916e-11 8.47493084e-06 1.37982312e-12
 3.98248104e-16 6.92066946e-18 9.99840736e-01], sum to 1.0000
[2019-04-08 15:24:09,272] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5471
[2019-04-08 15:24:09,307] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 80.0, 134.3333333333333, 552.3333333333333, 19.0, 26.57954819876321, 0.7224694509856994, 0.0, 1.0, 65.0, 45325.82127656046], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 568200.0000, 
sim time next is 568800.0000, 
raw observation next is [-1.2, 80.0, 132.5, 531.0, 19.0, 26.5902886866665, 0.7255421953342548, 0.0, 1.0, 65.0, 45064.62752465088], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.8, 0.44166666666666665, 0.5867403314917127, 0.08333333333333333, 0.7158573905555418, 0.7418473984447517, 0.0, 1.0, 1.0, 0.21459346440309943], 
reward next is 0.7854, 
noisyNet noise sample is [array([2.57718], dtype=float32), -0.58554035]. 
=============================================
[2019-04-08 15:24:09,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.9605905e-14 1.5103264e-07 3.1440181e-04 2.9575819e-04 3.6408781e-07
 1.0568603e-09 5.4418751e-06 5.6802330e-10 1.0372862e-14 3.1502116e-16
 9.9938381e-01], sum to 1.0000
[2019-04-08 15:24:09,666] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2303
[2019-04-08 15:24:09,684] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 65.5, 0.0, 0.0, 19.0, 25.99020740433705, 0.5056406948649144, 0.0, 1.0, 65.0, 57608.50262386219], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 625800.0000, 
sim time next is 626400.0000, 
raw observation next is [-4.5, 65.0, 0.0, 0.0, 19.0, 26.01465306467359, 0.5015582228387336, 0.0, 1.0, 65.0, 56594.52069243343], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.65, 0.0, 0.0, 0.08333333333333333, 0.6678877553894657, 0.6671860742795778, 0.0, 1.0, 1.0, 0.2694977175830163], 
reward next is 0.7305, 
noisyNet noise sample is [array([1.5347629], dtype=float32), -1.3286844]. 
=============================================
[2019-04-08 15:24:10,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3450910e-15 3.0125398e-09 1.9296841e-04 5.9764512e-05 1.8914818e-08
 5.8885827e-11 3.6456834e-06 9.0267095e-13 8.3401121e-17 9.2013164e-18
 9.9974364e-01], sum to 1.0000
[2019-04-08 15:24:10,184] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2509
[2019-04-08 15:24:10,215] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 26.18872465207173, 0.4842103105421192, 0.0, 1.0, 65.0, 53613.24293301612], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 716400.0000, 
sim time next is 717000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 22.5, 26.14220167878574, 0.4874647342583171, 0.0, 1.0, 65.0, 54798.46729821297], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.375, 0.6785168065654782, 0.6624882447527723, 0.0, 1.0, 1.0, 0.26094508237244274], 
reward next is 0.7391, 
noisyNet noise sample is [array([-0.12424044], dtype=float32), 1.3861938]. 
=============================================
[2019-04-08 15:24:10,222] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.33931 ]
 [77.32847 ]
 [77.32239 ]
 [77.28003 ]
 [77.283615]], R is [[77.53538513]
 [77.50473022]
 [77.48638153]
 [77.46633911]
 [77.43791199]].
[2019-04-08 15:24:11,783] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.86597724e-14 1.45586183e-08 2.95334189e-06 1.57386341e-04
 7.54345297e-09 3.24637789e-10 4.24602604e-07 8.72599475e-12
 1.85972627e-15 1.24862665e-17 9.99839187e-01], sum to 1.0000
[2019-04-08 15:24:11,784] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-08 15:24:11,806] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 19.0, 26.33006252879859, 0.6005599892331315, 0.0, 1.0, 65.0, 52413.73151146144], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 595800.0000, 
sim time next is 596400.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 19.0, 26.31864003770827, 0.5971365581661182, 0.0, 1.0, 65.0, 52677.01561520519], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6932200031423559, 0.6990455193887061, 0.0, 1.0, 1.0, 0.2508429315009771], 
reward next is 0.7492, 
noisyNet noise sample is [array([1.0110061], dtype=float32), -0.6128643]. 
=============================================
[2019-04-08 15:24:12,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8210857e-16 3.2394745e-10 6.1532387e-06 2.6735281e-05 6.7015091e-09
 2.9788927e-12 2.4954718e-07 2.4669462e-13 5.3350258e-17 3.1283149e-19
 9.9996686e-01], sum to 1.0000
[2019-04-08 15:24:12,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9586
[2019-04-08 15:24:12,527] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.066666666666666, 51.0, 56.66666666666667, 2.833333333333333, 22.5, 27.00418561293245, 0.8789472148994132, 1.0, 1.0, 65.0, 62092.68632290498], 
current ob forecast is [], 
actual action is [2.933333333333334, 65.0], 
sim time this is 751200.0000, 
sim time next is 751800.0000, 
raw observation next is [-2.433333333333334, 52.5, 45.33333333333334, 2.666666666666667, 22.5, 26.36260119928436, 0.7719021189372389, 1.0, 1.0, 65.0, 35836.96418175451], 
processed observation next is [1.0, 0.6956521739130435, 0.3951985226223454, 0.525, 0.15111111111111114, 0.002946593001841621, 0.375, 0.6968834332736966, 0.757300706312413, 1.0, 1.0, 1.0, 0.1706522103893072], 
reward next is 0.8293, 
noisyNet noise sample is [array([-0.40251258], dtype=float32), -0.69632083]. 
=============================================
[2019-04-08 15:24:12,584] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4378360e-16 3.5438705e-10 6.5972185e-06 2.7341399e-05 7.5698487e-09
 3.5387335e-12 2.8898961e-07 2.9753500e-13 6.0325150e-17 3.9104527e-19
 9.9996579e-01], sum to 1.0000
[2019-04-08 15:24:12,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3536
[2019-04-08 15:24:12,639] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 54.0, 34.0, 2.5, 22.5, 27.533112007794, 0.8551607734657024, 1.0, 1.0, 65.0, 26809.10627966619], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 752400.0000, 
sim time next is 753000.0000, 
raw observation next is [-2.983333333333333, 54.33333333333333, 0.0, 0.0, 22.5, 27.66992630335456, 0.8510728053425272, 1.0, 1.0, 65.0, 29445.23661639201], 
processed observation next is [1.0, 0.7391304347826086, 0.37996306555863346, 0.5433333333333333, 0.0, 0.0, 0.375, 0.8058271919462134, 0.7836909351141758, 1.0, 1.0, 1.0, 0.14021541245900956], 
reward next is 0.8598, 
noisyNet noise sample is [array([-0.40251258], dtype=float32), -0.69632083]. 
=============================================
[2019-04-08 15:24:12,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.061356]
 [76.19322 ]
 [76.37576 ]
 [76.596634]
 [76.89642 ]], R is [[75.96389008]
 [76.07659149]
 [76.14517212]
 [76.08804321]
 [76.12078857]].
[2019-04-08 15:24:12,844] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.6322397e-13 6.5792858e-07 5.4460124e-04 8.4454287e-04 6.2272829e-07
 3.0492988e-09 6.1399303e-05 4.5263369e-11 9.8255070e-14 2.7100594e-17
 9.9854809e-01], sum to 1.0000
[2019-04-08 15:24:12,845] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1775
[2019-04-08 15:24:12,881] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.899999999999999, 86.0, 0.0, 0.0, 19.0, 26.17020053365238, 0.5469746449803914, 0.0, 1.0, 65.0, 53380.51627672518], 
current ob forecast is [], 
actual action is [1.100000000000001, 65.0], 
sim time this is 610800.0000, 
sim time next is 611400.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 26.15079488172031, 0.5451403389461924, 0.0, 1.0, 65.0, 54417.31077156165], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6792329068100257, 0.6817134463153974, 0.0, 1.0, 1.0, 0.2591300512931507], 
reward next is 0.7409, 
noisyNet noise sample is [array([1.113639], dtype=float32), 1.4849374]. 
=============================================
[2019-04-08 15:24:14,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3307387e-13 3.7373792e-07 7.0576527e-05 9.8095916e-04 3.5488934e-08
 6.3392153e-10 3.6358488e-05 7.5830647e-11 3.0609901e-15 1.5968358e-14
 9.9891174e-01], sum to 1.0000
[2019-04-08 15:24:14,593] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9165
[2019-04-08 15:24:14,609] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 19.0, 26.20756639379421, 0.5236204558480498, 0.0, 1.0, 65.0, 52644.19269631406], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 19.0, 26.19717245797484, 0.521227865374286, 0.0, 1.0, 65.0, 52849.68130634935], 
processed observation next is [0.0, 0.8695652173913043, 0.37673130193905824, 0.67, 0.0, 0.0, 0.08333333333333333, 0.6830977048312367, 0.6737426217914287, 0.0, 1.0, 1.0, 0.25166514907785403], 
reward next is 0.7483, 
noisyNet noise sample is [array([0.76289225], dtype=float32), 0.024487337]. 
=============================================
[2019-04-08 15:24:14,834] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.00088774e-12 2.47788961e-07 2.31766433e-04 2.10522005e-04
 1.10455112e-07 7.16281390e-09 4.63290708e-05 3.48216345e-10
 1.04161424e-13 6.99009969e-16 9.99511003e-01], sum to 1.0000
[2019-04-08 15:24:14,835] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4148
[2019-04-08 15:24:14,861] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.14147590885462, 0.5139566918081118, 0.0, 1.0, 65.0, 53380.26345894449], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 683400.0000, 
sim time next is 684000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.17419666752932, 0.5088831422432013, 0.0, 1.0, 65.0, 52356.27470756292], 
processed observation next is [0.0, 0.9565217391304348, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6811830556274433, 0.669627714081067, 0.0, 1.0, 1.0, 0.24931559384553773], 
reward next is 0.7507, 
noisyNet noise sample is [array([1.7524916], dtype=float32), 0.04512269]. 
=============================================
[2019-04-08 15:24:14,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.022095]
 [67.98466 ]
 [67.925316]
 [67.8982  ]
 [67.82009 ]], R is [[68.13173676]
 [68.19622803]
 [68.2559967 ]
 [68.31497192]
 [68.37507629]].
[2019-04-08 15:24:15,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6272119e-13 1.4014398e-08 2.4845367e-06 1.1727182e-04 4.3119570e-08
 8.6340479e-10 7.1982940e-06 1.7426009e-11 7.1950123e-14 2.1923527e-16
 9.9987292e-01], sum to 1.0000
[2019-04-08 15:24:15,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3373
[2019-04-08 15:24:15,301] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.23094570180838, 0.534084537666961, 0.0, 1.0, 65.0, 53678.94378713745], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 673200.0000, 
sim time next is 673800.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 19.0, 26.21673905525395, 0.5318638899091587, 0.0, 1.0, 65.0, 54046.57853090623], 
processed observation next is [0.0, 0.8260869565217391, 0.3965835641735919, 0.625, 0.0, 0.0, 0.08333333333333333, 0.6847282546044958, 0.677287963303053, 0.0, 1.0, 1.0, 0.25736465967098204], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.1668046], dtype=float32), 0.24325371]. 
=============================================
[2019-04-08 15:24:15,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1976741e-13 6.2000694e-08 1.9416948e-04 1.2674592e-04 3.1111220e-08
 6.7532797e-09 7.6232016e-07 5.4536831e-12 3.4347315e-15 1.9610566e-15
 9.9967825e-01], sum to 1.0000
[2019-04-08 15:24:15,308] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2940
[2019-04-08 15:24:15,325] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.12079008693234, 0.5114010126569009, 0.0, 1.0, 65.0, 54234.97747287538], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 682800.0000, 
sim time next is 683400.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.14156880069797, 0.5139856312822281, 0.0, 1.0, 65.0, 53379.16939652772], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.678464066724831, 0.6713285437607427, 0.0, 1.0, 1.0, 0.2541865209358463], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.093549], dtype=float32), -0.8592673]. 
=============================================
[2019-04-08 15:24:15,762] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7991999e-13 1.2152928e-07 4.2245656e-04 3.8064391e-04 5.2220418e-08
 8.4755719e-10 3.6875858e-06 1.3143407e-09 4.1579892e-14 1.2398449e-15
 9.9919301e-01], sum to 1.0000
[2019-04-08 15:24:15,777] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6326
[2019-04-08 15:24:15,803] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 19.0, 26.29906859651512, 0.5504469729282773, 0.0, 1.0, 65.0, 53096.87957237198], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 667800.0000, 
sim time next is 668400.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 19.0, 26.28395147175284, 0.5493380741175622, 0.0, 1.0, 65.0, 53891.36981892448], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.57, 0.0, 0.0, 0.08333333333333333, 0.6903292893127366, 0.6831126913725208, 0.0, 1.0, 1.0, 0.2566255705663071], 
reward next is 0.7434, 
noisyNet noise sample is [array([-1.2366015], dtype=float32), 2.1171267]. 
=============================================
[2019-04-08 15:24:16,212] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2761245e-14 8.9642151e-09 2.4038280e-05 9.5894320e-05 2.6650090e-08
 2.7222899e-10 1.8674693e-06 1.5104906e-10 1.5732698e-14 2.4689773e-17
 9.9987817e-01], sum to 1.0000
[2019-04-08 15:24:16,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1631
[2019-04-08 15:24:16,268] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 19.0, 26.11092491425809, 0.5076400517329328, 0.0, 1.0, 65.0, 54188.54181849799], 
current ob forecast is [], 
actual action is [1.35, 65.0], 
sim time this is 685800.0000, 
sim time next is 686400.0000, 
raw observation next is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 26.14221444106067, 0.502472869190716, 0.0, 1.0, 65.0, 52847.9339649601], 
processed observation next is [0.0, 0.9565217391304348, 0.35918744228993543, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.6785178700883892, 0.6674909563969053, 0.0, 1.0, 1.0, 0.25165682840457193], 
reward next is 0.7483, 
noisyNet noise sample is [array([-0.05583097], dtype=float32), -1.9393992]. 
=============================================
[2019-04-08 15:24:17,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1674216e-15 1.8391717e-08 5.5837227e-06 4.5386882e-05 7.1435352e-10
 2.7381904e-11 8.2020750e-07 2.0157578e-13 1.3031182e-16 2.1843415e-19
 9.9994814e-01], sum to 1.0000
[2019-04-08 15:24:17,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5837
[2019-04-08 15:24:17,398] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 22.5, 26.1468067545322, 0.4882807593248569, 1.0, 1.0, 65.0, 53551.73039068779], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 717600.0000, 
sim time next is 718200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 22.5, 26.16675820153736, 0.4960071087670006, 1.0, 1.0, 65.0, 52595.04594946497], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.375, 0.6805631834614466, 0.6653357029223336, 1.0, 1.0, 1.0, 0.250452599759357], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.21814412], dtype=float32), 1.0741657]. 
=============================================
[2019-04-08 15:24:18,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.5722695e-19 5.7491789e-11 5.3276545e-07 2.5294088e-07 9.3759243e-11
 8.9618195e-15 1.0643390e-08 9.5946971e-15 6.3136189e-20 6.6479305e-22
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:24:18,183] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9311
[2019-04-08 15:24:18,204] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 72.0, 101.0, 49.0, 22.5, 26.96042722700051, 0.6271354769843862, 1.0, 1.0, 65.0, 37966.59769742164], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 725400.0000, 
sim time next is 726000.0000, 
raw observation next is [-1.9, 70.66666666666666, 107.3333333333333, 52.16666666666666, 22.5, 27.0266916171633, 0.6376366019617309, 1.0, 1.0, 65.0, 37256.17041731504], 
processed observation next is [1.0, 0.391304347826087, 0.4099722991689751, 0.7066666666666666, 0.3577777777777777, 0.05764272559852669, 0.375, 0.7522243014302751, 0.7125455339872436, 1.0, 1.0, 1.0, 0.1774103353205478], 
reward next is 0.8226, 
noisyNet noise sample is [array([0.89423174], dtype=float32), 2.0398383]. 
=============================================
[2019-04-08 15:24:18,211] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[85.34421 ]
 [85.094635]
 [84.98723 ]
 [84.81623 ]
 [84.54254 ]], R is [[85.32289886]
 [85.28887939]
 [85.245224  ]
 [85.19618988]
 [85.14580536]].
[2019-04-08 15:24:18,880] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.0226186e-15 1.0501664e-08 8.8372908e-06 2.6053802e-05 3.4711839e-08
 2.6751353e-11 7.5135728e-07 2.2736274e-13 6.7579425e-17 3.5233349e-18
 9.9996436e-01], sum to 1.0000
[2019-04-08 15:24:18,880] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3219
[2019-04-08 15:24:18,902] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 72.0, 0.0, 0.0, 19.0, 26.1121001929363, 0.4942801103280893, 0.0, 1.0, 65.0, 52091.29169393061], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 694800.0000, 
sim time next is 695400.0000, 
raw observation next is [-3.4, 72.5, 0.0, 0.0, 19.0, 26.13873225269617, 0.4980989488237189, 0.0, 1.0, 65.0, 52212.23112653806], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.725, 0.0, 0.0, 0.08333333333333333, 0.6782276877246808, 0.6660329829412396, 0.0, 1.0, 1.0, 0.24862967203113365], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.39058033], dtype=float32), 0.20360063]. 
=============================================
[2019-04-08 15:24:19,132] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4459869e-17 1.0300537e-09 1.0540344e-05 8.7023305e-05 9.9828412e-10
 2.4781126e-12 1.1025978e-05 2.9132642e-13 4.2227533e-18 7.8426849e-20
 9.9989140e-01], sum to 1.0000
[2019-04-08 15:24:19,133] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0990
[2019-04-08 15:24:19,164] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.25, 46.0, 80.0, 714.0, 22.5, 27.82724257541358, 0.9355582856634962, 1.0, 1.0, 65.0, 26218.28360331447], 
current ob forecast is [], 
actual action is [5.25, 65.0], 
sim time this is 743400.0000, 
sim time next is 744000.0000, 
raw observation next is [0.1666666666666667, 46.33333333333334, 80.83333333333334, 600.1666666666666, 22.5, 27.8494965367025, 0.9375791103583238, 1.0, 1.0, 65.0, 26045.48729455741], 
processed observation next is [1.0, 0.6086956521739131, 0.4672206832871654, 0.46333333333333343, 0.2694444444444445, 0.6631675874769797, 0.375, 0.8207913780585416, 0.8125263701194413, 1.0, 1.0, 1.0, 0.12402612997408291], 
reward next is 0.8760, 
noisyNet noise sample is [array([2.3090823], dtype=float32), -1.3736588]. 
=============================================
[2019-04-08 15:24:19,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[87.13208]
 [87.29388]
 [87.45411]
 [87.60067]
 [87.83782]], R is [[86.99575043]
 [87.00094604]
 [87.00093842]
 [86.99581146]
 [86.99356842]].
[2019-04-08 15:24:19,509] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.22950591e-18 4.14560617e-08 3.15287725e-05 1.18325916e-04
 2.50965426e-09 8.00796755e-12 5.49599690e-08 3.11780156e-14
 3.31408713e-18 8.43952477e-22 9.99850035e-01], sum to 1.0000
[2019-04-08 15:24:19,510] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7458
[2019-04-08 15:24:19,571] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.08333333333333331, 46.66666666666667, 81.66666666666667, 486.3333333333334, 22.5, 27.87281747202734, 0.6854378795762971, 1.0, 1.0, 65.0, 31266.35941341717], 
current ob forecast is [], 
actual action is [5.083333333333333, 65.0], 
sim time this is 744600.0000, 
sim time next is 745200.0000, 
raw observation next is [0.0, 47.0, 82.5, 372.5, 22.5, 27.68882109029802, 0.9448891600277746, 1.0, 1.0, 65.0, 48321.00006574047], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.275, 0.4116022099447514, 0.375, 0.807401757524835, 0.8149630533425914, 1.0, 1.0, 1.0, 0.23010000031304984], 
reward next is 0.7699, 
noisyNet noise sample is [array([2.1904624], dtype=float32), -0.48606953]. 
=============================================
[2019-04-08 15:24:19,784] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3409154e-15 2.1406143e-07 1.4820993e-06 1.4406300e-05 4.8358459e-09
 2.6614520e-11 3.6058057e-06 8.6245967e-12 1.1791731e-15 2.5036201e-18
 9.9998033e-01], sum to 1.0000
[2019-04-08 15:24:19,784] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8591
[2019-04-08 15:24:19,798] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 72.5, 0.0, 0.0, 19.0, 26.13843378661538, 0.4980086370126145, 0.0, 1.0, 65.0, 52215.96073208727], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 695400.0000, 
sim time next is 696000.0000, 
raw observation next is [-3.4, 73.0, 0.0, 0.0, 19.0, 26.16616094325998, 0.4912880820016562, 0.0, 1.0, 65.0, 51266.19891292694], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6805134119383318, 0.663762694000552, 0.0, 1.0, 1.0, 0.24412475672822354], 
reward next is 0.7559, 
noisyNet noise sample is [array([-0.66888624], dtype=float32), 0.90066266]. 
=============================================
[2019-04-08 15:24:19,825] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.80337]
 [75.34871]
 [74.93974]
 [74.2673 ]
 [74.08967]], R is [[76.03807068]
 [76.02903748]
 [76.02067566]
 [76.01359558]
 [75.99778748]].
[2019-04-08 15:24:20,799] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.1019808e-16 5.0299781e-10 8.6886785e-06 2.6839296e-06 5.1500459e-10
 2.6372611e-12 7.0317611e-07 7.8960964e-13 1.1193941e-17 2.3725135e-19
 9.9998796e-01], sum to 1.0000
[2019-04-08 15:24:20,800] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4624
[2019-04-08 15:24:20,813] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 72.33333333333333, 0.0, 0.0, 19.0, 26.03405507224467, 0.5305627991377267, 0.0, 1.0, 65.0, 54270.05548483002], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 794400.0000, 
sim time next is 795000.0000, 
raw observation next is [-7.299999999999999, 71.66666666666667, 0.0, 0.0, 19.0, 26.09711898354715, 0.521043460541733, 0.0, 1.0, 65.0, 52172.69871740296], 
processed observation next is [1.0, 0.17391304347826086, 0.2603878116343491, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.674759915295596, 0.673681153513911, 0.0, 1.0, 1.0, 0.2484414224638236], 
reward next is 0.7516, 
noisyNet noise sample is [array([-1.0455396], dtype=float32), 0.03682848]. 
=============================================
[2019-04-08 15:24:20,818] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.61147]
 [77.58996]
 [77.59763]
 [77.55447]
 [77.53675]], R is [[77.58042145]
 [77.54618835]
 [77.51322174]
 [77.48326874]
 [77.45531464]].
[2019-04-08 15:24:22,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7662490e-17 1.4867341e-09 7.7411113e-07 1.3492442e-07 2.2129194e-11
 2.4293756e-13 2.6869531e-08 9.7620505e-15 3.6758252e-19 5.0101086e-21
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:24:22,414] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5044
[2019-04-08 15:24:22,436] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 66.0, 111.5, 423.5, 22.5, 27.30258985176289, 0.727844845826683, 1.0, 1.0, 65.00000000000004, 33288.71514979679], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 730800.0000, 
sim time next is 731400.0000, 
raw observation next is [-0.6, 64.5, 102.3333333333333, 542.0, 22.5, 27.29406502218704, 0.7517213557423944, 1.0, 1.0, 65.0, 32913.69393941703], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.645, 0.341111111111111, 0.5988950276243094, 0.375, 0.7745054185155867, 0.7505737852474649, 1.0, 1.0, 1.0, 0.15673187590198587], 
reward next is 0.8433, 
noisyNet noise sample is [array([0.5343706], dtype=float32), -0.26801535]. 
=============================================
[2019-04-08 15:24:22,623] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.5694265e-16 1.3423057e-09 1.1934242e-05 1.4792702e-06 3.4781251e-09
 3.6451567e-12 8.2978538e-08 1.4079832e-13 6.5269427e-18 6.4689085e-20
 9.9998653e-01], sum to 1.0000
[2019-04-08 15:24:22,624] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5002
[2019-04-08 15:24:22,687] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 22.5, 27.83980096648894, 0.9039923207595976, 1.0, 1.0, 65.0, 28449.49178875065], 
current ob forecast is [], 
actual action is [3.666666666666667, 65.0], 
sim time this is 750000.0000, 
sim time next is 750600.0000, 
raw observation next is [-1.7, 49.5, 68.0, 3.0, 22.5, 27.82470029816294, 0.7444385013623643, 1.0, 1.0, 65.0, 43337.92406350455], 
processed observation next is [1.0, 0.6956521739130435, 0.4155124653739613, 0.495, 0.22666666666666666, 0.0033149171270718232, 0.375, 0.8187250248469118, 0.7481461671207881, 1.0, 1.0, 1.0, 0.2063710669690693], 
reward next is 0.7936, 
noisyNet noise sample is [array([-0.5167015], dtype=float32), 0.7480807]. 
=============================================
[2019-04-08 15:24:22,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5918530e-16 5.5782567e-10 4.2025295e-06 3.6422725e-06 3.6610601e-10
 2.3455532e-12 1.5648764e-08 1.8382965e-14 2.7272287e-19 2.7904324e-20
 9.9999213e-01], sum to 1.0000
[2019-04-08 15:24:22,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8678
[2019-04-08 15:24:22,759] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 71.0, 98.5, 0.0, 22.5, 26.92542586084552, 0.6564176220776813, 1.0, 1.0, 65.0, 38962.14504369904], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 817200.0000, 
sim time next is 817800.0000, 
raw observation next is [-4.5, 71.0, 102.3333333333333, 0.0, 22.5, 26.93752891476651, 0.6641874770664686, 1.0, 1.0, 65.0, 38884.93881724402], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.341111111111111, 0.0, 0.375, 0.7447940762305425, 0.7213958256888229, 1.0, 1.0, 1.0, 0.18516637532020963], 
reward next is 0.8148, 
noisyNet noise sample is [array([-0.8760184], dtype=float32), -0.17435154]. 
=============================================
[2019-04-08 15:24:22,795] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6045206e-16 6.9381365e-08 1.9220275e-05 3.4648813e-06 3.0605356e-08
 7.8894807e-10 3.0933807e-06 1.2963893e-12 2.0940697e-16 7.5340677e-18
 9.9997413e-01], sum to 1.0000
[2019-04-08 15:24:22,795] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-08 15:24:22,817] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.633333333333333, 56.33333333333333, 0.0, 0.0, 22.5, 26.64542454147883, 0.7073068201134506, 1.0, 1.0, 65.0, 49682.0342074276], 
current ob forecast is [], 
actual action is [0.36666666666666714, 65.0], 
sim time this is 762000.0000, 
sim time next is 762600.0000, 
raw observation next is [-4.816666666666666, 57.16666666666667, 0.0, 0.0, 22.5, 26.58506463325726, 0.7015917523937301, 1.0, 1.0, 65.0, 50593.91192323934], 
processed observation next is [1.0, 0.8260869565217391, 0.32917820867959374, 0.5716666666666668, 0.0, 0.0, 0.375, 0.7154220527714384, 0.7338639174645767, 1.0, 1.0, 1.0, 0.24092339011066352], 
reward next is 0.7591, 
noisyNet noise sample is [array([0.06036374], dtype=float32), -0.3432936]. 
=============================================
[2019-04-08 15:24:26,199] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4313037e-16 8.8835378e-10 4.6723246e-05 5.7802486e-06 2.8255190e-10
 5.0452503e-11 2.3338194e-08 6.2916767e-13 4.3063184e-16 6.3965660e-19
 9.9994743e-01], sum to 1.0000
[2019-04-08 15:24:26,201] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2109
[2019-04-08 15:24:26,237] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 86.0, 54.0, 0.0, 22.5, 27.13798858032398, 0.7150745881384469, 1.0, 1.0, 65.0, 36235.24370866585], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 831600.0000, 
sim time next is 832200.0000, 
raw observation next is [-3.9, 85.33333333333334, 52.33333333333333, 0.0, 22.5, 27.13272345717218, 0.7164910287627005, 1.0, 1.0, 65.0, 36690.98409719262], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8533333333333334, 0.17444444444444443, 0.0, 0.375, 0.7610602880976817, 0.7388303429209002, 1.0, 1.0, 1.0, 0.1747189718913934], 
reward next is 0.8253, 
noisyNet noise sample is [array([3.358173], dtype=float32), -0.73767143]. 
=============================================
[2019-04-08 15:24:26,586] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4227814e-16 5.9940724e-08 2.2633190e-04 1.7528388e-05 5.3022942e-09
 1.8080235e-10 3.3031162e-07 1.3887246e-12 4.7722593e-16 5.5592964e-18
 9.9975580e-01], sum to 1.0000
[2019-04-08 15:24:26,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9279
[2019-04-08 15:24:26,614] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 71.5, 0.0, 0.0, 19.0, 26.06664489153893, 0.5479360818700557, 0.0, 1.0, 25.0, 0.0], 
current ob forecast is [], 
actual action is [-2.8, 65.0], 
sim time this is 785400.0000, 
sim time next is 786000.0000, 
raw observation next is [-7.8, 72.0, 0.0, 0.0, 19.0, 26.03769655184394, 0.5433720769420899, 0.0, 1.0, 65.0, 78349.47596748694], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.72, 0.0, 0.0, 0.08333333333333333, 0.669808045986995, 0.6811240256473633, 0.0, 1.0, 1.0, 0.3730927427023188], 
reward next is 0.6269, 
noisyNet noise sample is [array([-0.75392467], dtype=float32), -0.74332464]. 
=============================================
[2019-04-08 15:24:26,619] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[75.40683 ]
 [76.2555  ]
 [76.111336]
 [76.23998 ]
 [76.31771 ]], R is [[75.2939682 ]
 [75.54103088]
 [75.52384949]
 [75.51242828]
 [75.50650787]].
[2019-04-08 15:24:27,157] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2989724e-16 3.1090061e-10 2.1809956e-06 3.1450953e-05 2.7048648e-09
 1.6689929e-12 2.9506126e-07 2.9256084e-14 3.8156179e-18 3.7663873e-20
 9.9996603e-01], sum to 1.0000
[2019-04-08 15:24:27,157] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6341
[2019-04-08 15:24:27,242] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.483333333333333, 83.5, 0.0, 0.0, 22.5, 26.17286488433557, 0.5889024711291043, 1.0, 1.0, 65.0, 53833.86942435768], 
current ob forecast is [], 
actual action is [1.516666666666667, 65.0], 
sim time this is 849000.0000, 
sim time next is 849600.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 22.5, 26.16565448187304, 0.5847357873376885, 0.0, 1.0, 65.0, 53261.6963917644], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.375, 0.6804712068227534, 0.6949119291125628, 0.0, 1.0, 1.0, 0.25362712567506857], 
reward next is 0.7464, 
noisyNet noise sample is [array([-1.1419374], dtype=float32), -0.13214982]. 
=============================================
[2019-04-08 15:24:27,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1969390e-15 5.8353841e-08 3.5910343e-05 1.7122924e-05 2.7090550e-09
 1.5799622e-11 2.6603932e-06 2.2823984e-12 6.8822128e-17 6.3489255e-18
 9.9994421e-01], sum to 1.0000
[2019-04-08 15:24:27,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4969
[2019-04-08 15:24:27,685] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.433333333333334, 83.0, 0.0, 0.0, 19.0, 26.97082163479216, 0.8295078385964421, 0.0, 1.0, 65.0, 38532.73991748422], 
current ob forecast is [], 
actual action is [13.433333333333334, 65.0], 
sim time this is 967200.0000, 
sim time next is 967800.0000, 
raw observation next is [8.616666666666667, 83.0, 0.0, 0.0, 19.0, 27.00575522943861, 0.8273747345440658, 0.0, 1.0, 65.0, 37372.31651920706], 
processed observation next is [1.0, 0.17391304347826086, 0.7012927054478302, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7504796024532174, 0.7757915781813552, 0.0, 1.0, 1.0, 0.1779634119962241], 
reward next is 0.8220, 
noisyNet noise sample is [array([1.0188897], dtype=float32), 0.24174704]. 
=============================================
[2019-04-08 15:24:28,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7650397e-15 1.2688957e-08 1.9961362e-05 2.1020953e-04 8.2812573e-10
 1.6793992e-11 2.8830304e-06 1.3806847e-12 6.5273017e-18 3.6083074e-19
 9.9976689e-01], sum to 1.0000
[2019-04-08 15:24:28,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4927
[2019-04-08 15:24:28,588] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 85.33333333333334, 19.33333333333334, 0.0, 22.5, 27.10672347632643, 0.720383499211969, 1.0, 1.0, 65.0, 38510.68053548906], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 838200.0000, 
sim time next is 838800.0000, 
raw observation next is [-3.9, 86.0, 14.5, 0.0, 22.5, 27.10674926776623, 0.7165049580175052, 1.0, 1.0, 65.0, 37975.85412057133], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.86, 0.04833333333333333, 0.0, 0.375, 0.7588957723138524, 0.7388349860058351, 1.0, 1.0, 1.0, 0.1808374005741492], 
reward next is 0.8192, 
noisyNet noise sample is [array([1.0216918], dtype=float32), -0.9328773]. 
=============================================
[2019-04-08 15:24:30,609] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2742489e-15 1.9914492e-09 2.5139766e-06 1.0348247e-05 9.2530650e-10
 7.4997831e-13 8.4594677e-07 2.9299103e-15 1.5080489e-17 2.1784095e-18
 9.9998629e-01], sum to 1.0000
[2019-04-08 15:24:30,610] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5696
[2019-04-08 15:24:30,630] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 26.19124070584724, 0.5613851650275444, 0.0, 1.0, 65.0, 49787.31849507116], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 874800.0000, 
sim time next is 875400.0000, 
raw observation next is [-1.616666666666667, 78.50000000000001, 0.0, 0.0, 19.0, 26.19503894630144, 0.5622715312501672, 0.0, 1.0, 65.0, 49534.65676006135], 
processed observation next is [1.0, 0.13043478260869565, 0.4178208679593721, 0.7850000000000001, 0.0, 0.0, 0.08333333333333333, 0.6829199121917867, 0.6874238437500558, 0.0, 1.0, 1.0, 0.23587931790505404], 
reward next is 0.7641, 
noisyNet noise sample is [array([0.6802548], dtype=float32), 1.5434548]. 
=============================================
[2019-04-08 15:24:30,867] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8589164e-14 2.4571813e-09 6.9943039e-06 6.9713657e-05 8.9659080e-09
 2.6830496e-11 1.2806393e-06 1.5885571e-11 3.5113555e-16 2.9815580e-18
 9.9992204e-01], sum to 1.0000
[2019-04-08 15:24:30,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5241
[2019-04-08 15:24:30,918] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.1, 79.66666666666667, 0.0, 0.0, 19.0, 26.27391718991768, 0.5707601645239865, 0.0, 1.0, 65.0, 48405.33662667646], 
current ob forecast is [], 
actual action is [2.9, 65.0], 
sim time this is 868800.0000, 
sim time next is 869400.0000, 
raw observation next is [-2.0, 79.5, 0.0, 0.0, 19.0, 26.2485846841132, 0.5790162660638948, 0.0, 1.0, 65.0, 49362.6763429526], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.795, 0.0, 0.0, 0.08333333333333333, 0.6873820570094334, 0.6930054220212982, 0.0, 1.0, 1.0, 0.23506036353786955], 
reward next is 0.7649, 
noisyNet noise sample is [array([0.40184093], dtype=float32), -0.58992606]. 
=============================================
[2019-04-08 15:24:31,197] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3930213e-17 6.6430911e-10 2.4066558e-05 7.2217508e-06 6.2987088e-11
 1.4140409e-12 5.1035439e-08 1.0603693e-12 2.8030653e-18 9.9973398e-20
 9.9996865e-01], sum to 1.0000
[2019-04-08 15:24:31,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4814
[2019-04-08 15:24:31,308] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 22.5, 26.16546490205268, 0.5846835758104469, 0.0, 1.0, 65.0, 53263.59436530741], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 849600.0000, 
sim time next is 850200.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 26.15068090861762, 0.5818160865850426, 0.0, 1.0, 65.0, 53465.85297558009], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6792234090514683, 0.6939386955283475, 0.0, 1.0, 1.0, 0.2545992998837147], 
reward next is 0.7454, 
noisyNet noise sample is [array([-1.0722376], dtype=float32), -0.030933773]. 
=============================================
[2019-04-08 15:24:31,448] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-08 15:24:31,450] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:24:31,450] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:24:31,451] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:24:31,453] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:24:31,453] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:24:31,454] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run9
[2019-04-08 15:24:31,455] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:24:31,483] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run9
[2019-04-08 15:24:31,506] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run9
[2019-04-08 15:25:03,897] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.058042053]
[2019-04-08 15:25:03,898] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [0.1, 58.0, 152.5, 541.5, 19.0, 26.96123402129716, 0.8063987466618897, 0.0, 1.0, 65.0, 32234.73919279666]
[2019-04-08 15:25:03,898] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:25:03,898] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [1.4002188e-15 4.6028075e-09 6.2460522e-06 1.5379193e-05 1.8866229e-09
 1.6038318e-11 6.7269838e-07 8.2489679e-13 1.8323136e-16 1.2750139e-18
 9.9997771e-01], sampled 0.8500788759605492
[2019-04-08 15:25:31,695] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.058042053]
[2019-04-08 15:25:31,695] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-6.0, 59.0, 0.0, 0.0, 19.0, 26.25330123634861, 0.5466514984077572, 0.0, 1.0, 65.0, 50434.88814812985]
[2019-04-08 15:25:31,695] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:25:31,696] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [7.4344136e-14 3.0528213e-08 2.1268830e-05 5.2609194e-05 2.1883336e-08
 2.2822942e-10 3.8455237e-06 1.8011568e-11 7.5234085e-15 1.1665636e-16
 9.9992228e-01], sampled 0.546555897154812
[2019-04-08 15:26:03,900] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.1483 316228865.2349 2958.0533
[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:03,931] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:04,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,506] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.0901 355931073.9494 2370.6153
[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,527] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:10,674] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,055] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3481 342856907.0426 2767.9989
[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:26:15,077] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 160000, evaluation results [160000.0, 6863.348061701706, 342856907.0426486, 2767.9989478961493, 6990.148260786054, 316228865.2349291, 2958.053298624441, 6801.09012405027, 355931073.9494421, 2370.6152837986697]
[2019-04-08 15:26:15,244] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.7068318e-15 2.4863505e-09 7.1322465e-05 3.6686272e-06 6.9634671e-09
 3.8488018e-11 6.6072170e-07 1.4351634e-12 8.2932694e-16 4.2757359e-18
 9.9992430e-01], sum to 1.0000
[2019-04-08 15:26:15,248] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7924
[2019-04-08 15:26:15,266] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4039896e-15 4.6739475e-09 8.7384196e-06 4.1057788e-06 8.7833263e-10
 4.3733694e-11 1.2780928e-06 1.4933198e-12 4.3463902e-16 1.8427359e-18
 9.9998581e-01], sum to 1.0000
[2019-04-08 15:26:15,266] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0714
[2019-04-08 15:26:15,269] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 72.0, 0.0, 0.0, 19.0, 26.23761400809059, 0.5760450645453389, 0.0, 1.0, 65.0, 48960.36254638543], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 882000.0000, 
sim time next is 882600.0000, 
raw observation next is [-0.5, 72.0, 0.0, 0.0, 19.0, 26.32364530456444, 0.5790694224603575, 0.0, 1.0, 65.0, 46395.24291926595], 
processed observation next is [1.0, 0.21739130434782608, 0.44875346260387816, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6936371087137033, 0.6930231408201192, 0.0, 1.0, 1.0, 0.2209297281869807], 
reward next is 0.7791, 
noisyNet noise sample is [array([-0.9526374], dtype=float32), 0.8947741]. 
=============================================
[2019-04-08 15:26:15,301] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 19.0, 26.10967113501774, 0.571946467368646, 0.0, 1.0, 65.0, 53790.04082355741], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 853800.0000, 
sim time next is 854400.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 19.0, 26.10092439658653, 0.570051865507011, 0.0, 1.0, 65.0, 53833.26522730495], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6750770330488777, 0.6900172885023371, 0.0, 1.0, 1.0, 0.2563488820347855], 
reward next is 0.7437, 
noisyNet noise sample is [array([0.16029304], dtype=float32), 1.5243387]. 
=============================================
[2019-04-08 15:26:15,318] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6695942e-17 1.6996438e-10 8.8064053e-06 7.1196467e-07 1.5419746e-10
 7.5394014e-12 3.2005948e-07 7.3499353e-15 1.0193810e-18 2.5309893e-21
 9.9999011e-01], sum to 1.0000
[2019-04-08 15:26:15,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7553
[2019-04-08 15:26:15,349] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.67623980276997, 1.049090138907386, 0.0, 1.0, 65.0, 27381.32995693024], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1036800.0000, 
sim time next is 1037400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.67370984236825, 1.06097952592468, 0.0, 1.0, 65.0, 27352.34955398663], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8061424868640209, 0.8536598419748933, 0.0, 1.0, 1.0, 0.13024928359041252], 
reward next is 0.8698, 
noisyNet noise sample is [array([0.2529844], dtype=float32), 0.48399884]. 
=============================================
[2019-04-08 15:26:15,367] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5314890e-17 3.1654459e-09 9.7911716e-08 1.1530985e-06 8.3441747e-11
 1.0751332e-13 2.1295794e-08 4.5801117e-15 6.0158823e-20 1.2231982e-21
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:26:15,369] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9416
[2019-04-08 15:26:15,384] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.4, 96.66666666666666, 0.0, 0.0, 22.5, 27.29261771368539, 0.8530278935577992, 1.0, 1.0, 65.0, 32436.16157675139], 
current ob forecast is [], 
actual action is [9.4, 65.0], 
sim time this is 929400.0000, 
sim time next is 930000.0000, 
raw observation next is [4.4, 97.33333333333333, 0.0, 0.0, 22.5, 27.14934211629193, 0.8465225754498094, 1.0, 1.0, 65.0, 38116.03610289084], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9733333333333333, 0.0, 0.0, 0.375, 0.7624451763576608, 0.7821741918166031, 1.0, 1.0, 1.0, 0.1815049338232897], 
reward next is 0.8185, 
noisyNet noise sample is [array([1.204621], dtype=float32), -0.05214638]. 
=============================================
[2019-04-08 15:26:15,400] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[87.17108 ]
 [87.19716 ]
 [87.193954]
 [87.2068  ]
 [87.26532 ]], R is [[87.15768433]
 [87.13165283]
 [87.11306   ]
 [87.06344604]
 [86.95514679]].
[2019-04-08 15:26:15,635] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0795461e-17 1.0016034e-09 1.9936583e-06 5.0283252e-06 1.5153918e-09
 5.4911414e-13 1.8889065e-08 7.4225938e-15 3.4722788e-18 1.0552384e-19
 9.9999297e-01], sum to 1.0000
[2019-04-08 15:26:15,636] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2881
[2019-04-08 15:26:15,676] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 81.33333333333334, 44.83333333333333, 0.0, 22.5, 27.03367659325904, 0.6768258827744947, 1.0, 1.0, 65.0, 35461.8128306478], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 897600.0000, 
sim time next is 898200.0000, 
raw observation next is [1.1, 82.0, 48.0, 0.0, 22.5, 27.00277975190322, 0.6942753973084246, 1.0, 1.0, 65.0, 36462.11900730385], 
processed observation next is [1.0, 0.391304347826087, 0.49307479224376743, 0.82, 0.16, 0.0, 0.375, 0.7502316459919349, 0.7314251324361415, 1.0, 1.0, 1.0, 0.17362913813001835], 
reward next is 0.8264, 
noisyNet noise sample is [array([1.1148316], dtype=float32), 0.8177117]. 
=============================================
[2019-04-08 15:26:15,993] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4279881e-18 1.8542480e-11 1.2930082e-07 3.9263364e-06 1.1830546e-12
 4.2344149e-14 1.1245720e-08 3.2696660e-16 1.7270956e-20 5.8153526e-23
 9.9999595e-01], sum to 1.0000
[2019-04-08 15:26:15,998] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8121
[2019-04-08 15:26:16,011] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.9, 90.5, 97.0, 0.0, 22.5, 27.34513003867164, 0.7771393072972005, 1.0, 1.0, 65.0, 32109.79027173902], 
current ob forecast is [], 
actual action is [6.9, 65.0], 
sim time this is 905400.0000, 
sim time next is 906000.0000, 
raw observation next is [2.166666666666667, 92.66666666666666, 98.16666666666667, 0.0, 22.5, 27.38277479242093, 0.7805091275741493, 1.0, 1.0, 65.0, 31368.61729521249], 
processed observation next is [1.0, 0.4782608695652174, 0.5226223453370269, 0.9266666666666665, 0.32722222222222225, 0.0, 0.375, 0.7818978993684107, 0.7601697091913832, 1.0, 1.0, 1.0, 0.14937436807244042], 
reward next is 0.8506, 
noisyNet noise sample is [array([-0.4064926], dtype=float32), -1.0138459]. 
=============================================
[2019-04-08 15:26:16,016] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.10321 ]
 [82.85066 ]
 [82.55186 ]
 [82.31141 ]
 [82.083954]], R is [[83.42910767]
 [83.44191742]
 [83.44815063]
 [83.4549942 ]
 [83.46663666]].
[2019-04-08 15:26:16,146] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.0356269e-19 2.2867969e-10 8.2696289e-08 2.6003809e-06 2.2605372e-11
 5.5742747e-14 9.3603752e-08 1.3852850e-15 8.7062569e-20 7.9103904e-23
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:26:16,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2683
[2019-04-08 15:26:16,163] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.4, 93.0, 72.0, 0.0, 22.5, 27.65528030617448, 0.8918810714147738, 1.0, 1.0, 65.0, 28252.30642740552], 
current ob forecast is [], 
actual action is [9.4, 65.0], 
sim time this is 918000.0000, 
sim time next is 918600.0000, 
raw observation next is [4.4, 93.0, 66.0, 0.0, 22.5, 27.6646984904312, 0.8941139917949498, 1.0, 1.0, 65.0, 27488.88364047874], 
processed observation next is [1.0, 0.6521739130434783, 0.5844875346260389, 0.93, 0.22, 0.0, 0.375, 0.8053915408692666, 0.7980379972649833, 1.0, 1.0, 1.0, 0.1308994459070416], 
reward next is 0.8691, 
noisyNet noise sample is [array([0.5833652], dtype=float32), 0.85131836]. 
=============================================
[2019-04-08 15:26:16,443] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.3389656e-14 2.9566019e-09 3.3948240e-07 1.9112465e-05 4.9303651e-11
 6.5600829e-13 5.6286817e-06 1.1146277e-11 1.5311789e-16 4.0653168e-20
 9.9997497e-01], sum to 1.0000
[2019-04-08 15:26:16,447] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4895
[2019-04-08 15:26:16,464] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 26.16494634724824, 0.5634281412255527, 0.0, 1.0, 65.0, 51012.6030540461], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 874200.0000, 
sim time next is 874800.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 19.0, 26.19125459030805, 0.5613892784029755, 0.0, 1.0, 65.0, 49787.15589954132], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.79, 0.0, 0.0, 0.08333333333333333, 0.6826045491923374, 0.6871297594676585, 0.0, 1.0, 1.0, 0.23708169475972057], 
reward next is 0.7629, 
noisyNet noise sample is [array([1.2711387], dtype=float32), -1.3841392]. 
=============================================
[2019-04-08 15:26:17,151] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.2683947e-17 8.0558371e-10 2.9608880e-06 5.4057782e-06 4.1620274e-10
 5.9598301e-13 7.2513991e-08 2.1075156e-14 5.7671348e-19 9.8822937e-21
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:26:17,159] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4113
[2019-04-08 15:26:17,173] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 19.0, 27.08072349032131, 0.8351806673994281, 0.0, 1.0, 65.0, 36090.94285206683], 
current ob forecast is [], 
actual action is [13.8, 65.0], 
sim time this is 969600.0000, 
sim time next is 970200.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 19.0, 27.06495096616393, 0.829747592804957, 0.0, 1.0, 65.0, 36807.67167073427], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7554125805136609, 0.7765825309349856, 0.0, 1.0, 1.0, 0.17527462700349652], 
reward next is 0.8247, 
noisyNet noise sample is [array([-0.46237677], dtype=float32), 0.17576863]. 
=============================================
[2019-04-08 15:26:17,457] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.47096255e-19 2.25372759e-11 1.00647526e-07 2.19814694e-07
 6.06557443e-11 6.59992623e-14 2.36167370e-08 4.06739530e-16
 3.49513122e-19 6.87087372e-23 9.99999642e-01], sum to 1.0000
[2019-04-08 15:26:17,458] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3703
[2019-04-08 15:26:17,481] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.7, 100.0, 0.0, 0.0, 22.5, 26.94935640131362, 0.8216471532510227, 0.0, 1.0, 65.0, 40152.012289445], 
current ob forecast is [], 
actual action is [9.7, 65.0], 
sim time this is 934200.0000, 
sim time next is 934800.0000, 
raw observation next is [4.8, 100.0, 0.0, 0.0, 22.5, 26.88171470824028, 0.817488873247966, 0.0, 1.0, 65.0, 41918.8867844803], 
processed observation next is [1.0, 0.8260869565217391, 0.5955678670360112, 1.0, 0.0, 0.0, 0.375, 0.7401428923533567, 0.7724962910826553, 0.0, 1.0, 1.0, 0.19961374659276335], 
reward next is 0.8004, 
noisyNet noise sample is [array([-1.7265395], dtype=float32), 1.7193685]. 
=============================================
[2019-04-08 15:26:17,595] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5808099e-15 7.8107470e-10 3.0427254e-06 2.4673427e-06 7.8611301e-10
 1.6881731e-12 2.5788219e-07 2.5156139e-13 1.6001991e-17 5.9502587e-21
 9.9999428e-01], sum to 1.0000
[2019-04-08 15:26:17,597] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5463
[2019-04-08 15:26:17,625] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.600000000000001, 89.66666666666667, 0.0, 0.0, 22.5, 27.10566460087239, 0.851267583426084, 1.0, 1.0, 65.0, 37395.26534783014], 
current ob forecast is [], 
actual action is [14.600000000000001, 65.0], 
sim time this is 978000.0000, 
sim time next is 978600.0000, 
raw observation next is [9.5, 91.33333333333333, 8.999999999999998, 0.0, 22.5, 27.09321852524166, 0.8573386894579049, 1.0, 1.0, 65.0, 37452.9978763994], 
processed observation next is [1.0, 0.30434782608695654, 0.7257617728531857, 0.9133333333333333, 0.029999999999999995, 0.0, 0.375, 0.757768210436805, 0.785779563152635, 1.0, 1.0, 1.0, 0.17834760893523524], 
reward next is 0.8217, 
noisyNet noise sample is [array([0.9062363], dtype=float32), -0.27497554]. 
=============================================
[2019-04-08 15:26:17,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.88718195e-16 3.27496386e-09 4.13859624e-07 1.71891975e-06
 2.94509583e-10 9.77588288e-11 1.13652284e-07 1.95035221e-12
 1.03374928e-17 1.87807726e-19 9.99997735e-01], sum to 1.0000
[2019-04-08 15:26:17,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7146
[2019-04-08 15:26:17,715] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.416666666666667, 90.16666666666667, 0.0, 0.0, 19.0, 26.80341400151245, 0.8000964760748014, 0.0, 1.0, 65.0, 41929.10141810661], 
current ob forecast is [], 
actual action is [10.416666666666668, 65.0], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [5.5, 89.0, 0.0, 0.0, 19.0, 26.78152194046967, 0.8040597686703096, 0.0, 1.0, 65.0, 42242.04092365405], 
processed observation next is [1.0, 0.043478260869565216, 0.6149584487534627, 0.89, 0.0, 0.0, 0.08333333333333333, 0.7317934950391392, 0.7680199228901032, 0.0, 1.0, 1.0, 0.20115257582692406], 
reward next is 0.7988, 
noisyNet noise sample is [array([1.4638379], dtype=float32), -1.1099542]. 
=============================================
[2019-04-08 15:26:17,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.219475]
 [83.11835 ]
 [83.43112 ]
 [83.524864]
 [84.0319  ]], R is [[83.09160614]
 [83.06102753]
 [83.03856659]
 [83.01734924]
 [82.98600769]].
[2019-04-08 15:26:18,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.5548225e-19 8.9999758e-11 8.1882916e-07 1.0368477e-06 4.3734731e-11
 3.5658463e-13 5.6645092e-08 2.5477076e-15 1.1256261e-19 5.8504376e-21
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:26:18,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8539
[2019-04-08 15:26:18,120] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.5, 91.33333333333333, 8.999999999999998, 0.0, 22.5, 27.09322533070251, 0.8573406533802387, 1.0, 1.0, 65.0, 37452.91919858352], 
current ob forecast is [], 
actual action is [14.5, 65.0], 
sim time this is 978600.0000, 
sim time next is 979200.0000, 
raw observation next is [9.4, 93.0, 13.5, 0.0, 22.5, 27.11261017290813, 0.8625253948081824, 1.0, 1.0, 65.0, 36671.84370326892], 
processed observation next is [1.0, 0.34782608695652173, 0.7229916897506927, 0.93, 0.045, 0.0, 0.375, 0.7593841810756775, 0.7875084649360607, 1.0, 1.0, 1.0, 0.17462782715842343], 
reward next is 0.8254, 
noisyNet noise sample is [array([-0.04610761], dtype=float32), 1.8896738]. 
=============================================
[2019-04-08 15:26:18,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5844318e-17 7.0561610e-09 3.7572401e-05 2.1028382e-05 6.2223267e-09
 4.1327289e-12 1.7859759e-06 5.1498877e-13 3.1909051e-17 5.3613965e-19
 9.9993956e-01], sum to 1.0000
[2019-04-08 15:26:18,730] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5660
[2019-04-08 15:26:18,747] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.6, 82.0, 0.0, 0.0, 19.0, 27.00333533639584, 0.8237551425105484, 0.0, 1.0, 65.0, 36839.4465507955], 
current ob forecast is [], 
actual action is [11.6, 65.0], 
sim time this is 957600.0000, 
sim time next is 958200.0000, 
raw observation next is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 19.0, 27.01699457260469, 0.8239898862305067, 0.0, 1.0, 65.0, 38046.48652197082], 
processed observation next is [1.0, 0.08695652173913043, 0.6505078485687905, 0.8166666666666668, 0.0, 0.0, 0.08333333333333333, 0.7514162143837243, 0.774663295410169, 0.0, 1.0, 1.0, 0.1811737453427182], 
reward next is 0.8188, 
noisyNet noise sample is [array([0.52720463], dtype=float32), 0.02896877]. 
=============================================
[2019-04-08 15:26:19,300] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.8818244e-17 6.9586122e-11 5.3060046e-07 4.7736785e-06 8.1369122e-12
 2.7002716e-13 7.4415394e-09 1.3329231e-16 2.9124794e-18 4.1092855e-21
 9.9999464e-01], sum to 1.0000
[2019-04-08 15:26:19,300] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3792
[2019-04-08 15:26:19,327] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.7, 94.0, 0.0, 0.0, 22.5, 26.8665220185695, 0.8913942907376757, 1.0, 1.0, 65.0, 62088.05772932632], 
current ob forecast is [], 
actual action is [9.7, 65.0], 
sim time this is 927000.0000, 
sim time next is 927600.0000, 
raw observation next is [4.600000000000001, 94.66666666666667, 0.0, 0.0, 22.5, 26.17125642117141, 0.7938365084356107, 1.0, 1.0, 65.0, 49909.54512464904], 
processed observation next is [1.0, 0.7391304347826086, 0.5900277008310251, 0.9466666666666668, 0.0, 0.0, 0.375, 0.6809380350976175, 0.7646121694785369, 1.0, 1.0, 1.0, 0.23766450059356686], 
reward next is 0.7623, 
noisyNet noise sample is [array([-0.4748586], dtype=float32), 0.77266884]. 
=============================================
[2019-04-08 15:26:20,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1571565e-18 1.1086265e-09 3.5230114e-06 4.2273135e-05 1.3114814e-09
 6.6366808e-12 1.4713748e-07 1.0865057e-13 9.5931891e-18 1.8281126e-20
 9.9995410e-01], sum to 1.0000
[2019-04-08 15:26:20,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6389
[2019-04-08 15:26:20,428] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.74053295342659, 1.05116814404484, 0.0, 1.0, 65.0, 26868.13006068353], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1032000.0000, 
sim time next is 1032600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.71609186277719, 1.055979172868791, 0.0, 1.0, 65.0, 27443.32646890253], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8096743218980992, 0.8519930576229303, 0.0, 1.0, 1.0, 0.13068250699477396], 
reward next is 0.8693, 
noisyNet noise sample is [array([-0.38406098], dtype=float32), 0.108144104]. 
=============================================
[2019-04-08 15:26:20,555] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.3958231e-20 1.5098284e-10 7.2354686e-07 6.9775859e-07 6.4372611e-11
 1.9943423e-14 4.1807183e-08 8.8867785e-16 2.5125516e-20 1.4341920e-23
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:26:20,561] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7532
[2019-04-08 15:26:20,580] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.7, 86.0, 112.0, 0.0, 22.5, 28.22746687165881, 1.075187946654394, 1.0, 1.0, 65.0, 20294.21235314566], 
current ob forecast is [], 
actual action is [16.7, 65.0], 
sim time this is 990600.0000, 
sim time next is 991200.0000, 
raw observation next is [11.8, 86.0, 116.0, 0.0, 22.5, 28.22123154479166, 1.085768826674251, 1.0, 1.0, 65.0, 20807.85325675683], 
processed observation next is [1.0, 0.4782608695652174, 0.7894736842105264, 0.86, 0.38666666666666666, 0.0, 0.375, 0.851769295399305, 0.8619229422247502, 1.0, 1.0, 1.0, 0.09908501550836585], 
reward next is 0.9009, 
noisyNet noise sample is [array([0.06665372], dtype=float32), 2.3287134]. 
=============================================
[2019-04-08 15:26:20,817] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0391997e-18 3.9567566e-10 1.8807567e-06 2.0054213e-05 2.8134989e-10
 1.8431335e-13 3.0320803e-07 8.6367471e-14 2.0223214e-17 9.5022736e-21
 9.9997771e-01], sum to 1.0000
[2019-04-08 15:26:20,818] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7210
[2019-04-08 15:26:20,834] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 78.33333333333333, 0.0, 0.0, 22.5, 27.57618159219654, 1.027650993148903, 1.0, 1.0, 65.0, 27652.69196921534], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1021200.0000, 
sim time next is 1021800.0000, 
raw observation next is [14.4, 77.66666666666667, 0.0, 0.0, 22.5, 27.63384048203741, 1.042792855715634, 1.0, 1.0, 65.0, 28829.50839215889], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7766666666666667, 0.0, 0.0, 0.375, 0.8028200401697841, 0.847597618571878, 1.0, 1.0, 1.0, 0.13728337329599472], 
reward next is 0.8627, 
noisyNet noise sample is [array([-1.0164193], dtype=float32), -1.1409256]. 
=============================================
[2019-04-08 15:26:20,905] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.20294289e-18 3.32161243e-10 1.12490618e-07 7.71245936e-07
 5.62740618e-11 7.60967624e-13 3.18386043e-07 3.10163729e-15
 1.08150595e-19 4.42589328e-21 9.99998808e-01], sum to 1.0000
[2019-04-08 15:26:20,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4471
[2019-04-08 15:26:20,965] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 22.5, 27.9883419640424, 1.121866475617469, 1.0, 1.0, 65.0, 23988.55638058319], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1018200.0000, 
sim time next is 1018800.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 22.5, 27.99863383723029, 0.9323311157515732, 0.0, 1.0, 65.0, 38755.10444113391], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.81, 0.0, 0.0, 0.375, 0.8332194864358575, 0.8107770385838577, 0.0, 1.0, 1.0, 0.18454811638635196], 
reward next is 0.8155, 
noisyNet noise sample is [array([0.73138064], dtype=float32), 1.4851876]. 
=============================================
[2019-04-08 15:26:21,252] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3490081e-18 1.9362263e-10 4.5943034e-07 1.0362975e-06 6.0480995e-11
 2.1997020e-13 3.6989263e-08 6.1635886e-15 1.3755847e-18 3.5441982e-20
 9.9999845e-01], sum to 1.0000
[2019-04-08 15:26:21,258] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1205
[2019-04-08 15:26:21,276] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 19.0, 27.87060790772619, 1.082132503509513, 0.0, 1.0, 65.0, 23887.9300439106], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1048200.0000, 
sim time next is 1048800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 19.0, 27.93102328926661, 1.082589154984098, 0.0, 1.0, 65.0, 22585.06266159412], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8275852741055507, 0.860863051661366, 0.0, 1.0, 1.0, 0.10754791743616247], 
reward next is 0.8925, 
noisyNet noise sample is [array([-0.5510625], dtype=float32), 0.3855951]. 
=============================================
[2019-04-08 15:26:21,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0344658e-20 1.6083768e-11 7.2819375e-08 6.0472644e-08 8.1447918e-13
 1.2959832e-15 4.6005839e-09 3.9355885e-14 4.4676267e-21 4.1037573e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:26:21,433] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0315
[2019-04-08 15:26:21,442] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 81.0, 106.5, 0.0, 22.5, 28.27481713509903, 1.133122877028677, 1.0, 1.0, 65.0, 18845.76419714228], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1000800.0000, 
sim time next is 1001400.0000, 
raw observation next is [14.4, 81.0, 102.3333333333333, 0.0, 22.5, 28.49098579232965, 1.147888015321383, 1.0, 1.0, 65.0, 18847.2717997383], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.341111111111111, 0.0, 0.375, 0.8742488160274707, 0.8826293384404611, 1.0, 1.0, 1.0, 0.08974891333208715], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.10511505], dtype=float32), -2.0802217]. 
=============================================
[2019-04-08 15:26:23,335] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2678048e-20 9.4832410e-13 5.4854965e-09 2.0487651e-08 2.5450211e-12
 2.0273507e-15 9.1409946e-10 1.6092136e-16 1.2793230e-21 1.7779035e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:26:23,340] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9998
[2019-04-08 15:26:23,362] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.76666666666667, 79.0, 63.16666666666666, 0.0, 22.5, 28.34319664566075, 1.148860927554308, 1.0, 1.0, 64.99999999999997, 18846.22139038475], 
current ob forecast is [], 
actual action is [19.76666666666667, 65.0], 
sim time this is 1005600.0000, 
sim time next is 1006200.0000, 
raw observation next is [14.95, 78.0, 57.0, 0.0, 22.5, 28.61424452388057, 1.16203117279879, 1.0, 1.0, 65.0, 18849.22838549053], 
processed observation next is [1.0, 0.6521739130434783, 0.8767313019390582, 0.78, 0.19, 0.0, 0.375, 0.8845203769900474, 0.8873437242662634, 1.0, 1.0, 1.0, 0.08975823040709775], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.10937582], dtype=float32), 0.6902939]. 
=============================================
[2019-04-08 15:26:23,492] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8960829e-17 2.6813612e-10 8.0985512e-07 1.4712027e-06 3.5915070e-11
 4.5380379e-15 7.5744531e-09 2.6833306e-14 3.7517387e-19 5.2377168e-22
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:26:23,496] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9887
[2019-04-08 15:26:23,505] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 22.5, 29.91311871073461, 1.539796449328627, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [24.11666666666667, 65.0], 
sim time this is 1095000.0000, 
sim time next is 1095600.0000, 
raw observation next is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 22.5, 29.94212542730359, 1.541397250095967, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9843028624192061, 0.4933333333333334, 0.14833333333333334, 0.0, 0.375, 0.9951771189419659, 1.0137990833653223, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07410832], dtype=float32), 0.41194913]. 
=============================================
[2019-04-08 15:26:24,102] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.9297958e-17 1.9764482e-10 2.4630256e-06 9.6983371e-05 4.5164097e-10
 4.5802017e-12 2.6366582e-08 5.6174290e-14 9.6824197e-17 4.8591538e-21
 9.9990046e-01], sum to 1.0000
[2019-04-08 15:26:24,107] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9706
[2019-04-08 15:26:24,132] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.75442809439998, 1.076202842049139, 0.0, 1.0, 65.0, 25155.56312682089], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1038000.0000, 
sim time next is 1038600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.88504539032369, 1.06782462574209, 0.0, 1.0, 65.0, 22424.93514062273], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8237537825269742, 0.85594154191403, 0.0, 1.0, 1.0, 0.1067854054315368], 
reward next is 0.8932, 
noisyNet noise sample is [array([-0.3303426], dtype=float32), 0.7121296]. 
=============================================
[2019-04-08 15:26:24,326] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5679060e-18 7.4746076e-10 4.4852285e-08 1.6752263e-06 1.5613118e-11
 2.0030677e-14 2.6597274e-08 8.8250923e-15 5.5488361e-20 1.5015764e-22
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:26:24,327] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4897
[2019-04-08 15:26:24,338] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.1, 53.0, 0.0, 0.0, 22.5, 29.0003831430887, 1.382499931074057, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [21.1, 65.0], 
sim time this is 1101600.0000, 
sim time next is 1102200.0000, 
raw observation next is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 22.5, 28.9100184961837, 1.385540993738529, 1.0, 1.0, 65.0, 18848.5481632407], 
processed observation next is [1.0, 0.782608695652174, 0.9035087719298247, 0.5366666666666667, 0.0, 0.0, 0.375, 0.9091682080153083, 0.961846997912843, 1.0, 1.0, 1.0, 0.08975499125352714], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.8887124], dtype=float32), 0.83784485]. 
=============================================
[2019-04-08 15:26:24,388] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3699095e-18 4.5661785e-10 2.9166677e-07 3.9555087e-07 2.0332319e-11
 1.6331165e-13 2.3347241e-08 3.2758584e-15 3.9594907e-18 7.2759673e-22
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:26:24,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7262
[2019-04-08 15:26:24,401] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.46666666666667, 61.33333333333333, 0.0, 0.0, 19.0, 28.19556257035839, 1.275244361367752, 0.0, 1.0, 65.0, 18844.33279491435], 
current ob forecast is [], 
actual action is [18.46666666666667, 65.0], 
sim time this is 1111200.0000, 
sim time next is 1111800.0000, 
raw observation next is [13.38333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 28.17353356326277, 1.27135319118402, 0.0, 1.0, 65.0, 19272.56551510479], 
processed observation next is [1.0, 0.8695652173913043, 0.8333333333333334, 0.6166666666666667, 0.0, 0.0, 0.08333333333333333, 0.8477944636052307, 0.9237843970613401, 0.0, 1.0, 1.0, 0.091774121500499], 
reward next is 0.9082, 
noisyNet noise sample is [array([-0.02996776], dtype=float32), -1.8887608]. 
=============================================
[2019-04-08 15:26:24,611] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1990696e-18 1.3794265e-09 4.6895914e-07 1.3351492e-06 1.4300214e-10
 3.7733790e-14 2.1457535e-07 7.0225947e-15 4.4580064e-18 5.4030378e-20
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:26:24,613] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6953
[2019-04-08 15:26:24,622] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.28333333333333, 65.66666666666666, 0.0, 0.0, 19.0, 28.02434556740307, 1.229840277667395, 0.0, 1.0, 65.0, 24252.06174701938], 
current ob forecast is [], 
actual action is [17.28333333333333, 65.0], 
sim time this is 1119000.0000, 
sim time next is 1119600.0000, 
raw observation next is [12.2, 66.0, 0.0, 0.0, 19.0, 28.01389903513083, 1.226692840976422, 0.0, 1.0, 65.0, 24401.48015422371], 
processed observation next is [1.0, 1.0, 0.8005540166204987, 0.66, 0.0, 0.0, 0.08333333333333333, 0.8344915862609025, 0.9088976136588073, 0.0, 1.0, 1.0, 0.11619752454392243], 
reward next is 0.8838, 
noisyNet noise sample is [array([0.26526454], dtype=float32), 1.308772]. 
=============================================
[2019-04-08 15:26:24,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.71780970e-17 2.17793628e-09 1.28316708e-06 1.20404297e-06
 1.29614985e-08 2.30556405e-13 3.43270017e-07 1.28835829e-16
 2.81667241e-18 2.84546925e-22 9.99997139e-01], sum to 1.0000
[2019-04-08 15:26:24,897] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1034
[2019-04-08 15:26:24,908] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.75, 81.5, 100.0, 234.0, 22.5, 28.54220333806336, 1.220537616091351, 1.0, 1.0, 65.0, 18849.45236461278], 
current ob forecast is [], 
actual action is [17.75, 65.0], 
sim time this is 1071000.0000, 
sim time next is 1071600.0000, 
raw observation next is [12.93333333333333, 81.0, 102.3333333333333, 195.0, 22.5, 28.59880980078174, 1.238141686910096, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8208679593721145, 0.81, 0.341111111111111, 0.2154696132596685, 0.375, 0.883234150065145, 0.9127138956366987, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5620406], dtype=float32), 0.00265965]. 
=============================================
[2019-04-08 15:26:24,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2169737e-17 8.8045979e-09 4.8384441e-06 2.6178666e-05 1.5836280e-10
 1.0601433e-11 5.0088835e-07 2.2091764e-13 1.6445911e-17 6.1628478e-20
 9.9996853e-01], sum to 1.0000
[2019-04-08 15:26:24,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7849
[2019-04-08 15:26:24,968] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.67620133443683, 1.049078905470263, 0.0, 1.0, 65.0, 27381.75517166244], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1036800.0000, 
sim time next is 1037400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.6736715604499, 1.060968332162418, 0.0, 1.0, 65.0, 27352.7724745116], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8061392967041584, 0.8536561107208059, 0.0, 1.0, 1.0, 0.13025129749767428], 
reward next is 0.8697, 
noisyNet noise sample is [array([0.17693427], dtype=float32), 1.4125276]. 
=============================================
[2019-04-08 15:26:25,258] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.1242789e-17 4.5981743e-10 4.1127873e-07 3.2853916e-07 3.8972896e-09
 2.2183434e-13 4.1448882e-08 8.1061875e-15 1.8190560e-18 2.2075471e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:26:25,266] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3146
[2019-04-08 15:26:25,277] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.0, 77.0, 0.0, 0.0, 19.0, 27.85652852209003, 1.062569415002978, 0.0, 1.0, 65.0, 25346.60394182182], 
current ob forecast is [], 
actual action is [19.0, 65.0], 
sim time this is 1042800.0000, 
sim time next is 1043400.0000, 
raw observation next is [13.9, 77.5, 0.0, 0.0, 19.0, 27.79706564644133, 1.064263953153078, 0.0, 1.0, 65.0, 26556.80501298352], 
processed observation next is [1.0, 0.043478260869565216, 0.847645429362881, 0.775, 0.0, 0.0, 0.08333333333333333, 0.816422137203444, 0.8547546510510259, 0.0, 1.0, 1.0, 0.1264609762523025], 
reward next is 0.8735, 
noisyNet noise sample is [array([-0.4415862], dtype=float32), -1.0573204]. 
=============================================
[2019-04-08 15:26:25,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4139716e-18 4.3940973e-10 1.4271446e-05 1.5921935e-07 3.6882112e-12
 3.7696483e-14 1.8298702e-07 6.3302086e-16 1.8810655e-19 3.1252800e-21
 9.9998534e-01], sum to 1.0000
[2019-04-08 15:26:25,843] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6785
[2019-04-08 15:26:25,851] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [19.4, 49.0, 63.5, 0.0, 22.5, 29.89011247234655, 1.536339173568194, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [24.4, 65.0], 
sim time this is 1094400.0000, 
sim time next is 1095000.0000, 
raw observation next is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 22.5, 29.91312515672272, 1.5397981718207, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9921514312096033, 0.4916666666666667, 0.18, 0.0, 0.375, 0.9927604297268934, 1.0132660572735668, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3333621], dtype=float32), -0.76044554]. 
=============================================
[2019-04-08 15:26:25,863] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[89.55258 ]
 [89.639435]
 [89.75879 ]
 [89.89055 ]
 [90.06731 ]], R is [[89.45825958]
 [89.56367493]
 [89.66803741]
 [89.72647858]
 [89.78434753]].
[2019-04-08 15:26:26,017] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.59752404e-17 2.02378864e-10 3.90081004e-06 5.87415570e-06
 7.92956867e-10 4.42153692e-12 3.44832955e-07 1.12703385e-13
 6.93412670e-17 3.93963734e-20 9.99989867e-01], sum to 1.0000
[2019-04-08 15:26:26,021] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2201
[2019-04-08 15:26:26,040] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.36666666666667, 78.33333333333334, 0.0, 0.0, 19.0, 27.80206611715719, 1.165183228253943, 0.0, 1.0, 65.0, 27843.0576901746], 
current ob forecast is [], 
actual action is [15.36666666666667, 65.0], 
sim time this is 1131600.0000, 
sim time next is 1132200.0000, 
raw observation next is [10.55, 78.0, 0.0, 0.0, 19.0, 27.79598404434067, 1.163196867070597, 0.0, 1.0, 65.0, 27896.36878785577], 
processed observation next is [0.0, 0.08695652173913043, 0.754847645429363, 0.78, 0.0, 0.0, 0.08333333333333333, 0.8163320036950559, 0.8877322890235323, 0.0, 1.0, 1.0, 0.13283985137074175], 
reward next is 0.8672, 
noisyNet noise sample is [array([0.34758994], dtype=float32), -0.08071523]. 
=============================================
[2019-04-08 15:26:26,437] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.1678663e-18 5.0903620e-10 3.0777596e-06 2.2519218e-06 4.4753126e-10
 6.9886757e-14 2.3795673e-08 1.9227933e-14 2.6367509e-19 7.8450165e-21
 9.9999464e-01], sum to 1.0000
[2019-04-08 15:26:26,437] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-08 15:26:26,451] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 22.5, 29.75504904833177, 1.508635331982522, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [24.0, 65.0], 
sim time this is 1088400.0000, 
sim time next is 1089000.0000, 
raw observation next is [19.1, 51.5, 140.0, 0.0, 22.5, 29.79198286317406, 1.511721780525437, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9916897506925209, 0.515, 0.4666666666666667, 0.0, 0.375, 0.9826652385978383, 1.0039072601751455, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35511], dtype=float32), 0.5259345]. 
=============================================
[2019-04-08 15:26:26,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5979551e-20 2.0553649e-11 4.2855689e-08 4.7916400e-09 2.0103522e-13
 3.1288201e-14 7.1074002e-10 3.0115147e-16 1.4209740e-20 1.9304208e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:26:26,458] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8967
[2019-04-08 15:26:26,464] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[90.41708 ]
 [90.76347 ]
 [91.138145]
 [91.578735]
 [92.16101 ]], R is [[90.20677948]
 [90.30471039]
 [90.40166473]
 [90.49765015]
 [90.59267426]].
[2019-04-08 15:26:26,468] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.9, 51.5, 0.0, 0.0, 22.5, 28.78091246499345, 1.382490588801833, 1.0, 1.0, 65.0, 18845.46289852027], 
current ob forecast is [], 
actual action is [21.9, 65.0], 
sim time this is 1099800.0000, 
sim time next is 1100400.0000, 
raw observation next is [16.63333333333333, 52.0, 0.0, 0.0, 22.5, 28.87456149005463, 1.39003352908403, 1.0, 1.0, 65.0, 18848.32143588115], 
processed observation next is [1.0, 0.7391304347826086, 0.9233610341643583, 0.52, 0.0, 0.0, 0.375, 0.9062134575045526, 0.9633445096946766, 1.0, 1.0, 1.0, 0.08975391159943405], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.2886259], dtype=float32), -0.2081393]. 
=============================================
[2019-04-08 15:26:26,572] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2016107e-19 1.1143766e-10 9.5431210e-07 4.1320842e-08 4.3875639e-11
 6.4600543e-15 1.1534838e-08 6.0734735e-16 3.1255391e-20 1.5320709e-21
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:26:26,575] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9524
[2019-04-08 15:26:26,580] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5725836e-17 1.7667258e-09 8.6891498e-07 2.3540591e-07 4.5681228e-12
 2.0186339e-13 5.3238150e-08 1.8983076e-16 2.6732223e-19 4.6379093e-20
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:26:26,583] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.56666666666667, 82.0, 87.00000000000001, 206.5, 22.5, 28.4268226113119, 1.210418323940959, 1.0, 1.0, 65.0, 18848.00854998223], 
current ob forecast is [], 
actual action is [17.56666666666667, 65.0], 
sim time this is 1070400.0000, 
sim time next is 1071000.0000, 
raw observation next is [12.75, 81.5, 100.0, 234.0, 22.5, 28.54215774755503, 1.220525070997608, 1.0, 1.0, 65.0, 18849.4515947621], 
processed observation next is [1.0, 0.391304347826087, 0.8157894736842106, 0.815, 0.3333333333333333, 0.2585635359116022, 0.375, 0.8785131456295859, 0.906841690332536, 1.0, 1.0, 1.0, 0.08975929330839096], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.51355684], dtype=float32), -0.028062016]. 
=============================================
[2019-04-08 15:26:26,584] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2431
[2019-04-08 15:26:26,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.79514]
 [90.68491]
 [90.54378]
 [90.33459]
 [89.93281]], R is [[91.02095795]
 [91.02099609]
 [91.02104187]
 [91.02108765]
 [91.02113342]].
[2019-04-08 15:26:26,606] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.75, 66.0, 130.0, 0.0, 19.0, 28.04527865080765, 1.196748207153719, 0.0, 1.0, 65.0, 22096.21517755613], 
current ob forecast is [], 
actual action is [22.75, 65.0], 
sim time this is 1161000.0000, 
sim time next is 1161600.0000, 
raw observation next is [17.93333333333333, 65.66666666666666, 135.0, 0.0, 19.0, 28.06078909130459, 1.207298412475549, 0.0, 1.0, 65.0, 21759.99309912488], 
processed observation next is [0.0, 0.43478260869565216, 0.9593721144967682, 0.6566666666666666, 0.45, 0.0, 0.08333333333333333, 0.838399090942049, 0.9024328041585163, 0.0, 1.0, 1.0, 0.10361901475773752], 
reward next is 0.8964, 
noisyNet noise sample is [array([-0.4541031], dtype=float32), 0.08202567]. 
=============================================
[2019-04-08 15:26:26,624] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7589434e-17 3.8862100e-09 4.9929090e-06 8.1731368e-06 1.7773635e-10
 4.4202386e-13 1.4274758e-07 3.2979571e-13 6.9107340e-17 8.4242448e-21
 9.9998665e-01], sum to 1.0000
[2019-04-08 15:26:26,629] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0329
[2019-04-08 15:26:26,638] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.0, 67.66666666666667, 0.0, 0.0, 19.0, 27.99348129076001, 1.220225559878241, 0.0, 1.0, 65.0, 24713.5793190474], 
current ob forecast is [], 
actual action is [17.0, 65.0], 
sim time this is 1120800.0000, 
sim time next is 1121400.0000, 
raw observation next is [11.9, 68.5, 0.0, 0.0, 19.0, 27.98303840698404, 1.21701394747187, 0.0, 1.0, 65.0, 24887.72200870823], 
processed observation next is [1.0, 1.0, 0.7922437673130196, 0.685, 0.0, 0.0, 0.08333333333333333, 0.83191986724867, 0.9056713158239567, 0.0, 1.0, 1.0, 0.11851296194622968], 
reward next is 0.8815, 
noisyNet noise sample is [array([1.0218246], dtype=float32), 0.26191542]. 
=============================================
[2019-04-08 15:26:26,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5772991e-15 2.5283549e-08 9.4792529e-05 3.2167216e-05 2.7046800e-09
 4.3793528e-11 4.6480153e-08 3.9878222e-13 2.5696399e-16 6.1515049e-19
 9.9987292e-01], sum to 1.0000
[2019-04-08 15:26:26,710] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9074
[2019-04-08 15:26:26,712] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0479943e-15 2.1274322e-09 3.6513673e-05 9.9935796e-06 8.8429798e-11
 6.0282552e-13 9.5286348e-09 2.3022894e-14 2.3476664e-18 7.5235586e-22
 9.9995351e-01], sum to 1.0000
[2019-04-08 15:26:26,713] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6220
[2019-04-08 15:26:26,723] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [19.4, 49.0, 73.0, 0.0, 22.5, 29.75073566174444, 1.526440414062462, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [24.4, 65.0], 
sim time this is 1093800.0000, 
sim time next is 1094400.0000, 
raw observation next is [19.4, 49.0, 63.5, 0.0, 22.5, 29.89012390411286, 1.53634224924444, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 1.0, 0.49, 0.21166666666666667, 0.0, 0.375, 0.9908436586760715, 1.01211408308148, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6586148], dtype=float32), -0.6935734]. 
=============================================
[2019-04-08 15:26:26,727] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.73333333333333, 77.66666666666666, 0.0, 0.0, 19.0, 27.7923234820346, 1.161327612102812, 0.0, 1.0, 65.0, 27932.6664352614], 
current ob forecast is [], 
actual action is [15.73333333333333, 65.0], 
sim time this is 1132800.0000, 
sim time next is 1133400.0000, 
raw observation next is [10.91666666666667, 77.33333333333334, 0.0, 0.0, 19.0, 27.79018471952518, 1.159641407105949, 0.0, 1.0, 65.0, 27959.17678062935], 
processed observation next is [0.0, 0.08695652173913043, 0.765004616805171, 0.7733333333333334, 0.0, 0.0, 0.08333333333333333, 0.8158487266270983, 0.8865471357019831, 0.0, 1.0, 1.0, 0.13313893705061594], 
reward next is 0.8669, 
noisyNet noise sample is [array([0.7336958], dtype=float32), -1.0282716]. 
=============================================
[2019-04-08 15:26:26,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6000830e-16 8.3522023e-10 1.8940625e-07 3.3020046e-06 3.8445766e-10
 8.3717071e-14 1.9762744e-08 6.5101402e-13 1.6162320e-18 1.3269474e-19
 9.9999654e-01], sum to 1.0000
[2019-04-08 15:26:26,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8500
[2019-04-08 15:26:26,745] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.43333333333333, 77.0, 0.0, 0.0, 19.0, 27.77768783714281, 1.14293534573166, 0.0, 1.0, 65.0, 28351.68690048422], 
current ob forecast is [], 
actual action is [16.43333333333333, 65.0], 
sim time this is 1140000.0000, 
sim time next is 1140600.0000, 
raw observation next is [11.51666666666667, 77.0, 0.0, 0.0, 19.0, 27.77329711311901, 1.143083328758415, 0.0, 1.0, 65.0, 28305.5317692962], 
processed observation next is [0.0, 0.17391304347826086, 0.7816251154201295, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8144414260932509, 0.881027776252805, 0.0, 1.0, 1.0, 0.1347882465204581], 
reward next is 0.8652, 
noisyNet noise sample is [array([0.5164721], dtype=float32), 0.87388057]. 
=============================================
[2019-04-08 15:26:27,653] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8946355e-16 8.2902929e-09 9.2067912e-06 1.6348635e-06 6.5260147e-10
 2.0080127e-12 6.2972497e-08 1.1995876e-13 2.1707846e-17 2.2523107e-19
 9.9998903e-01], sum to 1.0000
[2019-04-08 15:26:27,657] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6267
[2019-04-08 15:26:27,672] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 27.79254510195705, 1.153363064681606, 0.0, 1.0, 65.0, 27915.11102111142], 
current ob forecast is [], 
actual action is [16.1, 65.0], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 19.0, 27.78975561749026, 1.151250540357982, 0.0, 1.0, 65.0, 27994.64058780382], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8158129681241885, 0.8837501801193274, 0.0, 1.0, 1.0, 0.13330781232287534], 
reward next is 0.8667, 
noisyNet noise sample is [array([-0.35425168], dtype=float32), -0.21404202]. 
=============================================
[2019-04-08 15:26:27,721] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.14760232e-16 1.32547762e-09 2.71052959e-05 5.84169584e-06
 1.14806116e-10 2.26815441e-12 2.27675878e-06 3.69685971e-13
 1.12996065e-17 4.32460784e-20 9.99964714e-01], sum to 1.0000
[2019-04-08 15:26:27,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8919
[2019-04-08 15:26:27,740] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.0, 65.0, 0.0, 0.0, 19.0, 28.38639776814968, 1.275631546064743, 0.0, 1.0, 65.0, 18848.94592339938], 
current ob forecast is [], 
actual action is [23.0, 65.0], 
sim time this is 1189800.0000, 
sim time next is 1190400.0000, 
raw observation next is [17.9, 65.66666666666667, 0.0, 0.0, 19.0, 28.38110365867468, 1.272889282538029, 0.0, 1.0, 65.0, 18849.03914641665], 
processed observation next is [0.0, 0.782608695652174, 0.9584487534626038, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.8650919715562232, 0.9242964275126763, 0.0, 1.0, 1.0, 0.08975732926865071], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.3285157], dtype=float32), -1.1902143]. 
=============================================
[2019-04-08 15:26:28,215] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8569132e-17 1.9712834e-09 2.1750000e-07 3.3670264e-07 4.0799196e-12
 3.6251359e-13 5.2180887e-08 3.2218880e-14 9.5018411e-18 8.2273542e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:26:28,220] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0169
[2019-04-08 15:26:28,240] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 27.78149112912577, 1.15048065924832, 0.0, 1.0, 65.0, 28230.70704908695], 
current ob forecast is [], 
actual action is [16.1, 65.0], 
sim time this is 1137600.0000, 
sim time next is 1138200.0000, 
raw observation next is [11.18333333333333, 77.0, 0.0, 0.0, 19.0, 27.79528350081149, 1.147653964396599, 0.0, 1.0, 65.0, 27797.86267493373], 
processed observation next is [0.0, 0.17391304347826086, 0.7723915050784858, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8162736250676241, 0.8825513214655331, 0.0, 1.0, 1.0, 0.13237077464254157], 
reward next is 0.8676, 
noisyNet noise sample is [array([1.4449654], dtype=float32), -0.892155]. 
=============================================
[2019-04-08 15:26:28,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1297789e-18 8.9749469e-10 1.7104331e-07 4.4759642e-07 1.2898940e-11
 1.5339681e-11 2.8206394e-08 1.0634153e-13 5.4380375e-18 1.5462626e-21
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:26:28,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8942
[2019-04-08 15:26:28,272] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 19.0, 28.32347410373445, 1.261100791706344, 0.0, 1.0, 65.0, 18848.12844173717], 
current ob forecast is [], 
actual action is [21.6, 65.0], 
sim time this is 1203000.0000, 
sim time next is 1203600.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 19.0, 28.3059271106512, 1.258410054506215, 0.0, 1.0, 65.0, 18847.90490169764], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8588272592209334, 0.9194700181687384, 0.0, 1.0, 1.0, 0.0897519281033221], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.35231292], dtype=float32), -1.0518585]. 
=============================================
[2019-04-08 15:26:28,373] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.6409868e-17 6.6770345e-09 3.5996090e-06 2.5528163e-06 6.0035030e-09
 6.9443881e-11 7.3671026e-07 8.5032740e-13 1.6161542e-16 9.6718993e-19
 9.9999309e-01], sum to 1.0000
[2019-04-08 15:26:28,375] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4050
[2019-04-08 15:26:28,389] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.6, 77.0, 0.0, 0.0, 19.0, 27.78110160571078, 1.143471620982847, 0.0, 1.0, 65.0, 28031.30390044634], 
current ob forecast is [], 
actual action is [16.6, 65.0], 
sim time this is 1141200.0000, 
sim time next is 1141800.0000, 
raw observation next is [11.6, 78.0, 0.0, 0.0, 19.0, 27.79456395163173, 1.145528180384073, 0.0, 1.0, 65.0, 27663.9342344338], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.78, 0.0, 0.0, 0.08333333333333333, 0.8162136626359775, 0.8818427267946909, 0.0, 1.0, 1.0, 0.13173302016397048], 
reward next is 0.8683, 
noisyNet noise sample is [array([0.05000118], dtype=float32), -1.4589202]. 
=============================================
[2019-04-08 15:26:28,617] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.72148501e-16 2.59009680e-09 1.98017951e-05 2.32836464e-05
 4.85560747e-09 1.23414441e-12 1.07744756e-06 2.00072719e-13
 1.05262366e-16 3.51673692e-19 9.99955773e-01], sum to 1.0000
[2019-04-08 15:26:28,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5026
[2019-04-08 15:26:28,634] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.55, 64.0, 160.0, 0.0, 19.0, 28.16047145157521, 1.22617855320094, 0.0, 1.0, 65.0, 20195.0098173347], 
current ob forecast is [], 
actual action is [23.55, 65.0], 
sim time this is 1164600.0000, 
sim time next is 1165200.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 161.8333333333333, 0.0, 19.0, 28.17298701454552, 1.232442335276625, 0.0, 1.0, 65.0, 19939.51490449047], 
processed observation next is [0.0, 0.4782608695652174, 0.9787626962142197, 0.6366666666666667, 0.5394444444444443, 0.0, 0.08333333333333333, 0.8477489178787933, 0.910814111758875, 0.0, 1.0, 1.0, 0.09495007097376414], 
reward next is 0.9050, 
noisyNet noise sample is [array([-1.5365723], dtype=float32), 0.73270476]. 
=============================================
[2019-04-08 15:26:29,008] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0167404e-17 4.6257673e-10 2.6218257e-07 4.8500842e-06 3.9822359e-09
 3.8937525e-12 6.8350140e-08 1.6791493e-13 9.0351913e-18 2.7320247e-20
 9.9999487e-01], sum to 1.0000
[2019-04-08 15:26:29,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6845
[2019-04-08 15:26:29,024] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 19.0, 28.34694968134986, 1.265293309102755, 0.0, 1.0, 65.0, 18848.63864921807], 
current ob forecast is [], 
actual action is [22.33333333333334, 65.0], 
sim time this is 1200000.0000, 
sim time next is 1200600.0000, 
raw observation next is [17.15, 71.0, 0.0, 0.0, 19.0, 28.32939659427944, 1.272687690550502, 0.0, 1.0, 65.0, 18848.45119873492], 
processed observation next is [0.0, 0.9130434782608695, 0.9376731301939059, 0.71, 0.0, 0.0, 0.08333333333333333, 0.8607830495232868, 0.9242292301835007, 0.0, 1.0, 1.0, 0.08975452951778534], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.58374745], dtype=float32), 0.4730759]. 
=============================================
[2019-04-08 15:26:29,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0318072e-17 5.9274413e-10 2.6614159e-06 3.8552858e-05 1.5152218e-10
 1.5585482e-13 1.2724973e-06 2.1187704e-13 7.4234240e-18 2.6475875e-20
 9.9995756e-01], sum to 1.0000
[2019-04-08 15:26:29,054] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5540
[2019-04-08 15:26:29,066] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8022289e-17 1.6420566e-09 1.3686683e-06 7.6581625e-07 3.5241674e-09
 6.7011392e-13 2.6705294e-07 7.2559024e-14 7.5515919e-19 1.9588291e-20
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:26:29,070] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.3, 65.0, 112.0, 0.0, 19.0, 28.38208725828157, 1.283628291707316, 0.0, 1.0, 65.0, 18847.98222640871], 
current ob forecast is [], 
actual action is [23.3, 65.0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [18.3, 65.0, 104.0, 0.0, 19.0, 28.37550860378915, 1.28996771935938, 0.0, 1.0, 65.0, 18848.32216288147], 
processed observation next is [0.0, 0.6521739130434783, 0.9695290858725764, 0.65, 0.3466666666666667, 0.0, 0.08333333333333333, 0.8646257169824292, 0.92998923978646, 0.0, 1.0, 1.0, 0.08975391506134034], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.67688394], dtype=float32), 1.2103732]. 
=============================================
[2019-04-08 15:26:29,071] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1836
[2019-04-08 15:26:29,089] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.7, 89.66666666666667, 0.0, 0.0, 19.0, 28.25324429824025, 1.249133308919513, 0.0, 1.0, 65.0, 21489.5512134398], 
current ob forecast is [], 
actual action is [20.7, 65.0], 
sim time this is 1219200.0000, 
sim time next is 1219800.0000, 
raw observation next is [15.6, 91.33333333333333, 0.0, 0.0, 19.0, 28.22655693696025, 1.249305925095793, 0.0, 1.0, 65.0, 21899.68015204833], 
processed observation next is [0.0, 0.08695652173913043, 0.8947368421052633, 0.9133333333333333, 0.0, 0.0, 0.08333333333333333, 0.8522130780800209, 0.9164353083652643, 0.0, 1.0, 1.0, 0.10428419120023015], 
reward next is 0.8957, 
noisyNet noise sample is [array([-0.18559529], dtype=float32), -0.69659036]. 
=============================================
[2019-04-08 15:26:29,218] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7938944e-17 3.5503201e-11 4.6324388e-07 2.1227092e-06 7.2556773e-11
 5.0902591e-14 4.9494595e-08 9.7405333e-15 4.7242249e-18 1.0501100e-21
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:26:29,218] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8881202e-15 1.9485560e-09 8.5393058e-06 4.8935776e-06 6.2973564e-11
 1.9992323e-13 3.0142392e-08 5.8131918e-15 1.5929981e-19 1.4228633e-21
 9.9998653e-01], sum to 1.0000
[2019-04-08 15:26:29,223] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0147
[2019-04-08 15:26:29,228] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1110
[2019-04-08 15:26:29,232] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.71666666666667, 63.33333333333333, 63.00000000000001, 0.0, 19.0, 28.4373616945886, 1.290104635292005, 0.0, 1.0, 65.0, 18848.48444370558], 
current ob forecast is [], 
actual action is [23.71666666666667, 65.0], 
sim time this is 1180200.0000, 
sim time next is 1180800.0000, 
raw observation next is [18.8, 63.0, 54.5, 0.0, 19.0, 28.43765672819659, 1.288244190466874, 0.0, 1.0, 65.0, 18848.6116162081], 
processed observation next is [0.0, 0.6956521739130435, 0.9833795013850417, 0.63, 0.18166666666666667, 0.0, 0.08333333333333333, 0.8698047273497158, 0.9294147301556247, 0.0, 1.0, 1.0, 0.08975529341051477], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.6825935], dtype=float32), -0.62568754]. 
=============================================
[2019-04-08 15:26:29,238] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.75, 66.0, 130.0, 0.0, 19.0, 28.04528530060399, 1.196750191876671, 0.0, 1.0, 65.0, 22096.14066060814], 
current ob forecast is [], 
actual action is [22.75, 65.0], 
sim time this is 1161000.0000, 
sim time next is 1161600.0000, 
raw observation next is [17.93333333333333, 65.66666666666666, 135.0, 0.0, 19.0, 28.06079571876229, 1.20730039448923, 0.0, 1.0, 65.0, 21759.91882199582], 
processed observation next is [0.0, 0.43478260869565216, 0.9593721144967682, 0.6566666666666666, 0.45, 0.0, 0.08333333333333333, 0.8383996432301908, 0.9024334648297433, 0.0, 1.0, 1.0, 0.10361866105712295], 
reward next is 0.8964, 
noisyNet noise sample is [array([-0.18761529], dtype=float32), -1.2723799]. 
=============================================
[2019-04-08 15:26:29,240] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9609249e-17 3.7477681e-11 4.8185359e-07 2.2056645e-06 7.6725168e-11
 5.4475050e-14 5.1686591e-08 1.0480246e-14 5.1484234e-18 1.1668082e-21
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:26:29,245] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0013
[2019-04-08 15:26:29,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5419520e-15 1.8664825e-09 8.2397182e-06 4.6620389e-06 5.7740888e-11
 1.8123755e-13 2.7890733e-08 5.2947924e-15 1.4510842e-19 1.2528408e-21
 9.9998713e-01], sum to 1.0000
[2019-04-08 15:26:29,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5969
[2019-04-08 15:26:29,264] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.8, 63.0, 54.5, 0.0, 19.0, 28.43765672819659, 1.288244190466874, 0.0, 1.0, 65.0, 18848.6116162081], 
current ob forecast is [], 
actual action is [23.8, 65.0], 
sim time this is 1180800.0000, 
sim time next is 1181400.0000, 
raw observation next is [18.71666666666667, 63.33333333333333, 46.0, 0.0, 19.0, 28.4232331659592, 1.287233684106705, 0.0, 1.0, 65.0, 18848.98454492998], 
processed observation next is [0.0, 0.6956521739130435, 0.981071098799631, 0.6333333333333333, 0.15333333333333332, 0.0, 0.08333333333333333, 0.8686027638299333, 0.929077894702235, 0.0, 1.0, 1.0, 0.08975706926157133], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.6825935], dtype=float32), -0.62568754]. 
=============================================
[2019-04-08 15:26:29,269] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.11666666666667, 65.33333333333334, 140.0, 0.0, 19.0, 28.11779015779609, 1.209656784013574, 0.0, 1.0, 65.0, 20395.19070308911], 
current ob forecast is [], 
actual action is [23.11666666666667, 65.0], 
sim time this is 1162200.0000, 
sim time next is 1162800.0000, 
raw observation next is [18.3, 65.0, 145.0, 0.0, 19.0, 28.13510596484163, 1.212995522688302, 0.0, 1.0, 65.0, 20144.99450601581], 
processed observation next is [0.0, 0.4782608695652174, 0.9695290858725764, 0.65, 0.48333333333333334, 0.0, 0.08333333333333333, 0.8445921637368027, 0.9043318408961006, 0.0, 1.0, 1.0, 0.09592854526674195], 
reward next is 0.9041, 
noisyNet noise sample is [array([-0.18761529], dtype=float32), -1.2723799]. 
=============================================
[2019-04-08 15:26:29,527] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1699408e-17 1.3559567e-10 4.4660291e-07 1.2422502e-05 6.0456345e-10
 7.6590183e-14 1.4138439e-07 1.5373084e-15 5.5059111e-19 1.5411732e-21
 9.9998701e-01], sum to 1.0000
[2019-04-08 15:26:29,530] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1671
[2019-04-08 15:26:29,543] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.13272625694428, 1.245687144662991, 0.0, 1.0, 65.0, 22727.24995187118], 
current ob forecast is [], 
actual action is [20.0, 65.0], 
sim time this is 1229400.0000, 
sim time next is 1230000.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.16531672214223, 1.243016028987928, 0.0, 1.0, 65.0, 21914.89164403459], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8471097268451858, 0.9143386763293093, 0.0, 1.0, 1.0, 0.10435662687635519], 
reward next is 0.8956, 
noisyNet noise sample is [array([1.4354588], dtype=float32), -1.5520877]. 
=============================================
[2019-04-08 15:26:29,556] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[89.6386  ]
 [89.584236]
 [89.47111 ]
 [89.37899 ]
 [89.269875]], R is [[89.72052765]
 [89.71510315]
 [89.71003723]
 [89.70455933]
 [89.69974518]].
[2019-04-08 15:26:29,804] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0041180e-16 1.0961405e-10 4.8175206e-08 1.9148763e-06 2.1400606e-10
 3.7944311e-14 1.2725687e-07 3.4521353e-14 8.8823523e-19 3.8863469e-22
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:26:29,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2746
[2019-04-08 15:26:29,819] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.33333333333333, 94.0, 0.0, 0.0, 19.0, 28.17326764607693, 1.245159849936218, 0.0, 1.0, 65.0, 22210.19933097412], 
current ob forecast is [], 
actual action is [20.33333333333333, 65.0], 
sim time this is 1225200.0000, 
sim time next is 1225800.0000, 
raw observation next is [15.25, 94.5, 0.0, 0.0, 19.0, 28.17254850060126, 1.243596717491857, 0.0, 1.0, 65.0, 22064.64430263776], 
processed observation next is [0.0, 0.17391304347826086, 0.8850415512465375, 0.945, 0.0, 0.0, 0.08333333333333333, 0.8477123750501049, 0.9145322391639524, 0.0, 1.0, 1.0, 0.10506973477446552], 
reward next is 0.8949, 
noisyNet noise sample is [array([-0.53126323], dtype=float32), 1.230736]. 
=============================================
[2019-04-08 15:26:30,239] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9477708e-18 4.9489753e-12 2.6064255e-08 1.1857544e-06 4.8268971e-12
 1.3118700e-13 5.5116356e-09 4.2585270e-16 1.3223361e-20 1.3549691e-22
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:26:30,243] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9257
[2019-04-08 15:26:30,258] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 98.0, 28.0, 0.0, 19.0, 28.1324741833098, 1.24901387238356, 0.0, 1.0, 65.0, 21805.04906715612], 
current ob forecast is [], 
actual action is [20.0, 65.0], 
sim time this is 1240200.0000, 
sim time next is 1240800.0000, 
raw observation next is [15.0, 98.66666666666666, 35.66666666666666, 0.0, 19.0, 28.13406642597163, 1.250682085956693, 0.0, 1.0, 65.0, 21602.5749478386], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9866666666666666, 0.11888888888888886, 0.0, 0.08333333333333333, 0.8445055354976357, 0.9168940286522309, 0.0, 1.0, 1.0, 0.10286940451351714], 
reward next is 0.8971, 
noisyNet noise sample is [array([-0.14869185], dtype=float32), 0.46416622]. 
=============================================
[2019-04-08 15:26:30,491] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7543267e-17 1.4614365e-09 1.1706449e-06 4.8411739e-06 3.0030860e-09
 7.7958581e-14 2.9011994e-07 5.2937212e-13 3.8974011e-17 5.8527778e-21
 9.9999368e-01], sum to 1.0000
[2019-04-08 15:26:30,494] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2311
[2019-04-08 15:26:30,503] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.37503484466422, 1.269124360869693, 0.0, 1.0, 65.0, 18848.92833670203], 
current ob forecast is [], 
actual action is [22.7, 65.0], 
sim time this is 1198800.0000, 
sim time next is 1199400.0000, 
raw observation next is [17.51666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 28.36437094605857, 1.266883913160748, 0.0, 1.0, 65.0, 18848.85459843282], 
processed observation next is [0.0, 0.9130434782608695, 0.9478301015697139, 0.6833333333333335, 0.0, 0.0, 0.08333333333333333, 0.8636975788382143, 0.9222946377202493, 0.0, 1.0, 1.0, 0.08975645046872771], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.1084747], dtype=float32), 0.59473217]. 
=============================================
[2019-04-08 15:26:30,566] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0141093e-18 9.3328114e-12 1.7981636e-08 2.9557004e-05 2.0097857e-10
 8.8907110e-13 1.4162345e-07 2.9542354e-14 5.7495024e-20 8.4673068e-22
 9.9997032e-01], sum to 1.0000
[2019-04-08 15:26:30,572] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6279
[2019-04-08 15:26:30,584] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 97.33333333333334, 96.0, 0.0, 19.0, 28.19640502416555, 1.284090018627761, 0.0, 1.0, 65.0, 19294.07595597259], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1251600.0000, 
sim time next is 1252200.0000, 
raw observation next is [14.4, 96.66666666666666, 97.0, 0.0, 19.0, 28.19615229637809, 1.286277376079098, 0.0, 1.0, 65.0, 19207.06419743077], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9666666666666666, 0.3233333333333333, 0.0, 0.08333333333333333, 0.8496793580315076, 0.9287591253596993, 0.0, 1.0, 1.0, 0.09146221046395604], 
reward next is 0.9085, 
noisyNet noise sample is [array([-0.0742595], dtype=float32), -0.45971406]. 
=============================================
[2019-04-08 15:26:31,163] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.61399556e-19 3.31842713e-11 9.66845377e-08 4.47013826e-06
 9.08804369e-12 2.06906009e-14 1.03501225e-08 1.03706574e-15
 2.31081725e-20 2.82734085e-22 9.99995470e-01], sum to 1.0000
[2019-04-08 15:26:31,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2276
[2019-04-08 15:26:31,174] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 28.30823237585555, 1.271511211090956, 0.0, 1.0, 65.0, 18845.28901082338], 
current ob forecast is [], 
actual action is [23.3, 65.0], 
sim time this is 1172400.0000, 
sim time next is 1173000.0000, 
raw observation next is [18.3, 65.0, 148.6666666666667, 0.0, 19.0, 28.32666464590692, 1.272641616542937, 0.0, 1.0, 65.0, 18845.60313670532], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.4955555555555557, 0.0, 0.08333333333333333, 0.86055538715891, 0.924213872180979, 0.0, 1.0, 1.0, 0.08974096731764437], 
reward next is 0.9103, 
noisyNet noise sample is [array([-0.42003712], dtype=float32), -0.5271577]. 
=============================================
[2019-04-08 15:26:31,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.28806]
 [89.34247]
 [89.42442]
 [89.45506]
 [89.52069]], R is [[89.22228241]
 [89.24032593]
 [89.25818634]
 [89.27587128]
 [89.29337311]].
[2019-04-08 15:26:31,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.20929344e-16 5.74335557e-09 8.95455742e-06 3.53845317e-05
 5.94367844e-09 2.42447121e-12 1.17097706e-07 3.08304516e-13
 3.53491036e-17 1.61662744e-19 9.99955535e-01], sum to 1.0000
[2019-04-08 15:26:31,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2618
[2019-04-08 15:26:31,619] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.63333333333333, 63.66666666666667, 37.5, 0.0, 19.0, 28.40778984582739, 1.28538350001435, 0.0, 1.0, 65.0, 18849.31245008067], 
current ob forecast is [], 
actual action is [23.63333333333333, 65.0], 
sim time this is 1182000.0000, 
sim time next is 1182600.0000, 
raw observation next is [18.55, 64.0, 29.0, 0.0, 19.0, 28.39859653247106, 1.28298066730544, 0.0, 1.0, 65.0, 18848.40083692607], 
processed observation next is [0.0, 0.6956521739130435, 0.976454293628809, 0.64, 0.09666666666666666, 0.0, 0.08333333333333333, 0.866549711039255, 0.9276602224351467, 0.0, 1.0, 1.0, 0.08975428969964795], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.42119464], dtype=float32), -0.4215362]. 
=============================================
[2019-04-08 15:26:31,670] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.3318168e-17 2.4231520e-10 1.8348286e-06 2.4319725e-06 3.3323072e-10
 2.0701707e-13 1.4127880e-08 4.4335778e-13 1.8556631e-18 1.0088137e-19
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:26:31,678] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7868
[2019-04-08 15:26:31,689] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.0, 65.0, 0.0, 0.0, 19.0, 28.38640683223987, 1.275634336489775, 0.0, 1.0, 65.0, 18848.94685039571], 
current ob forecast is [], 
actual action is [23.0, 65.0], 
sim time this is 1189800.0000, 
sim time next is 1190400.0000, 
raw observation next is [17.9, 65.66666666666667, 0.0, 0.0, 19.0, 28.38111274464305, 1.272892062310726, 0.0, 1.0, 65.0, 18849.04009511463], 
processed observation next is [0.0, 0.782608695652174, 0.9584487534626038, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.8650927287202542, 0.9242973541035754, 0.0, 1.0, 1.0, 0.08975733378626014], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.5903027], dtype=float32), 1.7441809]. 
=============================================
[2019-04-08 15:26:32,086] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8132313e-17 8.7981611e-10 3.6235979e-06 1.4964943e-06 5.8224017e-09
 3.2188731e-12 5.7154335e-08 7.3683294e-15 3.4915182e-18 8.1161217e-21
 9.9999487e-01], sum to 1.0000
[2019-04-08 15:26:32,087] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9501
[2019-04-08 15:26:32,096] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.08333333333333, 95.5, 0.0, 0.0, 19.0, 28.15372870560813, 1.241769477346321, 0.0, 1.0, 65.0, 22493.65915548245], 
current ob forecast is [], 
actual action is [20.08333333333333, 65.0], 
sim time this is 1227000.0000, 
sim time next is 1227600.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.14566552619221, 1.241111239937514, 0.0, 1.0, 65.0, 22632.55295500479], 
processed observation next is [0.0, 0.21739130434782608, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8454721271826843, 0.913703746645838, 0.0, 1.0, 1.0, 0.107774061690499], 
reward next is 0.8922, 
noisyNet noise sample is [array([0.2398975], dtype=float32), -0.61082494]. 
=============================================
[2019-04-08 15:26:32,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4701775e-17 9.2790298e-10 3.5975656e-05 3.8615372e-06 1.5296813e-09
 1.2179601e-12 2.8524869e-07 4.4122759e-14 1.3044117e-16 8.3782819e-22
 9.9995983e-01], sum to 1.0000
[2019-04-08 15:26:32,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0307
[2019-04-08 15:26:32,210] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 97.33333333333333, 23.33333333333334, 0.0, 19.0, 28.14013409324554, 1.247597579557749, 0.0, 1.0, 65.0, 21970.35929581505], 
current ob forecast is [], 
actual action is [20.0, 65.0], 
sim time this is 1239600.0000, 
sim time next is 1240200.0000, 
raw observation next is [15.0, 98.0, 28.0, 0.0, 19.0, 28.13247487678946, 1.249014084409191, 0.0, 1.0, 65.0, 21805.04158011376], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.98, 0.09333333333333334, 0.0, 0.08333333333333333, 0.8443729063991215, 0.916338028136397, 0.0, 1.0, 1.0, 0.10383353133387505], 
reward next is 0.8962, 
noisyNet noise sample is [array([0.5943124], dtype=float32), 0.37332273]. 
=============================================
[2019-04-08 15:26:32,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0658872e-18 9.1517342e-11 3.1460935e-08 2.0248852e-07 1.1725916e-10
 3.4384059e-15 2.9195213e-09 3.5145099e-16 1.6534459e-21 1.7422872e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:26:32,932] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4295
[2019-04-08 15:26:32,940] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 97.5, 46.0, 0.0, 22.5, 27.5511501242473, 0.933090394211003, 1.0, 1.0, 65.0, 30346.02009393616], 
current ob forecast is [], 
actual action is [4.7, 65.0], 
sim time this is 1416600.0000, 
sim time next is 1417200.0000, 
raw observation next is [-0.2, 96.66666666666666, 50.33333333333333, 0.0, 22.5, 27.60110894616786, 0.9353577864370917, 1.0, 1.0, 65.0, 28134.70329224183], 
processed observation next is [1.0, 0.391304347826087, 0.4570637119113574, 0.9666666666666666, 0.16777777777777778, 0.0, 0.375, 0.8000924121806549, 0.8117859288123639, 1.0, 1.0, 1.0, 0.13397477758210394], 
reward next is 0.8660, 
noisyNet noise sample is [array([1.3109603], dtype=float32), -0.7413722]. 
=============================================
[2019-04-08 15:26:33,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6877004e-16 2.0262907e-10 6.6105054e-06 2.2970948e-05 3.6275902e-09
 8.2902591e-14 5.7871759e-08 7.5359767e-15 5.4527940e-18 2.8290082e-21
 9.9997032e-01], sum to 1.0000
[2019-04-08 15:26:33,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9028
[2019-04-08 15:26:33,365] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.25, 94.5, 0.0, 0.0, 19.0, 28.17254060650824, 1.243594289148994, 0.0, 1.0, 65.0, 22064.73142645476], 
current ob forecast is [], 
actual action is [20.25, 65.0], 
sim time this is 1225800.0000, 
sim time next is 1226400.0000, 
raw observation next is [15.16666666666667, 95.0, 0.0, 0.0, 19.0, 28.16386343006877, 1.242331794785551, 0.0, 1.0, 65.0, 22179.57934306753], 
processed observation next is [0.0, 0.17391304347826086, 0.8827331486611266, 0.95, 0.0, 0.0, 0.08333333333333333, 0.8469886191723974, 0.9141105982618504, 0.0, 1.0, 1.0, 0.10561704449079776], 
reward next is 0.8944, 
noisyNet noise sample is [array([1.1264569], dtype=float32), -0.2035074]. 
=============================================
[2019-04-08 15:26:33,861] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0052196e-17 2.0171885e-10 1.8611192e-05 3.4266429e-07 9.3995853e-11
 1.6512965e-14 1.5962657e-07 1.1531728e-15 1.2184288e-17 3.4420561e-22
 9.9998093e-01], sum to 1.0000
[2019-04-08 15:26:33,862] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0536
[2019-04-08 15:26:33,883] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.75, 96.0, 0.0, 0.0, 19.0, 27.88226224020915, 1.228538594982628, 0.0, 1.0, 65.0, 28007.59952294753], 
current ob forecast is [], 
actual action is [12.75, 65.0], 
sim time this is 1276200.0000, 
sim time next is 1276800.0000, 
raw observation next is [7.566666666666666, 96.0, 0.0, 0.0, 19.0, 27.85131851145472, 1.223411190316025, 0.0, 1.0, 65.0, 28447.64134490201], 
processed observation next is [0.0, 0.782608695652174, 0.672206832871653, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8209432092878934, 0.9078037301053415, 0.0, 1.0, 1.0, 0.13546495878524767], 
reward next is 0.8645, 
noisyNet noise sample is [array([-0.21455906], dtype=float32), 1.2099303]. 
=============================================
[2019-04-08 15:26:34,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.75972284e-19 4.82115910e-12 4.18080020e-07 1.89393354e-08
 2.97041280e-11 6.09132267e-15 1.21086663e-09 2.03578056e-16
 2.03327618e-21 1.03754104e-22 9.99999523e-01], sum to 1.0000
[2019-04-08 15:26:34,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5720
[2019-04-08 15:26:34,108] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8, 92.0, 102.0, 0.0, 22.5, 28.06307359060806, 1.094879528657772, 1.0, 1.0, 65.0, 26594.96837021627], 
current ob forecast is [], 
actual action is [5.8, 65.0], 
sim time this is 1333800.0000, 
sim time next is 1334400.0000, 
raw observation next is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 22.5, 28.08031169809279, 1.10119319388343, 1.0, 1.0, 65.0, 26649.83206805423], 
processed observation next is [1.0, 0.43478260869565216, 0.48753462603878117, 0.92, 0.353888888888889, 0.0, 0.375, 0.8400259748410658, 0.8670643979611433, 1.0, 1.0, 1.0, 0.12690396222882966], 
reward next is 0.8731, 
noisyNet noise sample is [array([-2.2270625], dtype=float32), 0.5909134]. 
=============================================
[2019-04-08 15:26:34,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1864079e-19 6.2151902e-11 2.5977701e-07 1.0722810e-06 3.6056626e-11
 1.8490061e-13 2.2155980e-07 2.1446984e-15 7.2930338e-19 2.8946093e-21
 9.9999845e-01], sum to 1.0000
[2019-04-08 15:26:34,212] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1345
[2019-04-08 15:26:34,241] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 96.0, 14.0, 0.0, 19.0, 28.16070014851019, 1.245556435708791, 0.0, 1.0, 65.0, 21166.90426148801], 
current ob forecast is [], 
actual action is [20.0, 65.0], 
sim time this is 1238400.0000, 
sim time next is 1239000.0000, 
raw observation next is [15.0, 96.66666666666666, 18.66666666666667, 0.0, 19.0, 28.14769766320428, 1.247067208653357, 0.0, 1.0, 65.0, 21943.37432866294], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9666666666666666, 0.06222222222222224, 0.0, 0.08333333333333333, 0.84564147193369, 0.9156890695511191, 0.0, 1.0, 1.0, 0.10449225870791876], 
reward next is 0.8955, 
noisyNet noise sample is [array([-1.8537658], dtype=float32), 0.1547182]. 
=============================================
[2019-04-08 15:26:34,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[89.222786]
 [89.17087 ]
 [89.13741 ]
 [89.11934 ]
 [89.140305]], R is [[89.28020477]
 [89.28661346]
 [89.29273224]
 [89.2938385 ]
 [89.29718018]].
[2019-04-08 15:26:35,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4521108e-16 1.2282202e-09 9.6333122e-07 7.9289885e-06 1.5757873e-10
 5.2806751e-14 1.3172871e-07 3.7240105e-14 8.5997601e-18 1.7913296e-20
 9.9999094e-01], sum to 1.0000
[2019-04-08 15:26:35,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5350
[2019-04-08 15:26:35,060] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.95, 98.0, 0.0, 0.0, 19.0, 27.53899664756715, 1.121924775407355, 0.0, 1.0, 65.0, 34187.58275166755], 
current ob forecast is [], 
actual action is [9.95, 65.0], 
sim time this is 1294200.0000, 
sim time next is 1294800.0000, 
raw observation next is [4.766666666666667, 97.33333333333334, 0.0, 0.0, 19.0, 27.51687474090181, 1.117325619981055, 0.0, 1.0, 65.0, 34987.99593219038], 
processed observation next is [0.0, 1.0, 0.5946445060018468, 0.9733333333333334, 0.0, 0.0, 0.08333333333333333, 0.7930728950751508, 0.8724418733270184, 0.0, 1.0, 1.0, 0.16660950443900183], 
reward next is 0.8334, 
noisyNet noise sample is [array([-0.14949673], dtype=float32), 2.0233393]. 
=============================================
[2019-04-08 15:26:35,371] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1914860e-17 1.6445088e-10 1.0474740e-06 6.2651474e-05 1.2638509e-09
 2.8332049e-13 3.3259352e-07 8.7867200e-15 5.9772882e-18 3.3663921e-22
 9.9993598e-01], sum to 1.0000
[2019-04-08 15:26:35,372] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4986
[2019-04-08 15:26:35,396] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 19.0, 27.38132079328735, 1.035300295126766, 0.0, 1.0, 65.0, 36967.61119481589], 
current ob forecast is [], 
actual action is [6.9, 65.0], 
sim time this is 1312200.0000, 
sim time next is 1312800.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 19.0, 27.4041284068813, 1.025038181717619, 0.0, 1.0, 65.0, 36844.70993383083], 
processed observation next is [1.0, 0.17391304347826086, 0.5124653739612189, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7836773672401082, 0.841679393905873, 0.0, 1.0, 1.0, 0.17545099968490874], 
reward next is 0.8245, 
noisyNet noise sample is [array([-0.4244577], dtype=float32), -0.5979726]. 
=============================================
[2019-04-08 15:26:35,583] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.41879223e-16 6.83822374e-11 1.65206495e-06 1.06002946e-04
 1.82762561e-09 8.36180777e-12 4.22486181e-07 3.04467280e-13
 5.28350346e-19 1.66104299e-21 9.99891877e-01], sum to 1.0000
[2019-04-08 15:26:35,598] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4440
[2019-04-08 15:26:35,616] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.533333333333334, 92.0, 0.0, 0.0, 19.0, 27.4499417631887, 1.054063888495676, 0.0, 1.0, 65.0, 36019.04612200054], 
current ob forecast is [], 
actual action is [7.533333333333334, 65.0], 
sim time this is 1308000.0000, 
sim time next is 1308600.0000, 
raw observation next is [2.45, 92.0, 0.0, 0.0, 19.0, 27.4353644069499, 1.060561081846491, 0.0, 1.0, 65.0, 36923.26186116946], 
processed observation next is [1.0, 0.13043478260869565, 0.5304709141274239, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7862803672458251, 0.8535203606154971, 0.0, 1.0, 1.0, 0.17582505648175936], 
reward next is 0.8242, 
noisyNet noise sample is [array([-1.4675262], dtype=float32), 0.07788614]. 
=============================================
[2019-04-08 15:26:36,363] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.3709271e-18 7.6701999e-11 5.5768805e-07 8.1514963e-07 2.1253600e-11
 4.3501617e-12 6.8411263e-09 8.8730848e-14 3.9385034e-18 1.9876156e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:26:36,363] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5337
[2019-04-08 15:26:36,395] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 19.0, 27.80348781186806, 1.214297610655394, 0.0, 1.0, 65.0, 29308.55728939023], 
current ob forecast is [], 
actual action is [12.2, 65.0], 
sim time this is 1278000.0000, 
sim time next is 1278600.0000, 
raw observation next is [7.016666666666667, 96.0, 0.0, 0.0, 19.0, 27.7826774994869, 1.209881995636527, 0.0, 1.0, 65.0, 29682.29032094651], 
processed observation next is [0.0, 0.8260869565217391, 0.656971375807941, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8152231249572418, 0.9032939985455091, 0.0, 1.0, 1.0, 0.1413442396235548], 
reward next is 0.8587, 
noisyNet noise sample is [array([1.2009339], dtype=float32), 1.106452]. 
=============================================
[2019-04-08 15:26:36,426] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2319579e-18 4.1752490e-11 2.0912059e-06 2.2228078e-06 2.0388517e-11
 4.8211934e-13 2.6903749e-08 1.9221974e-14 1.3522406e-19 4.5067720e-21
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:26:36,432] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2930
[2019-04-08 15:26:36,450] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 109.5, 0.0, 22.5, 28.0018997695584, 1.111731821658503, 1.0, 1.0, 65.0, 28154.8452993608], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [1.1, 92.0, 108.3333333333333, 0.0, 22.5, 27.96770754586499, 1.12220150273745, 1.0, 1.0, 65.0, 29122.58608140579], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.361111111111111, 0.0, 0.375, 0.8306422954887491, 0.8740671675791499, 1.0, 1.0, 1.0, 0.13867898134002757], 
reward next is 0.8613, 
noisyNet noise sample is [array([-0.7195535], dtype=float32), -0.51791275]. 
=============================================
[2019-04-08 15:26:36,608] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8813524e-17 5.4702458e-11 5.6588387e-06 3.4564798e-07 6.2144873e-11
 1.3032379e-12 1.5117884e-08 4.5904451e-15 2.9220596e-18 2.5268330e-20
 9.9999404e-01], sum to 1.0000
[2019-04-08 15:26:36,609] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4577
[2019-04-08 15:26:36,642] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 5.999999999999998, 0.0, 22.5, 27.27948799158139, 0.9709507793091299, 1.0, 1.0, 65.0, 38619.99151500103], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1324200.0000, 
sim time next is 1324800.0000, 
raw observation next is [1.1, 92.0, 9.0, 0.0, 22.5, 27.23139191216235, 0.9746707962668671, 1.0, 1.0, 65.0, 40907.36178463569], 
processed observation next is [1.0, 0.34782608695652173, 0.49307479224376743, 0.92, 0.03, 0.0, 0.375, 0.7692826593468626, 0.824890265422289, 1.0, 1.0, 1.0, 0.19479696087921755], 
reward next is 0.8052, 
noisyNet noise sample is [array([-1.2651539], dtype=float32), 1.780604]. 
=============================================
[2019-04-08 15:26:37,109] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9894261e-18 2.1443987e-11 2.1349166e-07 1.1426257e-08 9.9895196e-11
 2.1496506e-13 2.9653346e-08 2.6334024e-15 3.4635711e-18 1.1066965e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:26:37,109] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6542
[2019-04-08 15:26:37,127] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 92.0, 110.3333333333333, 0.0, 22.5, 28.10231363615768, 1.106515981063236, 1.0, 1.0, 65.0, 26190.74912383604], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 1335000.0000, 
sim time next is 1335600.0000, 
raw observation next is [1.1, 92.0, 114.5, 0.0, 22.5, 28.12563803871739, 1.11097006059546, 1.0, 1.0, 65.0, 25656.113830489], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.38166666666666665, 0.0, 0.375, 0.8438031698931159, 0.8703233535318201, 1.0, 1.0, 1.0, 0.12217197062137619], 
reward next is 0.8778, 
noisyNet noise sample is [array([-0.57471794], dtype=float32), 0.9645437]. 
=============================================
[2019-04-08 15:26:38,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7755683e-17 2.1259604e-10 3.2244025e-06 5.9687700e-06 1.1253143e-09
 1.3825169e-12 1.6749793e-07 5.2585811e-14 6.6761179e-19 1.3629628e-21
 9.9999058e-01], sum to 1.0000
[2019-04-08 15:26:38,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3323
[2019-04-08 15:26:38,444] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.7000000000000001, 95.0, 15.0, 0.0, 22.5, 28.02616755374978, 1.136440456159073, 1.0, 1.0, 65.0, 31892.28009582874], 
current ob forecast is [], 
actual action is [5.7, 65.0], 
sim time this is 1356000.0000, 
sim time next is 1356600.0000, 
raw observation next is [0.6, 95.5, 12.0, 0.0, 22.5, 28.00433639922672, 0.9789263205768245, 1.0, 1.0, 65.0, 44867.25017614805], 
processed observation next is [1.0, 0.6956521739130435, 0.479224376731302, 0.955, 0.04, 0.0, 0.375, 0.83369469993556, 0.8263087735256082, 1.0, 1.0, 1.0, 0.21365357226737167], 
reward next is 0.7863, 
noisyNet noise sample is [array([0.1418788], dtype=float32), -0.9103647]. 
=============================================
[2019-04-08 15:26:38,813] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1355603e-18 2.8961420e-11 2.0883592e-06 3.4937029e-06 4.3915038e-11
 1.2528334e-13 5.1124239e-08 7.0707166e-14 9.6862064e-18 4.1727569e-20
 9.9999440e-01], sum to 1.0000
[2019-04-08 15:26:38,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7199
[2019-04-08 15:26:38,845] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.87665686221494, 0.9097728678169168, 0.0, 1.0, 65.0, 44192.11069904358], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 1381200.0000, 
sim time next is 1381800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.86404930768591, 0.911482149869979, 0.0, 1.0, 65.0, 44014.00661678472], 
processed observation next is [1.0, 1.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7386707756404925, 0.803827383289993, 0.0, 1.0, 1.0, 0.20959050769897486], 
reward next is 0.7904, 
noisyNet noise sample is [array([0.24375327], dtype=float32), -1.1608475]. 
=============================================
[2019-04-08 15:26:39,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2016537e-16 1.1377589e-08 5.0447770e-06 8.1721264e-05 2.0847775e-09
 8.4028739e-12 2.2041912e-05 1.5897522e-13 3.3947971e-19 1.4750208e-18
 9.9989116e-01], sum to 1.0000
[2019-04-08 15:26:39,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6268
[2019-04-08 15:26:39,695] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.09999999999999999, 95.83333333333334, 0.0, 0.0, 19.0, 26.99963644594588, 0.8873253993705258, 0.0, 1.0, 65.0, 39693.34907394773], 
current ob forecast is [], 
actual action is [4.9, 65.0], 
sim time this is 1393800.0000, 
sim time next is 1394400.0000, 
raw observation next is [-0.2, 96.66666666666667, 0.0, 0.0, 19.0, 27.03516167788157, 0.8873245591957303, 0.0, 1.0, 65.0, 39126.62539255167], 
processed observation next is [1.0, 0.13043478260869565, 0.4570637119113574, 0.9666666666666667, 0.0, 0.0, 0.08333333333333333, 0.7529301398234644, 0.7957748530652434, 0.0, 1.0, 1.0, 0.18631726377405558], 
reward next is 0.8137, 
noisyNet noise sample is [array([-0.71848935], dtype=float32), -0.2887184]. 
=============================================
[2019-04-08 15:26:39,747] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1657927e-19 6.1993796e-12 3.0830188e-07 1.5132632e-06 6.6904442e-12
 1.6594260e-15 7.7191614e-10 8.1632377e-17 4.4250968e-21 1.2598944e-22
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:26:39,748] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0638
[2019-04-08 15:26:39,761] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.2, 73.0, 92.5, 700.5, 22.5, 28.13252072568645, 1.127500815022343, 1.0, 1.0, 65.0, 18846.57328194405], 
current ob forecast is [], 
actual action is [12.2, 65.0], 
sim time this is 1515600.0000, 
sim time next is 1516200.0000, 
raw observation next is [7.666666666666666, 71.33333333333334, 90.0, 700.6666666666666, 22.5, 28.18944078213882, 1.13689941897545, 1.0, 1.0, 65.0, 18847.39241535271], 
processed observation next is [1.0, 0.5652173913043478, 0.674976915974146, 0.7133333333333334, 0.3, 0.7742173112338858, 0.375, 0.849120065178235, 0.8789664729918166, 1.0, 1.0, 1.0, 0.08974948769215577], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.63870835], dtype=float32), 1.4407713]. 
=============================================
[2019-04-08 15:26:40,261] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8989913e-17 5.9616340e-10 5.0375133e-06 1.3244104e-06 5.1202487e-10
 6.0703933e-12 4.8000697e-07 2.9111666e-13 1.3287722e-18 3.7143091e-21
 9.9999321e-01], sum to 1.0000
[2019-04-08 15:26:40,261] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9886
[2019-04-08 15:26:40,288] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 100.0, 15.0, 0.0, 22.5, 26.84147749317486, 0.8676616830750487, 1.0, 1.0, 64.99999999999997, 39373.74229221243], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 1412400.0000, 
sim time next is 1413000.0000, 
raw observation next is [-0.6, 100.0, 18.0, 0.0, 22.5, 27.16302855663515, 0.8717181245747065, 1.0, 1.0, 65.0, 29380.80734403069], 
processed observation next is [1.0, 0.34782608695652173, 0.44598337950138506, 1.0, 0.06, 0.0, 0.375, 0.7635857130529292, 0.7905727081915689, 1.0, 1.0, 1.0, 0.13990860640014616], 
reward next is 0.8601, 
noisyNet noise sample is [array([-1.4633762], dtype=float32), -1.2045653]. 
=============================================
[2019-04-08 15:26:40,302] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9950082e-17 8.0454726e-11 1.1544462e-08 2.9334643e-07 3.6168596e-11
 6.0664676e-14 4.5642841e-09 1.4716702e-15 1.1148760e-19 2.5003814e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:26:40,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.72412]
 [85.94273]
 [84.92642]
 [84.49824]
 [83.95938]], R is [[86.68058777]
 [86.62628937]
 [86.56248474]
 [86.49584961]
 [86.42990112]].
[2019-04-08 15:26:40,321] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4627
[2019-04-08 15:26:40,359] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 54.5, 0.0, 22.5, 27.97332537735017, 1.06829248962101, 1.0, 1.0, 65.0, 26355.99408880648], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1330800.0000, 
sim time next is 1331400.0000, 
raw observation next is [0.5, 92.0, 64.0, 0.0, 22.5, 27.99480528202259, 1.068383469984203, 1.0, 1.0, 65.0, 27337.88092078199], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.21333333333333335, 0.0, 0.375, 0.8329004401685491, 0.8561278233280677, 1.0, 1.0, 1.0, 0.1301803853370571], 
reward next is 0.8698, 
noisyNet noise sample is [array([-1.5041314], dtype=float32), 1.7701436]. 
=============================================
[2019-04-08 15:26:40,365] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.1939435e-17 9.2929273e-09 1.5054415e-05 4.3872871e-05 2.1410402e-08
 4.2764851e-11 1.4875882e-06 2.6696023e-13 1.3799086e-17 8.4929058e-19
 9.9993956e-01], sum to 1.0000
[2019-04-08 15:26:40,368] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9779
[2019-04-08 15:26:40,397] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 27.13129205936337, 1.00171113244788, 0.0, 1.0, 65.0, 38933.25658205909], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1369800.0000, 
sim time next is 1370400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 27.1194160004135, 0.9991678696529845, 0.0, 1.0, 65.0, 38922.51250081512], 
processed observation next is [1.0, 0.8695652173913043, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7599513333677917, 0.8330559565509948, 0.0, 1.0, 1.0, 0.18534529762292914], 
reward next is 0.8147, 
noisyNet noise sample is [array([0.8603039], dtype=float32), -0.8042659]. 
=============================================
[2019-04-08 15:26:40,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6763275e-14 3.0950435e-08 8.6603330e-05 2.1632145e-04 1.8852824e-09
 9.7379736e-11 8.4468784e-06 2.1863162e-12 4.7243949e-15 3.5515202e-18
 9.9968863e-01], sum to 1.0000
[2019-04-08 15:26:40,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2662
[2019-04-08 15:26:40,495] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.87934681822555, 0.9019591541083997, 0.0, 1.0, 65.0, 43118.84242510068], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 1383000.0000, 
sim time next is 1383600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.86330165193537, 0.9207888787851299, 0.0, 1.0, 65.0, 43289.75265316966], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7386084709946141, 0.8069296262617099, 0.0, 1.0, 1.0, 0.2061416793008079], 
reward next is 0.7939, 
noisyNet noise sample is [array([-0.51359177], dtype=float32), -0.5732168]. 
=============================================
[2019-04-08 15:26:40,653] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5410099e-16 3.4163272e-10 4.6257858e-07 8.5327434e-07 2.9311825e-10
 1.3915953e-14 3.6524122e-07 6.9349101e-14 2.0896973e-18 1.5060985e-20
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:26:40,661] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1501
[2019-04-08 15:26:40,672] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 118.6666666666667, 0.0, 22.5, 28.14109390573845, 1.115179024174336, 1.0, 1.0, 65.0, 25477.20201581029], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1336200.0000, 
sim time next is 1336800.0000, 
raw observation next is [1.1, 92.0, 122.8333333333333, 0.0, 22.5, 28.15416466014154, 1.11916212449505, 1.0, 1.0, 65.0, 25371.07137243435], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.40944444444444433, 0.0, 0.375, 0.8461803883451283, 0.87305404149835, 1.0, 1.0, 1.0, 0.12081462558302072], 
reward next is 0.8792, 
noisyNet noise sample is [array([-1.0476749], dtype=float32), 0.8436712]. 
=============================================
[2019-04-08 15:26:40,837] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9625648e-16 4.8447601e-09 6.1146197e-06 3.2246835e-06 3.2401475e-09
 9.8967883e-11 1.5885601e-06 8.8898599e-13 3.2864855e-17 6.4829318e-20
 9.9998903e-01], sum to 1.0000
[2019-04-08 15:26:40,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9655
[2019-04-08 15:26:40,863] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.87168377198763, 0.8822324120468396, 0.0, 1.0, 65.0, 43605.82082420677], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 1389000.0000, 
sim time next is 1389600.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.85471247095172, 0.8767553967033787, 0.0, 1.0, 65.0, 43369.54087900927], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7378927059126434, 0.7922517989011263, 0.0, 1.0, 1.0, 0.20652162323337747], 
reward next is 0.7935, 
noisyNet noise sample is [array([0.24808872], dtype=float32), -0.34707814]. 
=============================================
[2019-04-08 15:26:41,618] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5633276e-16 1.0909058e-09 4.4912477e-07 2.0626261e-05 8.2597852e-11
 1.2291648e-11 8.7764576e-08 1.7216255e-13 6.2249642e-18 1.7430775e-19
 9.9997878e-01], sum to 1.0000
[2019-04-08 15:26:41,618] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6707
[2019-04-08 15:26:41,647] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.31747624440634, 1.022082932157895, 0.0, 1.0, 65.0, 36585.37124022582], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1366200.0000, 
sim time next is 1366800.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.25667468425619, 1.018270830253007, 0.0, 1.0, 65.0, 38690.81615427707], 
processed observation next is [1.0, 0.8260869565217391, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7713895570213491, 0.8394236100843356, 0.0, 1.0, 1.0, 0.18424198168703365], 
reward next is 0.8158, 
noisyNet noise sample is [array([-1.5528483], dtype=float32), 0.08608646]. 
=============================================
[2019-04-08 15:26:42,159] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.17267811e-16 1.40456065e-08 2.11951527e-04 1.93911746e-05
 5.02377417e-10 3.32502879e-11 1.26644920e-07 5.43373029e-12
 2.18321832e-17 2.31123724e-19 9.99768555e-01], sum to 1.0000
[2019-04-08 15:26:42,160] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0071
[2019-04-08 15:26:42,177] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 93.0, 0.0, 22.5, 26.33526523241708, 0.8811427322650206, 1.0, 1.0, 65.00000000000018, 42096.72622954949], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1429200.0000, 
sim time next is 1429800.0000, 
raw observation next is [0.6000000000000001, 92.0, 92.0, 0.0, 22.5, 27.2503164599142, 0.9252267099280856, 1.0, 1.0, 65.0, 29006.02853958978], 
processed observation next is [1.0, 0.5652173913043478, 0.479224376731302, 0.92, 0.30666666666666664, 0.0, 0.375, 0.77085970499285, 0.8084089033093619, 1.0, 1.0, 1.0, 0.13812394542661802], 
reward next is 0.8619, 
noisyNet noise sample is [array([-0.27920255], dtype=float32), 0.82225627]. 
=============================================
[2019-04-08 15:26:42,707] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9054691e-16 1.2220548e-10 8.4606227e-06 5.2134601e-07 1.2625002e-10
 8.7562622e-12 3.0877476e-07 6.3879098e-14 3.9473337e-17 3.5793625e-19
 9.9999070e-01], sum to 1.0000
[2019-04-08 15:26:42,716] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5173
[2019-04-08 15:26:42,735] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.86405216600004, 0.91148307981143, 0.0, 1.0, 65.0, 44013.97474837593], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 1381800.0000, 
sim time next is 1382400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.88802034072073, 0.906344040685561, 0.0, 1.0, 65.0, 43051.29730666818], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7406683617267275, 0.8021146802285203, 0.0, 1.0, 1.0, 0.20500617765080087], 
reward next is 0.7950, 
noisyNet noise sample is [array([1.7058347], dtype=float32), 0.8702736]. 
=============================================
[2019-04-08 15:26:45,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.6648486e-15 1.0100683e-08 6.7519718e-06 7.5229782e-06 1.6526655e-10
 1.9953088e-11 9.3893753e-08 1.4670695e-12 9.3104379e-17 4.1662963e-19
 9.9998558e-01], sum to 1.0000
[2019-04-08 15:26:45,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6536
[2019-04-08 15:26:45,678] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.516666666666667, 89.5, 0.0, 0.0, 19.0, 26.99634848465908, 0.909001808669596, 0.0, 1.0, 65.0, 37055.55330759872], 
current ob forecast is [], 
actual action is [6.5166666666666675, 65.0], 
sim time this is 1457400.0000, 
sim time next is 1458000.0000, 
raw observation next is [1.6, 89.0, 0.0, 0.0, 19.0, 27.0093054031059, 0.9080721547174827, 0.0, 1.0, 65.0, 36514.44508829164], 
processed observation next is [1.0, 0.9130434782608695, 0.5069252077562327, 0.89, 0.0, 0.0, 0.08333333333333333, 0.7507754502588249, 0.8026907182391608, 0.0, 1.0, 1.0, 0.1738783099442459], 
reward next is 0.8261, 
noisyNet noise sample is [array([-0.04485108], dtype=float32), -0.3352348]. 
=============================================
[2019-04-08 15:26:45,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.630394]
 [75.11798 ]
 [75.430336]
 [75.73001 ]
 [76.248665]], R is [[74.1636734 ]
 [74.24558258]
 [74.3258667 ]
 [74.40499115]
 [74.48425293]].
[2019-04-08 15:26:45,921] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.59577474e-16 1.50410340e-09 1.10604415e-05 5.68425276e-06
 5.78363635e-10 2.33865293e-12 1.17039480e-08 4.05572223e-13
 4.44415316e-18 1.04580277e-19 9.99983191e-01], sum to 1.0000
[2019-04-08 15:26:45,921] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0721
[2019-04-08 15:26:45,934] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 91.0, 0.0, 22.5, 27.68066142513577, 0.9780070581435306, 1.0, 1.0, 65.0, 28600.48260158246], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 1424400.0000, 
sim time next is 1425000.0000, 
raw observation next is [0.0, 95.0, 92.0, 0.0, 22.5, 27.67678246225978, 0.9835901372330049, 1.0, 1.0, 65.0, 28031.70992566464], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.30666666666666664, 0.0, 0.375, 0.8063985385216483, 0.8278633790776683, 1.0, 1.0, 1.0, 0.13348433297935544], 
reward next is 0.8665, 
noisyNet noise sample is [array([-0.50336546], dtype=float32), -0.3884119]. 
=============================================
[2019-04-08 15:26:45,952] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[85.504036]
 [85.49366 ]
 [85.51774 ]
 [85.47731 ]
 [85.46734 ]], R is [[85.50650787]
 [85.51525116]
 [85.51777649]
 [85.52401733]
 [85.53435516]].
[2019-04-08 15:26:45,989] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2188389e-14 1.8517733e-08 3.0579173e-05 2.6885883e-04 7.3032984e-09
 6.0272412e-11 1.2195100e-07 1.4947321e-11 4.2601154e-16 1.4326317e-17
 9.9970043e-01], sum to 1.0000
[2019-04-08 15:26:45,989] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1039
[2019-04-08 15:26:46,009] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.433333333333334, 90.0, 0.0, 0.0, 19.0, 27.00768362386659, 0.910785235912909, 0.0, 1.0, 65.0, 36445.63813633382], 
current ob forecast is [], 
actual action is [6.433333333333334, 65.0], 
sim time this is 1459200.0000, 
sim time next is 1459800.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 19.0, 27.03659680579796, 0.9077549532142619, 0.0, 1.0, 65.0, 35530.54075711311], 
processed observation next is [1.0, 0.9130434782608695, 0.5000000000000001, 0.905, 0.0, 0.0, 0.08333333333333333, 0.7530497338164966, 0.802584984404754, 0.0, 1.0, 1.0, 0.16919305122434813], 
reward next is 0.8308, 
noisyNet noise sample is [array([-0.03721459], dtype=float32), 0.6170982]. 
=============================================
[2019-04-08 15:26:46,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7041841e-16 8.8785346e-10 7.2174313e-05 1.5828780e-04 8.0856938e-10
 1.6526572e-11 1.2698911e-07 7.5786746e-13 1.8953823e-17 1.5273045e-18
 9.9976939e-01], sum to 1.0000
[2019-04-08 15:26:46,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9025
[2019-04-08 15:26:46,140] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 97.5, 46.0, 0.0, 22.5, 27.55114575715776, 0.9330891357291516, 1.0, 1.0, 65.0, 30346.06872538682], 
current ob forecast is [], 
actual action is [4.7, 65.0], 
sim time this is 1416600.0000, 
sim time next is 1417200.0000, 
raw observation next is [-0.2, 96.66666666666666, 50.33333333333333, 0.0, 22.5, 27.60110456000452, 0.9353565282002368, 1.0, 1.0, 65.0, 28134.75133010642], 
processed observation next is [1.0, 0.391304347826087, 0.4570637119113574, 0.9666666666666666, 0.16777777777777778, 0.0, 0.375, 0.8000920466670433, 0.811785509400079, 1.0, 1.0, 1.0, 0.1339750063338401], 
reward next is 0.8660, 
noisyNet noise sample is [array([0.96724254], dtype=float32), -0.38593924]. 
=============================================
[2019-04-08 15:26:46,744] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.67968149e-17 1.88497981e-10 1.73408822e-07 1.33705072e-07
 5.97139976e-11 1.99177575e-13 5.54502648e-08 5.89228051e-15
 8.61841807e-19 1.13823994e-20 9.99999642e-01], sum to 1.0000
[2019-04-08 15:26:46,744] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9439
[2019-04-08 15:26:46,783] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.0, 26.86928566934199, 0.8112695946158929, 0.0, 1.0, 65.0, 40010.96420104209], 
current ob forecast is [], 
actual action is [7.2, 65.0], 
sim time this is 1489800.0000, 
sim time next is 1490400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 26.894822058128, 0.807708745706198, 0.0, 1.0, 65.0, 39160.86044867794], 
processed observation next is [1.0, 0.2608695652173913, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7412351715106666, 0.7692362485687326, 0.0, 1.0, 1.0, 0.18648028785084733], 
reward next is 0.8135, 
noisyNet noise sample is [array([0.278214], dtype=float32), 0.5454299]. 
=============================================
[2019-04-08 15:26:47,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5729138e-15 9.1824264e-09 1.9379309e-05 2.5558958e-05 5.6918386e-09
 2.0638725e-11 1.3817292e-06 4.7165423e-12 1.5188881e-17 1.0922246e-18
 9.9995363e-01], sum to 1.0000
[2019-04-08 15:26:47,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5817
[2019-04-08 15:26:47,393] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 94.0, 0.0, 0.0, 19.0, 26.92335585990673, 0.8238620620171383, 0.0, 1.0, 65.0, 38311.21221937838], 
current ob forecast is [], 
actual action is [7.2, 65.0], 
sim time this is 1479600.0000, 
sim time next is 1480200.0000, 
raw observation next is [2.2, 94.33333333333334, 0.0, 0.0, 19.0, 26.88330485910311, 0.8190360041045345, 0.0, 1.0, 65.0, 39058.46914430401], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.9433333333333335, 0.0, 0.0, 0.08333333333333333, 0.7402754049252591, 0.7730120013681782, 0.0, 1.0, 1.0, 0.18599271021097147], 
reward next is 0.8140, 
noisyNet noise sample is [array([-0.28384414], dtype=float32), -0.20087261]. 
=============================================
[2019-04-08 15:26:48,062] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9613531e-16 1.3345073e-09 1.3074925e-06 1.1203169e-05 3.1164207e-10
 2.8652100e-10 6.6115695e-07 2.6341494e-13 8.8707167e-17 1.5870492e-18
 9.9998677e-01], sum to 1.0000
[2019-04-08 15:26:48,064] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0433
[2019-04-08 15:26:48,103] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 93.0, 0.0, 0.0, 19.0, 27.01656528225239, 0.8349335497276185, 0.0, 1.0, 65.0, 35049.39361935359], 
current ob forecast is [], 
actual action is [7.2, 65.0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [2.2, 93.33333333333334, 0.0, 0.0, 19.0, 26.98664385665762, 0.8332591791010128, 0.0, 1.0, 65.0, 37069.55144366199], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.7488869880548016, 0.7777530597003376, 0.0, 1.0, 1.0, 0.1765216735412476], 
reward next is 0.8235, 
noisyNet noise sample is [array([0.23578799], dtype=float32), 0.2670899]. 
=============================================
[2019-04-08 15:26:48,405] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1138726e-17 3.0942859e-11 3.6313979e-06 1.7575054e-06 2.8532551e-11
 2.5041015e-13 5.7365295e-09 2.3159219e-15 6.7199393e-19 4.4868001e-21
 9.9999464e-01], sum to 1.0000
[2019-04-08 15:26:48,407] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0182
[2019-04-08 15:26:48,427] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.8, 57.5, 75.0, 663.0, 22.5, 28.78074265886631, 1.247353745887161, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [15.8, 65.0], 
sim time this is 1521000.0000, 
sim time next is 1521600.0000, 
raw observation next is [11.06666666666667, 55.66666666666667, 75.33333333333333, 632.1666666666666, 22.5, 28.82445214357537, 1.256367407714033, 1.0, 1.0, 65.0, 18849.41363896852], 
processed observation next is [1.0, 0.6086956521739131, 0.7691597414589106, 0.5566666666666668, 0.2511111111111111, 0.6985267034990792, 0.375, 0.9020376786312809, 0.9187891359046777, 1.0, 1.0, 1.0, 0.08975911256651675], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.111716], dtype=float32), -1.5798371]. 
=============================================
[2019-04-08 15:26:49,331] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.36102197e-15 9.32556921e-10 1.39716785e-05 3.28353744e-05
 3.50255114e-09 1.81161960e-11 8.76381819e-07 3.40086114e-13
 1.58394698e-17 5.13079920e-19 9.99952316e-01], sum to 1.0000
[2019-04-08 15:26:49,331] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2465
[2019-04-08 15:26:49,344] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 26.84273002220472, 0.8428014117844914, 0.0, 1.0, 65.0, 40077.75276697249], 
current ob forecast is [], 
actual action is [6.6, 65.0], 
sim time this is 1468200.0000, 
sim time next is 1468800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 26.83780508448744, 0.8409142265212141, 0.0, 1.0, 65.0, 39972.72181569693], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7364837570406199, 0.780304742173738, 0.0, 1.0, 1.0, 0.19034629436046158], 
reward next is 0.8097, 
noisyNet noise sample is [array([0.7576682], dtype=float32), 1.5126826]. 
=============================================
[2019-04-08 15:26:49,572] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0129353e-18 2.0017608e-10 4.5207435e-07 3.7202317e-06 1.8099645e-11
 1.8321366e-13 2.2679442e-08 1.0142141e-15 7.4565142e-19 9.2106087e-22
 9.9999583e-01], sum to 1.0000
[2019-04-08 15:26:49,573] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4323
[2019-04-08 15:26:49,632] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.333333333333334, 86.33333333333334, 98.0, 701.3333333333334, 22.5, 28.11661217074406, 0.902522856940447, 1.0, 1.0, 65.0, 21144.0941069386], 
current ob forecast is [], 
actual action is [10.333333333333334, 65.0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [5.800000000000001, 83.0, 100.0, 700.0, 22.5, 28.00839636010161, 1.13784705289158, 1.0, 1.0, 65.0, 35103.91332469683], 
processed observation next is [1.0, 0.5217391304347826, 0.6232686980609419, 0.83, 0.3333333333333333, 0.7734806629834254, 0.375, 0.8340330300084675, 0.87928235096386, 1.0, 1.0, 1.0, 0.16716149202236585], 
reward next is 0.8328, 
noisyNet noise sample is [array([1.5324332], dtype=float32), -0.10001181]. 
=============================================
[2019-04-08 15:26:49,941] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.9064046e-18 2.1921240e-09 1.0254947e-05 2.4262845e-05 5.2614860e-11
 1.0808220e-12 1.3990352e-07 8.0258218e-15 2.3920357e-18 7.1913467e-20
 9.9996531e-01], sum to 1.0000
[2019-04-08 15:26:49,941] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0704
[2019-04-08 15:26:49,961] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.8, 100.0, 42.16666666666667, 0.0, 22.5, 27.40208397574981, 0.8969993863822013, 1.0, 1.0, 65.0, 29693.77114583207], 
current ob forecast is [], 
actual action is [6.8, 65.0], 
sim time this is 1502400.0000, 
sim time next is 1503000.0000, 
raw observation next is [1.9, 100.0, 47.0, 0.0, 22.5, 27.55281894412219, 0.9102755283821229, 1.0, 1.0, 65.0, 25597.46210750539], 
processed observation next is [1.0, 0.391304347826087, 0.515235457063712, 1.0, 0.15666666666666668, 0.0, 0.375, 0.7960682453435158, 0.8034251761273743, 1.0, 1.0, 1.0, 0.12189267670240662], 
reward next is 0.8781, 
noisyNet noise sample is [array([-0.9754196], dtype=float32), -0.7262269]. 
=============================================
[2019-04-08 15:26:49,990] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.48139]
 [85.60511]
 [85.46194]
 [85.90927]
 [85.35549]], R is [[85.28041077]
 [85.28620911]
 [85.29138947]
 [85.29238892]
 [85.29024506]].
[2019-04-08 15:26:50,754] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.8777666e-16 6.5079604e-09 8.0888975e-07 1.9356610e-06 2.5255864e-10
 3.2657110e-13 8.4371244e-08 5.2947496e-13 5.9210726e-19 1.2164961e-20
 9.9999714e-01], sum to 1.0000
[2019-04-08 15:26:50,773] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4770
[2019-04-08 15:26:50,808] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.63333333333333, 52.66666666666667, 85.33333333333333, 103.0, 22.5, 29.07379673602033, 1.286658395012135, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.63333333333333, 65.0], 
sim time this is 1527600.0000, 
sim time next is 1528200.0000, 
raw observation next is [11.35, 54.0, 87.0, 28.0, 22.5, 29.10944517596375, 1.284884524585603, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7770083102493075, 0.54, 0.29, 0.030939226519337018, 0.375, 0.9257870979969791, 0.9282948415285345, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9463999], dtype=float32), 0.0009878693]. 
=============================================
[2019-04-08 15:26:51,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4938844e-16 1.8888491e-08 3.2412411e-06 3.6755227e-06 9.9994280e-10
 3.7853397e-11 2.8605024e-08 2.6496814e-13 3.2865098e-18 2.1083336e-19
 9.9999309e-01], sum to 1.0000
[2019-04-08 15:26:51,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0975
[2019-04-08 15:26:51,486] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.39495345506872, 1.012382594715437, 0.0, 1.0, 65.0, 34748.81630913828], 
current ob forecast is [], 
actual action is [10.0, 65.0], 
sim time this is 1555200.0000, 
sim time next is 1555800.0000, 
raw observation next is [5.0, 82.00000000000001, 0.0, 0.0, 19.0, 27.36719897803883, 1.00774866550311, 0.0, 1.0, 65.0, 35106.16403610624], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.8200000000000002, 0.0, 0.0, 0.08333333333333333, 0.7805999148365691, 0.83591622183437, 0.0, 1.0, 1.0, 0.16717220969574398], 
reward next is 0.8328, 
noisyNet noise sample is [array([-0.03240751], dtype=float32), 0.19590816]. 
=============================================
[2019-04-08 15:26:52,638] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1602180e-16 1.9591953e-09 3.4174580e-05 5.5612854e-06 1.6030520e-09
 4.6413212e-12 7.1172707e-07 6.2961469e-13 1.6363293e-16 1.0833629e-19
 9.9995959e-01], sum to 1.0000
[2019-04-08 15:26:52,639] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1363
[2019-04-08 15:26:52,649] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.5, 60.83333333333334, 0.0, 0.0, 22.5, 28.37812195128378, 1.202887561300487, 1.0, 1.0, 65.0, 18848.44850288804], 
current ob forecast is [], 
actual action is [14.5, 65.0], 
sim time this is 1536600.0000, 
sim time next is 1537200.0000, 
raw observation next is [9.4, 61.0, 0.0, 0.0, 22.5, 28.2981205135253, 1.19133493003854, 1.0, 1.0, 65.0, 18848.4252874966], 
processed observation next is [1.0, 0.8260869565217391, 0.7229916897506927, 0.61, 0.0, 0.0, 0.375, 0.8581767094604418, 0.89711164334618, 1.0, 1.0, 1.0, 0.08975440613093619], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.10171918], dtype=float32), -1.9107206]. 
=============================================
[2019-04-08 15:26:52,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7766465e-16 6.1398564e-10 9.3931842e-05 2.5124546e-06 9.3348593e-11
 1.6672504e-11 2.9473392e-07 8.2057611e-14 1.6487981e-18 4.2412167e-21
 9.9990320e-01], sum to 1.0000
[2019-04-08 15:26:52,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2520
[2019-04-08 15:26:52,747] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.25, 80.5, 0.0, 0.0, 19.0, 27.41059060893212, 0.9462211090882252, 0.0, 1.0, 65.0, 33662.09779521506], 
current ob forecast is [], 
actual action is [10.25, 65.0], 
sim time this is 1578600.0000, 
sim time next is 1579200.0000, 
raw observation next is [5.333333333333334, 80.0, 0.0, 0.0, 19.0, 27.399118537298, 0.9396532961539203, 0.0, 1.0, 65.0, 33169.82843929164], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 0.8, 0.0, 0.0, 0.08333333333333333, 0.7832598781081668, 0.8132177653846401, 0.0, 1.0, 1.0, 0.15795156399662685], 
reward next is 0.8420, 
noisyNet noise sample is [array([-2.1775029], dtype=float32), 0.22397268]. 
=============================================
[2019-04-08 15:26:53,695] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1595177e-16 1.4023091e-10 2.5113025e-07 1.4055400e-06 2.2870564e-10
 5.8270869e-13 7.6889144e-08 1.6510542e-14 1.7749996e-17 6.2432000e-20
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:26:53,699] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8337
[2019-04-08 15:26:53,710] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.91666666666667, 51.33333333333334, 83.66666666666667, 177.9999999999999, 22.5, 28.92415567797952, 1.283798989644711, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.91666666666667, 65.0], 
sim time this is 1527000.0000, 
sim time next is 1527600.0000, 
raw observation next is [11.63333333333333, 52.66666666666667, 85.33333333333333, 103.0, 22.5, 29.07380193863141, 1.286659877237662, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7848568790397045, 0.5266666666666667, 0.28444444444444444, 0.1138121546961326, 0.375, 0.922816828219284, 0.9288866257458874, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.59805566], dtype=float32), -0.9359887]. 
=============================================
[2019-04-08 15:26:54,332] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.2244070e-18 1.0895801e-09 5.6887874e-08 1.2094411e-06 6.9073726e-11
 3.9926089e-13 5.1399400e-08 4.7016393e-15 5.1817687e-18 3.2657792e-20
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:26:54,335] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5121
[2019-04-08 15:26:54,360] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.44325857406043, 0.9608833709319539, 0.0, 1.0, 65.0, 30827.84328183719], 
current ob forecast is [], 
actual action is [10.0, 65.0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 19.0, 27.50424733585255, 0.9514826691876999, 0.0, 1.0, 65.0, 30020.12090131234], 
processed observation next is [1.0, 0.2608695652173913, 0.6034164358264081, 0.8150000000000002, 0.0, 0.0, 0.08333333333333333, 0.7920206113210458, 0.8171608897292333, 0.0, 1.0, 1.0, 0.1429529566729159], 
reward next is 0.8570, 
noisyNet noise sample is [array([-1.9682964], dtype=float32), 0.48918456]. 
=============================================
[2019-04-08 15:26:54,364] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.26600906e-16 1.67314096e-09 3.95472398e-06 3.71388660e-06
 2.08840556e-10 3.13682934e-12 3.01763947e-07 1.89225619e-14
 1.06900236e-16 5.08041925e-20 9.99992013e-01], sum to 1.0000
[2019-04-08 15:26:54,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5147
[2019-04-08 15:26:54,382] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.366666666666667, 75.33333333333333, 0.0, 0.0, 19.0, 27.85489840022591, 1.16342500217569, 0.0, 1.0, 65.0, 26292.61896584746], 
current ob forecast is [], 
actual action is [12.366666666666667, 65.0], 
sim time this is 1629600.0000, 
sim time next is 1630200.0000, 
raw observation next is [7.283333333333333, 75.66666666666667, 0.0, 0.0, 19.0, 27.83237769074647, 1.162405113186865, 0.0, 1.0, 65.0, 26588.7946944284], 
processed observation next is [1.0, 0.8695652173913043, 0.6643582640812559, 0.7566666666666667, 0.0, 0.0, 0.08333333333333333, 0.8193648075622058, 0.8874683710622883, 0.0, 1.0, 1.0, 0.12661330806870666], 
reward next is 0.8734, 
noisyNet noise sample is [array([-0.34479955], dtype=float32), -1.1049309]. 
=============================================
[2019-04-08 15:26:54,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3387878e-17 2.0735862e-10 4.3405663e-07 4.7706021e-06 2.6773461e-10
 1.3667871e-10 1.9769068e-07 4.1381682e-13 1.7181844e-18 1.6907327e-19
 9.9999464e-01], sum to 1.0000
[2019-04-08 15:26:54,747] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5246
[2019-04-08 15:26:54,757] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.083333333333334, 81.5, 13.0, 15.0, 22.5, 27.33049688476089, 0.9262990601699025, 1.0, 1.0, 65.0, 34897.69252597649], 
current ob forecast is [], 
actual action is [10.083333333333334, 65.0], 
sim time this is 1583400.0000, 
sim time next is 1584000.0000, 
raw observation next is [5.0, 82.0, 19.0, 20.0, 22.5, 27.29488818570614, 0.9244790368598478, 1.0, 1.0, 65.0, 35619.5706956432], 
processed observation next is [1.0, 0.34782608695652173, 0.6011080332409973, 0.82, 0.06333333333333334, 0.022099447513812154, 0.375, 0.7745740154755115, 0.8081596789532827, 1.0, 1.0, 1.0, 0.16961700331258664], 
reward next is 0.8304, 
noisyNet noise sample is [array([3.003507], dtype=float32), -0.582378]. 
=============================================
[2019-04-08 15:26:54,801] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.38284]
 [84.683  ]
 [84.08715]
 [83.65546]
 [83.02258]], R is [[85.7089386 ]
 [85.68567657]
 [85.67559052]
 [85.67034149]
 [85.64810944]].
[2019-04-08 15:26:55,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2055609e-18 1.5916150e-10 5.0710605e-07 1.1033990e-06 1.0550512e-11
 5.9560100e-12 2.5766383e-08 7.9028862e-16 1.2473305e-17 7.1661120e-21
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:26:55,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1507
[2019-04-08 15:26:55,336] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.166666666666667, 81.0, 0.0, 0.0, 19.0, 27.46980217420683, 0.944224890291717, 0.0, 1.0, 65.0, 31580.83148964021], 
current ob forecast is [], 
actual action is [10.166666666666668, 65.0], 
sim time this is 1578000.0000, 
sim time next is 1578600.0000, 
raw observation next is [5.25, 80.5, 0.0, 0.0, 19.0, 27.41059027153823, 0.946220980954347, 0.0, 1.0, 65.0, 33662.10138407181], 
processed observation next is [1.0, 0.2608695652173913, 0.60803324099723, 0.805, 0.0, 0.0, 0.08333333333333333, 0.7842158559615191, 0.815406993651449, 0.0, 1.0, 1.0, 0.1602957208765324], 
reward next is 0.8397, 
noisyNet noise sample is [array([-0.07325612], dtype=float32), 0.07296162]. 
=============================================
[2019-04-08 15:26:56,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5122459e-16 2.1477144e-10 3.2463453e-07 3.2824079e-07 1.8489611e-10
 5.0304877e-13 3.8752326e-08 5.1925190e-13 1.8062786e-17 2.7969212e-20
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:26:57,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9551
[2019-04-08 15:26:57,029] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.38333333333333, 53.5, 33.66666666666667, 24.66666666666667, 22.5, 28.72066696401793, 1.310423903197337, 1.0, 1.0, 65.0, 36353.74885871406], 
current ob forecast is [], 
actual action is [17.38333333333333, 65.0], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [12.2, 54.0, 25.5, 18.5, 22.5, 28.2274273952794, 1.246825983255735, 1.0, 1.0, 65.00000000000009, 20894.78277625462], 
processed observation next is [1.0, 0.7391304347826086, 0.8005540166204987, 0.54, 0.085, 0.020441988950276244, 0.375, 0.8522856162732833, 0.915608661085245, 1.0, 1.0, 1.0000000000000018, 0.09949896560121246], 
reward next is 0.9005, 
noisyNet noise sample is [array([-0.8766855], dtype=float32), -0.41963243]. 
=============================================
[2019-04-08 15:26:57,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7708801e-16 3.0427788e-10 4.2427341e-06 1.0963642e-04 2.4405386e-10
 9.6591008e-12 4.6980610e-08 7.5275062e-13 2.6729002e-19 4.2789617e-20
 9.9988604e-01], sum to 1.0000
[2019-04-08 15:26:57,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8312
[2019-04-08 15:26:57,185] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.3, 51.0, 64.0, 18.5, 22.5, 29.13270029795662, 1.303435686419868, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [18.3, 65.0], 
sim time this is 1612800.0000, 
sim time next is 1613400.0000, 
raw observation next is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 22.5, 29.15578786815302, 1.307390170785633, 1.0, 1.0, 65.0, 18849.17993385146], 
processed observation next is [1.0, 0.6956521739130435, 0.8259464450600187, 0.515, 0.19777777777777777, 0.027255985267034995, 0.375, 0.9296489890127516, 0.935796723595211, 1.0, 1.0, 1.0, 0.08975799968500696], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.09086], dtype=float32), 0.18115862]. 
=============================================
[2019-04-08 15:26:57,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6859367e-17 6.4650224e-10 1.0424365e-05 2.3859598e-06 3.0716951e-10
 7.6701510e-12 1.6791974e-07 3.4519005e-14 5.9727838e-18 3.3933989e-20
 9.9998701e-01], sum to 1.0000
[2019-04-08 15:26:57,506] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6155
[2019-04-08 15:26:57,527] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.25, 80.5, 0.0, 0.0, 22.5, 27.38740530195295, 0.9308560271712746, 1.0, 1.0, 65.0, 31182.38301529366], 
current ob forecast is [], 
actual action is [10.25, 65.0], 
sim time this is 1582200.0000, 
sim time next is 1582800.0000, 
raw observation next is [5.166666666666666, 81.0, 0.0, 0.0, 22.5, 27.38366216978253, 0.926969109753517, 1.0, 1.0, 65.0, 32177.59312277026], 
processed observation next is [1.0, 0.30434782608695654, 0.6057248384118191, 0.81, 0.0, 0.0, 0.375, 0.7819718474818774, 0.8089897032511724, 1.0, 1.0, 1.0, 0.15322663391795363], 
reward next is 0.8468, 
noisyNet noise sample is [array([0.9888171], dtype=float32), -0.04395519]. 
=============================================
[2019-04-08 15:26:57,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4063768e-19 1.1133189e-11 1.2103661e-07 2.0305620e-08 1.0437162e-11
 3.9055720e-14 7.9455598e-10 9.9621144e-16 7.9803154e-20 3.8940399e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:26:57,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8317
[2019-04-08 15:26:57,953] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.233333333333333, 97.0, 0.0, 0.0, 19.0, 27.62377134047263, 1.016582222584215, 0.0, 1.0, 65.0, 30035.3032423748], 
current ob forecast is [], 
actual action is [11.233333333333334, 65.0], 
sim time this is 1660800.0000, 
sim time next is 1661400.0000, 
raw observation next is [6.05, 97.0, 0.0, 0.0, 19.0, 27.52846654823838, 1.015600537471478, 0.0, 1.0, 65.0, 33251.76301643423], 
processed observation next is [1.0, 0.21739130434782608, 0.6301939058171746, 0.97, 0.0, 0.0, 0.08333333333333333, 0.794038879019865, 0.8385335124904927, 0.0, 1.0, 1.0, 0.15834172864968682], 
reward next is 0.8417, 
noisyNet noise sample is [array([-2.0152733], dtype=float32), 1.0233313]. 
=============================================
[2019-04-08 15:26:58,477] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2509639e-18 1.0629262e-11 5.9480843e-08 2.9667945e-08 2.8899374e-12
 5.3917789e-15 1.2883862e-09 1.2326607e-15 2.4706603e-19 1.2466252e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:26:58,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0611
[2019-04-08 15:26:58,495] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.5, 59.0, 216.0, 249.0, 22.5, 28.65385868921685, 1.219519351494086, 1.0, 1.0, 65.0, 9424.583863005404], 
current ob forecast is [], 
actual action is [15.5, 65.0], 
sim time this is 1596600.0000, 
sim time next is 1597200.0000, 
raw observation next is [10.86666666666667, 58.33333333333334, 204.8333333333333, 228.1666666666667, 22.5, 28.72114799164524, 1.229774311248051, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7636195752539245, 0.5833333333333335, 0.6827777777777776, 0.2521178637200737, 0.375, 0.89342899930377, 0.909924770416017, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25543496], dtype=float32), 0.5900236]. 
=============================================
[2019-04-08 15:26:58,828] A3C_AGENT_WORKER-Thread-7 INFO:Evaluating...
[2019-04-08 15:26:58,829] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:26:58,830] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:26:58,830] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:26:58,831] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:26:58,835] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:26:58,836] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:26:58,838] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run10
[2019-04-08 15:26:58,857] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run10
[2019-04-08 15:26:58,875] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run10
[2019-04-08 15:27:09,309] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.059494033]
[2019-04-08 15:27:09,309] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-6.133333333333334, 74.66666666666667, 0.0, 0.0, 19.0, 25.93796068894861, 0.5361501049535287, 0.0, 1.0, 65.0, 58576.54446994865]
[2019-04-08 15:27:09,309] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:27:09,310] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [1.9362745e-15 3.0545397e-09 2.9457212e-06 8.8332163e-06 2.2048285e-09
 1.2579865e-11 5.3975640e-07 7.4380107e-13 1.5461365e-16 1.2194874e-18
 9.9998772e-01], sampled 0.04610155360913648
[2019-04-08 15:27:28,372] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.059494033]
[2019-04-08 15:27:28,373] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [4.4, 98.66666666666666, 0.0, 0.0, 22.5, 26.99605465551998, 0.8327668817522133, 1.0, 1.0, 65.0, 39867.8171629471]
[2019-04-08 15:27:28,373] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:27:28,373] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [1.01968003e-16 1.14711307e-09 1.11053214e-06 2.55538953e-06
 3.57946034e-10 1.34901834e-12 8.53411350e-08 5.85472358e-14
 6.70796741e-18 3.96747326e-20 9.99996305e-01], sampled 0.7328929552977018
[2019-04-08 15:28:32,833] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6989.7628 316309803.5283 2958.2603
[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,853] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:32,976] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,290] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.1591 355916579.5925 2370.4710
[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:39,449] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,542] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3022 342866528.2949 2768.2231
[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,564] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:42,681] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:28:43,566] A3C_AGENT_WORKER-Thread-7 INFO:Global step: 180000, evaluation results [180000.0, 6863.302246214903, 342866528.29487085, 2768.2231395015347, 6989.762840341211, 316309803.5283474, 2958.260328491998, 6801.1591447976925, 355916579.5924773, 2370.4709912649328]
[2019-04-08 15:28:44,077] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6176189e-18 3.8935921e-09 4.8871884e-06 7.0892673e-07 1.0120816e-10
 2.4500153e-13 2.0032979e-07 2.3016356e-15 4.1194267e-19 1.4196430e-21
 9.9999416e-01], sum to 1.0000
[2019-04-08 15:28:44,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8375
[2019-04-08 15:28:44,091] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.3, 51.0, 64.0, 18.5, 22.5, 29.13269798789726, 1.30343507357135, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [18.3, 65.0], 
sim time this is 1612800.0000, 
sim time next is 1613400.0000, 
raw observation next is [13.11666666666667, 51.5, 59.33333333333333, 24.66666666666667, 22.5, 29.15578554559867, 1.30738956110267, 1.0, 1.0, 65.0, 18849.17987662394], 
processed observation next is [1.0, 0.6956521739130435, 0.8259464450600187, 0.515, 0.19777777777777777, 0.027255985267034995, 0.375, 0.9296487954665557, 0.9357965203675566, 1.0, 1.0, 1.0, 0.08975799941249496], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.11116748], dtype=float32), -0.7823419]. 
=============================================
[2019-04-08 15:28:44,372] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.1985452e-17 2.2993942e-09 6.4640881e-05 2.7418819e-05 1.0798360e-09
 2.3449346e-12 3.1297540e-07 2.5858588e-14 4.5774193e-17 1.3468339e-20
 9.9990761e-01], sum to 1.0000
[2019-04-08 15:28:44,374] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8758
[2019-04-08 15:28:44,388] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.46666666666667, 50.33333333333334, 73.33333333333333, 6.166666666666665, 22.5, 29.0464537076958, 1.29278745833838, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [18.46666666666667, 65.0], 
sim time this is 1611600.0000, 
sim time next is 1612200.0000, 
raw observation next is [13.38333333333333, 50.66666666666666, 68.66666666666667, 12.33333333333333, 22.5, 29.09537658220275, 1.299046223359018, 1.0, 1.0, 65.0, 18849.20020491314], 
processed observation next is [1.0, 0.6521739130434783, 0.8333333333333334, 0.5066666666666666, 0.2288888888888889, 0.013627992633517492, 0.375, 0.9246147151835625, 0.9330154077863394, 1.0, 1.0, 1.0, 0.0897580962138721], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.92252463], dtype=float32), -0.5265617]. 
=============================================
[2019-04-08 15:28:44,812] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6579714e-19 9.3231249e-11 1.9104461e-07 1.8208840e-07 1.4692974e-11
 5.2372597e-13 1.1471091e-07 1.6200208e-14 2.7547798e-19 5.7211155e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:28:44,813] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6913
[2019-04-08 15:28:44,827] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.699999999999999, 96.83333333333334, 0.0, 0.0, 19.0, 27.74640003504877, 1.053599741135948, 0.0, 1.0, 65.0, 28495.9516141077], 
current ob forecast is [], 
actual action is [11.7, 65.0], 
sim time this is 1651800.0000, 
sim time next is 1652400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.67845152038657, 1.047398396282364, 0.0, 1.0, 65.0, 30593.58728509465], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.8065376266988808, 0.8491327987607881, 0.0, 1.0, 1.0, 0.1456837489766412], 
reward next is 0.8543, 
noisyNet noise sample is [array([-0.10395887], dtype=float32), 0.2646733]. 
=============================================
[2019-04-08 15:28:45,302] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.1598449e-17 5.6680765e-08 4.3180703e-06 9.1152157e-07 5.3151317e-10
 3.4037380e-12 2.8306968e-07 3.7532531e-14 2.5048685e-18 7.2281349e-20
 9.9999440e-01], sum to 1.0000
[2019-04-08 15:28:45,304] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.57756101e-16 8.22342694e-10 1.34858806e-06 1.23841291e-05
 4.30911890e-10 1.36309551e-12 1.69561474e-07 1.01234986e-13
 9.15833364e-18 3.06106134e-20 9.99986053e-01], sum to 1.0000
[2019-04-08 15:28:45,307] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5489
[2019-04-08 15:28:45,307] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0293
[2019-04-08 15:28:45,320] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 22.5, 27.97243613127638, 1.062835367953962, 1.0, 1.0, 65.0, 29284.53277247536], 
current ob forecast is [], 
actual action is [6.433333333333333, 65.0], 
sim time this is 1698000.0000, 
sim time next is 1698600.0000, 
raw observation next is [1.516666666666667, 82.16666666666666, 45.33333333333334, 0.0, 22.5, 28.00805743554794, 1.064183331482669, 1.0, 1.0, 65.0, 27708.56091394342], 
processed observation next is [1.0, 0.6521739130434783, 0.5046168051708219, 0.8216666666666665, 0.15111111111111114, 0.0, 0.375, 0.8340047862956617, 0.8547277771608895, 1.0, 1.0, 1.0, 0.13194552816163532], 
reward next is 0.8681, 
noisyNet noise sample is [array([-1.3207998], dtype=float32), 0.54307514]. 
=============================================
[2019-04-08 15:28:45,331] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 89.33333333333334, 80.16666666666667, 0.0, 22.5, 28.0684310196112, 1.090159408274488, 1.0, 1.0, 65.0, 30041.92622953339], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1682400.0000, 
sim time next is 1683000.0000, 
raw observation next is [1.1, 88.0, 83.0, 0.0, 22.5, 28.07779506048209, 1.093432077940589, 1.0, 1.0, 65.0, 29923.97494573142], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.88, 0.27666666666666667, 0.0, 0.375, 0.8398162550401741, 0.8644773593135296, 1.0, 1.0, 1.0, 0.14249511878919724], 
reward next is 0.8575, 
noisyNet noise sample is [array([-0.63236016], dtype=float32), 2.7862513]. 
=============================================
[2019-04-08 15:28:45,349] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.591034]
 [86.58423 ]
 [86.59833 ]
 [86.607765]
 [86.62167 ]], R is [[86.5297699 ]
 [86.52141571]
 [86.51508331]
 [86.5056839 ]
 [86.48901367]].
[2019-04-08 15:28:45,615] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4879824e-18 1.0004417e-09 1.4430789e-08 1.2913829e-07 3.2528097e-12
 1.4628342e-13 1.4060134e-09 1.6087233e-15 9.5481994e-20 1.2294536e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:28:45,619] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9542
[2019-04-08 15:28:45,694] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 22.5, 27.89525851670349, 0.9381110103384861, 1.0, 1.0, 65.0, 46445.59110676667], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 22.5, 27.2201973960419, 1.079403280190565, 1.0, 1.0, 65.0, 62697.09838506082], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.375, 0.7683497830034917, 0.8598010933968551, 1.0, 1.0, 1.0, 0.2985576113574325], 
reward next is 0.7014, 
noisyNet noise sample is [array([0.00995959], dtype=float32), -1.0674367]. 
=============================================
[2019-04-08 15:28:45,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.00763389e-18 2.58104677e-10 1.17847625e-08 1.52445324e-07
 5.80342163e-11 7.55001598e-14 2.37390752e-09 5.25351476e-15
 2.87088541e-19 4.85913427e-22 9.99999881e-01], sum to 1.0000
[2019-04-08 15:28:45,866] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2179
[2019-04-08 15:28:45,900] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 74.5, 0.0, 22.5, 28.04368004379841, 1.086252344175735, 1.0, 1.0, 65.0, 30291.50638917488], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 22.5, 28.07234170970819, 1.085593554076852, 1.0, 1.0, 65.0, 29635.53677425806], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.9066666666666667, 0.25777777777777783, 0.0, 0.375, 0.8393618091423493, 0.8618645180256174, 1.0, 1.0, 1.0, 0.14112160368694315], 
reward next is 0.8589, 
noisyNet noise sample is [array([0.7278229], dtype=float32), 0.17245509]. 
=============================================
[2019-04-08 15:28:46,961] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8866639e-17 3.4149135e-11 5.6889212e-07 2.5127140e-06 1.4046711e-10
 3.9194295e-12 5.9601470e-09 4.5517653e-15 4.7694893e-19 1.9401840e-20
 9.9999690e-01], sum to 1.0000
[2019-04-08 15:28:46,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1481
[2019-04-08 15:28:47,021] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 22.5, 28.1090223220766, 0.9272776898862309, 1.0, 1.0, 65.0, 42399.49761045464], 
current ob forecast is [], 
actual action is [6.433333333333334, 65.0], 
sim time this is 1700400.0000, 
sim time next is 1701000.0000, 
raw observation next is [1.35, 84.5, 30.0, 0.0, 22.5, 27.34276734637081, 1.045309645470164, 1.0, 1.0, 65.0, 61900.83814984206], 
processed observation next is [1.0, 0.6956521739130435, 0.5000000000000001, 0.845, 0.1, 0.0, 0.375, 0.7785639455309008, 0.8484365484900547, 1.0, 1.0, 1.0, 0.29476589595162883], 
reward next is 0.7052, 
noisyNet noise sample is [array([0.81084305], dtype=float32), 0.34888673]. 
=============================================
[2019-04-08 15:28:47,039] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.171425]
 [81.31197 ]
 [81.458916]
 [81.55378 ]
 [81.68986 ]], R is [[80.9610672 ]
 [80.94955444]
 [81.01182556]
 [81.07377625]
 [81.13109589]].
[2019-04-08 15:28:47,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6424043e-16 1.0730281e-10 1.1324578e-06 6.6169578e-06 7.3017214e-10
 1.7388767e-12 3.3291162e-08 4.2020182e-13 1.0719018e-16 6.8260002e-21
 9.9999225e-01], sum to 1.0000
[2019-04-08 15:28:47,962] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8598
[2019-04-08 15:28:48,000] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1, 91.0, 0.0, 0.0, 19.0, 26.73846919794183, 0.7913926678914307, 0.0, 1.0, 65.0, 47892.07110326144], 
current ob forecast is [], 
actual action is [5.1, 65.0], 
sim time this is 1737000.0000, 
sim time next is 1737600.0000, 
raw observation next is [0.06666666666666668, 91.0, 0.0, 0.0, 19.0, 26.751050960303, 0.7868879106369399, 0.0, 1.0, 65.0, 47565.16611828039], 
processed observation next is [0.0, 0.08695652173913043, 0.46445060018467227, 0.91, 0.0, 0.0, 0.08333333333333333, 0.7292542466919167, 0.7622959702123132, 0.0, 1.0, 1.0, 0.2265007910394304], 
reward next is 0.7735, 
noisyNet noise sample is [array([-1.8303112], dtype=float32), 0.27162415]. 
=============================================
[2019-04-08 15:28:48,619] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.13585213e-15 1.14474197e-09 1.85442025e-06 2.69772318e-06
 1.32311262e-09 1.84146491e-12 6.89159378e-07 8.73568660e-14
 1.22017187e-16 1.05896746e-17 9.99994755e-01], sum to 1.0000
[2019-04-08 15:28:48,621] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7117
[2019-04-08 15:28:48,642] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7, 83.66666666666667, 0.0, 0.0, 19.0, 26.60265669536167, 0.7487645357463387, 0.0, 1.0, 65.0, 50556.85126449189], 
current ob forecast is [], 
actual action is [4.3, 65.0], 
sim time this is 1746600.0000, 
sim time next is 1747200.0000, 
raw observation next is [-0.8, 84.33333333333334, 0.0, 0.0, 19.0, 26.59672148833069, 0.7550690057411482, 0.0, 1.0, 65.0, 50678.02318944073], 
processed observation next is [0.0, 0.21739130434782608, 0.4404432132963989, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.7163934573608909, 0.7516896685803828, 0.0, 1.0, 1.0, 0.24132391994971777], 
reward next is 0.7587, 
noisyNet noise sample is [array([1.3015298], dtype=float32), -1.3434587]. 
=============================================
[2019-04-08 15:28:48,885] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1300436e-13 1.4429503e-07 5.3035183e-05 3.5110625e-05 3.6137628e-08
 1.7437184e-09 5.2516702e-06 9.1654609e-12 2.6865177e-13 5.4784470e-17
 9.9990642e-01], sum to 1.0000
[2019-04-08 15:28:48,885] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2950
[2019-04-08 15:28:48,931] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 71.0, 178.0, 62.0, 19.0, 26.20395019943281, 0.5831427325439145, 0.0, 1.0, 65.0, 54682.51620938792], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 1864800.0000, 
sim time next is 1865400.0000, 
raw observation next is [-4.5, 73.0, 180.6666666666667, 69.33333333333334, 19.0, 26.21368414359779, 0.5857566221441972, 0.0, 1.0, 65.0, 54526.67810270486], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.73, 0.6022222222222223, 0.07661141804788214, 0.08333333333333333, 0.684473678633149, 0.6952522073813991, 0.0, 1.0, 1.0, 0.25965084810811834], 
reward next is 0.7403, 
noisyNet noise sample is [array([1.596358], dtype=float32), 2.4710252]. 
=============================================
[2019-04-08 15:28:48,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5380366e-16 1.3005350e-10 8.4265622e-08 4.1650210e-07 3.3412365e-10
 1.1341659e-12 1.6145229e-08 1.4874288e-14 2.3145089e-17 1.4148558e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:28:48,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1257
[2019-04-08 15:28:48,995] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 26.78019858867728, 0.8183796454924682, 0.0, 1.0, 65.0, 47465.85752694996], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1730400.0000, 
sim time next is 1731000.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 19.0, 26.81672028177926, 0.8133109895701387, 0.0, 1.0, 65.0, 46337.29322817502], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7347266901482717, 0.7711036631900462, 0.0, 1.0, 1.0, 0.2206537772770239], 
reward next is 0.7793, 
noisyNet noise sample is [array([-0.6181411], dtype=float32), -0.23735948]. 
=============================================
[2019-04-08 15:28:49,002] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0343849e-15 1.5713709e-09 9.3932704e-07 1.7566168e-06 1.3223055e-10
 4.5413825e-13 5.3908995e-08 4.0818338e-12 5.9396068e-17 2.9286291e-19
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:28:49,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8213
[2019-04-08 15:28:49,024] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.93557 ]
 [76.472206]
 [76.97289 ]
 [77.66341 ]
 [78.06049 ]], R is [[75.7215271 ]
 [75.73828125]
 [75.75537109]
 [75.77250671]
 [75.78919983]].
[2019-04-08 15:28:49,033] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.2, 91.0, 0.0, 0.0, 19.0, 26.75605657249372, 0.7944331423341521, 0.0, 1.0, 65.0, 48056.45370158245], 
current ob forecast is [], 
actual action is [5.2, 65.0], 
sim time this is 1735200.0000, 
sim time next is 1735800.0000, 
raw observation next is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 19.0, 26.73986394096088, 0.7926086437285688, 0.0, 1.0, 65.0, 48286.37951470775], 
processed observation next is [0.0, 0.08695652173913043, 0.4672206832871654, 0.9100000000000001, 0.0, 0.0, 0.08333333333333333, 0.7283219950800733, 0.7642028812428562, 0.0, 1.0, 1.0, 0.22993514054622738], 
reward next is 0.7701, 
noisyNet noise sample is [array([0.04747017], dtype=float32), -0.33603927]. 
=============================================
[2019-04-08 15:28:49,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3619249e-16 3.0410303e-09 1.9880140e-07 1.3923616e-06 9.1418600e-10
 7.8605574e-13 1.0396640e-06 6.7800989e-14 4.3342689e-17 3.0214672e-19
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:28:49,317] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2267
[2019-04-08 15:28:49,345] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.25, 93.5, 0.0, 0.0, 19.0, 26.83430123350809, 0.8300177001316357, 0.0, 1.0, 65.0, 46984.08463816488], 
current ob forecast is [], 
actual action is [5.25, 65.0], 
sim time this is 1726200.0000, 
sim time next is 1726800.0000, 
raw observation next is [0.3333333333333333, 93.0, 0.0, 0.0, 19.0, 26.82099973494214, 0.8266671081606427, 0.0, 1.0, 65.0, 47066.95459080273], 
processed observation next is [1.0, 1.0, 0.4718374884579871, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7350833112451785, 0.7755557027202142, 0.0, 1.0, 1.0, 0.22412835519429872], 
reward next is 0.7759, 
noisyNet noise sample is [array([-0.29220897], dtype=float32), 0.0019393584]. 
=============================================
[2019-04-08 15:28:49,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8753625e-15 9.4128243e-09 3.2086161e-06 1.3085205e-05 6.7133525e-09
 5.5495105e-11 1.5422933e-07 2.0984507e-13 3.6349304e-17 2.1775062e-18
 9.9998355e-01], sum to 1.0000
[2019-04-08 15:28:49,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6125
[2019-04-08 15:28:49,755] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 87.0, 100.6666666666667, 0.0, 19.0, 26.52055390343213, 0.7143376392545209, 0.0, 1.0, 65.0, 51928.99885509151], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 1766400.0000, 
sim time next is 1767000.0000, 
raw observation next is [-2.3, 87.0, 104.3333333333333, 0.0, 19.0, 26.50580476783612, 0.7145581821281004, 0.0, 1.0, 65.0, 52351.01641978986], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3477777777777777, 0.0, 0.08333333333333333, 0.7088170639863435, 0.7381860607093668, 0.0, 1.0, 1.0, 0.24929055437995173], 
reward next is 0.7507, 
noisyNet noise sample is [array([1.3557254], dtype=float32), 0.45195967]. 
=============================================
[2019-04-08 15:28:49,765] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.2421  ]
 [73.203964]
 [73.18658 ]
 [73.11117 ]
 [73.06374 ]], R is [[73.26210022]
 [73.28219604]
 [73.30592346]
 [73.32757568]
 [73.3453598 ]].
[2019-04-08 15:28:50,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1354578e-14 7.5368327e-08 3.4682227e-05 3.0355930e-06 1.5584817e-08
 7.5369218e-11 1.7216742e-07 3.6934544e-12 3.0641967e-15 1.6762622e-17
 9.9996197e-01], sum to 1.0000
[2019-04-08 15:28:50,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7465
[2019-04-08 15:28:50,068] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 19.0, 26.28832860914384, 0.623260552775443, 0.0, 1.0, 65.0, 55972.74259821675], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 1803600.0000, 
sim time next is 1804200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 26.32188018250677, 0.6153429591704228, 0.0, 1.0, 65.0, 55026.01251555192], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6934900152088975, 0.7051143197234743, 0.0, 1.0, 1.0, 0.2620286310264377], 
reward next is 0.7380, 
noisyNet noise sample is [array([0.89309764], dtype=float32), 0.974877]. 
=============================================
[2019-04-08 15:28:50,726] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1248342e-15 5.2408469e-09 4.7051424e-07 1.9965555e-06 1.9723212e-09
 1.7784668e-11 1.2140660e-07 2.0479701e-12 4.5116246e-16 7.4747274e-18
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:28:50,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0244
[2019-04-08 15:28:50,772] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 85.0, 65.0, 0.0, 19.0, 26.5154954861735, 0.7120445794800293, 0.0, 1.0, 65.0, 51629.09280087631], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 1762200.0000, 
sim time next is 1762800.0000, 
raw observation next is [-2.1, 85.66666666666667, 70.33333333333334, 0.0, 19.0, 26.50269724049623, 0.7210055129236616, 0.0, 1.0, 65.0, 52334.57614287756], 
processed observation next is [0.0, 0.391304347826087, 0.404432132963989, 0.8566666666666667, 0.23444444444444448, 0.0, 0.08333333333333333, 0.7085581033746857, 0.7403351709745539, 0.0, 1.0, 1.0, 0.249212267347036], 
reward next is 0.7508, 
noisyNet noise sample is [array([0.40794924], dtype=float32), 0.76345503]. 
=============================================
[2019-04-08 15:28:50,935] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4896550e-13 1.5299642e-08 3.5045301e-05 3.4595185e-05 6.0276491e-08
 5.4636112e-11 1.5573513e-06 7.0648577e-11 6.7705584e-15 5.3649272e-16
 9.9992871e-01], sum to 1.0000
[2019-04-08 15:28:50,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2551
[2019-04-08 15:28:50,954] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 25.85780672962254, 0.4629320428076216, 0.0, 1.0, 65.0, 59445.2927249519], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 1900800.0000, 
sim time next is 1901400.0000, 
raw observation next is [-7.3, 80.83333333333334, 0.0, 0.0, 19.0, 25.84901717349163, 0.4605238384635484, 0.0, 1.0, 65.0, 59508.14293736155], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.8083333333333335, 0.0, 0.0, 0.08333333333333333, 0.654084764457636, 0.6535079461545161, 0.0, 1.0, 1.0, 0.2833721092255312], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.8658737], dtype=float32), 1.2472714]. 
=============================================
[2019-04-08 15:28:51,074] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1289828e-14 9.6563291e-09 3.8992021e-06 5.6965532e-06 4.2110777e-09
 6.4267931e-11 1.1381134e-05 3.6826982e-12 2.3037615e-15 8.2711603e-17
 9.9997902e-01], sum to 1.0000
[2019-04-08 15:28:51,078] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-08 15:28:51,094] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 87.0, 108.0, 0.0, 19.0, 26.49585038531515, 0.7141839338641822, 0.0, 1.0, 65.0, 52301.17482771501], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 1767600.0000, 
sim time next is 1768200.0000, 
raw observation next is [-2.3, 86.33333333333333, 111.6666666666667, 0.0, 19.0, 26.49181452035233, 0.714281736083922, 0.0, 1.0, 65.0, 52256.6711423321], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8633333333333333, 0.37222222222222234, 0.0, 0.08333333333333333, 0.7076512100293609, 0.738093912027974, 0.0, 1.0, 1.0, 0.2488412911539624], 
reward next is 0.7512, 
noisyNet noise sample is [array([-0.2511249], dtype=float32), 0.7881817]. 
=============================================
[2019-04-08 15:28:51,175] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0155555e-14 7.6723630e-08 1.9976438e-04 2.4243543e-04 4.1903064e-08
 2.9535874e-10 1.4120312e-06 7.6710194e-11 6.1744386e-15 5.0267336e-17
 9.9955624e-01], sum to 1.0000
[2019-04-08 15:28:51,181] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7953
[2019-04-08 15:28:51,210] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5724937e-16 3.2583078e-09 7.2004391e-06 2.4337780e-05 3.0994420e-09
 1.3183208e-11 1.6878773e-06 2.2087143e-12 4.0477743e-16 1.3840416e-18
 9.9996674e-01], sum to 1.0000
[2019-04-08 15:28:51,210] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0933
[2019-04-08 15:28:51,211] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 87.0, 0.0, 0.0, 19.0, 26.55932716734556, 0.7218981360067104, 0.0, 1.0, 65.0, 50825.92416010686], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 1755600.0000, 
sim time next is 1756200.0000, 
raw observation next is [-1.7, 87.0, 9.33333333333333, 0.0, 19.0, 26.56172884909973, 0.7177743132625384, 0.0, 1.0, 65.0, 50671.99460810026], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0311111111111111, 0.0, 0.08333333333333333, 0.7134774040916442, 0.7392581044208462, 0.0, 1.0, 1.0, 0.24129521241952506], 
reward next is 0.7587, 
noisyNet noise sample is [array([-0.46061468], dtype=float32), 0.28853884]. 
=============================================
[2019-04-08 15:28:51,234] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 84.33333333333333, 120.1666666666667, 0.0, 19.0, 26.4926259258575, 0.7157530351585589, 0.0, 1.0, 65.0, 52140.28678041802], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 1770000.0000, 
sim time next is 1770600.0000, 
raw observation next is [-2.3, 83.66666666666667, 121.3333333333333, 0.0, 19.0, 26.49496347853843, 0.7161854461395455, 0.0, 1.0, 65.0, 52069.42438724517], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8366666666666667, 0.40444444444444433, 0.0, 0.08333333333333333, 0.7079136232115358, 0.7387284820465152, 0.0, 1.0, 1.0, 0.24794963993926272], 
reward next is 0.7521, 
noisyNet noise sample is [array([1.5175403], dtype=float32), -0.9276582]. 
=============================================
[2019-04-08 15:28:51,715] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7142483e-14 4.2776083e-08 6.4043388e-06 7.2619376e-05 1.4604962e-07
 1.1846484e-10 2.4746560e-06 1.4495270e-11 5.5974516e-15 5.0506629e-17
 9.9991834e-01], sum to 1.0000
[2019-04-08 15:28:51,716] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6005
[2019-04-08 15:28:51,731] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 19.0, 26.56565290924352, 0.7157120016220855, 0.0, 1.0, 65.0, 51649.87792493555], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 19.0, 26.54285301601257, 0.7142896899497001, 0.0, 1.0, 65.0, 52381.30508940577], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8433333333333334, 0.341111111111111, 0.0, 0.08333333333333333, 0.7119044180010476, 0.7380965633165667, 0.0, 1.0, 1.0, 0.24943478614002745], 
reward next is 0.7506, 
noisyNet noise sample is [array([-1.4446484], dtype=float32), 0.18243088]. 
=============================================
[2019-04-08 15:28:51,924] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2740575e-14 1.2429543e-08 3.5107803e-06 1.7443069e-06 3.4039166e-10
 2.2083353e-10 3.0038933e-07 2.4834486e-13 4.8470889e-16 5.0939821e-18
 9.9999440e-01], sum to 1.0000
[2019-04-08 15:28:51,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1799
[2019-04-08 15:28:51,941] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.816666666666666, 82.83333333333334, 19.0, 0.0, 19.0, 26.42873670652819, 0.6772309134819486, 0.0, 1.0, 65.0, 54284.64735622908], 
current ob forecast is [], 
actual action is [1.183333333333334, 65.0], 
sim time this is 1788600.0000, 
sim time next is 1789200.0000, 
raw observation next is [-3.9, 82.0, 14.5, 0.0, 19.0, 26.41840612709081, 0.6739870404246439, 0.0, 1.0, 65.0, 54441.92092915704], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.04833333333333333, 0.0, 0.08333333333333333, 0.7015338439242343, 0.7246623468082146, 0.0, 1.0, 1.0, 0.2592472425197954], 
reward next is 0.7408, 
noisyNet noise sample is [array([0.36454564], dtype=float32), -0.4504127]. 
=============================================
[2019-04-08 15:28:52,016] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0745448e-14 9.2158317e-09 6.0775492e-06 1.5689395e-05 1.9133251e-08
 2.8644040e-10 3.3435210e-08 4.2657419e-11 3.9254980e-16 8.0801073e-17
 9.9997818e-01], sum to 1.0000
[2019-04-08 15:28:52,017] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8246
[2019-04-08 15:28:52,040] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 19.0, 25.81553542628971, 0.4085252069345084, 0.0, 1.0, 65.0, 58842.00921860078], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 1921200.0000, 
sim time next is 1921800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 19.0, 25.75703524613921, 0.4028407075216821, 0.0, 1.0, 65.0, 59500.93926675553], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.08333333333333333, 0.6464196038449342, 0.6342802358405607, 0.0, 1.0, 1.0, 0.2833378060321692], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.5024766], dtype=float32), 0.540983]. 
=============================================
[2019-04-08 15:28:54,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8625353e-15 6.9628037e-09 5.2421119e-06 8.4834346e-06 1.3106404e-08
 1.7270467e-10 8.2922168e-07 1.0253723e-11 7.1474224e-15 2.6943711e-17
 9.9998546e-01], sum to 1.0000
[2019-04-08 15:28:54,399] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9668
[2019-04-08 15:28:54,421] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.75, 71.0, 120.0, 0.0, 19.0, 26.11012047314116, 0.5517838806917411, 0.0, 1.0, 65.0, 56922.90680253736], 
current ob forecast is [], 
actual action is [0.25, 65.0], 
sim time this is 1859400.0000, 
sim time next is 1860000.0000, 
raw observation next is [-4.666666666666667, 71.0, 128.3333333333333, 6.666666666666665, 19.0, 26.10997398980642, 0.5547159578642211, 0.0, 1.0, 65.0, 56939.60334696346], 
processed observation next is [0.0, 0.5217391304347826, 0.3333333333333333, 0.71, 0.42777777777777765, 0.00736648250460405, 0.08333333333333333, 0.6758311658172017, 0.6849053192880737, 0.0, 1.0, 1.0, 0.2711409683188736], 
reward next is 0.7289, 
noisyNet noise sample is [array([0.46279547], dtype=float32), 0.7651134]. 
=============================================
[2019-04-08 15:28:54,443] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[69.62637 ]
 [69.88731 ]
 [70.099045]
 [70.325775]
 [70.543434]], R is [[69.55464935]
 [69.58803558]
 [69.62204742]
 [69.65641022]
 [69.68911743]].
[2019-04-08 15:28:54,566] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.5572913e-15 3.1932498e-08 8.4105932e-07 1.0378148e-05 7.5168867e-09
 5.0633411e-11 4.0270756e-06 2.6633463e-12 3.6403642e-15 6.5760435e-17
 9.9998474e-01], sum to 1.0000
[2019-04-08 15:28:54,566] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8772
[2019-04-08 15:28:54,581] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 19.0, 26.32188026817773, 0.6153429848696195, 0.0, 1.0, 65.0, 55026.0114311696], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 1804200.0000, 
sim time next is 1804800.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 26.29566454293116, 0.6116454732548559, 0.0, 1.0, 65.0, 56528.53835957783], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6913053785775967, 0.7038818244182853, 0.0, 1.0, 1.0, 0.2691835159979897], 
reward next is 0.7308, 
noisyNet noise sample is [array([-0.19595425], dtype=float32), -0.16594473]. 
=============================================
[2019-04-08 15:28:54,602] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1002013e-14 1.4759708e-08 2.3388211e-06 2.6353249e-05 1.0012546e-09
 7.8476101e-11 5.9447430e-06 6.8659601e-12 3.9299144e-15 2.3799236e-16
 9.9996531e-01], sum to 1.0000
[2019-04-08 15:28:54,602] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3244
[2019-04-08 15:28:54,619] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.283333333333333, 78.83333333333334, 0.0, 0.0, 19.0, 25.93293864685555, 0.5056601113510822, 0.0, 1.0, 65.0, 61352.82720884221], 
current ob forecast is [], 
actual action is [-1.2833333333333332, 65.0], 
sim time this is 1836600.0000, 
sim time next is 1837200.0000, 
raw observation next is [-6.366666666666667, 78.66666666666667, 0.0, 0.0, 19.0, 25.91613267773922, 0.5024078818029676, 0.0, 1.0, 65.0, 61279.3047512594], 
processed observation next is [0.0, 0.2608695652173913, 0.28624192059095105, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.659677723144935, 0.6674692939343224, 0.0, 1.0, 1.0, 0.2918062131012352], 
reward next is 0.7082, 
noisyNet noise sample is [array([0.1684945], dtype=float32), 0.8777719]. 
=============================================
[2019-04-08 15:28:54,822] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.00833799e-14 1.06340437e-07 1.13812304e-04 4.48631472e-05
 4.14848188e-07 4.71176320e-10 9.26498524e-06 2.74157086e-11
 1.42432046e-14 3.57451246e-17 9.99831557e-01], sum to 1.0000
[2019-04-08 15:28:54,823] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5121
[2019-04-08 15:28:54,846] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 84.66666666666667, 0.0, 0.0, 19.0, 26.21234024529701, 0.5976169633183316, 0.0, 1.0, 65.0, 57967.72411113557], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 1808400.0000, 
sim time next is 1809000.0000, 
raw observation next is [-5.0, 84.0, 0.0, 0.0, 19.0, 26.19670704786292, 0.5947718685857339, 0.0, 1.0, 65.0, 57948.62968748836], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6830589206552432, 0.6982572895285779, 0.0, 1.0, 1.0, 0.2759458556547065], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.9284108], dtype=float32), 0.16822034]. 
=============================================
[2019-04-08 15:28:54,863] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[69.97201 ]
 [69.95537 ]
 [69.96128 ]
 [69.963356]
 [69.971344]], R is [[69.95891571]
 [69.98329163]
 [70.00748444]
 [70.03583527]
 [70.07061005]].
[2019-04-08 15:28:56,128] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.49247944e-14 1.43986867e-08 7.38414928e-06 1.41049095e-05
 4.15553316e-08 1.87577975e-09 2.62760159e-06 3.46635372e-12
 2.97033464e-16 4.37050207e-17 9.99975801e-01], sum to 1.0000
[2019-04-08 15:28:56,129] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3100
[2019-04-08 15:28:56,145] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.9, 79.0, 0.0, 0.0, 19.0, 26.03156282457577, 0.5038247748433823, 0.0, 1.0, 65.0, 58290.20797384818], 
current ob forecast is [], 
actual action is [-0.9000000000000004, 65.0], 
sim time this is 1891800.0000, 
sim time next is 1892400.0000, 
raw observation next is [-6.0, 77.66666666666667, 0.0, 0.0, 19.0, 26.02078117522228, 0.5015616491396967, 0.0, 1.0, 65.0, 58580.08188348539], 
processed observation next is [0.0, 0.9130434782608695, 0.296398891966759, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6683984312685233, 0.667187216379899, 0.0, 1.0, 1.0, 0.27895277087374], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.4979337], dtype=float32), -0.04643574]. 
=============================================
[2019-04-08 15:28:56,745] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6471900e-14 7.4091808e-08 5.2444328e-05 6.7984306e-06 3.5824201e-09
 3.4534495e-09 1.4861200e-06 7.3091299e-12 8.7550674e-14 5.5994447e-16
 9.9993920e-01], sum to 1.0000
[2019-04-08 15:28:56,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8277
[2019-04-08 15:28:56,763] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 83.0, 15.0, 0.0, 19.0, 26.22213700340815, 0.5614446174678831, 0.0, 1.0, 65.0, 54702.18087287479], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 19.0, 26.2171971174308, 0.5584638466397395, 0.0, 1.0, 65.0, 54525.26296839306], 
processed observation next is [0.0, 0.7391304347826086, 0.3356417359187443, 0.835, 0.03444444444444444, 0.0, 0.08333333333333333, 0.6847664264525667, 0.6861546155465797, 0.0, 1.0, 1.0, 0.2596441093733003], 
reward next is 0.7404, 
noisyNet noise sample is [array([0.32678035], dtype=float32), 2.4704106]. 
=============================================
[2019-04-08 15:28:58,232] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.71599717e-15 1.09666916e-08 9.40990049e-06 7.64384276e-06
 2.88969559e-09 3.84684125e-11 3.58194097e-06 2.99138774e-12
 2.73599292e-16 2.72245431e-19 9.99979377e-01], sum to 1.0000
[2019-04-08 15:28:58,235] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0260
[2019-04-08 15:28:58,251] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.483333333333334, 78.66666666666667, 0.0, 0.0, 19.0, 25.8536294045476, 0.4309765770935294, 0.0, 1.0, 65.0, 59504.16863046699], 
current ob forecast is [], 
actual action is [-3.4833333333333343, 65.0], 
sim time this is 1915800.0000, 
sim time next is 1916400.0000, 
raw observation next is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 19.0, 25.84759983097931, 0.4306748978459438, 0.0, 1.0, 65.0, 59724.66887137591], 
processed observation next is [1.0, 0.17391304347826086, 0.22530009233610343, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.6539666525816091, 0.6435582992819813, 0.0, 1.0, 1.0, 0.28440318510179], 
reward next is 0.7156, 
noisyNet noise sample is [array([1.1154017], dtype=float32), -1.2707562]. 
=============================================
[2019-04-08 15:28:58,319] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7877709e-14 3.1770110e-08 1.6856847e-06 5.4376696e-06 1.3233064e-09
 3.6364231e-11 4.7789769e-07 2.4196682e-12 3.4899150e-16 2.0578680e-18
 9.9999237e-01], sum to 1.0000
[2019-04-08 15:28:58,320] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8813
[2019-04-08 15:28:58,336] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.383333333333333, 75.5, 0.0, 0.0, 19.0, 25.96377086636362, 0.4601486123398136, 0.0, 1.0, 65.0, 58370.49229438888], 
current ob forecast is [], 
actual action is [-2.383333333333333, 65.0], 
sim time this is 1905000.0000, 
sim time next is 1905600.0000, 
raw observation next is [-7.466666666666667, 76.0, 0.0, 0.0, 19.0, 25.92359329756795, 0.4611406002720946, 0.0, 1.0, 65.0, 59585.99754308083], 
processed observation next is [1.0, 0.043478260869565216, 0.25577100646352724, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6602994414639959, 0.6537135334240315, 0.0, 1.0, 1.0, 0.2837428454432421], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.9442139], dtype=float32), 0.5575129]. 
=============================================
[2019-04-08 15:28:58,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7318002e-14 1.9901162e-08 1.4821624e-06 4.8396628e-06 1.0250607e-09
 2.7227264e-11 3.8390598e-07 1.8475135e-12 1.8347693e-16 1.2675317e-18
 9.9999332e-01], sum to 1.0000
[2019-04-08 15:28:58,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0667
[2019-04-08 15:28:58,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.716666666666667, 77.5, 0.0, 0.0, 19.0, 25.95801879174907, 0.4589086005493058, 0.0, 1.0, 65.0, 57969.55344844437], 
current ob forecast is [], 
actual action is [-2.716666666666667, 65.0], 
sim time this is 1907400.0000, 
sim time next is 1908000.0000, 
raw observation next is [-7.8, 78.0, 0.0, 0.0, 19.0, 25.94627437605013, 0.4576469653313991, 0.0, 1.0, 65.0, 58483.34435323351], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6621895313375109, 0.6525489884437997, 0.0, 1.0, 1.0, 0.2784921159677786], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.9442139], dtype=float32), 0.5575129]. 
=============================================
[2019-04-08 15:28:58,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.28411]
 [72.20086]
 [72.08204]
 [71.56597]
 [71.35159]], R is [[72.7286911 ]
 [72.72535706]
 [72.71956635]
 [72.70948792]
 [72.69864655]].
[2019-04-08 15:28:58,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4300721e-16 6.2170864e-09 4.5661659e-06 5.4884254e-06 1.3060336e-09
 4.5645127e-11 3.2325477e-07 6.2425763e-12 3.8819853e-16 2.1043419e-18
 9.9998963e-01], sum to 1.0000
[2019-04-08 15:28:58,728] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2325
[2019-04-08 15:28:58,761] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.583333333333333, 72.33333333333334, 0.0, 0.0, 22.5, 26.48269047518303, 0.6836246772175425, 1.0, 1.0, 65.0, 45603.69062888104], 
current ob forecast is [], 
actual action is [0.41666666666666696, 65.0], 
sim time this is 1968600.0000, 
sim time next is 1969200.0000, 
raw observation next is [-4.5, 71.0, 0.0, 0.0, 22.5, 26.58372755685988, 0.6719828472640392, 1.0, 1.0, 65.0, 41766.24522674591], 
processed observation next is [1.0, 0.8260869565217391, 0.3379501385041552, 0.71, 0.0, 0.0, 0.375, 0.7153106297383234, 0.7239942824213464, 1.0, 1.0, 1.0, 0.19888688203212337], 
reward next is 0.8011, 
noisyNet noise sample is [array([0.16751269], dtype=float32), -0.46077347]. 
=============================================
[2019-04-08 15:28:59,511] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4469222e-14 2.9447495e-08 1.1666944e-05 5.9996573e-06 1.1875532e-08
 5.5519166e-11 1.4101259e-07 7.3377858e-12 5.3619223e-15 4.3524278e-17
 9.9998212e-01], sum to 1.0000
[2019-04-08 15:28:59,511] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3739
[2019-04-08 15:28:59,524] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 62.0, 105.6666666666667, 0.0, 22.5, 27.28183998270211, 0.7477294786705889, 1.0, 1.0, 65.0, 36415.12499337096], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 1953600.0000, 
sim time next is 1954200.0000, 
raw observation next is [-2.9, 62.0, 99.33333333333333, 0.0, 22.5, 27.29682041708761, 0.751492525166697, 1.0, 1.0, 65.0, 36465.14211600641], 
processed observation next is [1.0, 0.6086956521739131, 0.38227146814404434, 0.62, 0.3311111111111111, 0.0, 0.375, 0.7747350347573008, 0.7504975083888991, 1.0, 1.0, 1.0, 0.17364353388574483], 
reward next is 0.8264, 
noisyNet noise sample is [array([0.54712903], dtype=float32), 0.0548123]. 
=============================================
[2019-04-08 15:28:59,794] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1281420e-15 8.2353866e-09 7.9550755e-06 2.5871044e-04 9.3329664e-09
 3.9965735e-12 3.3531412e-06 1.0183067e-12 3.5644155e-16 2.3142104e-18
 9.9972993e-01], sum to 1.0000
[2019-04-08 15:28:59,794] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7497
[2019-04-08 15:28:59,810] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.031936731503, 0.5514743606860685, 0.0, 1.0, 65.0, 55547.42690881797], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 26.03714140311743, 0.5485411996665204, 0.0, 1.0, 65.0, 55181.10068366097], 
processed observation next is [1.0, 0.0, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6697617835931192, 0.6828470665555068, 0.0, 1.0, 1.0, 0.26276714611267127], 
reward next is 0.7372, 
noisyNet noise sample is [array([2.0100248], dtype=float32), -2.0891633]. 
=============================================
[2019-04-08 15:29:00,518] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1050806e-15 7.7840973e-08 4.3917075e-06 3.6280861e-05 5.3510188e-08
 9.8542244e-11 3.8580524e-06 6.2946584e-12 2.5524527e-15 2.0125410e-17
 9.9995530e-01], sum to 1.0000
[2019-04-08 15:29:00,527] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7987
[2019-04-08 15:29:00,558] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 19.0, 25.90301584324968, 0.4426825207946573, 0.0, 1.0, 65.0, 58956.28092518514], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 1914600.0000, 
sim time next is 1915200.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 19.0, 25.91281839619206, 0.4306277319977589, 0.0, 1.0, 65.0, 58127.6892008912], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6594015330160049, 0.6435425773325864, 0.0, 1.0, 1.0, 0.2767985200042438], 
reward next is 0.7232, 
noisyNet noise sample is [array([-0.8745704], dtype=float32), 1.0989051]. 
=============================================
[2019-04-08 15:29:00,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2653007e-14 1.3933849e-10 5.8306040e-07 1.8775838e-05 4.2749745e-10
 1.9717265e-10 5.4226337e-08 2.2251332e-13 1.1964542e-16 1.9597383e-19
 9.9998057e-01], sum to 1.0000
[2019-04-08 15:29:00,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8056
[2019-04-08 15:29:00,632] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 19.0, 26.17965332589809, 0.5251143247862996, 0.0, 1.0, 65.0, 50101.95154791885], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2004000.0000, 
sim time next is 2004600.0000, 
raw observation next is [-6.1, 86.33333333333333, 0.0, 0.0, 19.0, 26.16358783816808, 0.5273301174996878, 0.0, 1.0, 65.0, 52444.93904085612], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8633333333333333, 0.0, 0.0, 0.08333333333333333, 0.6802989865140067, 0.6757767058332292, 0.0, 1.0, 1.0, 0.24973780495645773], 
reward next is 0.7503, 
noisyNet noise sample is [array([1.8093837], dtype=float32), 0.11260802]. 
=============================================
[2019-04-08 15:29:00,643] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0987499e-15 1.8157226e-10 7.7043396e-06 7.8268595e-06 3.2580358e-09
 1.2732651e-11 2.8750435e-08 2.0361839e-13 8.9242875e-17 6.2487157e-19
 9.9998438e-01], sum to 1.0000
[2019-04-08 15:29:00,646] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9736
[2019-04-08 15:29:00,660] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 65.0, 229.5, 7.0, 22.5, 27.054290090068, 0.6910122149787101, 1.0, 1.0, 65.0, 37655.9272190585], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 1944000.0000, 
sim time next is 1944600.0000, 
raw observation next is [-4.816666666666666, 65.0, 228.6666666666667, 6.0, 22.5, 27.09113317733372, 0.6941756480386941, 1.0, 1.0, 65.0, 37512.67164322402], 
processed observation next is [1.0, 0.5217391304347826, 0.32917820867959374, 0.65, 0.7622222222222224, 0.0066298342541436465, 0.375, 0.7575944314444767, 0.7313918826795648, 1.0, 1.0, 1.0, 0.1786317697296382], 
reward next is 0.8214, 
noisyNet noise sample is [array([-0.775793], dtype=float32), 0.5375888]. 
=============================================
[2019-04-08 15:29:01,435] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2395568e-15 6.9637895e-10 6.0837638e-06 3.6595493e-06 9.7312403e-10
 1.6357752e-11 3.8421224e-08 3.1706639e-13 3.6851211e-16 1.6249438e-19
 9.9999022e-01], sum to 1.0000
[2019-04-08 15:29:01,437] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5027
[2019-04-08 15:29:01,462] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.8, 84.33333333333333, 0.0, 0.0, 19.0, 26.10753257475025, 0.5424623980263515, 0.0, 1.0, 65.0, 54413.93761025497], 
current ob forecast is [], 
actual action is [-0.7999999999999998, 65.0], 
sim time this is 1993200.0000, 
sim time next is 1993800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 26.0919363277506, 0.539229841389309, 0.0, 1.0, 65.0, 54289.24174393105], 
processed observation next is [1.0, 0.043478260869565216, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6743280273125499, 0.6797432804631031, 0.0, 1.0, 1.0, 0.25852019878062404], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.3640311], dtype=float32), -0.16692773]. 
=============================================
[2019-04-08 15:29:01,758] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.00866556e-16 3.02774805e-10 2.24627840e-07 3.41707596e-06
 3.35538530e-09 1.25013173e-12 3.31417567e-07 2.18527673e-14
 2.70471848e-17 4.64310081e-20 9.99996066e-01], sum to 1.0000
[2019-04-08 15:29:01,758] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1546
[2019-04-08 15:29:01,803] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.15247078918506, 0.5437103787038361, 0.0, 1.0, 65.0, 52336.28135545013], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 1999200.0000, 
sim time next is 1999800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.19801893078113, 0.5333843325616935, 0.0, 1.0, 65.0, 50214.20313562925], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6831682442317609, 0.6777947775205645, 0.0, 1.0, 1.0, 0.23911525302680595], 
reward next is 0.7609, 
noisyNet noise sample is [array([1.473059], dtype=float32), 1.6552122]. 
=============================================
[2019-04-08 15:29:01,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.6926878e-15 1.4103754e-09 2.1494894e-05 1.4778664e-05 5.8155498e-09
 9.4128550e-12 8.8311495e-08 1.0182360e-12 6.0405850e-17 9.2579456e-20
 9.9996364e-01], sum to 1.0000
[2019-04-08 15:29:01,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8443
[2019-04-08 15:29:01,833] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.55, 78.5, 208.0, 60.0, 22.5, 26.94455688242902, 0.6958851447279041, 1.0, 1.0, 65.0, 38414.74908584595], 
current ob forecast is [], 
actual action is [-2.55, 65.0], 
sim time this is 2111400.0000, 
sim time next is 2112000.0000, 
raw observation next is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 22.5, 26.93710646237163, 0.7061241646636317, 1.0, 1.0, 65.0, 38627.14985693229], 
processed observation next is [1.0, 0.43478260869565216, 0.25577100646352724, 0.7733333333333334, 0.7405555555555557, 0.07384898710865562, 0.375, 0.7447588718643026, 0.7353747215545439, 1.0, 1.0, 1.0, 0.1839388088425347], 
reward next is 0.8161, 
noisyNet noise sample is [array([0.5027331], dtype=float32), -1.2901874]. 
=============================================
[2019-04-08 15:29:01,850] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.19111]
 [78.72716]
 [79.21959]
 [79.53124]
 [79.8538 ]], R is [[78.27732086]
 [78.31161499]
 [78.34470367]
 [78.38102722]
 [78.41159821]].
[2019-04-08 15:29:02,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0242426e-15 2.3208653e-09 7.0082064e-07 4.3875731e-05 3.4507590e-09
 1.3364385e-10 2.2450708e-07 1.5389449e-13 5.5080575e-15 2.3787171e-18
 9.9995518e-01], sum to 1.0000
[2019-04-08 15:29:02,135] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0061
[2019-04-08 15:29:02,153] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.07118704624814, 0.5640071747585175, 0.0, 1.0, 65.0, 55919.34119008694], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 1983600.0000, 
sim time next is 1984200.0000, 
raw observation next is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 26.06147283513257, 0.5613403120206488, 0.0, 1.0, 65.0, 55936.44041979402], 
processed observation next is [1.0, 1.0, 0.3074792243767313, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6717894029277142, 0.6871134373402162, 0.0, 1.0, 1.0, 0.2663640019990191], 
reward next is 0.7336, 
noisyNet noise sample is [array([0.8328437], dtype=float32), -0.7290139]. 
=============================================
[2019-04-08 15:29:02,220] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.6776771e-14 1.4488991e-08 2.5914285e-05 1.1453329e-05 3.1241962e-09
 1.1020776e-11 2.9856159e-07 1.7936787e-12 3.7231282e-15 5.5030271e-18
 9.9996233e-01], sum to 1.0000
[2019-04-08 15:29:02,229] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2989
[2019-04-08 15:29:02,247] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 62.0, 74.0, 0.0, 22.5, 27.3057092558956, 0.7488238770632156, 1.0, 1.0, 65.0, 36651.46249780866], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 1956600.0000, 
sim time next is 1957200.0000, 
raw observation next is [-2.8, 62.0, 66.66666666666667, 0.0, 22.5, 27.24884179061976, 0.7539755241115423, 1.0, 1.0, 65.0, 38496.69656499268], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.22222222222222224, 0.0, 0.375, 0.77073681588498, 0.7513251747038474, 1.0, 1.0, 1.0, 0.18331760269044134], 
reward next is 0.8167, 
noisyNet noise sample is [array([-0.29071853], dtype=float32), -1.182289]. 
=============================================
[2019-04-08 15:29:02,634] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0710354e-14 1.5822799e-08 1.3336764e-05 6.5331919e-06 8.3101408e-11
 9.5740568e-11 2.7839894e-07 2.2602597e-12 4.1001223e-16 4.1606502e-19
 9.9997985e-01], sum to 1.0000
[2019-04-08 15:29:02,634] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9711
[2019-04-08 15:29:02,648] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.3, 68.0, 125.0, 0.0, 22.5, 27.26479615342366, 0.7674190255046174, 1.0, 1.0, 65.0, 36125.86285471881], 
current ob forecast is [], 
actual action is [-0.2999999999999998, 65.0], 
sim time this is 2125800.0000, 
sim time next is 2126400.0000, 
raw observation next is [-5.199999999999999, 68.0, 118.5, 0.0, 22.5, 27.21555505249738, 0.7739379541256133, 1.0, 1.0, 65.0, 37929.03128644855], 
processed observation next is [1.0, 0.6086956521739131, 0.31855955678670367, 0.68, 0.395, 0.0, 0.375, 0.7679629210414483, 0.7579793180418711, 1.0, 1.0, 1.0, 0.18061443469737407], 
reward next is 0.8194, 
noisyNet noise sample is [array([0.511604], dtype=float32), 1.2489433]. 
=============================================
[2019-04-08 15:29:03,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.5198447e-18 2.5296791e-09 4.4880449e-06 2.8172462e-06 2.4709612e-09
 9.8651892e-13 6.1652912e-08 1.4502506e-12 6.9081923e-17 8.4933887e-19
 9.9999261e-01], sum to 1.0000
[2019-04-08 15:29:03,612] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3043
[2019-04-08 15:29:03,629] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 75.66666666666667, 153.0, 0.0, 22.5, 27.04300393333241, 0.7006197066145692, 1.0, 1.0, 65.0, 39877.69422102219], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2031000.0000, 
sim time next is 2031600.0000, 
raw observation next is [-4.5, 76.33333333333334, 154.5, 0.0, 22.5, 27.06699708157122, 0.6789994894384237, 1.0, 1.0, 65.0, 40872.13131014192], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7633333333333334, 0.515, 0.0, 0.375, 0.7555830901309349, 0.7263331631461413, 1.0, 1.0, 1.0, 0.19462919671496154], 
reward next is 0.8054, 
noisyNet noise sample is [array([-1.6334231], dtype=float32), -1.8471797]. 
=============================================
[2019-04-08 15:29:03,864] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8110760e-14 2.6571538e-08 7.2085454e-06 8.8766166e-05 2.5583640e-08
 1.2212117e-11 2.4739765e-07 5.7244872e-13 3.9981215e-16 8.5782699e-18
 9.9990368e-01], sum to 1.0000
[2019-04-08 15:29:03,866] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5077
[2019-04-08 15:29:03,916] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.1, 81.33333333333334, 111.3333333333333, 0.0, 22.5, 27.13738633973078, 0.7205732220310646, 1.0, 1.0, 65.0, 36888.08863159999], 
current ob forecast is [], 
actual action is [0.9000000000000004, 65.0], 
sim time this is 2038800.0000, 
sim time next is 2039400.0000, 
raw observation next is [-4.2, 82.5, 104.0, 0.0, 22.5, 27.17058775787707, 0.7356642670033638, 1.0, 1.0, 65.0, 38513.32903219427], 
processed observation next is [1.0, 0.6086956521739131, 0.34626038781163443, 0.825, 0.3466666666666667, 0.0, 0.375, 0.7642156464897557, 0.7452214223344545, 1.0, 1.0, 1.0, 0.18339680491521082], 
reward next is 0.8166, 
noisyNet noise sample is [array([0.3770179], dtype=float32), 1.3200982]. 
=============================================
[2019-04-08 15:29:04,116] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0843429e-15 3.0233458e-08 1.4914940e-06 9.7862161e-05 6.3559673e-09
 4.4632410e-12 6.3009804e-07 2.5545641e-13 4.7187088e-16 1.3159472e-18
 9.9989998e-01], sum to 1.0000
[2019-04-08 15:29:04,116] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6467
[2019-04-08 15:29:04,131] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.3, 83.66666666666666, 98.5, 0.0, 22.5, 27.16562884212566, 0.735904110741939, 1.0, 1.0, 65.0, 38912.2069440116], 
current ob forecast is [], 
actual action is [0.7000000000000002, 65.0], 
sim time this is 2040000.0000, 
sim time next is 2040600.0000, 
raw observation next is [-4.399999999999999, 84.83333333333334, 93.0, 0.0, 22.5, 27.16569187982184, 0.734140493890108, 1.0, 1.0, 65.0, 38458.1262502825], 
processed observation next is [1.0, 0.6086956521739131, 0.3407202216066483, 0.8483333333333334, 0.31, 0.0, 0.375, 0.7638076566518199, 0.7447134979633693, 1.0, 1.0, 1.0, 0.18313393452515475], 
reward next is 0.8169, 
noisyNet noise sample is [array([0.5563742], dtype=float32), 0.62260234]. 
=============================================
[2019-04-08 15:29:04,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6811166e-15 3.1390798e-08 1.9991969e-06 1.3650622e-05 5.6051239e-09
 7.8357480e-11 4.6275847e-08 2.1167790e-13 2.3261142e-16 8.0942466e-19
 9.9998426e-01], sum to 1.0000
[2019-04-08 15:29:04,266] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4805061e-16 2.0250040e-08 3.6193369e-06 2.4083085e-05 3.9418843e-10
 6.6501123e-12 9.9141073e-07 9.7801650e-13 3.4392095e-17 1.1123259e-18
 9.9997127e-01], sum to 1.0000
[2019-04-08 15:29:04,269] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7859
[2019-04-08 15:29:04,270] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2664
[2019-04-08 15:29:04,285] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.1, 86.5, 29.0, 0.0, 22.5, 26.39067649321547, 0.551059969201274, 1.0, 1.0, 65.0, 41709.3028509131], 
current ob forecast is [], 
actual action is [-1.0999999999999996, 65.0], 
sim time this is 2017800.0000, 
sim time next is 2018400.0000, 
raw observation next is [-6.066666666666666, 86.33333333333333, 35.66666666666666, 0.0, 22.5, 26.57796550305873, 0.5747991088035914, 1.0, 1.0, 65.00000000000003, 42270.04166605992], 
processed observation next is [1.0, 0.34782608695652173, 0.2945521698984303, 0.8633333333333333, 0.11888888888888886, 0.0, 0.375, 0.7148304585882276, 0.6915997029345305, 1.0, 1.0, 1.0000000000000007, 0.20128591269552343], 
reward next is 0.7987, 
noisyNet noise sample is [array([-1.3613505], dtype=float32), -0.779478]. 
=============================================
[2019-04-08 15:29:04,287] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 87.66666666666667, 0.0, 0.0, 19.0, 26.08031734071088, 0.5827083731010417, 0.0, 1.0, 65.0, 55638.75275117209], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2071200.0000, 
sim time next is 2071800.0000, 
raw observation next is [-4.5, 88.5, 0.0, 0.0, 19.0, 26.07589641837802, 0.5799237928212555, 0.0, 1.0, 65.0, 55589.54726168672], 
processed observation next is [1.0, 1.0, 0.3379501385041552, 0.885, 0.0, 0.0, 0.08333333333333333, 0.6729913681981682, 0.6933079309404185, 0.0, 1.0, 1.0, 0.2647121298175558], 
reward next is 0.7353, 
noisyNet noise sample is [array([-1.7340482], dtype=float32), 0.48812744]. 
=============================================
[2019-04-08 15:29:04,371] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7195596e-15 1.9044316e-08 1.5657677e-06 1.6874737e-05 7.6492377e-09
 1.1939436e-10 8.1339255e-07 3.5519978e-13 9.6611011e-16 1.9924167e-18
 9.9998069e-01], sum to 1.0000
[2019-04-08 15:29:04,382] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1886
[2019-04-08 15:29:04,410] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 76.33333333333334, 154.5, 0.0, 22.5, 27.06699710398266, 0.6789994957810981, 1.0, 1.0, 65.0, 40872.13104973404], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2031600.0000, 
sim time next is 2032200.0000, 
raw observation next is [-4.5, 77.0, 156.0, 0.0, 22.5, 26.90197717012288, 0.6790444554646116, 1.0, 1.0, 65.0, 45906.73670172833], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.77, 0.52, 0.0, 0.375, 0.7418314308435733, 0.7263481518215372, 1.0, 1.0, 1.0, 0.21860350810346824], 
reward next is 0.7814, 
noisyNet noise sample is [array([0.08650429], dtype=float32), 0.5588653]. 
=============================================
[2019-04-08 15:29:04,535] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.99262294e-16 8.40815750e-10 3.00284967e-07 3.55258362e-06
 1.12876576e-10 3.43746104e-12 7.69426620e-08 4.90886357e-13
 8.29854457e-17 2.56563276e-19 9.99996066e-01], sum to 1.0000
[2019-04-08 15:29:04,537] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0135
[2019-04-08 15:29:04,552] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 19.0, 26.06268735426194, 0.5370269166379592, 0.0, 1.0, 65.0, 54130.90436591191], 
current ob forecast is [], 
actual action is [-0.6000000000000014, 65.0], 
sim time this is 1995000.0000, 
sim time next is 1995600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.07976052755745, 0.5467881998466179, 0.0, 1.0, 65.0, 53453.46611875977], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6733133772964542, 0.6822627332822059, 0.0, 1.0, 1.0, 0.25454031485123696], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.6256432], dtype=float32), -0.17155333]. 
=============================================
[2019-04-08 15:29:04,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4395906e-16 8.4654478e-10 1.1528529e-06 2.0057560e-06 3.2794441e-11
 1.2771537e-13 4.2583008e-07 1.0296903e-13 1.7152283e-17 1.2585924e-19
 9.9999642e-01], sum to 1.0000
[2019-04-08 15:29:04,938] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2337
[2019-04-08 15:29:04,956] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 19.0, 26.09303232506626, 0.5547841212108476, 0.0, 1.0, 65.0, 55875.92294939934], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2084400.0000, 
sim time next is 2085000.0000, 
raw observation next is [-5.100000000000001, 86.83333333333334, 0.0, 0.0, 19.0, 26.09690769330207, 0.554888956347315, 0.0, 1.0, 65.0, 55290.57074749506], 
processed observation next is [1.0, 0.13043478260869565, 0.32132963988919666, 0.8683333333333334, 0.0, 0.0, 0.08333333333333333, 0.6747423077751726, 0.684962985449105, 0.0, 1.0, 1.0, 0.26328843213092884], 
reward next is 0.7367, 
noisyNet noise sample is [array([1.4836879], dtype=float32), -2.3788016]. 
=============================================
[2019-04-08 15:29:04,966] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.30989]
 [78.39463]
 [78.30289]
 [78.25502]
 [77.93082]], R is [[78.36417389]
 [78.31446075]
 [78.26318359]
 [78.21917725]
 [78.18275452]].
[2019-04-08 15:29:05,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8603720e-16 4.7976745e-10 3.5623177e-06 3.6213281e-05 6.7526551e-10
 3.2133937e-12 1.0566683e-07 7.0970134e-13 5.0369460e-16 1.7622482e-17
 9.9996006e-01], sum to 1.0000
[2019-04-08 15:29:05,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4108
[2019-04-08 15:29:05,192] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 22.5, 26.35882466607192, 0.6603204857246349, 0.0, 1.0, 65.0, 49314.58884260617], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 2059200.0000, 
sim time next is 2059800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 19.0, 26.33756474092037, 0.6575486119614015, 0.0, 1.0, 65.0, 49350.93749178269], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6947970617433642, 0.7191828706538005, 0.0, 1.0, 1.0, 0.23500446424658425], 
reward next is 0.7650, 
noisyNet noise sample is [array([1.2129191], dtype=float32), 2.2913945]. 
=============================================
[2019-04-08 15:29:05,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.4597358e-15 7.4221155e-08 3.7417161e-05 2.7115206e-05 7.0087358e-09
 4.9580669e-12 4.7448430e-06 2.8730742e-12 2.8647835e-16 1.7089307e-17
 9.9993062e-01], sum to 1.0000
[2019-04-08 15:29:05,697] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8225
[2019-04-08 15:29:05,720] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 22.5, 25.91025400209294, 0.4984676350188473, 1.0, 1.0, 65.0, 56086.53616769573], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 2100600.0000, 
sim time next is 2101200.0000, 
raw observation next is [-7.1, 78.66666666666667, 0.0, 0.0, 22.5, 25.90189669115415, 0.4938982793616228, 1.0, 1.0, 65.0, 56009.58518581387], 
processed observation next is [1.0, 0.30434782608695654, 0.2659279778393352, 0.7866666666666667, 0.0, 0.0, 0.375, 0.6584913909295125, 0.6646327597872076, 1.0, 1.0, 1.0, 0.26671231040863747], 
reward next is 0.7333, 
noisyNet noise sample is [array([0.50638115], dtype=float32), -0.18922666]. 
=============================================
[2019-04-08 15:29:05,929] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0639229e-15 9.4896349e-09 2.4871019e-04 1.1582150e-05 1.9148578e-08
 6.8722888e-10 1.5557998e-05 9.2867730e-14 4.8196165e-17 8.8778876e-19
 9.9972409e-01], sum to 1.0000
[2019-04-08 15:29:05,929] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1809
[2019-04-08 15:29:05,945] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 22.5, 26.31530144781281, 0.589283567290797, 1.0, 1.0, 65.0, 46993.76212800741], 
current ob forecast is [], 
actual action is [-2.716666666666667, 65.0], 
sim time this is 2105400.0000, 
sim time next is 2106000.0000, 
raw observation next is [-7.8, 82.0, 123.0, 77.5, 22.5, 26.43793256875272, 0.6103281783553696, 1.0, 1.0, 65.0, 43859.54405265403], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.82, 0.41, 0.0856353591160221, 0.375, 0.7031610473960601, 0.7034427261184565, 1.0, 1.0, 1.0, 0.20885497167930492], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.1073897], dtype=float32), 1.2279059]. 
=============================================
[2019-04-08 15:29:05,954] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.28415 ]
 [81.16321 ]
 [81.27871 ]
 [81.01286 ]
 [80.992775]], R is [[81.18404388]
 [81.14842987]
 [81.1137619 ]
 [81.07540131]
 [81.01996613]].
[2019-04-08 15:29:06,177] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2697651e-17 1.2624820e-09 3.7267844e-07 1.0173981e-06 2.4557269e-11
 4.0933836e-13 4.1966569e-08 1.7630516e-14 1.3934308e-18 1.6537727e-20
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:29:06,177] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1931
[2019-04-08 15:29:06,211] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.4, 73.66666666666667, 91.83333333333333, 419.5, 22.5, 26.6853832267323, 0.6475806048246855, 1.0, 1.0, 65.0, 41371.92284478801], 
current ob forecast is [], 
actual action is [-0.40000000000000036, 65.0], 
sim time this is 2193600.0000, 
sim time next is 2194200.0000, 
raw observation next is [-5.3, 73.0, 102.0, 451.0, 22.5, 26.75819903130033, 0.6645897276186692, 1.0, 1.0, 65.0, 39566.76824045101], 
processed observation next is [1.0, 0.391304347826087, 0.31578947368421056, 0.73, 0.34, 0.4983425414364641, 0.375, 0.7298499192750274, 0.7215299092062231, 1.0, 1.0, 1.0, 0.18841318209738578], 
reward next is 0.8116, 
noisyNet noise sample is [array([2.032396], dtype=float32), -1.790231]. 
=============================================
[2019-04-08 15:29:06,324] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.7340969e-15 1.1022530e-08 2.2066706e-05 5.8041136e-05 4.4458495e-09
 9.8613513e-12 1.6324684e-07 2.6437841e-12 2.1693935e-16 4.6661858e-19
 9.9991965e-01], sum to 1.0000
[2019-04-08 15:29:06,324] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9276
[2019-04-08 15:29:06,343] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 88.5, 0.0, 0.0, 19.0, 26.10839407543172, 0.5722100132801099, 0.0, 1.0, 65.0, 54903.23233964887], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2079000.0000, 
sim time next is 2079600.0000, 
raw observation next is [-4.5, 87.66666666666666, 0.0, 0.0, 19.0, 26.14380681725527, 0.5651803066190318, 0.0, 1.0, 65.0, 54199.69532629596], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.8766666666666666, 0.0, 0.0, 0.08333333333333333, 0.6786505681046059, 0.6883934355396772, 0.0, 1.0, 1.0, 0.25809378726807597], 
reward next is 0.7419, 
noisyNet noise sample is [array([1.1518863], dtype=float32), 0.0339599]. 
=============================================
[2019-04-08 15:29:06,400] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [9.6504465e-18 3.1309408e-09 1.2790787e-06 1.7634712e-06 1.4505122e-09
 2.3606089e-10 9.8177043e-07 7.2472124e-12 1.0192286e-16 1.7641064e-18
 9.9999595e-01], sum to 1.0000
[2019-04-08 15:29:06,401] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6895
[2019-04-08 15:29:06,438] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 79.0, 133.3333333333333, 0.0, 22.5, 26.40820063084058, 0.7088018884220362, 1.0, 1.0, 65.0, 63695.47433184564], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 2037000.0000, 
sim time next is 2037600.0000, 
raw observation next is [-3.9, 79.0, 126.0, 0.0, 22.5, 25.93685896362047, 0.6341610175839478, 1.0, 1.0, 65.0, 43226.7604637537], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.79, 0.42, 0.0, 0.375, 0.6614049136350392, 0.711387005861316, 1.0, 1.0, 1.0, 0.20584171649406524], 
reward next is 0.7942, 
noisyNet noise sample is [array([-1.169529], dtype=float32), -0.7203507]. 
=============================================
[2019-04-08 15:29:06,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0144923e-14 2.0755643e-08 3.0755775e-05 6.2360677e-06 1.3181569e-08
 7.4070562e-11 3.8249050e-06 1.0984520e-11 3.6551859e-16 1.4879105e-18
 9.9995911e-01], sum to 1.0000
[2019-04-08 15:29:06,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0133
[2019-04-08 15:29:06,581] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 19.0, 26.27314671451883, 0.6453363419882309, 0.0, 1.0, 65.0, 48623.13020707849], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 2066400.0000, 
sim time next is 2067000.0000, 
raw observation next is [-4.0, 82.66666666666667, 0.0, 0.0, 19.0, 26.27095108856459, 0.6029175286121684, 0.0, 1.0, 65.0, 49930.58290011239], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6892459240470492, 0.7009725095373894, 0.0, 1.0, 1.0, 0.2377646804767257], 
reward next is 0.7622, 
noisyNet noise sample is [array([-1.0392064], dtype=float32), -0.51547277]. 
=============================================
[2019-04-08 15:29:06,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.801   ]
 [75.6361  ]
 [75.025536]
 [74.83769 ]
 [74.81179 ]], R is [[76.03090668]
 [76.0390625 ]
 [76.04678345]
 [76.05404663]
 [76.06082916]].
[2019-04-08 15:29:06,771] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2701532e-16 1.0798580e-09 1.2948507e-06 4.0046421e-06 1.5345029e-10
 4.7351962e-12 4.3387097e-08 9.0833688e-13 1.0366722e-17 1.6501822e-18
 9.9999464e-01], sum to 1.0000
[2019-04-08 15:29:06,772] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1151
[2019-04-08 15:29:06,793] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 25.89905807609499, 0.501351129502962, 0.0, 1.0, 65.0, 57045.07713916526], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 2098800.0000, 
sim time next is 2099400.0000, 
raw observation next is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 22.5, 25.89992148761927, 0.4972428929291539, 0.0, 1.0, 65.0, 56747.40547707283], 
processed observation next is [1.0, 0.30434782608695654, 0.2742382271468144, 0.7816666666666667, 0.0, 0.0, 0.375, 0.6583267906349392, 0.6657476309763847, 0.0, 1.0, 1.0, 0.27022574036701347], 
reward next is 0.7298, 
noisyNet noise sample is [array([0.56159854], dtype=float32), 1.9599261]. 
=============================================
[2019-04-08 15:29:07,687] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3868257e-16 6.5268404e-09 5.2470032e-06 2.5624155e-05 2.8711815e-09
 5.8882847e-12 2.8190293e-06 1.2927167e-13 1.4068941e-17 3.9302888e-17
 9.9996626e-01], sum to 1.0000
[2019-04-08 15:29:07,692] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1191
[2019-04-08 15:29:07,712] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 82.0, 174.0, 118.0, 22.5, 26.73477369902741, 0.6585093992702419, 1.0, 1.0, 65.0, 39080.21253983359], 
current ob forecast is [], 
actual action is [-2.8, 65.0], 
sim time this is 2107800.0000, 
sim time next is 2108400.0000, 
raw observation next is [-7.799999999999999, 82.0, 179.6666666666667, 108.3333333333333, 22.5, 26.84356612663035, 0.6635825946970856, 1.0, 1.0, 65.0, 37607.59409125317], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188372, 0.82, 0.598888888888889, 0.1197053406998158, 0.375, 0.7369638438858624, 0.7211941982323619, 1.0, 1.0, 1.0, 0.17908378138691988], 
reward next is 0.8209, 
noisyNet noise sample is [array([-0.80189216], dtype=float32), 0.5462893]. 
=============================================
[2019-04-08 15:29:08,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4951541e-16 4.2051722e-09 7.5554240e-06 6.2546233e-06 5.8899186e-10
 2.1395306e-11 9.7019438e-07 3.8761459e-12 2.4355289e-15 9.7599961e-19
 9.9998522e-01], sum to 1.0000
[2019-04-08 15:29:08,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9418
[2019-04-08 15:29:08,512] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.100000000000001, 64.66666666666667, 149.6666666666667, 44.66666666666666, 22.5, 26.2566832503678, 0.712878888966903, 1.0, 1.0, 65.0, 65585.42306869318], 
current ob forecast is [], 
actual action is [-1.1000000000000014, 65.0], 
sim time this is 2121000.0000, 
sim time next is 2121600.0000, 
raw observation next is [-6.0, 65.33333333333334, 149.3333333333333, 22.33333333333333, 22.5, 25.49477983038878, 0.6392508380198719, 1.0, 1.0, 65.0, 46859.87001349375], 
processed observation next is [1.0, 0.5652173913043478, 0.296398891966759, 0.6533333333333334, 0.4977777777777776, 0.024677716390423567, 0.375, 0.6245649858657316, 0.7130836126732906, 1.0, 1.0, 1.0, 0.22314223815949408], 
reward next is 0.7769, 
noisyNet noise sample is [array([1.6573519], dtype=float32), -0.69125646]. 
=============================================
[2019-04-08 15:29:08,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.15080173e-15 1.66304004e-09 6.29274800e-05 3.39418730e-05
 4.75152084e-10 1.93917798e-11 2.95011915e-07 8.77640517e-13
 1.19351804e-14 7.89793455e-18 9.99902844e-01], sum to 1.0000
[2019-04-08 15:29:08,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9756
[2019-04-08 15:29:08,620] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 25.89905811764617, 0.5013511410241898, 0.0, 1.0, 65.0, 57045.07693550896], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 2098800.0000, 
sim time next is 2099400.0000, 
raw observation next is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 22.5, 25.89992152955665, 0.4972429059367315, 0.0, 1.0, 65.0, 56747.40526982458], 
processed observation next is [1.0, 0.30434782608695654, 0.2742382271468144, 0.7816666666666667, 0.0, 0.0, 0.375, 0.6583267941297208, 0.6657476353122438, 0.0, 1.0, 1.0, 0.27022573938011707], 
reward next is 0.7298, 
noisyNet noise sample is [array([-1.7718645], dtype=float32), 1.3938953]. 
=============================================
[2019-04-08 15:29:08,629] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9329467e-15 1.3765782e-09 2.6839557e-06 5.3361384e-07 2.3392814e-09
 2.8234698e-11 5.2473077e-08 8.1291538e-13 2.8631813e-16 2.2415284e-19
 9.9999678e-01], sum to 1.0000
[2019-04-08 15:29:08,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8964
[2019-04-08 15:29:08,658] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.583333333333333, 65.5, 66.0, 0.0, 22.5, 27.26101982572516, 0.7677074000299348, 1.0, 1.0, 65.0, 37022.33159977252], 
current ob forecast is [], 
actual action is [0.41666666666666696, 65.0], 
sim time this is 2130600.0000, 
sim time next is 2131200.0000, 
raw observation next is [-4.5, 65.0, 56.0, 0.0, 22.5, 27.26119305154185, 0.7665412085787354, 1.0, 1.0, 65.0, 37146.20834192941], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.65, 0.18666666666666668, 0.0, 0.375, 0.7717660876284874, 0.7555137361929117, 1.0, 1.0, 1.0, 0.17688670639014004], 
reward next is 0.8231, 
noisyNet noise sample is [array([-0.34704703], dtype=float32), 1.9144275]. 
=============================================
[2019-04-08 15:29:08,691] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.1926554e-16 3.7030612e-09 1.6927606e-06 2.3877477e-05 2.4783850e-10
 3.1682143e-11 7.2917226e-07 7.9996983e-14 1.3977363e-16 2.6900438e-19
 9.9997365e-01], sum to 1.0000
[2019-04-08 15:29:08,691] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5292
[2019-04-08 15:29:08,709] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 19.0, 26.13330156979055, 0.5753191482875529, 0.0, 1.0, 65.0, 55499.52827342203], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2076600.0000, 
sim time next is 2077200.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 19.0, 26.12238558402503, 0.572326442445425, 0.0, 1.0, 65.0, 55106.97622912872], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.91, 0.0, 0.0, 0.08333333333333333, 0.676865465335419, 0.6907754808151417, 0.0, 1.0, 1.0, 0.2624141725196606], 
reward next is 0.7376, 
noisyNet noise sample is [array([-0.80115396], dtype=float32), -0.32234013]. 
=============================================
[2019-04-08 15:29:08,862] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5759485e-14 3.0365328e-08 4.1739031e-06 1.5722311e-05 5.5516018e-09
 1.4221453e-10 3.8188801e-07 9.1821958e-12 1.8928349e-15 1.8979591e-17
 9.9997973e-01], sum to 1.0000
[2019-04-08 15:29:08,866] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6001
[2019-04-08 15:29:08,889] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.5, 70.5, 0.0, 0.0, 19.0, 26.24185595306749, 0.6372556617340418, 0.0, 1.0, 65.0, 52317.56596171424], 
current ob forecast is [], 
actual action is [-0.5, 65.0], 
sim time this is 2238600.0000, 
sim time next is 2239200.0000, 
raw observation next is [-5.6, 71.0, 0.0, 0.0, 19.0, 26.23163353579316, 0.6350533341031381, 0.0, 1.0, 65.0, 52321.85161263654], 
processed observation next is [1.0, 0.9565217391304348, 0.30747922437673136, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6859694613160968, 0.7116844447010461, 0.0, 1.0, 1.0, 0.2491516743458883], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.89551306], dtype=float32), 0.3044419]. 
=============================================
[2019-04-08 15:29:09,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6462236e-14 2.7222482e-09 1.5276637e-05 9.7010070e-06 2.6469618e-09
 6.5407735e-11 8.0665366e-08 1.3149953e-13 1.9422780e-16 1.0215422e-17
 9.9997497e-01], sum to 1.0000
[2019-04-08 15:29:09,069] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6673
[2019-04-08 15:29:09,113] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.916666666666667, 67.5, 99.0, 0.0, 22.5, 26.0610446419126, 0.6839326876474163, 1.0, 1.0, 64.99999999999999, 50031.78443235328], 
current ob forecast is [], 
actual action is [0.08333333333333304, 65.0], 
sim time this is 2128200.0000, 
sim time next is 2128800.0000, 
raw observation next is [-4.833333333333334, 67.0, 92.5, 0.0, 22.5, 26.87674315989699, 0.7397631484853778, 1.0, 1.0, 65.00000000000001, 37680.06480818109], 
processed observation next is [1.0, 0.6521739130434783, 0.32871652816251157, 0.67, 0.30833333333333335, 0.0, 0.375, 0.7397285966580824, 0.7465877161617925, 1.0, 1.0, 1.0000000000000002, 0.17942888003895754], 
reward next is 0.8206, 
noisyNet noise sample is [array([-0.8937911], dtype=float32), 1.6396701]. 
=============================================
[2019-04-08 15:29:10,721] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.8740676e-17 9.8483832e-10 6.4869170e-07 2.2491179e-06 4.1834175e-11
 3.3640920e-12 5.9061595e-08 2.1320200e-14 5.5777642e-17 1.2859104e-20
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:29:10,721] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1900
[2019-04-08 15:29:10,749] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 22.5, 26.36257538786685, 0.5633623610510979, 1.0, 1.0, 65.0, 46868.06867653635], 
current ob forecast is [], 
actual action is [-2.633333333333333, 65.0], 
sim time this is 2104800.0000, 
sim time next is 2105400.0000, 
raw observation next is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 22.5, 26.31530151005882, 0.5892835817477736, 1.0, 1.0, 65.0, 46993.76135247535], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.815, 0.35333333333333333, 0.07071823204419889, 0.375, 0.6929417925049016, 0.6964278605825912, 1.0, 1.0, 1.0, 0.22377981596416832], 
reward next is 0.7762, 
noisyNet noise sample is [array([-0.20375377], dtype=float32), 0.8574307]. 
=============================================
[2019-04-08 15:29:12,993] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9729640e-15 5.4790950e-10 1.1941941e-05 4.9007617e-06 3.2798395e-10
 1.5321971e-11 1.9764862e-07 3.8025561e-13 9.3334567e-17 2.9723931e-18
 9.9998295e-01], sum to 1.0000
[2019-04-08 15:29:12,996] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8333
[2019-04-08 15:29:13,039] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 26.02467792698079, 0.5458092051217657, 0.0, 1.0, 65.0, 54683.63601635163], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 2167200.0000, 
sim time next is 2167800.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 26.1014785579034, 0.5561352491617799, 0.0, 1.0, 65.0, 52616.24250346335], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6751232131586166, 0.6853784163872599, 0.0, 1.0, 1.0, 0.2505535357307779], 
reward next is 0.7494, 
noisyNet noise sample is [array([0.36894614], dtype=float32), -0.17443815]. 
=============================================
[2019-04-08 15:29:13,773] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.19140429e-13 4.05860554e-08 4.78543598e-06 3.90740979e-06
 1.47137964e-08 1.34837239e-10 1.89572506e-06 3.85334776e-12
 1.07077485e-16 4.44340084e-18 9.99989390e-01], sum to 1.0000
[2019-04-08 15:29:13,798] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5449
[2019-04-08 15:29:13,824] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 71.5, 0.0, 0.0, 22.5, 26.84467598899095, 0.7225124477154709, 1.0, 1.0, 65.0, 40298.72132053745], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2139000.0000, 
sim time next is 2139600.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 22.5, 26.80671959092908, 0.7097398901791278, 1.0, 1.0, 65.0, 41962.2659590127], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.72, 0.0, 0.0, 0.375, 0.7338932992440901, 0.7365799633930425, 1.0, 1.0, 1.0, 0.19982031409053666], 
reward next is 0.8002, 
noisyNet noise sample is [array([0.61874783], dtype=float32), 1.5137584]. 
=============================================
[2019-04-08 15:29:14,112] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0413236e-14 9.1777830e-09 1.1009302e-05 2.6514395e-05 4.0740300e-09
 2.1347878e-10 1.6094934e-06 1.6877635e-11 1.8834932e-15 4.1541050e-17
 9.9996090e-01], sum to 1.0000
[2019-04-08 15:29:14,119] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8537
[2019-04-08 15:29:14,149] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.783333333333333, 83.0, 0.0, 0.0, 19.0, 26.27355806377436, 0.6495805560687297, 0.0, 1.0, 65.0, 49096.30511284958], 
current ob forecast is [], 
actual action is [-0.7833333333333332, 65.0], 
sim time this is 2149800.0000, 
sim time next is 2150400.0000, 
raw observation next is [-5.966666666666667, 83.0, 0.0, 0.0, 19.0, 26.27430865028539, 0.6461998842930169, 0.0, 1.0, 65.0, 48954.97640435708], 
processed observation next is [1.0, 0.9130434782608695, 0.2973222530009234, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6895257208571159, 0.7153999614310056, 0.0, 1.0, 1.0, 0.23311893525884325], 
reward next is 0.7669, 
noisyNet noise sample is [array([1.4343908], dtype=float32), 1.036201]. 
=============================================
[2019-04-08 15:29:17,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.74690130e-15 3.60649199e-09 1.84792279e-05 1.56407277e-05
 7.74315612e-10 1.12671004e-10 5.45419289e-06 3.17603019e-12
 2.22593403e-16 5.70781868e-19 9.99960423e-01], sum to 1.0000
[2019-04-08 15:29:17,935] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3630
[2019-04-08 15:29:17,953] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 25.94389813725601, 0.5468666697406619, 0.0, 1.0, 65.0, 58252.01013244443], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 2246400.0000, 
sim time next is 2247000.0000, 
raw observation next is [-6.700000000000001, 77.5, 0.0, 0.0, 19.0, 25.93157096321699, 0.5456497936662792, 0.0, 1.0, 65.0, 58305.13611038718], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.775, 0.0, 0.0, 0.08333333333333333, 0.6609642469347493, 0.6818832645554265, 0.0, 1.0, 1.0, 0.277643505287558], 
reward next is 0.7224, 
noisyNet noise sample is [array([-0.6172827], dtype=float32), -0.52569294]. 
=============================================
[2019-04-08 15:29:17,993] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.4971  ]
 [75.619385]
 [75.612434]
 [75.59085 ]
 [75.61151 ]], R is [[75.40457916]
 [75.37314606]
 [75.34270477]
 [75.31321716]
 [75.28401184]].
[2019-04-08 15:29:18,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9661335e-15 9.6508346e-10 6.6208827e-06 3.1391521e-06 2.7141832e-11
 1.1904949e-12 1.4546306e-07 2.6597398e-13 5.3646600e-17 1.7027899e-19
 9.9999011e-01], sum to 1.0000
[2019-04-08 15:29:18,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6420
[2019-04-08 15:29:18,747] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 70.5, 128.0, 0.0, 22.5, 27.18146951228328, 0.7422335567875727, 1.0, 1.0, 65.0, 38381.76888923004], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 2211000.0000, 
sim time next is 2211600.0000, 
raw observation next is [-3.9, 70.0, 124.0, 0.0, 22.5, 27.19034021342647, 0.7456191670598217, 1.0, 1.0, 65.0, 38481.91630997345], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.7, 0.41333333333333333, 0.0, 0.375, 0.7658616844522058, 0.7485397223532738, 1.0, 1.0, 1.0, 0.18324722052368309], 
reward next is 0.8168, 
noisyNet noise sample is [array([0.27208906], dtype=float32), -0.60268706]. 
=============================================
[2019-04-08 15:29:20,175] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9706340e-17 4.7448635e-11 4.0468231e-06 3.0461526e-06 4.8097054e-10
 1.4995429e-13 1.6532789e-07 1.2189246e-13 1.5856176e-19 3.5577133e-18
 9.9999273e-01], sum to 1.0000
[2019-04-08 15:29:20,176] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4637
[2019-04-08 15:29:20,206] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 91.0, 9.999999999999998, 19.33333333333334, 22.5, 25.66761247627192, 0.436004900814599, 1.0, 1.0, 65.0, 55557.79058045711], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 2274000.0000, 
sim time next is 2274600.0000, 
raw observation next is [-9.5, 91.0, 17.0, 18.66666666666667, 22.5, 25.68428014262934, 0.4299193892667524, 1.0, 1.0, 65.0, 55837.18467239503], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.056666666666666664, 0.02062615101289135, 0.375, 0.640356678552445, 0.6433064630889175, 1.0, 1.0, 1.0, 0.2658913555828335], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.8367312], dtype=float32), 0.6613941]. 
=============================================
[2019-04-08 15:29:20,255] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.3945519e-15 5.8188355e-08 1.0811773e-05 2.7484806e-05 5.9416594e-10
 9.7927909e-11 1.8330509e-07 1.5537082e-12 4.0941371e-17 2.5307953e-18
 9.9996150e-01], sum to 1.0000
[2019-04-08 15:29:20,256] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8924
[2019-04-08 15:29:20,277] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 69.5, 0.0, 0.0, 19.0, 26.30204654780325, 0.6533502144527722, 0.0, 1.0, 65.0, 52049.80925396573], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2233800.0000, 
sim time next is 2234400.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 19.0, 26.28711538046256, 0.6508939347784172, 0.0, 1.0, 65.0, 52177.05135989391], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6905929483718799, 0.7169646449261391, 0.0, 1.0, 1.0, 0.24846214933282812], 
reward next is 0.7515, 
noisyNet noise sample is [array([0.67727476], dtype=float32), 0.58033586]. 
=============================================
[2019-04-08 15:29:20,602] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4503725e-15 5.6710109e-10 3.1137366e-05 3.9297778e-05 3.5395886e-09
 8.6789421e-12 2.5655851e-07 9.8849618e-13 7.3731285e-16 7.0547436e-20
 9.9992931e-01], sum to 1.0000
[2019-04-08 15:29:20,616] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6255
[2019-04-08 15:29:20,632] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.7, 66.33333333333334, 166.6666666666667, 90.0, 22.5, 26.89901188494511, 0.6677409529954127, 1.0, 1.0, 65.0, 38342.65244259559], 
current ob forecast is [], 
actual action is [0.2999999999999998, 65.0], 
sim time this is 2286600.0000, 
sim time next is 2287200.0000, 
raw observation next is [-4.4, 64.66666666666667, 163.8333333333333, 100.0, 22.5, 26.93234521289746, 0.6724901669188262, 1.0, 1.0, 65.0, 37816.83969002771], 
processed observation next is [1.0, 0.4782608695652174, 0.3407202216066482, 0.6466666666666667, 0.546111111111111, 0.11049723756906077, 0.375, 0.7443621010747883, 0.7241633889729421, 1.0, 1.0, 1.0, 0.18008018900013195], 
reward next is 0.8199, 
noisyNet noise sample is [array([-1.2059277], dtype=float32), 0.6548278]. 
=============================================
[2019-04-08 15:29:21,652] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8513608e-17 9.6091968e-10 3.6669385e-06 1.9434451e-07 6.6827807e-11
 3.6677736e-12 1.5301898e-08 1.5356211e-13 8.2813683e-18 2.3008966e-20
 9.9999607e-01], sum to 1.0000
[2019-04-08 15:29:21,653] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7618
[2019-04-08 15:29:21,723] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.133333333333335, 74.66666666666667, 165.1666666666667, 48.16666666666667, 22.5, 26.71187878391035, 0.620124253256299, 1.0, 1.0, 65.0, 41248.72693439086], 
current ob forecast is [], 
actual action is [-1.1333333333333346, 65.0], 
sim time this is 2283600.0000, 
sim time next is 2284200.0000, 
raw observation next is [-5.85, 73.0, 178.0, 50.0, 22.5, 26.76649367846609, 0.6276607300629768, 1.0, 1.0, 65.0, 40449.42433428756], 
processed observation next is [1.0, 0.43478260869565216, 0.30055401662049863, 0.73, 0.5933333333333334, 0.055248618784530384, 0.375, 0.7305411398721743, 0.7092202433543257, 1.0, 1.0, 1.0, 0.1926163063537503], 
reward next is 0.8074, 
noisyNet noise sample is [array([-0.16835696], dtype=float32), -0.69881594]. 
=============================================
[2019-04-08 15:29:21,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9610757e-15 3.1958476e-09 1.9699330e-06 6.3581542e-06 4.5956261e-09
 2.3925441e-12 6.1009587e-08 2.7400833e-13 6.1460486e-17 2.0583691e-18
 9.9999166e-01], sum to 1.0000
[2019-04-08 15:29:21,821] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9684
[2019-04-08 15:29:21,834] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.65, 89.0, 0.0, 0.0, 19.0, 25.93295462828828, 0.491926328831515, 0.0, 1.0, 65.0, 56423.16821116429], 
current ob forecast is [], 
actual action is [-3.6500000000000004, 65.0], 
sim time this is 2262600.0000, 
sim time next is 2263200.0000, 
raw observation next is [-8.733333333333334, 89.66666666666667, 0.0, 0.0, 19.0, 25.91597823308265, 0.4842532579719612, 0.0, 1.0, 65.0, 56648.46592305017], 
processed observation next is [1.0, 0.17391304347826086, 0.22068328716528163, 0.8966666666666667, 0.0, 0.0, 0.08333333333333333, 0.6596648527568876, 0.6614177526573204, 0.0, 1.0, 1.0, 0.26975459963357223], 
reward next is 0.7302, 
noisyNet noise sample is [array([1.2346463], dtype=float32), 0.91617227]. 
=============================================
[2019-04-08 15:29:21,856] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.8079793e-17 1.0989715e-09 3.7522152e-06 1.8610726e-07 7.2875261e-11
 3.9465970e-12 1.4737769e-08 1.6827616e-13 9.5153034e-18 2.5513919e-20
 9.9999607e-01], sum to 1.0000
[2019-04-08 15:29:21,867] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5903
[2019-04-08 15:29:21,894] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.283333333333333, 69.66666666666667, 172.3333333333333, 70.0, 22.5, 26.8228819817245, 0.6492444266072203, 1.0, 1.0, 65.0, 39922.11366371423], 
current ob forecast is [], 
actual action is [-0.2833333333333332, 65.0], 
sim time this is 2285400.0000, 
sim time next is 2286000.0000, 
raw observation next is [-5.0, 68.0, 169.5, 80.0, 22.5, 26.86237924375935, 0.6573899021506731, 1.0, 1.0, 65.0, 38888.13059968601], 
processed observation next is [1.0, 0.4782608695652174, 0.32409972299168976, 0.68, 0.565, 0.08839779005524862, 0.375, 0.7385316036466124, 0.7191299673835577, 1.0, 1.0, 1.0, 0.1851815742842191], 
reward next is 0.8148, 
noisyNet noise sample is [array([-0.16835696], dtype=float32), -0.69881594]. 
=============================================
[2019-04-08 15:29:21,900] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.91348]
 [78.94418]
 [78.90481]
 [79.28011]
 [79.60563]], R is [[78.88737488]
 [78.90840149]
 [78.92568207]
 [78.94380951]
 [78.95795441]].
[2019-04-08 15:29:22,279] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.6993474e-13 5.8833031e-08 1.1763062e-04 1.2322812e-04 2.1929017e-08
 1.0554958e-09 8.9829817e-05 1.5176095e-11 4.8182169e-15 7.0083718e-17
 9.9966919e-01], sum to 1.0000
[2019-04-08 15:29:22,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9849
[2019-04-08 15:29:22,320] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 51.0, 0.0, 0.0, 22.5, 27.26130850386807, 0.7955835252972019, 1.0, 1.0, 65.0, 33968.70566829864], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 2310000.0000, 
sim time next is 2310600.0000, 
raw observation next is [-1.1, 51.5, 0.0, 0.0, 22.5, 27.17311094039044, 0.7845278835785964, 1.0, 1.0, 65.0, 31969.62633578011], 
processed observation next is [1.0, 0.7391304347826086, 0.4321329639889197, 0.515, 0.0, 0.0, 0.375, 0.7644259116992034, 0.7615092945261988, 1.0, 1.0, 1.0, 0.15223631588466718], 
reward next is 0.8478, 
noisyNet noise sample is [array([1.9227573], dtype=float32), 0.42848969]. 
=============================================
[2019-04-08 15:29:22,707] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7258935e-16 1.5435225e-07 3.4857455e-06 1.5071933e-05 3.1849105e-08
 3.1555314e-11 1.9592879e-07 2.5831736e-11 2.0142530e-16 2.2727502e-17
 9.9998105e-01], sum to 1.0000
[2019-04-08 15:29:22,707] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7471
[2019-04-08 15:29:22,762] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 50.0, 11.0, 0.0, 22.5, 27.11346886099727, 0.7557670760053719, 1.0, 1.0, 65.00000000000001, 30779.46263887344], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 2308800.0000, 
sim time next is 2309400.0000, 
raw observation next is [-0.8999999999999999, 50.5, 0.0, 0.0, 22.5, 27.31432034232657, 0.7509288326432749, 1.0, 1.0, 65.0, 30509.51201399989], 
processed observation next is [1.0, 0.7391304347826086, 0.43767313019390586, 0.505, 0.0, 0.0, 0.375, 0.7761933618605475, 0.7503096108810916, 1.0, 1.0, 1.0, 0.14528339054285663], 
reward next is 0.8547, 
noisyNet noise sample is [array([-0.33217287], dtype=float32), -0.03979285]. 
=============================================
[2019-04-08 15:29:22,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0819374e-15 4.2628523e-09 9.8937844e-06 9.7202956e-06 5.2405309e-08
 4.8478565e-11 1.7722073e-06 1.7173756e-12 6.3859984e-16 2.2428926e-17
 9.9997854e-01], sum to 1.0000
[2019-04-08 15:29:22,804] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4961
[2019-04-08 15:29:22,845] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.37265742840415, 0.604398500749948, 0.0, 1.0, 65.0, 46763.15755974721], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 2341200.0000, 
sim time next is 2341800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.34786943193182, 0.5985713023636744, 0.0, 1.0, 65.0, 47692.38956520169], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6956557859943183, 0.6995237674545581, 0.0, 1.0, 1.0, 0.2271066169771509], 
reward next is 0.7729, 
noisyNet noise sample is [array([-0.2993652], dtype=float32), -2.062031]. 
=============================================
[2019-04-08 15:29:23,269] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8560159e-17 2.3308774e-10 3.4687637e-06 8.2688739e-06 2.7946971e-09
 5.2832906e-13 3.7178805e-07 6.0406053e-13 2.7881142e-18 4.6232501e-20
 9.9998784e-01], sum to 1.0000
[2019-04-08 15:29:23,271] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7327
[2019-04-08 15:29:23,301] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.550000000000001, 82.5, 101.0, 39.0, 22.5, 26.4494355050172, 0.5623365226622775, 1.0, 1.0, 65.0, 46515.69752708007], 
current ob forecast is [], 
actual action is [-2.5500000000000007, 65.0], 
sim time this is 2280600.0000, 
sim time next is 2281200.0000, 
raw observation next is [-7.266666666666667, 81.0, 113.8333333333333, 40.83333333333333, 22.5, 26.51038970889664, 0.5766685770878053, 1.0, 1.0, 65.0, 44960.76467711345], 
processed observation next is [1.0, 0.391304347826087, 0.26131117266851345, 0.81, 0.3794444444444443, 0.04511970534069981, 0.375, 0.7091991424080533, 0.6922228590292684, 1.0, 1.0, 1.0, 0.21409887941482597], 
reward next is 0.7859, 
noisyNet noise sample is [array([-0.6491052], dtype=float32), -0.54042196]. 
=============================================
[2019-04-08 15:29:23,812] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8624057e-14 4.3163129e-10 3.7536311e-06 9.3470790e-06 1.0897218e-08
 3.6703301e-12 1.6817418e-06 1.0183200e-12 5.2086987e-15 8.3767861e-18
 9.9998522e-01], sum to 1.0000
[2019-04-08 15:29:23,812] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0019
[2019-04-08 15:29:23,847] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 61.0, 0.0, 0.0, 19.0, 25.8306386973297, 0.4440655171009227, 0.0, 1.0, 65.0, 58719.33420796845], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 2434200.0000, 
sim time next is 2434800.0000, 
raw observation next is [-8.4, 61.0, 0.0, 0.0, 19.0, 25.81546384146587, 0.4407572712459125, 0.0, 1.0, 65.0, 58769.62564827911], 
processed observation next is [0.0, 0.17391304347826086, 0.2299168975069252, 0.61, 0.0, 0.0, 0.08333333333333333, 0.6512886534554893, 0.6469190904153042, 0.0, 1.0, 1.0, 0.2798553602299005], 
reward next is 0.7201, 
noisyNet noise sample is [array([-0.04579556], dtype=float32), 1.6886659]. 
=============================================
[2019-04-08 15:29:24,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.3426963e-15 2.4857044e-10 4.9451376e-07 4.9826090e-06 4.8371773e-10
 1.5133367e-11 3.5205824e-06 2.3784184e-13 4.7678135e-16 2.0506335e-17
 9.9999106e-01], sum to 1.0000
[2019-04-08 15:29:24,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5204
[2019-04-08 15:29:24,871] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 19.0, 26.23311076993165, 0.5495003178814216, 0.0, 1.0, 65.0, 51579.36783520934], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 19.0, 26.25485963302837, 0.550542708141499, 0.0, 1.0, 65.0, 51550.83829434364], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.08333333333333333, 0.6879049694190309, 0.6835142360471663, 0.0, 1.0, 1.0, 0.2454801823540173], 
reward next is 0.7545, 
noisyNet noise sample is [array([-1.445551], dtype=float32), -1.6303272]. 
=============================================
[2019-04-08 15:29:24,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1693051e-15 2.7747015e-08 8.7585868e-06 3.5486776e-06 2.9380047e-09
 2.1847134e-10 1.6047061e-06 6.2890674e-12 2.5574436e-15 8.4497749e-18
 9.9998605e-01], sum to 1.0000
[2019-04-08 15:29:24,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0916
[2019-04-08 15:29:24,995] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.55, 43.5, 138.0, 42.0, 22.5, 26.8062011760488, 0.8062034235306009, 1.0, 1.0, 65.0, 57311.88775822071], 
current ob forecast is [], 
actual action is [5.55, 65.0], 
sim time this is 2302200.0000, 
sim time next is 2302800.0000, 
raw observation next is [0.3666666666666668, 43.66666666666667, 121.8333333333333, 35.0, 22.5, 26.30084545760483, 0.7096498595727713, 1.0, 1.0, 65.0, 33303.95021965653], 
processed observation next is [1.0, 0.6521739130434783, 0.4727608494921515, 0.4366666666666667, 0.406111111111111, 0.03867403314917127, 0.375, 0.6917371214670691, 0.7365499531909238, 1.0, 1.0, 1.0, 0.15859023914122158], 
reward next is 0.8414, 
noisyNet noise sample is [array([-0.19472553], dtype=float32), 0.37414658]. 
=============================================
[2019-04-08 15:29:25,719] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.7001181e-16 1.1126100e-08 8.4629555e-06 5.9065965e-06 1.1116464e-09
 4.4820758e-11 5.1726715e-07 2.5101747e-13 2.9134979e-17 9.3586377e-19
 9.9998510e-01], sum to 1.0000
[2019-04-08 15:29:25,720] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5572
[2019-04-08 15:29:25,736] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.7, 55.66666666666667, 245.1666666666667, 80.0, 22.5, 27.02670671982008, 0.721344063263615, 1.0, 1.0, 65.0, 35944.48534196304], 
current ob forecast is [], 
actual action is [2.3, 65.0], 
sim time this is 2290800.0000, 
sim time next is 2291400.0000, 
raw observation next is [-2.45, 54.5, 262.0, 74.0, 22.5, 27.05407838067356, 0.7177443983471009, 1.0, 1.0, 65.0, 35762.51326835455], 
processed observation next is [1.0, 0.5217391304347826, 0.3947368421052632, 0.545, 0.8733333333333333, 0.08176795580110498, 0.375, 0.7545065317227966, 0.7392481327823669, 1.0, 1.0, 1.0, 0.17029768223025976], 
reward next is 0.8297, 
noisyNet noise sample is [array([-0.25075138], dtype=float32), -0.1705165]. 
=============================================
[2019-04-08 15:29:26,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1035805e-15 2.2405821e-08 6.2796930e-06 2.7005519e-05 9.3133812e-09
 1.4331965e-11 9.9547526e-07 7.5314850e-12 1.8459450e-15 1.4691338e-17
 9.9996567e-01], sum to 1.0000
[2019-04-08 15:29:26,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-08 15:29:26,086] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.33287911567414, 0.6093826848997758, 0.0, 1.0, 65.0, 48302.96298705177], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 2338200.0000, 
sim time next is 2338800.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.30708307504071, 0.606185461976214, 0.0, 1.0, 65.0, 48664.93836633202], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.692256922920059, 0.702061820658738, 0.0, 1.0, 1.0, 0.23173780174443817], 
reward next is 0.7683, 
noisyNet noise sample is [array([0.08084352], dtype=float32), 0.9522654]. 
=============================================
[2019-04-08 15:29:27,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.5770988e-17 7.3597067e-10 6.1879956e-07 6.6595207e-06 4.9592391e-10
 3.7449401e-12 2.2995040e-07 8.7621831e-14 2.0993710e-16 2.6939559e-19
 9.9999249e-01], sum to 1.0000
[2019-04-08 15:29:27,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1382
[2019-04-08 15:29:27,055] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 19.0, 26.20566465740746, 0.5702424732604437, 0.0, 1.0, 65.0, 51999.37232387126], 
current ob forecast is [], 
actual action is [1.9, 65.0], 
sim time this is 2349000.0000, 
sim time next is 2349600.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 19.0, 26.18457207215556, 0.5674415750581838, 0.0, 1.0, 65.0, 52362.01305386245], 
processed observation next is [0.0, 0.17391304347826086, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6820476726796301, 0.6891471916860613, 0.0, 1.0, 1.0, 0.2493429193041069], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.93831986], dtype=float32), -0.5962472]. 
=============================================
[2019-04-08 15:29:27,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0057278e-15 5.4640190e-09 1.9388654e-06 3.1816426e-06 2.5089919e-10
 3.9663883e-11 8.0262865e-08 1.0910694e-12 4.6323812e-17 1.1253021e-16
 9.9999475e-01], sum to 1.0000
[2019-04-08 15:29:27,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9384
[2019-04-08 15:29:27,638] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.566666666666667, 26.0, 13.0, 82.33333333333333, 19.0, 27.1038718054646, 0.7334171815090468, 0.0, 1.0, 65.0, 36378.88165351474], 
current ob forecast is [], 
actual action is [7.566666666666666, 65.0], 
sim time this is 2481600.0000, 
sim time next is 2482200.0000, 
raw observation next is [2.2, 26.5, 0.0, 0.0, 19.0, 27.06141326181998, 0.7189876302915655, 0.0, 1.0, 65.0, 37387.45774777542], 
processed observation next is [0.0, 0.7391304347826086, 0.5235457063711911, 0.265, 0.0, 0.0, 0.08333333333333333, 0.7551177718183316, 0.7396625434305218, 0.0, 1.0, 1.0, 0.17803551308464485], 
reward next is 0.8220, 
noisyNet noise sample is [array([-0.20218286], dtype=float32), 0.13510089]. 
=============================================
[2019-04-08 15:29:28,362] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.8387444e-15 3.5623815e-09 1.4820245e-06 1.2488065e-06 2.1161597e-09
 1.2687328e-11 6.2523833e-07 6.1897969e-13 4.5072868e-16 1.5401031e-18
 9.9999666e-01], sum to 1.0000
[2019-04-08 15:29:28,364] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6745
[2019-04-08 15:29:28,405] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.36040935678537, 0.6114242028843044, 0.0, 1.0, 65.0, 46690.01700029637], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 2337600.0000, 
sim time next is 2338200.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.33287912636658, 0.6093826881241947, 0.0, 1.0, 65.0, 48302.9628646931], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6944065938638818, 0.7031275627080649, 0.0, 1.0, 1.0, 0.23001410887949095], 
reward next is 0.7700, 
noisyNet noise sample is [array([-0.5612382], dtype=float32), -0.8843172]. 
=============================================
[2019-04-08 15:29:29,428] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9113457e-15 3.3293052e-08 1.2813253e-05 1.3921833e-05 4.7372675e-09
 1.0220326e-10 2.3871478e-06 3.2407091e-11 1.3469122e-15 1.4128602e-16
 9.9997079e-01], sum to 1.0000
[2019-04-08 15:29:29,429] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5900
[2019-04-08 15:29:29,460] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 7.833333333333332, 0.0, 19.0, 26.09932956942926, 0.5398235822262701, 0.0, 1.0, 65.0, 54144.93487968772], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 2360400.0000, 
sim time next is 2361000.0000, 
raw observation next is [-3.4, 69.0, 13.66666666666666, 0.0, 19.0, 26.09069644835969, 0.5392169976192588, 0.0, 1.0, 65.0, 54400.66000758057], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.04555555555555554, 0.0, 0.08333333333333333, 0.6742247040299741, 0.6797389992064197, 0.0, 1.0, 1.0, 0.25905076194085985], 
reward next is 0.7409, 
noisyNet noise sample is [array([-0.24625802], dtype=float32), -1.2429262]. 
=============================================
[2019-04-08 15:29:29,488] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[71.85623 ]
 [71.812126]
 [71.82475 ]
 [71.79303 ]
 [71.77673 ]], R is [[71.91537476]
 [71.93838501]
 [71.96214294]
 [71.98688507]
 [72.01451111]].
[2019-04-08 15:29:29,863] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.26509216e-14 7.03121428e-09 4.31378839e-06 7.77638706e-06
 2.86480373e-09 2.66058287e-11 3.64748502e-07 2.45555468e-11
 9.49709963e-16 5.60575752e-18 9.99987483e-01], sum to 1.0000
[2019-04-08 15:29:29,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4278
[2019-04-08 15:29:29,884] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 67.0, 121.0, 360.0, 19.0, 26.16464880722893, 0.5964172380632958, 0.0, 1.0, 65.0, 51641.3749088382], 
current ob forecast is [], 
actual action is [1.9, 65.0], 
sim time this is 2367000.0000, 
sim time next is 2367600.0000, 
raw observation next is [-3.0, 66.33333333333333, 124.0, 375.0, 19.0, 26.19137644876043, 0.6033048567801145, 0.0, 1.0, 65.0, 50733.14832831515], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.6633333333333333, 0.41333333333333333, 0.4143646408839779, 0.08333333333333333, 0.6826147040633691, 0.7011016189267049, 0.0, 1.0, 1.0, 0.2415864206110245], 
reward next is 0.7584, 
noisyNet noise sample is [array([-1.009378], dtype=float32), -0.79329574]. 
=============================================
[2019-04-08 15:29:30,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1227250e-16 1.6992773e-09 2.5432284e-06 2.3065739e-07 2.4752933e-09
 5.7745623e-12 4.5435765e-08 2.4129600e-12 4.1861692e-16 7.4993162e-18
 9.9999714e-01], sum to 1.0000
[2019-04-08 15:29:30,229] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7145
[2019-04-08 15:29:30,278] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 19.0, 26.74258574549642, 0.7016242503001701, 0.0, 1.0, 65.0, 44533.23039036787], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 2393400.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 19.0, 26.73395471760358, 0.6976057636479185, 0.0, 1.0, 65.0, 44720.64067526635], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.08333333333333333, 0.7278295598002984, 0.7325352545493061, 0.0, 1.0, 1.0, 0.21295543178698265], 
reward next is 0.7870, 
noisyNet noise sample is [array([-0.56333476], dtype=float32), 0.84503484]. 
=============================================
[2019-04-08 15:29:30,324] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[73.774475]
 [74.06189 ]
 [74.25446 ]
 [74.45884 ]
 [74.514725]], R is [[73.56665039]
 [73.61891937]
 [73.67194366]
 [73.7261734 ]
 [73.78121948]].
[2019-04-08 15:29:31,062] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-08 15:29:31,064] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:29:31,065] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:29:31,066] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run11
[2019-04-08 15:29:31,092] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:29:31,093] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:29:31,112] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:29:31,112] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:29:31,118] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run11
[2019-04-08 15:29:31,134] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run11
[2019-04-08 15:31:00,674] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06010898]
[2019-04-08 15:31:00,675] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [11.95, 89.0, 0.0, 0.0, 19.0, 28.34428995346797, 1.318440425086509, 0.0, 1.0, 65.0, 18845.43601577141]
[2019-04-08 15:31:00,675] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:31:00,675] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [7.3615791e-18 1.3421299e-10 2.2233738e-07 6.4904077e-07 8.5440752e-11
 1.6301350e-13 3.3398813e-08 7.5894499e-15 4.4517889e-19 1.9435723e-21
 9.9999905e-01], sampled 0.4886872189095379
[2019-04-08 15:31:07,364] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6989.9851 316263137.0936 2958.0840
[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,398] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:07,549] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,633] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.2366 355900315.8327 2370.1879
[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,654] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:14,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:16,898] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3833 342849500.9925 2768.1963
[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:16,919] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,031] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:31:17,922] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 200000, evaluation results [200000.0, 6863.383328607384, 342849500.9924529, 2768.1963418353835, 6989.985061458947, 316263137.0936194, 2958.084015711506, 6801.236591273086, 355900315.83265483, 2370.1878548765894]
[2019-04-08 15:31:18,022] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8850219e-14 2.7541393e-08 7.9397287e-06 1.0827822e-05 7.2320113e-08
 4.6547471e-11 4.0911164e-06 8.6163958e-12 1.3015569e-15 6.7274705e-17
 9.9997699e-01], sum to 1.0000
[2019-04-08 15:31:18,025] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0199
[2019-04-08 15:31:18,041] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.733333333333333, 42.66666666666667, 0.0, 0.0, 19.0, 26.52226476820489, 0.6324843342091239, 0.0, 1.0, 65.0, 49438.36506941027], 
current ob forecast is [], 
actual action is [2.266666666666667, 65.0], 
sim time this is 2402400.0000, 
sim time next is 2403000.0000, 
raw observation next is [-2.9, 42.5, 0.0, 0.0, 19.0, 26.5059327365237, 0.6283019250173407, 0.0, 1.0, 65.0, 49753.2196551063], 
processed observation next is [0.0, 0.8260869565217391, 0.38227146814404434, 0.425, 0.0, 0.0, 0.08333333333333333, 0.7088277280436417, 0.7094339750057802, 0.0, 1.0, 1.0, 0.23692009359574429], 
reward next is 0.7631, 
noisyNet noise sample is [array([-0.48846], dtype=float32), 0.87937164]. 
=============================================
[2019-04-08 15:31:18,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[71.228424]
 [71.144264]
 [71.07632 ]
 [71.016945]
 [70.96994 ]], R is [[71.28395081]
 [71.33569336]
 [71.38859558]
 [71.44326782]
 [71.49880981]].
[2019-04-08 15:31:18,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4019852e-14 2.0224247e-08 1.1706774e-06 2.1819985e-06 2.2506850e-08
 3.2273809e-10 9.7892416e-07 5.8069730e-11 1.8755999e-15 7.2191129e-18
 9.9999559e-01], sum to 1.0000
[2019-04-08 15:31:18,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6421
[2019-04-08 15:31:18,322] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 19.0, 25.9961295517227, 0.4793223843216528, 0.0, 1.0, 65.0, 57311.60696344771], 
current ob forecast is [], 
actual action is [-2.633333333333333, 65.0], 
sim time this is 2428800.0000, 
sim time next is 2429400.0000, 
raw observation next is [-7.716666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 25.97277509237257, 0.4752719984125589, 0.0, 1.0, 65.0, 57761.39747225201], 
processed observation next is [0.0, 0.08695652173913043, 0.24884579870729456, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6643979243643807, 0.658423999470853, 0.0, 1.0, 1.0, 0.2750542736773905], 
reward next is 0.7249, 
noisyNet noise sample is [array([1.6595898], dtype=float32), -1.7602698]. 
=============================================
[2019-04-08 15:31:19,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2637202e-15 1.6473791e-09 3.5779667e-07 5.8734611e-07 1.3523029e-10
 1.6704299e-12 1.0663428e-07 3.0388430e-13 5.0332042e-16 1.0035514e-18
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:31:19,376] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7854
[2019-04-08 15:31:19,395] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 19.0, 26.7536995921746, 0.7054059487432394, 0.0, 1.0, 65.0, 44264.16257573729], 
current ob forecast is [], 
actual action is [4.6, 65.0], 
sim time this is 2392800.0000, 
sim time next is 2393400.0000, 
raw observation next is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 19.0, 26.74258573401277, 0.7016242467668296, 0.0, 1.0, 65.0, 44533.2305274898], 
processed observation next is [0.0, 0.6956521739130435, 0.44875346260387816, 0.4533333333333333, 0.18222222222222223, 0.04861878453038674, 0.08333333333333333, 0.7285488111677308, 0.7338747489222764, 0.0, 1.0, 1.0, 0.2120630025118562], 
reward next is 0.7879, 
noisyNet noise sample is [array([-1.5607321], dtype=float32), 0.20646213]. 
=============================================
[2019-04-08 15:31:20,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0187174e-16 2.3834732e-09 1.4653337e-07 9.6343433e-07 1.8583188e-10
 9.9721429e-12 2.2383242e-08 7.2375049e-13 6.6424443e-17 1.0721987e-17
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:31:20,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6304
[2019-04-08 15:31:20,732] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 36.0, 81.0, 803.0, 19.0, 26.28967371887173, 0.5834073127187809, 0.0, 1.0, 65.0, 45638.41922753125], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 2458800.0000, 
sim time next is 2459400.0000, 
raw observation next is [-2.016666666666667, 35.16666666666666, 82.66666666666667, 811.6666666666667, 19.0, 26.3359011746416, 0.5923475793623162, 0.0, 1.0, 65.0, 45325.08548479079], 
processed observation next is [0.0, 0.4782608695652174, 0.4067405355493998, 0.35166666666666657, 0.27555555555555555, 0.8968692449355433, 0.08333333333333333, 0.6946584312201333, 0.697449193120772, 0.0, 1.0, 1.0, 0.21583374040376566], 
reward next is 0.7842, 
noisyNet noise sample is [array([0.85879016], dtype=float32), 0.21245219]. 
=============================================
[2019-04-08 15:31:20,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9415397e-15 4.5744139e-10 2.4628164e-06 2.0481980e-06 2.6577800e-09
 5.0962963e-11 6.2291906e-07 7.5539748e-12 3.7397455e-16 1.0711724e-17
 9.9999487e-01], sum to 1.0000
[2019-04-08 15:31:20,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3722
[2019-04-08 15:31:20,798] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.65, 60.5, 0.0, 0.0, 19.0, 25.76795048536813, 0.4225117758318584, 0.0, 1.0, 65.0, 58426.70368203413], 
current ob forecast is [], 
actual action is [-3.6500000000000004, 65.0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 19.0, 25.75037178585938, 0.4185818439509821, 0.0, 1.0, 65.0, 58474.73824342682], 
processed observation next is [0.0, 0.21739130434782608, 0.22068328716528163, 0.6033333333333334, 0.0, 0.0, 0.08333333333333333, 0.6458643154882816, 0.639527281316994, 0.0, 1.0, 1.0, 0.2784511344925087], 
reward next is 0.7215, 
noisyNet noise sample is [array([2.5911899], dtype=float32), 0.36219138]. 
=============================================
[2019-04-08 15:31:21,432] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1435440e-15 3.3048266e-08 1.0054724e-05 2.2622698e-05 2.3186415e-08
 3.6307737e-10 4.6371238e-07 8.6995541e-12 2.4113110e-14 1.1749628e-16
 9.9996686e-01], sum to 1.0000
[2019-04-08 15:31:21,436] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8592
[2019-04-08 15:31:21,450] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.3, 25.0, 27.0, 161.0, 19.0, 27.19027309871014, 0.7534331540680649, 0.0, 1.0, 65.0, 34387.11010524207], 
current ob forecast is [], 
actual action is [8.3, 65.0], 
sim time this is 2480400.0000, 
sim time next is 2481000.0000, 
raw observation next is [2.933333333333334, 25.5, 20.0, 121.6666666666667, 19.0, 27.14880032173949, 0.7431869456376083, 0.0, 1.0, 65.0, 35191.08317240125], 
processed observation next is [0.0, 0.7391304347826086, 0.543859649122807, 0.255, 0.06666666666666667, 0.134438305709024, 0.08333333333333333, 0.7624000268116241, 0.7477289818792028, 0.0, 1.0, 1.0, 0.16757658653524407], 
reward next is 0.8324, 
noisyNet noise sample is [array([-0.11867823], dtype=float32), 0.8950641]. 
=============================================
[2019-04-08 15:31:21,453] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[70.005875]
 [70.16686 ]
 [70.358055]
 [70.516594]
 [70.735214]], R is [[69.96643829]
 [70.10302734]
 [70.24565887]
 [70.39569855]
 [70.53652191]].
[2019-04-08 15:31:21,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.0983896e-15 1.6296414e-07 1.3136715e-05 2.8812958e-04 1.2315552e-08
 7.5065337e-10 5.7831869e-07 2.2794976e-11 2.6700480e-16 2.9414565e-18
 9.9969804e-01], sum to 1.0000
[2019-04-08 15:31:21,685] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6915
[2019-04-08 15:31:21,705] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.666666666666666, 51.33333333333334, 47.0, 499.0, 19.0, 25.72798076704042, 0.4453984454049449, 0.0, 1.0, 65.0, 55903.14136323273], 
current ob forecast is [], 
actual action is [-2.666666666666666, 65.0], 
sim time this is 2451000.0000, 
sim time next is 2451600.0000, 
raw observation next is [-7.3, 50.0, 50.5, 540.5, 19.0, 25.75368494316741, 0.4570280064442676, 0.0, 1.0, 65.0, 54759.12741477853], 
processed observation next is [0.0, 0.391304347826087, 0.26038781163434904, 0.5, 0.16833333333333333, 0.5972375690607735, 0.08333333333333333, 0.6461404119306176, 0.6523426688147559, 0.0, 1.0, 1.0, 0.26075774959418346], 
reward next is 0.7392, 
noisyNet noise sample is [array([1.0978214], dtype=float32), -0.8316967]. 
=============================================
[2019-04-08 15:31:22,052] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.31537517e-16 1.36491429e-09 9.19002730e-07 6.38140991e-05
 3.15312731e-09 2.97125276e-11 1.02091256e-07 5.12502475e-13
 1.31365744e-16 1.58015320e-18 9.99935150e-01], sum to 1.0000
[2019-04-08 15:31:22,057] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0516
[2019-04-08 15:31:22,073] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 39.33333333333333, 0.0, 0.0, 19.0, 26.79898219377176, 0.6000416911910952, 0.0, 1.0, 65.0, 43530.60888899371], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 2508000.0000, 
sim time next is 2508600.0000, 
raw observation next is [-1.7, 39.66666666666667, 0.0, 0.0, 19.0, 26.69487548872178, 0.5901056606651479, 0.0, 1.0, 65.0, 45508.76747946403], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3966666666666667, 0.0, 0.0, 0.08333333333333333, 0.7245729573934817, 0.6967018868883826, 0.0, 1.0, 1.0, 0.21670841656887635], 
reward next is 0.7833, 
noisyNet noise sample is [array([-0.5925512], dtype=float32), 2.8337028]. 
=============================================
[2019-04-08 15:31:22,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.87079972e-18 3.44288903e-10 6.92725450e-07 1.70797910e-06
 3.40339479e-10 1.15921845e-12 1.01943506e-07 5.70721398e-15
 3.22481273e-19 2.75068799e-20 9.99997497e-01], sum to 1.0000
[2019-04-08 15:31:22,613] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4443
[2019-04-08 15:31:22,636] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 19.0, 26.29849816719113, 0.5622334115907569, 0.0, 1.0, 65.0, 53892.58117898127], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 65.0], 
sim time this is 2610000.0000, 
sim time next is 2610600.0000, 
raw observation next is [-6.283333333333333, 82.16666666666667, 0.0, 0.0, 19.0, 26.23430487386918, 0.5552361899370163, 0.0, 1.0, 65.0, 55209.52245928931], 
processed observation next is [1.0, 0.21739130434782608, 0.288550323176362, 0.8216666666666668, 0.0, 0.0, 0.08333333333333333, 0.6861920728224318, 0.6850787299790054, 0.0, 1.0, 1.0, 0.26290248790137766], 
reward next is 0.7371, 
noisyNet noise sample is [array([-0.97418386], dtype=float32), 1.322855]. 
=============================================
[2019-04-08 15:31:23,538] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5732320e-16 7.9178584e-09 1.1194626e-05 8.4022849e-06 9.1460273e-10
 1.8932716e-12 2.7159740e-07 1.1903512e-12 1.4022098e-16 2.3464472e-19
 9.9998009e-01], sum to 1.0000
[2019-04-08 15:31:23,539] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8460
[2019-04-08 15:31:23,570] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 46.5, 0.0, 0.0, 19.0, 26.65197964375933, 0.574307740167569, 0.0, 1.0, 65.0, 42693.45292650088], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 2518200.0000, 
sim time next is 2518800.0000, 
raw observation next is [-1.7, 47.33333333333333, 0.0, 0.0, 19.0, 26.65950595030422, 0.5676266470386719, 0.0, 1.0, 65.0, 43299.42887523824], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.4733333333333333, 0.0, 0.0, 0.08333333333333333, 0.7216254958586849, 0.689208882346224, 0.0, 1.0, 1.0, 0.20618775654875354], 
reward next is 0.7938, 
noisyNet noise sample is [array([-0.9452011], dtype=float32), -0.35932064]. 
=============================================
[2019-04-08 15:31:24,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4825679e-16 4.7805657e-09 1.2651792e-06 7.2118684e-07 1.5858070e-08
 5.4297231e-12 1.2478478e-07 2.2459627e-13 4.3278110e-17 3.1961801e-19
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:31:24,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6854
[2019-04-08 15:31:24,827] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 39.0, 0.0, 0.0, 19.0, 26.74911392236771, 0.602732619903472, 0.0, 1.0, 65.0, 41969.87443782261], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 2507400.0000, 
sim time next is 2508000.0000, 
raw observation next is [-1.7, 39.33333333333333, 0.0, 0.0, 19.0, 26.79898219684292, 0.6000416920955471, 0.0, 1.0, 65.0, 43530.60885260699], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3933333333333333, 0.0, 0.0, 0.08333333333333333, 0.7332485164035768, 0.7000138973651824, 0.0, 1.0, 1.0, 0.2072886135838428], 
reward next is 0.7927, 
noisyNet noise sample is [array([-1.8712667], dtype=float32), 0.061960608]. 
=============================================
[2019-04-08 15:31:24,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.66201 ]
 [75.969666]
 [76.0803  ]
 [75.78672 ]
 [75.50727 ]], R is [[76.91270447]
 [76.94371796]
 [76.95593262]
 [76.96635437]
 [76.97975922]].
[2019-04-08 15:31:25,846] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5125567e-16 2.5526830e-09 2.6225846e-06 1.3690209e-06 5.4348854e-09
 2.8485736e-11 1.2623202e-06 1.0166743e-12 1.9519501e-16 1.1351990e-20
 9.9999475e-01], sum to 1.0000
[2019-04-08 15:31:25,847] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9470
[2019-04-08 15:31:25,866] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 39.66666666666667, 0.0, 0.0, 19.0, 26.56798478466903, 0.5789438109217433, 0.0, 1.0, 65.0, 46483.90171229844], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 2509800.0000, 
sim time next is 2510400.0000, 
raw observation next is [-1.7, 39.33333333333334, 0.0, 0.0, 19.0, 26.54652855586947, 0.5755494063695902, 0.0, 1.0, 65.0, 46529.86101785641], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3933333333333334, 0.0, 0.0, 0.08333333333333333, 0.7122107129891226, 0.6918498021231967, 0.0, 1.0, 1.0, 0.2215707667516972], 
reward next is 0.7784, 
noisyNet noise sample is [array([1.3845193], dtype=float32), -0.81909394]. 
=============================================
[2019-04-08 15:31:26,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0212217e-16 1.9193223e-09 5.2620017e-05 9.5594907e-05 1.7021289e-09
 2.4373475e-11 1.3173125e-06 1.0578270e-14 5.3497287e-17 6.1621195e-20
 9.9985039e-01], sum to 1.0000
[2019-04-08 15:31:26,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0734
[2019-04-08 15:31:26,382] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 48.16666666666667, 0.0, 0.0, 19.0, 26.60742554138442, 0.5666431553588483, 0.0, 1.0, 65.0, 44811.5524954131], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 2519400.0000, 
sim time next is 2520000.0000, 
raw observation next is [-1.7, 49.0, 0.0, 0.0, 19.0, 26.58967436418225, 0.5595841900510767, 0.0, 1.0, 65.0, 45098.07077870008], 
processed observation next is [1.0, 0.17391304347826086, 0.4155124653739613, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7158061970151876, 0.6865280633503589, 0.0, 1.0, 1.0, 0.2147527179938099], 
reward next is 0.7852, 
noisyNet noise sample is [array([0.76371366], dtype=float32), 0.6620501]. 
=============================================
[2019-04-08 15:31:26,404] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.14709 ]
 [85.06483 ]
 [84.98014 ]
 [84.94793 ]
 [84.849075]], R is [[85.1684494 ]
 [85.1033783 ]
 [85.04615784]
 [84.99239349]
 [84.93664551]].
[2019-04-08 15:31:26,963] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4007053e-16 3.8738360e-08 3.3230011e-05 3.2675555e-06 1.5222064e-09
 1.0586090e-13 2.5784294e-07 2.7159145e-13 2.1276074e-16 9.7631231e-21
 9.9996316e-01], sum to 1.0000
[2019-04-08 15:31:26,974] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9951
[2019-04-08 15:31:26,992] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 22.5, 27.99346526856629, 0.9305532509218052, 1.0, 1.0, 65.0, 22624.30253343864], 
current ob forecast is [], 
actual action is [8.466666666666667, 65.0], 
sim time this is 2558400.0000, 
sim time next is 2559000.0000, 
raw observation next is [3.383333333333333, 28.5, 144.0, 300.3333333333333, 22.5, 27.97051151633685, 0.9320388023788974, 1.0, 1.0, 65.0, 23537.06676426405], 
processed observation next is [1.0, 0.6086956521739131, 0.5563250230840259, 0.285, 0.48, 0.3318600368324125, 0.375, 0.8308759596947374, 0.8106796007929659, 1.0, 1.0, 1.0, 0.11208127030601929], 
reward next is 0.8879, 
noisyNet noise sample is [array([0.30633697], dtype=float32), -0.87977666]. 
=============================================
[2019-04-08 15:31:27,012] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.49499 ]
 [87.669334]
 [87.98264 ]
 [88.21052 ]
 [88.43389 ]], R is [[87.32897186]
 [87.34794617]
 [87.3738327 ]
 [87.39624023]
 [87.40484619]].
[2019-04-08 15:31:27,035] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.0305123e-17 7.3695133e-10 2.7530432e-06 4.4649200e-06 3.9567500e-09
 1.0981678e-12 4.7666589e-08 1.7376490e-14 2.2641404e-17 8.9177421e-21
 9.9999273e-01], sum to 1.0000
[2019-04-08 15:31:27,044] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5685
[2019-04-08 15:31:27,064] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.55, 55.5, 0.0, 0.0, 19.0, 26.48563543810945, 0.5338998440766723, 0.0, 1.0, 65.0, 46101.23326316346], 
current ob forecast is [], 
actual action is [2.45, 65.0], 
sim time this is 2529000.0000, 
sim time next is 2529600.0000, 
raw observation next is [-2.633333333333333, 55.0, 0.0, 0.0, 19.0, 26.43986824399951, 0.5592763640929679, 0.0, 1.0, 65.0, 46796.46153731629], 
processed observation next is [1.0, 0.2608695652173913, 0.38965835641735924, 0.55, 0.0, 0.0, 0.08333333333333333, 0.7033223536666258, 0.6864254546976559, 0.0, 1.0, 1.0, 0.22284029303483946], 
reward next is 0.7772, 
noisyNet noise sample is [array([1.7947782], dtype=float32), 0.3490299]. 
=============================================
[2019-04-08 15:31:29,009] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0896361e-16 1.0244583e-09 2.1555248e-08 1.3681490e-06 5.7434463e-10
 5.3015882e-13 3.9574395e-08 2.3520081e-14 5.9881636e-17 1.4275630e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:31:29,013] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7833
[2019-04-08 15:31:29,045] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 19.0, 26.892880702155, 0.7760195321991376, 0.0, 1.0, 65.0, 41077.05357241504], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 2581800.0000, 
sim time next is 2582400.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 19.0, 26.88935137210683, 0.7709532222635564, 0.0, 1.0, 65.0, 40766.57967586724], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.08333333333333333, 0.7407792810089026, 0.7569844074211854, 0.0, 1.0, 1.0, 0.1941265698850821], 
reward next is 0.8059, 
noisyNet noise sample is [array([1.5985061], dtype=float32), 1.0780742]. 
=============================================
[2019-04-08 15:31:29,529] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7220607e-18 1.6927933e-09 1.7925406e-07 1.3173906e-06 2.9623706e-10
 5.6454375e-13 1.2933788e-07 1.2887919e-13 8.3381915e-18 2.7197859e-19
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:31:29,529] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0824
[2019-04-08 15:31:29,546] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.58148525897317, 0.6311224051302493, 0.0, 1.0, 65.0, 48720.90852199648], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2600400.0000, 
sim time next is 2601000.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.54425030718882, 0.6234180241564741, 0.0, 1.0, 65.0, 48986.84607820128], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7120208589324015, 0.7078060080521581, 0.0, 1.0, 1.0, 0.23327069561048228], 
reward next is 0.7667, 
noisyNet noise sample is [array([-0.5525822], dtype=float32), -2.562526]. 
=============================================
[2019-04-08 15:31:29,556] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.90022 ]
 [84.92203 ]
 [84.743706]
 [84.51074 ]
 [84.56257 ]], R is [[85.19520569]
 [85.11125183]
 [85.03390503]
 [84.96492767]
 [84.8899231 ]].
[2019-04-08 15:31:30,180] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9603168e-17 5.0884331e-12 1.5969956e-08 5.4845968e-09 3.9846967e-12
 3.1072692e-13 2.8639708e-09 1.7228233e-15 1.7461094e-19 8.4744088e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:31:30,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2115
[2019-04-08 15:31:30,199] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 49.0, 141.6666666666667, 192.3333333333333, 22.5, 27.82323157262503, 0.9037264610498821, 1.0, 1.0, 65.0, 29499.86531010498], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 2648400.0000, 
sim time next is 2649000.0000, 
raw observation next is [0.5, 49.5, 128.3333333333333, 178.6666666666667, 22.5, 27.77986237122718, 0.8919789105731261, 1.0, 1.0, 65.0, 30535.68491523715], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.495, 0.42777777777777765, 0.19742173112338862, 0.375, 0.8149885309355982, 0.7973263035243754, 1.0, 1.0, 1.0, 0.1454080234058912], 
reward next is 0.8546, 
noisyNet noise sample is [array([1.4836178], dtype=float32), 0.7314741]. 
=============================================
[2019-04-08 15:31:30,216] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.81817]
 [84.76079]
 [84.75811]
 [84.71609]
 [84.75774]], R is [[84.78829956]
 [84.79994202]
 [84.81713104]
 [84.84875488]
 [84.83737946]].
[2019-04-08 15:31:30,747] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5174555e-17 8.4514187e-11 2.0078380e-07 2.0047280e-06 2.8991970e-11
 2.2746098e-13 2.2693071e-08 7.1188300e-15 2.6388031e-18 3.7813599e-22
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:31:30,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5723
[2019-04-08 15:31:30,769] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 26.64525711050389, 0.6485498208458399, 0.0, 1.0, 65.0, 46020.86201068978], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2597400.0000, 
sim time next is 2598000.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 19.0, 26.62090607862378, 0.6485903424120231, 0.0, 1.0, 65.0, 47251.45346696846], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7184088398853149, 0.7161967808040077, 0.0, 1.0, 1.0, 0.22500692127127836], 
reward next is 0.7750, 
noisyNet noise sample is [array([1.012549], dtype=float32), -0.09914035]. 
=============================================
[2019-04-08 15:31:30,792] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.11469 ]
 [84.79896 ]
 [84.65835 ]
 [84.5749  ]
 [84.490685]], R is [[85.02870178]
 [84.95926666]
 [84.88769531]
 [84.81153107]
 [84.7341156 ]].
[2019-04-08 15:31:30,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9441565e-17 7.5701365e-09 1.9229753e-07 3.3093738e-06 5.9647308e-11
 7.8990688e-13 3.0170364e-07 3.8273611e-13 3.9022010e-17 7.5730111e-20
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:31:30,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4864
[2019-04-08 15:31:31,005] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 60.0, 0.0, 0.0, 22.5, 27.03238253921404, 0.8031359016426634, 1.0, 1.0, 65.0, 38423.90896239521], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 2660400.0000, 
sim time next is 2661000.0000, 
raw observation next is [-1.2, 60.5, 0.0, 0.0, 22.5, 27.07661151598702, 0.7976997922629927, 1.0, 1.0, 65.0, 38764.07033179353], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.605, 0.0, 0.0, 0.375, 0.7563842929989185, 0.7658999307543309, 1.0, 1.0, 1.0, 0.18459081110377873], 
reward next is 0.8154, 
noisyNet noise sample is [array([-0.5289982], dtype=float32), 0.899276]. 
=============================================
[2019-04-08 15:31:31,010] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.26488 ]
 [83.09052 ]
 [82.97272 ]
 [82.629944]
 [82.475784]], R is [[83.49884033]
 [83.48088837]
 [83.47383881]
 [83.45518494]
 [83.38552094]].
[2019-04-08 15:31:31,200] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.69260337e-17 1.05123736e-10 3.43247592e-07 1.01113972e-06
 1.20438326e-10 3.31857317e-13 8.88939127e-08 8.76360684e-14
 1.51500975e-17 2.31997488e-20 9.99998569e-01], sum to 1.0000
[2019-04-08 15:31:31,203] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1222
[2019-04-08 15:31:31,225] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1333333333333334, 51.33333333333334, 18.33333333333333, 89.0, 22.5, 27.84355631497654, 0.858391239716307, 1.0, 1.0, 64.99999999999997, 31058.08157027136], 
current ob forecast is [], 
actual action is [5.133333333333334, 65.0], 
sim time this is 2654400.0000, 
sim time next is 2655000.0000, 
raw observation next is [-0.04999999999999999, 52.0, 7.0, 82.0, 22.5, 27.57540627939987, 0.8645018923269516, 1.0, 1.0, 65.0, 39265.51631833497], 
processed observation next is [1.0, 0.7391304347826086, 0.461218836565097, 0.52, 0.023333333333333334, 0.09060773480662983, 0.375, 0.7979505232833226, 0.7881672974423172, 1.0, 1.0, 1.0, 0.18697864913492843], 
reward next is 0.8130, 
noisyNet noise sample is [array([0.04034689], dtype=float32), 0.6013596]. 
=============================================
[2019-04-08 15:31:31,230] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.241806]
 [81.20317 ]
 [81.36535 ]
 [81.571945]
 [81.56395 ]], R is [[81.12498474]
 [81.16584015]
 [81.2339325 ]
 [81.26035309]
 [81.15582275]].
[2019-04-08 15:31:31,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.23656404e-17 1.85071986e-10 2.05074775e-06 1.53599024e-06
 5.48956547e-10 1.12870683e-12 6.21901677e-08 1.05066153e-14
 2.35172990e-17 4.45588593e-20 9.99996305e-01], sum to 1.0000
[2019-04-08 15:31:31,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0262
[2019-04-08 15:31:31,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3450464e-17 3.1561684e-11 1.8419911e-08 8.2966203e-08 3.9882188e-11
 9.6154469e-14 6.3273992e-10 8.5296057e-15 6.8732484e-19 2.8309594e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:31:31,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0039
[2019-04-08 15:31:31,266] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.04999999999999999, 45.0, 164.0, 211.0, 22.5, 27.52760215678605, 0.8525046732663975, 1.0, 1.0, 65.0, 33248.55427895375], 
current ob forecast is [], 
actual action is [4.95, 65.0], 
sim time this is 2640600.0000, 
sim time next is 2641200.0000, 
raw observation next is [0.1333333333333333, 44.33333333333333, 172.6666666666667, 197.5, 22.5, 27.59613586486652, 0.7174993869972554, 1.0, 1.0, 65.0, 46081.47264415765], 
processed observation next is [1.0, 0.5652173913043478, 0.46629732225300097, 0.4433333333333333, 0.5755555555555557, 0.21823204419889503, 0.375, 0.7996779887388765, 0.7391664623324185, 1.0, 1.0, 1.0, 0.21943558401979832], 
reward next is 0.7806, 
noisyNet noise sample is [array([-0.6260192], dtype=float32), -0.66172004]. 
=============================================
[2019-04-08 15:31:31,277] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 22.5, 26.56524543070069, 0.6065505124947593, 1.0, 1.0, 65.0, 45838.40141499213], 
current ob forecast is [], 
actual action is [-1.9000000000000004, 65.0], 
sim time this is 2623200.0000, 
sim time next is 2623800.0000, 
raw observation next is [-6.800000000000001, 75.66666666666666, 82.33333333333333, 30.33333333333333, 22.5, 26.7214070130546, 0.6159059462245379, 1.0, 1.0, 65.0, 44241.57744463428], 
processed observation next is [1.0, 0.34782608695652173, 0.2742382271468144, 0.7566666666666666, 0.27444444444444444, 0.03351749539594843, 0.375, 0.7267839177545499, 0.705301982074846, 1.0, 1.0, 1.0, 0.21067417830778226], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.8266735], dtype=float32), 0.99586046]. 
=============================================
[2019-04-08 15:31:32,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.97622592e-18 7.97054422e-10 3.35574384e-07 1.71537624e-06
 7.73781733e-11 2.84158634e-12 7.12758963e-09 5.24243387e-15
 8.16080345e-18 1.08936754e-20 9.99997973e-01], sum to 1.0000
[2019-04-08 15:31:32,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1575
[2019-04-08 15:31:32,027] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.45, 80.5, 0.0, 0.0, 19.0, 26.14469448090149, 0.5441990140594598, 0.0, 1.0, 65.0, 56433.13592758618], 
current ob forecast is [], 
actual action is [-1.4500000000000002, 65.0], 
sim time this is 2611800.0000, 
sim time next is 2612400.0000, 
raw observation next is [-6.533333333333333, 79.66666666666667, 0.0, 0.0, 19.0, 26.1163779932815, 0.5408598860389263, 0.0, 1.0, 65.0, 56835.28444110365], 
processed observation next is [1.0, 0.21739130434782608, 0.2816251154201293, 0.7966666666666667, 0.0, 0.0, 0.08333333333333333, 0.6763648327734583, 0.6802866286796422, 0.0, 1.0, 1.0, 0.2706442116243031], 
reward next is 0.7294, 
noisyNet noise sample is [array([0.5223055], dtype=float32), 1.1065156]. 
=============================================
[2019-04-08 15:31:32,061] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1100647e-16 3.5372011e-08 1.4336332e-06 2.2082065e-06 6.3943073e-10
 1.7466448e-12 4.2675069e-07 3.7022126e-12 5.0483197e-17 1.3962411e-19
 9.9999595e-01], sum to 1.0000
[2019-04-08 15:31:32,064] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3613
[2019-04-08 15:31:32,079] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.916666666666667, 67.0, 0.0, 0.0, 19.0, 26.55369572644113, 0.654505471889764, 0.0, 1.0, 65.0, 47479.78985527465], 
current ob forecast is [], 
actual action is [0.08333333333333304, 65.0], 
sim time this is 2595000.0000, 
sim time next is 2595600.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 19.0, 26.57733360433694, 0.6528972409016212, 0.0, 1.0, 65.0, 48152.64059311862], 
processed observation next is [1.0, 0.043478260869565216, 0.32409972299168976, 0.68, 0.0, 0.0, 0.08333333333333333, 0.7147778003614116, 0.7176324136338738, 0.0, 1.0, 1.0, 0.22929828853866008], 
reward next is 0.7707, 
noisyNet noise sample is [array([-0.79806197], dtype=float32), -0.7207856]. 
=============================================
[2019-04-08 15:31:32,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.38297695e-17 7.92735669e-11 6.30220498e-08 4.97354733e-07
 6.17728591e-10 6.51763528e-14 4.69139394e-08 1.36273524e-14
 1.22175586e-17 7.16362897e-22 9.99999404e-01], sum to 1.0000
[2019-04-08 15:31:32,492] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3720
[2019-04-08 15:31:32,504] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.85, 70.0, 93.0, 91.0, 22.5, 26.8966319667605, 0.6550490110914361, 1.0, 1.0, 65.0, 42756.19819126149], 
current ob forecast is [], 
actual action is [-0.8499999999999996, 65.0], 
sim time this is 2626200.0000, 
sim time next is 2626800.0000, 
raw observation next is [-5.566666666666666, 68.33333333333333, 102.8333333333333, 121.6666666666667, 22.5, 26.96894327395134, 0.6652506538411532, 1.0, 1.0, 65.0, 41672.34043538874], 
processed observation next is [1.0, 0.391304347826087, 0.3084025854108957, 0.6833333333333332, 0.3427777777777777, 0.134438305709024, 0.375, 0.747411939495945, 0.7217502179470511, 1.0, 1.0, 1.0, 0.198439716358994], 
reward next is 0.8016, 
noisyNet noise sample is [array([1.2744772], dtype=float32), -1.0482004]. 
=============================================
[2019-04-08 15:31:33,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3696789e-17 6.8848114e-09 9.8879923e-07 1.7421341e-06 4.7571574e-10
 3.1132007e-12 2.5011170e-08 1.4412512e-12 2.9092240e-17 2.6148317e-19
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:31:33,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3789
[2019-04-08 15:31:33,457] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2989967e-16 1.7262181e-10 1.9462227e-07 1.1183115e-07 1.8561169e-10
 8.3187675e-12 1.9877926e-07 3.6043426e-13 7.1795935e-18 6.4970162e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:31:33,460] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.92416777486364, 0.4570685222762137, 0.0, 1.0, 65.0, 80479.93316437672], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 2789400.0000, 
sim time next is 2790000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 25.83877560428832, 0.4457151630825719, 0.0, 1.0, 65.0, 82049.157296185], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6532313003573599, 0.648571721027524, 0.0, 1.0, 1.0, 0.3907102728389762], 
reward next is 0.6093, 
noisyNet noise sample is [array([-0.628097], dtype=float32), -0.38479415]. 
=============================================
[2019-04-08 15:31:33,462] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4682
[2019-04-08 15:31:33,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.2966  ]
 [80.206406]
 [80.104805]
 [80.071045]
 [80.018234]], R is [[80.21354675]
 [80.02817535]
 [79.85132599]
 [79.70085907]
 [79.60983276]].
[2019-04-08 15:31:33,483] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 44.33333333333334, 207.3333333333333, 143.5, 22.5, 27.71319890997337, 0.8627987278666692, 1.0, 1.0, 65.0, 28488.33167451968], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 2643600.0000, 
sim time next is 2644200.0000, 
raw observation next is [0.5, 45.0, 216.0, 130.0, 22.5, 27.69028952821762, 0.8885685817025104, 1.0, 1.0, 65.0, 31174.81069059482], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.45, 0.72, 0.143646408839779, 0.375, 0.8075241273514683, 0.7961895272341701, 1.0, 1.0, 1.0, 0.14845147947902296], 
reward next is 0.8515, 
noisyNet noise sample is [array([0.9967739], dtype=float32), 0.3985852]. 
=============================================
[2019-04-08 15:31:33,973] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7104753e-16 3.2341710e-08 4.1717635e-06 5.4833476e-05 5.3219279e-10
 4.9331481e-12 2.9147497e-08 2.4462333e-13 2.1572002e-17 5.3544952e-19
 9.9994087e-01], sum to 1.0000
[2019-04-08 15:31:33,976] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2241
[2019-04-08 15:31:33,997] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 50.0, 88.33333333333333, 137.6666666666667, 22.5, 27.79820403760515, 0.9075842513683469, 1.0, 1.0, 65.0, 29594.92580247857], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 2650800.0000, 
sim time next is 2651400.0000, 
raw observation next is [0.5, 50.0, 75.0, 124.0, 22.5, 27.83654135796466, 0.9100157142844775, 1.0, 1.0, 65.0, 29376.80347992996], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.25, 0.13701657458563535, 0.375, 0.8197117798303882, 0.8033385714281591, 1.0, 1.0, 1.0, 0.13988954038061885], 
reward next is 0.8601, 
noisyNet noise sample is [array([-1.0754948], dtype=float32), -0.89703345]. 
=============================================
[2019-04-08 15:31:34,295] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.34998994e-17 1.27494029e-10 6.80662069e-08 1.29406416e-07
 1.55243765e-12 4.38773854e-13 2.92162694e-09 4.10324147e-15
 2.17045906e-18 2.21937688e-21 9.99999762e-01], sum to 1.0000
[2019-04-08 15:31:34,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5272
[2019-04-08 15:31:34,316] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 66.0, 115.3333333333333, 709.6666666666666, 22.5, 26.96578695223399, 0.7246883818644299, 1.0, 1.0, 65.0, 33395.20260697754], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 2717400.0000, 
sim time next is 2718000.0000, 
raw observation next is [-9.0, 64.0, 114.5, 727.5, 22.5, 27.02873876785605, 0.7340966785529908, 1.0, 1.0, 65.0, 33429.20791517819], 
processed observation next is [1.0, 0.4782608695652174, 0.21329639889196678, 0.64, 0.38166666666666665, 0.8038674033149171, 0.375, 0.7523948973213376, 0.7446988928509969, 1.0, 1.0, 1.0, 0.15918670435799137], 
reward next is 0.8408, 
noisyNet noise sample is [array([0.19354852], dtype=float32), -0.33385]. 
=============================================
[2019-04-08 15:31:34,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.93724]
 [79.15618]
 [79.83152]
 [80.53545]
 [80.99105]], R is [[78.79949951]
 [78.85247803]
 [78.89842224]
 [78.93941498]
 [78.9779129 ]].
[2019-04-08 15:31:34,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0240474e-16 6.6776773e-10 1.6921946e-07 7.1354516e-06 1.6403517e-09
 8.2375703e-12 9.1803024e-08 3.8728491e-12 3.6520747e-16 2.0571620e-18
 9.9999261e-01], sum to 1.0000
[2019-04-08 15:31:34,359] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4485
[2019-04-08 15:31:34,393] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.4166666666666667, 53.33333333333333, 0.0, 0.0, 22.5, 27.43282649560755, 0.8727143244412655, 1.0, 1.0, 65.0, 31899.49064188681], 
current ob forecast is [], 
actual action is [4.583333333333333, 65.0], 
sim time this is 2656200.0000, 
sim time next is 2656800.0000, 
raw observation next is [-0.6, 54.0, 0.0, 0.0, 22.5, 27.47919826032551, 0.8768072711647128, 1.0, 1.0, 65.0, 31560.9557454382], 
processed observation next is [1.0, 0.782608695652174, 0.44598337950138506, 0.54, 0.0, 0.0, 0.375, 0.7899331883604592, 0.7922690903882376, 1.0, 1.0, 1.0, 0.15029026545446764], 
reward next is 0.8497, 
noisyNet noise sample is [array([-0.72279906], dtype=float32), 0.55238795]. 
=============================================
[2019-04-08 15:31:34,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2128133e-16 5.3222073e-11 3.3101585e-08 4.0924069e-06 4.1642048e-10
 1.5509420e-13 3.0271092e-08 3.2467079e-13 4.3861707e-18 7.7524756e-21
 9.9999583e-01], sum to 1.0000
[2019-04-08 15:31:34,797] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6723
[2019-04-08 15:31:34,826] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.66666666666667, 81.0, 100.3333333333333, 622.3333333333333, 22.5, 26.54111612973445, 0.6136060933413056, 1.0, 1.0, 65.0, 39949.5908803948], 
current ob forecast is [], 
actual action is [-7.66666666666667, 65.0], 
sim time this is 2713200.0000, 
sim time next is 2713800.0000, 
raw observation next is [-12.33333333333333, 78.5, 103.6666666666667, 632.6666666666667, 22.5, 26.63981655347368, 0.6386149996162621, 1.0, 1.0, 65.0, 37896.14007162159], 
processed observation next is [1.0, 0.391304347826087, 0.12096029547553101, 0.785, 0.34555555555555567, 0.6990791896869246, 0.375, 0.7199847127894733, 0.7128716665387541, 1.0, 1.0, 1.0, 0.1804578098648647], 
reward next is 0.8195, 
noisyNet noise sample is [array([-0.947847], dtype=float32), -0.5394487]. 
=============================================
[2019-04-08 15:31:34,937] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.96796884e-16 1.19967245e-08 3.98579596e-06 1.13627777e-06
 1.09353937e-09 5.22503568e-11 6.21188235e-07 7.40168357e-14
 4.43006917e-18 4.95463041e-19 9.99994278e-01], sum to 1.0000
[2019-04-08 15:31:34,943] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4605
[2019-04-08 15:31:34,956] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 72.0, 113.6666666666667, 663.6666666666667, 22.5, 26.88210104650047, 0.6804627823626914, 1.0, 1.0, 65.0, 36143.62439956526], 
current ob forecast is [], 
actual action is [-6.0, 65.0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [-10.5, 70.0, 117.0, 674.0, 22.5, 26.87968992468319, 0.6975343276787328, 1.0, 1.0, 65.0, 35704.96762522958], 
processed observation next is [1.0, 0.43478260869565216, 0.17174515235457063, 0.7, 0.39, 0.7447513812154696, 0.375, 0.739974160390266, 0.7325114425595776, 1.0, 1.0, 1.0, 0.1700236553582361], 
reward next is 0.8300, 
noisyNet noise sample is [array([-1.9223969], dtype=float32), 0.83796525]. 
=============================================
[2019-04-08 15:31:35,102] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0254122e-16 1.2331545e-09 2.2117099e-05 5.8130290e-06 6.7084494e-09
 6.3378326e-12 1.2496186e-07 2.4065244e-12 3.1049854e-16 1.8033014e-19
 9.9997199e-01], sum to 1.0000
[2019-04-08 15:31:35,105] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1681
[2019-04-08 15:31:35,128] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.466666666666666, 67.66666666666667, 0.0, 0.0, 19.0, 26.74087492467904, 0.7424832175335232, 0.0, 1.0, 65.0, 45534.87753159267], 
current ob forecast is [], 
actual action is [2.533333333333334, 65.0], 
sim time this is 2670000.0000, 
sim time next is 2670600.0000, 
raw observation next is [-2.783333333333333, 68.33333333333333, 0.0, 0.0, 19.0, 26.73174961991507, 0.7439041254271888, 0.0, 1.0, 65.0, 45363.22735123736], 
processed observation next is [1.0, 0.9130434782608695, 0.3855032317636196, 0.6833333333333332, 0.0, 0.0, 0.08333333333333333, 0.7276458016595893, 0.747968041809063, 0.0, 1.0, 1.0, 0.21601536833922552], 
reward next is 0.7840, 
noisyNet noise sample is [array([0.37897992], dtype=float32), 1.5588561]. 
=============================================
[2019-04-08 15:31:35,517] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0932073e-16 2.0381095e-10 4.3443262e-07 5.9894751e-06 8.0403722e-12
 2.5515514e-12 3.6385956e-09 2.2888795e-13 1.4694280e-16 1.9027390e-19
 9.9999356e-01], sum to 1.0000
[2019-04-08 15:31:35,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0491
[2019-04-08 15:31:35,551] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 22.5, 26.96249234952789, 0.4498327770491106, 1.0, 1.0, 65.0, 33156.54947469638], 
current ob forecast is [], 
actual action is [-2.666666666666667, 65.0], 
sim time this is 2722200.0000, 
sim time next is 2722800.0000, 
raw observation next is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 22.5, 27.02569984553016, 0.7936998938260832, 1.0, 1.0, 65.0, 47627.85487545822], 
processed observation next is [1.0, 0.5217391304347826, 0.2594644506001847, 0.6233333333333334, 0.376111111111111, 0.8795580110497238, 0.375, 0.75214165379418, 0.7645666312753611, 1.0, 1.0, 1.0, 0.22679930893075345], 
reward next is 0.7732, 
noisyNet noise sample is [array([1.034449], dtype=float32), -0.9601957]. 
=============================================
[2019-04-08 15:31:36,421] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0096036e-14 1.9262076e-09 7.0943111e-06 3.8105540e-05 4.9009946e-10
 6.0041909e-11 5.4883174e-07 8.3471538e-13 8.6797695e-16 1.0859892e-18
 9.9995422e-01], sum to 1.0000
[2019-04-08 15:31:36,421] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0589
[2019-04-08 15:31:36,449] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 111.0, 793.5, 22.5, 27.25395177328754, 0.4997610318386314, 1.0, 1.0, 65.0, 33248.21592122682], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2725200.0000, 
sim time next is 2725800.0000, 
raw observation next is [-5.8, 58.5, 110.3333333333333, 791.6666666666666, 22.5, 27.04483462323716, 0.8146048578331603, 1.0, 1.0, 65.0, 49637.28586299653], 
processed observation next is [1.0, 0.5652173913043478, 0.30193905817174516, 0.585, 0.36777777777777765, 0.8747697974217311, 0.375, 0.7537362186030968, 0.7715349526110534, 1.0, 1.0, 1.0, 0.23636802791903108], 
reward next is 0.7636, 
noisyNet noise sample is [array([0.4596498], dtype=float32), 0.28133968]. 
=============================================
[2019-04-08 15:31:36,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5646703e-15 9.2423607e-09 1.5503181e-05 1.7759547e-06 6.5536493e-10
 1.9812033e-10 4.9641454e-07 4.9419418e-12 5.7376191e-16 3.8849894e-18
 9.9998224e-01], sum to 1.0000
[2019-04-08 15:31:36,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0458
[2019-04-08 15:31:36,993] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 52.0, 3.0, 53.0, 22.5, 26.47338611915291, 0.7707425120482244, 1.0, 1.0, 65.0, 88627.28684533929], 
current ob forecast is [], 
actual action is [1.5, 65.0], 
sim time this is 2741400.0000, 
sim time next is 2742000.0000, 
raw observation next is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 22.5, 25.74045033655809, 0.7068019764904712, 1.0, 1.0, 65.0, 66304.08705510723], 
processed observation next is [1.0, 0.7391304347826086, 0.3610341643582641, 0.5266666666666667, 0.0, 0.0, 0.375, 0.6450375280465076, 0.735600658830157, 1.0, 1.0, 1.0, 0.315733747881463], 
reward next is 0.6843, 
noisyNet noise sample is [array([1.0102623], dtype=float32), 0.08430468]. 
=============================================
[2019-04-08 15:31:37,002] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.022644]
 [73.14168 ]
 [73.209145]
 [73.25594 ]
 [73.25238 ]], R is [[72.8625946 ]
 [72.71193695]
 [72.66883087]
 [72.70375061]
 [72.72877502]].
[2019-04-08 15:31:37,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.29497754e-15 1.17723555e-08 8.15908152e-06 1.82313561e-05
 2.56899879e-09 2.15107914e-11 1.02660431e-06 1.63690968e-12
 8.16412283e-16 1.09912513e-18 9.99972582e-01], sum to 1.0000
[2019-04-08 15:31:37,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3205
[2019-04-08 15:31:37,039] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3368253e-15 1.8365933e-09 1.3761688e-06 1.4700148e-05 6.9067729e-10
 1.2276810e-11 6.3717017e-08 2.2744309e-13 1.1374171e-16 6.2965211e-19
 9.9998391e-01], sum to 1.0000
[2019-04-08 15:31:37,040] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0212
[2019-04-08 15:31:37,060] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.16666666666667, 83.0, 0.0, 0.0, 19.0, 25.57290302439312, 0.4144153621364373, 0.0, 1.0, 65.0, 57685.99419077703], 
current ob forecast is [], 
actual action is [-10.16666666666667, 65.0], 
sim time this is 2703000.0000, 
sim time next is 2703600.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 19.0, 25.52306729971575, 0.4060410934960072, 0.0, 1.0, 65.0, 58520.27052919503], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6269222749763124, 0.6353470311653358, 0.0, 1.0, 1.0, 0.2786679549009287], 
reward next is 0.7213, 
noisyNet noise sample is [array([-1.3506204], dtype=float32), -0.48597202]. 
=============================================
[2019-04-08 15:31:37,070] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.33887863028738, 0.627012633337095, 0.0, 1.0, 65.0, 57927.3913615548], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2760000.0000, 
sim time next is 2760600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.30907092239203, 0.621847701756823, 0.0, 1.0, 65.0, 57313.76647455378], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6924225768660026, 0.7072825672522743, 0.0, 1.0, 1.0, 0.2729226974978751], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.01633809], dtype=float32), -0.874983]. 
=============================================
[2019-04-08 15:31:37,602] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5380825e-17 3.2626870e-09 2.2831620e-07 2.1032561e-06 3.5249254e-10
 5.6451209e-13 3.8474437e-09 3.6819059e-14 3.2587372e-17 2.3711432e-19
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:31:37,604] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8725
[2019-04-08 15:31:37,635] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 54.0, 454.0, 22.5, 27.73840725005088, 0.6174471744392528, 1.0, 1.0, 65.0, 55513.27833264511], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2737800.0000, 
sim time next is 2738400.0000, 
raw observation next is [-3.0, 50.0, 45.5, 387.1666666666667, 22.5, 27.53339324278032, 0.9052364574051248, 1.0, 1.0, 65.0, 71742.04349388798], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.15166666666666667, 0.4278084714548803, 0.375, 0.79444943689836, 0.8017454858017082, 1.0, 1.0, 1.0, 0.3416287785423237], 
reward next is 0.6584, 
noisyNet noise sample is [array([-1.1272676], dtype=float32), 0.523477]. 
=============================================
[2019-04-08 15:31:37,806] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2439438e-16 6.4773159e-10 4.0785981e-06 3.1559221e-06 3.0008560e-09
 3.1975961e-12 3.7266936e-08 1.3204777e-13 1.3032950e-17 5.0604066e-19
 9.9999273e-01], sum to 1.0000
[2019-04-08 15:31:37,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7700
[2019-04-08 15:31:37,848] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 60.66666666666666, 0.0, 0.0, 19.0, 26.49735977926268, 0.6876528234544349, 0.0, 1.0, 65.0, 55108.41262660344], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2756400.0000, 
sim time next is 2757000.0000, 
raw observation next is [-6.0, 59.83333333333334, 0.0, 0.0, 19.0, 26.48783752839564, 0.6864555231363411, 0.0, 1.0, 65.0, 54469.20253536205], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.5983333333333334, 0.0, 0.0, 0.08333333333333333, 0.7073197940329701, 0.7288185077121137, 0.0, 1.0, 1.0, 0.25937715493029545], 
reward next is 0.7406, 
noisyNet noise sample is [array([1.2273844], dtype=float32), -0.47293463]. 
=============================================
[2019-04-08 15:31:37,856] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.69956]
 [74.308  ]
 [74.25968]
 [73.80006]
 [74.1519 ]], R is [[74.76421356]
 [74.75415039]
 [74.74082947]
 [74.72383881]
 [74.70339966]].
[2019-04-08 15:31:37,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3821146e-16 1.7097877e-10 2.2231221e-07 4.2593874e-06 3.0441624e-10
 1.3835815e-12 1.8108093e-08 9.5816272e-14 3.5677686e-17 3.4413068e-21
 9.9999547e-01], sum to 1.0000
[2019-04-08 15:31:37,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8105
[2019-04-08 15:31:37,998] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 87.16666666666667, 0.0, 0.0, 19.0, 26.63140858107134, 0.6771357134212539, 0.0, 1.0, 65.0, 59047.61306565016], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2862600.0000, 
sim time next is 2863200.0000, 
raw observation next is [1.0, 88.33333333333334, 0.0, 0.0, 19.0, 26.70114806327636, 0.6713404471698724, 0.0, 1.0, 65.0, 56588.97440581898], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.8833333333333334, 0.0, 0.0, 0.08333333333333333, 0.7250956719396967, 0.7237801490566241, 0.0, 1.0, 1.0, 0.26947130669437613], 
reward next is 0.7305, 
noisyNet noise sample is [array([0.16446765], dtype=float32), -1.966375]. 
=============================================
[2019-04-08 15:31:38,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3228808e-14 2.4393788e-08 9.1043381e-05 1.3746793e-05 7.3295561e-09
 1.1375744e-10 9.7961401e-07 1.1663372e-12 3.6912684e-15 2.9014548e-17
 9.9989414e-01], sum to 1.0000
[2019-04-08 15:31:38,526] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8656
[2019-04-08 15:31:38,553] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.86155037932424, 0.7495205707284581, 1.0, 1.0, 65.0, 62410.89262149382], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2746800.0000, 
sim time next is 2747400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.83585620173982, 0.7393527938825212, 1.0, 1.0, 65.0, 63532.42259640624], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.7363213501449849, 0.7464509312941737, 1.0, 1.0, 1.0, 0.3025353456971726], 
reward next is 0.6975, 
noisyNet noise sample is [array([0.48969764], dtype=float32), -1.291772]. 
=============================================
[2019-04-08 15:31:38,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3103930e-15 1.2658758e-09 1.5757792e-05 8.4107356e-05 3.1820250e-08
 1.1274250e-10 1.1116054e-05 1.4588955e-11 2.9928020e-15 7.1471317e-18
 9.9988902e-01], sum to 1.0000
[2019-04-08 15:31:38,993] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6130
[2019-04-08 15:31:39,016] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 26.15720042375098, 0.5249515948978313, 0.0, 1.0, 65.0, 52922.87305056718], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 2785800.0000, 
sim time next is 2786400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 19.0, 26.19406623328904, 0.5265259699741215, 0.0, 1.0, 65.0, 51150.13984367942], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.08333333333333333, 0.6828388527740866, 0.6755086566580405, 0.0, 1.0, 1.0, 0.2435720944937115], 
reward next is 0.7564, 
noisyNet noise sample is [array([-0.8750413], dtype=float32), 1.5601153]. 
=============================================
[2019-04-08 15:31:39,123] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3339316e-14 1.2222241e-08 6.6846310e-06 1.7424196e-05 2.9262130e-09
 8.8740279e-11 2.8702800e-07 1.7944014e-11 8.3270448e-16 1.5574044e-18
 9.9997556e-01], sum to 1.0000
[2019-04-08 15:31:39,123] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5386
[2019-04-08 15:31:39,150] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 70.0, 534.0, 22.5, 27.65827606188255, 0.8751052805524869, 1.0, 1.0, 65.00000000000004, 40567.87074364316], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2736000.0000, 
sim time next is 2736600.0000, 
raw observation next is [-3.0, 50.0, 64.66666666666667, 507.3333333333333, 22.5, 27.63207943835915, 0.8882760992574769, 1.0, 1.0, 65.0, 49137.80304063595], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.5, 0.21555555555555558, 0.5605893186003683, 0.375, 0.8026732865299291, 0.7960920330858255, 1.0, 1.0, 1.0, 0.2339895382887426], 
reward next is 0.7660, 
noisyNet noise sample is [array([-0.673145], dtype=float32), 0.87525946]. 
=============================================
[2019-04-08 15:31:39,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.18250604e-14 1.84973992e-08 1.62445258e-06 1.20440214e-04
 1.87886107e-09 6.16853721e-11 2.11514546e-07 1.83679135e-12
 3.06616828e-17 4.68105727e-18 9.99877691e-01], sum to 1.0000
[2019-04-08 15:31:39,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4671
[2019-04-08 15:31:39,282] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.21313400850597, 0.5920789131230036, 0.0, 1.0, 65.0, 54539.32135812703], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2766600.0000, 
sim time next is 2767200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.23989472845901, 0.6016120014266785, 0.0, 1.0, 65.0, 53366.75234862443], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6866578940382508, 0.7005373338088928, 0.0, 1.0, 1.0, 0.2541273921363068], 
reward next is 0.7459, 
noisyNet noise sample is [array([-1.1650451], dtype=float32), -0.029557412]. 
=============================================
[2019-04-08 15:31:39,450] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7463936e-15 2.6872639e-08 8.9859153e-05 2.5063480e-05 2.0202483e-08
 2.2193556e-11 2.9124224e-06 9.6874765e-12 1.0271530e-15 9.6827918e-18
 9.9988210e-01], sum to 1.0000
[2019-04-08 15:31:39,450] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-08 15:31:39,472] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.22973797343636, 0.5529157131867802, 0.0, 1.0, 65.0, 51358.49503952555], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2777400.0000, 
sim time next is 2778000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.23879338540359, 0.5527266078541366, 0.0, 1.0, 65.0, 50894.23141141662], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6865661154502991, 0.6842422026180456, 0.0, 1.0, 1.0, 0.2423534829115077], 
reward next is 0.7576, 
noisyNet noise sample is [array([0.97816855], dtype=float32), 0.4641334]. 
=============================================
[2019-04-08 15:31:39,497] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.891914]
 [74.94632 ]
 [74.94549 ]
 [74.91762 ]
 [75.02304 ]], R is [[74.89014435]
 [74.89667511]
 [74.90265656]
 [74.9076767 ]
 [74.91455078]].
[2019-04-08 15:31:39,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0525061e-15 1.7719799e-08 2.5500333e-06 8.5150357e-05 3.1200378e-09
 7.2535884e-12 1.8719791e-07 3.5409048e-13 4.6134295e-17 4.7982462e-19
 9.9991214e-01], sum to 1.0000
[2019-04-08 15:31:39,882] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3095
[2019-04-08 15:31:39,909] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 22.5, 27.42841970099513, 0.8006006274969266, 1.0, 1.0, 65.0, 34061.1500025658], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 2803200.0000, 
sim time next is 2803800.0000, 
raw observation next is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 22.5, 27.44560570218326, 0.8174579271865388, 1.0, 1.0, 65.0, 32687.91078019704], 
processed observation next is [1.0, 0.43478260869565216, 0.42566943674976926, 0.5083333333333334, 0.5255555555555557, 0.6552486187845303, 0.375, 0.7871338085152718, 0.7724859757288463, 1.0, 1.0, 1.0, 0.15565671800093828], 
reward next is 0.8443, 
noisyNet noise sample is [array([-0.80386037], dtype=float32), 0.0059433277]. 
=============================================
[2019-04-08 15:31:40,044] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.9745685e-15 3.4701628e-08 3.2811352e-06 1.3694841e-04 6.7187029e-09
 2.2148387e-11 2.6463711e-06 1.4513462e-12 2.6994597e-15 1.9229181e-17
 9.9985707e-01], sum to 1.0000
[2019-04-08 15:31:40,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0808
[2019-04-08 15:31:40,066] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.2107707579089, 0.5907476797412631, 0.0, 1.0, 65.0, 55044.4740103373], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2766000.0000, 
sim time next is 2766600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.21313602728701, 0.5920808262888797, 0.0, 1.0, 65.0, 54539.27163774792], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6844280022739175, 0.6973602754296265, 0.0, 1.0, 1.0, 0.25971081732260914], 
reward next is 0.7403, 
noisyNet noise sample is [array([-0.26359794], dtype=float32), -0.6258519]. 
=============================================
[2019-04-08 15:31:41,160] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4287695e-15 7.1824203e-08 2.6636312e-06 6.4977676e-06 1.9165050e-09
 1.3428082e-10 1.3306939e-06 1.7264062e-12 1.3261814e-15 7.5773309e-18
 9.9998939e-01], sum to 1.0000
[2019-04-08 15:31:41,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0320
[2019-04-08 15:31:41,190] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 64.0, 90.0, 172.5, 22.5, 26.23233593648722, 0.5682828332650857, 1.0, 1.0, 65.0, 67925.85718118734], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2794800.0000, 
sim time next is 2795400.0000, 
raw observation next is [-6.0, 64.0, 108.0, 207.0, 22.5, 26.44396890239119, 0.584207489443065, 1.0, 1.0, 65.0, 59686.25459924743], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.36, 0.2287292817679558, 0.375, 0.7036640751992659, 0.694735829814355, 1.0, 1.0, 1.0, 0.28422025999641637], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.5033535], dtype=float32), 0.67991585]. 
=============================================
[2019-04-08 15:31:41,305] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.0609995e-15 2.0040944e-09 1.4144681e-06 1.6192757e-06 3.2782591e-10
 3.5873755e-11 3.1261120e-08 7.9382686e-13 5.4482843e-17 4.9268111e-17
 9.9999690e-01], sum to 1.0000
[2019-04-08 15:31:41,306] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4518
[2019-04-08 15:31:41,324] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 63.16666666666667, 0.0, 0.0, 19.0, 26.53431987005189, 0.6971479719962453, 0.0, 1.0, 65.0, 57371.53759488933], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2754600.0000, 
sim time next is 2755200.0000, 
raw observation next is [-6.0, 62.33333333333334, 0.0, 0.0, 19.0, 26.5198461442682, 0.6939418030000827, 0.0, 1.0, 65.0, 56613.01253570347], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6233333333333334, 0.0, 0.0, 0.08333333333333333, 0.7099871786890167, 0.7313139343333609, 0.0, 1.0, 1.0, 0.2695857739795403], 
reward next is 0.7304, 
noisyNet noise sample is [array([-0.886942], dtype=float32), 0.8521614]. 
=============================================
[2019-04-08 15:31:41,547] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6530637e-14 1.5848652e-08 4.2494150e-05 2.6188327e-05 1.8660817e-08
 1.6309458e-10 1.0270985e-06 1.3922009e-11 1.5385060e-15 2.2099176e-17
 9.9993026e-01], sum to 1.0000
[2019-04-08 15:31:41,547] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5626
[2019-04-08 15:31:41,572] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.0, 33.5, 0.0, 0.0, 22.5, 27.38669657898264, 0.8243496700679428, 1.0, 1.0, 65.0, 51855.54460491495], 
current ob forecast is [], 
actual action is [9.0, 65.0], 
sim time this is 2831400.0000, 
sim time next is 2832000.0000, 
raw observation next is [3.666666666666667, 34.66666666666666, 0.0, 0.0, 22.5, 27.21140992152008, 0.8086497890938373, 1.0, 1.0, 65.0, 54347.10380894358], 
processed observation next is [1.0, 0.782608695652174, 0.564173591874423, 0.34666666666666657, 0.0, 0.0, 0.375, 0.7676174934600066, 0.7695499296979458, 1.0, 1.0, 1.0, 0.25879573242354087], 
reward next is 0.7412, 
noisyNet noise sample is [array([-1.7049105], dtype=float32), -0.41680533]. 
=============================================
[2019-04-08 15:31:41,575] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.56602]
 [73.66476]
 [73.63381]
 [73.67412]
 [73.8445 ]], R is [[73.64027405]
 [73.65693665]
 [73.69324493]
 [73.78717041]
 [73.90242004]].
[2019-04-08 15:31:41,702] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5512800e-15 3.8145460e-09 9.4373627e-06 4.7307003e-06 6.2390271e-09
 3.0510604e-11 6.6672413e-08 4.1360821e-12 4.9570157e-16 1.2483852e-18
 9.9998581e-01], sum to 1.0000
[2019-04-08 15:31:41,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1669
[2019-04-08 15:31:41,760] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.333333333333334, 31.66666666666667, 227.1666666666667, 144.1666666666667, 22.5, 27.37204212613323, 0.847608968182728, 1.0, 1.0, 65.0, 46670.44393233857], 
current ob forecast is [], 
actual action is [10.333333333333334, 65.0], 
sim time this is 2814000.0000, 
sim time next is 2814600.0000, 
raw observation next is [5.666666666666666, 30.83333333333333, 205.3333333333333, 115.3333333333333, 22.5, 27.58563999761653, 0.8618108695172616, 1.0, 1.0, 65.0, 44743.07293836292], 
processed observation next is [1.0, 0.5652173913043478, 0.6195752539242845, 0.3083333333333333, 0.6844444444444443, 0.12744014732965006, 0.375, 0.7988033331347107, 0.7872702898390872, 1.0, 1.0, 1.0, 0.21306225208744248], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.8921686], dtype=float32), 0.21559905]. 
=============================================
[2019-04-08 15:31:43,378] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2054893e-15 2.7527722e-08 9.5234436e-06 3.8798276e-05 1.2577744e-09
 6.2464985e-11 8.0990922e-08 5.9122894e-13 1.3504406e-16 5.6860610e-18
 9.9995160e-01], sum to 1.0000
[2019-04-08 15:31:43,379] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8439
[2019-04-08 15:31:43,416] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.333333333333334, 34.16666666666667, 225.3333333333333, 343.6666666666666, 22.5, 27.54332431914974, 0.7520776044862298, 1.0, 1.0, 65.0, 61311.19504985567], 
current ob forecast is [], 
actual action is [9.333333333333334, 65.0], 
sim time this is 2812200.0000, 
sim time next is 2812800.0000, 
raw observation next is [4.666666666666667, 33.33333333333334, 237.1666666666667, 258.3333333333333, 22.5, 27.01912091724607, 0.8677085084409525, 1.0, 1.0, 65.0, 74593.7039309463], 
processed observation next is [1.0, 0.5652173913043478, 0.5918744228993538, 0.3333333333333334, 0.7905555555555557, 0.285451197053407, 0.375, 0.7515934097705058, 0.7892361694803175, 1.0, 1.0, 1.0, 0.3552081139568871], 
reward next is 0.6448, 
noisyNet noise sample is [array([-1.2224507], dtype=float32), 0.3816733]. 
=============================================
[2019-04-08 15:31:44,139] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.7880494e-16 1.1007301e-08 2.1583360e-06 1.6698921e-06 2.1656787e-11
 1.2859366e-12 1.3764049e-08 2.1350471e-13 8.6952088e-17 2.3091812e-19
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:31:44,141] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7951
[2019-04-08 15:31:44,170] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 22.5, 28.0194414142195, 0.9053009526547419, 1.0, 1.0, 65.0, 30062.09686701327], 
current ob forecast is [], 
actual action is [10.833333333333332, 65.0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 22.5, 28.00091546921544, 0.8905529276116395, 1.0, 1.0, 65.0, 31434.58678045523], 
processed observation next is [1.0, 0.7391304347826086, 0.6195752539242845, 0.28666666666666674, 0.05333333333333334, 0.056353591160221, 0.375, 0.8334096224346199, 0.7968509758705465, 1.0, 1.0, 1.0, 0.14968850847835824], 
reward next is 0.8503, 
noisyNet noise sample is [array([1.830997], dtype=float32), -1.0032902]. 
=============================================
[2019-04-08 15:31:44,235] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.3633037e-18 4.2014672e-10 1.4512921e-06 9.1509719e-06 7.0227657e-10
 1.6511103e-12 1.7025650e-07 3.0277616e-14 2.2987416e-19 2.8612965e-21
 9.9998927e-01], sum to 1.0000
[2019-04-08 15:31:44,237] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-08 15:31:44,250] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 19.0, 26.52018962674629, 0.631832128552899, 0.0, 1.0, 65.0, 66194.06861867086], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 2874000.0000, 
sim time next is 2874600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 19.0, 26.47626374096991, 0.6416287298397226, 0.0, 1.0, 65.0, 66456.32254355293], 
processed observation next is [1.0, 0.2608695652173913, 0.5041551246537397, 0.965, 0.0, 0.0, 0.08333333333333333, 0.7063553117474924, 0.7138762432799076, 0.0, 1.0, 1.0, 0.3164586787788235], 
reward next is 0.6835, 
noisyNet noise sample is [array([-0.6902215], dtype=float32), 0.5916025]. 
=============================================
[2019-04-08 15:31:44,262] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.2318881e-16 1.8464575e-09 3.5801634e-06 2.8607252e-05 4.9374664e-09
 3.0408578e-13 3.2770456e-07 1.4141651e-13 6.8864626e-19 4.1401431e-19
 9.9996746e-01], sum to 1.0000
[2019-04-08 15:31:44,266] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7801
[2019-04-08 15:31:44,277] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 26.48104643373144, 0.6479285594791385, 0.0, 1.0, 65.0, 66685.79174463333], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 26.51183860250661, 0.6458696497575961, 0.0, 1.0, 65.0, 65269.78970396567], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7093198835422175, 0.715289883252532, 0.0, 1.0, 1.0, 0.31080852239983653], 
reward next is 0.6892, 
noisyNet noise sample is [array([-1.2282887], dtype=float32), 0.14674295]. 
=============================================
[2019-04-08 15:31:44,358] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5719839e-18 3.9350073e-10 1.4118419e-06 8.9723271e-06 6.6543765e-10
 1.5247170e-12 1.6454592e-07 2.7884277e-14 2.0418389e-19 2.5027131e-21
 9.9998951e-01], sum to 1.0000
[2019-04-08 15:31:44,363] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4133
[2019-04-08 15:31:44,379] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 26.55730362374431, 0.6435706623864695, 0.0, 1.0, 65.0, 61177.60534025512], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2876400.0000, 
sim time next is 2877000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 22.5, 26.54796415594074, 0.6479886983984796, 0.0, 1.0, 65.0, 60853.58628070957], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.375, 0.712330346328395, 0.7159962327994932, 0.0, 1.0, 1.0, 0.2897789822890932], 
reward next is 0.7102, 
noisyNet noise sample is [array([-0.6902215], dtype=float32), 0.5916025]. 
=============================================
[2019-04-08 15:31:44,399] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.14311 ]
 [86.1148  ]
 [86.06622 ]
 [85.96673 ]
 [85.893326]], R is [[86.20539093]
 [86.05201721]
 [85.90350342]
 [85.74282074]
 [85.56893158]].
[2019-04-08 15:31:44,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9463182e-15 1.6500644e-08 2.5645073e-05 3.4163164e-05 3.8284262e-10
 4.3066963e-11 1.0127637e-06 4.2972192e-11 2.9623844e-16 3.6699609e-18
 9.9993920e-01], sum to 1.0000
[2019-04-08 15:31:44,468] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7995
[2019-04-08 15:31:44,493] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.5, 40.5, 0.0, 0.0, 22.5, 26.93615350817742, 0.7744276114640164, 1.0, 1.0, 65.0, 57449.022498959], 
current ob forecast is [], 
actual action is [7.5, 65.0], 
sim time this is 2835000.0000, 
sim time next is 2835600.0000, 
raw observation next is [2.333333333333333, 41.66666666666666, 0.0, 0.0, 22.5, 26.88316499577195, 0.7692605765110567, 0.0, 1.0, 65.0, 59576.20992457672], 
processed observation next is [1.0, 0.8260869565217391, 0.5272391505078486, 0.4166666666666666, 0.0, 0.0, 0.375, 0.7402637496476624, 0.7564201921703523, 0.0, 1.0, 1.0, 0.2836962377360796], 
reward next is 0.7163, 
noisyNet noise sample is [array([1.0581566], dtype=float32), -1.3445438]. 
=============================================
[2019-04-08 15:31:44,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.3261609e-20 1.4479897e-11 1.8773752e-08 2.5498127e-07 1.2885536e-12
 4.0900070e-14 1.7903453e-08 8.5808335e-17 4.1451420e-20 3.5267071e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:31:44,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5301
[2019-04-08 15:31:44,708] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 93.0, 77.0, 78.0, 22.5, 27.05951896012684, 0.7499453944839635, 1.0, 1.0, 65.0, 55324.04350076219], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2883600.0000, 
sim time next is 2884200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 67.66666666666666, 51.99999999999999, 22.5, 27.17549644461839, 0.7480612378623936, 1.0, 1.0, 65.0, 51671.99241592925], 
processed observation next is [1.0, 0.391304347826087, 0.4856879039704525, 0.9416666666666665, 0.22555555555555554, 0.0574585635359116, 0.375, 0.7646247037181991, 0.7493537459541312, 1.0, 1.0, 1.0, 0.24605710674252024], 
reward next is 0.7539, 
noisyNet noise sample is [array([0.0140247], dtype=float32), -0.15385547]. 
=============================================
[2019-04-08 15:31:44,824] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6059670e-18 1.1507861e-11 9.5282733e-09 5.6797467e-07 3.9193145e-11
 8.2569297e-15 3.3889659e-08 3.8413236e-15 4.9531627e-19 9.8291675e-22
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:31:44,825] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5840
[2019-04-08 15:31:44,843] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 22.5, 26.54212425780438, 0.6423187913700367, 1.0, 1.0, 65.0, 58083.43997567109], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2878200.0000, 
sim time next is 2878800.0000, 
raw observation next is [2.0, 93.0, 17.5, 48.49999999999999, 22.5, 26.53136174396305, 0.6566700351209313, 1.0, 1.0, 65.0, 57857.75087889425], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.058333333333333334, 0.053591160220994465, 0.375, 0.7109468119969208, 0.7188900117069771, 1.0, 1.0, 1.0, 0.27551309942330593], 
reward next is 0.7245, 
noisyNet noise sample is [array([-1.2721198], dtype=float32), -1.2361354]. 
=============================================
[2019-04-08 15:31:44,896] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3377494e-16 3.1970959e-08 5.2016230e-06 5.4727984e-06 9.1977664e-10
 4.4299697e-10 4.8609712e-07 2.0002602e-12 2.3412329e-15 6.5724806e-18
 9.9998879e-01], sum to 1.0000
[2019-04-08 15:31:44,899] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7341
[2019-04-08 15:31:44,927] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 26.84469877210505, 0.7753449472630014, 0.0, 1.0, 65.0, 54075.40221760969], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 26.85350446190878, 0.7753986641817364, 0.0, 1.0, 65.0, 52993.69964436678], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.7377920384923984, 0.7584662213939121, 0.0, 1.0, 1.0, 0.2523509506874609], 
reward next is 0.7476, 
noisyNet noise sample is [array([0.57023805], dtype=float32), 0.6849815]. 
=============================================
[2019-04-08 15:31:45,024] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2924559e-17 4.8081885e-09 1.3199212e-07 1.5938624e-05 1.3691416e-10
 1.8307567e-12 4.0786045e-08 2.9088680e-14 9.3453244e-18 4.7299086e-20
 9.9998391e-01], sum to 1.0000
[2019-04-08 15:31:45,025] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5259
[2019-04-08 15:31:45,048] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.6666666666666666, 95.33333333333334, 79.5, 0.0, 22.5, 27.48131737503152, 0.8166240781506073, 1.0, 1.0, 65.0, 41235.90467543966], 
current ob forecast is [], 
actual action is [5.666666666666667, 65.0], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [0.8333333333333334, 94.16666666666666, 81.0, 0.0, 22.5, 27.50989773877868, 0.8230089115424452, 1.0, 1.0, 65.0, 40140.59204272826], 
processed observation next is [1.0, 0.43478260869565216, 0.4856879039704525, 0.9416666666666665, 0.27, 0.0, 0.375, 0.7924914782315566, 0.7743363038474818, 1.0, 1.0, 1.0, 0.19114567639394411], 
reward next is 0.8089, 
noisyNet noise sample is [array([-0.85442936], dtype=float32), 0.8289186]. 
=============================================
[2019-04-08 15:31:46,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.54287217e-17 2.87745561e-09 5.49751076e-06 2.15137552e-05
 5.39884026e-10 2.60523872e-12 1.11134135e-07 2.58880699e-13
 1.10918354e-18 2.42419213e-21 9.99972820e-01], sum to 1.0000
[2019-04-08 15:31:46,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8337
[2019-04-08 15:31:46,104] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 19.0, 26.61778669784867, 0.641127683027504, 0.0, 1.0, 65.0, 62787.71728750304], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2872800.0000, 
sim time next is 2873400.0000, 
raw observation next is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 19.0, 26.57310168627114, 0.6343919572564427, 0.0, 1.0, 65.0, 63694.9728954481], 
processed observation next is [1.0, 0.2608695652173913, 0.49492151431209613, 0.9883333333333334, 0.0, 0.0, 0.08333333333333333, 0.7144251405225951, 0.7114639857521476, 0.0, 1.0, 1.0, 0.30330939474022905], 
reward next is 0.6967, 
noisyNet noise sample is [array([0.51744497], dtype=float32), -0.36720443]. 
=============================================
[2019-04-08 15:31:46,547] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1859159e-18 5.9061234e-10 5.3725582e-08 3.6235645e-07 5.7247575e-11
 1.5331513e-12 1.1658309e-08 2.8334319e-14 9.7967663e-19 1.8448343e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:31:46,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3646
[2019-04-08 15:31:46,576] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.5, 100.0, 175.0, 0.0, 22.5, 27.34424778420794, 0.9034901868527773, 1.0, 1.0, 65.0, 53854.30126000156], 
current ob forecast is [], 
actual action is [6.5, 65.0], 
sim time this is 2896200.0000, 
sim time next is 2896800.0000, 
raw observation next is [1.666666666666667, 100.0, 173.1666666666667, 0.0, 22.5, 27.07630772197773, 0.8423103538239758, 1.0, 1.0, 64.99999999999994, 41433.84115542807], 
processed observation next is [1.0, 0.5217391304347826, 0.5087719298245615, 1.0, 0.5772222222222224, 0.0, 0.375, 0.7563589768314776, 0.7807701179413252, 1.0, 1.0, 0.9999999999999989, 0.19730400550203842], 
reward next is 0.8027, 
noisyNet noise sample is [array([0.81456786], dtype=float32), -0.18643019]. 
=============================================
[2019-04-08 15:31:46,858] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7618670e-17 4.8291842e-09 7.0643102e-07 8.7079898e-05 2.6130509e-10
 3.0044335e-13 1.9317041e-09 1.0120853e-14 5.8514839e-18 5.6896681e-21
 9.9991214e-01], sum to 1.0000
[2019-04-08 15:31:46,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2451
[2019-04-08 15:31:46,876] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 100.0, 102.3333333333333, 0.0, 22.5, 27.6267329506948, 0.9091774994983609, 1.0, 1.0, 65.0, 26901.72251134503], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2902800.0000, 
sim time next is 2903400.0000, 
raw observation next is [2.0, 100.0, 90.0, 0.0, 22.5, 27.71667725334433, 0.9190350464144207, 1.0, 1.0, 65.0, 27502.83079442838], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.3, 0.0, 0.375, 0.8097231044453608, 0.8063450154714736, 1.0, 1.0, 1.0, 0.13096586092584944], 
reward next is 0.8690, 
noisyNet noise sample is [array([0.8821029], dtype=float32), -0.03062629]. 
=============================================
[2019-04-08 15:31:47,788] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0736432e-14 1.2781867e-08 2.2480572e-06 7.2510688e-06 4.5141375e-09
 5.2095654e-12 4.4487601e-06 9.5431486e-13 1.2968554e-15 1.2387036e-18
 9.9998605e-01], sum to 1.0000
[2019-04-08 15:31:47,790] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1664
[2019-04-08 15:31:47,821] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.35710360361771, 0.6755662506762254, 0.0, 1.0, 65.0, 51948.44200762097], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2950800.0000, 
sim time next is 2951400.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.335939650532, 0.672659258580436, 0.0, 1.0, 65.0, 52903.90448229195], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6946616375443334, 0.7242197528601454, 0.0, 1.0, 1.0, 0.2519233546775807], 
reward next is 0.7481, 
noisyNet noise sample is [array([0.0320747], dtype=float32), 0.26225042]. 
=============================================
[2019-04-08 15:31:48,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.5481274e-17 3.4213629e-10 3.2204230e-07 4.8287438e-06 2.1278720e-10
 7.9026334e-12 6.7830488e-09 3.9438870e-14 2.2392263e-18 1.2229002e-20
 9.9999487e-01], sum to 1.0000
[2019-04-08 15:31:48,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0375
[2019-04-08 15:31:48,220] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 100.0, 165.8333333333333, 0.0, 22.5, 27.6764415016378, 0.9048533577729293, 1.0, 1.0, 65.0, 29547.72156400994], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2899200.0000, 
sim time next is 2899800.0000, 
raw observation next is [2.0, 100.0, 164.0, 0.0, 22.5, 27.72228466497585, 0.9125499743309265, 1.0, 1.0, 65.0, 29231.87998030848], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.5466666666666666, 0.0, 0.375, 0.8101903887479874, 0.8041833247769755, 1.0, 1.0, 1.0, 0.13919942847765943], 
reward next is 0.8608, 
noisyNet noise sample is [array([0.55971825], dtype=float32), -0.94241977]. 
=============================================
[2019-04-08 15:31:48,345] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2273245e-16 5.1770881e-09 1.8330636e-06 4.5400204e-05 7.6703199e-10
 1.5880514e-11 1.2742562e-07 6.9207457e-14 5.7768433e-18 6.8975573e-20
 9.9995267e-01], sum to 1.0000
[2019-04-08 15:31:48,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3728
[2019-04-08 15:31:48,373] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.166666666666667, 79.16666666666667, 0.0, 0.0, 19.0, 26.71633388827759, 0.7708587996495796, 0.0, 1.0, 65.0, 46458.70289039328], 
current ob forecast is [], 
actual action is [3.833333333333333, 65.0], 
sim time this is 2931000.0000, 
sim time next is 2931600.0000, 
raw observation next is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 19.0, 26.69720928989405, 0.7727325130426469, 0.0, 1.0, 65.0, 50810.14310317206], 
processed observation next is [1.0, 0.9565217391304348, 0.42566943674976926, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.7247674408245043, 0.757577504347549, 0.0, 1.0, 1.0, 0.2419530623960574], 
reward next is 0.7580, 
noisyNet noise sample is [array([-1.1866221], dtype=float32), -1.2797964]. 
=============================================
[2019-04-08 15:31:48,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2212495e-18 9.2226799e-12 1.5354969e-07 2.0036789e-06 8.0303828e-12
 2.7065698e-14 1.5528664e-09 2.1786613e-16 1.0763895e-18 2.7090945e-22
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:31:48,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1623
[2019-04-08 15:31:48,687] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.0603079e-17 1.5334342e-10 9.8259295e-07 9.9162924e-07 2.3268365e-10
 1.4663120e-13 2.3842079e-09 7.9608612e-15 1.1299158e-18 2.0144674e-20
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:31:48,689] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0774
[2019-04-08 15:31:48,703] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 19.0, 26.52018909409043, 0.6318319123468651, 0.0, 1.0, 65.0, 66194.07775004058], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 2874000.0000, 
sim time next is 2874600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 19.0, 26.47626321191808, 0.6416285152490193, 0.0, 1.0, 65.0, 66456.33147753733], 
processed observation next is [1.0, 0.2608695652173913, 0.5041551246537397, 0.965, 0.0, 0.0, 0.08333333333333333, 0.70635526765984, 0.7138761717496731, 0.0, 1.0, 1.0, 0.31645872132160635], 
reward next is 0.6835, 
noisyNet noise sample is [array([0.9881864], dtype=float32), -0.6730201]. 
=============================================
[2019-04-08 15:31:48,708] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666666, 98.83333333333334, 58.66666666666666, 0.0, 22.5, 27.39830970078425, 0.7821381562976355, 1.0, 1.0, 65.0, 45713.56590148975], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 2886600.0000, 
sim time next is 2887200.0000, 
raw observation next is [0.0, 100.0, 63.5, 0.0, 22.5, 27.39408525804447, 0.7885190859675704, 1.0, 1.0, 65.0, 45971.40588224315], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 1.0, 0.21166666666666667, 0.0, 0.375, 0.7828404381703725, 0.7628396953225235, 1.0, 1.0, 1.0, 0.21891145658211023], 
reward next is 0.7811, 
noisyNet noise sample is [array([-1.7631724], dtype=float32), -0.603889]. 
=============================================
[2019-04-08 15:31:50,395] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.97656324e-14 1.01964588e-08 1.15174021e-06 1.93952633e-06
 2.20704499e-09 5.64221369e-12 3.70526188e-08 9.80565913e-15
 1.92555090e-17 1.11468945e-20 9.99996901e-01], sum to 1.0000
[2019-04-08 15:31:50,400] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9463
[2019-04-08 15:31:50,438] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 100.0, 78.0, 27.0, 22.5, 27.13131262698842, 0.9569325972171084, 1.0, 1.0, 65.0, 59084.15341865765], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2908800.0000, 
sim time next is 2909400.0000, 
raw observation next is [2.0, 98.83333333333334, 75.66666666666666, 36.00000000000001, 22.5, 26.65573338705202, 0.8779344595439067, 1.0, 1.0, 64.99999999999999, 40914.6588722441], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9883333333333334, 0.2522222222222222, 0.03977900552486189, 0.375, 0.7213111155876684, 0.7926448198479689, 1.0, 1.0, 0.9999999999999997, 0.1948317089154481], 
reward next is 0.8052, 
noisyNet noise sample is [array([-1.2163213], dtype=float32), -1.2645639]. 
=============================================
[2019-04-08 15:31:50,805] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.63435113e-16 1.64279146e-10 2.45456158e-06 3.02002178e-07
 6.61339108e-11 1.19864665e-12 1.42772985e-08 2.10202762e-14
 3.54024979e-19 6.00031194e-20 9.99997258e-01], sum to 1.0000
[2019-04-08 15:31:50,812] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3870
[2019-04-08 15:31:50,846] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 93.0, 0.0, 0.0, 22.5, 27.5253044738884, 0.8426157747249046, 1.0, 1.0, 65.0, 49765.83443984651], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 2914800.0000, 
sim time next is 2915400.0000, 
raw observation next is [1.166666666666667, 93.0, 0.0, 0.0, 22.5, 26.76356222229555, 0.9373499250146766, 1.0, 1.0, 65.0, 62607.12377626987], 
processed observation next is [1.0, 0.7391304347826086, 0.49492151431209613, 0.93, 0.0, 0.0, 0.375, 0.7302968518579626, 0.8124499750048922, 1.0, 1.0, 1.0, 0.29812916083938035], 
reward next is 0.7019, 
noisyNet noise sample is [array([0.08147077], dtype=float32), -0.90886694]. 
=============================================
[2019-04-08 15:31:51,929] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6719748e-14 7.2078251e-08 1.2621947e-04 6.9782475e-04 1.3475978e-08
 1.3292344e-10 5.9537397e-06 1.2595432e-10 5.0513949e-15 9.3966775e-18
 9.9916995e-01], sum to 1.0000
[2019-04-08 15:31:51,933] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5943
[2019-04-08 15:31:51,944] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 19.0, 26.75152428114387, 0.8225976589919717, 0.0, 1.0, 65.0, 44825.58028610417], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 2928600.0000, 
sim time next is 2929200.0000, 
raw observation next is [-1.0, 80.33333333333334, 0.0, 0.0, 19.0, 26.74084205499483, 0.8201341789427401, 0.0, 1.0, 65.0, 44891.49447445839], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.7284035045829024, 0.77337805964758, 0.0, 1.0, 1.0, 0.21376902130694472], 
reward next is 0.7862, 
noisyNet noise sample is [array([2.2824578], dtype=float32), -0.95745254]. 
=============================================
[2019-04-08 15:31:52,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1429222e-13 7.3839530e-09 5.6666458e-06 5.3982869e-05 2.0087818e-09
 1.4454968e-10 9.5830876e-07 1.0871789e-11 2.6331328e-15 5.1356329e-17
 9.9993932e-01], sum to 1.0000
[2019-04-08 15:31:52,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5799
[2019-04-08 15:31:52,434] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7333333333333334, 85.33333333333334, 0.0, 0.0, 19.0, 27.00878365651955, 0.7689875520007344, 0.0, 1.0, 65.0, 41166.62806383644], 
current ob forecast is [], 
actual action is [4.266666666666667, 65.0], 
sim time this is 3090000.0000, 
sim time next is 3090600.0000, 
raw observation next is [-0.8, 87.0, 0.0, 0.0, 19.0, 26.97683345437817, 0.7635107898872863, 0.0, 1.0, 65.0, 41670.05288943021], 
processed observation next is [0.0, 0.782608695652174, 0.4404432132963989, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7480694545315142, 0.7545035966290955, 0.0, 1.0, 1.0, 0.19842882328300102], 
reward next is 0.8016, 
noisyNet noise sample is [array([0.9078932], dtype=float32), 0.7049206]. 
=============================================
[2019-04-08 15:31:53,024] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1146873e-16 6.3643593e-09 6.1222236e-06 6.4437454e-06 8.4587741e-09
 1.3629340e-11 1.2320083e-07 1.2401863e-12 8.1837311e-16 3.0727909e-18
 9.9998724e-01], sum to 1.0000
[2019-04-08 15:31:53,024] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9799
[2019-04-08 15:31:53,041] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8044247e-14 8.1493363e-09 1.1635025e-05 4.4925600e-06 8.1519907e-10
 3.3922518e-10 4.8748166e-07 6.8981656e-12 1.6423729e-15 5.1580068e-17
 9.9998343e-01], sum to 1.0000
[2019-04-08 15:31:53,042] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6066
[2019-04-08 15:31:53,050] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 158.0, 114.0, 19.0, 26.23334267376305, 0.6495825630857647, 0.0, 1.0, 65.0, 59841.92073360051], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 2972400.0000, 
sim time next is 2973000.0000, 
raw observation next is [-4.0, 71.0, 162.0, 96.00000000000001, 19.0, 26.23335539009897, 0.6499390672437669, 0.0, 1.0, 65.0, 59970.21482017908], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.54, 0.10607734806629836, 0.08333333333333333, 0.6861129491749143, 0.7166463557479222, 0.0, 1.0, 1.0, 0.28557245152466226], 
reward next is 0.7144, 
noisyNet noise sample is [array([-1.231299], dtype=float32), 0.9852863]. 
=============================================
[2019-04-08 15:31:53,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.30318 ]
 [73.32099 ]
 [73.288956]
 [73.16846 ]
 [73.05822 ]], R is [[73.19210815]
 [73.17523193]
 [73.15312958]
 [73.11843872]
 [73.08161163]].
[2019-04-08 15:31:53,073] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 218.5, 487.5, 19.0, 26.49924296924745, 0.7413796273829957, 0.0, 1.0, 65.0, 48770.22073249317], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2980800.0000, 
sim time next is 2981400.0000, 
raw observation next is [-3.0, 65.0, 206.0, 555.3333333333334, 19.0, 26.52433380231016, 0.7488940373338538, 0.0, 1.0, 65.0, 47923.22912831327], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6866666666666666, 0.6136279926335175, 0.08333333333333333, 0.7103611501925133, 0.7496313457779512, 0.0, 1.0, 1.0, 0.22820585299196797], 
reward next is 0.7718, 
noisyNet noise sample is [array([-1.5122808], dtype=float32), -1.7379192]. 
=============================================
[2019-04-08 15:31:53,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6184753e-15 2.5413360e-09 6.8888289e-06 2.1621826e-04 9.6852496e-08
 1.7916506e-10 7.2164050e-07 1.7243245e-11 1.9061718e-15 1.0366196e-17
 9.9977607e-01], sum to 1.0000
[2019-04-08 15:31:53,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4232
[2019-04-08 15:31:53,327] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.01006050613232, 0.5907858597791525, 0.0, 1.0, 65.0, 77999.86806880006], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 2963400.0000, 
sim time next is 2964000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 25.99513213961911, 0.5865882878268138, 0.0, 1.0, 65.0, 77906.94381535145], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6662610116349258, 0.6955294292756046, 0.0, 1.0, 1.0, 0.3709854467397688], 
reward next is 0.6290, 
noisyNet noise sample is [array([0.5813965], dtype=float32), -0.07917671]. 
=============================================
[2019-04-08 15:31:53,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.39157 ]
 [72.396866]
 [72.406105]
 [72.42728 ]
 [72.445076]], R is [[72.30882263]
 [72.21430206]
 [72.12051392]
 [72.02836609]
 [71.94088745]].
[2019-04-08 15:31:54,305] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.2610945e-13 1.7370492e-07 2.4736658e-04 4.1242783e-05 6.5668183e-07
 2.6568678e-09 5.5622190e-06 6.9821725e-11 6.4380099e-15 2.4116799e-16
 9.9970502e-01], sum to 1.0000
[2019-04-08 15:31:54,305] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4212
[2019-04-08 15:31:54,357] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 60.0, 101.0, 751.0, 19.0, 26.92591561193405, 0.8344343564515575, 0.0, 1.0, 65.0, 37542.20639415347], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 2989800.0000, 
sim time next is 2990400.0000, 
raw observation next is [-2.0, 60.0, 98.0, 737.0, 19.0, 26.92784798674947, 0.8364024375224813, 0.0, 1.0, 65.0, 38245.66302256546], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.32666666666666666, 0.8143646408839779, 0.08333333333333333, 0.7439873322291225, 0.7788008125074938, 0.0, 1.0, 1.0, 0.18212220486935932], 
reward next is 0.8179, 
noisyNet noise sample is [array([0.47268996], dtype=float32), -1.2649206]. 
=============================================
[2019-04-08 15:31:54,563] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.4385156e-14 2.6713864e-09 2.1961261e-05 3.3612505e-05 7.9317453e-10
 5.4104143e-10 5.9035176e-07 5.8368059e-12 7.6991033e-16 1.7802192e-17
 9.9994385e-01], sum to 1.0000
[2019-04-08 15:31:54,563] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1450
[2019-04-08 15:31:54,582] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 231.0, 419.6666666666666, 19.0, 26.47477798872352, 0.7325126439018478, 0.0, 1.0, 65.0, 49565.50674391795], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2980200.0000, 
sim time next is 2980800.0000, 
raw observation next is [-3.0, 65.0, 218.5, 487.5, 19.0, 26.49924300618076, 0.7413796385445979, 0.0, 1.0, 65.0, 48770.22026540653], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.7283333333333334, 0.5386740331491713, 0.08333333333333333, 0.7082702505150632, 0.7471265461815326, 0.0, 1.0, 1.0, 0.23223914412098348], 
reward next is 0.7678, 
noisyNet noise sample is [array([1.3778055], dtype=float32), 0.06161269]. 
=============================================
[2019-04-08 15:31:55,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4615453e-17 1.0557838e-10 1.3762187e-07 2.4825789e-05 2.3736986e-09
 6.8888596e-13 1.3461398e-07 1.2957586e-14 3.2693317e-18 9.2539490e-21
 9.9997485e-01], sum to 1.0000
[2019-04-08 15:31:55,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0080
[2019-04-08 15:31:55,433] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.333333333333334, 100.0, 0.0, 0.0, 19.0, 26.92754040294723, 0.674515673042353, 0.0, 1.0, 65.0, 56176.36797792977], 
current ob forecast is [], 
actual action is [9.333333333333334, 65.0], 
sim time this is 3132600.0000, 
sim time next is 3133200.0000, 
raw observation next is [4.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.88573589852221, 0.6763698890712128, 0.0, 1.0, 65.0, 57485.74453657572], 
processed observation next is [1.0, 0.2608695652173913, 0.5918744228993538, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7404779915435175, 0.7254566296904043, 0.0, 1.0, 1.0, 0.2737416406503606], 
reward next is 0.7263, 
noisyNet noise sample is [array([0.9829874], dtype=float32), 0.18008405]. 
=============================================
[2019-04-08 15:31:55,482] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4431577e-15 1.1872119e-08 4.2254974e-06 9.3942363e-05 2.5260826e-08
 2.0851025e-10 1.1409120e-06 8.1586918e-13 1.0038760e-14 2.7724313e-17
 9.9990070e-01], sum to 1.0000
[2019-04-08 15:31:55,482] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4005
[2019-04-08 15:31:55,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.57596739e-14 2.93675377e-08 5.47913260e-06 1.09620023e-05
 1.28867237e-08 3.88859667e-10 4.21133427e-06 1.36368234e-11
 6.71675172e-14 3.42018802e-17 9.99979258e-01], sum to 1.0000
[2019-04-08 15:31:55,491] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4761
[2019-04-08 15:31:55,540] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.61831918491556, 0.671248693140995, 0.0, 1.0, 65.0, 43911.72276081508], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3018600.0000, 
sim time next is 3019200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 26.58644068919924, 0.6678268721395257, 0.0, 1.0, 65.0, 45072.32645609223], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7155367240999366, 0.7226089573798419, 0.0, 1.0, 1.0, 0.21463012598139158], 
reward next is 0.7854, 
noisyNet noise sample is [array([0.55922633], dtype=float32), 0.5036528]. 
=============================================
[2019-04-08 15:31:55,548] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.86953176812537, 0.7692496451516581, 0.0, 1.0, 65.0, 41784.49294373308], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3004200.0000, 
sim time next is 3004800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.85178961687732, 0.7641890818082042, 0.0, 1.0, 65.0, 42007.33204201546], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7376491347397766, 0.754729693936068, 0.0, 1.0, 1.0, 0.20003491448578792], 
reward next is 0.8000, 
noisyNet noise sample is [array([-0.1631309], dtype=float32), -0.9664937]. 
=============================================
[2019-04-08 15:31:55,890] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.38797223e-15 7.08893211e-09 7.66113772e-06 1.74798333e-05
 1.25490915e-08 2.36939157e-09 1.75547066e-05 5.78325200e-11
 5.38409141e-14 1.07623199e-16 9.99957323e-01], sum to 1.0000
[2019-04-08 15:31:55,895] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7848
[2019-04-08 15:31:55,910] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 26.31433135733739, 0.5934473205734888, 0.0, 1.0, 65.0, 48585.16881126972], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3031200.0000, 
sim time next is 3031800.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.31758983126998, 0.5885646591989321, 0.0, 1.0, 65.0, 48360.98898587609], 
processed observation next is [0.0, 0.08695652173913043, 0.31948291782086796, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6931324859391651, 0.6961882197329774, 0.0, 1.0, 1.0, 0.23029042374226708], 
reward next is 0.7697, 
noisyNet noise sample is [array([2.2371526], dtype=float32), -1.2491528]. 
=============================================
[2019-04-08 15:31:56,049] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0493156e-14 2.8633042e-07 7.0318083e-06 1.5699612e-05 2.9791249e-08
 3.0721889e-10 4.7118338e-06 3.6941280e-11 1.2361589e-14 1.4596700e-16
 9.9997222e-01], sum to 1.0000
[2019-04-08 15:31:56,051] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7206
[2019-04-08 15:31:56,065] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666666, 69.0, 0.0, 0.0, 19.0, 26.4333904189093, 0.621993481693968, 0.0, 1.0, 65.0, 46430.90235067494], 
current ob forecast is [], 
actual action is [0.3333333333333339, 65.0], 
sim time this is 3026400.0000, 
sim time next is 3027000.0000, 
raw observation next is [-4.833333333333334, 70.0, 0.0, 0.0, 19.0, 26.41345243668878, 0.617400366842794, 0.0, 1.0, 65.0, 46745.08314112864], 
processed observation next is [0.0, 0.0, 0.32871652816251157, 0.7, 0.0, 0.0, 0.08333333333333333, 0.7011210363907315, 0.7058001222809313, 0.0, 1.0, 1.0, 0.2225956340053745], 
reward next is 0.7774, 
noisyNet noise sample is [array([-0.4334054], dtype=float32), -0.921154]. 
=============================================
[2019-04-08 15:31:56,109] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[67.307625]
 [67.48846 ]
 [67.46311 ]
 [67.87483 ]
 [68.038414]], R is [[67.57122803]
 [67.67441559]
 [67.77891541]
 [67.88374329]
 [67.98686981]].
[2019-04-08 15:31:56,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7731051e-15 2.1163480e-09 1.2024635e-06 8.8390061e-07 5.4103690e-09
 6.4711465e-11 4.2497039e-08 7.9136073e-12 3.5090240e-16 1.2412873e-17
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:31:56,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2601
[2019-04-08 15:31:56,440] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 193.5, 623.1666666666667, 19.0, 26.55182971356094, 0.7571172317942226, 0.0, 1.0, 65.0, 75193.95032978759], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2982000.0000, 
sim time next is 2982600.0000, 
raw observation next is [-3.0, 65.0, 181.0, 691.0, 19.0, 26.58081309022185, 0.7666894038507519, 0.0, 1.0, 65.0, 60481.16426498775], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.6033333333333334, 0.7635359116022099, 0.08333333333333333, 0.7150677575184874, 0.7555631346169173, 0.0, 1.0, 1.0, 0.28800554411898927], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.09237505], dtype=float32), 1.0431992]. 
=============================================
[2019-04-08 15:31:56,471] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4905173e-14 3.1480482e-08 1.5323865e-06 1.0483932e-05 6.5877517e-09
 3.0860436e-10 3.3523347e-07 9.5309455e-12 8.8881719e-16 1.0488653e-16
 9.9998760e-01], sum to 1.0000
[2019-04-08 15:31:56,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9932
[2019-04-08 15:31:56,521] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 19.0, 26.31433162610604, 0.5934474229385805, 0.0, 1.0, 65.0, 48585.16500985713], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3031200.0000, 
sim time next is 3031800.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.31759010238659, 0.5885647643463278, 0.0, 1.0, 65.0, 48360.98515520558], 
processed observation next is [0.0, 0.08695652173913043, 0.31948291782086796, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6931325085322158, 0.6961882547821093, 0.0, 1.0, 1.0, 0.23029040550097893], 
reward next is 0.7697, 
noisyNet noise sample is [array([0.82774055], dtype=float32), 0.3211051]. 
=============================================
[2019-04-08 15:31:58,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4326637e-14 2.8873853e-07 3.1332186e-06 1.3057339e-04 9.5105550e-09
 9.4177310e-10 1.6057477e-06 2.6347405e-10 1.7490299e-14 1.2740637e-16
 9.9986434e-01], sum to 1.0000
[2019-04-08 15:31:58,053] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8126
[2019-04-08 15:31:58,079] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.20053872741533, 0.5582616262907351, 0.0, 1.0, 65.0, 51430.64911615715], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3037200.0000, 
sim time next is 3037800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.18382324342481, 0.5553275034859283, 0.0, 1.0, 65.0, 51692.93383033871], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6819852702854009, 0.6851091678286427, 0.0, 1.0, 1.0, 0.2461568277635177], 
reward next is 0.7538, 
noisyNet noise sample is [array([-0.833904], dtype=float32), -1.1677192]. 
=============================================
[2019-04-08 15:31:58,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1751810e-13 1.1287372e-08 1.5474782e-04 2.4322284e-05 2.0322986e-08
 1.8930410e-10 1.7942757e-06 9.4632279e-12 1.9595092e-14 5.1105882e-17
 9.9981910e-01], sum to 1.0000
[2019-04-08 15:31:58,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0725
[2019-04-08 15:31:58,365] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 19.0, 26.69998699716081, 0.7412718562512461, 0.0, 1.0, 65.0, 36813.25434540791], 
current ob forecast is [], 
actual action is [2.333333333333333, 65.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 19.0, 26.71878547876773, 0.747771139035911, 0.0, 1.0, 65.0, 36551.08534505928], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 0.08333333333333333, 0.7265654565639776, 0.7492570463453037, 0.0, 1.0, 1.0, 0.17405278735742513], 
reward next is 0.8259, 
noisyNet noise sample is [array([1.5168834], dtype=float32), -0.232526]. 
=============================================
[2019-04-08 15:31:58,377] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[69.79865 ]
 [69.841446]
 [69.926216]
 [70.023026]
 [70.15774 ]], R is [[69.8568573 ]
 [69.98298645]
 [70.10723114]
 [70.23087311]
 [70.35318756]].
[2019-04-08 15:31:58,579] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9166269e-15 2.3283549e-08 1.3482467e-06 1.5192595e-06 1.2920762e-09
 2.1187916e-11 1.2143863e-07 4.2460137e-12 6.9880853e-16 1.6858944e-18
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:31:58,580] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5389
[2019-04-08 15:31:58,620] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.2140801566341, 0.5698749910809183, 0.0, 1.0, 65.0, 50848.02733183335], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3035400.0000, 
sim time next is 3036000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.23147386297953, 0.5650589454140234, 0.0, 1.0, 65.0, 50269.3908493729], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6859561552482942, 0.6883529818046744, 0.0, 1.0, 1.0, 0.23937805166368048], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.49463212], dtype=float32), -1.5374911]. 
=============================================
[2019-04-08 15:31:58,640] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[71.78894 ]
 [71.69847 ]
 [71.759384]
 [71.656815]
 [71.64627 ]], R is [[71.90280914]
 [71.94165039]
 [71.98179626]
 [72.02324677]
 [72.0657196 ]].
[2019-04-08 15:31:58,740] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7239764e-15 5.2298086e-09 2.8328500e-07 1.1737583e-06 1.0533392e-09
 1.0535129e-11 3.1172032e-08 6.4180701e-12 2.9837802e-16 1.9019221e-18
 9.9999845e-01], sum to 1.0000
[2019-04-08 15:31:58,741] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.08310094e-14 1.17465850e-08 1.29234752e-06 1.62329743e-05
 4.26243751e-09 1.40887912e-10 1.94400215e-07 2.27586461e-12
 1.76091818e-15 3.20283407e-17 9.99982238e-01], sum to 1.0000
[2019-04-08 15:31:58,741] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2278
[2019-04-08 15:31:58,742] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5682
[2019-04-08 15:31:58,765] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.833333333333333, 54.16666666666666, 109.3333333333333, 789.6666666666667, 19.0, 26.63552310868382, 0.7098256463191163, 0.0, 1.0, 65.0, 35920.69686393256], 
current ob forecast is [], 
actual action is [1.166666666666667, 65.0], 
sim time this is 3064200.0000, 
sim time next is 3064800.0000, 
raw observation next is [-3.666666666666667, 54.33333333333334, 110.1666666666667, 797.3333333333334, 19.0, 26.68770275422873, 0.7144003558866606, 0.0, 1.0, 65.0, 36264.02950453875], 
processed observation next is [0.0, 0.4782608695652174, 0.3610341643582641, 0.5433333333333334, 0.36722222222222234, 0.8810313075506446, 0.08333333333333333, 0.7239752295190609, 0.7381334519622201, 0.0, 1.0, 1.0, 0.17268585478351783], 
reward next is 0.8273, 
noisyNet noise sample is [array([-0.06968636], dtype=float32), -0.2907328]. 
=============================================
[2019-04-08 15:31:58,776] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.95962131834998, 0.7834131615334495, 0.0, 1.0, 65.0, 39652.34024466443], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3002400.0000, 
sim time next is 3003000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 26.92573485563869, 0.7795889375964081, 0.0, 1.0, 65.0, 41608.7229867509], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7438112379698909, 0.7598629791988026, 0.0, 1.0, 1.0, 0.19813677612738526], 
reward next is 0.8019, 
noisyNet noise sample is [array([0.48121813], dtype=float32), -0.29552796]. 
=============================================
[2019-04-08 15:31:58,782] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[69.74881]
 [69.74804]
 [69.82525]
 [69.95435]
 [70.12624]], R is [[69.90757751]
 [70.01968384]
 [70.13510132]
 [70.24237061]
 [70.35139465]].
[2019-04-08 15:31:59,439] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4975195e-13 3.9228641e-07 8.8267399e-05 3.3277590e-03 2.7944434e-08
 1.2272729e-09 5.5972700e-06 3.4714881e-10 1.1982150e-12 4.2417923e-15
 9.9657792e-01], sum to 1.0000
[2019-04-08 15:31:59,439] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0076
[2019-04-08 15:31:59,459] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 56.0, 57.0, 486.0, 19.0, 27.27300289740537, 0.8543638446647509, 0.0, 1.0, 65.0, 32431.42099566287], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 3083400.0000, 
sim time next is 3084000.0000, 
raw observation next is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 19.0, 27.26442407604758, 0.847241357605113, 0.0, 1.0, 65.0, 33071.57331837065], 
processed observation next is [0.0, 0.6956521739130435, 0.4718374884579871, 0.6133333333333334, 0.16222222222222218, 0.4637200736648251, 0.08333333333333333, 0.7720353396706315, 0.782413785868371, 0.0, 1.0, 1.0, 0.15748368246843164], 
reward next is 0.8425, 
noisyNet noise sample is [array([0.6241252], dtype=float32), -0.42991132]. 
=============================================
[2019-04-08 15:31:59,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.30372 ]
 [70.266426]
 [70.227486]
 [70.267784]
 [70.33313 ]], R is [[70.54603577]
 [70.68614197]
 [70.82653809]
 [70.96730042]
 [71.11190796]].
[2019-04-08 15:32:01,474] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5441131e-14 2.8971264e-08 1.8542856e-05 5.9667433e-04 3.2599543e-09
 1.4479425e-10 1.6848118e-06 2.3342033e-11 1.0029883e-15 2.0464728e-17
 9.9938309e-01], sum to 1.0000
[2019-04-08 15:32:01,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9910
[2019-04-08 15:32:01,494] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 19.0, 26.69998648987855, 0.7412722928976537, 0.0, 1.0, 65.0, 36813.2442194996], 
current ob forecast is [], 
actual action is [2.333333333333333, 65.0], 
sim time this is 3068400.0000, 
sim time next is 3069000.0000, 
raw observation next is [-2.5, 52.5, 114.0, 817.0, 19.0, 26.71878574359119, 0.747771643731077, 0.0, 1.0, 65.0, 36551.07311830862], 
processed observation next is [0.0, 0.5217391304347826, 0.39335180055401664, 0.525, 0.38, 0.9027624309392265, 0.08333333333333333, 0.7265654786325992, 0.7492572145770257, 0.0, 1.0, 1.0, 0.17405272913480294], 
reward next is 0.8259, 
noisyNet noise sample is [array([-0.21242495], dtype=float32), 0.047547806]. 
=============================================
[2019-04-08 15:32:01,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.52492 ]
 [72.55877 ]
 [72.629524]
 [72.71343 ]
 [72.835335]], R is [[72.56459045]
 [72.66364288]
 [72.76107788]
 [72.85818481]
 [72.95422363]].
[2019-04-08 15:32:01,627] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3549228e-14 3.9783430e-08 6.1689731e-05 4.0443128e-06 1.1147914e-09
 4.0478908e-11 3.7596092e-07 5.6143766e-12 1.5784245e-15 1.6846308e-18
 9.9993384e-01], sum to 1.0000
[2019-04-08 15:32:01,628] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2599
[2019-04-08 15:32:01,704] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 77.0, 7.0, 88.0, 19.0, 27.14434241777727, 0.8018120697820056, 0.0, 1.0, 65.0, 37156.30035208], 
current ob forecast is [], 
actual action is [4.7, 65.0], 
sim time this is 3087000.0000, 
sim time next is 3087600.0000, 
raw observation next is [-0.4, 78.66666666666667, 0.0, 0.0, 19.0, 27.10734114198365, 0.7883881614931137, 0.0, 1.0, 65.0, 38294.39252400216], 
processed observation next is [0.0, 0.7391304347826086, 0.45152354570637127, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.7589450951653042, 0.762796053831038, 0.0, 1.0, 1.0, 0.182354250114296], 
reward next is 0.8176, 
noisyNet noise sample is [array([-1.7090508], dtype=float32), 0.14498976]. 
=============================================
[2019-04-08 15:32:02,473] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0956126e-18 4.2244352e-10 4.2086464e-07 1.0831358e-07 3.0077777e-11
 4.1189659e-13 8.6655358e-09 2.1947620e-14 2.6956296e-19 1.3756057e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:32:02,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1993
[2019-04-08 15:32:02,518] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.88271344484816, 0.7153169948959842, 0.0, 1.0, 65.0, 43929.56582063061], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3100800.0000, 
sim time next is 3101400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.86659952975054, 0.7134954220633475, 0.0, 1.0, 65.0, 44029.99188448317], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7388832941458784, 0.7378318073544491, 0.0, 1.0, 1.0, 0.20966662802134842], 
reward next is 0.7903, 
noisyNet noise sample is [array([0.07605459], dtype=float32), 0.9790613]. 
=============================================
[2019-04-08 15:32:03,344] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.81338576e-16 7.92007182e-10 3.59701971e-07 4.60556384e-06
 5.94160054e-09 8.52978885e-12 5.42396386e-08 1.00010664e-13
 1.24891170e-17 2.43787161e-19 9.99994993e-01], sum to 1.0000
[2019-04-08 15:32:03,364] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6142
[2019-04-08 15:32:03,393] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 19.0, 27.26442844983314, 0.8472426551073565, 0.0, 1.0, 65.0, 33071.52880040255], 
current ob forecast is [], 
actual action is [5.333333333333333, 65.0], 
sim time this is 3084000.0000, 
sim time next is 3084600.0000, 
raw observation next is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 19.0, 27.24733655343998, 0.8411324275180395, 0.0, 1.0, 65.0, 33610.21080373161], 
processed observation next is [0.0, 0.6956521739130435, 0.4672206832871654, 0.6666666666666667, 0.13444444444444448, 0.39042357274401485, 0.08333333333333333, 0.7706113794533316, 0.7803774758393466, 0.0, 1.0, 1.0, 0.16004862287491242], 
reward next is 0.8400, 
noisyNet noise sample is [array([0.71145433], dtype=float32), 1.5368849]. 
=============================================
[2019-04-08 15:32:03,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.0934308e-16 8.3633482e-09 1.1717561e-06 2.7012173e-05 3.6108394e-10
 1.3395529e-11 1.6213941e-07 5.6214115e-13 1.9165117e-16 4.3599723e-19
 9.9997163e-01], sum to 1.0000
[2019-04-08 15:32:03,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2976
[2019-04-08 15:32:03,413] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.333333333333333, 87.5, 112.3333333333333, 815.6666666666666, 22.5, 28.12819157870695, 1.13439170450981, 1.0, 1.0, 65.0, 21520.75930205362], 
current ob forecast is [], 
actual action is [2.666666666666667, 65.0], 
sim time this is 3244200.0000, 
sim time next is 3244800.0000, 
raw observation next is [-2.666666666666667, 90.0, 111.6666666666667, 813.8333333333334, 22.5, 28.16219347360318, 1.148467201532091, 1.0, 1.0, 65.0, 21793.56822081581], 
processed observation next is [1.0, 0.5652173913043478, 0.38873499538319484, 0.9, 0.37222222222222234, 0.8992633517495396, 0.375, 0.8468494561335982, 0.8828224005106969, 1.0, 1.0, 1.0, 0.1037788962895991], 
reward next is 0.8962, 
noisyNet noise sample is [array([-0.08149742], dtype=float32), -0.8034352]. 
=============================================
[2019-04-08 15:32:04,983] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.9785618e-14 5.6399713e-08 3.0965992e-05 2.1369948e-05 4.7966049e-09
 3.8407031e-11 1.0966887e-06 1.2653511e-11 2.9808807e-14 1.1349462e-18
 9.9994648e-01], sum to 1.0000
[2019-04-08 15:32:04,983] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7967
[2019-04-08 15:32:05,011] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.09999999999999999, 73.66666666666667, 23.66666666666666, 220.6666666666666, 19.0, 27.21352268077461, 0.8239056369002838, 0.0, 1.0, 65.0, 35076.82579410126], 
current ob forecast is [], 
actual action is [4.9, 65.0], 
sim time this is 3085800.0000, 
sim time next is 3086400.0000, 
raw observation next is [-0.2, 75.33333333333334, 15.33333333333333, 154.3333333333333, 19.0, 27.17729373383501, 0.8131609423487105, 0.0, 1.0, 65.0, 36003.88957503876], 
processed observation next is [0.0, 0.7391304347826086, 0.4570637119113574, 0.7533333333333334, 0.0511111111111111, 0.17053406998158374, 0.08333333333333333, 0.7647744778195843, 0.7710536474495702, 0.0, 1.0, 1.0, 0.1714470932144703], 
reward next is 0.8286, 
noisyNet noise sample is [array([-0.46560565], dtype=float32), 1.3761036]. 
=============================================
[2019-04-08 15:32:05,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6600158e-15 7.8132262e-10 4.9600067e-07 6.4976268e-07 8.7420576e-10
 2.4354924e-11 4.9481262e-08 4.4748038e-13 1.4648911e-16 8.4199862e-19
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:32:05,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9566
[2019-04-08 15:32:05,045] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 80.33333333333334, 0.0, 0.0, 19.0, 27.06692655177905, 0.7818960336979309, 0.0, 1.0, 65.0, 39526.89200952004], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 3088200.0000, 
sim time next is 3088800.0000, 
raw observation next is [-0.6, 82.0, 0.0, 0.0, 19.0, 27.05319647701077, 0.778424550592015, 0.0, 1.0, 65.0, 40234.45354227876], 
processed observation next is [0.0, 0.782608695652174, 0.44598337950138506, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7544330397508974, 0.7594748501973383, 0.0, 1.0, 1.0, 0.19159263591561315], 
reward next is 0.8084, 
noisyNet noise sample is [array([1.7997249], dtype=float32), -1.0234119]. 
=============================================
[2019-04-08 15:32:05,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6667853e-19 7.0523166e-12 8.2364164e-07 3.9521169e-07 1.4988073e-11
 6.2547331e-15 3.5766050e-09 7.3981010e-15 1.3948550e-20 3.5712014e-23
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:32:05,080] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8438
[2019-04-08 15:32:05,107] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 100.0, 102.8333333333333, 770.1666666666667, 22.5, 28.41628896583062, 1.327083513923593, 1.0, 1.0, 65.0, 40679.74522430015], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 3162000.0000, 
sim time next is 3162600.0000, 
raw observation next is [7.0, 100.0, 101.0, 763.0, 22.5, 27.8729419775794, 1.282432387617855, 1.0, 1.0, 64.99999999999999, 22517.44148035758], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.33666666666666667, 0.8430939226519337, 0.375, 0.8227451647982834, 0.9274774625392851, 1.0, 1.0, 0.9999999999999997, 0.10722591181122657], 
reward next is 0.8928, 
noisyNet noise sample is [array([-0.59829944], dtype=float32), -0.42688274]. 
=============================================
[2019-04-08 15:32:05,233] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.2401535e-20 1.7368461e-12 1.2157378e-08 3.0706071e-06 6.8177316e-13
 9.4790602e-15 9.9931405e-09 8.2142701e-17 1.1255512e-20 2.1383878e-23
 9.9999690e-01], sum to 1.0000
[2019-04-08 15:32:05,234] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3547
[2019-04-08 15:32:05,245] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.666666666666666, 99.16666666666666, 75.33333333333334, 619.0, 22.5, 29.26324351444023, 1.388396899622553, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [11.666666666666666, 65.0], 
sim time this is 3167400.0000, 
sim time next is 3168000.0000, 
raw observation next is [6.6, 99.0, 71.0, 589.0, 22.5, 29.29359374896769, 1.393920783286366, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.99, 0.23666666666666666, 0.6508287292817679, 0.375, 0.9411328124139743, 0.9646402610954553, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.87020385], dtype=float32), 0.7267991]. 
=============================================
[2019-04-08 15:32:05,298] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[92.329956]
 [92.36102 ]
 [92.393875]
 [92.46011 ]
 [92.34716 ]], R is [[92.39668274]
 [92.47271729]
 [92.54798889]
 [92.62251282]
 [92.69628906]].
[2019-04-08 15:32:05,879] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3608494e-20 3.8800515e-11 1.3178432e-09 3.9882860e-08 4.3194935e-12
 3.8968496e-15 4.6854653e-09 1.1804249e-15 1.0911831e-20 4.0775734e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:32:05,883] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4264
[2019-04-08 15:32:05,927] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 100.0, 101.0, 763.0, 22.5, 27.87293491815396, 1.282429275194415, 1.0, 1.0, 64.99999999999999, 22517.82354170845], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 3162600.0000, 
sim time next is 3163200.0000, 
raw observation next is [7.0, 100.0, 98.16666666666667, 749.0, 22.5, 28.5805747078148, 1.321959976877161, 1.0, 1.0, 65.00000000000003, 12565.10075734683], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.32722222222222225, 0.8276243093922652, 0.375, 0.8817145589845667, 0.9406533256257203, 1.0, 1.0, 1.0000000000000007, 0.059833813130223006], 
reward next is 0.9402, 
noisyNet noise sample is [array([0.3409368], dtype=float32), -1.1744508]. 
=============================================
[2019-04-08 15:32:06,012] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-08 15:32:06,012] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:32:06,013] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:32:06,025] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:32:06,026] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:32:06,028] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:32:06,028] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:32:06,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run12
[2019-04-08 15:32:06,057] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run12
[2019-04-08 15:32:06,057] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run12
[2019-04-08 15:32:15,891] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.061266646]
[2019-04-08 15:32:15,891] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-1.516666666666667, 91.0, 0.0, 0.0, 19.0, 26.24830479151615, 0.6139594452356802, 0.0, 1.0, 65.0, 52892.57747641546]
[2019-04-08 15:32:15,891] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:32:15,892] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [4.5496674e-17 3.5484304e-10 2.5201754e-07 1.1546014e-06 2.9915340e-10
 5.1371819e-13 5.2555510e-08 3.5338746e-14 3.5633034e-18 1.5165167e-20
 9.9999857e-01], sampled 0.5797465205525463
[2019-04-08 15:33:27,772] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.061266646]
[2019-04-08 15:33:27,772] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [4.0, 59.0, 0.0, 0.0, 19.0, 27.87205434715433, 0.9966217286818332, 0.0, 1.0, 65.0, 28311.43951750831]
[2019-04-08 15:33:27,772] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:33:27,773] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [2.2545751e-16 1.2959657e-09 5.8819228e-07 2.6189630e-06 6.4165961e-10
 2.0394400e-12 8.6351911e-08 1.6013724e-13 3.2146680e-17 1.7204104e-19
 9.9999666e-01], sampled 0.9960306763941437
[2019-04-08 15:33:32,461] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.061266646]
[2019-04-08 15:33:32,461] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [7.866666666666666, 81.16666666666667, 173.0, 280.0, 22.5, 28.27622748442461, 1.152721243241719, 1.0, 1.0, 65.0, 18847.20174640029]
[2019-04-08 15:33:32,461] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:33:32,462] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [7.6109224e-19 7.0001935e-11 6.7945315e-08 2.9080530e-07 2.3686038e-11
 2.9600305e-14 6.5072197e-09 1.1098584e-15 6.0129284e-20 1.8409449e-22
 9.9999964e-01], sampled 0.7478393957667036
[2019-04-08 15:33:41,648] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.6140 316131064.7509 2958.1777
[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,669] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:41,806] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:47,952] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6800.8527 355980929.4075 2370.5733
[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:47,985] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:48,102] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,035] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3968 342846678.2579 2768.2239
[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,056] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:52,166] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:33:53,058] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 220000, evaluation results [220000.0, 6863.396770200469, 342846678.25790673, 2768.223939251608, 6990.613977376838, 316131064.75086564, 2958.1776522580626, 6800.852717107343, 355980929.40745914, 2370.573292155841]
[2019-04-08 15:33:53,331] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9232419e-18 2.8748143e-10 2.8177902e-07 5.5836426e-07 5.4734779e-11
 1.8296458e-13 7.3415474e-09 8.3742278e-16 7.6139019e-21 5.5860450e-22
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:33:53,331] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4150
[2019-04-08 15:33:53,338] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.833333333333334, 94.16666666666666, 113.3333333333333, 817.0, 22.5, 28.64124971101707, 1.19132961182916, 1.0, 1.0, 65.0, 18848.91775176354], 
current ob forecast is [], 
actual action is [12.833333333333334, 65.0], 
sim time this is 3154200.0000, 
sim time next is 3154800.0000, 
raw observation next is [7.666666666666667, 95.33333333333334, 113.1666666666667, 820.0, 22.5, 28.69695063555237, 1.199416766243295, 1.0, 1.0, 65.0, 18849.09486309669], 
processed observation next is [1.0, 0.5217391304347826, 0.674976915974146, 0.9533333333333335, 0.37722222222222235, 0.9060773480662984, 0.375, 0.8914125529626974, 0.899805588747765, 1.0, 1.0, 1.0, 0.0897575945861747], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.8905188], dtype=float32), -0.3905395]. 
=============================================
[2019-04-08 15:33:53,476] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8986783e-18 6.6567113e-10 1.3404502e-08 1.0118810e-06 1.3156309e-10
 1.9464515e-13 1.3121056e-09 1.0765077e-14 1.8099317e-19 2.7532535e-20
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:33:53,477] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4247
[2019-04-08 15:33:53,523] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 19.0, 26.80682960686153, 0.6893632812946658, 0.0, 1.0, 65.0, 44092.10701044727], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3108600.0000, 
sim time next is 3109200.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 26.79517008238724, 0.6871364946402405, 0.0, 1.0, 65.0, 44154.45858985975], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7329308401989367, 0.7290454982134135, 0.0, 1.0, 1.0, 0.21025932661837976], 
reward next is 0.7897, 
noisyNet noise sample is [array([1.5637677], dtype=float32), 0.19313312]. 
=============================================
[2019-04-08 15:33:54,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3884474e-16 1.5452537e-09 1.8879044e-05 1.0703723e-05 4.3185637e-09
 8.1619970e-13 2.8249730e-07 7.5881630e-14 1.9346142e-17 1.9920070e-19
 9.9997008e-01], sum to 1.0000
[2019-04-08 15:33:54,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7303
[2019-04-08 15:33:54,813] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 19.0, 26.18506001946685, 0.5976506158155336, 0.0, 1.0, 65.0, 56713.4014836842], 
current ob forecast is [], 
actual action is [-6.0, 65.0], 
sim time this is 3305400.0000, 
sim time next is 3306000.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 26.19508756765901, 0.5857978549, 0.0, 1.0, 65.0, 55396.06357721095], 
processed observation next is [1.0, 0.2608695652173913, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6829239639715841, 0.6952659516333334, 0.0, 1.0, 1.0, 0.2637907789390998], 
reward next is 0.7362, 
noisyNet noise sample is [array([0.7977055], dtype=float32), 0.45468155]. 
=============================================
[2019-04-08 15:33:54,839] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.94562 ]
 [82.046265]
 [82.1275  ]
 [82.22073 ]
 [82.30067 ]], R is [[81.79967499]
 [81.71161652]
 [81.62792206]
 [81.55177307]
 [81.47396851]].
[2019-04-08 15:33:54,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.00807677e-18 1.43734885e-11 1.39951823e-07 2.02074702e-07
 1.00074254e-10 9.84114117e-14 3.43209514e-08 8.17061110e-15
 8.27242199e-20 6.27318752e-22 9.99999642e-01], sum to 1.0000
[2019-04-08 15:33:54,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5493
[2019-04-08 15:33:54,872] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 27.36428153447744, 1.061045621244959, 0.0, 1.0, 65.0, 35560.25559438451], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3194400.0000, 
sim time next is 3195000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 19.0, 27.34690168479035, 1.056976069216222, 0.0, 1.0, 65.0, 35710.24399878228], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7789084737325291, 0.8523253564054073, 0.0, 1.0, 1.0, 0.1700487809465823], 
reward next is 0.8300, 
noisyNet noise sample is [array([-0.6287132], dtype=float32), 1.5178361]. 
=============================================
[2019-04-08 15:33:54,894] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[91.9002  ]
 [91.51291 ]
 [91.668755]
 [91.94107 ]
 [91.883965]], R is [[91.68781281]
 [91.60160065]
 [91.5169754 ]
 [91.43437195]
 [91.35364532]].
[2019-04-08 15:33:55,108] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.8294077e-19 2.1533995e-11 1.4655411e-08 3.8072017e-08 1.1853267e-12
 5.3157421e-16 1.6278852e-07 1.0134717e-15 2.7407020e-20 2.5097823e-23
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:33:55,108] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7002
[2019-04-08 15:33:55,135] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 100.0, 1.0, 82.0, 22.5, 26.94165327341505, 0.7100179822132602, 1.0, 1.0, 65.0, 50269.12453847234], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 3137400.0000, 
sim time next is 3138000.0000, 
raw observation next is [6.0, 100.0, 14.66666666666666, 133.6666666666667, 22.5, 27.04397343828585, 0.7222004778512611, 1.0, 1.0, 65.0, 45428.70283870376], 
processed observation next is [1.0, 0.30434782608695654, 0.6288088642659281, 1.0, 0.04888888888888887, 0.1476979742173113, 0.375, 0.7536644531904875, 0.7407334926170871, 1.0, 1.0, 1.0, 0.21632715637477978], 
reward next is 0.7837, 
noisyNet noise sample is [array([1.3059319], dtype=float32), 0.8164439]. 
=============================================
[2019-04-08 15:33:55,147] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[89.606766]
 [89.17351 ]
 [88.581535]
 [88.320435]
 [88.23704 ]], R is [[90.11547852]
 [89.97494507]
 [89.83448029]
 [89.69329834]
 [89.54076385]].
[2019-04-08 15:33:55,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6008898e-19 4.9822993e-12 1.6633884e-07 2.4763079e-07 1.5789472e-11
 2.0014782e-14 9.2058694e-10 2.1722429e-15 3.7381316e-19 1.5083997e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:33:55,152] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9151
[2019-04-08 15:33:55,166] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 98.83333333333334, 0.0, 0.0, 19.0, 27.72231289533577, 1.103353076136268, 0.0, 1.0, 65.0, 28564.94066946515], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3190200.0000, 
sim time next is 3190800.0000, 
raw observation next is [2.0, 97.66666666666667, 0.0, 0.0, 19.0, 27.6503369749497, 1.104006651249584, 0.0, 1.0, 65.0, 35123.99784057384], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8041947479124749, 0.8680022170831947, 0.0, 1.0, 1.0, 0.16725713257416114], 
reward next is 0.8327, 
noisyNet noise sample is [array([0.72111803], dtype=float32), -0.08284142]. 
=============================================
[2019-04-08 15:33:56,680] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.7084865e-18 1.4627630e-10 3.2996588e-06 1.2686087e-06 2.7469043e-11
 3.2578558e-12 1.0240844e-08 5.0002757e-14 4.5146591e-18 1.3702635e-20
 9.9999547e-01], sum to 1.0000
[2019-04-08 15:33:56,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2645
[2019-04-08 15:33:56,693] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 100.0, 106.0, 790.5, 22.5, 28.41054515175882, 1.181965276803345, 1.0, 1.0, 65.0, 20043.55832195554], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3247200.0000, 
sim time next is 3247800.0000, 
raw observation next is [-3.666666666666667, 95.16666666666667, 104.3333333333333, 783.3333333333334, 22.5, 28.42432957270411, 1.19110950576498, 1.0, 1.0, 65.0, 20414.7560403936], 
processed observation next is [1.0, 0.6086956521739131, 0.3610341643582641, 0.9516666666666667, 0.3477777777777777, 0.8655616942909761, 0.375, 0.8686941310586759, 0.8970365019216601, 1.0, 1.0, 1.0, 0.09721312400187429], 
reward next is 0.9028, 
noisyNet noise sample is [array([-0.76513994], dtype=float32), -0.64211714]. 
=============================================
[2019-04-08 15:33:57,221] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.6980169e-17 1.3344700e-09 7.9878072e-07 4.0832547e-06 1.1377169e-11
 3.8477934e-13 1.0983298e-08 1.6815750e-14 2.1943160e-19 9.1994399e-21
 9.9999511e-01], sum to 1.0000
[2019-04-08 15:33:57,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3340
[2019-04-08 15:33:57,242] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 98.83333333333334, 0.0, 0.0, 19.0, 27.72227069025454, 1.103339427818263, 0.0, 1.0, 65.0, 28565.36947148692], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3190200.0000, 
sim time next is 3190800.0000, 
raw observation next is [2.0, 97.66666666666667, 0.0, 0.0, 19.0, 27.65029165767388, 1.103992558040395, 0.0, 1.0, 65.0, 35124.3216373601], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9766666666666667, 0.0, 0.0, 0.08333333333333333, 0.8041909714728233, 0.8679975193467984, 0.0, 1.0, 1.0, 0.16725867446361953], 
reward next is 0.8327, 
noisyNet noise sample is [array([-0.43864977], dtype=float32), -0.8513346]. 
=============================================
[2019-04-08 15:33:57,405] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5673284e-15 3.3600120e-09 9.1073400e-07 2.8848121e-05 9.5131947e-09
 2.6890861e-11 1.9001773e-08 6.1179767e-13 2.3384751e-16 9.7022382e-19
 9.9997020e-01], sum to 1.0000
[2019-04-08 15:33:57,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8227
[2019-04-08 15:33:57,480] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 22.5, 27.67323813951185, 0.7961532924198891, 1.0, 1.0, 65.0, 43025.14519541969], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3348600.0000, 
sim time next is 3349200.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 22.5, 26.89098013139665, 0.9198591512413355, 1.0, 1.0, 65.0, 62832.82934353512], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.375, 0.7409150109497208, 0.8066197170804452, 1.0, 1.0, 1.0, 0.29920394925492916], 
reward next is 0.7008, 
noisyNet noise sample is [array([-0.0409819], dtype=float32), 0.5541202]. 
=============================================
[2019-04-08 15:33:57,513] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3088392e-17 1.4173337e-11 1.2500534e-07 3.4475408e-07 4.5005822e-12
 3.5526049e-14 3.5133887e-09 2.7130166e-16 8.9962623e-19 9.6494668e-22
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:33:57,515] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8055
[2019-04-08 15:33:57,526] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.333333333333333, 75.83333333333334, 95.66666666666667, 741.3333333333334, 22.5, 28.15678996053073, 1.167053945382233, 1.0, 1.0, 65.0, 18844.83218969279], 
current ob forecast is [], 
actual action is [2.666666666666667, 65.0], 
sim time this is 3250200.0000, 
sim time next is 3250800.0000, 
raw observation next is [-2.0, 71.0, 93.0, 727.5, 22.5, 28.38704876161246, 1.180344132759134, 1.0, 1.0, 65.0, 18845.35266619367], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.71, 0.31, 0.8038674033149171, 0.375, 0.8655873968010382, 0.8934480442530447, 1.0, 1.0, 1.0, 0.08973977460092224], 
reward next is 0.9103, 
noisyNet noise sample is [array([-0.5317684], dtype=float32), 1.0641617]. 
=============================================
[2019-04-08 15:33:57,571] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5806454e-17 1.3322056e-09 8.4015909e-07 1.0488747e-06 4.0855958e-10
 6.9240837e-14 3.3152130e-09 2.1623694e-14 5.2919717e-19 2.5239518e-21
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:33:57,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8523
[2019-04-08 15:33:57,587] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 27.84424899921287, 1.19590782757097, 0.0, 1.0, 65.0, 25867.69074873337], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3184200.0000, 
sim time next is 3184800.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 19.0, 27.81407516696579, 1.190638066478426, 0.0, 1.0, 65.0, 26270.24139259035], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8178395972471492, 0.8968793554928087, 0.0, 1.0, 1.0, 0.12509638758376357], 
reward next is 0.8749, 
noisyNet noise sample is [array([-0.23358779], dtype=float32), -0.73175293]. 
=============================================
[2019-04-08 15:33:59,209] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3706365e-18 3.1189359e-10 1.8554940e-07 3.5701873e-06 1.8969756e-11
 5.8764522e-13 3.2141354e-08 1.3921126e-14 1.4740739e-17 5.6304225e-21
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:33:59,213] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7036
[2019-04-08 15:33:59,228] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 100.0, 0.0, 0.0, 19.0, 27.06022058530178, 0.9207881016438276, 0.0, 1.0, 65.0, 42164.90920038208], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 3213000.0000, 
sim time next is 3213600.0000, 
raw observation next is [-1.666666666666667, 100.0, 0.0, 0.0, 19.0, 27.03293184394288, 0.915852403526619, 0.0, 1.0, 65.0, 42510.99882738973], 
processed observation next is [1.0, 0.17391304347826086, 0.4164358264081256, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7527443203285733, 0.8052841345088729, 0.0, 1.0, 1.0, 0.20243332774947492], 
reward next is 0.7976, 
noisyNet noise sample is [array([-1.2512463], dtype=float32), -1.11122]. 
=============================================
[2019-04-08 15:33:59,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2526058e-16 4.2297715e-10 4.9779641e-07 2.0947809e-07 4.1022399e-10
 3.2923998e-12 1.5424469e-07 2.6992490e-13 1.8213961e-18 5.4657510e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:33:59,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7600
[2019-04-08 15:33:59,707] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 96.0, 0.0, 0.0, 19.0, 26.86227112822916, 0.8605574330947935, 0.0, 1.0, 65.0, 45665.82035910225], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3220200.0000, 
sim time next is 3220800.0000, 
raw observation next is [-3.0, 94.66666666666666, 0.0, 0.0, 19.0, 26.81435763531469, 0.8544625846742031, 0.0, 1.0, 65.0, 46235.19249888218], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9466666666666665, 0.0, 0.0, 0.08333333333333333, 0.7345298029428907, 0.7848208615580677, 0.0, 1.0, 1.0, 0.22016758332801037], 
reward next is 0.7798, 
noisyNet noise sample is [array([0.44323847], dtype=float32), 0.29456258]. 
=============================================
[2019-04-08 15:33:59,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.06769489e-16 3.65774699e-09 4.42017374e-08 7.45439621e-08
 2.75963169e-10 1.12027046e-13 2.15216716e-08 3.42598454e-13
 4.94041148e-18 2.92646872e-20 9.99999881e-01], sum to 1.0000
[2019-04-08 15:33:59,830] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4600
[2019-04-08 15:33:59,851] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.82983923783578, 0.8132868074869418, 0.0, 1.0, 65.0, 46551.6551687047], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3279000.0000, 
sim time next is 3279600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.84208654983039, 0.8040029295534209, 0.0, 1.0, 65.0, 46051.98826579985], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7368405458191992, 0.768000976517807, 0.0, 1.0, 1.0, 0.21929518221809455], 
reward next is 0.7807, 
noisyNet noise sample is [array([-0.804399], dtype=float32), 0.79700804]. 
=============================================
[2019-04-08 15:33:59,954] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1407837e-16 2.7914646e-10 9.9131751e-07 5.0327634e-07 9.6467723e-10
 1.0040919e-12 1.1456657e-07 4.1818447e-14 5.5584610e-18 4.5932585e-19
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:33:59,973] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4439
[2019-04-08 15:33:59,994] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 22.5, 26.64621736580447, 0.6780064052170073, 1.0, 1.0, 65.00000000000003, 41482.68040436568], 
current ob forecast is [], 
actual action is [-4.666666666666668, 65.0], 
sim time this is 3314400.0000, 
sim time next is 3315000.0000, 
raw observation next is [-9.333333333333332, 78.16666666666667, 92.0, 469.0, 22.5, 26.7187431519549, 0.7050257159192763, 1.0, 1.0, 65.0, 42269.85300417762], 
processed observation next is [1.0, 0.34782608695652173, 0.20406278855032323, 0.7816666666666667, 0.30666666666666664, 0.518232044198895, 0.375, 0.726561929329575, 0.7350085719730921, 1.0, 1.0, 1.0, 0.2012850143056077], 
reward next is 0.7987, 
noisyNet noise sample is [array([-1.4399941], dtype=float32), 1.760392]. 
=============================================
[2019-04-08 15:34:00,017] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.62119]
 [77.74951]
 [77.47726]
 [77.42502]
 [76.80499]], R is [[77.61317444]
 [77.63950348]
 [77.66269684]
 [77.6398468 ]
 [77.61116791]].
[2019-04-08 15:34:01,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1284695e-14 9.5913011e-09 4.1927060e-06 7.5165270e-05 1.9229454e-09
 4.7050164e-10 5.7611174e-08 2.4034686e-13 5.8944844e-17 5.3202454e-18
 9.9992061e-01], sum to 1.0000
[2019-04-08 15:34:01,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8220
[2019-04-08 15:34:01,482] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.5, 67.0, 111.0, 740.0, 22.5, 27.36240916402058, 0.8449430172636516, 1.0, 1.0, 65.0, 30508.25534393024], 
current ob forecast is [], 
actual action is [-2.5, 65.0], 
sim time this is 3321000.0000, 
sim time next is 3321600.0000, 
raw observation next is [-7.333333333333333, 66.0, 111.8333333333333, 749.6666666666667, 22.5, 27.41941164235563, 0.86146213092653, 1.0, 1.0, 65.0, 29055.15368073463], 
processed observation next is [1.0, 0.43478260869565216, 0.25946445060018475, 0.66, 0.37277777777777765, 0.8283609576427257, 0.375, 0.7849509701963026, 0.7871540436421767, 1.0, 1.0, 1.0, 0.13835787467016492], 
reward next is 0.8616, 
noisyNet noise sample is [array([0.2736129], dtype=float32), 0.13167195]. 
=============================================
[2019-04-08 15:34:02,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.8355213e-15 1.9973113e-08 2.8434952e-05 4.5060365e-06 6.8972330e-09
 1.3592542e-10 2.1572426e-07 2.3632735e-11 8.3673795e-16 5.0634478e-17
 9.9996686e-01], sum to 1.0000
[2019-04-08 15:34:02,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5491
[2019-04-08 15:34:03,003] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.833333333333333, 63.33333333333334, 0.0, 0.0, 19.0, 27.03283596551145, 0.8219444310332685, 0.0, 1.0, 65.0, 38018.13191365731], 
current ob forecast is [], 
actual action is [1.166666666666667, 65.0], 
sim time this is 3358200.0000, 
sim time next is 3358800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 19.0, 27.01007959401693, 0.815518145646274, 0.0, 1.0, 65.0, 38315.98344795782], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7508399661680775, 0.7718393818820913, 0.0, 1.0, 1.0, 0.18245706403789438], 
reward next is 0.8175, 
noisyNet noise sample is [array([0.16298172], dtype=float32), -0.4531067]. 
=============================================
[2019-04-08 15:34:03,211] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6302992e-14 1.5336219e-08 2.0782406e-06 4.3021178e-06 1.1753608e-10
 3.1861611e-11 2.7068502e-08 3.8217576e-12 2.7977634e-17 1.9902166e-18
 9.9999356e-01], sum to 1.0000
[2019-04-08 15:34:03,215] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4616
[2019-04-08 15:34:03,228] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 22.5, 27.98918645571269, 0.9862938526422745, 1.0, 1.0, 65.00000000000003, 18844.91337629566], 
current ob forecast is [], 
actual action is [1.666666666666667, 65.0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 22.5, 28.00746425678388, 1.007729025110147, 1.0, 1.0, 65.0, 19575.92735254611], 
processed observation next is [1.0, 0.6086956521739131, 0.3748845798707295, 0.5, 0.3322222222222222, 0.8022099447513812, 0.375, 0.83395535473199, 0.8359096750367158, 1.0, 1.0, 1.0, 0.093218701678791], 
reward next is 0.9068, 
noisyNet noise sample is [array([-1.1106629], dtype=float32), 0.12968591]. 
=============================================
[2019-04-08 15:34:03,259] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.54547287e-14 1.55193369e-08 2.10837038e-06 4.43624367e-06
 1.14038785e-10 3.09114991e-11 2.68721987e-08 3.74567261e-12
 2.82502462e-17 1.95269777e-18 9.99993443e-01], sum to 1.0000
[2019-04-08 15:34:03,264] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2411
[2019-04-08 15:34:03,282] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 96.5, 713.0, 22.5, 28.03542378703018, 1.014255821355862, 1.0, 1.0, 65.0, 21297.01155776806], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3337200.0000, 
sim time next is 3337800.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 93.33333333333334, 700.0, 22.5, 28.04692275663914, 1.012850910770994, 1.0, 1.0, 65.0, 21607.61998781694], 
processed observation next is [1.0, 0.6521739130434783, 0.3841181902123731, 0.4933333333333334, 0.3111111111111111, 0.7734806629834254, 0.375, 0.8372435630532618, 0.837616970256998, 1.0, 1.0, 1.0, 0.102893428513414], 
reward next is 0.8971, 
noisyNet noise sample is [array([-1.1106629], dtype=float32), 0.12968591]. 
=============================================
[2019-04-08 15:34:04,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6965403e-15 1.3638452e-08 4.7394528e-07 6.8356417e-06 4.0439929e-10
 3.5161013e-11 4.5120439e-07 3.3442037e-13 3.3998780e-15 2.3456268e-18
 9.9999225e-01], sum to 1.0000
[2019-04-08 15:34:04,428] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6411
[2019-04-08 15:34:04,451] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 52.0, 114.0, 800.0, 22.5, 27.58995912362866, 0.9494464764339877, 1.0, 1.0, 65.0, 22183.09096481097], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 3331800.0000, 
sim time next is 3332400.0000, 
raw observation next is [-4.333333333333334, 51.33333333333333, 112.6666666666667, 792.0, 22.5, 27.87109104105423, 0.9709238814670673, 1.0, 1.0, 65.00000000000003, 19530.08125891288], 
processed observation next is [1.0, 0.5652173913043478, 0.3425669436749769, 0.5133333333333333, 0.37555555555555564, 0.8751381215469614, 0.375, 0.8225909200878524, 0.8236412938223557, 1.0, 1.0, 1.0000000000000007, 0.0930003869472042], 
reward next is 0.9070, 
noisyNet noise sample is [array([-0.3722436], dtype=float32), 2.3475344]. 
=============================================
[2019-04-08 15:34:06,196] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0046625e-15 1.2733168e-09 5.9199151e-06 8.6188214e-05 2.1243016e-09
 5.8563016e-11 3.2153241e-07 2.1198472e-11 8.4230816e-17 2.7319259e-17
 9.9990761e-01], sum to 1.0000
[2019-04-08 15:34:06,197] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-08 15:34:06,224] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.51929682653305, 0.690313866239407, 0.0, 1.0, 65.0, 48592.27917081326], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3372000.0000, 
sim time next is 3372600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.63421936583181, 0.681113859276992, 0.0, 1.0, 65.0, 44423.54722939553], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7195182804859842, 0.7270379530923307, 0.0, 1.0, 1.0, 0.21154070109235967], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.5488078], dtype=float32), -1.3788295]. 
=============================================
[2019-04-08 15:34:06,269] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.0235043e-17 1.8853739e-10 9.8538230e-08 9.5304955e-07 1.8711695e-10
 1.2481548e-13 2.0801243e-09 1.8510077e-13 1.3514161e-17 4.8310620e-19
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:34:06,272] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7094
[2019-04-08 15:34:06,306] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 50.0, 110.0, 776.0, 22.5, 27.987483614338, 0.8265086633441873, 1.0, 1.0, 65.0, 37558.29836640538], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3333600.0000, 
sim time next is 3334200.0000, 
raw observation next is [-3.833333333333333, 50.0, 108.6666666666667, 768.0, 22.5, 27.12403880674876, 0.976675697066771, 1.0, 1.0, 65.0, 57244.97055772338], 
processed observation next is [1.0, 0.6086956521739131, 0.3564173591874424, 0.5, 0.36222222222222233, 0.8486187845303867, 0.375, 0.7603365672290634, 0.8255585656889237, 1.0, 1.0, 1.0, 0.2725950978939209], 
reward next is 0.7274, 
noisyNet noise sample is [array([-0.5926708], dtype=float32), -1.5729479]. 
=============================================
[2019-04-08 15:34:06,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.24476288e-15 9.66278080e-09 9.79679726e-07 8.04762476e-06
 9.97142702e-10 1.36897741e-12 1.09521075e-07 2.67399926e-13
 1.21778036e-18 1.69189403e-19 9.99990821e-01], sum to 1.0000
[2019-04-08 15:34:06,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5567
[2019-04-08 15:34:07,009] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.66072987235827, 0.6727951093214779, 0.0, 1.0, 65.0, 46673.24461706542], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3373800.0000, 
sim time next is 3374400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.59522855965746, 0.666775378606856, 0.0, 1.0, 65.0, 47973.51690011966], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7162690466381217, 0.7222584595356186, 0.0, 1.0, 1.0, 0.2284453185719984], 
reward next is 0.7716, 
noisyNet noise sample is [array([-0.9580447], dtype=float32), -1.3793764]. 
=============================================
[2019-04-08 15:34:08,357] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0011268e-15 1.3100948e-09 9.3050164e-07 1.9985930e-06 5.2316458e-09
 3.7613232e-11 1.1001165e-07 2.4887479e-12 4.8860652e-17 2.1234423e-20
 9.9999690e-01], sum to 1.0000
[2019-04-08 15:34:08,358] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2638
[2019-04-08 15:34:08,372] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.65563430542767, 0.6824183599565009, 0.0, 1.0, 65.0, 45306.52981344276], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3373200.0000, 
sim time next is 3373800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.66072961007584, 0.6727949284323363, 0.0, 1.0, 65.0, 46673.24542344699], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.72172746750632, 0.7242649761441121, 0.0, 1.0, 1.0, 0.22225354963546187], 
reward next is 0.7777, 
noisyNet noise sample is [array([0.18382718], dtype=float32), 1.9696122]. 
=============================================
[2019-04-08 15:34:08,780] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6641149e-18 4.0059435e-11 3.0270925e-08 1.2285872e-07 1.0809731e-11
 2.5955865e-13 1.1776611e-07 1.8560331e-13 4.9328403e-18 1.3388539e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:34:08,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4336
[2019-04-08 15:34:08,800] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 67.0, 72.5, 608.5, 22.5, 28.67774139923056, 1.161964485359449, 1.0, 1.0, 65.0, 18844.83244013332], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3427200.0000, 
sim time next is 3427800.0000, 
raw observation next is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 22.5, 28.68986595759364, 1.156633479880509, 1.0, 1.0, 65.0, 18844.52484207441], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.22888888888888886, 0.6372007366482505, 0.375, 0.8908221631328033, 0.8855444932935029, 1.0, 1.0, 1.0, 0.08973583258130671], 
reward next is 0.9103, 
noisyNet noise sample is [array([2.702983], dtype=float32), -0.71521705]. 
=============================================
[2019-04-08 15:34:09,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4990558e-16 5.2533449e-09 6.5400206e-07 2.1290798e-05 5.0463123e-10
 1.8407222e-11 1.1940060e-07 1.6466846e-13 3.6986643e-17 1.2121694e-20
 9.9997795e-01], sum to 1.0000
[2019-04-08 15:34:09,311] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0136
[2019-04-08 15:34:09,325] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 22.5, 27.76269524225025, 1.007370701232159, 1.0, 1.0, 65.0, 27977.58355279878], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3438000.0000, 
sim time next is 3438600.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 22.5, 27.69550452840574, 0.998105510451205, 1.0, 1.0, 65.0, 31706.64410161145], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.375, 0.8079587107004782, 0.8327018368170683, 1.0, 1.0, 1.0, 0.1509840195314831], 
reward next is 0.8490, 
noisyNet noise sample is [array([-0.19070327], dtype=float32), -0.7886682]. 
=============================================
[2019-04-08 15:34:09,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.6416506e-17 3.9071699e-10 2.5583179e-07 1.0220135e-06 7.5605522e-11
 2.2349149e-12 1.5980753e-08 4.1014081e-14 3.6107848e-18 3.2493156e-19
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:34:09,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0590
[2019-04-08 15:34:09,839] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 22.5, 26.49125149904111, 0.6162074894465249, 1.0, 1.0, 65.0, 48247.88549473634], 
current ob forecast is [], 
actual action is [2.666666666666667, 65.0], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 22.5, 26.53640825264952, 0.6357321236864477, 1.0, 1.0, 65.0, 47300.46810134752], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.6083333333333333, 0.1011111111111111, 0.23425414364640884, 0.375, 0.7113673543874599, 0.7119107078954826, 1.0, 1.0, 1.0, 0.22524032429213106], 
reward next is 0.7748, 
noisyNet noise sample is [array([0.58153695], dtype=float32), 0.21348184]. 
=============================================
[2019-04-08 15:34:10,391] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.2676974e-17 5.4784483e-11 3.3491480e-07 6.8750734e-07 2.0564999e-11
 3.5994233e-12 1.6742739e-08 4.8931468e-16 3.3896373e-19 4.7570998e-22
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:34:10,403] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9362
[2019-04-08 15:34:10,430] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 67.0, 72.5, 608.5, 22.5, 28.67774115794698, 1.161964408719604, 1.0, 1.0, 65.0, 18844.83243707803], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3427200.0000, 
sim time next is 3427800.0000, 
raw observation next is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 22.5, 28.6898657177587, 1.156633400309846, 1.0, 1.0, 65.0, 18844.52483827055], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.22888888888888886, 0.6372007366482505, 0.375, 0.8908221431465583, 0.8855444667699487, 1.0, 1.0, 1.0, 0.08973583256319309], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.7705159], dtype=float32), -1.9676198]. 
=============================================
[2019-04-08 15:34:10,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.9878101e-17 2.2339390e-10 1.9090328e-07 2.1752763e-07 1.3532607e-10
 6.5138025e-12 8.7856336e-08 2.5684279e-15 1.6045350e-17 1.8056916e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:34:10,627] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0418
[2019-04-08 15:34:10,653] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 26.8117486785255, 0.759801608942117, 0.0, 1.0, 65.0, 46208.31295498266], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3475200.0000, 
sim time next is 3475800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.83708801932352, 0.763190392075363, 0.0, 1.0, 65.0, 45247.69882203705], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7364240016102933, 0.7543967973584543, 0.0, 1.0, 1.0, 0.2154652324858907], 
reward next is 0.7845, 
noisyNet noise sample is [array([0.18079308], dtype=float32), 0.65974313]. 
=============================================
[2019-04-08 15:34:10,947] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.08496725e-15 1.05055626e-07 3.24527286e-06 1.07707074e-05
 3.48840956e-08 4.67225607e-12 1.76617004e-06 1.03678448e-12
 5.24662203e-15 3.38105410e-17 9.99984026e-01], sum to 1.0000
[2019-04-08 15:34:10,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9800
[2019-04-08 15:34:10,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.24970394e-16 2.10463147e-10 9.24706364e-06 1.39302765e-05
 3.17915227e-09 4.11056128e-12 5.76888567e-07 1.20172494e-13
 7.18175943e-17 1.36911513e-18 9.99976277e-01], sum to 1.0000
[2019-04-08 15:34:10,962] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5989
[2019-04-08 15:34:10,970] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8333333333333334, 42.16666666666666, 91.33333333333334, 728.6666666666667, 19.0, 27.28593620163184, 0.9528993235168993, 0.0, 1.0, 65.0, 31390.4849129802], 
current ob forecast is [], 
actual action is [4.166666666666667, 65.0], 
sim time this is 3597000.0000, 
sim time next is 3597600.0000, 
raw observation next is [-0.6666666666666667, 42.33333333333334, 88.66666666666667, 713.8333333333333, 19.0, 27.32680375815824, 0.9632657473540899, 0.0, 1.0, 65.0, 30387.81949271846], 
processed observation next is [0.0, 0.6521739130434783, 0.44413665743305636, 0.42333333333333345, 0.29555555555555557, 0.7887661141804787, 0.08333333333333333, 0.7772336465131865, 0.8210885824513633, 0.0, 1.0, 1.0, 0.1447039023462784], 
reward next is 0.8553, 
noisyNet noise sample is [array([0.25494942], dtype=float32), -0.929686]. 
=============================================
[2019-04-08 15:34:10,988] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 75.5, 0.0, 0.0, 19.0, 27.10548615247036, 0.81203528024973, 0.0, 1.0, 65.0, 40397.29914094815], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3465000.0000, 
sim time next is 3465600.0000, 
raw observation next is [1.0, 74.33333333333334, 0.0, 0.0, 19.0, 27.0567850843031, 0.8041432182115581, 0.0, 1.0, 65.0, 41917.11017056703], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7433333333333334, 0.0, 0.0, 0.08333333333333333, 0.7547320903585918, 0.7680477394038526, 0.0, 1.0, 1.0, 0.19960528652650966], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.5741656], dtype=float32), -1.1892943]. 
=============================================
[2019-04-08 15:34:11,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.5178432e-17 4.3569557e-09 4.0248247e-07 2.4952507e-07 5.3792209e-12
 1.4798988e-13 2.5719956e-08 3.4111489e-13 1.3637412e-18 1.1912136e-21
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:34:11,371] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7904
[2019-04-08 15:34:11,391] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 97.33333333333334, 749.6666666666667, 22.5, 27.59025618438183, 1.138852821717565, 1.0, 1.0, 65.00000000000018, 24645.60742054234], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3509400.0000, 
sim time next is 3510000.0000, 
raw observation next is [3.0, 49.0, 95.0, 734.0, 22.5, 28.2784022118558, 1.187173782491779, 1.0, 1.0, 65.0, 18846.00513826313], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.31666666666666665, 0.8110497237569061, 0.375, 0.8565335176546499, 0.8957245941639264, 1.0, 1.0, 1.0, 0.08974288161077681], 
reward next is 0.9103, 
noisyNet noise sample is [array([1.0159833], dtype=float32), -0.9151529]. 
=============================================
[2019-04-08 15:34:11,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.22981]
 [79.2684 ]
 [79.29846]
 [79.37396]
 [79.3995 ]], R is [[79.29039764]
 [79.38013458]
 [79.38955688]
 [79.4790802 ]
 [79.59453583]].
[2019-04-08 15:34:11,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.02864220e-14 2.28084964e-08 5.93567938e-06 9.89750843e-05
 1.19726495e-08 3.63384219e-11 3.02368335e-07 3.16773929e-12
 2.50165315e-15 7.63470169e-17 9.99894738e-01], sum to 1.0000
[2019-04-08 15:34:11,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9117
[2019-04-08 15:34:11,477] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.33694705856728, 0.8847546554661445, 0.0, 1.0, 65.0, 33075.50497186006], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3607800.0000, 
sim time next is 3608400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.30731230878313, 0.8770884637168069, 0.0, 1.0, 65.0, 33561.71014897825], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7756093590652607, 0.7923628212389356, 0.0, 1.0, 1.0, 0.1598176673760869], 
reward next is 0.8402, 
noisyNet noise sample is [array([-0.49771565], dtype=float32), 0.13329142]. 
=============================================
[2019-04-08 15:34:11,707] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3107381e-15 7.1738988e-09 1.2398916e-05 1.9641971e-06 1.1615146e-09
 2.9473955e-11 2.6152853e-07 2.2873362e-14 3.1274686e-17 5.4523293e-20
 9.9998534e-01], sum to 1.0000
[2019-04-08 15:34:11,709] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4266
[2019-04-08 15:34:11,733] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 45.66666666666667, 400.3333333333334, 22.5, 28.82219036372931, 1.211300256003922, 1.0, 1.0, 65.0, 18848.82820198971], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3516600.0000, 
sim time next is 3517200.0000, 
raw observation next is [3.0, 49.0, 37.5, 338.0, 22.5, 28.79244767935345, 1.221726695487871, 1.0, 1.0, 65.0, 18849.29020278584], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.49, 0.125, 0.3734806629834254, 0.375, 0.8993706399461209, 0.9072422318292904, 1.0, 1.0, 1.0, 0.08975852477517067], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.5444968], dtype=float32), -0.9495711]. 
=============================================
[2019-04-08 15:34:11,877] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8210769e-19 5.1185115e-12 1.5372576e-08 4.7553308e-09 1.0761457e-10
 2.2115088e-14 2.1172498e-08 9.9631844e-15 3.1534707e-20 6.1757439e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:34:11,880] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1313
[2019-04-08 15:34:11,909] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 71.0, 73.83333333333334, 350.3333333333333, 22.5, 26.89845859862409, 0.8018334436604476, 1.0, 1.0, 65.0, 52099.91845579931], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3486000.0000, 
sim time next is 3486600.0000, 
raw observation next is [-1.0, 71.0, 88.0, 399.0, 22.5, 27.03736487642891, 0.8265661558633464, 1.0, 1.0, 65.0, 45823.20844829552], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.29333333333333333, 0.4408839779005525, 0.375, 0.7531137397024091, 0.7755220519544488, 1.0, 1.0, 1.0, 0.21820575451569293], 
reward next is 0.7818, 
noisyNet noise sample is [array([-1.3483481], dtype=float32), 0.8769891]. 
=============================================
[2019-04-08 15:34:13,033] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0684590e-17 2.1263193e-10 1.6216630e-07 6.5223946e-07 2.4306915e-10
 3.5710706e-13 3.4338594e-09 2.6336936e-14 8.1521635e-18 1.1550847e-19
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:34:13,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4500
[2019-04-08 15:34:13,049] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 88.0, 687.0, 22.5, 28.87924334524184, 1.242515566024205, 1.0, 1.0, 65.0, 18848.52353042965], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3511800.0000, 
sim time next is 3512400.0000, 
raw observation next is [3.0, 49.0, 83.66666666666667, 660.0, 22.5, 28.90777379029706, 1.249273315919378, 1.0, 1.0, 65.0, 18848.93267524123], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.2788888888888889, 0.7292817679558011, 0.375, 0.9089811491914217, 0.9164244386397927, 1.0, 1.0, 1.0, 0.08975682226305348], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.6609609], dtype=float32), 0.006044942]. 
=============================================
[2019-04-08 15:34:13,552] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1341529e-16 7.1498009e-11 5.9282138e-07 1.7582935e-06 1.8416749e-11
 3.2905536e-13 4.6192778e-08 3.9167791e-16 7.2704999e-19 2.8190006e-22
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:34:13,554] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5084
[2019-04-08 15:34:13,578] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 62.0, 0.0, 0.0, 19.0, 27.09869555337567, 0.8821387987481923, 0.0, 1.0, 65.0, 43630.43348896383], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 3541200.0000, 
sim time next is 3541800.0000, 
raw observation next is [-1.833333333333333, 61.0, 0.0, 0.0, 19.0, 27.0666852199641, 0.8753504919427137, 0.0, 1.0, 65.0, 43985.83593554818], 
processed observation next is [1.0, 1.0, 0.41181902123730385, 0.61, 0.0, 0.0, 0.08333333333333333, 0.755557101663675, 0.7917834973142379, 0.0, 1.0, 1.0, 0.20945636159784847], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.6592572], dtype=float32), 0.90363336]. 
=============================================
[2019-04-08 15:34:14,480] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0986970e-16 1.7682295e-10 2.3466232e-07 3.2822453e-07 6.9018194e-11
 3.9629164e-13 1.4664185e-08 1.1824287e-13 5.6660032e-17 1.2596007e-18
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:34:14,481] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3703
[2019-04-08 15:34:14,499] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666666, 67.0, 0.0, 0.0, 19.0, 26.74096191489446, 0.7501250302195568, 0.0, 1.0, 65.0, 45336.64492024806], 
current ob forecast is [], 
actual action is [0.3333333333333339, 65.0], 
sim time this is 3559200.0000, 
sim time next is 3559800.0000, 
raw observation next is [-4.833333333333334, 66.0, 0.0, 0.0, 19.0, 26.72318844292504, 0.7432757309274662, 0.0, 1.0, 65.0, 46482.20861195723], 
processed observation next is [0.0, 0.17391304347826086, 0.32871652816251157, 0.66, 0.0, 0.0, 0.08333333333333333, 0.7269323702437532, 0.747758576975822, 0.0, 1.0, 1.0, 0.22134385053312966], 
reward next is 0.7787, 
noisyNet noise sample is [array([-0.09597051], dtype=float32), 0.37263662]. 
=============================================
[2019-04-08 15:34:14,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2324860e-17 4.3552868e-09 2.1387697e-07 1.5381904e-05 1.8009817e-09
 1.4964346e-11 1.1670017e-07 5.0486752e-13 3.4801387e-18 9.1451111e-20
 9.9998426e-01], sum to 1.0000
[2019-04-08 15:34:14,618] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2439
[2019-04-08 15:34:14,635] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 27.07048630954614, 0.8619238161223494, 0.0, 1.0, 65.0, 42795.87808676284], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 27.04854172283822, 0.8539213807226962, 0.0, 1.0, 65.0, 43054.64998483381], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7540451435698516, 0.7846404602408987, 0.0, 1.0, 1.0, 0.20502214278492292], 
reward next is 0.7950, 
noisyNet noise sample is [array([-0.8575275], dtype=float32), -1.0474488]. 
=============================================
[2019-04-08 15:34:14,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9708832e-18 2.7498942e-10 6.8661826e-08 3.9818860e-07 1.3519728e-11
 9.6636845e-15 1.0733210e-09 1.1057428e-14 3.7098471e-19 1.8964098e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:34:14,669] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9879
[2019-04-08 15:34:14,680] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 92.66666666666666, 718.3333333333333, 22.5, 28.64631098645092, 1.212339062748952, 1.0, 1.0, 65.0, 18848.84946270077], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3510600.0000, 
sim time next is 3511200.0000, 
raw observation next is [3.0, 49.0, 90.33333333333334, 702.6666666666666, 22.5, 28.82102555711564, 1.23294214532943, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.30111111111111116, 0.776427255985267, 0.375, 0.9017521297596366, 0.9109807151098099, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4062192], dtype=float32), 0.43336514]. 
=============================================
[2019-04-08 15:34:14,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9915437e-17 1.1759668e-10 7.4819425e-08 2.1859677e-07 7.0474980e-11
 1.2622406e-12 8.6591285e-09 3.4951598e-14 1.1695356e-18 1.9709824e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:34:14,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4615
[2019-04-08 15:34:14,787] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.5415586671415, 1.025685442022336, 0.0, 1.0, 65.0, 34198.16305627381], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3529200.0000, 
sim time next is 3529800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.50536708717573, 1.019623169008359, 0.0, 1.0, 65.0, 34497.21283335556], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.792113923931311, 0.839874389669453, 0.0, 1.0, 1.0, 0.1642724420635979], 
reward next is 0.8357, 
noisyNet noise sample is [array([0.39091742], dtype=float32), 1.9664857]. 
=============================================
[2019-04-08 15:34:15,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8434122e-16 5.1515664e-10 2.5091392e-06 1.6936780e-05 2.3582442e-10
 1.6205757e-11 2.5708644e-06 6.6483272e-13 2.5682328e-17 1.4285149e-18
 9.9997795e-01], sum to 1.0000
[2019-04-08 15:34:15,531] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4124
[2019-04-08 15:34:15,544] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.4764791328731, 1.009081227084777, 0.0, 1.0, 65.0, 33967.87104081261], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3531600.0000, 
sim time next is 3532200.0000, 
raw observation next is [-0.1666666666666667, 73.0, 0.0, 0.0, 19.0, 27.46003572900483, 1.008396771253356, 0.0, 1.0, 65.0, 34118.26444427489], 
processed observation next is [1.0, 0.9130434782608695, 0.4579870729455217, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7883363107504024, 0.8361322570844519, 0.0, 1.0, 1.0, 0.16246792592511855], 
reward next is 0.8375, 
noisyNet noise sample is [array([-0.41469368], dtype=float32), -0.74147135]. 
=============================================
[2019-04-08 15:34:16,185] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.8468611e-18 6.6622992e-11 1.8339718e-07 5.7079171e-07 1.2105471e-10
 1.8675562e-12 6.2815209e-10 5.9236021e-16 1.6846767e-18 7.4392012e-21
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:34:16,187] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7207
[2019-04-08 15:34:16,202] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 68.0, 0.0, 0.0, 19.0, 27.204987003475, 0.9122771089722098, 0.0, 1.0, 65.0, 42540.81690770359], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3538200.0000, 
sim time next is 3538800.0000, 
raw observation next is [-1.0, 66.0, 0.0, 0.0, 19.0, 27.16700023512312, 0.9125997561707929, 0.0, 1.0, 65.0, 42845.70685414007], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.66, 0.0, 0.0, 0.08333333333333333, 0.7639166862602599, 0.8041999187235976, 0.0, 1.0, 1.0, 0.2040271754959051], 
reward next is 0.7960, 
noisyNet noise sample is [array([-0.409828], dtype=float32), -2.4678414]. 
=============================================
[2019-04-08 15:34:16,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1670714e-14 3.7277501e-09 1.8234998e-06 5.1216434e-06 5.8630754e-09
 6.4313437e-11 3.9344928e-07 6.6864522e-12 1.5363547e-16 1.9722728e-18
 9.9999261e-01], sum to 1.0000
[2019-04-08 15:34:16,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5982
[2019-04-08 15:34:16,604] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.166666666666666, 70.0, 0.0, 0.0, 19.0, 26.42190386885474, 0.6745407550640327, 0.0, 1.0, 65.0, 50660.93317075767], 
current ob forecast is [], 
actual action is [-1.166666666666666, 65.0], 
sim time this is 3568200.0000, 
sim time next is 3568800.0000, 
raw observation next is [-6.333333333333333, 70.0, 0.0, 0.0, 19.0, 26.40704431653896, 0.6698882350709949, 0.0, 1.0, 65.0, 50940.75499381744], 
processed observation next is [0.0, 0.30434782608695654, 0.28716528162511545, 0.7, 0.0, 0.0, 0.08333333333333333, 0.7005870263782468, 0.7232960783569983, 0.0, 1.0, 1.0, 0.24257502378008305], 
reward next is 0.7574, 
noisyNet noise sample is [array([0.69056785], dtype=float32), -0.022375004]. 
=============================================
[2019-04-08 15:34:17,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8167942e-16 9.3737018e-09 4.0625969e-06 1.7376580e-06 1.6463122e-09
 1.2705363e-10 3.5653659e-07 4.3946807e-13 1.8331401e-16 1.2686309e-17
 9.9999380e-01], sum to 1.0000
[2019-04-08 15:34:17,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3936
[2019-04-08 15:34:17,675] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.1666666666666666, 42.83333333333334, 78.33333333333334, 637.6666666666667, 19.0, 27.48238201254446, 0.9706158209312998, 0.0, 1.0, 65.0, 28448.68700598669], 
current ob forecast is [], 
actual action is [4.833333333333333, 65.0], 
sim time this is 3599400.0000, 
sim time next is 3600000.0000, 
raw observation next is [0.0, 43.0, 74.5, 607.0, 19.0, 27.47460663137457, 0.9717328063187782, 0.0, 1.0, 65.0, 29532.10948788227], 
processed observation next is [0.0, 0.6956521739130435, 0.46260387811634357, 0.43, 0.24833333333333332, 0.6707182320441989, 0.08333333333333333, 0.7895505526145475, 0.8239109354395927, 0.0, 1.0, 1.0, 0.1406290927994394], 
reward next is 0.8594, 
noisyNet noise sample is [array([1.2612764], dtype=float32), -0.49628434]. 
=============================================
[2019-04-08 15:34:17,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.52288 ]
 [75.60292 ]
 [75.65942 ]
 [75.757385]
 [75.818146]], R is [[75.57478333]
 [75.68357086]
 [75.79607391]
 [75.90163422]
 [75.99790955]].
[2019-04-08 15:34:17,802] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.8209595e-16 1.2194594e-09 4.9283847e-07 8.1016765e-07 4.1515840e-09
 8.3381260e-11 4.1089851e-07 1.8939201e-12 3.5824141e-16 3.6170420e-18
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:34:17,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9604
[2019-04-08 15:34:17,820] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.24718079223793, 0.853718345338686, 0.0, 1.0, 65.0, 34123.98548527295], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3611400.0000, 
sim time next is 3612000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.21622354611389, 0.8492345330391177, 0.0, 1.0, 65.0, 35597.40952261948], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7680186288428242, 0.7830781776797059, 0.0, 1.0, 1.0, 0.16951147391723562], 
reward next is 0.8305, 
noisyNet noise sample is [array([-0.65005356], dtype=float32), 1.4327197]. 
=============================================
[2019-04-08 15:34:17,826] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.91348 ]
 [73.769806]
 [73.69087 ]
 [73.58428 ]
 [73.5267  ]], R is [[74.11436462]
 [74.21072388]
 [74.30775452]
 [74.39636993]
 [74.48483276]].
[2019-04-08 15:34:17,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2949389e-16 3.5132450e-10 4.6580549e-08 2.3671397e-05 2.6940272e-10
 9.4996157e-13 1.9406665e-08 3.4700446e-13 1.0811362e-17 1.5917115e-20
 9.9997628e-01], sum to 1.0000
[2019-04-08 15:34:17,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3596
[2019-04-08 15:34:17,863] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 106.0, 796.0, 19.0, 27.25833784657096, 0.929261011202937, 0.0, 1.0, 65.0, 30950.85669211031], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3593400.0000, 
sim time next is 3594000.0000, 
raw observation next is [-1.0, 42.0, 104.0, 792.0, 19.0, 27.25868821476138, 0.930607043626424, 0.0, 1.0, 65.0, 30828.80773132834], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.3466666666666667, 0.8751381215469614, 0.08333333333333333, 0.7715573512301152, 0.8102023478754746, 0.0, 1.0, 1.0, 0.14680384633965876], 
reward next is 0.8532, 
noisyNet noise sample is [array([1.5218971], dtype=float32), 0.6109428]. 
=============================================
[2019-04-08 15:34:17,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.90326 ]
 [74.95914 ]
 [75.01058 ]
 [75.078384]
 [75.13309 ]], R is [[74.98468018]
 [75.08744812]
 [75.18694305]
 [75.2828064 ]
 [75.37609863]].
[2019-04-08 15:34:18,868] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0844692e-16 1.0156314e-09 8.3779023e-06 9.4856514e-06 2.6216007e-09
 9.9312572e-12 4.7767475e-07 6.1033605e-13 1.1636461e-16 6.2036199e-19
 9.9998164e-01], sum to 1.0000
[2019-04-08 15:34:18,871] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5155
[2019-04-08 15:34:18,890] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 19.0, 27.23281467995888, 0.7607669230859377, 0.0, 1.0, 65.0, 33501.22487058894], 
current ob forecast is [], 
actual action is [14.0, 65.0], 
sim time this is 3654000.0000, 
sim time next is 3654600.0000, 
raw observation next is [8.833333333333334, 27.83333333333333, 0.0, 0.0, 19.0, 27.20988565845272, 0.756903893731974, 0.0, 1.0, 65.0, 33766.08066468701], 
processed observation next is [0.0, 0.30434782608695654, 0.7072945521698984, 0.27833333333333327, 0.0, 0.0, 0.08333333333333333, 0.7674904715377266, 0.752301297910658, 0.0, 1.0, 1.0, 0.16079086030803338], 
reward next is 0.8392, 
noisyNet noise sample is [array([1.3857818], dtype=float32), -2.1089828]. 
=============================================
[2019-04-08 15:34:19,616] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.0620187e-15 4.4891227e-10 2.7476106e-06 1.2570736e-06 2.4669651e-09
 7.3117250e-13 6.9030568e-07 5.6260365e-14 3.4550722e-17 1.1260622e-17
 9.9999535e-01], sum to 1.0000
[2019-04-08 15:34:19,617] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0424
[2019-04-08 15:34:19,634] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.21621201389286, 0.8699643445740342, 0.0, 1.0, 65.0, 35343.71514659157], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3610200.0000, 
sim time next is 3610800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.26392077975478, 0.8611289835439586, 0.0, 1.0, 65.0, 33781.54309096638], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7719933983128984, 0.7870429945146529, 0.0, 1.0, 1.0, 0.1608644909093637], 
reward next is 0.8391, 
noisyNet noise sample is [array([0.58045626], dtype=float32), -1.4228023]. 
=============================================
[2019-04-08 15:34:20,084] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7351545e-15 3.3607211e-10 1.7119494e-07 2.8842269e-06 4.0426174e-09
 1.9266060e-12 2.8019409e-07 1.0950973e-12 4.4037150e-16 4.3458503e-19
 9.9999666e-01], sum to 1.0000
[2019-04-08 15:34:20,084] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3969
[2019-04-08 15:34:20,104] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.5, 29.5, 4.0, 121.0, 19.0, 27.17444574167679, 0.7632268705406585, 0.0, 1.0, 65.0, 34907.70317269536], 
current ob forecast is [], 
actual action is [13.5, 65.0], 
sim time this is 3655800.0000, 
sim time next is 3656400.0000, 
raw observation next is [8.333333333333334, 30.33333333333334, 18.16666666666666, 168.0, 19.0, 27.17730159075067, 0.7720622365411091, 0.0, 1.0, 65.0, 34512.2592160334], 
processed observation next is [0.0, 0.30434782608695654, 0.6934441366574331, 0.3033333333333334, 0.060555555555555536, 0.1856353591160221, 0.08333333333333333, 0.7647751325625558, 0.7573540788470363, 0.0, 1.0, 1.0, 0.16434409150492096], 
reward next is 0.8357, 
noisyNet noise sample is [array([0.18260233], dtype=float32), 0.09069628]. 
=============================================
[2019-04-08 15:34:20,174] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.2094333e-16 2.5964519e-08 2.8096279e-06 1.0372272e-06 4.9452437e-10
 2.8928021e-12 1.0986221e-06 1.0265381e-12 2.7551694e-17 2.4412297e-20
 9.9999499e-01], sum to 1.0000
[2019-04-08 15:34:20,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7798
[2019-04-08 15:34:20,206] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.833333333333333, 47.5, 92.66666666666666, 728.6666666666667, 19.0, 28.04646912270252, 1.101495982734356, 0.0, 1.0, 65.0, 22138.96773378775], 
current ob forecast is [], 
actual action is [10.833333333333332, 65.0], 
sim time this is 3683400.0000, 
sim time next is 3684000.0000, 
raw observation next is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 19.0, 28.10294234614903, 1.101108605893407, 0.0, 1.0, 65.0, 20531.06152594871], 
processed observation next is [0.0, 0.6521739130434783, 0.6195752539242845, 0.48, 0.2994444444444444, 0.7920810313075506, 0.08333333333333333, 0.841911862179086, 0.867036201964469, 0.0, 1.0, 1.0, 0.09776695964737481], 
reward next is 0.9022, 
noisyNet noise sample is [array([-1.4679629], dtype=float32), 0.3534041]. 
=============================================
[2019-04-08 15:34:20,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.27709 ]
 [82.19604 ]
 [82.110344]
 [82.039795]
 [81.931465]], R is [[82.44503021]
 [82.51515961]
 [82.58547211]
 [82.66142273]
 [82.74072266]].
[2019-04-08 15:34:20,336] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8068998e-17 1.4421248e-11 4.2906132e-09 3.2950165e-08 1.8657560e-12
 7.3019678e-15 3.9443665e-10 3.6307949e-15 3.9407466e-19 4.3979584e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:34:20,337] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2529
[2019-04-08 15:34:20,347] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.166666666666667, 49.5, 79.33333333333334, 644.0, 19.0, 28.10971814464097, 1.104671851051572, 0.0, 1.0, 65.0, 21690.48586581751], 
current ob forecast is [], 
actual action is [10.166666666666668, 65.0], 
sim time this is 3685800.0000, 
sim time next is 3686400.0000, 
raw observation next is [5.0, 50.0, 75.5, 613.5, 19.0, 28.11172797633341, 1.105036787182389, 0.0, 1.0, 65.0, 21580.49088864218], 
processed observation next is [0.0, 0.6956521739130435, 0.6011080332409973, 0.5, 0.25166666666666665, 0.6779005524861879, 0.08333333333333333, 0.8426439980277841, 0.868345595727463, 0.0, 1.0, 1.0, 0.10276424232686752], 
reward next is 0.8972, 
noisyNet noise sample is [array([-0.8183851], dtype=float32), 0.6824117]. 
=============================================
[2019-04-08 15:34:20,457] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.95455870e-16 7.97097777e-10 6.31931584e-07 3.96619134e-06
 1.17234625e-11 1.60515351e-12 2.83254877e-07 7.19108882e-14
 1.15690680e-17 1.54112847e-19 9.99995112e-01], sum to 1.0000
[2019-04-08 15:34:20,457] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9824
[2019-04-08 15:34:20,512] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 19.0, 27.63916621169952, 0.9429621831304907, 0.0, 1.0, 65.0, 22674.76062689946], 
current ob forecast is [], 
actual action is [16.33333333333333, 65.0], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [11.5, 26.0, 111.0, 763.0, 19.0, 27.66981440249396, 0.9548283238746107, 0.0, 1.0, 65.0, 21934.20254669975], 
processed observation next is [0.0, 0.43478260869565216, 0.7811634349030472, 0.26, 0.37, 0.8430939226519337, 0.08333333333333333, 0.8058178668744965, 0.8182761079582036, 0.0, 1.0, 1.0, 0.1044485835557131], 
reward next is 0.8956, 
noisyNet noise sample is [array([-1.158402], dtype=float32), -1.7959036]. 
=============================================
[2019-04-08 15:34:20,999] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.0895954e-17 1.3263446e-11 2.5039968e-08 1.7124543e-07 3.1773629e-11
 1.8732203e-14 8.9108418e-08 9.9299443e-15 1.5274108e-19 2.2990101e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:34:21,000] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7935
[2019-04-08 15:34:21,017] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 19.0, 27.85566327045333, 1.005179404960883, 0.0, 1.0, 65.0, 29002.8903355888], 
current ob forecast is [], 
actual action is [9.0, 65.0], 
sim time this is 3696000.0000, 
sim time next is 3696600.0000, 
raw observation next is [4.0, 59.0, 0.0, 0.0, 19.0, 27.87205373392337, 0.9966215837266391, 0.0, 1.0, 65.0, 28311.44508955624], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.59, 0.0, 0.0, 0.08333333333333333, 0.8226711444936141, 0.8322071945755464, 0.0, 1.0, 1.0, 0.13481640518836305], 
reward next is 0.8652, 
noisyNet noise sample is [array([-0.2958503], dtype=float32), -0.45307207]. 
=============================================
[2019-04-08 15:34:21,067] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8289673e-16 2.2257030e-09 1.2125358e-07 3.6972391e-08 9.6800568e-10
 1.0175932e-12 1.1538478e-08 4.2453270e-13 1.7851610e-17 6.0719374e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:34:21,067] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9920
[2019-04-08 15:34:21,093] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.0, 32.0, 46.5, 262.0, 19.0, 27.18899504427072, 0.7915067666175478, 0.0, 1.0, 65.0, 33446.18949854832], 
current ob forecast is [], 
actual action is [13.0, 65.0], 
sim time this is 3657600.0000, 
sim time next is 3658200.0000, 
raw observation next is [8.5, 31.0, 60.66666666666668, 309.0, 19.0, 27.21652706713877, 0.7995417900940079, 0.0, 1.0, 65.0, 32316.17790910905], 
processed observation next is [0.0, 0.34782608695652173, 0.698060941828255, 0.31, 0.20222222222222228, 0.3414364640883978, 0.08333333333333333, 0.7680439222615641, 0.766513930031336, 0.0, 1.0, 1.0, 0.15388656147194785], 
reward next is 0.8461, 
noisyNet noise sample is [array([0.5767169], dtype=float32), 0.39406702]. 
=============================================
[2019-04-08 15:34:21,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1919913e-18 2.1550886e-10 3.2244984e-08 8.1964008e-06 6.0361431e-11
 1.3258092e-13 4.3432014e-08 3.8594724e-14 2.7663118e-18 7.8473890e-21
 9.9999177e-01], sum to 1.0000
[2019-04-08 15:34:21,563] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1347
[2019-04-08 15:34:21,590] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.166666666666667, 42.16666666666666, 114.3333333333333, 821.6666666666666, 19.0, 27.75357419138621, 1.022695804199813, 0.0, 1.0, 65.0, 23721.15860447612], 
current ob forecast is [], 
actual action is [10.166666666666668, 65.0], 
sim time this is 3676200.0000, 
sim time next is 3676800.0000, 
raw observation next is [5.333333333333334, 42.33333333333334, 113.6666666666667, 819.8333333333334, 19.0, 27.76993903896116, 1.028082784971481, 0.0, 1.0, 65.0, 23723.16025312503], 
processed observation next is [0.0, 0.5652173913043478, 0.6103416435826409, 0.42333333333333345, 0.378888888888889, 0.9058931860036833, 0.08333333333333333, 0.8141615865800965, 0.8426942616571603, 0.0, 1.0, 1.0, 0.11296742977678585], 
reward next is 0.8870, 
noisyNet noise sample is [array([0.3300871], dtype=float32), -0.12709008]. 
=============================================
[2019-04-08 15:34:21,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9659566e-18 9.5181259e-11 1.4689006e-07 2.6555495e-07 6.9012163e-10
 1.2248994e-12 3.6949470e-07 1.3000654e-14 6.1714184e-18 9.7594525e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:34:21,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6788
[2019-04-08 15:34:21,872] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0197120e-18 9.0033869e-11 2.3577159e-07 2.7649662e-07 6.1542355e-10
 6.0762166e-14 3.2366437e-08 7.8976844e-15 6.1245147e-18 1.0100569e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:34:21,877] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2768
[2019-04-08 15:34:21,878] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 43.0, 108.5, 797.0, 19.0, 27.86202245757659, 1.061198795646751, 0.0, 1.0, 65.0, 22943.37190630545], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 3679200.0000, 
sim time next is 3679800.0000, 
raw observation next is [6.0, 43.66666666666667, 107.0, 790.0, 19.0, 27.92748486548098, 1.064162315706568, 0.0, 1.0, 65.0, 21667.52828165199], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.4366666666666667, 0.3566666666666667, 0.8729281767955801, 0.08333333333333333, 0.8272904054567484, 0.8547207719021893, 0.0, 1.0, 1.0, 0.10317870610310471], 
reward next is 0.8968, 
noisyNet noise sample is [array([1.0318571], dtype=float32), 0.83365226]. 
=============================================
[2019-04-08 15:34:21,910] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.40661129743923, 0.8860671897490949, 0.0, 1.0, 65.0, 36833.0318024378], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3708000.0000, 
sim time next is 3708600.0000, 
raw observation next is [-0.5, 70.83333333333333, 0.0, 0.0, 19.0, 27.37564260945062, 0.8787829109971014, 0.0, 1.0, 65.0, 37310.3691028269], 
processed observation next is [0.0, 0.9565217391304348, 0.44875346260387816, 0.7083333333333333, 0.0, 0.0, 0.08333333333333333, 0.7813035507875515, 0.7929276369990338, 0.0, 1.0, 1.0, 0.17766842429917573], 
reward next is 0.8223, 
noisyNet noise sample is [array([-1.2738332], dtype=float32), -2.219519]. 
=============================================
[2019-04-08 15:34:22,436] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9872175e-18 8.9463367e-11 8.2231729e-08 8.7600533e-08 9.6045741e-11
 1.4748382e-14 5.6027316e-09 7.2932815e-15 1.1260834e-19 3.6404063e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:34:22,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1879
[2019-04-08 15:34:22,489] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 19.0, 27.81483281148765, 0.9837449540691247, 0.0, 1.0, 65.0, 30188.5904354724], 
current ob forecast is [], 
actual action is [8.833333333333332, 65.0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [3.666666666666667, 60.33333333333334, 0.0, 0.0, 19.0, 27.78232472416284, 0.9774535537645842, 0.0, 1.0, 65.0, 30818.81497054005], 
processed observation next is [0.0, 0.8260869565217391, 0.564173591874423, 0.6033333333333334, 0.0, 0.0, 0.08333333333333333, 0.8151937270135701, 0.8258178512548614, 0.0, 1.0, 1.0, 0.14675626176447643], 
reward next is 0.8532, 
noisyNet noise sample is [array([-1.0246774], dtype=float32), -1.0864774]. 
=============================================
[2019-04-08 15:34:22,718] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.75260777e-17 3.85612514e-10 4.82324936e-07 1.01403224e-07
 2.49273324e-10 1.18067543e-13 1.33143319e-08 4.53398983e-14
 6.73172197e-19 2.16172215e-20 9.99999404e-01], sum to 1.0000
[2019-04-08 15:34:22,723] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7013
[2019-04-08 15:34:22,736] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4136194e-16 1.2061686e-09 1.3553775e-06 3.1297859e-06 2.2366373e-10
 9.3144036e-13 9.4257819e-09 1.8108080e-13 1.1136483e-17 1.3546308e-19
 9.9999547e-01], sum to 1.0000
[2019-04-08 15:34:22,737] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1210
[2019-04-08 15:34:22,752] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.0, 27.0, 0.0, 0.0, 19.0, 27.23281145331888, 0.7607661725387288, 0.0, 1.0, 65.0, 33501.25403740348], 
current ob forecast is [], 
actual action is [14.0, 65.0], 
sim time this is 3654000.0000, 
sim time next is 3654600.0000, 
raw observation next is [8.833333333333334, 27.83333333333333, 0.0, 0.0, 19.0, 27.20988247144512, 0.7569031531120775, 0.0, 1.0, 65.0, 33766.10941476138], 
processed observation next is [0.0, 0.30434782608695654, 0.7072945521698984, 0.27833333333333327, 0.0, 0.0, 0.08333333333333333, 0.7674902059537599, 0.7523010510373592, 0.0, 1.0, 1.0, 0.1607909972131494], 
reward next is 0.8392, 
noisyNet noise sample is [array([-1.0942992], dtype=float32), -0.88359165]. 
=============================================
[2019-04-08 15:34:22,759] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 19.0, 27.19122520740568, 0.7695784554402528, 0.0, 1.0, 65.0, 34985.16769552261], 
current ob forecast is [], 
actual action is [14.0, 65.0], 
sim time this is 3635400.0000, 
sim time next is 3636000.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 19.0, 27.18738034856929, 0.7683781596735207, 0.0, 1.0, 65.0, 34990.19188021273], 
processed observation next is [0.0, 0.08695652173913043, 0.7119113573407203, 0.25, 0.0, 0.0, 0.08333333333333333, 0.7656150290474409, 0.756126053224507, 0.0, 1.0, 1.0, 0.16661996133434634], 
reward next is 0.8334, 
noisyNet noise sample is [array([-0.9943832], dtype=float32), 0.29571924]. 
=============================================
[2019-04-08 15:34:22,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.93243 ]
 [81.29226 ]
 [81.59652 ]
 [81.64057 ]
 [81.767784]], R is [[80.84194183]
 [80.8669281 ]
 [80.891716  ]
 [80.91635895]
 [80.9408493 ]].
[2019-04-08 15:34:23,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6178473e-18 4.1653788e-11 1.9904316e-09 4.0871475e-07 2.8298450e-11
 9.3964355e-13 1.9342439e-09 1.9080897e-15 3.6306219e-18 1.0941540e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:34:23,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9224
[2019-04-08 15:34:23,491] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 19.0, 28.06781919032693, 1.085367798198945, 0.0, 1.0, 65.0, 20621.89474719315], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 3682200.0000, 
sim time next is 3682800.0000, 
raw observation next is [6.0, 47.0, 95.5, 740.5, 19.0, 28.0546005073383, 1.089128893170195, 0.0, 1.0, 65.0, 21952.37101713709], 
processed observation next is [0.0, 0.6521739130434783, 0.6288088642659281, 0.47, 0.31833333333333336, 0.8182320441988951, 0.08333333333333333, 0.837883375611525, 0.8630429643900651, 0.0, 1.0, 1.0, 0.1045351000816052], 
reward next is 0.8955, 
noisyNet noise sample is [array([-0.39808133], dtype=float32), -0.8520042]. 
=============================================
[2019-04-08 15:34:24,955] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3811449e-19 2.1468924e-10 2.1435385e-09 8.2310187e-08 3.5600409e-10
 7.0783257e-14 8.7825452e-10 1.3618517e-15 6.0651581e-19 1.5462169e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:34:24,963] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2019
[2019-04-08 15:34:24,974] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.83333333333333, 24.66666666666666, 112.6666666666667, 780.6666666666667, 19.0, 27.75471841434414, 0.9779383397438606, 0.0, 1.0, 65.0, 20361.70366069331], 
current ob forecast is [], 
actual action is [16.83333333333333, 65.0], 
sim time this is 3667800.0000, 
sim time next is 3668400.0000, 
raw observation next is [12.0, 24.0, 113.5, 789.5, 19.0, 27.80480311981533, 0.9861172863694465, 0.0, 1.0, 65.0, 19897.40286779922], 
processed observation next is [0.0, 0.4782608695652174, 0.7950138504155125, 0.24, 0.37833333333333335, 0.8723756906077348, 0.08333333333333333, 0.8170669266512774, 0.8287057621231488, 0.0, 1.0, 1.0, 0.09474953746571058], 
reward next is 0.9053, 
noisyNet noise sample is [array([-0.5625651], dtype=float32), -1.1617701]. 
=============================================
[2019-04-08 15:34:24,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.5252815e-20 8.8856899e-12 3.1866829e-09 3.6582080e-09 1.9152862e-13
 2.6384208e-15 1.3780031e-10 2.9379275e-17 6.4283753e-22 2.9014209e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:34:24,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7844
[2019-04-08 15:34:25,023] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 60.0, 117.0, 823.5, 22.5, 28.14451916930729, 1.065247006613072, 1.0, 1.0, 65.0, 23796.31421441083], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3762000.0000, 
sim time next is 3762600.0000, 
raw observation next is [-0.8333333333333334, 60.0, 116.3333333333333, 821.6666666666666, 22.5, 28.0892014211677, 1.061614539667662, 1.0, 1.0, 65.0, 24764.41122801119], 
processed observation next is [1.0, 0.5652173913043478, 0.43951985226223456, 0.6, 0.38777777777777767, 0.9079189686924494, 0.375, 0.8407667850973084, 0.8538715132225541, 1.0, 1.0, 1.0, 0.11792576775243424], 
reward next is 0.8821, 
noisyNet noise sample is [array([0.43363053], dtype=float32), 0.9463841]. 
=============================================
[2019-04-08 15:34:25,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6938240e-20 9.0883785e-12 3.2320662e-09 3.6959684e-09 1.9566115e-13
 2.6746385e-15 1.3833226e-10 3.0195884e-17 6.6824642e-22 3.0162867e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:34:25,047] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1256
[2019-04-08 15:34:25,074] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8333333333333334, 60.0, 116.3333333333333, 821.6666666666666, 22.5, 28.0892014211677, 1.061614539667662, 1.0, 1.0, 65.0, 24764.41122801119], 
current ob forecast is [], 
actual action is [4.166666666666667, 65.0], 
sim time this is 3762600.0000, 
sim time next is 3763200.0000, 
raw observation next is [-0.6666666666666667, 60.00000000000001, 115.6666666666667, 819.8333333333334, 22.5, 28.03499698859484, 1.092704211873707, 1.0, 1.0, 65.0, 25824.45406520518], 
processed observation next is [1.0, 0.5652173913043478, 0.44413665743305636, 0.6000000000000001, 0.38555555555555565, 0.9058931860036833, 0.375, 0.8362497490495701, 0.8642347372912357, 1.0, 1.0, 1.0, 0.12297359078669133], 
reward next is 0.8770, 
noisyNet noise sample is [array([0.43363053], dtype=float32), 0.9463841]. 
=============================================
[2019-04-08 15:34:25,240] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1547152e-18 1.5564942e-10 1.6622219e-07 5.8389139e-07 2.6101846e-10
 3.1135077e-13 3.5093443e-08 6.5608638e-14 5.4548002e-19 3.5442666e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:34:25,240] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1385
[2019-04-08 15:34:25,254] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.666666666666667, 63.66666666666667, 0.0, 0.0, 19.0, 27.58242375747602, 0.9207231795761338, 0.0, 1.0, 65.0, 33444.6025599587], 
current ob forecast is [], 
actual action is [6.666666666666667, 65.0], 
sim time this is 3705000.0000, 
sim time next is 3705600.0000, 
raw observation next is [1.333333333333333, 65.33333333333334, 0.0, 0.0, 19.0, 27.55585990135179, 0.9126853161551677, 0.0, 1.0, 65.0, 33946.86039537979], 
processed observation next is [0.0, 0.9130434782608695, 0.4995383194829178, 0.6533333333333334, 0.0, 0.0, 0.08333333333333333, 0.7963216584459826, 0.8042284387183892, 0.0, 1.0, 1.0, 0.16165171616847518], 
reward next is 0.8383, 
noisyNet noise sample is [array([1.0627292], dtype=float32), 0.37596253]. 
=============================================
[2019-04-08 15:34:25,552] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.2854306e-18 7.5096145e-11 2.8641270e-07 1.4640694e-06 3.7175710e-11
 3.6303420e-13 5.2542033e-09 2.1982995e-14 2.6323197e-18 2.4320249e-21
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:34:25,556] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2272
[2019-04-08 15:34:25,578] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 55.5, 0.0, 0.0, 19.0, 27.52473827694497, 0.9808380581279194, 0.0, 1.0, 65.0, 30725.0385314841], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3875400.0000, 
sim time next is 3876000.0000, 
raw observation next is [-0.3333333333333333, 57.0, 0.0, 0.0, 19.0, 27.48646479693617, 0.9743885305195973, 0.0, 1.0, 65.0, 31937.13256212894], 
processed observation next is [1.0, 0.8695652173913043, 0.4533702677747, 0.57, 0.0, 0.0, 0.08333333333333333, 0.790538733078014, 0.8247961768398658, 0.0, 1.0, 1.0, 0.15208158362918542], 
reward next is 0.8479, 
noisyNet noise sample is [array([-0.46685624], dtype=float32), -0.19306004]. 
=============================================
[2019-04-08 15:34:25,619] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.563354]
 [86.38468 ]
 [86.755615]
 [86.74548 ]
 [86.884186]], R is [[86.52443695]
 [86.51288605]
 [86.50376892]
 [86.49153137]
 [86.48146057]].
[2019-04-08 15:34:26,118] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.7942028e-18 7.2991391e-11 2.2337423e-07 1.7620073e-07 2.2282282e-10
 3.8346222e-13 3.4954834e-08 3.5067335e-13 1.9808742e-18 1.2809882e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:34:26,118] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-08 15:34:26,162] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.0, 59.0, 31.33333333333333, 284.0, 19.0, 28.12257737626796, 1.075151738479782, 0.0, 1.0, 65.0, 23586.60388120042], 
current ob forecast is [], 
actual action is [9.0, 65.0], 
sim time this is 3690600.0000, 
sim time next is 3691200.0000, 
raw observation next is [4.0, 59.0, 23.16666666666666, 224.5, 19.0, 28.08576379067528, 1.065776344748126, 0.0, 1.0, 65.0, 23972.30394260459], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.0772222222222222, 0.24806629834254143, 0.08333333333333333, 0.8404803158896067, 0.8552587815827087, 0.0, 1.0, 1.0, 0.1141538282981171], 
reward next is 0.8858, 
noisyNet noise sample is [array([0.40420607], dtype=float32), 0.6459682]. 
=============================================
[2019-04-08 15:34:26,482] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.99982117e-18 1.42960956e-11 1.05653754e-07 1.46595721e-06
 1.18210691e-10 6.94841523e-14 2.62728150e-09 3.25333841e-16
 1.62031486e-19 3.07435670e-21 9.99998450e-01], sum to 1.0000
[2019-04-08 15:34:26,502] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6809
[2019-04-08 15:34:26,525] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 22.5, 27.24028474332696, 1.097331998894479, 1.0, 1.0, 65.0, 59131.58005982017], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3780600.0000, 
sim time next is 3781200.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 22.5, 26.82650115252224, 0.9756710177099758, 1.0, 1.0, 65.0, 29537.32759342442], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.375, 0.7355417627101867, 0.825223672569992, 1.0, 1.0, 1.0, 0.14065394092106867], 
reward next is 0.8593, 
noisyNet noise sample is [array([0.33391953], dtype=float32), 0.9737846]. 
=============================================
[2019-04-08 15:34:26,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9837913e-17 9.3887575e-10 3.1206437e-08 9.9175406e-07 3.1531687e-11
 3.5497600e-13 2.6850302e-09 1.7487507e-15 9.0138450e-19 6.9036304e-23
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:34:26,753] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9647
[2019-04-08 15:34:26,779] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.95701397400588, 0.8126376440835331, 0.0, 1.0, 65.0, 41524.98367198094], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3891600.0000, 
sim time next is 3892200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.98212733056906, 0.8182154531601129, 0.0, 1.0, 65.0, 40845.8882228324], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.748510610880755, 0.7727384843867043, 0.0, 1.0, 1.0, 0.19450422963253525], 
reward next is 0.8055, 
noisyNet noise sample is [array([1.2051529], dtype=float32), 1.166255]. 
=============================================
[2019-04-08 15:34:26,857] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0882477e-17 1.0557605e-09 1.5590396e-07 1.8979518e-06 3.5051753e-10
 1.6775891e-13 1.0887924e-08 1.1177300e-14 1.4226485e-18 1.9526823e-20
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:34:26,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1412
[2019-04-08 15:34:26,864] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3023656e-19 2.7200766e-11 7.6704234e-09 2.0228773e-08 1.8939315e-11
 8.7135795e-15 8.7838192e-10 7.7440993e-17 2.4697465e-19 3.4139280e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:34:26,868] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8556
[2019-04-08 15:34:26,899] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 74.0, 5.0, 136.0, 22.5, 26.84250765936222, 0.6791849749312194, 1.0, 1.0, 65.0, 45752.47067093969], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3742200.0000, 
sim time next is 3742800.0000, 
raw observation next is [-4.0, 73.0, 19.0, 184.8333333333333, 22.5, 26.7873126857003, 0.7015334568123716, 1.0, 1.0, 65.0, 47306.20329760286], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.73, 0.06333333333333334, 0.20423572744014729, 0.375, 0.7322760571416916, 0.7338444856041239, 1.0, 1.0, 1.0, 0.2252676347504898], 
reward next is 0.7747, 
noisyNet noise sample is [array([-1.5878365], dtype=float32), 1.8001313]. 
=============================================
[2019-04-08 15:34:26,939] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 60.00000000000001, 107.5, 783.0, 22.5, 28.54119170362543, 1.15387176451621, 1.0, 1.0, 65.0, 18844.97142607673], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3766800.0000, 
sim time next is 3767400.0000, 
raw observation next is [0.0, 60.0, 106.0, 776.0, 22.5, 28.53589945389707, 1.020735599889473, 1.0, 1.0, 65.0, 33208.58437007954], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.35333333333333333, 0.8574585635359117, 0.375, 0.8779916211580892, 0.8402451999631576, 1.0, 1.0, 1.0, 0.1581361160479978], 
reward next is 0.8419, 
noisyNet noise sample is [array([0.90371823], dtype=float32), 0.5732045]. 
=============================================
[2019-04-08 15:34:28,044] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.87333157e-18 9.07568604e-11 1.16566525e-07 7.47499962e-06
 1.59900260e-10 1.53143947e-12 4.95271202e-08 1.99554762e-14
 6.80560078e-18 2.26989588e-20 9.99992371e-01], sum to 1.0000
[2019-04-08 15:34:28,046] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6792
[2019-04-08 15:34:28,064] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.87831350797298, 0.7061217001705639, 0.0, 1.0, 65.0, 45515.68077357751], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.85390968428045, 0.7020699119916358, 0.0, 1.0, 65.0, 46176.55440340309], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7378258070233707, 0.7340233039972119, 0.0, 1.0, 1.0, 0.21988835430191947], 
reward next is 0.7801, 
noisyNet noise sample is [array([-0.66453105], dtype=float32), -0.88202995]. 
=============================================
[2019-04-08 15:34:28,280] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0582862e-19 7.8426279e-11 3.8862112e-08 1.5431274e-07 7.0570098e-11
 1.3539304e-13 1.6793990e-08 8.3104237e-13 1.8012547e-19 5.5079263e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:34:28,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3125
[2019-04-08 15:34:28,302] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8979636e-17 1.7032845e-10 6.3988705e-07 1.7880034e-06 1.4789954e-10
 8.2743903e-13 1.9582775e-07 4.9661067e-14 7.2964007e-17 4.9325628e-19
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:34:28,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7804
[2019-04-08 15:34:28,325] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.666666666666667, 69.16666666666667, 0.0, 0.0, 22.5, 27.99334785006655, 1.10081251673011, 1.0, 1.0, 65.0, 23870.00806841888], 
current ob forecast is [], 
actual action is [3.333333333333333, 65.0], 
sim time this is 3779400.0000, 
sim time next is 3780000.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 22.5, 28.01013112248244, 0.945346785767977, 1.0, 1.0, 65.0, 41101.15571938392], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.375, 0.8341775935402035, 0.8151155952559924, 1.0, 1.0, 1.0, 0.1957197891399234], 
reward next is 0.8043, 
noisyNet noise sample is [array([-2.6260836], dtype=float32), 1.4570532]. 
=============================================
[2019-04-08 15:34:28,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.83557 ]
 [86.09506 ]
 [86.187256]
 [86.45608 ]
 [86.652985]], R is [[85.63346863]
 [85.66346741]
 [85.68197632]
 [85.69399261]
 [85.71590424]].
[2019-04-08 15:34:28,345] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.5, 61.0, 6.0, 163.0, 22.5, 26.59051721308608, 0.6685052749718555, 1.0, 1.0, 65.0, 48839.71257419635], 
current ob forecast is [], 
actual action is [-2.5, 65.0], 
sim time this is 3915000.0000, 
sim time next is 3915600.0000, 
raw observation next is [-7.666666666666666, 60.0, 20.16666666666666, 213.5, 22.5, 26.53364371457673, 0.6959472145984417, 1.0, 1.0, 65.0, 48294.81848468714], 
processed observation next is [1.0, 0.30434782608695654, 0.25023084025854114, 0.6, 0.0672222222222222, 0.23591160220994475, 0.375, 0.7111369762147275, 0.7319824048661472, 1.0, 1.0, 1.0, 0.2299753261175578], 
reward next is 0.7700, 
noisyNet noise sample is [array([-1.6695957], dtype=float32), 0.7607618]. 
=============================================
[2019-04-08 15:34:29,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1079026e-18 2.6525943e-11 2.2550212e-08 2.0577497e-07 5.9355743e-11
 1.3135220e-12 9.8456727e-09 3.1062106e-15 1.7146444e-18 3.1050654e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:34:29,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5517
[2019-04-08 15:34:29,198] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.333333333333334, 73.0, 0.0, 0.0, 19.0, 26.6801528021008, 0.6927346206105289, 0.0, 1.0, 65.0, 48739.08187153504], 
current ob forecast is [], 
actual action is [0.6666666666666661, 65.0], 
sim time this is 3820800.0000, 
sim time next is 3821400.0000, 
raw observation next is [-4.5, 74.0, 0.0, 0.0, 19.0, 26.67321183393068, 0.6909219008422114, 0.0, 1.0, 65.0, 49858.37135608211], 
processed observation next is [1.0, 0.21739130434782608, 0.3379501385041552, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7227676528275566, 0.7303073002807371, 0.0, 1.0, 1.0, 0.2374208159813434], 
reward next is 0.7626, 
noisyNet noise sample is [array([-0.88650537], dtype=float32), -0.320918]. 
=============================================
[2019-04-08 15:34:29,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2342623e-17 7.7000367e-10 2.3658619e-07 7.2324019e-07 1.4188971e-10
 7.6716696e-14 7.3119342e-09 3.5779956e-14 4.1693098e-19 2.3257445e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:34:29,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5176
[2019-04-08 15:34:29,767] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 68.0, 0.0, 0.0, 22.5, 27.60634288704068, 0.9955481926613463, 0.0, 1.0, 65.0, 33839.27426638365], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3785400.0000, 
sim time next is 3786000.0000, 
raw observation next is [-2.0, 67.0, 0.0, 0.0, 22.5, 27.52136833066421, 0.9869790763458116, 1.0, 1.0, 65.0, 35592.528963462], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.67, 0.0, 0.0, 0.375, 0.7934473608886842, 0.8289930254486039, 1.0, 1.0, 1.0, 0.16948823315934286], 
reward next is 0.8305, 
noisyNet noise sample is [array([-1.5176879], dtype=float32), 0.09371808]. 
=============================================
[2019-04-08 15:34:29,781] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.06393 ]
 [85.285995]
 [85.18996 ]
 [84.99283 ]
 [84.92799 ]], R is [[85.38105774]
 [85.36610413]
 [85.35727692]
 [85.35269928]
 [85.35879517]].
[2019-04-08 15:34:29,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2682593e-17 8.6206164e-10 7.7677652e-07 8.8103457e-07 4.2441123e-10
 2.5097786e-12 5.1295764e-08 8.3986712e-14 2.3791749e-17 5.3850353e-20
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:34:29,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7891
[2019-04-08 15:34:29,865] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.333333333333333, 69.0, 0.0, 0.0, 19.0, 26.86073012831711, 0.7034643614582224, 0.0, 1.0, 65.0, 45965.06085453377], 
current ob forecast is [], 
actual action is [1.666666666666667, 65.0], 
sim time this is 3738000.0000, 
sim time next is 3738600.0000, 
raw observation next is [-3.5, 71.0, 0.0, 0.0, 19.0, 26.93624172304327, 0.6854527662427533, 0.0, 1.0, 65.0, 44810.19100768883], 
processed observation next is [1.0, 0.2608695652173913, 0.36565096952908593, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7446868102536058, 0.7284842554142511, 0.0, 1.0, 1.0, 0.21338186194137537], 
reward next is 0.7866, 
noisyNet noise sample is [array([-0.51462096], dtype=float32), -0.28751647]. 
=============================================
[2019-04-08 15:34:30,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.54769858e-19 8.42351744e-10 3.85927365e-07 3.63121535e-06
 1.09290244e-10 1.29032668e-12 1.06103826e-07 3.60033523e-14
 2.50124383e-18 6.50365587e-21 9.99995828e-01], sum to 1.0000
[2019-04-08 15:34:30,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4544665e-18 5.0787836e-11 3.0563407e-07 5.0219512e-07 6.7225996e-11
 5.6737677e-13 8.7480032e-09 2.9306633e-15 1.4420566e-20 1.9598263e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:34:30,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3690
[2019-04-08 15:34:30,091] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9353
[2019-04-08 15:34:30,110] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 60.0, 48.33333333333334, 408.3333333333334, 22.5, 28.87266677577864, 1.204968531083476, 1.0, 1.0, 65.0, 18849.29333505349], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3775800.0000, 
sim time next is 3776400.0000, 
raw observation next is [0.0, 60.0, 40.5, 343.0, 22.5, 28.8707529347499, 0.9582334293520826, 1.0, 1.0, 65.0, 18847.45552301703], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.6, 0.135, 0.37900552486187844, 0.375, 0.9058960778958248, 0.8194111431173609, 1.0, 1.0, 1.0, 0.089749788204843], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.42177078], dtype=float32), 0.6446741]. 
=============================================
[2019-04-08 15:34:30,133] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 22.5, 26.47250640523474, 0.6918525461355642, 1.0, 1.0, 65.0, 49747.39080614519], 
current ob forecast is [], 
actual action is [0.16666666666666607, 65.0], 
sim time this is 3831000.0000, 
sim time next is 3831600.0000, 
raw observation next is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 22.5, 26.53804095126075, 0.7312511602054531, 1.0, 1.0, 65.0, 46661.58528357729], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333333, 0.75, 0.2555555555555556, 0.43904235727440144, 0.375, 0.7115034126050626, 0.743750386735151, 1.0, 1.0, 1.0, 0.22219802515989184], 
reward next is 0.7778, 
noisyNet noise sample is [array([-1.3998834], dtype=float32), -1.4317794]. 
=============================================
[2019-04-08 15:34:30,539] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.4229764e-18 1.1524801e-11 1.2126596e-07 3.6283566e-08 1.2991395e-10
 9.7093687e-13 2.8714681e-10 1.9138053e-15 2.0123753e-19 3.8172115e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:34:30,543] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8712
[2019-04-08 15:34:30,576] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 22.5, 27.98325551094927, 0.9460114977872434, 1.0, 1.0, 65.0, 23220.64374013303], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3751800.0000, 
sim time next is 3752400.0000, 
raw observation next is [-3.0, 75.0, 109.1666666666667, 753.3333333333334, 22.5, 28.03931837230441, 0.963547096825189, 1.0, 1.0, 65.0, 22638.65549769156], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.75, 0.363888888888889, 0.8324125230202579, 0.375, 0.8366098643587009, 0.8211823656083963, 1.0, 1.0, 1.0, 0.10780312141757886], 
reward next is 0.8922, 
noisyNet noise sample is [array([-0.7933142], dtype=float32), -0.979475]. 
=============================================
[2019-04-08 15:34:30,792] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6053248e-18 4.5170985e-11 2.3087128e-09 3.9304695e-08 2.7047308e-12
 3.0272413e-13 3.4192210e-10 1.7735019e-15 3.5237948e-19 4.7102276e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:34:30,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5770
[2019-04-08 15:34:30,824] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 60.00000000000001, 115.8333333333333, 814.1666666666666, 22.5, 27.92435868578359, 0.7448795219655935, 1.0, 1.0, 65.0, 29811.46810318419], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3842400.0000, 
sim time next is 3843000.0000, 
raw observation next is [-1.0, 60.0, 117.0, 822.0, 22.5, 27.75943443297108, 1.045792175029596, 1.0, 1.0, 65.0, 46128.75894568376], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9082872928176795, 0.375, 0.81328620274759, 0.8485973916765319, 1.0, 1.0, 1.0, 0.21966075688420839], 
reward next is 0.7803, 
noisyNet noise sample is [array([0.34643865], dtype=float32), 0.2051412]. 
=============================================
[2019-04-08 15:34:30,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.78474]
 [82.84887]
 [82.87313]
 [82.8942 ]
 [82.96529]], R is [[82.71750641]
 [82.74837494]
 [82.80581665]
 [82.86990356]
 [82.93070984]].
[2019-04-08 15:34:31,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.73530915e-18 1.59237470e-10 1.13444045e-07 1.17621244e-06
 2.19099211e-10 5.29530304e-13 2.64106497e-08 1.41058540e-14
 8.56430546e-18 8.32778703e-21 9.99998689e-01], sum to 1.0000
[2019-04-08 15:34:31,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0615
[2019-04-08 15:34:31,828] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.333333333333334, 73.0, 0.0, 0.0, 19.0, 26.6801485564451, 0.6927332516037628, 0.0, 1.0, 65.0, 48739.13400787943], 
current ob forecast is [], 
actual action is [0.6666666666666661, 65.0], 
sim time this is 3820800.0000, 
sim time next is 3821400.0000, 
raw observation next is [-4.5, 74.0, 0.0, 0.0, 19.0, 26.67320765954162, 0.6909205519259031, 0.0, 1.0, 65.0, 49858.4227511932], 
processed observation next is [1.0, 0.21739130434782608, 0.3379501385041552, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7227673049618017, 0.7303068506419677, 0.0, 1.0, 1.0, 0.2374210607199676], 
reward next is 0.7626, 
noisyNet noise sample is [array([1.1193126], dtype=float32), 1.3637687]. 
=============================================
[2019-04-08 15:34:32,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.96816304e-17 1.02267125e-10 5.09605016e-08 1.30103604e-06
 2.56047850e-10 6.25618101e-13 1.79020532e-07 1.71872621e-13
 2.33610425e-18 1.31748691e-20 9.99998450e-01], sum to 1.0000
[2019-04-08 15:34:32,346] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2313
[2019-04-08 15:34:32,382] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.7020783889846, 0.7190979673814591, 0.0, 1.0, 65.0, 50155.21036356455], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3815400.0000, 
sim time next is 3816000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.69658440534135, 0.7153378862089269, 0.0, 1.0, 65.0, 49823.32456967927], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7247153671117793, 0.7384459620696423, 0.0, 1.0, 1.0, 0.23725392652228225], 
reward next is 0.7627, 
noisyNet noise sample is [array([-0.73795617], dtype=float32), -1.2769935]. 
=============================================
[2019-04-08 15:34:32,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.67967 ]
 [81.73011 ]
 [81.75815 ]
 [81.810646]
 [81.81842 ]], R is [[81.59480286]
 [81.5400238 ]
 [81.49339294]
 [81.44936371]
 [81.4009552 ]].
[2019-04-08 15:34:32,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0187451e-17 9.5439667e-10 9.9181861e-08 2.6180030e-07 3.2805182e-11
 2.3379198e-12 4.5333561e-08 1.5253593e-14 9.4757417e-19 5.7716840e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:34:32,508] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1774
[2019-04-08 15:34:32,516] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9197220e-18 1.5899028e-10 2.0061659e-08 4.2062635e-07 1.0455175e-10
 1.6579934e-13 2.1446622e-09 2.5493155e-15 2.5742800e-18 6.7263290e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:34:32,517] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5004
[2019-04-08 15:34:32,520] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 77.0, 48.0, 298.0, 22.5, 26.47720650485483, 0.6753416607683294, 1.0, 1.0, 65.0, 49720.78856210397], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3830400.0000, 
sim time next is 3831000.0000, 
raw observation next is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 22.5, 26.47250782569708, 0.6918534907010211, 1.0, 1.0, 65.0, 49747.41307101587], 
processed observation next is [1.0, 0.34782608695652173, 0.32871652816251157, 0.76, 0.2077777777777778, 0.38416206261510133, 0.375, 0.7060423188080899, 0.7306178302336738, 1.0, 1.0, 1.0, 0.23689244319531366], 
reward next is 0.7631, 
noisyNet noise sample is [array([-0.06271368], dtype=float32), -1.338236]. 
=============================================
[2019-04-08 15:34:32,522] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.125305]
 [82.66316 ]
 [81.92655 ]
 [81.23562 ]
 [80.729095]], R is [[83.74179077]
 [83.66761017]
 [83.59268188]
 [83.51322174]
 [83.43473053]].
[2019-04-08 15:34:32,533] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 74.0, 0.0, 0.0, 19.0, 26.97795352763345, 0.7573286608840625, 0.0, 1.0, 65.0, 45386.78211368612], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3810600.0000, 
sim time next is 3811200.0000, 
raw observation next is [-4.0, 73.0, 0.0, 0.0, 19.0, 26.88689462881908, 0.7462231231538455, 0.0, 1.0, 65.0, 47784.53520140384], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7405745524015899, 0.7487410410512818, 0.0, 1.0, 1.0, 0.22754540572097068], 
reward next is 0.7725, 
noisyNet noise sample is [array([1.170016], dtype=float32), -0.6243431]. 
=============================================
[2019-04-08 15:34:32,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3176570e-18 2.8102837e-10 1.5023005e-07 4.0721125e-06 3.5114578e-10
 1.5743746e-13 8.5080520e-09 1.7529114e-13 5.3925889e-19 9.9622822e-20
 9.9999583e-01], sum to 1.0000
[2019-04-08 15:34:32,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0377
[2019-04-08 15:34:32,694] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 50.0, 0.0, 0.0, 22.5, 27.40430636234517, 1.029008572769739, 1.0, 1.0, 65.00000000000003, 27311.6657156999], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [1.166666666666667, 50.5, 0.0, 0.0, 22.5, 27.69361209972229, 1.030854070388065, 1.0, 1.0, 65.0, 23070.63619786119], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.505, 0.0, 0.0, 0.375, 0.8078010083101909, 0.8436180234626883, 1.0, 1.0, 1.0, 0.10986017237076758], 
reward next is 0.8901, 
noisyNet noise sample is [array([2.1359951], dtype=float32), 0.062313527]. 
=============================================
[2019-04-08 15:34:33,122] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.6411057e-16 4.3174818e-11 2.4518331e-06 5.9475628e-06 4.8247970e-09
 1.2108040e-12 2.7207798e-07 1.1082538e-12 1.8936996e-17 5.2051643e-19
 9.9999130e-01], sum to 1.0000
[2019-04-08 15:34:33,123] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8958
[2019-04-08 15:34:33,140] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 19.0, 26.67010515132946, 0.6687250522040928, 0.0, 1.0, 65.0, 48846.88056250956], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3824400.0000, 
sim time next is 3825000.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 19.0, 26.58642226878326, 0.6690145167145922, 0.0, 1.0, 65.0, 51487.61948721856], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7155351890652716, 0.7230048389048641, 0.0, 1.0, 1.0, 0.24517914041532646], 
reward next is 0.7548, 
noisyNet noise sample is [array([-0.42681125], dtype=float32), -0.31400472]. 
=============================================
[2019-04-08 15:34:33,149] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.985115]
 [79.941696]
 [79.92986 ]
 [79.90805 ]
 [79.91414 ]], R is [[79.93192291]
 [79.90000153]
 [79.87523651]
 [79.84560394]
 [79.81467438]].
[2019-04-08 15:34:34,063] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.6887275e-16 1.6035429e-10 8.9448724e-09 8.3645564e-06 3.2004004e-11
 1.3698735e-13 5.3266913e-08 1.6760998e-14 1.8662411e-17 8.0392805e-20
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:34:34,064] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0135
[2019-04-08 15:34:34,088] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 74.0, 0.0, 0.0, 19.0, 26.84957453639903, 0.8049965199416604, 0.0, 1.0, 65.0, 47797.54966966929], 
current ob forecast is [], 
actual action is [1.5, 65.0], 
sim time this is 3803400.0000, 
sim time next is 3804000.0000, 
raw observation next is [-3.666666666666667, 75.0, 0.0, 0.0, 19.0, 26.92389064602748, 0.793436669699187, 0.0, 1.0, 65.0, 45652.89556810725], 
processed observation next is [1.0, 0.0, 0.3610341643582641, 0.75, 0.0, 0.0, 0.08333333333333333, 0.7436575538356234, 0.764478889899729, 0.0, 1.0, 1.0, 0.21739474080051072], 
reward next is 0.7826, 
noisyNet noise sample is [array([0.08880055], dtype=float32), -1.4359705]. 
=============================================
[2019-04-08 15:34:34,109] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.41849 ]
 [80.5242  ]
 [81.1328  ]
 [81.415504]
 [81.79294 ]], R is [[80.03250885]
 [80.00457764]
 [79.97697449]
 [79.95191193]
 [79.93032837]].
[2019-04-08 15:34:35,265] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7347030e-19 1.3908190e-11 8.5698140e-08 3.4002147e-07 6.3180437e-12
 2.5100517e-14 1.3210184e-09 6.1800964e-16 5.2868142e-19 1.5559845e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:34:35,265] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3569
[2019-04-08 15:34:35,292] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2665269e-17 3.9692946e-08 6.7144142e-06 1.5928707e-06 1.8013905e-08
 3.0037351e-11 2.5840976e-07 8.7549400e-14 2.4916358e-16 7.5623230e-20
 9.9999142e-01], sum to 1.0000
[2019-04-08 15:34:35,292] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.666666666666667, 49.0, 111.8333333333333, 817.0, 22.5, 28.43191370211546, 1.10736986394759, 1.0, 1.0, 65.00000000000003, 18848.20510715288], 
current ob forecast is [], 
actual action is [6.666666666666667, 65.0], 
sim time this is 3850800.0000, 
sim time next is 3851400.0000, 
raw observation next is [1.833333333333333, 48.5, 110.6666666666667, 810.0, 22.5, 28.4326764514591, 1.114273749711087, 1.0, 1.0, 65.0, 18848.36798646228], 
processed observation next is [1.0, 0.5652173913043478, 0.5133887349953832, 0.485, 0.368888888888889, 0.8950276243093923, 0.375, 0.8693897042882582, 0.871424583237029, 1.0, 1.0, 1.0, 0.089754133268868], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.18446428], dtype=float32), 2.7492304]. 
=============================================
[2019-04-08 15:34:35,300] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6464666e-16 9.8831221e-11 3.7716825e-07 1.7116615e-06 3.8272049e-10
 1.7848996e-12 1.9645783e-07 3.6020750e-13 3.8511122e-18 5.4649329e-20
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:34:35,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1980
[2019-04-08 15:34:35,308] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5381
[2019-04-08 15:34:35,329] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 55.83333333333334, 0.0, 0.0, 19.0, 27.30915787100345, 0.8889921975318207, 0.0, 1.0, 65.0, 35123.98631399315], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3881400.0000, 
sim time next is 3882000.0000, 
raw observation next is [-1.0, 56.66666666666667, 0.0, 0.0, 19.0, 27.30054545701596, 0.8904508614512859, 0.0, 1.0, 65.0, 39495.08001900798], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.5666666666666668, 0.0, 0.0, 0.08333333333333333, 0.77504545475133, 0.7968169538170953, 0.0, 1.0, 1.0, 0.18807180961432374], 
reward next is 0.8119, 
noisyNet noise sample is [array([-0.8587211], dtype=float32), 0.8641105]. 
=============================================
[2019-04-08 15:34:35,331] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.333333333333334, 65.0, 0.0, 0.0, 19.0, 26.68782392084246, 0.703663402435945, 0.0, 1.0, 65.0, 48936.80303045009], 
current ob forecast is [], 
actual action is [-0.3333333333333339, 65.0], 
sim time this is 3908400.0000, 
sim time next is 3909000.0000, 
raw observation next is [-5.666666666666666, 62.0, 0.0, 0.0, 19.0, 26.65250158858445, 0.6938054870775966, 0.0, 1.0, 65.0, 48917.44327117741], 
processed observation next is [1.0, 0.21739130434782608, 0.3056325023084026, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7210417990487041, 0.7312684956925323, 0.0, 1.0, 1.0, 0.23294020605322577], 
reward next is 0.7671, 
noisyNet noise sample is [array([-0.7160542], dtype=float32), 0.20909567]. 
=============================================
[2019-04-08 15:34:35,364] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.51258 ]
 [80.23293 ]
 [80.18732 ]
 [79.70522 ]
 [79.476814]], R is [[80.79070282]
 [80.8155365 ]
 [80.84825134]
 [80.88383484]
 [80.91788483]].
[2019-04-08 15:34:35,379] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.33326 ]
 [82.388855]
 [82.4628  ]
 [82.43568 ]
 [82.39844 ]], R is [[82.15402985]
 [82.09945679]
 [82.05680847]
 [82.02338409]
 [81.98685455]].
[2019-04-08 15:34:35,934] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4745222e-17 3.4714596e-09 8.2001216e-06 1.1317085e-05 2.1780222e-09
 1.7038182e-11 1.1362749e-06 7.2743011e-13 1.0663863e-16 2.5470427e-19
 9.9997938e-01], sum to 1.0000
[2019-04-08 15:34:35,935] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1088
[2019-04-08 15:34:35,971] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 27.01398137203606, 0.8327950844563997, 0.0, 1.0, 65.0, 41123.14590941748], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3888000.0000, 
sim time next is 3888600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.99878776353254, 0.8277541548070649, 0.0, 1.0, 65.0, 41293.50774853186], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7498989802943784, 0.775918051602355, 0.0, 1.0, 1.0, 0.19663575118348506], 
reward next is 0.8034, 
noisyNet noise sample is [array([-0.41978085], dtype=float32), -0.47710735]. 
=============================================
[2019-04-08 15:34:36,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.50139661e-17 5.51849400e-10 1.14395004e-06 1.50241385e-06
 3.94284223e-11 1.39906316e-12 1.48065089e-08 1.00414545e-14
 5.08297192e-18 4.92349654e-21 9.99997377e-01], sum to 1.0000
[2019-04-08 15:34:36,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8510
[2019-04-08 15:34:36,279] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.0, 53.83333333333334, 94.0, 541.3333333333333, 22.5, 27.01133806880222, 0.7882388599471941, 1.0, 1.0, 65.0, 40157.6654625141], 
current ob forecast is [], 
actual action is [-3.0, 65.0], 
sim time this is 3919800.0000, 
sim time next is 3920400.0000, 
raw observation next is [-8.0, 53.0, 95.5, 579.0, 22.5, 27.10651940800471, 0.794006009304533, 1.0, 1.0, 65.0, 37527.46885755886], 
processed observation next is [1.0, 0.391304347826087, 0.24099722991689754, 0.53, 0.31833333333333336, 0.6397790055248619, 0.375, 0.7588766173337259, 0.7646686697681777, 1.0, 1.0, 1.0, 0.17870223265504218], 
reward next is 0.8213, 
noisyNet noise sample is [array([-0.24468936], dtype=float32), -0.6360221]. 
=============================================
[2019-04-08 15:34:36,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6365398e-15 5.6538274e-10 2.2715315e-06 7.5101402e-06 9.2627666e-09
 1.3463627e-12 1.7204535e-06 1.2989158e-11 2.5615393e-16 5.8843856e-19
 9.9998844e-01], sum to 1.0000
[2019-04-08 15:34:36,985] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4227
[2019-04-08 15:34:36,995] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 26.89970317263084, 0.7652428754024502, 0.0, 1.0, 65.0, 43570.72525828544], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3902400.0000, 
sim time next is 3903000.0000, 
raw observation next is [-3.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.97526365173928, 0.758662205366322, 0.0, 1.0, 65.0, 40318.30102978083], 
processed observation next is [1.0, 0.17391304347826086, 0.3748845798707295, 0.72, 0.0, 0.0, 0.08333333333333333, 0.74793863764494, 0.752887401788774, 0.0, 1.0, 1.0, 0.191991909665623], 
reward next is 0.8080, 
noisyNet noise sample is [array([0.477452], dtype=float32), 1.1938249]. 
=============================================
[2019-04-08 15:34:37,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.219444]
 [80.17132 ]
 [80.14649 ]
 [80.10941 ]
 [80.12052 ]], R is [[80.25311279]
 [80.24310303]
 [80.23755646]
 [80.23612213]
 [80.24310303]].
[2019-04-08 15:34:37,242] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-08 15:34:37,242] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:34:37,243] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:34:37,243] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:34:37,243] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:34:37,244] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:34:37,245] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:34:37,249] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run13
[2019-04-08 15:34:37,287] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run13
[2019-04-08 15:34:37,287] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run13
[2019-04-08 15:34:53,577] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06100328]
[2019-04-08 15:34:53,577] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-10.78333333333333, 50.33333333333334, 0.0, 0.0, 22.5, 26.89875608316364, 0.6441466289153646, 1.0, 1.0, 65.0, 43259.23186881487]
[2019-04-08 15:34:53,577] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:34:53,577] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [7.1522488e-15 1.1466481e-08 1.9518523e-06 5.0008261e-06 3.0745049e-09
 2.9620986e-11 1.7963231e-07 2.2941718e-12 1.1738202e-15 1.1894193e-17
 9.9999285e-01], sampled 0.7229860743039134
[2019-04-08 15:35:27,844] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06100328]
[2019-04-08 15:35:27,844] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-8.001389060333333, 85.49110508333334, 0.0, 0.0, 19.0, 25.90817862327784, 0.5236994438055054, 0.0, 1.0, 65.0, 56648.20613518958]
[2019-04-08 15:35:27,844] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:35:27,845] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [2.0637939e-16 8.1946705e-10 3.8398440e-07 1.3685583e-06 4.7984799e-10
 1.7870704e-12 6.9235774e-08 1.0604103e-13 1.9243270e-17 1.1223975e-19
 9.9999821e-01], sampled 0.05804506343838611
[2019-04-08 15:36:12,307] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6989.9711 316266059.7372 2958.1803
[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,328] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:12,485] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,382] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.0306 355943583.6214 2370.6695
[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,405] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:20,530] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,794] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3968 342846678.2579 2768.2239
[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,815] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:22,930] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:36:23,817] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 240000, evaluation results [240000.0, 6863.396770200469, 342846678.25790673, 2768.223939251608, 6989.971144108372, 316266059.73724324, 2958.180307199658, 6801.030554183986, 355943583.62136, 2370.669476478192]
[2019-04-08 15:36:24,192] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5768669e-16 1.7863968e-09 6.2631841e-07 1.5119998e-06 2.0214863e-09
 1.2903712e-11 4.7099192e-07 7.8112365e-13 3.6270302e-17 6.2559973e-20
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:36:24,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9059
[2019-04-08 15:36:24,227] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.81400512706945, 0.7313131619678727, 0.0, 1.0, 65.0, 45942.20211248867], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3906000.0000, 
sim time next is 3906600.0000, 
raw observation next is [-4.333333333333334, 74.0, 0.0, 0.0, 19.0, 26.80555449866847, 0.7302602049824616, 0.0, 1.0, 65.0, 45421.67330313801], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7337962082223726, 0.7434200683274872, 0.0, 1.0, 1.0, 0.2162936823958953], 
reward next is 0.7837, 
noisyNet noise sample is [array([0.6348082], dtype=float32), -1.9258554]. 
=============================================
[2019-04-08 15:36:24,449] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2725004e-18 4.7045051e-10 4.3523863e-07 5.5133137e-07 1.8406845e-10
 1.5939172e-12 3.7853173e-08 1.1331651e-14 4.2507771e-20 7.1896128e-20
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:36:24,454] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7328
[2019-04-08 15:36:24,469] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.97526513884414, 0.7586626655396179, 0.0, 1.0, 65.0, 40318.28428989103], 
current ob forecast is [], 
actual action is [1.833333333333333, 65.0], 
sim time this is 3903000.0000, 
sim time next is 3903600.0000, 
raw observation next is [-3.333333333333333, 73.0, 0.0, 0.0, 19.0, 26.98863698443085, 0.7484412439584657, 0.0, 1.0, 65.0, 40959.37678234551], 
processed observation next is [1.0, 0.17391304347826086, 0.37026777469990774, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7490530820359043, 0.7494804146528219, 0.0, 1.0, 1.0, 0.19504465134450244], 
reward next is 0.8050, 
noisyNet noise sample is [array([-1.3001331], dtype=float32), 1.9591922]. 
=============================================
[2019-04-08 15:36:24,496] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6737874e-17 3.5345542e-09 9.2059295e-08 4.7126198e-07 1.6127327e-10
 1.1692311e-12 2.4047095e-08 6.1563570e-15 9.2951245e-18 2.0749305e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:36:24,497] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2043
[2019-04-08 15:36:24,522] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.0, 57.16666666666667, 62.66666666666667, 365.0, 22.5, 26.68020924903924, 0.7154344122280062, 1.0, 1.0, 65.0, 47514.78048746911], 
current ob forecast is [], 
actual action is [-3.0, 65.0], 
sim time this is 3917400.0000, 
sim time next is 3918000.0000, 
raw observation next is [-8.0, 56.33333333333334, 76.83333333333334, 415.5000000000001, 22.5, 26.6891855353957, 0.742871071000501, 1.0, 1.0, 65.0, 45574.03673739127], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.5633333333333335, 0.2561111111111111, 0.45911602209944763, 0.375, 0.7240987946163084, 0.7476236903335004, 1.0, 1.0, 1.0, 0.21701922255900602], 
reward next is 0.7830, 
noisyNet noise sample is [array([0.79725474], dtype=float32), -1.0374726]. 
=============================================
[2019-04-08 15:36:24,544] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.06143 ]
 [84.567566]
 [84.338036]
 [83.83356 ]
 [83.350204]], R is [[84.82345581]
 [84.7489624 ]
 [84.6776886 ]
 [84.61488342]
 [84.53876495]].
[2019-04-08 15:36:24,611] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.11232992e-18 6.57490382e-11 1.14917036e-07 2.71647764e-06
 2.32412787e-11 3.37910479e-14 5.31733946e-09 1.93806356e-14
 4.04437429e-18 1.65172681e-21 9.99997139e-01], sum to 1.0000
[2019-04-08 15:36:24,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0474
[2019-04-08 15:36:24,632] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.5, 61.5, 0.0, 0.0, 19.0, 26.66479393064289, 0.6857351599137217, 0.0, 1.0, 65.0, 47831.03313184204], 
current ob forecast is [], 
actual action is [-1.5, 65.0], 
sim time this is 3911400.0000, 
sim time next is 3912000.0000, 
raw observation next is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 19.0, 26.64136106788064, 0.6749115882041448, 0.0, 1.0, 65.0, 49237.98835865816], 
processed observation next is [1.0, 0.2608695652173913, 0.2779316712834719, 0.6233333333333333, 0.0, 0.0, 0.08333333333333333, 0.7201134223233868, 0.7249705294013816, 0.0, 1.0, 1.0, 0.23446661123170554], 
reward next is 0.7655, 
noisyNet noise sample is [array([-1.2512736], dtype=float32), 0.21947831]. 
=============================================
[2019-04-08 15:36:24,646] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.923   ]
 [80.085434]
 [80.20256 ]
 [80.3432  ]
 [80.4977  ]], R is [[79.78857422]
 [79.76292419]
 [79.74010468]
 [79.70989227]
 [79.67700195]].
[2019-04-08 15:36:25,172] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2815532e-16 2.7279073e-10 3.0121011e-08 1.9706895e-06 2.0853957e-11
 4.6173297e-13 3.4720057e-10 3.6968326e-14 6.5226368e-18 7.8758677e-21
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:36:25,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8673
[2019-04-08 15:36:25,196] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.166666666666667, 39.16666666666666, 112.3333333333333, 812.0, 22.5, 28.03835493997779, 1.033247112637419, 1.0, 1.0, 65.0, 23055.02596666756], 
current ob forecast is [], 
actual action is [-0.16666666666666696, 65.0], 
sim time this is 3937800.0000, 
sim time next is 3938400.0000, 
raw observation next is [-5.0, 38.0, 110.5, 806.0, 22.5, 28.07033115821367, 1.045977936472176, 1.0, 1.0, 65.0, 22976.51532957055], 
processed observation next is [1.0, 0.6086956521739131, 0.32409972299168976, 0.38, 0.36833333333333335, 0.8906077348066298, 0.375, 0.8391942631844724, 0.848659312157392, 1.0, 1.0, 1.0, 0.10941197775985977], 
reward next is 0.8906, 
noisyNet noise sample is [array([0.13420977], dtype=float32), -1.8105124]. 
=============================================
[2019-04-08 15:36:25,323] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5550436e-15 1.2472908e-08 4.6830672e-07 8.1287872e-05 5.6366117e-10
 9.8225955e-11 6.6412795e-08 1.5174626e-12 2.8709163e-17 4.0984711e-19
 9.9991810e-01], sum to 1.0000
[2019-04-08 15:36:25,327] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7651
[2019-04-08 15:36:25,347] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 22.5, 27.69363805354183, 1.013700474108284, 0.0, 1.0, 65.0, 29170.08466713291], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3872400.0000, 
sim time next is 3873000.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 22.5, 27.62883793228941, 1.005293229433457, 0.0, 1.0, 65.0, 29935.70272107915], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.375, 0.8024031610241176, 0.8350977431444857, 0.0, 1.0, 1.0, 0.14255096533847214], 
reward next is 0.8574, 
noisyNet noise sample is [array([1.0833625], dtype=float32), -0.6647616]. 
=============================================
[2019-04-08 15:36:25,363] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.618515]
 [80.63651 ]
 [80.86166 ]
 [80.80293 ]
 [80.71226 ]], R is [[80.40480804]
 [80.46185303]
 [80.52444458]
 [80.596138  ]
 [80.67089081]].
[2019-04-08 15:36:25,468] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2694818e-15 1.4309329e-09 1.3734075e-07 4.0349855e-06 6.3881705e-11
 2.1762533e-11 8.9635130e-08 5.6554127e-13 1.5535856e-17 2.8973441e-19
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:36:25,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6663
[2019-04-08 15:36:25,492] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 38.0, 42.5, 352.5, 22.5, 28.21010056897059, 1.058848152189547, 1.0, 1.0, 65.0, 18843.97846930675], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 3949200.0000, 
sim time next is 3949800.0000, 
raw observation next is [-5.166666666666667, 38.5, 34.66666666666666, 291.3333333333333, 22.5, 28.4099972267588, 0.7776515426373413, 1.0, 1.0, 65.0, 23724.17708373768], 
processed observation next is [1.0, 0.7391304347826086, 0.31948291782086796, 0.385, 0.11555555555555552, 0.321915285451197, 0.375, 0.8674997688965668, 0.7592171808791138, 1.0, 1.0, 1.0, 0.11297227182732228], 
reward next is 0.8870, 
noisyNet noise sample is [array([0.1891467], dtype=float32), 0.39382434]. 
=============================================
[2019-04-08 15:36:25,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.57716216e-17 2.19667118e-10 7.23729386e-07 2.38682020e-07
 7.73602502e-11 3.01188978e-13 2.18343992e-08 1.08591165e-14
 3.35569990e-18 7.45917630e-21 9.99999046e-01], sum to 1.0000
[2019-04-08 15:36:25,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1056
[2019-04-08 15:36:25,860] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 19.0, 26.5096036641209, 0.6613021217452888, 0.0, 1.0, 65.0, 52146.71343620891], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 3913200.0000, 
sim time next is 3913800.0000, 
raw observation next is [-7.166666666666667, 63.0, 0.0, 0.0, 22.5, 26.46556800345569, 0.6650941736586409, 0.0, 1.0, 65.0, 52403.22086439096], 
processed observation next is [1.0, 0.30434782608695654, 0.26408125577100644, 0.63, 0.0, 0.0, 0.375, 0.7054640002879742, 0.7216980578862137, 0.0, 1.0, 1.0, 0.2495391469732903], 
reward next is 0.7505, 
noisyNet noise sample is [array([-1.5323032], dtype=float32), -1.2653477]. 
=============================================
[2019-04-08 15:36:26,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0669769e-16 5.7076326e-09 2.7296920e-07 6.6798964e-07 6.7937135e-11
 1.4900022e-12 2.5380722e-08 5.0256533e-13 7.6844384e-18 9.3061102e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:36:26,147] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8128
[2019-04-08 15:36:26,149] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7818811e-16 3.4568379e-10 1.7632950e-06 2.6064286e-06 6.4707378e-10
 5.3644787e-11 4.3371676e-07 3.8968647e-13 7.2833710e-17 2.0935366e-18
 9.9999523e-01], sum to 1.0000
[2019-04-08 15:36:26,170] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5374
[2019-04-08 15:36:26,175] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 19.0, 27.0161336546836, 0.8561598325007679, 0.0, 1.0, 65.0, 40928.67543772751], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 3961200.0000, 
sim time next is 3961800.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 19.0, 26.99018673586593, 0.8502035415940331, 0.0, 1.0, 65.0, 41242.02673398652], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7491822279888275, 0.7834011805313444, 0.0, 1.0, 1.0, 0.1963906034951739], 
reward next is 0.8036, 
noisyNet noise sample is [array([1.1750764], dtype=float32), 1.197488]. 
=============================================
[2019-04-08 15:36:26,201] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.83333333333333, 58.0, 0.0, 0.0, 19.0, 26.4258518896719, 0.6557015153398372, 0.0, 1.0, 65.0, 52609.43109079478], 
current ob forecast is [], 
actual action is [-5.83333333333333, 65.0], 
sim time this is 3977400.0000, 
sim time next is 3978000.0000, 
raw observation next is [-11.0, 58.0, 0.0, 0.0, 19.0, 26.39623813550924, 0.655471685784022, 0.0, 1.0, 65.0, 52961.04872147375], 
processed observation next is [1.0, 0.043478260869565216, 0.15789473684210528, 0.58, 0.0, 0.0, 0.08333333333333333, 0.6996865112924366, 0.7184905619280073, 0.0, 1.0, 1.0, 0.25219547010225596], 
reward next is 0.7478, 
noisyNet noise sample is [array([-1.1281505], dtype=float32), 0.18136643]. 
=============================================
[2019-04-08 15:36:26,218] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.2244 ]
 [77.25259]
 [77.6288 ]
 [77.71722]
 [78.32715]], R is [[77.08545685]
 [77.06408691]
 [77.03995514]
 [77.01776123]
 [76.98895264]].
[2019-04-08 15:36:26,583] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8634128e-16 3.8602077e-09 1.7557647e-06 2.4602983e-05 1.0356962e-09
 2.9915933e-12 2.7197913e-08 9.9054445e-13 9.1594429e-18 2.0414483e-19
 9.9997365e-01], sum to 1.0000
[2019-04-08 15:36:26,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6124
[2019-04-08 15:36:26,597] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 27.09846765522346, 0.7911528713445337, 0.0, 1.0, 65.0, 40089.04806740582], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3897600.0000, 
sim time next is 3898200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 27.03732797724772, 0.7830742962023786, 0.0, 1.0, 65.0, 41465.64187271629], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7531106647706434, 0.7610247654007929, 0.0, 1.0, 1.0, 0.1974554374891252], 
reward next is 0.8025, 
noisyNet noise sample is [array([-0.45709327], dtype=float32), -0.40769252]. 
=============================================
[2019-04-08 15:36:26,658] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8566497e-17 2.5737845e-10 7.5411123e-08 5.4692271e-07 7.5361398e-11
 4.5281179e-12 2.8692632e-08 7.7410199e-14 4.4226095e-19 5.5150361e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:36:26,659] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8854
[2019-04-08 15:36:26,695] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 19.0, 27.04870263640467, 0.8625425869492441, 0.0, 1.0, 65.0, 40472.71837870144], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 3960600.0000, 
sim time next is 3961200.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 19.0, 27.01613343930309, 0.8561597600297174, 0.0, 1.0, 65.0, 40928.67770754422], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7513444532752575, 0.7853865866765725, 0.0, 1.0, 1.0, 0.1948984652740201], 
reward next is 0.8051, 
noisyNet noise sample is [array([-0.18159452], dtype=float32), 0.26802814]. 
=============================================
[2019-04-08 15:36:28,064] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2892734e-17 5.6372157e-10 1.1342326e-07 1.3383359e-06 2.6877729e-11
 5.2572015e-13 2.6023464e-08 9.7148709e-14 2.8532942e-18 2.6840119e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:36:28,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3626
[2019-04-08 15:36:28,102] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.333333333333332, 45.5, 104.6666666666667, 725.6666666666666, 22.5, 27.13041225278034, 0.7334942132476056, 1.0, 1.0, 65.0, 31519.00807168531], 
current ob forecast is [], 
actual action is [-4.333333333333332, 65.0], 
sim time this is 4009800.0000, 
sim time next is 4010400.0000, 
raw observation next is [-9.0, 44.0, 106.5, 739.5, 22.5, 27.15552059530791, 0.7480253096109316, 1.0, 1.0, 65.0, 30830.54610502946], 
processed observation next is [1.0, 0.43478260869565216, 0.21329639889196678, 0.44, 0.355, 0.8171270718232044, 0.375, 0.7629600496089924, 0.7493417698703105, 1.0, 1.0, 1.0, 0.14681212430966412], 
reward next is 0.8532, 
noisyNet noise sample is [array([1.0969149], dtype=float32), -1.5588527]. 
=============================================
[2019-04-08 15:36:28,659] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.28831528e-16 5.11091669e-10 4.72111111e-07 1.43289128e-06
 1.64931280e-11 6.12753988e-12 7.10099446e-09 1.34731966e-14
 4.35216546e-17 1.05378765e-19 9.99998093e-01], sum to 1.0000
[2019-04-08 15:36:28,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3797
[2019-04-08 15:36:28,678] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 22.5, 27.4547863036037, 0.9326650477759201, 1.0, 1.0, 65.0, 33797.09485789476], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3955800.0000, 
sim time next is 3956400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 22.5, 27.40477728179204, 0.9207001386537977, 1.0, 1.0, 65.0, 34279.3673909658], 
processed observation next is [1.0, 0.8260869565217391, 0.296398891966759, 0.41, 0.0, 0.0, 0.375, 0.7837314401493366, 0.8069000462179327, 1.0, 1.0, 1.0, 0.16323508281412288], 
reward next is 0.8368, 
noisyNet noise sample is [array([-1.4397502], dtype=float32), -1.8901739]. 
=============================================
[2019-04-08 15:36:29,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.6039041e-16 7.4935164e-10 6.3413785e-07 5.4983338e-06 5.2570264e-09
 1.2570029e-12 1.7730785e-07 1.1855276e-12 1.7902861e-16 5.0734389e-19
 9.9999368e-01], sum to 1.0000
[2019-04-08 15:36:29,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7037
[2019-04-08 15:36:29,656] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 19.0, 26.41343468134907, 0.679645707623993, 0.0, 1.0, 65.0, 53753.65494468216], 
current ob forecast is [], 
actual action is [-4.833333333333334, 65.0], 
sim time this is 3973800.0000, 
sim time next is 3974400.0000, 
raw observation next is [-10.0, 58.0, 0.0, 0.0, 19.0, 26.39221584536653, 0.6746370960130745, 0.0, 1.0, 65.0, 54032.43238775824], 
processed observation next is [1.0, 0.0, 0.18559556786703602, 0.58, 0.0, 0.0, 0.08333333333333333, 0.6993513204472107, 0.7248790320043582, 0.0, 1.0, 1.0, 0.25729729708456306], 
reward next is 0.7427, 
noisyNet noise sample is [array([0.5642815], dtype=float32), -2.2635834]. 
=============================================
[2019-04-08 15:36:29,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4396615e-17 1.4926475e-09 2.8029708e-06 9.0589538e-06 2.7891739e-10
 4.0298268e-12 2.0900740e-07 6.1916200e-13 1.6901234e-17 1.6868183e-18
 9.9998796e-01], sum to 1.0000
[2019-04-08 15:36:29,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4809
[2019-04-08 15:36:30,015] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.0, 58.0, 93.0, 444.0, 22.5, 26.29535064298633, 0.5955916386584599, 1.0, 1.0, 65.0, 43638.70109826777], 
current ob forecast is [], 
actual action is [-7.0, 65.0], 
sim time this is 4005000.0000, 
sim time next is 4005600.0000, 
raw observation next is [-11.66666666666667, 56.33333333333333, 94.33333333333333, 486.3333333333333, 22.5, 26.54174231699051, 0.6169120937405982, 1.0, 1.0, 65.00000000000003, 38893.20489219899], 
processed observation next is [1.0, 0.34782608695652173, 0.13942751615881802, 0.5633333333333332, 0.3144444444444444, 0.5373848987108656, 0.375, 0.7118118597492092, 0.7056373645801993, 1.0, 1.0, 1.0000000000000007, 0.18520573758189995], 
reward next is 0.8148, 
noisyNet noise sample is [array([1.1954033], dtype=float32), 1.8160096]. 
=============================================
[2019-04-08 15:36:30,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9484000e-15 4.3995696e-09 1.3693070e-05 7.9215333e-06 4.9490034e-09
 2.6030738e-11 1.8225251e-07 3.3648933e-13 6.3222232e-16 9.3194965e-18
 9.9997818e-01], sum to 1.0000
[2019-04-08 15:36:30,440] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8218
[2019-04-08 15:36:30,454] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 28.5, 0.0, 0.0, 22.5, 27.61558536099771, 0.9128461584275204, 1.0, 1.0, 65.0, 28932.8623367638], 
current ob forecast is [], 
actual action is [1.5, 65.0], 
sim time this is 4041000.0000, 
sim time next is 4041600.0000, 
raw observation next is [-3.666666666666667, 29.33333333333333, 0.0, 0.0, 22.5, 27.48993197154691, 0.9019879941525613, 1.0, 1.0, 65.0, 30541.0769270338], 
processed observation next is [1.0, 0.782608695652174, 0.3610341643582641, 0.2933333333333333, 0.0, 0.0, 0.375, 0.7908276642955757, 0.8006626647175205, 1.0, 1.0, 1.0, 0.1454336996525419], 
reward next is 0.8546, 
noisyNet noise sample is [array([1.6900804], dtype=float32), 0.20909204]. 
=============================================
[2019-04-08 15:36:30,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1957777e-17 6.7186616e-09 3.1704920e-07 1.9065432e-06 1.3919883e-09
 8.0702433e-12 3.7824526e-09 3.6814589e-13 5.7846706e-17 3.9814882e-20
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:36:30,606] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-08 15:36:30,655] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.333333333333333, 37.5, 118.6666666666667, 824.3333333333334, 22.5, 27.24228157804719, 0.8623145691999069, 1.0, 1.0, 65.0, 45494.2198344718], 
current ob forecast is [], 
actual action is [-1.333333333333333, 65.0], 
sim time this is 4017000.0000, 
sim time next is 4017600.0000, 
raw observation next is [-6.0, 37.0, 118.5, 828.5, 22.5, 26.86872026820631, 0.811762906580037, 1.0, 1.0, 65.00000000000018, 33980.74533238136], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.37, 0.395, 0.9154696132596685, 0.375, 0.7390600223505258, 0.7705876355266791, 1.0, 1.0, 1.0000000000000038, 0.16181307301133982], 
reward next is 0.8382, 
noisyNet noise sample is [array([3.723727], dtype=float32), 0.039702483]. 
=============================================
[2019-04-08 15:36:30,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8981801e-14 7.3479653e-09 8.4749854e-07 7.3638521e-06 1.0445472e-09
 4.6507694e-11 4.9255494e-08 9.9482261e-13 4.2365928e-15 6.0475534e-18
 9.9999177e-01], sum to 1.0000
[2019-04-08 15:36:30,681] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0776
[2019-04-08 15:36:30,697] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 22.5, 27.1177775131224, 0.8505999789288299, 0.0, 1.0, 65.0, 35995.19510253704], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 4046400.0000, 
sim time next is 4047000.0000, 
raw observation next is [-4.0, 30.66666666666667, 0.0, 0.0, 19.0, 27.1500506580593, 0.8462230381667029, 1.0, 1.0, 65.0, 35242.32585921408], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.3066666666666667, 0.0, 0.0, 0.08333333333333333, 0.7625042215049417, 0.7820743460555676, 1.0, 1.0, 1.0, 0.16782059932959084], 
reward next is 0.8322, 
noisyNet noise sample is [array([-0.04943227], dtype=float32), 0.41199103]. 
=============================================
[2019-04-08 15:36:30,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.60879 ]
 [74.415184]
 [74.692314]
 [74.57801 ]
 [74.65065 ]], R is [[74.55729675]
 [74.64031982]
 [74.72413635]
 [74.80609131]
 [74.89241791]].
[2019-04-08 15:36:30,911] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.8442445e-15 9.8610375e-10 8.4208409e-08 5.6761019e-07 5.7173172e-10
 3.4449084e-12 5.8230523e-08 1.1969305e-13 2.0564247e-16 2.0992139e-19
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:36:30,911] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9928
[2019-04-08 15:36:30,929] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.24957067251713, 0.5699152025116602, 0.0, 1.0, 65.0, 53793.39732940552], 
current ob forecast is [], 
actual action is [-7.0, 65.0], 
sim time this is 3988200.0000, 
sim time next is 3988800.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.19493471914102, 0.5628100643373821, 0.0, 1.0, 65.0, 55159.40139941224], 
processed observation next is [1.0, 0.17391304347826086, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.6829112265950851, 0.6876033547791275, 0.0, 1.0, 1.0, 0.26266381618767737], 
reward next is 0.7373, 
noisyNet noise sample is [array([0.4508035], dtype=float32), 0.1505278]. 
=============================================
[2019-04-08 15:36:31,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5950812e-15 8.6962419e-09 4.2037464e-06 1.9446985e-04 4.4874714e-08
 9.9360131e-11 1.3210820e-06 5.7813689e-12 6.7551894e-15 3.0108190e-17
 9.9980003e-01], sum to 1.0000
[2019-04-08 15:36:31,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0649
[2019-04-08 15:36:31,322] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.0, 69.0, 0.0, 0.0, 19.0, 25.89745002645243, 0.4834443335560796, 0.0, 1.0, 65.0, 55742.79923184087], 
current ob forecast is [], 
actual action is [-9.0, 65.0], 
sim time this is 3999600.0000, 
sim time next is 4000200.0000, 
raw observation next is [-13.83333333333333, 68.0, 0.0, 0.0, 22.5, 25.89173772474305, 0.4779741635021345, 0.0, 1.0, 65.0, 55821.59434060403], 
processed observation next is [1.0, 0.30434782608695654, 0.07940904893813489, 0.68, 0.0, 0.0, 0.375, 0.657644810395254, 0.6593247211673782, 0.0, 1.0, 1.0, 0.26581711590763824], 
reward next is 0.7342, 
noisyNet noise sample is [array([1.2427015], dtype=float32), -1.2391882]. 
=============================================
[2019-04-08 15:36:31,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2469656e-16 4.4024997e-09 4.2881359e-07 3.7703603e-06 6.3888433e-10
 3.9938248e-12 1.6444589e-07 1.8832803e-13 4.6184482e-16 1.3229488e-18
 9.9999559e-01], sum to 1.0000
[2019-04-08 15:36:31,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4241
[2019-04-08 15:36:31,586] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 30.0, 0.0, 0.0, 19.0, 26.99233160135494, 0.8064018961275435, 0.0, 1.0, 65.0, 38176.59188381161], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 4051800.0000, 
sim time next is 4052400.0000, 
raw observation next is [-4.666666666666666, 30.33333333333333, 0.0, 0.0, 19.0, 26.97626011341118, 0.8019595476974697, 0.0, 1.0, 65.0, 38131.36370052779], 
processed observation next is [1.0, 0.9130434782608695, 0.33333333333333337, 0.3033333333333333, 0.0, 0.0, 0.08333333333333333, 0.7480216761175983, 0.7673198492324899, 0.0, 1.0, 1.0, 0.18157792238346568], 
reward next is 0.8184, 
noisyNet noise sample is [array([-0.7931212], dtype=float32), -2.8016734]. 
=============================================
[2019-04-08 15:36:31,713] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3471601e-14 2.8478087e-07 4.1230087e-05 4.5093591e-05 4.2270183e-09
 7.9949503e-10 1.3649811e-06 1.9601345e-12 4.2622550e-15 1.0407469e-17
 9.9991202e-01], sum to 1.0000
[2019-04-08 15:36:31,714] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6220
[2019-04-08 15:36:31,743] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 39.66666666666666, 0.0, 0.0, 19.0, 26.55703948220492, 0.6657496475950121, 0.0, 1.0, 65.0, 47103.74707252224], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4063200.0000, 
sim time next is 4063800.0000, 
raw observation next is [-6.0, 40.33333333333334, 0.0, 0.0, 19.0, 26.54223298995381, 0.6665071483038219, 0.0, 1.0, 65.0, 47424.71901223287], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.40333333333333343, 0.0, 0.0, 0.08333333333333333, 0.7118527491628175, 0.7221690494346072, 0.0, 1.0, 1.0, 0.22583199529634698], 
reward next is 0.7742, 
noisyNet noise sample is [array([-0.8181847], dtype=float32), 0.067005716]. 
=============================================
[2019-04-08 15:36:32,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2556629e-15 8.1208720e-09 3.5863155e-07 4.6292524e-07 4.3744444e-09
 3.6147568e-11 1.3326304e-06 9.4613044e-13 5.8671908e-16 7.7148290e-18
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:36:32,330] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6332
[2019-04-08 15:36:32,349] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 40.5, 0.0, 0.0, 19.0, 26.48784736439202, 0.6141196804136352, 0.0, 1.0, 65.0, 48523.65719405589], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 4074600.0000, 
sim time next is 4075200.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 19.0, 26.49835702157759, 0.611782817295953, 0.0, 1.0, 65.0, 47828.16986639838], 
processed observation next is [1.0, 0.17391304347826086, 0.32409972299168976, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7081964184647992, 0.7039276057653177, 0.0, 1.0, 1.0, 0.22775318983999226], 
reward next is 0.7722, 
noisyNet noise sample is [array([0.8811026], dtype=float32), 0.08874176]. 
=============================================
[2019-04-08 15:36:32,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4082831e-15 1.0436924e-09 3.3512301e-06 7.0266610e-06 1.1535888e-09
 4.9612914e-12 1.1823383e-07 2.3838239e-13 2.5943777e-16 9.2916960e-19
 9.9998951e-01], sum to 1.0000
[2019-04-08 15:36:32,741] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6472
[2019-04-08 15:36:32,757] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 39.0, 0.0, 0.0, 19.0, 26.59319230131054, 0.6214805027853302, 0.0, 1.0, 65.0, 46541.62769481858], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 4072800.0000, 
sim time next is 4073400.0000, 
raw observation next is [-5.0, 39.5, 0.0, 0.0, 19.0, 26.53722236894774, 0.6173310747660705, 0.0, 1.0, 65.0, 48312.08116328375], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.395, 0.0, 0.0, 0.08333333333333333, 0.7114351974123118, 0.7057770249220235, 0.0, 1.0, 1.0, 0.23005752934897025], 
reward next is 0.7699, 
noisyNet noise sample is [array([0.51387864], dtype=float32), -0.49145693]. 
=============================================
[2019-04-08 15:36:32,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9940655e-16 2.4595750e-09 3.1735627e-08 2.4487401e-06 1.3268178e-10
 1.7616865e-12 1.6983917e-07 1.3493948e-13 6.3152791e-17 3.5421011e-19
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:36:32,960] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9012
[2019-04-08 15:36:32,975] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.166666666666667, 30.66666666666666, 118.3333333333333, 838.6666666666667, 19.0, 27.23668892287427, 0.866629445642284, 0.0, 1.0, 65.0, 28901.06050463342], 
current ob forecast is [], 
actual action is [6.166666666666667, 65.0], 
sim time this is 4191000.0000, 
sim time next is 4191600.0000, 
raw observation next is [1.333333333333333, 31.33333333333334, 118.1666666666667, 842.8333333333334, 19.0, 27.2395753852288, 0.8747108152726701, 0.0, 1.0, 65.0, 29078.69084141246], 
processed observation next is [0.0, 0.5217391304347826, 0.4995383194829178, 0.3133333333333334, 0.393888888888889, 0.9313075506445673, 0.08333333333333333, 0.7699646154357334, 0.7915702717575567, 0.0, 1.0, 1.0, 0.13846995638767837], 
reward next is 0.8615, 
noisyNet noise sample is [array([0.35583195], dtype=float32), 0.32819283]. 
=============================================
[2019-04-08 15:36:34,959] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.5786412e-17 2.9594940e-09 1.9500008e-07 6.8186779e-07 1.5345476e-10
 1.2273969e-12 4.5937604e-08 2.2984598e-13 3.5033535e-18 7.7828765e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:36:34,960] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5380
[2019-04-08 15:36:35,012] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.333333333333333, 22.66666666666667, 70.66666666666667, 575.3333333333334, 22.5, 28.21324069412597, 1.054488662629592, 1.0, 1.0, 65.0, 27312.71704613601], 
current ob forecast is [], 
actual action is [3.666666666666667, 65.0], 
sim time this is 4033200.0000, 
sim time next is 4033800.0000, 
raw observation next is [-1.5, 23.0, 67.0, 548.0, 22.5, 27.72965039425886, 1.000323723601628, 1.0, 1.0, 64.99999999999993, 21646.14783137791], 
processed observation next is [1.0, 0.6956521739130435, 0.4210526315789474, 0.23, 0.22333333333333333, 0.605524861878453, 0.375, 0.8108041995215718, 0.8334412412005427, 1.0, 1.0, 0.9999999999999986, 0.10307689443513292], 
reward next is 0.8969, 
noisyNet noise sample is [array([0.36851332], dtype=float32), 0.41312578]. 
=============================================
[2019-04-08 15:36:35,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1683808e-16 1.9609794e-09 6.3598335e-08 1.3112105e-05 2.9707956e-10
 2.4982013e-12 2.7300034e-08 8.9918639e-14 3.8092706e-17 1.1124959e-18
 9.9998677e-01], sum to 1.0000
[2019-04-08 15:36:35,087] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2079
[2019-04-08 15:36:35,111] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 19.0, 26.5955903636774, 0.6656538302730335, 0.0, 1.0, 65.0, 46536.5524756173], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4065600.0000, 
sim time next is 4066200.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 19.0, 26.63674652581324, 0.6678245354883753, 0.0, 1.0, 65.0, 45337.94300630625], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7197288771511033, 0.7226081784961251, 0.0, 1.0, 1.0, 0.21589496669669642], 
reward next is 0.7841, 
noisyNet noise sample is [array([-1.4354892], dtype=float32), 0.9758474]. 
=============================================
[2019-04-08 15:36:35,330] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.7014460e-18 3.6227493e-10 4.4247321e-07 2.8947647e-05 2.5024363e-10
 1.0843125e-13 2.1913424e-08 2.0969408e-15 3.2353469e-19 8.7766408e-22
 9.9997056e-01], sum to 1.0000
[2019-04-08 15:36:35,330] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9091
[2019-04-08 15:36:35,358] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 29.66666666666667, 116.6666666666667, 831.8333333333334, 22.5, 28.05938732325266, 0.90614345501545, 1.0, 1.0, 65.0, 27111.04186719917], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4108800.0000, 
sim time next is 4109400.0000, 
raw observation next is [3.0, 30.0, 116.0, 830.0, 22.5, 27.55287781225057, 1.055579613080512, 1.0, 1.0, 65.0, 41371.3115811127], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3, 0.38666666666666666, 0.9171270718232044, 0.375, 0.7960731510208809, 0.8518598710268374, 1.0, 1.0, 1.0, 0.19700624562434618], 
reward next is 0.8030, 
noisyNet noise sample is [array([1.1522355], dtype=float32), -0.12523991]. 
=============================================
[2019-04-08 15:36:35,673] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4012631e-17 1.0123520e-09 2.9151259e-08 5.1454640e-07 1.9828364e-10
 5.0098418e-14 9.9220244e-08 3.4910751e-14 5.0281900e-19 1.2431456e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:36:35,688] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9377
[2019-04-08 15:36:35,713] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 22.5, 27.18390377136499, 0.8572247614154126, 1.0, 1.0, 65.0, 35868.96401945602], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 4045200.0000, 
sim time next is 4045800.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 22.5, 27.16557634744427, 0.8511176097289749, 0.0, 1.0, 65.0, 35655.3973430246], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.375, 0.7637980289536891, 0.7837058699096583, 0.0, 1.0, 1.0, 0.16978760639535523], 
reward next is 0.8302, 
noisyNet noise sample is [array([0.11087058], dtype=float32), -0.27489766]. 
=============================================
[2019-04-08 15:36:36,976] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.3786314e-17 3.0000835e-10 4.4682173e-07 2.3207695e-06 1.1523704e-09
 3.5866152e-12 5.7213580e-08 6.6835812e-14 9.7774833e-18 3.8782192e-19
 9.9999714e-01], sum to 1.0000
[2019-04-08 15:36:36,976] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5209
[2019-04-08 15:36:37,001] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 55.0, 26.5, 19.0, 26.87767134316958, 0.6943368632124666, 0.0, 1.0, 65.0, 42269.65948613804], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4262400.0000, 
sim time next is 4263000.0000, 
raw observation next is [3.0, 49.66666666666667, 73.33333333333334, 35.33333333333334, 19.0, 26.88026670122616, 0.6989245947514421, 0.0, 1.0, 65.0, 42011.55784717937], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.4966666666666667, 0.24444444444444446, 0.03904235727440148, 0.08333333333333333, 0.7400222251021799, 0.7329748649171474, 0.0, 1.0, 1.0, 0.2000550373675208], 
reward next is 0.7999, 
noisyNet noise sample is [array([1.3747034], dtype=float32), 1.0652728]. 
=============================================
[2019-04-08 15:36:37,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[81.69973 ]
 [81.476555]
 [81.31398 ]
 [81.19167 ]
 [81.16859 ]], R is [[81.93069458]
 [81.91010284]
 [81.88967133]
 [81.86921692]
 [81.84771729]].
[2019-04-08 15:36:37,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5790272e-19 1.8556540e-10 2.0980343e-08 1.0417250e-08 2.1575977e-11
 4.2984903e-14 1.1670901e-09 4.9731879e-15 1.3542339e-19 1.3500023e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:36:37,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3713
[2019-04-08 15:36:37,938] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 38.33333333333334, 0.0, 0.0, 19.0, 27.58393414857728, 0.9790475005964071, 0.0, 1.0, 65.0, 27436.36848084951], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4135200.0000, 
sim time next is 4135800.0000, 
raw observation next is [1.0, 37.16666666666666, 0.0, 0.0, 19.0, 27.56042083750046, 0.9741206694597997, 0.0, 1.0, 65.0, 27815.18419643395], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.3716666666666666, 0.0, 0.0, 0.08333333333333333, 0.7967017364583716, 0.8247068898199332, 0.0, 1.0, 1.0, 0.13245325807825692], 
reward next is 0.8675, 
noisyNet noise sample is [array([0.83781654], dtype=float32), -1.0941937]. 
=============================================
[2019-04-08 15:36:38,000] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.06611105e-18 9.35334709e-12 5.08733571e-08 2.33340188e-07
 1.27694496e-11 1.23878514e-13 2.13933538e-09 1.03816667e-15
 1.63343107e-18 1.89283174e-20 9.99999762e-01], sum to 1.0000
[2019-04-08 15:36:38,001] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9030
[2019-04-08 15:36:38,015] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 40.66666666666667, 0.0, 0.0, 19.0, 27.63208618197081, 0.9906375178927606, 0.0, 1.0, 65.0, 26984.7836356968], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4134000.0000, 
sim time next is 4134600.0000, 
raw observation next is [1.0, 39.5, 0.0, 0.0, 19.0, 27.60978305874744, 0.9846562039911014, 0.0, 1.0, 65.0, 27149.17397088011], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.395, 0.0, 0.0, 0.08333333333333333, 0.8008152548956199, 0.8282187346637006, 0.0, 1.0, 1.0, 0.1292817808137148], 
reward next is 0.8707, 
noisyNet noise sample is [array([0.8036159], dtype=float32), -1.0719099]. 
=============================================
[2019-04-08 15:36:38,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6306575e-16 3.0581981e-10 1.4198167e-07 1.0789892e-06 4.9410953e-10
 1.5832009e-11 2.4887576e-08 1.1765591e-12 1.1604432e-16 1.0768588e-18
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:36:38,136] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7234
[2019-04-08 15:36:38,149] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 41.0, 0.0, 0.0, 19.0, 26.39646705731913, 0.5673027385163615, 0.0, 1.0, 65.0, 48597.14929450738], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 4086000.0000, 
sim time next is 4086600.0000, 
raw observation next is [-4.833333333333334, 39.83333333333334, 0.0, 0.0, 22.5, 26.39158841165318, 0.5601741437888552, 0.0, 1.0, 65.0, 48647.04348892014], 
processed observation next is [1.0, 0.30434782608695654, 0.32871652816251157, 0.39833333333333343, 0.0, 0.0, 0.375, 0.6992990343044317, 0.6867247145962851, 0.0, 1.0, 1.0, 0.23165258804247688], 
reward next is 0.7683, 
noisyNet noise sample is [array([0.15804632], dtype=float32), -0.36964878]. 
=============================================
[2019-04-08 15:36:38,492] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4302198e-18 1.5777551e-11 2.6120750e-07 1.0465507e-07 3.6719378e-12
 1.1180731e-14 1.1793591e-09 4.4583943e-15 1.2363312e-19 2.1426092e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:36:38,492] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0312
[2019-04-08 15:36:38,506] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 22.5, 28.65733712058324, 1.119627402322417, 1.0, 1.0, 65.0, 18849.30217957628], 
current ob forecast is [], 
actual action is [8.333333333333332, 65.0], 
sim time this is 4112400.0000, 
sim time next is 4113000.0000, 
raw observation next is [3.5, 33.0, 106.0, 794.0, 22.5, 28.68217945034682, 1.114024142710599, 1.0, 1.0, 65.0, 9424.673976161352], 
processed observation next is [1.0, 0.6086956521739131, 0.5595567867036012, 0.33, 0.35333333333333333, 0.8773480662983425, 0.375, 0.890181620862235, 0.871341380903533, 1.0, 1.0, 1.0, 0.044879399886482624], 
reward next is 0.9551, 
noisyNet noise sample is [array([0.07701405], dtype=float32), -0.35040018]. 
=============================================
[2019-04-08 15:36:38,516] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[88.70091 ]
 [88.45066 ]
 [88.44624 ]
 [88.307396]
 [88.10265 ]], R is [[88.9120636 ]
 [88.93318176]
 [89.04385376]
 [89.15341949]
 [89.1721344 ]].
[2019-04-08 15:36:38,880] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3654576e-17 1.7928915e-10 4.1279254e-08 7.8774804e-07 6.5303890e-10
 1.2554307e-12 5.8945471e-08 7.5355087e-14 3.4957063e-17 2.8864645e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:36:38,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4927
[2019-04-08 15:36:38,896] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 19.0, 26.89332603610362, 0.7634258490267478, 0.0, 1.0, 65.0, 42428.0309009734], 
current ob forecast is [], 
actual action is [2.833333333333333, 65.0], 
sim time this is 4155000.0000, 
sim time next is 4155600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 19.0, 26.87057502262332, 0.7578731149989241, 0.0, 1.0, 65.0, 42762.19521652999], 
processed observation next is [0.0, 0.08695652173913043, 0.3979686057248385, 0.47333333333333344, 0.0, 0.0, 0.08333333333333333, 0.73921458521861, 0.752624371666308, 0.0, 1.0, 1.0, 0.20362950103109517], 
reward next is 0.7964, 
noisyNet noise sample is [array([0.15216942], dtype=float32), 0.6552246]. 
=============================================
[2019-04-08 15:36:39,043] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3151048e-18 1.0599581e-11 4.4852015e-08 1.4930809e-07 2.0032916e-11
 2.2368983e-14 9.4771817e-09 1.3237192e-14 9.9647534e-20 9.8276763e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:36:39,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9664
[2019-04-08 15:36:39,064] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 39.33333333333334, 0.0, 0.0, 19.0, 27.48353534174599, 0.9491463194602286, 0.0, 1.0, 65.0, 28492.84820295107], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4139400.0000, 
sim time next is 4140000.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 19.0, 27.47068590477003, 0.9439163566241692, 0.0, 1.0, 65.0, 28781.02450937973], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7892238253975025, 0.8146387855413897, 0.0, 1.0, 1.0, 0.137052497663713], 
reward next is 0.8629, 
noisyNet noise sample is [array([0.42037588], dtype=float32), 0.6061619]. 
=============================================
[2019-04-08 15:36:39,074] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.33611]
 [85.43729]
 [84.86837]
 [85.26551]
 [84.60795]], R is [[85.8506012 ]
 [85.85641479]
 [85.86055756]
 [85.86576843]
 [85.87197113]].
[2019-04-08 15:36:39,144] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.6488210e-18 1.5844749e-10 1.0870927e-08 3.2978966e-07 3.3799311e-11
 1.1191398e-14 5.5629346e-10 2.3532198e-16 2.8959301e-19 2.4903564e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:36:39,144] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4577
[2019-04-08 15:36:39,155] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 35.0, 19.16666666666667, 47.5, 22.5, 28.53455752162354, 1.163142165640046, 1.0, 1.0, 65.00000000000004, 18847.83014804656], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4124400.0000, 
sim time next is 4125000.0000, 
raw observation next is [3.0, 34.5, 15.33333333333334, 38.00000000000001, 22.5, 28.53140803400944, 1.153839685261405, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.345, 0.05111111111111113, 0.0419889502762431, 0.375, 0.8776173361674534, 0.8846132284204683, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12352135], dtype=float32), 0.30414426]. 
=============================================
[2019-04-08 15:36:39,182] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[87.202644]
 [87.46821 ]
 [87.77059 ]
 [87.92704 ]
 [88.12415 ]], R is [[87.1061554 ]
 [87.14533997]
 [87.18413544]
 [87.26741791]
 [87.30500031]].
[2019-04-08 15:36:40,456] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.8413111e-17 9.7273101e-10 8.0308575e-07 3.0040349e-06 1.1672436e-09
 2.7594276e-13 1.2854807e-08 6.9233407e-15 6.0306016e-18 9.3018916e-21
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:36:40,461] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2118
[2019-04-08 15:36:40,488] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 40.0, 108.0, 660.0, 19.0, 26.74868329929261, 0.7400925535972221, 0.0, 1.0, 65.0, 38652.16139858445], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 4181400.0000, 
sim time next is 4182000.0000, 
raw observation next is [-2.666666666666667, 38.33333333333334, 109.0, 679.0, 19.0, 26.7934441184344, 0.7493836132071134, 0.0, 1.0, 65.0, 38220.88915465021], 
processed observation next is [0.0, 0.391304347826087, 0.38873499538319484, 0.3833333333333334, 0.36333333333333334, 0.7502762430939226, 0.08333333333333333, 0.7327870098695334, 0.7497945377357045, 0.0, 1.0, 1.0, 0.1820042340697629], 
reward next is 0.8180, 
noisyNet noise sample is [array([-1.1430118], dtype=float32), -0.26451957]. 
=============================================
[2019-04-08 15:36:40,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.474365]
 [82.33625 ]
 [82.09665 ]
 [81.83945 ]
 [81.57185 ]], R is [[82.61423492]
 [82.60403442]
 [82.58164978]
 [82.55780792]
 [82.5326767 ]].
[2019-04-08 15:36:40,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0320150e-18 3.0359546e-09 1.3826411e-07 1.4269594e-07 1.4067689e-10
 1.2293347e-13 2.5673843e-08 2.4880593e-14 2.3921542e-18 8.8388578e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:36:40,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8309
[2019-04-08 15:36:40,683] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 32.5, 119.0, 822.0, 19.0, 27.11462711950251, 0.8378629312875598, 0.0, 1.0, 65.0, 31352.50269156485], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4188600.0000, 
sim time next is 4189200.0000, 
raw observation next is [0.3333333333333333, 31.66666666666667, 118.8333333333333, 826.1666666666666, 19.0, 27.1108911837714, 0.8422033195643058, 0.0, 1.0, 65.0, 31330.08114558373], 
processed observation next is [0.0, 0.4782608695652174, 0.4718374884579871, 0.3166666666666667, 0.396111111111111, 0.9128913443830571, 0.08333333333333333, 0.75924093198095, 0.7807344398547685, 0.0, 1.0, 1.0, 0.14919086259801775], 
reward next is 0.8508, 
noisyNet noise sample is [array([0.4423398], dtype=float32), 1.9381243]. 
=============================================
[2019-04-08 15:36:40,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5147820e-16 6.6484693e-11 1.4533564e-06 2.0753171e-06 3.5957057e-10
 1.4610008e-12 1.5863673e-08 1.1353377e-12 8.5428534e-17 5.8004535e-20
 9.9999642e-01], sum to 1.0000
[2019-04-08 15:36:40,776] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0644
[2019-04-08 15:36:40,793] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 27.05824662030515, 0.7478654615060916, 0.0, 1.0, 65.0, 40732.5466802606], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 27.05292675407107, 0.7468547997287599, 0.0, 1.0, 65.0, 41007.53133720077], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7544105628392558, 0.7489515999095867, 0.0, 1.0, 1.0, 0.19527395874857512], 
reward next is 0.8047, 
noisyNet noise sample is [array([-0.1486216], dtype=float32), 1.2525288]. 
=============================================
[2019-04-08 15:36:40,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1629654e-15 5.0401744e-10 2.0339219e-07 8.8522933e-07 3.2462231e-09
 3.9148708e-13 3.3983437e-08 1.3967477e-13 4.1953505e-17 6.7522940e-19
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:36:40,960] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4603
[2019-04-08 15:36:40,976] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 54.0, 46.0, 244.0, 19.0, 26.4991365765464, 0.6624619086092163, 0.0, 1.0, 65.0, 47052.46816982835], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 4176000.0000, 
sim time next is 4176600.0000, 
raw observation next is [-4.833333333333334, 52.5, 61.33333333333334, 325.3333333333334, 19.0, 26.52724092116336, 0.6793404808231273, 0.0, 1.0, 65.0, 45948.91934058065], 
processed observation next is [0.0, 0.34782608695652173, 0.32871652816251157, 0.525, 0.20444444444444448, 0.35948434622467784, 0.08333333333333333, 0.7106034100969468, 0.7264468269410425, 0.0, 1.0, 1.0, 0.21880437781228881], 
reward next is 0.7812, 
noisyNet noise sample is [array([-0.4009136], dtype=float32), -0.94503146]. 
=============================================
[2019-04-08 15:36:41,298] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [8.3478432e-19 6.8092337e-11 1.3775963e-07 4.4419867e-08 3.1602342e-11
 6.7151864e-14 6.1820065e-09 2.8338758e-14 5.7349761e-19 5.7373962e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:36:41,300] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4057
[2019-04-08 15:36:41,312] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3936073e-16 8.7866397e-10 1.4734479e-06 4.8431834e-06 5.2350591e-09
 1.4632443e-11 7.5408298e-07 3.8289115e-13 1.9295957e-16 8.3635553e-21
 9.9999297e-01], sum to 1.0000
[2019-04-08 15:36:41,313] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4961
[2019-04-08 15:36:41,336] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 40.5, 0.0, 0.0, 19.0, 27.45315329084725, 0.8967164886288271, 0.0, 1.0, 65.0, 30620.82161374222], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 4140600.0000, 
sim time next is 4141200.0000, 
raw observation next is [0.6666666666666667, 41.0, 0.0, 0.0, 19.0, 27.41909038116371, 0.8950579812871705, 0.0, 1.0, 65.0, 35510.12187216206], 
processed observation next is [1.0, 0.9565217391304348, 0.4810710987996307, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7849241984303091, 0.7983526604290568, 0.0, 1.0, 1.0, 0.16909581843886695], 
reward next is 0.8309, 
noisyNet noise sample is [array([1.1228287], dtype=float32), 0.7824452]. 
=============================================
[2019-04-08 15:36:41,337] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 53.16666666666666, 30.66666666666666, 162.6666666666666, 19.0, 26.51883998174348, 0.6522134427581686, 0.0, 1.0, 65.0, 47644.46576867417], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 4175400.0000, 
sim time next is 4176000.0000, 
raw observation next is [-5.0, 54.0, 46.0, 244.0, 19.0, 26.49913536540009, 0.6624615273098059, 0.0, 1.0, 65.0, 47052.4819257636], 
processed observation next is [0.0, 0.34782608695652173, 0.32409972299168976, 0.54, 0.15333333333333332, 0.2696132596685083, 0.08333333333333333, 0.7082612804500075, 0.7208205091032687, 0.0, 1.0, 1.0, 0.22405943774173143], 
reward next is 0.7759, 
noisyNet noise sample is [array([-0.7279958], dtype=float32), 1.2933766]. 
=============================================
[2019-04-08 15:36:41,350] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.74937 ]
 [78.54512 ]
 [78.443375]
 [78.5063  ]
 [78.52469 ]], R is [[79.05471039]
 [79.03729248]
 [79.01741791]
 [79.00485229]
 [78.99207306]].
[2019-04-08 15:36:41,378] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1783868e-18 6.7035953e-11 3.8563567e-07 7.5404063e-07 7.0466057e-11
 1.8264234e-13 9.0199954e-09 2.8246546e-15 6.3118893e-18 9.0528032e-22
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:36:41,379] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8734
[2019-04-08 15:36:41,395] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.833333333333333, 47.83333333333333, 0.0, 0.0, 19.0, 27.06760392577586, 0.7547917761531506, 0.0, 1.0, 65.0, 40350.68287564981], 
current ob forecast is [], 
actual action is [6.833333333333333, 65.0], 
sim time this is 4233000.0000, 
sim time next is 4233600.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 27.06542794526194, 0.7527859277172374, 0.0, 1.0, 65.0, 40504.39197114921], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7554523287718284, 0.7509286425724125, 0.0, 1.0, 1.0, 0.19287805700547245], 
reward next is 0.8071, 
noisyNet noise sample is [array([-1.6135328], dtype=float32), -0.47596622]. 
=============================================
[2019-04-08 15:36:41,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.89417520e-17 1.20821930e-09 3.09730268e-08 1.44444164e-06
 1.60063296e-10 8.23257261e-13 5.15639398e-09 1.66181240e-14
 2.02902358e-18 1.16234555e-20 9.99998569e-01], sum to 1.0000
[2019-04-08 15:36:41,846] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2006
[2019-04-08 15:36:41,857] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.5, 32.0, 118.0, 847.0, 19.0, 27.24909633510612, 0.8777766609686979, 0.0, 1.0, 65.0, 28532.47401485857], 
current ob forecast is [], 
actual action is [6.5, 65.0], 
sim time this is 4192200.0000, 
sim time next is 4192800.0000, 
raw observation next is [1.666666666666667, 32.66666666666666, 134.0, 817.3333333333334, 19.0, 27.27159688958884, 0.8842671035417006, 0.0, 1.0, 65.0, 28353.34846736097], 
processed observation next is [0.0, 0.5217391304347826, 0.5087719298245615, 0.32666666666666655, 0.44666666666666666, 0.9031307550644567, 0.08333333333333333, 0.7726330741324032, 0.7947557011805668, 0.0, 1.0, 1.0, 0.13501594508267128], 
reward next is 0.8650, 
noisyNet noise sample is [array([0.46817362], dtype=float32), -0.5785038]. 
=============================================
[2019-04-08 15:36:42,008] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.7542682e-17 1.3150329e-10 5.4279167e-07 1.2288216e-06 4.9065765e-11
 1.5190140e-13 8.7573264e-09 7.8747518e-15 9.4325938e-19 8.5941683e-21
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:36:42,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6182
[2019-04-08 15:36:42,041] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 38.0, 209.5, 572.3333333333334, 19.0, 27.38487711854743, 0.9233194955368345, 0.0, 1.0, 65.0, 27667.86927910426], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4196400.0000, 
sim time next is 4197000.0000, 
raw observation next is [2.0, 39.0, 205.0, 475.6666666666667, 19.0, 27.42201029158323, 0.9190444178580793, 0.0, 1.0, 65.0, 27199.19513947859], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.39, 0.6833333333333333, 0.525598526703499, 0.08333333333333333, 0.7851675242986026, 0.8063481392860264, 0.0, 1.0, 1.0, 0.12951997685465996], 
reward next is 0.8705, 
noisyNet noise sample is [array([0.9764038], dtype=float32), 0.8651182]. 
=============================================
[2019-04-08 15:36:42,054] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.911446]
 [85.772606]
 [85.56016 ]
 [85.3019  ]
 [85.0885  ]], R is [[85.91732025]
 [85.92639923]
 [85.93452454]
 [85.94269562]
 [85.95024109]].
[2019-04-08 15:36:42,804] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7440251e-18 1.4334647e-09 1.8186897e-07 6.6931989e-07 1.7415649e-11
 1.8900342e-13 2.8157848e-08 2.0585459e-14 2.6962794e-17 5.1000261e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:36:42,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0066
[2019-04-08 15:36:42,829] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 45.0, 0.0, 0.0, 19.0, 27.16136533580572, 0.7959634872230019, 0.0, 1.0, 65.0, 38279.647739387], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4224600.0000, 
sim time next is 4225200.0000, 
raw observation next is [1.0, 45.66666666666667, 0.0, 0.0, 19.0, 27.13630679347796, 0.7951602656932825, 0.0, 1.0, 65.0, 38505.76043424759], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.4566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7613588994564967, 0.7650534218977608, 0.0, 1.0, 1.0, 0.18336076397260756], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.82837695], dtype=float32), -0.26358467]. 
=============================================
[2019-04-08 15:36:43,016] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8710459e-18 3.5882283e-11 2.5119668e-08 2.8352476e-08 3.0783596e-12
 5.9534470e-14 2.0546340e-09 4.2596715e-15 3.4016597e-20 5.0161186e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:36:43,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3886
[2019-04-08 15:36:43,030] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 19.0, 27.06239715528236, 0.7504704285558708, 0.0, 1.0, 65.0, 40614.47273703957], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 19.0, 27.05824750259929, 0.7478657234612366, 0.0, 1.0, 65.0, 40732.53810270513], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.08333333333333333, 0.7548539585499409, 0.7492885744870789, 0.0, 1.0, 1.0, 0.19396446715573873], 
reward next is 0.8060, 
noisyNet noise sample is [array([-0.9933252], dtype=float32), -1.820286]. 
=============================================
[2019-04-08 15:36:43,472] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.6879065e-18 6.5790129e-10 9.4161045e-08 2.3292129e-07 2.0656650e-11
 4.7746390e-14 6.8654438e-10 2.3291224e-14 2.7982392e-17 9.4416697e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:36:43,472] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5506
[2019-04-08 15:36:43,503] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 37.0, 114.0, 544.0, 19.0, 27.58786398711723, 0.9705539722930223, 0.0, 1.0, 65.0, 27686.93440690033], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4204800.0000, 
sim time next is 4205400.0000, 
raw observation next is [2.833333333333333, 37.5, 98.66666666666666, 546.0, 19.0, 27.63459633678947, 0.9792794318437469, 0.0, 1.0, 65.0, 26494.0023672777], 
processed observation next is [0.0, 0.6956521739130435, 0.541089566020314, 0.375, 0.32888888888888884, 0.6033149171270719, 0.08333333333333333, 0.8028830280657893, 0.826426477281249, 0.0, 1.0, 1.0, 0.1261619160346557], 
reward next is 0.8738, 
noisyNet noise sample is [array([0.11669033], dtype=float32), 0.49117762]. 
=============================================
[2019-04-08 15:36:43,992] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3404645e-19 1.2640123e-11 1.3341329e-07 3.2632997e-08 1.4740136e-11
 3.7294090e-15 2.4787539e-09 1.1616321e-15 4.0673435e-19 3.6323351e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:36:43,992] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2022
[2019-04-08 15:36:44,013] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.966666666666667, 57.5, 215.3333333333333, 495.6666666666666, 19.0, 27.82573408185579, 1.012785806067032, 0.0, 1.0, 65.0, 23878.86000374331], 
current ob forecast is [], 
actual action is [11.966666666666667, 65.0], 
sim time this is 4284600.0000, 
sim time next is 4285200.0000, 
raw observation next is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 19.0, 27.84776844886161, 1.0160628708801, 0.0, 1.0, 65.0, 24048.38103118598], 
processed observation next is [0.0, 0.6086956521739131, 0.6546629732225301, 0.5800000000000001, 0.7405555555555557, 0.4865561694290977, 0.08333333333333333, 0.8206473707384676, 0.8386876236267001, 0.0, 1.0, 1.0, 0.11451610014850468], 
reward next is 0.8855, 
noisyNet noise sample is [array([-0.23507707], dtype=float32), 0.31579593]. 
=============================================
[2019-04-08 15:36:44,019] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.73140663e-17 2.04800454e-10 2.05727364e-08 1.96640983e-07
 6.41232178e-10 1.97036669e-13 1.33092835e-08 5.57325445e-15
 1.88370015e-18 1.28820753e-19 9.99999762e-01], sum to 1.0000
[2019-04-08 15:36:44,023] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1765
[2019-04-08 15:36:44,056] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.98301067694396, 0.7140262675481583, 0.0, 1.0, 65.0, 42015.16969794829], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4246800.0000, 
sim time next is 4247400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.98197464670077, 0.7222406583117249, 0.0, 1.0, 65.0, 41942.62983283793], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7484978872250642, 0.7407468861039083, 0.0, 1.0, 1.0, 0.19972680872779966], 
reward next is 0.8003, 
noisyNet noise sample is [array([1.1987607], dtype=float32), 0.17604108]. 
=============================================
[2019-04-08 15:36:44,365] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.8034062e-17 8.6026715e-09 2.7626291e-07 1.6108235e-05 1.4769937e-09
 1.7273486e-11 1.3601566e-07 2.6445008e-13 1.2654658e-18 4.5715837e-19
 9.9998343e-01], sum to 1.0000
[2019-04-08 15:36:44,366] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8669
[2019-04-08 15:36:44,392] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 40.0, 200.5, 379.0, 19.0, 27.42443006375865, 0.9242654071946276, 0.0, 1.0, 65.0, 26939.32096949113], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4197600.0000, 
sim time next is 4198200.0000, 
raw observation next is [2.0, 40.66666666666667, 196.0, 282.3333333333333, 19.0, 27.48158174146574, 0.9165481655835693, 0.0, 1.0, 65.0, 27350.90445567449], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.40666666666666673, 0.6533333333333333, 0.31197053406998154, 0.08333333333333333, 0.7901318117888115, 0.8055160551945231, 0.0, 1.0, 1.0, 0.1302424021698785], 
reward next is 0.8698, 
noisyNet noise sample is [array([-0.58056015], dtype=float32), 0.43059647]. 
=============================================
[2019-04-08 15:36:44,866] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0491990e-17 1.3363221e-09 3.2218804e-07 2.2969496e-06 9.9829610e-11
 5.9809314e-13 8.9211767e-08 1.1427900e-13 3.0710288e-18 7.4937297e-21
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:36:44,869] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2254
[2019-04-08 15:36:44,887] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.5, 54.5, 204.0, 604.0, 19.0, 27.24104086880844, 0.8411765563541639, 0.0, 1.0, 64.99999999999997, 33490.12102294578], 
current ob forecast is [], 
actual action is [9.5, 65.0], 
sim time this is 4271400.0000, 
sim time next is 4272000.0000, 
raw observation next is [4.666666666666666, 54.66666666666667, 190.1666666666667, 640.3333333333334, 19.0, 27.27288927489339, 0.852146241434999, 0.0, 1.0, 65.00000000000003, 32459.47189980199], 
processed observation next is [0.0, 0.43478260869565216, 0.5918744228993538, 0.5466666666666667, 0.6338888888888891, 0.7075506445672192, 0.08333333333333333, 0.7727407729077825, 0.7840487471449996, 0.0, 1.0, 1.0000000000000007, 0.1545689138085809], 
reward next is 0.8454, 
noisyNet noise sample is [array([0.08296382], dtype=float32), 0.22852987]. 
=============================================
[2019-04-08 15:36:44,895] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.938   ]
 [88.52846 ]
 [88.09276 ]
 [87.685036]
 [87.27539 ]], R is [[89.21331787]
 [89.16170502]
 [89.10706329]
 [89.05296326]
 [88.99667358]].
[2019-04-08 15:36:45,173] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0083670e-16 1.0696320e-10 1.6698360e-08 2.8768543e-08 1.7453572e-11
 3.0523170e-13 2.9601642e-08 6.0251601e-15 4.2059768e-18 1.6252803e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:36:45,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8215
[2019-04-08 15:36:45,190] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 27.00362040389227, 0.7094926770276458, 0.0, 1.0, 65.0, 42246.13795646657], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4249200.0000, 
sim time next is 4249800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.97694791112268, 0.7068664660566316, 0.0, 1.0, 65.0, 42523.17430562043], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7480789925935566, 0.7356221553522105, 0.0, 1.0, 1.0, 0.20249130621724012], 
reward next is 0.7975, 
noisyNet noise sample is [array([-0.62566805], dtype=float32), 0.28983328]. 
=============================================
[2019-04-08 15:36:46,288] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.5991072e-20 2.0450249e-11 9.5362068e-08 2.0666189e-07 5.2263966e-12
 1.0274346e-15 2.0575002e-09 5.3441685e-15 5.2007813e-19 1.7666886e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:36:46,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2046
[2019-04-08 15:36:46,337] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.3, 71.0, 0.0, 0.0, 19.0, 27.34471400087165, 0.8178043414064344, 0.0, 1.0, 65.0, 34762.6285453123], 
current ob forecast is [], 
actual action is [8.3, 65.0], 
sim time this is 4341600.0000, 
sim time next is 4342200.0000, 
raw observation next is [3.233333333333333, 71.66666666666667, 0.0, 0.0, 19.0, 27.41200716303164, 0.8086078081648739, 0.0, 1.0, 65.0, 33758.36735211238], 
processed observation next is [1.0, 0.2608695652173913, 0.5521698984302863, 0.7166666666666667, 0.0, 0.0, 0.08333333333333333, 0.7843339302526365, 0.769535936054958, 0.0, 1.0, 1.0, 0.1607541302481542], 
reward next is 0.8392, 
noisyNet noise sample is [array([-0.7364236], dtype=float32), -1.5934066]. 
=============================================
[2019-04-08 15:36:47,096] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8544249e-19 1.4627603e-11 6.0741286e-08 1.3952115e-07 2.1411822e-11
 3.8026164e-15 5.1317044e-09 2.2523988e-16 1.2277072e-19 1.4976079e-23
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:36:47,098] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6132
[2019-04-08 15:36:47,137] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.55, 71.16666666666667, 61.33333333333334, 327.3333333333334, 22.5, 27.22386871215596, 0.8297789062580336, 1.0, 1.0, 65.0, 36480.90047864019], 
current ob forecast is [], 
actual action is [8.55, 65.0], 
sim time this is 4349400.0000, 
sim time next is 4350000.0000, 
raw observation next is [4.1, 68.33333333333334, 76.66666666666667, 409.1666666666667, 22.5, 27.25654593156062, 0.8773981546146555, 1.0, 1.0, 64.99999999999997, 33331.56990849096], 
processed observation next is [1.0, 0.34782608695652173, 0.5761772853185596, 0.6833333333333335, 0.2555555555555556, 0.4521178637200737, 0.375, 0.7713788276300516, 0.7924660515382186, 1.0, 1.0, 0.9999999999999994, 0.15872176146900455], 
reward next is 0.8413, 
noisyNet noise sample is [array([-0.56855756], dtype=float32), 2.072561]. 
=============================================
[2019-04-08 15:36:47,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[94.71462]
 [94.14533]
 [93.23522]
 [92.37231]
 [91.79824]], R is [[95.57686615]
 [95.44738007]
 [95.31471252]
 [95.17809296]
 [95.04139709]].
[2019-04-08 15:36:47,504] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [7.5056262e-20 4.7719324e-12 8.8981558e-08 3.3037051e-09 2.9235850e-12
 2.7237723e-15 4.8512715e-11 9.5955778e-16 2.5518605e-19 4.8573364e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:36:47,516] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5439
[2019-04-08 15:36:47,527] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 53.66666666666667, 176.6666666666667, 738.6666666666666, 19.0, 27.71457555658059, 0.9828524194651799, 0.0, 1.0, 65.0, 24979.23300932011], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 4281600.0000, 
sim time next is 4282200.0000, 
raw observation next is [7.0, 54.5, 188.0, 717.0, 19.0, 27.73031327200325, 0.9903308121769486, 0.0, 1.0, 65.0, 24700.29138408227], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.545, 0.6266666666666667, 0.7922651933701658, 0.08333333333333333, 0.8108594393336043, 0.8301102707256495, 0.0, 1.0, 1.0, 0.11762043516229652], 
reward next is 0.8824, 
noisyNet noise sample is [array([-0.6822085], dtype=float32), 0.7706232]. 
=============================================
[2019-04-08 15:36:48,743] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6212070e-18 1.1334531e-09 1.7824350e-06 1.4716174e-05 9.4150215e-11
 4.9312459e-14 1.0383559e-07 3.4333054e-14 1.9026773e-18 6.6645877e-21
 9.9998343e-01], sum to 1.0000
[2019-04-08 15:36:48,746] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0851
[2019-04-08 15:36:48,763] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.65, 69.16666666666666, 0.0, 0.0, 19.0, 27.33136425601259, 0.8195150981798869, 0.0, 1.0, 65.0, 37673.67413970011], 
current ob forecast is [], 
actual action is [8.65, 65.0], 
sim time this is 4337400.0000, 
sim time next is 4338000.0000, 
raw observation next is [3.6, 69.0, 0.0, 0.0, 19.0, 27.35225323603322, 0.8150425009190109, 0.0, 1.0, 65.0, 37422.68418592933], 
processed observation next is [1.0, 0.21739130434782608, 0.5623268698060943, 0.69, 0.0, 0.0, 0.08333333333333333, 0.7793544363361017, 0.7716808336396702, 0.0, 1.0, 1.0, 0.1782032580282349], 
reward next is 0.8218, 
noisyNet noise sample is [array([0.58547044], dtype=float32), -1.2082094]. 
=============================================
[2019-04-08 15:36:48,780] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[90.429054]
 [90.47936 ]
 [90.51745 ]
 [90.59086 ]
 [90.608284]], R is [[90.30948639]
 [90.22699738]
 [90.15093994]
 [90.07645416]
 [90.01817322]].
[2019-04-08 15:36:49,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1710713e-18 3.0990564e-11 4.4932246e-08 1.9173568e-07 4.7568807e-13
 2.4320758e-13 2.8668723e-09 6.6840158e-15 1.3893765e-19 6.3949849e-23
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:36:49,590] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2568
[2019-04-08 15:36:49,617] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.93333333333333, 28.16666666666667, 118.6666666666667, 844.6666666666667, 22.5, 29.44299623707835, 1.339588552661823, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [19.93333333333333, 65.0], 
sim time this is 4363800.0000, 
sim time next is 4364400.0000, 
raw observation next is [14.86666666666667, 28.33333333333334, 118.3333333333333, 848.8333333333334, 22.5, 29.48306449811445, 1.341859349839927, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8744228993536475, 0.2833333333333334, 0.3944444444444443, 0.9379373848987109, 0.375, 0.9569220415095376, 0.9472864499466424, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52874583], dtype=float32), -1.8714374]. 
=============================================
[2019-04-08 15:36:50,632] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4347661e-19 1.9614102e-12 4.4669641e-09 3.8958824e-08 2.9603674e-12
 7.1369277e-15 2.6557363e-09 2.3277042e-16 1.0897729e-20 3.9791681e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:36:50,634] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9393
[2019-04-08 15:36:50,658] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.9, 75.0, 0.0, 0.0, 19.0, 27.22617948618658, 0.7980475838965324, 0.0, 1.0, 65.0, 37806.59575430833], 
current ob forecast is [], 
actual action is [7.9, 65.0], 
sim time this is 4345200.0000, 
sim time next is 4345800.0000, 
raw observation next is [2.916666666666667, 74.83333333333333, 0.0, 0.0, 22.5, 27.28479438107575, 0.7996594648712486, 0.0, 1.0, 65.0, 36705.6187125674], 
processed observation next is [1.0, 0.30434782608695654, 0.543397968605725, 0.7483333333333333, 0.0, 0.0, 0.375, 0.7737328650896457, 0.7665531549570829, 0.0, 1.0, 1.0, 0.17478866053603523], 
reward next is 0.8252, 
noisyNet noise sample is [array([0.96093553], dtype=float32), -1.6646966]. 
=============================================
[2019-04-08 15:36:50,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.86307491e-18 1.62989303e-10 4.57271568e-07 1.68505892e-06
 2.05328376e-11 4.81925189e-14 1.04441265e-08 1.92417425e-15
 2.39316173e-19 2.44388048e-22 9.99997854e-01], sum to 1.0000
[2019-04-08 15:36:50,879] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6848
[2019-04-08 15:36:50,898] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.3, 34.0, 92.33333333333334, 0.0, 22.5, 29.68331779245627, 1.447532712832033, 1.0, 1.0, 64.99999999999997, 0.0], 
current ob forecast is [], 
actual action is [18.3, 65.0], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [13.15, 34.5, 81.66666666666667, 0.0, 22.5, 29.83092250045367, 1.293773464168331, 1.0, 1.0, 65.0, 9424.252341372228], 
processed observation next is [1.0, 0.6521739130434783, 0.826869806094183, 0.345, 0.27222222222222225, 0.0, 0.375, 0.9859102083711392, 0.9312578213894437, 1.0, 1.0, 1.0, 0.044877392101772516], 
reward next is 0.9551, 
noisyNet noise sample is [array([-0.13488317], dtype=float32), 2.336653]. 
=============================================
[2019-04-08 15:36:50,908] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[93.60063 ]
 [93.95549 ]
 [94.400276]
 [94.76182 ]
 [95.0377  ]], R is [[93.27976227]
 [93.34696198]
 [93.34938049]
 [93.37100983]
 [93.43730164]].
[2019-04-08 15:36:51,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.02427634e-18 4.46071262e-12 9.83601911e-09 8.62304603e-07
 6.46703090e-12 1.34514626e-14 1.04143414e-08 1.64508240e-15
 9.16637805e-20 1.06299247e-22 9.99999166e-01], sum to 1.0000
[2019-04-08 15:36:51,122] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0783
[2019-04-08 15:36:51,150] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.0, 37.0, 34.0, 0.0, 22.5, 29.89423609276141, 1.382117965703475, 1.0, 1.0, 65.0, 16133.42043321368], 
current ob forecast is [], 
actual action is [18.0, 65.0], 
sim time this is 4380000.0000, 
sim time next is 4380600.0000, 
raw observation next is [13.0, 37.5, 29.0, 0.0, 22.5, 29.38505111180216, 1.427899586354671, 1.0, 1.0, 65.0, 30390.63501780733], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.375, 0.09666666666666666, 0.0, 0.375, 0.9487542593168466, 0.9759665287848903, 1.0, 1.0, 1.0, 0.14471730960860632], 
reward next is 0.8553, 
noisyNet noise sample is [array([0.96649134], dtype=float32), 0.40381438]. 
=============================================
[2019-04-08 15:36:51,222] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1084021e-19 3.9993366e-11 2.7150741e-06 1.9125888e-07 3.2719428e-11
 2.1127001e-13 9.6046626e-10 2.0385189e-16 5.5464414e-19 5.6246505e-22
 9.9999714e-01], sum to 1.0000
[2019-04-08 15:36:51,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1816
[2019-04-08 15:36:51,233] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 142.0, 0.0, 22.5, 28.50788653707532, 1.162633781213229, 1.0, 1.0, 65.0, 21144.73788382925], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4447800.0000, 
sim time next is 4448400.0000, 
raw observation next is [1.0, 86.0, 135.3333333333333, 0.0, 22.5, 28.50328410925906, 1.164954446293706, 1.0, 1.0, 65.0, 21574.98475720774], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.45111111111111096, 0.0, 0.375, 0.8752736757715883, 0.8883181487645686, 1.0, 1.0, 1.0, 0.1027380226533702], 
reward next is 0.8973, 
noisyNet noise sample is [array([0.8504478], dtype=float32), -0.9131835]. 
=============================================
[2019-04-08 15:36:51,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9937045e-19 1.7844933e-09 7.0245829e-07 1.8040286e-06 1.1460706e-10
 1.0816357e-13 1.6241653e-08 3.1925527e-16 1.7577527e-20 8.9688522e-22
 9.9999750e-01], sum to 1.0000
[2019-04-08 15:36:51,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0599
[2019-04-08 15:36:51,515] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.75, 32.5, 133.6666666666667, 209.6666666666666, 22.5, 29.57913062958174, 1.471736957812742, 1.0, 1.0, 64.99999999999999, 0.0], 
current ob forecast is [], 
actual action is [18.75, 65.0], 
sim time this is 4374600.0000, 
sim time next is 4375200.0000, 
raw observation next is [13.6, 33.0, 118.3333333333333, 104.8333333333333, 22.5, 29.83204594020462, 1.311583542520586, 1.0, 1.0, 65.00000000000001, 9424.433032290004], 
processed observation next is [1.0, 0.6521739130434783, 0.8393351800554018, 0.33, 0.3944444444444443, 0.11583793738489867, 0.375, 0.9860038283503849, 0.9371945141735286, 1.0, 1.0, 1.0000000000000002, 0.04487825253471431], 
reward next is 0.9551, 
noisyNet noise sample is [array([-0.07256168], dtype=float32), -0.59187424]. 
=============================================
[2019-04-08 15:36:51,844] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.1654152e-18 2.7389882e-10 4.6460819e-08 1.3544515e-06 1.5578528e-11
 1.2723206e-13 1.7698349e-09 2.4642816e-14 2.3383962e-18 2.0669667e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:36:51,849] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6894
[2019-04-08 15:36:51,866] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 19.0, 28.50021101851928, 1.27861346271659, 0.0, 1.0, 65.0, 18849.33364654223], 
current ob forecast is [], 
actual action is [15.33333333333333, 65.0], 
sim time this is 4395000.0000, 
sim time next is 4395600.0000, 
raw observation next is [10.2, 59.0, 0.0, 0.0, 19.0, 28.46785061341436, 1.276032464935795, 0.0, 1.0, 65.0, 18848.41055885048], 
processed observation next is [1.0, 0.9130434782608695, 0.7451523545706372, 0.59, 0.0, 0.0, 0.08333333333333333, 0.8723208844511966, 0.9253441549785983, 0.0, 1.0, 1.0, 0.08975433599452609], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.77929246], dtype=float32), -0.22363704]. 
=============================================
[2019-04-08 15:36:51,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8716810e-18 4.0106987e-11 6.9213961e-08 1.2605587e-07 1.9440184e-10
 7.9099374e-13 7.3852206e-08 1.0939947e-15 6.2487375e-20 4.9723749e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:36:51,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7404
[2019-04-08 15:36:51,981] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.449999999999999, 65.5, 0.0, 0.0, 19.0, 27.92439005178629, 1.108655222811196, 0.0, 1.0, 65.0, 27967.23429493819], 
current ob forecast is [], 
actual action is [11.45, 65.0], 
sim time this is 4411800.0000, 
sim time next is 4412400.0000, 
raw observation next is [6.333333333333334, 65.66666666666667, 0.0, 0.0, 19.0, 27.88715560067131, 1.109833140407303, 0.0, 1.0, 65.0, 28458.33335487038], 
processed observation next is [1.0, 0.043478260869565216, 0.6380424746075716, 0.6566666666666667, 0.0, 0.0, 0.08333333333333333, 0.8239296333892758, 0.8699443801357676, 0.0, 1.0, 1.0, 0.13551587311843039], 
reward next is 0.8645, 
noisyNet noise sample is [array([1.1388189], dtype=float32), 0.28941482]. 
=============================================
[2019-04-08 15:36:52,538] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.0682031e-19 1.1612153e-09 1.4224599e-06 1.8183531e-06 3.8492293e-10
 8.3130811e-12 1.1732038e-07 2.0337044e-14 2.8665551e-18 4.3418525e-21
 9.9999666e-01], sum to 1.0000
[2019-04-08 15:36:52,538] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8673
[2019-04-08 15:36:52,551] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.06666666666667, 49.0, 0.0, 0.0, 22.5, 29.18135089031127, 1.367105807613001, 1.0, 1.0, 65.0, 3769.857715205311], 
current ob forecast is [], 
actual action is [17.06666666666667, 65.0], 
sim time this is 4387800.0000, 
sim time next is 4388400.0000, 
raw observation next is [12.0, 50.0, 0.0, 0.0, 22.5, 29.11649898549391, 1.360188407219273, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7950138504155125, 0.5, 0.0, 0.0, 0.375, 0.9263749154578257, 0.9533961357397577, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4342248], dtype=float32), 0.24662627]. 
=============================================
[2019-04-08 15:36:52,775] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5994154e-16 4.2311682e-10 8.9650877e-07 9.1587817e-06 8.9626295e-10
 3.6329748e-11 1.5152816e-07 4.5547160e-13 4.9102086e-17 4.8764333e-20
 9.9998975e-01], sum to 1.0000
[2019-04-08 15:36:52,777] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5482
[2019-04-08 15:36:52,793] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 22.5, 27.49412072568739, 0.9704492387850515, 0.0, 1.0, 65.0, 36617.09336977825], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4432200.0000, 
sim time next is 4432800.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 22.5, 27.4430357727605, 0.9773577732067912, 1.0, 1.0, 65.0, 37255.94806076249], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.375, 0.7869196477300416, 0.8257859244022637, 1.0, 1.0, 1.0, 0.17740927647982135], 
reward next is 0.8226, 
noisyNet noise sample is [array([0.15413229], dtype=float32), -0.94657886]. 
=============================================
[2019-04-08 15:36:53,022] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9811408e-19 1.4746353e-11 9.3123864e-09 9.7363539e-09 4.8425541e-11
 7.6575255e-14 7.2884621e-10 2.5523125e-16 8.4499117e-21 1.1357670e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:36:53,022] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8477
[2019-04-08 15:36:53,035] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.65, 82.0, 120.0, 232.0, 22.5, 27.78585405180766, 1.057939760376623, 1.0, 1.0, 65.0, 24824.29804776175], 
current ob forecast is [], 
actual action is [6.65, 65.0], 
sim time this is 4437000.0000, 
sim time next is 4437600.0000, 
raw observation next is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 22.5, 28.00930589507354, 1.069978055309042, 1.0, 1.0, 65.0, 23031.35594658221], 
processed observation next is [1.0, 0.34782608695652173, 0.505078485687904, 0.8266666666666667, 0.425, 0.21933701657458562, 0.375, 0.8341088245894616, 0.8566593517696807, 1.0, 1.0, 1.0, 0.10967312355515338], 
reward next is 0.8903, 
noisyNet noise sample is [array([-0.5488946], dtype=float32), 1.5415741]. 
=============================================
[2019-04-08 15:36:53,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0248899e-18 7.4414608e-10 4.7913037e-08 2.9642436e-08 1.4983429e-11
 3.4995569e-14 5.8869837e-10 5.7016354e-15 2.9862965e-19 1.9698468e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:36:53,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6730
[2019-04-08 15:36:53,632] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 122.0, 0.0, 22.5, 27.7215582566654, 1.154943961055331, 1.0, 1.0, 65.0, 56754.18052475973], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4449600.0000, 
sim time next is 4450200.0000, 
raw observation next is [0.8333333333333334, 87.0, 115.3333333333333, 0.0, 22.5, 27.15684301757869, 1.054921856630762, 1.0, 1.0, 64.99999999999999, 35904.74262537386], 
processed observation next is [1.0, 0.5217391304347826, 0.4856879039704525, 0.87, 0.3844444444444443, 0.0, 0.375, 0.7630702514648909, 0.8516406188769207, 1.0, 1.0, 0.9999999999999997, 0.17097496488273267], 
reward next is 0.8290, 
noisyNet noise sample is [array([-0.23346436], dtype=float32), 0.20621164]. 
=============================================
[2019-04-08 15:36:54,171] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.7920688e-17 6.2759742e-11 6.0440335e-08 3.4066673e-07 1.3433938e-09
 2.0441619e-12 7.9493397e-08 2.9750933e-13 4.8349600e-17 7.8907699e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:36:54,192] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3752
[2019-04-08 15:36:54,212] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.866666666666667, 76.0, 0.0, 0.0, 19.0, 26.71380959584576, 0.7306887627545251, 0.0, 1.0, 65.0, 42213.18488846267], 
current ob forecast is [], 
actual action is [2.133333333333333, 65.0], 
sim time this is 4603200.0000, 
sim time next is 4603800.0000, 
raw observation next is [-2.933333333333333, 76.5, 0.0, 0.0, 19.0, 26.71289213064077, 0.7316870888397199, 0.0, 1.0, 65.0, 41372.67350583417], 
processed observation next is [1.0, 0.2608695652173913, 0.38134810710988, 0.765, 0.0, 0.0, 0.08333333333333333, 0.7260743442200642, 0.7438956962799067, 0.0, 1.0, 1.0, 0.19701273098016273], 
reward next is 0.8030, 
noisyNet noise sample is [array([0.90849185], dtype=float32), 2.1014926]. 
=============================================
[2019-04-08 15:36:54,711] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4079373e-19 2.1690681e-11 9.4781811e-08 1.0244957e-07 1.9388942e-13
 1.7513501e-14 4.7771270e-10 2.6563139e-16 4.8490468e-20 2.3684946e-23
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:36:54,711] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1239
[2019-04-08 15:36:54,760] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 160.1666666666667, 24.33333333333333, 22.5, 28.53607590794186, 1.165332341214277, 1.0, 1.0, 65.0, 20089.02602410976], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4447200.0000, 
sim time next is 4447800.0000, 
raw observation next is [1.0, 86.0, 142.0, 0.0, 22.5, 28.50788648582365, 1.162633774495684, 1.0, 1.0, 65.0, 21144.73845118141], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.47333333333333333, 0.0, 0.375, 0.8756572071519709, 0.8875445914985614, 1.0, 1.0, 1.0, 0.10068923071991147], 
reward next is 0.8993, 
noisyNet noise sample is [array([1.5794598], dtype=float32), 0.14490953]. 
=============================================
[2019-04-08 15:36:55,033] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4964279e-17 9.7042541e-10 1.1451392e-06 4.7291287e-06 8.1522789e-10
 3.6144980e-13 5.3809586e-08 6.6108778e-13 2.5722536e-18 9.5154192e-20
 9.9999404e-01], sum to 1.0000
[2019-04-08 15:36:55,042] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2666724e-16 1.3678767e-08 6.3262860e-07 1.0075209e-04 2.2685805e-10
 3.1730404e-12 7.2081832e-07 6.9968711e-13 6.5792072e-18 1.3944953e-21
 9.9989784e-01], sum to 1.0000
[2019-04-08 15:36:55,042] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2455
[2019-04-08 15:36:55,044] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8189
[2019-04-08 15:36:55,061] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.8, 61.66666666666666, 0.0, 0.0, 19.0, 28.11063921224384, 1.180334811370128, 0.0, 1.0, 65.0, 21740.15972155206], 
current ob forecast is [], 
actual action is [13.8, 65.0], 
sim time this is 4401600.0000, 
sim time next is 4402200.0000, 
raw observation next is [8.65, 61.83333333333334, 0.0, 0.0, 19.0, 28.06613037640296, 1.172548833199986, 0.0, 1.0, 65.0, 24885.97626208762], 
processed observation next is [1.0, 0.9565217391304348, 0.7022160664819946, 0.6183333333333334, 0.0, 0.0, 0.08333333333333333, 0.83884419803358, 0.890849611066662, 0.0, 1.0, 1.0, 0.11850464886708391], 
reward next is 0.8815, 
noisyNet noise sample is [array([-0.5742894], dtype=float32), -1.2750919]. 
=============================================
[2019-04-08 15:36:55,113] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.07539748866433, 0.856962496339123, 0.0, 1.0, 65.0, 42519.35568174584], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 4496400.0000, 
sim time next is 4497000.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.10472469750431, 0.8542601275873466, 0.0, 1.0, 65.0, 41524.51175945337], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7587270581253591, 0.7847533758624489, 0.0, 1.0, 1.0, 0.1977357702831113], 
reward next is 0.8023, 
noisyNet noise sample is [array([0.23900242], dtype=float32), -1.9076713]. 
=============================================
[2019-04-08 15:36:55,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.332886]
 [83.43413 ]
 [83.491325]
 [83.88372 ]
 [84.02524 ]], R is [[83.11500549]
 [83.08138275]
 [83.04315186]
 [83.00556183]
 [82.95619202]].
[2019-04-08 15:36:55,305] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.2517319e-17 5.4988702e-10 7.5638475e-08 1.4571043e-08 5.7672578e-12
 8.1491118e-13 7.2923303e-09 2.8593058e-15 6.7263574e-20 2.2220032e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:36:55,305] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7940
[2019-04-08 15:36:55,327] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 80.0, 60.0, 116.0, 22.5, 27.5559078965052, 0.9885583275938439, 1.0, 1.0, 65.0, 34532.78076217314], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4435200.0000, 
sim time next is 4435800.0000, 
raw observation next is [1.883333333333333, 80.66666666666667, 80.00000000000001, 154.6666666666667, 22.5, 27.53694682223879, 1.008962693330388, 1.0, 1.0, 65.0, 35128.09975255189], 
processed observation next is [1.0, 0.34782608695652173, 0.5147737765466298, 0.8066666666666668, 0.2666666666666667, 0.17090239410681404, 0.375, 0.7947455685198991, 0.8363208977767961, 1.0, 1.0, 1.0, 0.16727666548834233], 
reward next is 0.8327, 
noisyNet noise sample is [array([0.5452274], dtype=float32), 0.6134924]. 
=============================================
[2019-04-08 15:36:56,126] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5611249e-17 3.0432265e-10 6.6899517e-08 1.1123007e-06 1.2907750e-10
 9.9371260e-13 4.0271644e-09 1.0327823e-14 1.2314449e-18 1.7223429e-21
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:36:56,127] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3019
[2019-04-08 15:36:56,141] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.75, 49.16666666666667, 181.6666666666667, 670.3333333333333, 22.5, 28.90466035428932, 1.253961966125379, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [9.75, 65.0], 
sim time this is 4630200.0000, 
sim time next is 4630800.0000, 
raw observation next is [4.800000000000001, 49.33333333333334, 192.3333333333333, 634.6666666666666, 22.5, 28.97643056655155, 1.263033876171598, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5955678670360112, 0.4933333333333334, 0.641111111111111, 0.7012891344383057, 0.375, 0.9147025472126291, 0.9210112920571993, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.78532785], dtype=float32), -1.5382096]. 
=============================================
[2019-04-08 15:36:56,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7833381e-16 1.3568729e-09 1.2543742e-06 2.3499720e-06 7.9993727e-11
 2.7373744e-13 6.9450302e-08 7.9539265e-15 7.5134552e-18 4.2943805e-20
 9.9999630e-01], sum to 1.0000
[2019-04-08 15:36:56,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4255
[2019-04-08 15:36:56,351] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 78.0, 41.0, 18.33333333333333, 22.5, 28.27851390240793, 0.9538813727049394, 1.0, 1.0, 65.0, 42805.29020310154], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4467000.0000, 
sim time next is 4467600.0000, 
raw observation next is [0.0, 78.0, 37.0, 27.5, 22.5, 27.53467163930854, 1.086361425183476, 1.0, 1.0, 65.0, 61552.2237239538], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.78, 0.12333333333333334, 0.03038674033149171, 0.375, 0.7945559699423784, 0.8621204750611587, 1.0, 1.0, 1.0, 0.29310582725692286], 
reward next is 0.7069, 
noisyNet noise sample is [array([1.1966214], dtype=float32), 0.915915]. 
=============================================
[2019-04-08 15:36:56,891] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.9901883e-18 1.8341842e-11 1.6750006e-08 1.2049398e-08 8.2721902e-12
 1.1823419e-12 3.8416175e-08 2.2801083e-14 2.3366860e-19 1.7151858e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:36:56,891] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.1705
[2019-04-08 15:36:56,922] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 22.5, 27.49412012461243, 0.9704490442079462, 0.0, 1.0, 65.0, 36617.10059716232], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4432200.0000, 
sim time next is 4432800.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 22.5, 27.44303516423502, 0.9773575809455649, 1.0, 1.0, 65.0, 37255.95526419027], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.375, 0.786919597019585, 0.8257858603151883, 1.0, 1.0, 1.0, 0.17740931078185843], 
reward next is 0.8226, 
noisyNet noise sample is [array([-0.6205162], dtype=float32), -1.0394936]. 
=============================================
[2019-04-08 15:36:57,352] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.5884373e-16 3.1000114e-09 2.2707201e-05 2.3106595e-05 9.2893304e-11
 3.0857307e-12 2.2077188e-07 6.6838570e-13 5.6971039e-18 4.2271233e-20
 9.9995399e-01], sum to 1.0000
[2019-04-08 15:36:57,360] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8570
[2019-04-08 15:36:57,402] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 78.0, 56.33333333333333, 0.0, 22.5, 28.26776868251589, 1.095884488290982, 1.0, 1.0, 65.0, 27020.41669329135], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4464600.0000, 
sim time next is 4465200.0000, 
raw observation next is [0.0, 78.0, 52.66666666666666, 0.0, 22.5, 28.2263063098348, 1.094341083273465, 1.0, 1.0, 65.0, 28043.76823246742], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.17555555555555552, 0.0, 0.375, 0.8521921924862333, 0.864780361091155, 1.0, 1.0, 1.0, 0.1335417534879401], 
reward next is 0.8665, 
noisyNet noise sample is [array([1.6092733], dtype=float32), 0.82992405]. 
=============================================
[2019-04-08 15:36:57,660] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.7111399e-18 3.5172556e-11 2.9555104e-06 7.8684997e-07 1.0441031e-10
 1.2363164e-13 4.9282399e-08 3.7174230e-14 1.4786462e-18 3.9290814e-20
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:36:57,661] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0570635e-16 6.3681796e-11 4.3348354e-07 4.0658639e-07 9.4500407e-10
 4.6015448e-13 7.0485271e-08 5.6969810e-14 2.1563618e-18 1.2131439e-19
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:36:57,662] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6326
[2019-04-08 15:36:57,663] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8338
[2019-04-08 15:36:57,687] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 222.3333333333333, 107.6666666666667, 22.5, 28.36804932174582, 1.150328941611739, 1.0, 1.0, 65.0, 21830.45960899891], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4443000.0000, 
sim time next is 4443600.0000, 
raw observation next is [1.0, 86.0, 236.6666666666667, 126.8333333333333, 22.5, 28.41887587434555, 1.162303902602018, 1.0, 1.0, 65.0, 20115.49068286098], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.7888888888888891, 0.14014732965009205, 0.375, 0.8682396561954624, 0.8874346342006727, 1.0, 1.0, 1.0, 0.09578805087076657], 
reward next is 0.9042, 
noisyNet noise sample is [array([-0.45727682], dtype=float32), -0.084628716]. 
=============================================
[2019-04-08 15:36:57,696] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 73.0, 0.0, 0.0, 19.0, 26.90790241977805, 0.8075820150963283, 0.0, 1.0, 65.0, 45409.20220304651], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 4501800.0000, 
sim time next is 4502400.0000, 
raw observation next is [-0.8666666666666667, 73.0, 0.0, 0.0, 19.0, 26.85537957726801, 0.8203063546373452, 0.0, 1.0, 65.0, 46945.22824670035], 
processed observation next is [1.0, 0.08695652173913043, 0.4385964912280702, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7379482981056675, 0.7734354515457817, 0.0, 1.0, 1.0, 0.22354870593666834], 
reward next is 0.7765, 
noisyNet noise sample is [array([0.074301], dtype=float32), 0.18257745]. 
=============================================
[2019-04-08 15:36:58,239] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2880680e-16 4.2635626e-10 9.4160299e-07 2.5011152e-05 1.0349910e-10
 1.8617191e-12 3.9044700e-08 5.4645101e-13 6.0248172e-16 1.0840813e-19
 9.9997401e-01], sum to 1.0000
[2019-04-08 15:36:58,239] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1654
[2019-04-08 15:36:58,276] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.166666666666667, 51.5, 207.0, 32.0, 22.5, 27.69515605246495, 0.9706536799873011, 1.0, 1.0, 65.0, 26265.46284802258], 
current ob forecast is [], 
actual action is [7.166666666666667, 65.0], 
sim time this is 4540200.0000, 
sim time next is 4540800.0000, 
raw observation next is [2.333333333333333, 51.0, 227.0, 40.0, 22.5, 27.80506538098938, 0.9898051779768547, 1.0, 1.0, 65.0, 25380.76412155216], 
processed observation next is [1.0, 0.5652173913043478, 0.5272391505078486, 0.51, 0.7566666666666667, 0.04419889502762431, 0.375, 0.8170887817491149, 0.8299350593256182, 1.0, 1.0, 1.0, 0.12086078153120076], 
reward next is 0.8791, 
noisyNet noise sample is [array([-0.21986163], dtype=float32), 0.10144088]. 
=============================================
[2019-04-08 15:36:59,275] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5244302e-16 8.2098941e-09 1.0310949e-06 9.2395373e-07 3.3870466e-09
 3.3874162e-12 1.1451873e-08 2.5579191e-14 9.3970050e-18 1.7247947e-20
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:36:59,276] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8349
[2019-04-08 15:36:59,296] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 55.0, 41.33333333333333, 22.5, 28.14675671232853, 1.03009766538503, 1.0, 1.0, 65.0, 18844.40055529047], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4554600.0000, 
sim time next is 4555200.0000, 
raw observation next is [2.0, 52.0, 41.5, 34.66666666666666, 22.5, 28.26734489021597, 1.028613444169679, 1.0, 1.0, 65.0, 18844.08924600573], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.13833333333333334, 0.03830570902394106, 0.375, 0.8556120741846641, 0.8428711480565596, 1.0, 1.0, 1.0, 0.089733758314313], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.56262153], dtype=float32), 1.0790892]. 
=============================================
[2019-04-08 15:37:00,107] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6781666e-17 2.8698954e-10 4.9450136e-07 6.3016884e-08 1.3012169e-10
 1.3681250e-12 6.2201124e-09 2.0931345e-14 5.7048657e-18 1.4494028e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:37:00,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8923
[2019-04-08 15:37:00,139] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.6666666666666666, 64.66666666666667, 139.3333333333333, 2.999999999999999, 22.5, 27.62293022016532, 0.9033397471539294, 1.0, 1.0, 65.0, 30241.666183177], 
current ob forecast is [], 
actual action is [5.666666666666667, 65.0], 
sim time this is 4527600.0000, 
sim time next is 4528200.0000, 
raw observation next is [0.8333333333333334, 62.83333333333333, 155.6666666666667, 5.999999999999998, 22.5, 27.67646397022238, 0.916806152139551, 1.0, 1.0, 65.0, 28932.69846223029], 
processed observation next is [1.0, 0.391304347826087, 0.4856879039704525, 0.6283333333333333, 0.5188888888888891, 0.006629834254143645, 0.375, 0.8063719975185316, 0.8056020507131837, 1.0, 1.0, 1.0, 0.13777475458204902], 
reward next is 0.8622, 
noisyNet noise sample is [array([-0.16178332], dtype=float32), -0.19608574]. 
=============================================
[2019-04-08 15:37:01,301] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.01584852e-15 1.01733555e-08 1.00502466e-05 7.44923909e-06
 3.78587356e-10 1.55736882e-10 1.47440153e-06 1.13244375e-12
 3.48817194e-16 4.73210176e-18 9.99981046e-01], sum to 1.0000
[2019-04-08 15:37:01,302] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3429
[2019-04-08 15:37:01,319] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 62.66666666666666, 0.0, 0.0, 19.0, 27.11347708279279, 0.8381671873536702, 0.0, 1.0, 65.0, 34955.2791925151], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 4582200.0000, 
sim time next is 4582800.0000, 
raw observation next is [0.4, 63.0, 0.0, 0.0, 19.0, 27.05969007223303, 0.8358352999928087, 0.0, 1.0, 65.0, 36470.9861112446], 
processed observation next is [1.0, 0.043478260869565216, 0.4736842105263158, 0.63, 0.0, 0.0, 0.08333333333333333, 0.7549741726860857, 0.7786117666642696, 0.0, 1.0, 1.0, 0.1736713624344981], 
reward next is 0.8263, 
noisyNet noise sample is [array([0.09811304], dtype=float32), 0.90167105]. 
=============================================
[2019-04-08 15:37:01,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1735958e-14 7.1619422e-09 2.4285052e-06 6.6584595e-05 1.0458397e-08
 3.6829161e-11 1.3887436e-07 2.0744960e-12 1.9062551e-16 2.7140902e-18
 9.9993086e-01], sum to 1.0000
[2019-04-08 15:37:01,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2769
[2019-04-08 15:37:01,419] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 65.66666666666667, 0.0, 0.0, 19.0, 27.05931818980588, 0.8256047003564643, 0.0, 1.0, 65.0, 35799.51342594572], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 4587600.0000, 
sim time next is 4588200.0000, 
raw observation next is [-0.65, 66.0, 0.0, 0.0, 19.0, 27.11164605386827, 0.8262827374802572, 0.0, 1.0, 65.0, 34814.78156877061], 
processed observation next is [1.0, 0.08695652173913043, 0.4445983379501386, 0.66, 0.0, 0.0, 0.08333333333333333, 0.7593038378223559, 0.7754275791600858, 0.0, 1.0, 1.0, 0.1657846741370029], 
reward next is 0.8342, 
noisyNet noise sample is [array([-0.14651184], dtype=float32), -0.013170503]. 
=============================================
[2019-04-08 15:37:02,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4301000e-16 7.0669592e-10 5.9718957e-07 7.1597236e-07 3.0508804e-10
 1.4533949e-11 3.2525769e-08 4.0370382e-13 1.7453296e-17 2.5719588e-20
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:37:02,242] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5428
[2019-04-08 15:37:02,281] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 72.5, 0.0, 0.0, 19.0, 26.79901583867674, 0.750796356295727, 0.0, 1.0, 65.0, 41310.8670799758], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 4599000.0000, 
sim time next is 4599600.0000, 
raw observation next is [-2.4, 73.0, 0.0, 0.0, 19.0, 26.77977964570518, 0.7550957277758146, 0.0, 1.0, 65.0, 40886.31657353225], 
processed observation next is [1.0, 0.21739130434782608, 0.39612188365650974, 0.73, 0.0, 0.0, 0.08333333333333333, 0.731648303808765, 0.7516985759252716, 0.0, 1.0, 1.0, 0.1946967455882488], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.6976732], dtype=float32), -0.48345995]. 
=============================================
[2019-04-08 15:37:02,674] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5534009e-17 6.0677472e-09 1.2445521e-06 5.6085548e-07 1.1861975e-09
 1.7708235e-13 5.8663634e-09 6.9747543e-15 1.9859639e-18 4.1743805e-20
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:37:02,676] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8191
[2019-04-08 15:37:02,735] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 49.33333333333333, 116.3333333333333, 58.66666666666667, 22.5, 28.37072506092561, 1.06909581231692, 1.0, 1.0, 65.0, 18844.78002150909], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4551600.0000, 
sim time next is 4552200.0000, 
raw observation next is [2.0, 50.0, 109.0, 68.0, 22.5, 28.32957062336853, 0.937052665184741, 1.0, 1.0, 65.0, 33878.23372568983], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.5, 0.36333333333333334, 0.07513812154696133, 0.375, 0.8607975519473774, 0.8123508883949137, 1.0, 1.0, 1.0, 0.16132492250328492], 
reward next is 0.8387, 
noisyNet noise sample is [array([0.5943045], dtype=float32), 1.8268204]. 
=============================================
[2019-04-08 15:37:03,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9910289e-17 1.8241317e-10 3.3780492e-07 2.8563259e-06 1.5620312e-10
 4.3137225e-13 1.0001396e-08 7.4570666e-14 8.7922036e-18 8.7353568e-21
 9.9999678e-01], sum to 1.0000
[2019-04-08 15:37:03,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8324
[2019-04-08 15:37:03,058] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 19.0, 27.09287060575558, 0.8505894868013463, 0.0, 1.0, 65.0, 34921.57773854477], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4579200.0000, 
sim time next is 4579800.0000, 
raw observation next is [0.9, 61.33333333333334, 0.0, 0.0, 19.0, 27.07059788995277, 0.8433994139063952, 0.0, 1.0, 65.0, 35677.4843384722], 
processed observation next is [1.0, 0.0, 0.48753462603878117, 0.6133333333333334, 0.0, 0.0, 0.08333333333333333, 0.755883157496064, 0.7811331379687984, 0.0, 1.0, 1.0, 0.16989278256415333], 
reward next is 0.8301, 
noisyNet noise sample is [array([-1.1636498], dtype=float32), -1.1392311]. 
=============================================
[2019-04-08 15:37:03,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1343632e-15 9.6685193e-09 4.6992392e-07 4.3331797e-06 6.0887206e-10
 9.8333199e-12 2.6779325e-08 1.3999737e-11 1.1987519e-15 1.1467471e-19
 9.9999511e-01], sum to 1.0000
[2019-04-08 15:37:03,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2887
[2019-04-08 15:37:03,721] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 66.33333333333333, 0.0, 0.0, 19.0, 27.12287959714081, 0.8061769596609523, 0.0, 1.0, 65.0, 34925.74320338817], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 4588800.0000, 
sim time next is 4589400.0000, 
raw observation next is [-0.9500000000000001, 66.66666666666667, 0.0, 0.0, 19.0, 27.01384808072936, 0.8084190130722654, 0.0, 1.0, 65.0, 38630.33708152006], 
processed observation next is [1.0, 0.08695652173913043, 0.43628808864265933, 0.6666666666666667, 0.0, 0.0, 0.08333333333333333, 0.7511540067274467, 0.7694730043574217, 0.0, 1.0, 1.0, 0.18395398610247649], 
reward next is 0.8160, 
noisyNet noise sample is [array([-3.3048327], dtype=float32), -0.2642152]. 
=============================================
[2019-04-08 15:37:03,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6980581e-16 4.9757792e-11 6.5492588e-07 1.1480963e-06 7.9392287e-10
 1.2064765e-11 6.1874012e-08 1.3461285e-13 1.4744938e-16 1.7176324e-19
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:37:03,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4072
[2019-04-08 15:37:03,916] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.366666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 27.03970603859841, 0.79883939761944, 0.0, 1.0, 65.0, 35910.21446908896], 
current ob forecast is [], 
actual action is [3.633333333333333, 65.0], 
sim time this is 4592400.0000, 
sim time next is 4593000.0000, 
raw observation next is [-1.433333333333333, 68.66666666666666, 0.0, 0.0, 19.0, 27.04637253137335, 0.7949545456390541, 0.0, 1.0, 65.0, 36619.01453298979], 
processed observation next is [1.0, 0.13043478260869565, 0.4228993536472761, 0.6866666666666665, 0.0, 0.0, 0.08333333333333333, 0.7538643776144459, 0.7649848485463514, 0.0, 1.0, 1.0, 0.17437625968090378], 
reward next is 0.8256, 
noisyNet noise sample is [array([-0.6411567], dtype=float32), -1.1137991]. 
=============================================
[2019-04-08 15:37:03,970] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.89942 ]
 [78.887   ]
 [78.89956 ]
 [78.868706]
 [78.79789 ]], R is [[78.9392395 ]
 [78.97885132]
 [79.02210999]
 [79.06159973]
 [79.09954071]].
[2019-04-08 15:37:04,193] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.6657882e-17 6.3155866e-08 6.5316846e-07 1.8787605e-05 3.6609915e-09
 1.8742833e-12 9.2336805e-08 4.3472479e-15 6.1929991e-18 6.0244992e-20
 9.9998045e-01], sum to 1.0000
[2019-04-08 15:37:04,198] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5956
[2019-04-08 15:37:04,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6548123e-18 5.7280453e-10 6.0486172e-09 9.2848256e-07 5.4965664e-11
 3.5578536e-13 6.2992349e-08 4.5896738e-14 4.0965231e-18 1.1954742e-21
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:37:04,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9492
[2019-04-08 15:37:04,239] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.0, 49.0, 129.5, 836.0, 22.5, 28.45902063665157, 1.166437959368826, 1.0, 1.0, 65.0, 18849.03460191924], 
current ob forecast is [], 
actual action is [9.0, 65.0], 
sim time this is 4626000.0000, 
sim time next is 4626600.0000, 
raw observation next is [4.116666666666667, 49.0, 132.6666666666667, 828.3333333333334, 22.5, 28.55992890066603, 1.178016237951479, 1.0, 1.0, 65.0, 18849.26832221431], 
processed observation next is [1.0, 0.5652173913043478, 0.5766389658356418, 0.49, 0.4422222222222224, 0.9152854511970534, 0.375, 0.8799940750555025, 0.8926720793171596, 1.0, 1.0, 1.0, 0.0897584205819729], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.1756009], dtype=float32), 3.089252]. 
=============================================
[2019-04-08 15:37:04,276] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.3333333333333333, 68.33333333333334, 121.0, 11.0, 22.5, 27.6559630923486, 0.8899073455829881, 1.0, 1.0, 65.0, 28808.99329632711], 
current ob forecast is [], 
actual action is [5.333333333333333, 65.0], 
sim time this is 4526400.0000, 
sim time next is 4527000.0000, 
raw observation next is [0.5, 66.5, 123.0, 0.0, 22.5, 27.64778325295649, 0.8910741931448752, 1.0, 1.0, 65.0, 29769.83618774602], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.665, 0.41, 0.0, 0.375, 0.8039819377463742, 0.7970247310482917, 1.0, 1.0, 1.0, 0.14176112470355248], 
reward next is 0.8582, 
noisyNet noise sample is [array([-0.3865824], dtype=float32), 0.76236916]. 
=============================================
[2019-04-08 15:37:04,298] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.00098 ]
 [86.54027 ]
 [86.131775]
 [86.467026]
 [86.20994 ]], R is [[86.10617065]
 [86.10792542]
 [86.11647797]
 [86.12020111]
 [86.11081696]].
[2019-04-08 15:37:05,652] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.8999158e-17 2.7228117e-10 4.3810974e-07 2.5846722e-07 2.2081482e-10
 1.2439787e-14 5.2623728e-09 1.5008962e-15 5.4639822e-20 8.7135464e-21
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:37:05,654] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4783
[2019-04-08 15:37:05,680] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.166666666666667, 52.33333333333334, 0.0, 0.0, 22.5, 28.48391173310942, 1.185335661922789, 1.0, 1.0, 65.0, 18849.0000717027], 
current ob forecast is [], 
actual action is [8.166666666666668, 65.0], 
sim time this is 4647000.0000, 
sim time next is 4647600.0000, 
raw observation next is [3.0, 53.0, 0.0, 0.0, 22.5, 28.36779012444846, 1.167357372407087, 1.0, 1.0, 65.0, 18849.15645844104], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.53, 0.0, 0.0, 0.375, 0.8639825103707052, 0.8891191241356956, 1.0, 1.0, 1.0, 0.0897578878973383], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.11048316], dtype=float32), 0.7585373]. 
=============================================
[2019-04-08 15:37:06,314] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.2004580e-15 5.7935488e-09 1.2731894e-06 4.9100167e-06 9.5073975e-09
 9.0031384e-13 4.5549285e-08 1.4414057e-12 4.7916344e-18 1.2211274e-20
 9.9999380e-01], sum to 1.0000
[2019-04-08 15:37:06,377] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8046
[2019-04-08 15:37:06,383] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4397557e-15 1.1194614e-09 6.4184951e-07 4.9932046e-06 6.2262262e-10
 7.5231829e-13 1.9964295e-08 5.0827635e-14 7.9647655e-19 5.2478414e-21
 9.9999440e-01], sum to 1.0000
[2019-04-08 15:37:06,384] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2705
[2019-04-08 15:37:06,429] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.3, 68.0, 0.0, 0.0, 19.0, 27.0680708847059, 0.8018125065332118, 0.0, 1.0, 65.0, 35059.95477360691], 
current ob forecast is [], 
actual action is [3.7, 65.0], 
sim time this is 4591800.0000, 
sim time next is 4592400.0000, 
raw observation next is [-1.366666666666667, 68.33333333333334, 0.0, 0.0, 19.0, 27.03970605929278, 0.7988394038688056, 0.0, 1.0, 65.0, 35910.21424075043], 
processed observation next is [1.0, 0.13043478260869565, 0.42474607571560485, 0.6833333333333335, 0.0, 0.0, 0.08333333333333333, 0.7533088382743983, 0.7662798012896018, 0.0, 1.0, 1.0, 0.17100102019404967], 
reward next is 0.8290, 
noisyNet noise sample is [array([0.10563283], dtype=float32), 0.4787191]. 
=============================================
[2019-04-08 15:37:06,432] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 19.0, 27.70363410728179, 1.064744791962795, 0.0, 1.0, 65.0, 26168.86276860943], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4657800.0000, 
sim time next is 4658400.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 19.0, 27.68986532010174, 1.060398939206701, 0.0, 1.0, 65.0, 26359.57488928053], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.08333333333333333, 0.8074887766751452, 0.8534663130689003, 0.0, 1.0, 1.0, 0.12552178518705015], 
reward next is 0.8745, 
noisyNet noise sample is [array([0.73582476], dtype=float32), 1.1104579]. 
=============================================
[2019-04-08 15:37:06,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0808710e-19 1.6917381e-10 8.0347570e-07 1.9241348e-07 3.7579054e-10
 1.9899508e-14 1.2830366e-09 1.0798470e-15 4.3873706e-19 6.8838366e-21
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:37:06,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2548
[2019-04-08 15:37:06,634] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3333333333333333, 61.83333333333333, 152.3333333333333, 595.0, 22.5, 27.839752197925, 0.9673102630191702, 1.0, 1.0, 65.0, 19728.54299312695], 
current ob forecast is [], 
actual action is [4.666666666666667, 65.0], 
sim time this is 4614600.0000, 
sim time next is 4615200.0000, 
raw observation next is [0.0, 60.0, 146.5, 638.0, 22.5, 27.90742354425668, 0.9844949932871486, 1.0, 1.0, 65.0, 19194.60055597809], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.48833333333333334, 0.7049723756906078, 0.375, 0.8256186286880567, 0.828164997762383, 1.0, 1.0, 1.0, 0.09140285979037185], 
reward next is 0.9086, 
noisyNet noise sample is [array([0.57507914], dtype=float32), -1.0615503]. 
=============================================
[2019-04-08 15:37:06,880] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.30790935e-16 2.21217942e-08 2.39387168e-06 5.48274954e-07
 1.02257847e-09 1.30461015e-11 8.99369823e-08 8.26600995e-13
 5.14416410e-17 1.36391937e-19 9.99996901e-01], sum to 1.0000
[2019-04-08 15:37:06,881] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3275
[2019-04-08 15:37:06,894] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.916666666666667, 70.66666666666667, 0.0, 0.0, 19.0, 26.88949985671292, 0.7702616856408708, 0.0, 1.0, 65.0, 38248.43742499947], 
current ob forecast is [], 
actual action is [3.083333333333333, 65.0], 
sim time this is 4596600.0000, 
sim time next is 4597200.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 19.0, 26.90689318311125, 0.7715008937164781, 0.0, 1.0, 65.0, 38657.27698716125], 
processed observation next is [1.0, 0.21739130434782608, 0.40720221606648205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7422410985926042, 0.7571669645721594, 0.0, 1.0, 1.0, 0.18408227136743452], 
reward next is 0.8159, 
noisyNet noise sample is [array([-1.3724141], dtype=float32), -0.41561732]. 
=============================================
[2019-04-08 15:37:06,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2743893e-18 1.6262826e-10 5.9525001e-07 8.6601761e-07 1.7811347e-10
 4.4857751e-13 8.2418126e-09 1.0895751e-14 1.9350817e-18 4.7649306e-20
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:37:06,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1498
[2019-04-08 15:37:07,043] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 19.0, 27.45397373115673, 0.9860097134007288, 0.0, 1.0, 65.0, 33210.98730253713], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4662000.0000, 
sim time next is 4662600.0000, 
raw observation next is [2.0, 56.16666666666666, 0.0, 0.0, 19.0, 27.43997151346753, 0.9863285972466578, 0.0, 1.0, 65.0, 33375.10573957788], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5616666666666665, 0.0, 0.0, 0.08333333333333333, 0.7866642927889608, 0.8287761990822192, 0.0, 1.0, 1.0, 0.15892907495037084], 
reward next is 0.8411, 
noisyNet noise sample is [array([0.5578498], dtype=float32), -0.42046064]. 
=============================================
[2019-04-08 15:37:08,064] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-08 15:37:08,066] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:37:08,066] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:37:08,066] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:37:08,067] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:37:08,068] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:37:08,068] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:37:08,078] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run14
[2019-04-08 15:37:08,106] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run14
[2019-04-08 15:37:08,124] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run14
[2019-04-08 15:38:31,120] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06137759]
[2019-04-08 15:38:31,121] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-11.64805914, 63.89609994, 0.0, 0.0, 19.0, 25.87384381635961, 0.5420260971680896, 0.0, 1.0, 65.0, 58855.79705685092]
[2019-04-08 15:38:31,121] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:38:31,122] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [1.4530061e-15 2.0648057e-09 7.1792078e-07 2.6165158e-06 1.5361522e-09
 7.9567316e-12 1.8480701e-07 5.0362720e-13 1.2472906e-16 9.8961976e-19
 9.9999642e-01], sampled 0.3529448331721229
[2019-04-08 15:38:54,768] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.1483 316228865.2349 2958.0533
[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,803] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:38:54,975] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,411] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.0306 355943583.6214 2370.6695
[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,431] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:01,546] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,525] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.5381 342816998.5334 2767.9602
[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:05,664] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:39:06,548] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 260000, evaluation results [260000.0, 6863.538102221841, 342816998.5334171, 2767.9601997047357, 6990.148260786054, 316228865.2349291, 2958.053298624441, 6801.030554183986, 355943583.62136, 2370.669476478192]
[2019-04-08 15:39:07,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8324770e-18 1.0790393e-09 5.9952776e-09 8.3297016e-07 8.3390080e-11
 5.7050449e-12 1.0171318e-10 8.8466391e-15 3.3385891e-19 4.6019915e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:39:07,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3764
[2019-04-08 15:39:07,207] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.0, 49.0, 0.0, 0.0, 22.5, 28.87029093573548, 1.243083169864175, 1.0, 1.0, 65.0, 9424.702463147498], 
current ob forecast is [], 
actual action is [9.0, 65.0], 
sim time this is 4644000.0000, 
sim time next is 4644600.0000, 
raw observation next is [3.833333333333333, 49.66666666666667, 0.0, 0.0, 22.5, 28.79451137339381, 1.229086448153182, 1.0, 1.0, 65.0, 18848.87335889026], 
processed observation next is [1.0, 0.782608695652174, 0.5687903970452447, 0.4966666666666667, 0.0, 0.0, 0.375, 0.8995426144494841, 0.9096954827177273, 1.0, 1.0, 1.0, 0.08975653980423934], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.0568519], dtype=float32), 0.17035182]. 
=============================================
[2019-04-08 15:39:07,736] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0146914e-16 7.6718312e-11 2.1014776e-08 2.1724807e-06 5.5603877e-10
 2.7798705e-13 1.6715029e-07 7.5015528e-14 3.2812524e-17 1.1664577e-20
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:39:07,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6433
[2019-04-08 15:39:07,754] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 19.0, 27.36639068885109, 0.9234387866379602, 0.0, 1.0, 65.0, 32632.60429751633], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4674000.0000, 
sim time next is 4674600.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 27.39792187357749, 0.9161221724883603, 0.0, 1.0, 65.0, 31383.90087147629], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7831601561314576, 0.80537405749612, 0.0, 1.0, 1.0, 0.14944714700702996], 
reward next is 0.8506, 
noisyNet noise sample is [array([-1.3291422], dtype=float32), 0.95028406]. 
=============================================
[2019-04-08 15:39:08,776] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0802548e-18 5.7386224e-10 4.9519701e-07 1.7448383e-06 2.6642522e-10
 2.0407844e-12 1.5152530e-07 3.2183444e-14 1.7659148e-17 3.7051588e-19
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:39:08,777] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6657
[2019-04-08 15:39:08,831] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 19.0, 27.36201395167865, 0.918108299482245, 0.0, 1.0, 65.0, 33340.9116912195], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4675800.0000, 
sim time next is 4676400.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 27.40464802978195, 0.9215240305505463, 0.0, 1.0, 65.0, 31680.43141675479], 
processed observation next is [1.0, 0.13043478260869565, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7837206691484958, 0.8071746768501821, 0.0, 1.0, 1.0, 0.15085919722264185], 
reward next is 0.8491, 
noisyNet noise sample is [array([0.63566166], dtype=float32), -0.9173432]. 
=============================================
[2019-04-08 15:39:08,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4046099e-17 3.0732980e-10 3.1497130e-08 5.2481101e-07 3.2770545e-10
 1.3109561e-12 4.6900610e-08 3.3759191e-13 1.5913007e-18 7.1752635e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:39:08,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2397
[2019-04-08 15:39:08,869] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 19.0, 27.35106881199324, 0.9500805374721945, 0.0, 1.0, 65.0, 33732.2667566915], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4668600.0000, 
sim time next is 4669200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 19.0, 27.41548535438161, 0.950523816104821, 0.0, 1.0, 65.0, 31972.82972407977], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.52, 0.0, 0.0, 0.08333333333333333, 0.7846237795318007, 0.8168412720349404, 0.0, 1.0, 1.0, 0.15225157011466556], 
reward next is 0.8477, 
noisyNet noise sample is [array([0.46420884], dtype=float32), 0.5002906]. 
=============================================
[2019-04-08 15:39:09,199] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6644920e-17 5.0246496e-10 1.9631145e-07 1.5692333e-06 7.8336004e-11
 1.6595613e-12 1.4809379e-07 2.4183138e-14 6.5935575e-19 1.6226197e-21
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:39:09,200] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3230
[2019-04-08 15:39:09,220] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.11014469259577, 0.8664072994003162, 0.0, 1.0, 65.0, 36520.73591868324], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4685400.0000, 
sim time next is 4686000.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.282997671582, 0.8634067037160644, 0.0, 1.0, 65.0, 30687.38090419052], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7735831392984999, 0.7878022345720215, 0.0, 1.0, 1.0, 0.14613038525805008], 
reward next is 0.8539, 
noisyNet noise sample is [array([-0.10843808], dtype=float32), -0.31968006]. 
=============================================
[2019-04-08 15:39:09,239] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.00781 ]
 [84.767235]
 [84.480446]
 [84.225426]
 [83.95983 ]], R is [[85.2828064 ]
 [85.25606537]
 [85.22257996]
 [85.19356537]
 [85.18489075]].
[2019-04-08 15:39:09,717] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6381840e-17 2.9187397e-09 3.9351808e-06 7.8764356e-07 1.6687354e-09
 6.6262176e-11 5.7014987e-08 1.0470994e-14 1.4707860e-18 7.6247134e-21
 9.9999523e-01], sum to 1.0000
[2019-04-08 15:39:09,717] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2359
[2019-04-08 15:39:09,733] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 53.0, 0.0, 0.0, 22.5, 28.3677900425569, 1.167357353533551, 1.0, 1.0, 65.0, 18849.15645286892], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4647600.0000, 
sim time next is 4648200.0000, 
raw observation next is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 22.5, 28.22684049810251, 1.160758354236084, 0.0, 1.0, 65.0, 18848.38371995998], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.5283333333333334, 0.0, 0.0, 0.375, 0.8522367081752092, 0.886919451412028, 0.0, 1.0, 1.0, 0.08975420819028562], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.4048008], dtype=float32), 0.29475987]. 
=============================================
[2019-04-08 15:39:09,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1848935e-18 2.5973032e-10 1.1304164e-07 3.3944605e-07 3.3114286e-11
 1.1912097e-13 2.9253135e-08 3.3249071e-16 1.0967841e-18 5.4359769e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:39:09,898] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4573
[2019-04-08 15:39:09,926] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.26651686264271, 0.8500691482144006, 0.0, 1.0, 65.0, 35248.69478150181], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4687800.0000, 
sim time next is 4688400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.20608408502743, 0.8362969641622205, 0.0, 1.0, 65.0, 35390.47559151602], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.767173673752286, 0.7787656547207402, 0.0, 1.0, 1.0, 0.16852607424531438], 
reward next is 0.8315, 
noisyNet noise sample is [array([-0.77077204], dtype=float32), -0.24409154]. 
=============================================
[2019-04-08 15:39:10,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1521490e-18 5.7601250e-11 1.5333681e-08 3.6525699e-07 1.5632093e-11
 6.5307033e-14 3.5425440e-10 9.0576791e-15 6.9563059e-20 2.6764549e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:39:10,425] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7152
[2019-04-08 15:39:10,445] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 107.8333333333333, 9.166666666666668, 22.5, 26.8769594602743, 0.9395421115436678, 1.0, 1.0, 64.99999999999999, 43125.69931428711], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4724400.0000, 
sim time next is 4725000.0000, 
raw observation next is [1.0, 72.0, 100.0, 11.0, 22.5, 27.6150224923142, 1.001554696807096, 1.0, 1.0, 65.0, 30308.2923041617], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.72, 0.3333333333333333, 0.012154696132596685, 0.375, 0.8012518743595166, 0.8338515656023654, 1.0, 1.0, 1.0, 0.14432520144838903], 
reward next is 0.8557, 
noisyNet noise sample is [array([-0.6270358], dtype=float32), 0.6163295]. 
=============================================
[2019-04-08 15:39:10,464] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.526764]
 [85.83901 ]
 [86.11216 ]
 [86.42824 ]
 [86.653694]], R is [[85.34735107]
 [85.28852081]
 [85.139534  ]
 [85.07548523]
 [85.08579254]].
[2019-04-08 15:39:10,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6339260e-15 2.7249405e-09 3.1526562e-07 4.5419793e-06 4.1860613e-09
 2.3836925e-11 2.5483687e-06 2.7329923e-13 2.4638791e-16 3.0843383e-19
 9.9999261e-01], sum to 1.0000
[2019-04-08 15:39:10,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7832
[2019-04-08 15:39:10,569] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.38383346313238, 0.6835397782826073, 0.0, 1.0, 65.0, 50212.00983954919], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4765200.0000, 
sim time next is 4765800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.36387351735778, 0.6798682952256697, 0.0, 1.0, 65.0, 50727.6511059311], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6969894597798151, 0.7266227650752232, 0.0, 1.0, 1.0, 0.24156024336157667], 
reward next is 0.7584, 
noisyNet noise sample is [array([-0.5371505], dtype=float32), 1.5140467]. 
=============================================
[2019-04-08 15:39:10,799] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7456810e-18 2.9596825e-11 1.0833651e-06 7.2662516e-08 7.7936164e-12
 3.6155494e-13 1.7873844e-09 1.4487846e-14 2.6946215e-18 1.2516134e-21
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:39:10,800] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3232
[2019-04-08 15:39:10,819] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 121.5, 0.0, 22.5, 27.79418489391018, 0.9751938337327393, 1.0, 1.0, 65.0, 47525.70730482397], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4712400.0000, 
sim time next is 4713000.0000, 
raw observation next is [1.166666666666667, 83.83333333333334, 126.0, 0.0, 22.5, 27.76882608742008, 0.9677167356939226, 1.0, 1.0, 65.0, 47324.82192397973], 
processed observation next is [1.0, 0.5652173913043478, 0.49492151431209613, 0.8383333333333334, 0.42, 0.0, 0.375, 0.8140688406183401, 0.8225722452313075, 1.0, 1.0, 1.0, 0.22535629487609396], 
reward next is 0.7746, 
noisyNet noise sample is [array([-0.59555477], dtype=float32), -0.1355122]. 
=============================================
[2019-04-08 15:39:10,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.93027]
 [86.92862]
 [86.97159]
 [87.05789]
 [87.24729]], R is [[86.82190704]
 [86.72737885]
 [86.63819885]
 [86.5568924 ]
 [86.48587799]].
[2019-04-08 15:39:11,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4235204e-16 8.7226032e-10 2.7516896e-06 6.8908030e-06 1.5128360e-09
 3.3690016e-13 1.2325168e-06 2.8466265e-13 2.3246430e-17 9.2556444e-19
 9.9998915e-01], sum to 1.0000
[2019-04-08 15:39:11,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3511
[2019-04-08 15:39:11,583] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 44.83333333333333, 211.6666666666667, 390.0, 19.0, 27.38933474659151, 0.9167881438240709, 0.0, 1.0, 65.0, 29547.32147291148], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 4891800.0000, 
sim time next is 4892400.0000, 
raw observation next is [3.0, 45.0, 199.5, 398.0, 19.0, 27.40812892008303, 0.9224108132206489, 0.0, 1.0, 65.0, 29194.71073366654], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.665, 0.4397790055248619, 0.08333333333333333, 0.7840107433402524, 0.8074702710735496, 0.0, 1.0, 1.0, 0.13902243206507875], 
reward next is 0.8610, 
noisyNet noise sample is [array([-1.3945613], dtype=float32), -0.9318605]. 
=============================================
[2019-04-08 15:39:11,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9571353e-18 5.6267005e-11 6.1132596e-08 5.9344575e-07 1.2840295e-11
 9.7569251e-13 4.4392126e-09 1.2480849e-15 2.6700350e-18 7.1228632e-22
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:39:11,586] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5087
[2019-04-08 15:39:11,608] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 19.0, 27.04655005114975, 0.8977601780927355, 0.0, 1.0, 65.0, 39409.20372957296], 
current ob forecast is [], 
actual action is [3.166666666666667, 65.0], 
sim time this is 4740600.0000, 
sim time next is 4741200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 27.05715102494304, 0.892496657323392, 0.0, 1.0, 65.0, 38851.41865167559], 
processed observation next is [1.0, 0.9130434782608695, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7547625854119199, 0.797498885774464, 0.0, 1.0, 1.0, 0.18500675548416945], 
reward next is 0.8150, 
noisyNet noise sample is [array([0.04565891], dtype=float32), -0.86822486]. 
=============================================
[2019-04-08 15:39:12,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1644010e-16 1.0681975e-09 1.5690404e-06 1.9522953e-07 5.1460042e-10
 8.8073776e-13 4.6978979e-08 8.4658016e-14 5.8705455e-18 1.2464246e-20
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:39:12,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3719
[2019-04-08 15:39:12,400] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.066666666666666, 92.33333333333333, 20.66666666666666, 69.83333333333331, 19.0, 26.1617024978497, 0.6202934879696679, 0.0, 1.0, 65.0, 53558.06066651281], 
current ob forecast is [], 
actual action is [-1.0666666666666664, 65.0], 
sim time this is 4779600.0000, 
sim time next is 4780200.0000, 
raw observation next is [-6.033333333333333, 92.16666666666667, 41.33333333333332, 139.6666666666666, 19.0, 26.16196664142695, 0.6292547586890386, 0.0, 1.0, 65.0, 53130.83201024606], 
processed observation next is [0.0, 0.30434782608695654, 0.29547553093259465, 0.9216666666666667, 0.13777777777777775, 0.1543278084714548, 0.08333333333333333, 0.6801638867855792, 0.7097515862296796, 0.0, 1.0, 1.0, 0.2530039619535526], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.6947193], dtype=float32), -0.2882592]. 
=============================================
[2019-04-08 15:39:12,893] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.5176110e-18 2.3877279e-11 1.0371141e-07 3.7615699e-08 3.4607643e-11
 8.9823656e-14 6.9046608e-09 7.9684979e-16 5.4535406e-17 2.4303794e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:12,895] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9831
[2019-04-08 15:39:12,913] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666667, 91.0, 211.3333333333333, 6.0, 22.5, 28.07337301715195, 1.00430193844529, 1.0, 1.0, 65.0, 22266.59428200768], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 4705800.0000, 
sim time next is 4706400.0000, 
raw observation next is [0.3333333333333333, 90.0, 212.1666666666667, 6.0, 22.5, 28.02608792636445, 1.011515397337716, 1.0, 1.0, 65.0, 23665.5238524976], 
processed observation next is [1.0, 0.4782608695652174, 0.4718374884579871, 0.9, 0.7072222222222224, 0.0066298342541436465, 0.375, 0.8355073271970376, 0.8371717991125719, 1.0, 1.0, 1.0, 0.11269297072617905], 
reward next is 0.8873, 
noisyNet noise sample is [array([-0.3487648], dtype=float32), 0.5675968]. 
=============================================
[2019-04-08 15:39:12,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8088041e-16 9.5550867e-10 3.6926696e-07 8.5128733e-07 2.8924503e-11
 3.8477572e-12 6.3078282e-08 7.2798789e-14 1.2593690e-18 1.9495502e-20
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:39:12,934] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9154
[2019-04-08 15:39:12,954] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 78.16666666666667, 0.0, 0.0, 19.0, 26.75556405034217, 0.8050364351876839, 0.0, 1.0, 65.0, 46517.40674438375], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 4747800.0000, 
sim time next is 4748400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 19.0, 26.73430584349169, 0.8021235659124747, 0.0, 1.0, 65.0, 46708.47802653897], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7278588202909742, 0.7673745219708249, 0.0, 1.0, 1.0, 0.22242132393589986], 
reward next is 0.7776, 
noisyNet noise sample is [array([-0.5024292], dtype=float32), -0.6268488]. 
=============================================
[2019-04-08 15:39:13,008] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6679189e-17 9.4578831e-11 8.5306404e-08 5.3675160e-07 2.2229121e-11
 1.7183542e-13 4.1658188e-09 8.5077922e-16 2.2322772e-18 1.9056173e-21
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:39:13,010] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9313
[2019-04-08 15:39:13,039] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 87.0, 178.0, 4.0, 22.5, 28.03909526002507, 1.0043536012086, 1.0, 1.0, 65.0, 40159.87986464829], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 4708200.0000, 
sim time next is 4708800.0000, 
raw observation next is [1.0, 86.0, 160.5, 3.0, 22.5, 28.04531319791704, 1.00478891396375, 1.0, 1.0, 65.0, 41636.2904349149], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.535, 0.0033149171270718232, 0.375, 0.8371094331597533, 0.8349296379879166, 1.0, 1.0, 1.0, 0.19826804969007097], 
reward next is 0.8017, 
noisyNet noise sample is [array([-0.57556736], dtype=float32), -1.0008149]. 
=============================================
[2019-04-08 15:39:13,074] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7467881e-15 1.4636151e-09 2.8879228e-06 3.1559973e-06 2.0702973e-09
 2.8572008e-11 2.2579829e-07 3.5431258e-13 5.8038552e-17 1.6019432e-19
 9.9999368e-01], sum to 1.0000
[2019-04-08 15:39:13,078] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4617
[2019-04-08 15:39:13,105] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.133333333333333, 92.66666666666667, 0.0, 0.0, 19.0, 26.21080353072792, 0.6265579394882597, 0.0, 1.0, 65.0, 53081.2010580537], 
current ob forecast is [], 
actual action is [-1.1333333333333329, 65.0], 
sim time this is 4776000.0000, 
sim time next is 4776600.0000, 
raw observation next is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 19.0, 26.20059606878685, 0.6235446397961011, 0.0, 1.0, 65.0, 53202.70006550462], 
processed observation next is [0.0, 0.2608695652173913, 0.2917820867959372, 0.9283333333333332, 0.0, 0.0, 0.08333333333333333, 0.6833830057322375, 0.7078482132653671, 0.0, 1.0, 1.0, 0.25334619078811726], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.50631], dtype=float32), -0.79249984]. 
=============================================
[2019-04-08 15:39:13,856] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.05339357e-17 1.47205859e-10 3.16732915e-08 1.00979214e-07
 8.42637793e-10 6.57187954e-13 1.76149683e-07 1.29541450e-15
 1.97258009e-18 1.50402228e-20 9.99999642e-01], sum to 1.0000
[2019-04-08 15:39:13,857] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4315
[2019-04-08 15:39:13,894] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 147.0, 0.0, 22.5, 28.11777755347843, 1.041976256530776, 1.0, 1.0, 65.0, 30609.06923193599], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4721400.0000, 
sim time next is 4722000.0000, 
raw observation next is [1.0, 72.0, 139.1666666666667, 1.833333333333333, 22.5, 28.11891528098328, 1.042475045334511, 1.0, 1.0, 65.0, 29825.43254467665], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.4638888888888891, 0.002025782688766114, 0.375, 0.84324294008194, 0.8474916817781702, 1.0, 1.0, 1.0, 0.142025869260365], 
reward next is 0.8580, 
noisyNet noise sample is [array([0.3781824], dtype=float32), -1.0357258]. 
=============================================
[2019-04-08 15:39:13,902] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[85.19402]
 [85.47637]
 [85.5624 ]
 [85.66335]
 [85.80689]], R is [[84.96334076]
 [84.96794891]
 [84.97145081]
 [84.99118805]
 [84.95970154]].
[2019-04-08 15:39:14,603] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2021329e-17 4.2579535e-09 1.4911656e-07 9.7459441e-08 5.3266763e-11
 3.1457673e-13 4.5695985e-09 1.8603641e-13 4.3180875e-18 2.3515271e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:39:14,603] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3450
[2019-04-08 15:39:14,617] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8333333333333334, 78.0, 0.0, 0.0, 22.5, 27.39656038310585, 0.9511607467487281, 1.0, 1.0, 65.0, 36741.48626507463], 
current ob forecast is [], 
actual action is [4.166666666666667, 65.0], 
sim time this is 4733400.0000, 
sim time next is 4734000.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 22.5, 27.34615880344806, 0.9478163814242014, 1.0, 1.0, 65.0, 36689.48489525061], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.375, 0.7788465669540049, 0.8159387938080672, 1.0, 1.0, 1.0, 0.1747118328345267], 
reward next is 0.8253, 
noisyNet noise sample is [array([-0.19390127], dtype=float32), -0.25709733]. 
=============================================
[2019-04-08 15:39:14,656] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[80.30306]
 [80.34005]
 [80.36419]
 [80.4007 ]
 [80.35319]], R is [[80.29187775]
 [80.31400299]
 [80.35016632]
 [80.38890076]
 [80.43364716]].
[2019-04-08 15:39:14,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5150978e-14 1.2766542e-09 6.9749376e-06 5.4106073e-05 9.8158042e-09
 3.1410645e-11 5.7374327e-06 9.0585021e-13 5.9825954e-15 3.8823265e-17
 9.9993312e-01], sum to 1.0000
[2019-04-08 15:39:14,741] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8011
[2019-04-08 15:39:14,752] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 19.0, 27.25260623768472, 0.8675165959547281, 0.0, 1.0, 65.0, 36786.17754316092], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4824000.0000, 
sim time next is 4824600.0000, 
raw observation next is [0.8333333333333334, 47.66666666666667, 0.0, 0.0, 19.0, 27.22747301344752, 0.8616986477698103, 0.0, 1.0, 65.0, 37097.21454269847], 
processed observation next is [0.0, 0.8695652173913043, 0.4856879039704525, 0.47666666666666674, 0.0, 0.0, 0.08333333333333333, 0.7689560844539599, 0.7872328825899367, 0.0, 1.0, 1.0, 0.17665340258427845], 
reward next is 0.8233, 
noisyNet noise sample is [array([-1.0480375], dtype=float32), -0.1503904]. 
=============================================
[2019-04-08 15:39:16,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7779973e-18 5.3095711e-10 5.7354068e-08 1.1825425e-06 9.5342012e-11
 1.6206362e-12 1.5960952e-08 1.3756089e-14 2.6862672e-18 1.8037408e-20
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:39:16,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9232
[2019-04-08 15:39:16,238] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 37.0, 94.5, 697.3333333333334, 19.0, 27.5455702915605, 1.005956597385234, 0.0, 1.0, 65.0, 27196.66634914278], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4808400.0000, 
sim time next is 4809000.0000, 
raw observation next is [3.0, 37.0, 92.0, 667.6666666666667, 19.0, 27.56531948595098, 1.008577607662206, 0.0, 1.0, 65.0, 26677.78869823014], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.30666666666666664, 0.7377532228360959, 0.08333333333333333, 0.7971099571625816, 0.8361925358874019, 0.0, 1.0, 1.0, 0.12703708903919114], 
reward next is 0.8730, 
noisyNet noise sample is [array([-0.6502386], dtype=float32), -0.79891324]. 
=============================================
[2019-04-08 15:39:16,245] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[80.422455]
 [80.46983 ]
 [80.5846  ]
 [80.65541 ]
 [80.727066]], R is [[80.40619659]
 [80.47262573]
 [80.5379715 ]
 [80.60643768]
 [80.67366028]].
[2019-04-08 15:39:16,977] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.5750163e-18 1.4435455e-09 2.8134207e-06 9.8154169e-07 6.2229364e-09
 1.2391988e-12 6.2770272e-07 2.2437601e-14 1.2204661e-16 1.0844817e-18
 9.9999559e-01], sum to 1.0000
[2019-04-08 15:39:16,979] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4644
[2019-04-08 15:39:17,005] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 39.0, 211.6666666666667, 604.5, 19.0, 27.25451429510969, 0.9204854799950303, 0.0, 1.0, 65.0, 29762.14262608508], 
current ob forecast is [], 
actual action is [7.333333333333333, 65.0], 
sim time this is 4800000.0000, 
sim time next is 4800600.0000, 
raw observation next is [2.5, 38.5, 220.0, 568.0, 19.0, 27.27406945305961, 0.9268805615437904, 0.0, 1.0, 65.0, 29450.30748869973], 
processed observation next is [0.0, 0.5652173913043478, 0.5318559556786704, 0.385, 0.7333333333333333, 0.6276243093922652, 0.08333333333333333, 0.7728391210883009, 0.8089601871812634, 0.0, 1.0, 1.0, 0.14023955946999872], 
reward next is 0.8598, 
noisyNet noise sample is [array([-0.37302408], dtype=float32), -0.68389916]. 
=============================================
[2019-04-08 15:39:17,134] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.4297931e-16 4.4641864e-09 2.7846315e-07 1.4095975e-06 3.0274028e-09
 3.1993982e-13 4.9970828e-07 1.0933408e-13 1.8918646e-16 9.5867627e-19
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:39:17,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2913
[2019-04-08 15:39:17,171] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3333333333333333, 52.33333333333334, 0.0, 0.0, 19.0, 27.11871890999179, 0.8292633793755453, 0.0, 1.0, 65.0, 39191.78632077706], 
current ob forecast is [], 
actual action is [4.666666666666667, 65.0], 
sim time this is 4828800.0000, 
sim time next is 4829400.0000, 
raw observation next is [-0.5, 53.0, 0.0, 0.0, 19.0, 27.10572288038842, 0.822122003417801, 0.0, 1.0, 65.0, 39035.60999114662], 
processed observation next is [0.0, 0.9130434782608695, 0.44875346260387816, 0.53, 0.0, 0.0, 0.08333333333333333, 0.7588102400323683, 0.7740406678059336, 0.0, 1.0, 1.0, 0.1858838571006982], 
reward next is 0.8141, 
noisyNet noise sample is [array([1.4465026], dtype=float32), 0.9847389]. 
=============================================
[2019-04-08 15:39:17,312] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6298467e-16 3.0009366e-09 3.4028329e-07 9.5813448e-07 3.6561871e-09
 1.6866236e-11 5.3900436e-08 2.4471869e-13 2.3097884e-17 9.3535946e-18
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:39:17,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4628
[2019-04-08 15:39:17,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3925546e-17 1.6097748e-09 4.0261110e-08 1.8848780e-08 1.1664195e-10
 3.8636157e-13 1.0164916e-08 2.1209586e-13 1.0937767e-17 4.4321681e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:17,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1209
[2019-04-08 15:39:17,328] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 19.0, 26.63902141701682, 0.6677504951606613, 0.0, 1.0, 65.0, 45722.23931846076], 
current ob forecast is [], 
actual action is [1.833333333333333, 65.0], 
sim time this is 4853400.0000, 
sim time next is 4854000.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 19.0, 26.62533536026152, 0.6637832879854448, 0.0, 1.0, 65.0, 45837.79427843151], 
processed observation next is [0.0, 0.17391304347826086, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.08333333333333333, 0.71877794668846, 0.7212610959951483, 0.0, 1.0, 1.0, 0.21827521084967386], 
reward next is 0.7817, 
noisyNet noise sample is [array([-0.91589266], dtype=float32), 0.8614259]. 
=============================================
[2019-04-08 15:39:17,342] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.05147576526047, 0.7998383667812595, 0.0, 1.0, 65.0, 40340.8613550084], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4832400.0000, 
sim time next is 4833000.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 19.0, 27.03622046677975, 0.7959357490259552, 0.0, 1.0, 65.0, 40875.7178348705], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.08333333333333333, 0.7530183722316458, 0.765311916341985, 0.0, 1.0, 1.0, 0.19464627540414522], 
reward next is 0.8054, 
noisyNet noise sample is [array([1.6072251], dtype=float32), -0.522694]. 
=============================================
[2019-04-08 15:39:17,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.67394]
 [77.69391]
 [77.71718]
 [77.76224]
 [77.79114]], R is [[77.69605255]
 [77.70136261]
 [77.70723724]
 [77.71501923]
 [77.72336578]].
[2019-04-08 15:39:17,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.025154]
 [79.95998 ]
 [79.91292 ]
 [79.85498 ]
 [79.82034 ]], R is [[80.04426575]
 [80.05171967]
 [80.06012726]
 [80.06661987]
 [80.07508087]].
[2019-04-08 15:39:17,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2769707e-15 2.7376785e-09 3.7687680e-06 2.0102445e-05 3.8144279e-09
 8.6863877e-11 2.3150010e-06 9.9466286e-12 1.4808450e-15 5.6591562e-19
 9.9997377e-01], sum to 1.0000
[2019-04-08 15:39:17,731] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8704
[2019-04-08 15:39:17,752] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.333333333333333, 61.66666666666667, 0.0, 0.0, 19.0, 26.52727550519242, 0.6313513632671609, 0.0, 1.0, 65.0, 47100.92201600102], 
current ob forecast is [], 
actual action is [1.666666666666667, 65.0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [-3.5, 62.5, 0.0, 0.0, 19.0, 26.51610264299315, 0.6299383455440936, 0.0, 1.0, 65.0, 47238.41439790096], 
processed observation next is [0.0, 0.2608695652173913, 0.36565096952908593, 0.625, 0.0, 0.0, 0.08333333333333333, 0.7096752202494292, 0.7099794485146979, 0.0, 1.0, 1.0, 0.22494483046619504], 
reward next is 0.7751, 
noisyNet noise sample is [array([0.12691566], dtype=float32), -0.90406376]. 
=============================================
[2019-04-08 15:39:18,662] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.6695856e-17 7.4940699e-11 9.2446228e-08 5.2969494e-07 7.3179712e-11
 2.6447350e-13 5.2769473e-09 5.2187169e-15 2.0351115e-17 4.2242043e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:39:18,663] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7112
[2019-04-08 15:39:18,690] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 37.0, 92.0, 667.6666666666667, 19.0, 27.56533834134791, 1.008587910044026, 0.0, 1.0, 65.0, 26677.37865593804], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4809000.0000, 
sim time next is 4809600.0000, 
raw observation next is [3.0, 37.0, 89.5, 638.0, 19.0, 27.58325473799719, 1.011152015713171, 0.0, 1.0, 65.0, 26405.32303034326], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.29833333333333334, 0.7049723756906078, 0.08333333333333333, 0.7986045614997659, 0.8370506719043904, 0.0, 1.0, 1.0, 0.12573963347782505], 
reward next is 0.8743, 
noisyNet noise sample is [array([0.22333145], dtype=float32), 0.113855876]. 
=============================================
[2019-04-08 15:39:18,821] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2123641e-18 1.2885033e-10 4.2191747e-08 1.8733397e-07 2.6722619e-11
 1.6133193e-12 3.4150700e-09 8.6699975e-14 1.6092300e-18 6.8647251e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:39:18,829] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4835
[2019-04-08 15:39:18,859] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 37.0, 160.0, 713.0, 19.0, 27.36547072562767, 0.9609034895501645, 0.0, 1.0, 65.0, 28182.58048626769], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4803600.0000, 
sim time next is 4804200.0000, 
raw observation next is [3.0, 37.0, 148.0, 742.0, 19.0, 27.38086069098382, 0.9664122435276696, 0.0, 1.0, 65.0, 28110.56404955822], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.49333333333333335, 0.8198895027624309, 0.08333333333333333, 0.7817383909153183, 0.8221374145092232, 0.0, 1.0, 1.0, 0.1338598288074201], 
reward next is 0.8661, 
noisyNet noise sample is [array([-0.6271884], dtype=float32), 0.4915585]. 
=============================================
[2019-04-08 15:39:19,068] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9812359e-16 3.6627923e-10 1.7256046e-06 3.5579419e-07 4.5099222e-09
 1.1265341e-12 5.5938816e-08 1.9312687e-13 3.4607551e-16 7.8308475e-19
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:39:19,073] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7801
[2019-04-08 15:39:19,084] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 174.0, 246.0, 19.0, 26.62940022129315, 0.6854864267919609, 0.0, 1.0, 65.0, 41526.76369096488], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 185.0, 223.6666666666667, 19.0, 26.63929757495939, 0.6888938160498483, 0.0, 1.0, 65.0, 41672.83335994762], 
processed observation next is [0.0, 0.391304347826087, 0.3841181902123731, 0.6416666666666667, 0.6166666666666667, 0.24714548802946598, 0.08333333333333333, 0.7199414645799491, 0.7296312720166162, 0.0, 1.0, 1.0, 0.19844206361879818], 
reward next is 0.8016, 
noisyNet noise sample is [array([-0.74898356], dtype=float32), -2.1108046]. 
=============================================
[2019-04-08 15:39:19,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3181783e-16 5.0233578e-10 1.7919663e-07 1.3785493e-06 2.7748498e-10
 3.3287156e-13 1.0779406e-08 2.8980074e-13 1.8751883e-17 2.4561383e-19
 9.9999845e-01], sum to 1.0000
[2019-04-08 15:39:19,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5642
[2019-04-08 15:39:19,183] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 44.0, 254.0, 381.0, 19.0, 27.32551389559201, 0.89201412489707, 0.0, 1.0, 65.0, 30172.0197375123], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4888800.0000, 
sim time next is 4889400.0000, 
raw observation next is [2.166666666666667, 44.16666666666667, 248.0, 378.6666666666667, 19.0, 27.34144029463283, 0.8967228896741014, 0.0, 1.0, 65.0, 30321.36006815014], 
processed observation next is [0.0, 0.6086956521739131, 0.5226223453370269, 0.4416666666666667, 0.8266666666666667, 0.41841620626151016, 0.08333333333333333, 0.7784533578860691, 0.7989076298913672, 0.0, 1.0, 1.0, 0.14438742889595305], 
reward next is 0.8556, 
noisyNet noise sample is [array([-0.31536815], dtype=float32), -0.9913781]. 
=============================================
[2019-04-08 15:39:19,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1583023e-16 3.0002906e-10 6.4825105e-07 1.9052413e-06 1.4714975e-09
 3.1731917e-13 5.7166338e-09 4.3208790e-14 2.4509721e-17 1.6784387e-18
 9.9999750e-01], sum to 1.0000
[2019-04-08 15:39:19,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0805
[2019-04-08 15:39:19,319] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 37.33333333333334, 0.0, 0.0, 19.0, 27.16237784002265, 0.8110719361214497, 0.0, 1.0, 65.0, 37329.23050184978], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4912800.0000, 
sim time next is 4913400.0000, 
raw observation next is [1.0, 36.66666666666666, 0.0, 0.0, 19.0, 27.16346214803168, 0.8067154205310442, 0.0, 1.0, 65.0, 37056.74426624217], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.3666666666666666, 0.0, 0.0, 0.08333333333333333, 0.7636218456693067, 0.7689051401770147, 0.0, 1.0, 1.0, 0.17646068698210557], 
reward next is 0.8235, 
noisyNet noise sample is [array([0.9757097], dtype=float32), 0.09998137]. 
=============================================
[2019-04-08 15:39:19,868] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.5644598e-18 8.4187775e-11 5.1320210e-08 1.5681215e-08 1.5676793e-11
 3.7728104e-14 3.8830832e-09 1.0453443e-14 1.8933456e-19 4.2396536e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:19,869] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1926
[2019-04-08 15:39:19,892] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.7, 44.5, 272.0, 388.0, 19.0, 27.27381419228648, 0.8747317966164759, 0.0, 1.0, 65.0, 30764.69056884684], 
current ob forecast is [], 
actual action is [6.7, 65.0], 
sim time this is 4887000.0000, 
sim time next is 4887600.0000, 
raw observation next is [1.8, 44.33333333333334, 266.0, 385.6666666666667, 19.0, 27.29523271148325, 0.8847810595228108, 0.0, 1.0, 65.0, 30907.26819903382], 
processed observation next is [0.0, 0.5652173913043478, 0.5124653739612189, 0.4433333333333334, 0.8866666666666667, 0.4261510128913444, 0.08333333333333333, 0.7746027259569376, 0.7949270198409369, 0.0, 1.0, 1.0, 0.14717746761444678], 
reward next is 0.8528, 
noisyNet noise sample is [array([-2.1586885], dtype=float32), 0.10989007]. 
=============================================
[2019-04-08 15:39:20,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3078172e-16 1.3670393e-10 1.0318104e-07 6.3605107e-08 4.3400252e-11
 2.4424890e-13 1.3684545e-08 2.9147288e-14 2.1049495e-18 8.3567561e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:39:20,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6905
[2019-04-08 15:39:20,687] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 45.83333333333334, 0.0, 0.0, 19.0, 27.28771525306702, 0.8594232185738292, 0.0, 1.0, 65.0, 34408.28282714043], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4907400.0000, 
sim time next is 4908000.0000, 
raw observation next is [1.0, 44.66666666666667, 0.0, 0.0, 19.0, 27.35499305921446, 0.8496761295925076, 0.0, 1.0, 65.0, 32488.61768988281], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4466666666666667, 0.0, 0.0, 0.08333333333333333, 0.7795827549345384, 0.7832253765308358, 0.0, 1.0, 1.0, 0.15470770328515623], 
reward next is 0.8453, 
noisyNet noise sample is [array([-1.410979], dtype=float32), 0.030753648]. 
=============================================
[2019-04-08 15:39:20,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.14005 ]
 [78.10594 ]
 [78.10152 ]
 [78.13867 ]
 [78.190315]], R is [[78.21632385]
 [78.27030945]
 [78.32614136]
 [78.38382721]
 [78.44390869]].
[2019-04-08 15:39:21,122] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1984078e-16 3.8055972e-09 1.4686137e-07 1.5404985e-06 1.6446679e-10
 3.2766539e-12 1.0626180e-08 4.9474107e-14 3.3742846e-18 1.8588199e-19
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:39:21,128] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3228
[2019-04-08 15:39:21,153] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5333333333333332, 48.66666666666667, 282.6666666666667, 321.6666666666667, 19.0, 26.98954884421985, 0.8023323410195636, 0.0, 1.0, 65.0, 34842.51265805266], 
current ob forecast is [], 
actual action is [5.533333333333333, 65.0], 
sim time this is 4880400.0000, 
sim time next is 4881000.0000, 
raw observation next is [0.7666666666666667, 47.83333333333334, 282.3333333333333, 335.3333333333333, 19.0, 27.01016070112177, 0.8092208084027197, 0.0, 1.0, 65.0, 34636.09439557173], 
processed observation next is [0.0, 0.4782608695652174, 0.4838411819021238, 0.47833333333333344, 0.941111111111111, 0.3705340699815838, 0.08333333333333333, 0.7508467250934808, 0.7697402694675732, 0.0, 1.0, 1.0, 0.16493378283605586], 
reward next is 0.8351, 
noisyNet noise sample is [array([-0.23458597], dtype=float32), -0.6921491]. 
=============================================
[2019-04-08 15:39:21,190] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[81.883896]
 [81.906845]
 [81.98018 ]
 [82.03101 ]
 [82.08457 ]], R is [[81.86716461]
 [81.88257599]
 [81.89680481]
 [81.90818787]
 [81.91557312]].
[2019-04-08 15:39:21,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.3085780e-15 3.7804373e-09 5.0186677e-06 2.6744679e-05 6.6195915e-09
 5.7022906e-12 1.8334507e-06 1.1550223e-12 1.3113960e-15 1.3575044e-18
 9.9996638e-01], sum to 1.0000
[2019-04-08 15:39:21,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0304
[2019-04-08 15:39:21,540] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 19.0, 26.62529133090279, 0.6637706952036999, 0.0, 1.0, 65.0, 45838.26612955851], 
current ob forecast is [], 
actual action is [1.666666666666667, 65.0], 
sim time this is 4854000.0000, 
sim time next is 4854600.0000, 
raw observation next is [-3.5, 65.5, 0.0, 0.0, 19.0, 26.61118288828593, 0.6673655436501723, 0.0, 1.0, 65.0, 46059.92528968715], 
processed observation next is [0.0, 0.17391304347826086, 0.36565096952908593, 0.655, 0.0, 0.0, 0.08333333333333333, 0.7175985740238277, 0.722455181216724, 0.0, 1.0, 1.0, 0.2193329775699388], 
reward next is 0.7807, 
noisyNet noise sample is [array([0.443959], dtype=float32), -1.7693084]. 
=============================================
[2019-04-08 15:39:21,548] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.1737051e-18 2.5908952e-11 1.6427526e-09 7.0439903e-08 3.4959159e-13
 1.7915979e-13 5.6927285e-10 2.2398378e-16 3.2198052e-19 1.3990878e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:21,550] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3359
[2019-04-08 15:39:21,598] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.0, 26.0, 106.1666666666667, 811.5, 22.5, 29.12715973787175, 1.243352562000553, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [13.0, 65.0], 
sim time this is 4977600.0000, 
sim time next is 4978200.0000, 
raw observation next is [8.0, 26.0, 103.3333333333333, 804.0, 22.5, 29.16196039269911, 1.178089862152081, 1.0, 1.0, 65.0, 22223.709920316], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.34444444444444433, 0.8883977900552487, 0.375, 0.9301633660582592, 0.8926966207173604, 1.0, 1.0, 1.0, 0.10582719009674285], 
reward next is 0.8942, 
noisyNet noise sample is [array([-0.29041702], dtype=float32), -1.174191]. 
=============================================
[2019-04-08 15:39:21,952] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:22,121] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:22,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6520426e-18 5.0354154e-13 4.3189171e-08 4.5669175e-08 6.1700996e-12
 2.3816124e-13 2.7821564e-10 7.2025058e-17 6.0127003e-20 2.4044838e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:22,277] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6005
[2019-04-08 15:39:22,310] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 24.66666666666667, 0.0, 0.0, 22.5, 28.87876009320301, 1.157601854700974, 1.0, 1.0, 65.0, 23891.82784127028], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 4990200.0000, 
sim time next is 4990800.0000, 
raw observation next is [6.0, 24.33333333333334, 0.0, 0.0, 22.5, 28.25384462277959, 1.223196346957286, 1.0, 1.0, 65.0, 42309.13986811272], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2433333333333334, 0.0, 0.0, 0.375, 0.8544870518982991, 0.9077321156524286, 1.0, 1.0, 1.0, 0.20147209461006055], 
reward next is 0.7985, 
noisyNet noise sample is [array([-0.11229347], dtype=float32), -0.20229527]. 
=============================================
[2019-04-08 15:39:22,953] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:22,954] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:22,960] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res14/Eplus-env-sub_run3
[2019-04-08 15:39:23,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7014915e-15 5.5200855e-10 7.3743990e-07 2.4186238e-05 1.0882933e-09
 2.3655021e-11 2.4766931e-07 4.9962855e-13 4.5564377e-17 2.3981616e-18
 9.9997485e-01], sum to 1.0000
[2019-04-08 15:39:23,400] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8030
[2019-04-08 15:39:23,413] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 22.5, 26.64298152721717, 0.6151471581249252, 0.0, 1.0, 65.0, 45037.69761242488], 
current ob forecast is [], 
actual action is [2.166666666666667, 65.0], 
sim time this is 4950600.0000, 
sim time next is 4951200.0000, 
raw observation next is [-2.666666666666667, 48.66666666666667, 0.0, 0.0, 22.5, 26.63229588988178, 0.6204546973076506, 1.0, 1.0, 65.0, 44959.19653557804], 
processed observation next is [1.0, 0.30434782608695654, 0.38873499538319484, 0.4866666666666667, 0.0, 0.0, 0.375, 0.7193579908234818, 0.7068182324358835, 1.0, 1.0, 1.0, 0.21409141207418114], 
reward next is 0.7859, 
noisyNet noise sample is [array([-0.30393946], dtype=float32), -0.28659272]. 
=============================================
[2019-04-08 15:39:24,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6240330e-18 4.0652215e-10 1.5812940e-07 1.6771521e-06 2.5851163e-10
 2.8191653e-13 1.6076899e-07 1.1200562e-14 2.4614378e-18 3.9501131e-20
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:39:24,272] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7652
[2019-04-08 15:39:24,287] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 19.0, 27.04687736973346, 0.7980084801587699, 0.0, 1.0, 65.0, 38433.99547409765], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 5029200.0000, 
sim time next is 5029800.0000, 
raw observation next is [-1.0, 49.33333333333334, 0.0, 0.0, 19.0, 27.11591436007266, 0.8042714143611603, 0.0, 1.0, 65.0, 36595.21768401267], 
processed observation next is [1.0, 0.21739130434782608, 0.4349030470914128, 0.4933333333333334, 0.0, 0.0, 0.08333333333333333, 0.7596595300060551, 0.7680904714537201, 0.0, 1.0, 1.0, 0.17426294135244128], 
reward next is 0.8257, 
noisyNet noise sample is [array([-0.0875589], dtype=float32), -0.18701987]. 
=============================================
[2019-04-08 15:39:24,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6468761e-18 4.7984011e-10 8.7270041e-07 1.0597654e-07 2.5348812e-10
 4.9160528e-14 9.1236245e-09 2.1746307e-14 5.9597490e-17 1.6782218e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:39:24,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1846
[2019-04-08 15:39:24,842] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 22.5, 28.18542473213492, 1.122683184232714, 1.0, 1.0, 65.0, 18848.11205781581], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 4995600.0000, 
sim time next is 4996200.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 22.5, 28.14034377801305, 1.121981704895541, 0.0, 1.0, 65.0, 18847.75936982472], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.375, 0.8450286481677542, 0.873993901631847, 0.0, 1.0, 1.0, 0.08975123509440341], 
reward next is 0.9102, 
noisyNet noise sample is [array([-2.2184262], dtype=float32), 0.73003024]. 
=============================================
[2019-04-08 15:39:24,844] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1587481e-17 2.1181033e-10 1.7472388e-07 2.0418663e-07 1.3247896e-11
 1.1697549e-13 1.2010983e-08 6.8095820e-15 3.8890439e-19 3.1572715e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:39:24,845] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6392
[2019-04-08 15:39:24,855] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.333333333333334, 24.66666666666666, 118.0, 860.8333333333334, 22.5, 28.6139857149616, 1.147840884471913, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [12.333333333333334, 65.0], 
sim time this is 4972800.0000, 
sim time next is 4973400.0000, 
raw observation next is [7.5, 25.0, 117.0, 860.0, 22.5, 28.79643117217174, 1.156562957483628, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6703601108033241, 0.25, 0.39, 0.9502762430939227, 0.375, 0.8997025976809784, 0.885520985827876, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45276105], dtype=float32), -0.91213924]. 
=============================================
[2019-04-08 15:39:25,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2841376e-17 5.4512419e-11 7.4543414e-07 5.6719837e-06 2.1206328e-09
 4.0575339e-13 5.5721500e-07 7.7615940e-14 1.3660450e-17 2.0300167e-20
 9.9999297e-01], sum to 1.0000
[2019-04-08 15:39:25,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7089
[2019-04-08 15:39:25,141] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.3333333333333334, 38.0, 0.0, 0.0, 19.0, 27.12181968907943, 0.7667249618652203, 0.0, 1.0, 65.0, 37557.15076798626], 
current ob forecast is [], 
actual action is [5.333333333333333, 65.0], 
sim time this is 4920000.0000, 
sim time next is 4920600.0000, 
raw observation next is [0.1666666666666666, 38.5, 0.0, 0.0, 19.0, 27.10313571828714, 0.7676440405133734, 0.0, 1.0, 65.0, 38980.58563443013], 
processed observation next is [0.0, 0.9565217391304348, 0.4672206832871654, 0.385, 0.0, 0.0, 0.08333333333333333, 0.7585946431905951, 0.7558813468377911, 0.0, 1.0, 1.0, 0.1856218363544292], 
reward next is 0.8144, 
noisyNet noise sample is [array([-0.3252581], dtype=float32), -0.5958866]. 
=============================================
[2019-04-08 15:39:26,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.8179953e-18 3.9565785e-11 8.9980659e-08 5.1920303e-08 6.6317329e-12
 2.1755297e-13 2.1087940e-08 5.7888387e-15 2.3627436e-18 8.3578685e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:26,424] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4498
[2019-04-08 15:39:26,442] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 22.5, 28.22462957873194, 1.017364890272225, 1.0, 1.0, 65.0, 18847.98689318835], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [3.0, 41.0, 114.0, 753.5, 22.5, 28.2328701425935, 1.041275216028429, 1.0, 1.0, 65.0, 18849.18030531269], 
processed observation next is [1.0, 0.43478260869565216, 0.5457063711911359, 0.41, 0.38, 0.8325966850828729, 0.375, 0.8527391785494585, 0.8470917386761431, 1.0, 1.0, 1.0, 0.08975800145386995], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.29388207], dtype=float32), -0.6862423]. 
=============================================
[2019-04-08 15:39:26,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4678166e-18 2.0990323e-10 4.6784209e-08 2.0163178e-07 4.2551154e-11
 2.5163509e-14 1.4190775e-09 2.3747829e-15 5.0745540e-19 2.8275487e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:39:26,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1753
[2019-04-08 15:39:26,890] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 38.0, 0.0, 0.0, 19.0, 27.41163927350598, 0.9281153846585246, 0.0, 1.0, 65.0, 35044.72261549925], 
current ob forecast is [], 
actual action is [7.333333333333333, 65.0], 
sim time this is 5010000.0000, 
sim time next is 5010600.0000, 
raw observation next is [2.166666666666667, 39.0, 0.0, 0.0, 19.0, 27.38717478419151, 0.9221276626056235, 0.0, 1.0, 65.0, 35299.77721818006], 
processed observation next is [1.0, 1.0, 0.5226223453370269, 0.39, 0.0, 0.0, 0.08333333333333333, 0.7822645653492927, 0.8073758875352078, 0.0, 1.0, 1.0, 0.16809417722942885], 
reward next is 0.8319, 
noisyNet noise sample is [array([0.21802157], dtype=float32), -0.3986791]. 
=============================================
[2019-04-08 15:39:26,956] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6572425e-19 8.3338621e-11 3.5086572e-08 2.4634605e-08 2.2945918e-11
 1.1724954e-14 1.6363138e-09 1.9278959e-15 2.7701028e-19 1.6042141e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:26,959] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1232
[2019-04-08 15:39:26,983] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 23.33333333333334, 0.0, 0.0, 22.5, 28.36862598609203, 1.150536457083384, 1.0, 1.0, 65.0, 18848.13678716615], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 4992600.0000, 
sim time next is 4993200.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 22.5, 28.39410579845246, 1.162259652257625, 1.0, 1.0, 65.0, 18848.3255067914], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.375, 0.8661754832043718, 0.887419884085875, 1.0, 1.0, 1.0, 0.08975393098472095], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.0607464], dtype=float32), -0.58534145]. 
=============================================
[2019-04-08 15:39:27,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1894837e-16 3.3568632e-09 1.1546610e-06 1.0026398e-06 4.1474584e-09
 1.3530015e-12 5.0679944e-08 4.0390754e-13 5.6001967e-17 7.4260698e-19
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:39:27,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5014
[2019-04-08 15:39:27,053] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 46.66666666666667, 0.0, 0.0, 19.0, 27.13707859878586, 0.7790931660169808, 0.0, 1.0, 65.0, 36653.66214033646], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 5032200.0000, 
sim time next is 5032800.0000, 
raw observation next is [-1.0, 46.0, 0.0, 0.0, 19.0, 27.09037649661417, 0.7780096831924547, 0.0, 1.0, 65.0, 37495.0833332011], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.46, 0.0, 0.0, 0.08333333333333333, 0.7575313747178475, 0.7593365610641515, 0.0, 1.0, 1.0, 0.1785480158723862], 
reward next is 0.8215, 
noisyNet noise sample is [array([0.0116292], dtype=float32), 0.01428523]. 
=============================================
[2019-04-08 15:39:27,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6692296e-19 5.4926208e-10 1.8335351e-07 2.2463526e-07 2.9902573e-11
 6.9625783e-14 1.8998636e-08 2.1712154e-15 5.6583277e-20 5.9534228e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:39:27,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-08 15:39:27,383] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.66666666666667, 19.33333333333334, 108.5, 806.6666666666666, 22.5, 29.75482082110233, 1.423316150810389, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.66666666666667, 65.0], 
sim time this is 5064000.0000, 
sim time next is 5064600.0000, 
raw observation next is [11.83333333333333, 19.16666666666667, 106.0, 794.3333333333334, 22.5, 29.80613914175142, 1.428219397470328, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7903970452446908, 0.1916666666666667, 0.35333333333333333, 0.8777163904235727, 0.375, 0.983844928479285, 0.9760731324901094, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39921582], dtype=float32), -0.3882062]. 
=============================================
[2019-04-08 15:39:27,542] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.00514025e-17 5.56483248e-10 1.23774086e-07 2.52196764e-06
 6.81059764e-10 1.42372605e-13 2.79758239e-09 1.37984039e-14
 6.20999176e-19 1.12191177e-21 9.99997377e-01], sum to 1.0000
[2019-04-08 15:39:27,543] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4108
[2019-04-08 15:39:27,567] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 62.00000000000001, 84.33333333333333, 389.0, 22.5, 26.9895338729592, 0.8019557903340058, 1.0, 1.0, 65.0, 35579.92797831039], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 5040600.0000, 
sim time next is 5041200.0000, 
raw observation next is [-1.0, 59.0, 90.66666666666667, 461.0, 22.5, 27.05092253996277, 0.841673925813879, 1.0, 1.0, 65.0, 33101.75464864905], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.59, 0.3022222222222222, 0.5093922651933702, 0.375, 0.7542435449968975, 0.780557975271293, 1.0, 1.0, 1.0, 0.157627403088805], 
reward next is 0.8424, 
noisyNet noise sample is [array([0.9007946], dtype=float32), 0.26214153]. 
=============================================
[2019-04-08 15:39:27,722] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5990085e-18 1.5394405e-12 1.2055836e-08 1.6892169e-08 8.2375053e-12
 3.8112742e-14 2.8564369e-09 1.2711779e-16 1.8206532e-19 8.7236253e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:39:27,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1823
[2019-04-08 15:39:27,740] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 29.33333333333334, 123.1666666666667, 848.3333333333334, 22.5, 28.39865977567803, 1.180029900173622, 1.0, 1.0, 64.99999999999994, 12565.22459606885], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 5053200.0000, 
sim time next is 5053800.0000, 
raw observation next is [7.5, 27.66666666666666, 123.3333333333333, 851.6666666666666, 22.5, 28.77749015309886, 1.028361996224064, 1.0, 1.0, 65.0, 9423.502613902592], 
processed observation next is [1.0, 0.4782608695652174, 0.6703601108033241, 0.2766666666666666, 0.411111111111111, 0.9410681399631675, 0.375, 0.898124179424905, 0.842787332074688, 1.0, 1.0, 1.0, 0.044873821970964727], 
reward next is 0.9551, 
noisyNet noise sample is [array([-0.8325799], dtype=float32), -0.4374453]. 
=============================================
[2019-04-08 15:39:27,823] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8498421e-18 1.2260046e-10 7.4368998e-08 2.0155969e-07 2.9457786e-12
 3.9058976e-15 3.1405223e-09 2.3820775e-15 2.4675989e-19 4.3804456e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:39:27,823] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6439
[2019-04-08 15:39:27,835] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 43.0, 112.6666666666667, 716.5, 22.5, 28.09690821293238, 1.007826793905487, 1.0, 1.0, 65.0, 18846.74985826887], 
current ob forecast is [], 
actual action is [7.333333333333333, 65.0], 
sim time this is 5046000.0000, 
sim time next is 5046600.0000, 
raw observation next is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 22.5, 28.22463681672455, 1.017366901906386, 1.0, 1.0, 65.0, 18847.98696264585], 
processed observation next is [1.0, 0.391304347826087, 0.5364727608494922, 0.42, 0.37777777777777766, 0.8121546961325967, 0.375, 0.8520530680603793, 0.839122300635462, 1.0, 1.0, 1.0, 0.08975231886974214], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.7768959], dtype=float32), 0.7279652]. 
=============================================
[2019-04-08 15:39:27,905] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:28,009] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7473545e-18 3.6757999e-10 6.8970803e-07 2.8621224e-07 8.2579435e-13
 1.1798201e-12 4.9778488e-09 1.7812558e-16 6.3550617e-19 1.3981301e-22
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:39:28,012] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6269
[2019-04-08 15:39:28,019] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.66666666666667, 19.33333333333334, 108.5, 806.6666666666666, 22.5, 29.75482083021985, 1.423316153242794, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.66666666666667, 65.0], 
sim time this is 5064000.0000, 
sim time next is 5064600.0000, 
raw observation next is [11.83333333333333, 19.16666666666667, 106.0, 794.3333333333334, 22.5, 29.8061391510247, 1.428219399928184, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7903970452446908, 0.1916666666666667, 0.35333333333333333, 0.8777163904235727, 0.375, 0.9838449292520582, 0.9760731333093947, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5349656], dtype=float32), -1.0240103]. 
=============================================
[2019-04-08 15:39:28,066] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.4393681e-18 1.4835089e-09 9.0195101e-08 7.2836322e-08 2.8122118e-12
 1.6007424e-13 2.7981604e-09 8.7890537e-16 6.9442446e-20 6.4107916e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:28,066] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:28,067] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8651
[2019-04-08 15:39:28,087] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 22.5, 29.29462684422571, 1.350077455313049, 1.0, 1.0, 65.0, 18849.05816437273], 
current ob forecast is [], 
actual action is [16.0, 65.0], 
sim time this is 5079000.0000, 
sim time next is 5079600.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 22.5, 29.17124732248876, 1.338242345126313, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7673130193905818, 0.17, 0.0, 0.0, 0.375, 0.9309372768740634, 0.946080781708771, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4243991], dtype=float32), -0.50510114]. 
=============================================
[2019-04-08 15:39:28,129] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.2917951e-18 1.9403150e-09 1.2211463e-07 9.6755613e-08 3.8178983e-12
 2.3954070e-13 4.1780508e-09 1.3112696e-15 1.0625974e-19 1.0667226e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:39:28,129] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4973
[2019-04-08 15:39:28,164] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.5, 18.0, 0.0, 0.0, 22.5, 28.90829123154483, 1.295650212523987, 1.0, 1.0, 65.0, 18849.26882584704], 
current ob forecast is [], 
actual action is [15.5, 65.0], 
sim time this is 5081400.0000, 
sim time next is 5082000.0000, 
raw observation next is [10.33333333333333, 18.33333333333333, 0.0, 0.0, 22.5, 28.81908397583566, 1.284435367699347, 0.0, 1.0, 65.0, 18849.41829995258], 
processed observation next is [1.0, 0.8260869565217391, 0.7488457987072946, 0.1833333333333333, 0.0, 0.0, 0.375, 0.9015903313196384, 0.9281451225664489, 0.0, 1.0, 1.0, 0.08975913476167895], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.4243991], dtype=float32), -0.50510114]. 
=============================================
[2019-04-08 15:39:28,182] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.461555]
 [86.388336]
 [86.85304 ]
 [86.94005 ]
 [87.08663 ]], R is [[85.93580627]
 [85.98668671]
 [86.12682343]
 [86.17579651]
 [86.31404114]].
[2019-04-08 15:39:28,316] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8264958e-18 1.0941941e-09 7.8337138e-08 3.2786079e-07 1.1614546e-11
 9.2953282e-13 6.5097274e-09 3.6406268e-15 1.2641107e-19 1.2151071e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:39:28,320] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1528
[2019-04-08 15:39:28,343] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 22.5, 28.58389608439565, 0.933952957847139, 1.0, 1.0, 65.0, 18847.53250082728], 
current ob forecast is [], 
actual action is [9.666666666666668, 65.0], 
sim time this is 5050200.0000, 
sim time next is 5050800.0000, 
raw observation next is [5.0, 36.0, 119.5, 827.0, 22.5, 28.47700985424496, 1.176099928013026, 1.0, 1.0, 65.0, 23412.99772935297], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.36, 0.3983333333333333, 0.9138121546961326, 0.375, 0.8730841545204134, 0.8920333093376754, 1.0, 1.0, 1.0, 0.11149046537787129], 
reward next is 0.8885, 
noisyNet noise sample is [array([-0.48941797], dtype=float32), -1.0318558]. 
=============================================
[2019-04-08 15:39:28,534] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:28,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4036070e-17 1.9253734e-09 1.1158143e-06 2.8229445e-06 8.2534396e-11
 1.1942903e-13 2.4289299e-08 8.3438175e-15 1.3560281e-18 5.0974793e-22
 9.9999607e-01], sum to 1.0000
[2019-04-08 15:39:28,615] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9586
[2019-04-08 15:39:28,649] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.0, 17.0, 56.0, 438.5, 22.5, 30.3040514249608, 1.538160381368332, 1.0, 1.0, 65.00000000000009, 5384.606046804784], 
current ob forecast is [], 
actual action is [17.0, 65.0], 
sim time this is 5072400.0000, 
sim time next is 5073000.0000, 
raw observation next is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 22.5, 30.01785396719128, 1.504331540256611, 1.0, 1.0, 64.99999999999999, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7903970452446908, 0.17, 0.16444444444444442, 0.4305709023941067, 0.375, 1.0014878305992732, 1.0014438467522038, 1.0, 1.0, 0.9999999999999997, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28862002], dtype=float32), 0.43923855]. 
=============================================
[2019-04-08 15:39:28,665] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[92.845215]
 [93.025   ]
 [93.1802  ]
 [93.21851 ]
 [93.31508 ]], R is [[92.78364563]
 [92.83016968]
 [92.85699463]
 [92.92842865]
 [92.99914551]].
[2019-04-08 15:39:28,708] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:28,719] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7436524e-20 8.3941612e-12 8.2673930e-09 2.2556327e-08 3.7661481e-13
 8.2218896e-15 1.5184094e-08 3.0552542e-16 3.1412962e-20 5.2170243e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:39:28,722] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7588
[2019-04-08 15:39:28,730] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 22.5, 29.6203416351578, 1.397817162460199, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.33333333333333, 65.0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [11.5, 19.5, 111.0, 819.0, 22.5, 29.69286170985365, 1.411936168521649, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7811634349030472, 0.195, 0.37, 0.9049723756906077, 0.375, 0.9744051424878041, 0.9706453895072163, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9267335], dtype=float32), 0.5447333]. 
=============================================
[2019-04-08 15:39:28,852] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:28,905] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:28,905] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:28,905] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:28,909] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res17/Eplus-env-sub_run3
[2019-04-08 15:39:29,017] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:29,076] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:29,537] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:29,537] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:29,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1320914e-19 2.4435151e-10 7.5412459e-07 1.2125007e-05 5.8446373e-11
 2.8500477e-14 8.8424263e-09 8.7952090e-17 1.0643446e-19 2.4701149e-23
 9.9998713e-01], sum to 1.0000
[2019-04-08 15:39:29,546] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5077
[2019-04-08 15:39:29,548] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res15/Eplus-env-sub_run3
[2019-04-08 15:39:29,571] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:29,573] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.0, 18.33333333333334, 79.33333333333333, 611.6666666666666, 22.5, 30.21828111707811, 1.530107046669789, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [17.0, 65.0], 
sim time this is 5070000.0000, 
sim time next is 5070600.0000, 
raw observation next is [12.0, 18.0, 76.0, 585.0, 22.5, 30.28261777697332, 1.532425550964156, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.18, 0.25333333333333335, 0.6464088397790055, 0.375, 1.0235514814144435, 1.010808516988052, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7505386], dtype=float32), 0.3580819]. 
=============================================
[2019-04-08 15:39:29,631] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:29,663] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5661099e-20 4.1620922e-11 2.1258973e-08 5.6376566e-09 6.4643477e-12
 1.2891531e-13 5.3112017e-09 2.6404041e-15 2.2242106e-19 4.1370824e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:39:29,663] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2586
[2019-04-08 15:39:29,674] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.75, 19.0, 0.0, 0.0, 19.0, 28.33791979187321, 1.192045260314388, 0.0, 1.0, 65.0, 18848.4027038021], 
current ob forecast is [], 
actual action is [13.75, 65.0], 
sim time this is 5089800.0000, 
sim time next is 5090400.0000, 
raw observation next is [8.7, 19.0, 0.0, 0.0, 19.0, 28.31755689513938, 1.186825715945635, 0.0, 1.0, 65.0, 18848.44114657092], 
processed observation next is [1.0, 0.9565217391304348, 0.703601108033241, 0.19, 0.0, 0.0, 0.08333333333333333, 0.8597964079282816, 0.8956085719818784, 0.0, 1.0, 1.0, 0.08975448165033771], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.03414835], dtype=float32), 0.12743662]. 
=============================================
[2019-04-08 15:39:29,752] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:29,802] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:29,856] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:29,856] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:29,865] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res6/Eplus-env-sub_run3
[2019-04-08 15:39:29,906] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:29,907] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:29,910] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res2/Eplus-env-sub_run3
[2019-04-08 15:39:30,026] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:30,073] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:30,217] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:30,253] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:30,272] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:30,348] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:30,393] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:30,510] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:30,572] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:30,572] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:30,574] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res5/Eplus-env-sub_run3
[2019-04-08 15:39:30,594] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:30,616] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:30,616] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:30,618] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res10/Eplus-env-sub_run3
[2019-04-08 15:39:30,729] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:30,789] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:30,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:30,923] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0019843e-17 5.5309418e-10 1.5320741e-07 9.2900713e-08 4.5197002e-11
 7.4480869e-15 3.6270587e-10 5.0586188e-15 3.2871822e-19 3.3395032e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:39:30,923] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1166
[2019-04-08 15:39:30,930] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.83333333333333, 17.33333333333334, 0.0, 0.0, 22.5, 29.10494762008131, 1.324545156044333, 1.0, 1.0, 65.0, 18849.39575066593], 
current ob forecast is [], 
actual action is [15.83333333333333, 65.0], 
sim time this is 5080200.0000, 
sim time next is 5080800.0000, 
raw observation next is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 22.5, 29.01278391419507, 1.304757152658634, 0.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7580794090489382, 0.17666666666666672, 0.0, 0.0, 0.375, 0.9177319928495891, 0.9349190508862112, 0.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3979037], dtype=float32), 1.2360876]. 
=============================================
[2019-04-08 15:39:31,033] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:31,033] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:31,035] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res12/Eplus-env-sub_run3
[2019-04-08 15:39:31,073] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:31,073] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:31,075] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res4/Eplus-env-sub_run3
[2019-04-08 15:39:31,222] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:31,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:31,224] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res11/Eplus-env-sub_run3
[2019-04-08 15:39:31,345] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:31,345] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:31,347] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res13/Eplus-env-sub_run3
[2019-04-08 15:39:31,585] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:31,585] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:31,587] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res16/Eplus-env-sub_run3
[2019-04-08 15:39:31,647] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:31,725] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:31,725] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:31,727] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res9/Eplus-env-sub_run3
[2019-04-08 15:39:31,873] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:32,665] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:32,665] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:32,667] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-8_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res8/Eplus-env-sub_run3
[2019-04-08 15:39:32,717] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:33,024] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:33,593] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3-EPLUSPROCESS_EPI_1 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:39:33,718] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:33,719] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:33,720] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-7_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res7/Eplus-env-sub_run3
[2019-04-08 15:39:33,883] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3-EPLUSPROCESS_EPI_1 ERROR:Aborted (core dumped)

[2019-04-08 15:39:34,581] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-08 15:39:34,581] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:39:34,583] EPLUS_ENV_Part4-Light-Pit-Train-v2_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res3/Eplus-env-sub_run3
[2019-04-08 15:39:42,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8516166e-20 5.5151465e-12 2.5024729e-09 4.5327481e-10 2.7300977e-11
 1.2138101e-14 2.8118516e-09 3.2864624e-15 1.4863132e-20 4.2747603e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:39:42,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1303
[2019-04-08 15:39:42,905] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 45.33333333333333, 0.0, 19.0, 25.50798864791469, 0.4407223399912285, 0.0, 1.0, 65.0, 53512.41669793708], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 35400.0000, 
sim time next is 36000.0000, 
raw observation next is [7.7, 93.0, 49.0, 0.0, 19.0, 25.53488823528432, 0.4481107189113682, 0.0, 1.0, 65.0, 53349.08297970954], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.16333333333333333, 0.0, 0.08333333333333333, 0.62790735294036, 0.6493702396371227, 0.0, 1.0, 1.0, 0.25404325228433117], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.4412022], dtype=float32), -1.3150479]. 
=============================================
[2019-04-08 15:39:42,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.32886 ]
 [87.21207 ]
 [87.07019 ]
 [86.961105]
 [86.79582 ]], R is [[87.33628082]
 [87.20809937]
 [87.07962799]
 [86.95171356]
 [86.82807159]].
[2019-04-08 15:39:44,972] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9041381e-15 1.2047024e-09 2.4559623e-07 1.2643110e-06 1.2154510e-09
 5.3169400e-12 4.6465868e-07 4.2806291e-14 3.8862236e-17 2.5658628e-19
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:39:44,973] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6722
[2019-04-08 15:39:44,976] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.6331615e-19 1.0783271e-11 2.4373065e-08 5.5816152e-08 6.3179496e-12
 3.1783160e-14 3.6569030e-09 9.6811029e-15 1.6307159e-19 3.4768021e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:44,977] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4591
[2019-04-08 15:39:44,995] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 22.5, 26.47956856183929, 0.5875569327812571, 1.0, 1.0, 65.0, 46856.50329106954], 
current ob forecast is [], 
actual action is [-5.96666666666667, 65.0], 
sim time this is 297600.0000, 
sim time next is 298200.0000, 
raw observation next is [-10.78333333333333, 60.5, 93.33333333333334, 560.6666666666666, 22.5, 26.51530415337251, 0.5986973275708464, 1.0, 1.0, 65.0, 46864.11027430483], 
processed observation next is [1.0, 0.43478260869565216, 0.1638965835641737, 0.605, 0.3111111111111111, 0.6195211786372007, 0.375, 0.7096086794477093, 0.6995657758569488, 1.0, 1.0, 1.0, 0.22316242987764207], 
reward next is 0.7768, 
noisyNet noise sample is [array([1.2473311], dtype=float32), 1.1959969]. 
=============================================
[2019-04-08 15:39:44,997] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.7, 93.0, 14.0, 0.0, 19.0, 25.27763844914561, 0.3775446760731943, 0.0, 1.0, 65.0, 53430.41896304545], 
current ob forecast is [], 
actual action is [12.7, 65.0], 
sim time this is 29400.0000, 
sim time next is 30000.0000, 
raw observation next is [7.7, 93.0, 17.5, 0.0, 19.0, 25.31630463128963, 0.3861202507907875, 0.0, 1.0, 65.0, 53550.51979050488], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.058333333333333334, 0.0, 0.08333333333333333, 0.6096920526074691, 0.6287067502635958, 0.0, 1.0, 1.0, 0.2550024751928804], 
reward next is 0.7450, 
noisyNet noise sample is [array([0.7388822], dtype=float32), 1.8309232]. 
=============================================
[2019-04-08 15:39:45,007] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.89256 ]
 [85.75318 ]
 [85.602425]
 [85.4818  ]
 [85.36468 ]], R is [[85.9395752 ]
 [85.82574463]
 [85.70671082]
 [85.58786011]
 [85.47026062]].
[2019-04-08 15:39:47,088] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.16196474e-18 1.83048680e-11 1.16166730e-08 2.06070645e-08
 1.16721355e-11 1.21745089e-13 5.65237246e-10 8.90409716e-16
 9.26401030e-19 1.29113856e-21 1.00000000e+00], sum to 1.0000
[2019-04-08 15:39:47,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8404
[2019-04-08 15:39:47,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.25745688598154, 0.5951658548201305, 0.0, 1.0, 65.0, 52007.18689778061], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 97200.0000, 
sim time next is 97800.0000, 
raw observation next is [-2.9, 85.66666666666667, 0.0, 0.0, 19.0, 26.22458990104, 0.6023530185663041, 0.0, 1.0, 65.0, 53403.07538401038], 
processed observation next is [1.0, 0.13043478260869565, 0.38227146814404434, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.6853824917533334, 0.7007843395221013, 0.0, 1.0, 1.0, 0.254300358971478], 
reward next is 0.7457, 
noisyNet noise sample is [array([-0.09091281], dtype=float32), 2.2007883]. 
=============================================
[2019-04-08 15:39:47,296] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.1183815e-16 3.0929244e-09 5.0979821e-08 6.9147070e-08 1.2548200e-10
 1.6470734e-12 8.2604883e-09 2.8066254e-14 8.4226700e-17 5.4067566e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:47,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0613
[2019-04-08 15:39:47,312] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 22.5, 25.78743261025933, 0.5064800003292772, 1.0, 1.0, 65.0, 60463.76491230216], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 113400.0000, 
sim time next is 114000.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 22.5, 25.81690206329175, 0.5004963455974968, 0.0, 1.0, 65.0, 59447.39985628428], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.375, 0.6514085052743125, 0.6668321151991656, 0.0, 1.0, 1.0, 0.28308285645849657], 
reward next is 0.7169, 
noisyNet noise sample is [array([-0.48355097], dtype=float32), 0.48322743]. 
=============================================
[2019-04-08 15:39:47,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.16221]
 [79.94709]
 [79.50625]
 [79.47256]
 [79.6695 ]], R is [[80.04943848]
 [79.96102142]
 [79.87029266]
 [79.78565979]
 [79.70411682]].
[2019-04-08 15:39:47,531] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0232096e-17 1.3219395e-12 2.1453301e-09 5.7140097e-09 3.2502813e-11
 4.9703622e-15 1.4970428e-08 2.5274219e-16 2.7071416e-19 2.3806448e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:39:47,536] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2907
[2019-04-08 15:39:47,548] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.85, 74.5, 0.0, 0.0, 19.0, 25.93647628162185, 0.5441944782252975, 0.0, 1.0, 65.0, 59158.43994758644], 
current ob forecast is [], 
actual action is [-0.8499999999999996, 65.0], 
sim time this is 106200.0000, 
sim time next is 106800.0000, 
raw observation next is [-6.133333333333334, 74.66666666666667, 0.0, 0.0, 19.0, 25.93803633103302, 0.5361532777605665, 0.0, 1.0, 65.0, 58576.29218111863], 
processed observation next is [1.0, 0.21739130434782608, 0.2927054478301016, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6615030275860851, 0.6787177592535222, 0.0, 1.0, 1.0, 0.2789347246719935], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.11453199], dtype=float32), -0.510029]. 
=============================================
[2019-04-08 15:39:47,574] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1584957e-17 1.2942891e-10 2.8186554e-08 3.0804927e-07 3.1546390e-11
 1.5547264e-14 4.7613646e-09 6.3487301e-15 7.4900398e-19 8.5633534e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:39:47,574] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0525
[2019-04-08 15:39:47,588] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.1, 87.5, 0.0, 0.0, 19.0, 26.3260926137679, 0.6436062589996637, 0.0, 1.0, 65.0, 49520.20523837488], 
current ob forecast is [], 
actual action is [9.1, 65.0], 
sim time this is 66600.0000, 
sim time next is 67200.0000, 
raw observation next is [4.0, 87.0, 0.0, 0.0, 19.0, 26.32321401357975, 0.6433959442624245, 0.0, 1.0, 65.0, 49586.81519791673], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6936011677983126, 0.7144653147541415, 0.0, 1.0, 1.0, 0.2361276914186511], 
reward next is 0.7639, 
noisyNet noise sample is [array([0.4909756], dtype=float32), -0.66342026]. 
=============================================
[2019-04-08 15:39:47,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0812346e-16 5.9466752e-09 1.5064343e-07 1.7014577e-06 1.6426024e-09
 1.5395873e-12 1.8035582e-08 7.3369431e-14 1.8913301e-17 2.4837628e-19
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:39:47,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5379
[2019-04-08 15:39:47,845] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 80.0, 190.0, 36.0, 22.5, 26.67945054926937, 0.6564250438911259, 1.0, 1.0, 65.0, 47990.98112516249], 
current ob forecast is [], 
actual action is [-2.8, 65.0], 
sim time this is 124200.0000, 
sim time next is 124800.0000, 
raw observation next is [-7.799999999999999, 82.0, 189.0, 32.16666666666666, 22.5, 26.73138747673535, 0.6419875097078197, 1.0, 1.0, 65.0, 48092.68136400264], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188372, 0.82, 0.63, 0.03554327808471454, 0.375, 0.7276156230612791, 0.7139958365692732, 1.0, 1.0, 1.0, 0.22901276840001256], 
reward next is 0.7710, 
noisyNet noise sample is [array([-0.19117622], dtype=float32), 0.814389]. 
=============================================
[2019-04-08 15:39:47,882] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.47497402e-17 3.87741783e-11 8.23582091e-09 6.49261755e-09
 2.22555654e-11 1.73673061e-14 3.07578958e-08 2.48516705e-14
 2.72109955e-18 1.22974575e-20 1.00000000e+00], sum to 1.0000
[2019-04-08 15:39:47,884] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4419
[2019-04-08 15:39:47,912] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1, 95.0, 0.0, 0.0, 19.0, 26.22349102179247, 0.6196051127928762, 0.0, 1.0, 65.0, 50503.36532122752], 
current ob forecast is [], 
actual action is [5.1, 65.0], 
sim time this is 85200.0000, 
sim time next is 85800.0000, 
raw observation next is [0.04999999999999999, 95.0, 0.0, 0.0, 19.0, 26.21818287824615, 0.6185823896003616, 0.0, 1.0, 65.0, 50716.84414858738], 
processed observation next is [0.0, 1.0, 0.4639889196675901, 0.95, 0.0, 0.0, 0.08333333333333333, 0.6848485731871792, 0.7061941298667872, 0.0, 1.0, 1.0, 0.2415087816599399], 
reward next is 0.7585, 
noisyNet noise sample is [array([0.7639816], dtype=float32), -0.49099496]. 
=============================================
[2019-04-08 15:39:48,063] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9800166e-16 3.6064158e-09 7.5639950e-06 1.0589470e-06 5.8253997e-09
 1.8090468e-12 5.0611902e-07 4.3777850e-13 2.0950544e-17 1.3676318e-18
 9.9999082e-01], sum to 1.0000
[2019-04-08 15:39:48,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7953
[2019-04-08 15:39:48,092] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 19.0, 26.32236343336047, 0.6442114412118038, 0.0, 1.0, 65.0, 49629.71181623558], 
current ob forecast is [], 
actual action is [8.8, 65.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 19.0, 26.32123219170921, 0.6459225055687497, 0.0, 1.0, 65.0, 49642.93750864605], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.08333333333333333, 0.6934360159757675, 0.7153075018562499, 0.0, 1.0, 1.0, 0.23639494051736215], 
reward next is 0.7636, 
noisyNet noise sample is [array([0.6421127], dtype=float32), 1.5584632]. 
=============================================
[2019-04-08 15:39:48,097] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.96425 ]
 [81.943954]
 [81.93561 ]
 [81.9086  ]
 [81.92959 ]], R is [[81.91487885]
 [81.85940552]
 [81.80457306]
 [81.7505722 ]
 [81.69743347]].
[2019-04-08 15:39:48,452] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.9473400e-17 9.7023806e-11 2.9423315e-08 1.2277253e-07 1.5645816e-11
 1.4882986e-13 1.3679015e-08 7.8939797e-16 1.2498848e-18 3.5944587e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:39:48,452] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3447
[2019-04-08 15:39:48,480] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.2, 86.0, 64.5, 0.0, 19.0, 26.35154369486698, 0.642672547998134, 0.0, 1.0, 65.0, 46735.13464860096], 
current ob forecast is [], 
actual action is [12.2, 65.0], 
sim time this is 54000.0000, 
sim time next is 54600.0000, 
raw observation next is [7.1, 85.33333333333334, 59.66666666666666, 0.0, 19.0, 26.34756468531122, 0.6434964076776493, 0.0, 1.0, 65.0, 46790.98383538829], 
processed observation next is [0.0, 0.6521739130434783, 0.6592797783933518, 0.8533333333333334, 0.19888888888888887, 0.0, 0.08333333333333333, 0.6956303904426017, 0.7144988025592164, 0.0, 1.0, 1.0, 0.22281420873994423], 
reward next is 0.7772, 
noisyNet noise sample is [array([0.26612243], dtype=float32), -0.3019873]. 
=============================================
[2019-04-08 15:39:48,882] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.28612737e-16 3.81814858e-10 2.31729564e-06 1.38238761e-07
 1.17271800e-11 7.47386701e-12 7.36848405e-09 1.13163864e-14
 3.84148632e-18 1.23811182e-20 9.99997497e-01], sum to 1.0000
[2019-04-08 15:39:48,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4649
[2019-04-08 15:39:48,917] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.616666666666667, 86.0, 81.66666666666666, 0.0, 19.0, 26.24124109885662, 0.622339328956886, 0.0, 1.0, 65.0, 47309.56647027405], 
current ob forecast is [], 
actual action is [12.616666666666667, 65.0], 
sim time this is 51000.0000, 
sim time next is 51600.0000, 
raw observation next is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 19.0, 26.27412333555328, 0.6249070231164129, 0.0, 1.0, 65.0, 46647.50490656387], 
processed observation next is [0.0, 0.6086956521739131, 0.6712834718374886, 0.86, 0.26777777777777784, 0.0, 0.08333333333333333, 0.6895102779627734, 0.7083023410388044, 0.0, 1.0, 1.0, 0.22213097574554222], 
reward next is 0.7779, 
noisyNet noise sample is [array([1.1681793], dtype=float32), -0.026416466]. 
=============================================
[2019-04-08 15:39:49,193] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6994905e-16 7.2637087e-09 2.1726757e-07 1.3156730e-06 7.4895928e-10
 1.0633990e-11 9.5202928e-08 5.1909995e-13 2.1994372e-17 4.0403831e-19
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:39:49,194] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2582
[2019-04-08 15:39:49,218] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.15, 91.0, 0.0, 0.0, 19.0, 26.31347555533672, 0.6121898119848621, 0.0, 1.0, 65.0, 50219.75126077438], 
current ob forecast is [], 
actual action is [3.85, 65.0], 
sim time this is 91800.0000, 
sim time next is 92400.0000, 
raw observation next is [-1.333333333333333, 91.0, 0.0, 0.0, 19.0, 26.26660450262563, 0.6102784321273015, 0.0, 1.0, 65.0, 51728.15376268823], 
processed observation next is [1.0, 0.043478260869565216, 0.42566943674976926, 0.91, 0.0, 0.0, 0.08333333333333333, 0.6888837085521358, 0.7034261440424339, 0.0, 1.0, 1.0, 0.2463245417270868], 
reward next is 0.7537, 
noisyNet noise sample is [array([1.0018598], dtype=float32), 0.34990326]. 
=============================================
[2019-04-08 15:39:50,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.5436294e-16 1.3647491e-10 1.2140114e-06 7.9362371e-06 7.1780903e-10
 3.4571280e-11 6.9899215e-07 7.9953183e-13 8.5650536e-17 2.9931784e-19
 9.9999011e-01], sum to 1.0000
[2019-04-08 15:39:50,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-08 15:39:50,101] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 75.0, 0.0, 0.0, 19.0, 25.88698786821847, 0.5333826131327268, 0.0, 1.0, 65.0, 59700.9154380612], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 108000.0000, 
sim time next is 108600.0000, 
raw observation next is [-6.800000000000001, 73.83333333333333, 0.0, 0.0, 19.0, 25.90197587570842, 0.5332529836854912, 0.0, 1.0, 65.0, 59081.32849958439], 
processed observation next is [1.0, 0.2608695652173913, 0.2742382271468144, 0.7383333333333333, 0.0, 0.0, 0.08333333333333333, 0.6584979896423683, 0.6777509945618304, 0.0, 1.0, 1.0, 0.28133965952183043], 
reward next is 0.7187, 
noisyNet noise sample is [array([0.53009194], dtype=float32), -0.35641336]. 
=============================================
[2019-04-08 15:39:50,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5273056e-15 3.1660747e-09 3.2594073e-07 6.4981737e-07 2.4734712e-09
 8.7132506e-12 9.6608183e-08 9.3026465e-13 9.5273309e-15 2.7877760e-16
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:39:50,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9327
[2019-04-08 15:39:50,356] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 19.0, 25.65449748277029, 0.4984424825711237, 0.0, 1.0, 65.0, 61164.9990232312], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 162000.0000, 
sim time next is 162600.0000, 
raw observation next is [-8.4, 68.5, 0.0, 0.0, 19.0, 25.65091265068065, 0.495438762264242, 0.0, 1.0, 65.0, 61147.35894515166], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.685, 0.0, 0.0, 0.08333333333333333, 0.6375760542233874, 0.6651462540880807, 0.0, 1.0, 1.0, 0.2911778997388174], 
reward next is 0.7088, 
noisyNet noise sample is [array([0.61691874], dtype=float32), -1.9751494]. 
=============================================
[2019-04-08 15:39:50,455] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2206813e-15 9.1507951e-10 4.6604990e-07 3.5300698e-06 4.0395284e-10
 3.6358650e-12 1.7269305e-07 3.8755292e-12 3.9949052e-17 5.4104540e-19
 9.9999583e-01], sum to 1.0000
[2019-04-08 15:39:50,459] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0365
[2019-04-08 15:39:50,478] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.9, 85.66666666666667, 0.0, 0.0, 19.0, 26.2266494389709, 0.6029429131624435, 0.0, 1.0, 65.0, 53378.27186488732], 
current ob forecast is [], 
actual action is [2.1, 65.0], 
sim time this is 97800.0000, 
sim time next is 98400.0000, 
raw observation next is [-3.0, 84.33333333333334, 0.0, 0.0, 19.0, 26.26422851994069, 0.5992664124865117, 0.0, 1.0, 65.0, 53537.30423038373], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.8433333333333334, 0.0, 0.0, 0.08333333333333333, 0.6886857099950575, 0.6997554708288373, 0.0, 1.0, 1.0, 0.2549395439542082], 
reward next is 0.7451, 
noisyNet noise sample is [array([0.10351957], dtype=float32), -0.8434944]. 
=============================================
[2019-04-08 15:39:50,714] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6621839e-15 5.2015481e-10 7.2425678e-08 4.6817556e-07 1.4255029e-10
 8.7457271e-11 1.5179212e-09 1.5153718e-14 4.4381962e-17 4.9460448e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:39:50,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3786
[2019-04-08 15:39:50,735] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7833333333333333, 91.00000000000001, 0.0, 0.0, 19.0, 26.28015305644463, 0.617609384080436, 0.0, 1.0, 65.0, 50399.60496503112], 
current ob forecast is [], 
actual action is [4.216666666666667, 65.0], 
sim time this is 90600.0000, 
sim time next is 91200.0000, 
raw observation next is [-0.9666666666666667, 91.0, 0.0, 0.0, 19.0, 26.27943915522697, 0.6215704902494206, 0.0, 1.0, 65.0, 50962.03962327822], 
processed observation next is [1.0, 0.043478260869565216, 0.43582640812557716, 0.91, 0.0, 0.0, 0.08333333333333333, 0.6899532629355809, 0.7071901634164736, 0.0, 1.0, 1.0, 0.24267637915846774], 
reward next is 0.7573, 
noisyNet noise sample is [array([-0.1740564], dtype=float32), -0.3304265]. 
=============================================
[2019-04-08 15:39:50,767] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0186197e-16 1.8907811e-09 5.1727284e-06 1.2281105e-06 2.6153439e-09
 1.4123221e-11 6.0794569e-08 2.0269734e-12 4.4767714e-16 2.1862340e-18
 9.9999356e-01], sum to 1.0000
[2019-04-08 15:39:50,771] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6553
[2019-04-08 15:39:50,790] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 25.50970510814707, 0.441272635003771, 0.0, 1.0, 65.0, 60065.36740306004], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 172200.0000, 
sim time next is 172800.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 25.49513286019958, 0.4373722470951025, 0.0, 1.0, 65.0, 60040.80739737392], 
processed observation next is [1.0, 0.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6245944050166315, 0.6457907490317009, 0.0, 1.0, 1.0, 0.2859086066541615], 
reward next is 0.7141, 
noisyNet noise sample is [array([-1.5420827], dtype=float32), -0.6856583]. 
=============================================
[2019-04-08 15:39:52,138] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7676644e-15 4.8459077e-09 1.3309661e-07 9.5031135e-07 1.8802970e-09
 5.7250897e-12 2.2553333e-08 9.2496167e-14 5.3213776e-16 3.6768364e-18
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:39:52,139] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2374
[2019-04-08 15:39:52,151] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.700000000000001, 61.0, 146.5, 169.0, 22.5, 27.10412176250711, 0.786084252826634, 1.0, 1.0, 65.0, 41466.42546194365], 
current ob forecast is [], 
actual action is [-1.700000000000001, 65.0], 
sim time this is 138000.0000, 
sim time next is 138600.0000, 
raw observation next is [-6.7, 61.0, 148.0, 106.0, 22.5, 27.12771273077628, 0.7897096843521366, 1.0, 1.0, 65.0, 41012.10191319879], 
processed observation next is [1.0, 0.6086956521739131, 0.2770083102493075, 0.61, 0.49333333333333335, 0.11712707182320442, 0.375, 0.7606427275646901, 0.7632365614507122, 1.0, 1.0, 1.0, 0.19529572339618473], 
reward next is 0.8047, 
noisyNet noise sample is [array([-1.9484714], dtype=float32), -0.14794053]. 
=============================================
[2019-04-08 15:39:52,231] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9116117e-14 3.1044642e-09 8.3396429e-07 1.5385276e-06 2.5839850e-10
 6.1500881e-11 6.4075823e-07 9.0485128e-13 2.2991838e-16 6.5428948e-17
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:39:52,231] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2172
[2019-04-08 15:39:52,272] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 69.0, 0.0, 0.0, 19.0, 25.64246000818538, 0.4916880293432938, 0.0, 1.0, 65.0, 61096.61244685017], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 163200.0000, 
sim time next is 163800.0000, 
raw observation next is [-8.4, 69.5, 0.0, 0.0, 19.0, 25.63543784872481, 0.488411165852225, 0.0, 1.0, 65.0, 61009.90203239126], 
processed observation next is [1.0, 0.9130434782608695, 0.2299168975069252, 0.695, 0.0, 0.0, 0.08333333333333333, 0.6362864873937341, 0.6628037219507417, 0.0, 1.0, 1.0, 0.290523343011387], 
reward next is 0.7095, 
noisyNet noise sample is [array([-1.1631099], dtype=float32), -0.75898683]. 
=============================================
[2019-04-08 15:39:52,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7061640e-14 2.4236964e-09 6.9635672e-07 5.2123181e-07 5.1850946e-09
 2.9187453e-12 7.3399235e-08 6.7058480e-14 2.6021821e-16 1.5887036e-18
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:39:52,570] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2798
[2019-04-08 15:39:52,606] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 19.0, 25.87868158922886, 0.5114980030578503, 0.0, 1.0, 65.0, 59559.31020775628], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 111600.0000, 
sim time next is 112200.0000, 
raw observation next is [-7.3, 68.0, 0.0, 0.0, 22.5, 25.84102905570285, 0.5039637202021127, 0.0, 1.0, 65.0, 60025.68661366821], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.68, 0.0, 0.0, 0.375, 0.6534190879752376, 0.6679879067340376, 0.0, 1.0, 1.0, 0.28583660292222957], 
reward next is 0.7142, 
noisyNet noise sample is [array([-0.04939856], dtype=float32), 0.42760304]. 
=============================================
[2019-04-08 15:39:54,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0994952e-14 2.1016833e-08 8.3468620e-07 3.3340151e-05 7.9234255e-09
 2.2116864e-10 1.9146469e-06 1.5753262e-11 9.3726726e-16 3.1287501e-17
 9.9996388e-01], sum to 1.0000
[2019-04-08 15:39:54,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1879
[2019-04-08 15:39:54,203] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 19.0, 25.35262060674039, 0.3686466573998106, 0.0, 1.0, 65.0, 59093.68913737071], 
current ob forecast is [], 
actual action is [-3.900000000000002, 65.0], 
sim time this is 188400.0000, 
sim time next is 189000.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 19.0, 25.31058141872113, 0.364152842393199, 0.0, 1.0, 65.0, 60344.87197991949], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.08333333333333333, 0.609215118226761, 0.621384280797733, 0.0, 1.0, 1.0, 0.2873565332377118], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.69778585], dtype=float32), -0.72121626]. 
=============================================
[2019-04-08 15:39:54,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.76345 ]
 [70.74446 ]
 [70.730385]
 [70.7089  ]
 [70.700935]], R is [[70.75112915]
 [70.76222229]
 [70.77488708]
 [70.78231812]
 [70.78827667]].
[2019-04-08 15:39:55,678] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0333562e-16 2.3914168e-10 7.3849645e-07 1.1592061e-06 7.6547921e-11
 7.9499450e-13 2.8416848e-08 2.4350134e-14 9.6576162e-17 4.2621580e-19
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:39:55,678] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6698
[2019-04-08 15:39:55,724] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.216666666666667, 77.5, 64.0, 98.66666666666664, 22.5, 25.89285344717691, 0.4598150895758126, 1.0, 1.0, 65.0, 55412.95624170662], 
current ob forecast is [], 
actual action is [-3.216666666666667, 65.0], 
sim time this is 205800.0000, 
sim time next is 206400.0000, 
raw observation next is [-8.033333333333333, 77.0, 71.50000000000001, 49.33333333333332, 22.5, 26.0198693545542, 0.47180333968254, 1.0, 1.0, 65.0, 52603.06220747423], 
processed observation next is [1.0, 0.391304347826087, 0.24007386888273316, 0.77, 0.23833333333333337, 0.05451197053406997, 0.375, 0.66832244621285, 0.6572677798941801, 1.0, 1.0, 1.0, 0.25049077241654394], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.89129156], dtype=float32), -1.1497444]. 
=============================================
[2019-04-08 15:39:56,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9424973e-15 1.1897514e-08 9.9941212e-07 2.3263015e-06 2.9582889e-10
 6.1039680e-12 4.6892964e-08 8.1162930e-13 5.9501259e-17 2.7824849e-19
 9.9999666e-01], sum to 1.0000
[2019-04-08 15:39:56,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2108
[2019-04-08 15:39:56,646] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.23333333333333, 68.0, 0.0, 0.0, 19.0, 25.42955221567651, 0.39151090563709, 0.0, 1.0, 65.0, 63303.13795448759], 
current ob forecast is [], 
actual action is [-5.233333333333331, 65.0], 
sim time this is 276000.0000, 
sim time next is 276600.0000, 
raw observation next is [-10.41666666666667, 67.5, 0.0, 0.0, 19.0, 25.38251179870655, 0.3851050954832052, 0.0, 1.0, 65.0, 63728.8492517779], 
processed observation next is [1.0, 0.17391304347826086, 0.17405355493998145, 0.675, 0.0, 0.0, 0.08333333333333333, 0.6152093165588791, 0.6283683651610684, 0.0, 1.0, 1.0, 0.3034707107227519], 
reward next is 0.6965, 
noisyNet noise sample is [array([-0.14926973], dtype=float32), -2.2865222]. 
=============================================
[2019-04-08 15:39:56,790] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2124850e-16 1.9371267e-09 3.8609068e-07 2.2634200e-07 7.7742937e-11
 3.8853643e-12 6.4882826e-08 1.3383320e-13 2.5602553e-18 3.4942119e-18
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:39:56,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1638
[2019-04-08 15:39:56,828] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.116666666666666, 74.5, 109.0, 0.0, 22.5, 26.33682948996329, 0.5209728207617286, 1.0, 1.0, 65.0, 49780.63364452256], 
current ob forecast is [], 
actual action is [-2.1166666666666663, 65.0], 
sim time this is 209400.0000, 
sim time next is 210000.0000, 
raw observation next is [-6.933333333333334, 74.0, 116.5, 0.0, 22.5, 26.39599701985139, 0.5229836579754604, 1.0, 1.0, 65.0, 48402.52391560928], 
processed observation next is [1.0, 0.43478260869565216, 0.270544783010157, 0.74, 0.3883333333333333, 0.0, 0.375, 0.6996664183209491, 0.6743278859918201, 1.0, 1.0, 1.0, 0.23048820912194895], 
reward next is 0.7695, 
noisyNet noise sample is [array([-0.27475178], dtype=float32), -0.8673278]. 
=============================================
[2019-04-08 15:39:56,859] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.36949 ]
 [73.21445 ]
 [73.521034]
 [73.46729 ]
 [73.74387 ]], R is [[73.29037476]
 [73.32041931]
 [73.34548187]
 [73.36951447]
 [73.38938904]].
[2019-04-08 15:39:57,477] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.9293678e-14 2.9862435e-09 4.3557918e-07 3.4160696e-06 3.6692055e-10
 5.0389254e-12 8.7321235e-09 4.6618861e-13 1.4155449e-16 2.3595463e-18
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:39:57,477] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5160
[2019-04-08 15:39:57,497] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 36.0, 94.0, 0.0, 22.5, 26.80256677359106, 0.5979329992869488, 1.0, 1.0, 65.0, 39939.60362785783], 
current ob forecast is [], 
actual action is [4.7, 65.0], 
sim time this is 484200.0000, 
sim time next is 484800.0000, 
raw observation next is [-0.2, 36.33333333333333, 87.66666666666667, 0.0, 22.5, 26.84644792936524, 0.6007679545980634, 1.0, 1.0, 65.0, 39568.50602738583], 
processed observation next is [1.0, 0.6086956521739131, 0.4570637119113574, 0.3633333333333333, 0.2922222222222222, 0.0, 0.375, 0.73720399411377, 0.7002559848660211, 1.0, 1.0, 1.0, 0.18842145727326587], 
reward next is 0.8116, 
noisyNet noise sample is [array([-0.00562681], dtype=float32), -0.8940588]. 
=============================================
[2019-04-08 15:39:57,578] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.70009017e-17 1.19165977e-09 1.63523552e-07 2.44292920e-07
 3.66675496e-10 3.04919423e-11 1.56624669e-08 3.08083976e-14
 9.19286429e-18 1.02096615e-19 9.99999523e-01], sum to 1.0000
[2019-04-08 15:39:57,581] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3593
[2019-04-08 15:39:57,609] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 59.0, 86.5, 0.0, 22.5, 26.9064673462555, 0.6540669317513882, 1.0, 1.0, 65.0, 42917.4291609913], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 226800.0000, 
sim time next is 227400.0000, 
raw observation next is [-2.9, 59.5, 76.33333333333333, 0.0, 22.5, 26.85809952567199, 0.6588263748543952, 1.0, 1.0, 65.0, 44631.40157981591], 
processed observation next is [1.0, 0.6521739130434783, 0.38227146814404434, 0.595, 0.2544444444444444, 0.0, 0.375, 0.7381749604726657, 0.7196087916181316, 1.0, 1.0, 1.0, 0.2125304837134091], 
reward next is 0.7875, 
noisyNet noise sample is [array([-1.9913961], dtype=float32), 0.38713422]. 
=============================================
[2019-04-08 15:39:57,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.11209395e-14 9.10115716e-08 3.11110284e-06 1.08302511e-05
 7.79997733e-10 2.02841303e-11 1.79554306e-07 8.65619913e-13
 1.66224922e-16 1.78148946e-17 9.99985814e-01], sum to 1.0000
[2019-04-08 15:39:57,839] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.3528679e-16 8.8755718e-11 7.9930334e-07 3.7035571e-07 7.6350180e-09
 6.5510145e-12 2.3446253e-08 6.0831934e-14 1.6815187e-16 7.5682125e-19
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:39:57,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7963
[2019-04-08 15:39:57,852] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7235
[2019-04-08 15:39:57,853] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 64.0, 15.0, 0.0, 22.5, 26.89548960698794, 0.6562772467949304, 1.0, 1.0, 65.0, 43909.53106757259], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 232800.0000, 
sim time next is 233400.0000, 
raw observation next is [-3.4, 64.5, 12.0, 0.0, 22.5, 26.89801759298033, 0.6491413657210492, 1.0, 1.0, 65.0, 43370.18073970402], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.645, 0.04, 0.0, 0.375, 0.7415014660816942, 0.7163804552403498, 1.0, 1.0, 1.0, 0.20652467018906676], 
reward next is 0.7935, 
noisyNet noise sample is [array([-0.09899151], dtype=float32), 0.078072]. 
=============================================
[2019-04-08 15:39:57,889] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 64.5, 12.0, 0.0, 22.5, 26.89913185699757, 0.6494439104558697, 1.0, 1.0, 65.0, 43357.20572050192], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 233400.0000, 
sim time next is 234000.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 22.5, 26.85915342874767, 0.654285681167691, 1.0, 1.0, 65.0, 45082.52209079753], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.375, 0.7382627857289726, 0.718095227055897, 1.0, 1.0, 1.0, 0.21467867662284537], 
reward next is 0.7853, 
noisyNet noise sample is [array([-0.1901918], dtype=float32), 0.4171839]. 
=============================================
[2019-04-08 15:39:57,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[72.8915  ]
 [72.91396 ]
 [72.868225]
 [72.956184]
 [72.99839 ]], R is [[72.95317078]
 [73.0171814 ]
 [73.07798004]
 [73.1461792 ]
 [73.22382355]].
[2019-04-08 15:39:58,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0435738e-16 1.6713434e-08 3.2282375e-07 5.2656078e-06 1.7361521e-09
 3.1833036e-10 1.6690633e-07 3.7172479e-13 2.2760771e-17 4.2304759e-18
 9.9999428e-01], sum to 1.0000
[2019-04-08 15:39:58,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7824
[2019-04-08 15:39:58,924] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 60.5, 117.0, 0.0, 22.5, 25.62506819279336, 0.5542827945977123, 1.0, 1.0, 65.0, 47883.18822997412], 
current ob forecast is [], 
actual action is [1.9, 65.0], 
sim time this is 225000.0000, 
sim time next is 225600.0000, 
raw observation next is [-3.0, 60.0, 106.8333333333333, 0.0, 22.5, 26.73112847691124, 0.6415483200510225, 1.0, 1.0, 65.0, 40497.03273560024], 
processed observation next is [1.0, 0.6086956521739131, 0.3795013850415513, 0.6, 0.356111111111111, 0.0, 0.375, 0.7275940397426034, 0.7138494400170075, 1.0, 1.0, 1.0, 0.1928430130266678], 
reward next is 0.8072, 
noisyNet noise sample is [array([0.10878894], dtype=float32), -0.025989559]. 
=============================================
[2019-04-08 15:39:59,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5153065e-14 1.1971127e-08 6.3588789e-07 2.7617918e-06 1.3097196e-09
 2.5426890e-11 1.0716680e-07 1.5763263e-12 1.7664929e-16 4.6665642e-18
 9.9999642e-01], sum to 1.0000
[2019-04-08 15:39:59,378] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4890
[2019-04-08 15:39:59,394] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.05, 68.5, 0.0, 0.0, 19.0, 25.4382361761953, 0.3994047696699963, 0.0, 1.0, 65.0, 63226.06333279355], 
current ob forecast is [], 
actual action is [-5.050000000000001, 65.0], 
sim time this is 275400.0000, 
sim time next is 276000.0000, 
raw observation next is [-10.23333333333333, 68.0, 0.0, 0.0, 19.0, 25.42632559422123, 0.3907622063675902, 0.0, 1.0, 65.0, 63314.64005055759], 
processed observation next is [1.0, 0.17391304347826086, 0.1791320406278856, 0.68, 0.0, 0.0, 0.08333333333333333, 0.6188604661851024, 0.6302540687891968, 0.0, 1.0, 1.0, 0.30149828595503614], 
reward next is 0.6985, 
noisyNet noise sample is [array([1.0267177], dtype=float32), -0.4582993]. 
=============================================
[2019-04-08 15:39:59,424] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.052956]
 [74.170235]
 [74.22965 ]
 [74.31808 ]
 [74.39956 ]], R is [[73.91780853]
 [73.87755585]
 [73.84021759]
 [73.80175018]
 [73.76913452]].
[2019-04-08 15:39:59,585] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2367179e-17 2.2961077e-09 3.9058655e-06 3.4688685e-07 2.5385644e-10
 1.0228746e-11 1.4147557e-07 1.6399404e-13 2.0824840e-16 2.9590132e-18
 9.9999559e-01], sum to 1.0000
[2019-04-08 15:39:59,586] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6822
[2019-04-08 15:39:59,597] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 19.0, 25.86622159741043, 0.5020062668848265, 0.0, 1.0, 65.0, 57705.59535058196], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 255600.0000, 
sim time next is 256200.0000, 
raw observation next is [-4.0, 81.50000000000001, 0.0, 0.0, 19.0, 25.85870091163766, 0.4987276634272176, 0.0, 1.0, 65.0, 58192.90307345205], 
processed observation next is [1.0, 1.0, 0.3518005540166205, 0.8150000000000002, 0.0, 0.0, 0.08333333333333333, 0.6548917426364715, 0.6662425544757392, 0.0, 1.0, 1.0, 0.2771090622545336], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.8315941], dtype=float32), -0.74190754]. 
=============================================
[2019-04-08 15:39:59,689] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2070360e-17 2.2864628e-09 3.8976232e-06 3.4992706e-07 2.4202740e-10
 9.8211595e-12 1.3903151e-07 1.5717279e-13 1.9831747e-16 2.8524151e-18
 9.9999559e-01], sum to 1.0000
[2019-04-08 15:39:59,694] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1513
[2019-04-08 15:39:59,695] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.45193035e-14 2.15111093e-08 1.28007855e-06 5.07608956e-06
 2.51276722e-10 1.89143909e-11 1.78822901e-07 2.78247396e-13
 5.54987951e-16 1.20562765e-17 9.99993443e-01], sum to 1.0000
[2019-04-08 15:39:59,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9328
[2019-04-08 15:39:59,710] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.2, 80.5, 0.0, 0.0, 19.0, 25.81984288897552, 0.4941161880021317, 0.0, 1.0, 65.0, 59048.24488012466], 
current ob forecast is [], 
actual action is [0.7999999999999998, 65.0], 
sim time this is 257400.0000, 
sim time next is 258000.0000, 
raw observation next is [-4.3, 80.0, 0.0, 0.0, 19.0, 25.80387917800405, 0.4914348170517475, 0.0, 1.0, 65.0, 58913.21136517307], 
processed observation next is [1.0, 1.0, 0.34349030470914127, 0.8, 0.0, 0.0, 0.08333333333333333, 0.6503232648336708, 0.6638116056839158, 0.0, 1.0, 1.0, 0.2805391017389194], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.8315941], dtype=float32), -0.74190754]. 
=============================================
[2019-04-08 15:39:59,713] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.18569]
 [75.11113]
 [75.07618]
 [75.08389]
 [74.88848]], R is [[75.24495697]
 [75.2113266 ]
 [75.17816925]
 [75.14928436]
 [75.1230011 ]].
[2019-04-08 15:39:59,739] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.8, 63.66666666666666, 92.33333333333334, 426.0, 22.5, 26.22977721811892, 0.5295442268297518, 1.0, 1.0, 65.0, 50994.22002296748], 
current ob forecast is [], 
actual action is [-6.800000000000001, 65.0], 
sim time this is 294600.0000, 
sim time next is 295200.0000, 
raw observation next is [-11.7, 63.0, 91.0, 447.5, 22.5, 26.28005916490792, 0.5407696672014478, 1.0, 1.0, 65.0, 50173.26278676488], 
processed observation next is [1.0, 0.43478260869565216, 0.13850415512465375, 0.63, 0.30333333333333334, 0.494475138121547, 0.375, 0.6900049304089935, 0.6802565557338159, 1.0, 1.0, 1.0, 0.23892029898459466], 
reward next is 0.7611, 
noisyNet noise sample is [array([0.0371078], dtype=float32), 1.2669389]. 
=============================================
[2019-04-08 15:40:00,312] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.0262672e-16 1.6521698e-10 1.0570701e-06 4.3317350e-06 4.1710360e-10
 1.3517237e-11 3.5211617e-08 3.4801450e-12 3.0601811e-17 5.9214184e-19
 9.9999452e-01], sum to 1.0000
[2019-04-08 15:40:00,314] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4559
[2019-04-08 15:40:00,329] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 19.0, 25.8369757454824, 0.4797565903735418, 0.0, 1.0, 65.0, 59004.20885627565], 
current ob forecast is [], 
actual action is [-2.1999999999999993, 65.0], 
sim time this is 265800.0000, 
sim time next is 266400.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 25.86165990315022, 0.4728611786716917, 0.0, 1.0, 65.0, 58994.01201562364], 
processed observation next is [1.0, 0.08695652173913043, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6551383252625183, 0.6576203928905638, 0.0, 1.0, 1.0, 0.28092386674106495], 
reward next is 0.7191, 
noisyNet noise sample is [array([-0.19035587], dtype=float32), -0.9984616]. 
=============================================
[2019-04-08 15:40:00,661] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1391621e-16 5.7903743e-10 7.4093577e-08 2.5869376e-06 1.3795196e-10
 1.6966195e-12 1.4170274e-08 9.8729999e-15 1.1745595e-16 2.1906826e-19
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:40:00,661] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4753
[2019-04-08 15:40:00,684] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.100000000000001, 68.0, 0.0, 0.0, 19.0, 25.59719916183773, 0.4356883369009095, 0.0, 1.0, 65.0, 62412.28171508235], 
current ob forecast is [], 
actual action is [-4.100000000000001, 65.0], 
sim time this is 271200.0000, 
sim time next is 271800.0000, 
raw observation next is [-9.2, 68.5, 0.0, 0.0, 19.0, 25.59399353955557, 0.4292989538422494, 0.0, 1.0, 65.0, 61393.87792631819], 
processed observation next is [1.0, 0.13043478260869565, 0.20775623268698065, 0.685, 0.0, 0.0, 0.08333333333333333, 0.6328327949629641, 0.6430996512807498, 0.0, 1.0, 1.0, 0.29235179964913427], 
reward next is 0.7076, 
noisyNet noise sample is [array([0.4349312], dtype=float32), 1.0461937]. 
=============================================
[2019-04-08 15:40:01,587] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1945817e-15 3.9108200e-10 1.8780371e-07 2.9779684e-08 2.0853319e-11
 1.2741805e-12 2.1729141e-08 1.7632758e-14 9.0601887e-18 4.1241534e-18
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:40:01,597] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8536
[2019-04-08 15:40:01,647] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.1, 68.0, 0.0, 0.0, 19.0, 25.12758092053336, 0.3323284030659804, 0.0, 1.0, 65.0, 64396.46008636431], 
current ob forecast is [], 
actual action is [-7.1, 65.0], 
sim time this is 283200.0000, 
sim time next is 283800.0000, 
raw observation next is [-12.2, 67.5, 0.0, 0.0, 19.0, 25.10650743514807, 0.3384565538109487, 0.0, 1.0, 65.0, 64397.79846027889], 
processed observation next is [1.0, 0.2608695652173913, 0.12465373961218838, 0.675, 0.0, 0.0, 0.08333333333333333, 0.5922089529290059, 0.6128188512703162, 0.0, 1.0, 1.0, 0.3066561831441852], 
reward next is 0.6933, 
noisyNet noise sample is [array([2.0014114], dtype=float32), -1.1040646]. 
=============================================
[2019-04-08 15:40:01,678] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3986201e-13 2.5277513e-09 2.9205621e-06 3.4817713e-06 5.0482782e-09
 2.1691376e-10 2.3266105e-06 2.9294431e-12 4.0050229e-15 6.7496224e-17
 9.9999130e-01], sum to 1.0000
[2019-04-08 15:40:01,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-08 15:40:01,707] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.633333333333333, 67.66666666666667, 0.0, 0.0, 19.0, 25.72693092223173, 0.4564734811928462, 0.0, 1.0, 65.0, 60782.43556785113], 
current ob forecast is [], 
actual action is [-3.633333333333333, 65.0], 
sim time this is 269400.0000, 
sim time next is 270000.0000, 
raw observation next is [-8.9, 67.0, 0.0, 0.0, 19.0, 25.73978903827215, 0.4444960290764692, 0.0, 1.0, 65.0, 60471.39349302132], 
processed observation next is [1.0, 0.13043478260869565, 0.21606648199445982, 0.67, 0.0, 0.0, 0.08333333333333333, 0.6449824198560125, 0.6481653430254898, 0.0, 1.0, 1.0, 0.28795901663343487], 
reward next is 0.7120, 
noisyNet noise sample is [array([-0.54645485], dtype=float32), 0.65426093]. 
=============================================
[2019-04-08 15:40:01,715] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.480125]
 [73.5747  ]
 [73.73394 ]
 [73.64709 ]
 [73.874245]], R is [[73.23704529]
 [73.21524048]
 [73.19998932]
 [73.18122101]
 [73.1607132 ]].
[2019-04-08 15:40:02,133] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-08 15:40:02,133] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:40:02,134] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:40:02,137] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run15
[2019-04-08 15:40:02,154] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:40:02,155] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:40:02,156] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:40:02,157] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:40:02,176] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run15
[2019-04-08 15:40:02,200] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run15
[2019-04-08 15:40:14,197] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06160934]
[2019-04-08 15:40:14,197] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [7.8, 89.66666666666667, 14.0, 12.66666666666667, 22.5, 27.69180963617859, 0.9260696381735158, 1.0, 1.0, 65.0, 28035.49141689979]
[2019-04-08 15:40:14,197] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:40:14,198] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [1.6569852e-17 2.2902684e-10 9.0534122e-08 1.6595699e-07 5.1385556e-11
 2.4317327e-13 8.2420106e-09 1.0698847e-14 1.5145313e-18 1.0325004e-20
 9.9999976e-01], sampled 0.2590810026183473
[2019-04-08 15:41:06,669] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06160934]
[2019-04-08 15:41:06,670] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [5.447101638166666, 25.11636599666667, 108.7562085, 99.72554356666663, 22.5, 27.29501400443608, 0.8091163027536102, 1.0, 1.0, 65.0, 53495.78345204823]
[2019-04-08 15:41:06,670] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:41:06,671] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [2.2298018e-16 1.6146906e-09 4.1625174e-07 5.0334006e-07 2.7915517e-10
 2.1317950e-12 2.5816872e-08 1.2983932e-13 4.6778430e-17 4.7875823e-19
 9.9999905e-01], sampled 0.2275248737439054
[2019-04-08 15:41:38,598] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06160934]
[2019-04-08 15:41:38,599] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [2.0, 80.0, 39.99999999999999, 77.33333333333331, 22.5, 27.58427836108432, 0.9874810717859367, 1.0, 1.0, 65.0, 33814.65511949424]
[2019-04-08 15:41:38,599] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:41:38,600] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [4.0897973e-17 2.6447039e-10 1.6111383e-07 3.0495087e-07 1.3906977e-10
 4.1404071e-13 2.2578828e-08 2.8977936e-14 3.7072601e-18 4.5395233e-20
 9.9999952e-01], sampled 0.3285981030341484
[2019-04-08 15:41:40,889] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6989.9711 316266059.7372 2958.1803
[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:40,913] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:41,053] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,735] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6800.9589 355958625.6724 2370.5741
[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:46,869] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,833] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3494 342856628.8051 2768.2389
[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,854] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:49,967] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:41:50,856] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 280000, evaluation results [280000.0, 6863.34938664247, 342856628.80507565, 2768.238927169274, 6989.971144108372, 316266059.73724324, 2958.180307199658, 6800.958925369401, 355958625.67242324, 2370.57406521214]
[2019-04-08 15:41:50,988] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7217556e-14 2.1559240e-08 8.3687610e-07 3.9373526e-06 5.8095084e-10
 9.3482366e-12 2.2358429e-06 1.5224149e-12 1.9309580e-16 1.5670330e-17
 9.9999297e-01], sum to 1.0000
[2019-04-08 15:41:50,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1413
[2019-04-08 15:41:51,009] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.2, 66.33333333333334, 73.33333333333334, 384.0, 22.5, 25.88429941072044, 0.469915425438179, 1.0, 1.0, 65.0, 57479.63610685148], 
current ob forecast is [], 
actual action is [-7.199999999999999, 65.0], 
sim time this is 292200.0000, 
sim time next is 292800.0000, 
raw observation next is [-12.1, 65.66666666666667, 84.16666666666667, 383.5, 22.5, 25.92812201123105, 0.4908311858467935, 1.0, 1.0, 65.0, 55587.39276422385], 
processed observation next is [1.0, 0.391304347826087, 0.12742382271468145, 0.6566666666666667, 0.28055555555555556, 0.42375690607734806, 0.375, 0.6606768342692542, 0.6636103952822645, 1.0, 1.0, 1.0, 0.26470187030582787], 
reward next is 0.7353, 
noisyNet noise sample is [array([-0.5388185], dtype=float32), 0.035019487]. 
=============================================
[2019-04-08 15:41:51,235] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1059779e-15 1.7244807e-09 2.7897866e-07 2.3068458e-06 2.3232649e-10
 3.2325101e-11 1.5864346e-07 4.0217921e-13 7.0966167e-16 1.5956313e-18
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:41:51,235] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7322
[2019-04-08 15:41:51,270] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.15, 53.0, 0.0, 0.0, 22.5, 25.92324718998099, 0.548990909888643, 1.0, 1.0, 65.0, 70011.80427350065], 
current ob forecast is [], 
actual action is [-6.15, 65.0], 
sim time this is 322200.0000, 
sim time next is 322800.0000, 
raw observation next is [-11.33333333333333, 54.33333333333334, 0.0, 0.0, 22.5, 25.22997284069122, 0.4891584773808453, 1.0, 1.0, 65.0, 65491.76836487222], 
processed observation next is [1.0, 0.7391304347826086, 0.14866112650046176, 0.5433333333333334, 0.0, 0.0, 0.375, 0.6024977367242684, 0.6630528257936151, 1.0, 1.0, 1.0, 0.31186556364224866], 
reward next is 0.6881, 
noisyNet noise sample is [array([0.5269145], dtype=float32), 1.7650899]. 
=============================================
[2019-04-08 15:41:52,016] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3231108e-15 6.3140060e-10 5.3056289e-07 2.4118233e-05 1.8101260e-09
 1.0832805e-12 8.0904066e-08 8.8799729e-14 3.8258157e-16 2.5358553e-18
 9.9997532e-01], sum to 1.0000
[2019-04-08 15:41:52,017] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0799
[2019-04-08 15:41:52,035] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.6, 60.0, 96.5, 585.0, 22.5, 26.55643420169995, 0.6084370370679146, 1.0, 1.0, 65.0, 45959.05248653361], 
current ob forecast is [], 
actual action is [-5.6, 65.0], 
sim time this is 298800.0000, 
sim time next is 299400.0000, 
raw observation next is [-10.6, 58.16666666666666, 99.66666666666666, 609.3333333333334, 22.5, 26.61031701009311, 0.6208017725427458, 1.0, 1.0, 65.0, 45311.64228187758], 
processed observation next is [1.0, 0.4782608695652174, 0.1689750692520776, 0.5816666666666666, 0.3322222222222222, 0.6732965009208104, 0.375, 0.717526417507759, 0.7069339241809153, 1.0, 1.0, 1.0, 0.21576972515179801], 
reward next is 0.7842, 
noisyNet noise sample is [array([-0.4693114], dtype=float32), -0.06606834]. 
=============================================
[2019-04-08 15:41:52,091] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3053474e-14 5.4143294e-09 2.9976648e-06 3.0223040e-05 5.2782547e-09
 6.6863841e-11 1.8674459e-08 1.0764989e-12 1.7418969e-15 3.6643683e-18
 9.9996674e-01], sum to 1.0000
[2019-04-08 15:41:52,093] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9358
[2019-04-08 15:41:52,098] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7498263e-15 5.5956203e-08 2.4360465e-06 1.3538304e-06 8.3116564e-10
 3.9430018e-11 2.7872085e-07 8.5755161e-13 4.6385414e-16 6.9237199e-19
 9.9999583e-01], sum to 1.0000
[2019-04-08 15:41:52,098] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1430
[2019-04-08 15:41:52,132] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 44.0, 95.0, 631.0, 22.5, 26.59267416269537, 0.6951972945485979, 1.0, 1.0, 65.0, 60233.68155907298], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 307800.0000, 
sim time next is 308400.0000, 
raw observation next is [-9.5, 44.0, 92.83333333333334, 629.6666666666667, 22.5, 26.28260710854224, 0.6525173096238571, 1.0, 1.0, 64.99999999999994, 51987.94747135806], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.30944444444444447, 0.6957642725598527, 0.375, 0.6902172590451867, 0.7175057698746191, 1.0, 1.0, 0.9999999999999989, 0.24756165462551455], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.338494], dtype=float32), -0.7745931]. 
=============================================
[2019-04-08 15:41:52,133] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 43.0, 82.0, 623.0, 22.5, 25.43741128626992, 0.5849503645121946, 1.0, 1.0, 65.0, 50281.12595095011], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 22.5, 26.69237814054669, 0.6753452563771449, 1.0, 1.0, 65.0, 41799.60067000979], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4266666666666667, 0.26666666666666666, 0.661510128913444, 0.375, 0.7243648450455575, 0.7251150854590483, 1.0, 1.0, 1.0, 0.19904571747623712], 
reward next is 0.8010, 
noisyNet noise sample is [array([-0.10477907], dtype=float32), -0.9121684]. 
=============================================
[2019-04-08 15:41:52,141] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.016754]
 [71.20805 ]
 [71.26328 ]
 [71.41826 ]
 [71.471245]], R is [[71.06421661]
 [71.11414337]
 [71.07700348]
 [71.11270142]
 [71.20681763]].
[2019-04-08 15:41:53,364] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.86408726e-14 5.01053021e-09 1.42282113e-06 2.78025732e-06
 9.23882637e-10 5.46800799e-11 8.07440642e-07 1.51426944e-11
 1.02741065e-14 2.59054373e-17 9.99994993e-01], sum to 1.0000
[2019-04-08 15:41:53,377] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1717
[2019-04-08 15:41:53,390] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.63333333333333, 67.66666666666667, 0.0, 0.0, 22.5, 25.69537606143212, 0.4558982902385045, 1.0, 1.0, 65.0, 63567.9109481131], 
current ob forecast is [], 
actual action is [-7.633333333333329, 65.0], 
sim time this is 330000.0000, 
sim time next is 330600.0000, 
raw observation next is [-12.71666666666667, 68.83333333333333, 0.0, 0.0, 22.5, 25.64495168114017, 0.4469787837800235, 1.0, 1.0, 65.0, 63323.37667820408], 
processed observation next is [1.0, 0.8260869565217391, 0.1103416435826407, 0.6883333333333332, 0.0, 0.0, 0.375, 0.6370793067616809, 0.6489929279266745, 1.0, 1.0, 1.0, 0.30153988894382894], 
reward next is 0.6985, 
noisyNet noise sample is [array([-0.10733127], dtype=float32), 0.3203213]. 
=============================================
[2019-04-08 15:41:53,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4244773e-15 1.9262005e-08 2.7377696e-06 2.4554940e-05 3.5926824e-09
 7.5928153e-11 3.4053835e-06 4.7145113e-12 1.7174619e-15 2.8277284e-16
 9.9996924e-01], sum to 1.0000
[2019-04-08 15:41:53,422] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0923
[2019-04-08 15:41:53,446] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.4, 71.66666666666667, 0.0, 0.0, 19.0, 24.60176876558061, 0.1738538011253428, 0.0, 1.0, 65.0, 65946.00365657154], 
current ob forecast is [], 
actual action is [-10.4, 65.0], 
sim time this is 358800.0000, 
sim time next is 359400.0000, 
raw observation next is [-15.5, 72.33333333333333, 0.0, 0.0, 19.0, 24.54351265256952, 0.1594786638201094, 0.0, 1.0, 65.0, 65952.16873638687], 
processed observation next is [1.0, 0.13043478260869565, 0.033240997229916885, 0.7233333333333333, 0.0, 0.0, 0.08333333333333333, 0.5452927210474602, 0.5531595546067031, 0.0, 1.0, 1.0, 0.314057946363747], 
reward next is 0.6859, 
noisyNet noise sample is [array([1.1023173], dtype=float32), -2.392941]. 
=============================================
[2019-04-08 15:41:53,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.2964224e-14 2.3414946e-08 4.0467430e-06 1.8605737e-05 4.3339551e-10
 2.1506585e-10 5.7196786e-07 2.2746121e-11 5.5748486e-15 1.2868150e-15
 9.9997675e-01], sum to 1.0000
[2019-04-08 15:41:53,663] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2779
[2019-04-08 15:41:53,694] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.2, 62.0, 0.0, 0.0, 22.5, 25.92391578107992, 0.5003853248061202, 1.0, 1.0, 65.0, 62505.58555301787], 
current ob forecast is [], 
actual action is [-7.199999999999999, 65.0], 
sim time this is 327000.0000, 
sim time next is 327600.0000, 
raw observation next is [-12.3, 63.0, 0.0, 0.0, 22.5, 25.88379724032402, 0.4938288304632455, 1.0, 1.0, 65.0, 61385.74391046618], 
processed observation next is [1.0, 0.8260869565217391, 0.12188365650969527, 0.63, 0.0, 0.0, 0.375, 0.6569831033603352, 0.6646096101544151, 1.0, 1.0, 1.0, 0.29231306624031517], 
reward next is 0.7077, 
noisyNet noise sample is [array([-0.2088736], dtype=float32), -1.2117466]. 
=============================================
[2019-04-08 15:41:54,311] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1977208e-13 1.0288656e-08 1.3998099e-06 2.8235856e-05 1.5933005e-09
 3.5105266e-10 1.3066353e-06 3.8139960e-12 4.0627082e-14 6.5179000e-17
 9.9996901e-01], sum to 1.0000
[2019-04-08 15:41:54,313] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7669
[2019-04-08 15:41:54,351] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.4, 68.5, 0.0, 0.0, 19.0, 24.83447510043351, 0.2575908349569081, 0.0, 1.0, 65.0, 64401.87688828081], 
current ob forecast is [], 
actual action is [-9.4, 65.0], 
sim time this is 348600.0000, 
sim time next is 349200.0000, 
raw observation next is [-14.5, 69.0, 0.0, 0.0, 19.0, 24.8317434268201, 0.2594847041940791, 0.0, 1.0, 65.0, 64424.46656579992], 
processed observation next is [1.0, 0.043478260869565216, 0.06094182825484763, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5693119522350084, 0.5864949013980264, 0.0, 1.0, 1.0, 0.30678317412285677], 
reward next is 0.6932, 
noisyNet noise sample is [array([-0.10576355], dtype=float32), -0.35065553]. 
=============================================
[2019-04-08 15:41:54,380] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3613899e-13 1.1754417e-08 1.2812610e-06 2.7801221e-05 1.7341866e-09
 3.7392203e-10 1.2086865e-06 3.9365017e-12 4.2993116e-14 6.4810607e-17
 9.9996972e-01], sum to 1.0000
[2019-04-08 15:41:54,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7852
[2019-04-08 15:41:54,415] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.58333333333333, 69.0, 0.0, 0.0, 19.0, 24.86702737699383, 0.2599439945537106, 0.0, 1.0, 65.0, 64456.61555681626], 
current ob forecast is [], 
actual action is [-9.58333333333333, 65.0], 
sim time this is 349800.0000, 
sim time next is 350400.0000, 
raw observation next is [-14.66666666666667, 69.0, 0.0, 0.0, 19.0, 24.88291127994715, 0.2670140222121341, 0.0, 1.0, 65.0, 64516.88932017057], 
processed observation next is [1.0, 0.043478260869565216, 0.05632502308402576, 0.69, 0.0, 0.0, 0.08333333333333333, 0.573575939995596, 0.5890046740707113, 0.0, 1.0, 1.0, 0.3072232824770027], 
reward next is 0.6928, 
noisyNet noise sample is [array([-0.10576355], dtype=float32), -0.35065553]. 
=============================================
[2019-04-08 15:41:54,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4250108e-14 9.8278985e-10 1.2705439e-06 2.8966422e-06 1.6319496e-09
 2.3823377e-10 7.4837978e-08 7.9173761e-13 3.4405964e-15 1.3650642e-17
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:41:54,495] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7826
[2019-04-08 15:41:54,518] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.56666666666667, 78.0, 0.0, 0.0, 19.0, 25.16727344796674, 0.3480930796097192, 0.0, 1.0, 65.0, 64413.51609225784], 
current ob forecast is [], 
actual action is [-8.56666666666667, 65.0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 0.0, 0.0, 19.0, 25.17009096360753, 0.3462022391149895, 0.0, 1.0, 65.0, 64396.95621096768], 
processed observation next is [1.0, 0.9565217391304348, 0.08448753462603877, 0.76, 0.0, 0.0, 0.08333333333333333, 0.5975075803006277, 0.6154007463716632, 0.0, 1.0, 1.0, 0.30665217243317944], 
reward next is 0.6933, 
noisyNet noise sample is [array([-1.5525774], dtype=float32), 1.0736265]. 
=============================================
[2019-04-08 15:41:54,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.4892657e-15 5.3938893e-09 5.9077888e-06 7.1921716e-07 1.8453494e-10
 4.0144922e-11 7.9410157e-08 3.6398176e-12 5.8585314e-16 1.0722418e-17
 9.9999332e-01], sum to 1.0000
[2019-04-08 15:41:54,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1950
[2019-04-08 15:41:54,606] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 37.66666666666667, 26.33333333333333, 504.3333333333333, 22.5, 26.18628160176948, 0.5840618305183739, 1.0, 1.0, 64.99999999999999, 54259.29116810687], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 403800.0000, 
sim time next is 404400.0000, 
raw observation next is [-8.900000000000002, 37.33333333333334, 23.66666666666666, 453.6666666666667, 22.5, 26.51489954196985, 0.6178795942446036, 1.0, 1.0, 65.00000000000001, 46992.01827411034], 
processed observation next is [1.0, 0.6956521739130435, 0.2160664819944598, 0.3733333333333334, 0.07888888888888887, 0.5012891344383057, 0.375, 0.7095749618308208, 0.7059598647482012, 1.0, 1.0, 1.0000000000000002, 0.22377151559100164], 
reward next is 0.7762, 
noisyNet noise sample is [array([1.037551], dtype=float32), -1.0580144]. 
=============================================
[2019-04-08 15:41:54,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.89423147e-14 1.57381030e-09 1.82056501e-05 7.80744983e-07
 1.24761383e-08 1.30157232e-10 3.82625814e-07 1.80437620e-12
 6.23631872e-16 1.26375504e-17 9.99980569e-01], sum to 1.0000
[2019-04-08 15:41:54,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9854
[2019-04-08 15:41:54,718] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.3, 68.0, 0.0, 0.0, 19.0, 24.8228924315181, 0.259472655551141, 0.0, 1.0, 65.0, 64377.22201742091], 
current ob forecast is [], 
actual action is [-9.3, 65.0], 
sim time this is 348000.0000, 
sim time next is 348600.0000, 
raw observation next is [-14.4, 68.5, 0.0, 0.0, 19.0, 24.83449248526226, 0.2575954351792158, 0.0, 1.0, 65.0, 64401.86585760071], 
processed observation next is [1.0, 0.0, 0.0637119113573407, 0.685, 0.0, 0.0, 0.08333333333333333, 0.5695410404385216, 0.5858651450597386, 0.0, 1.0, 1.0, 0.30667555170286054], 
reward next is 0.6933, 
noisyNet noise sample is [array([0.6337753], dtype=float32), 0.71338594]. 
=============================================
[2019-04-08 15:41:54,747] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0886351e-13 8.7515026e-09 7.5697908e-06 1.8267649e-05 4.0095429e-09
 9.8435669e-11 4.2576519e-07 2.2937355e-11 2.9491058e-14 4.6832261e-16
 9.9997377e-01], sum to 1.0000
[2019-04-08 15:41:54,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6233
[2019-04-08 15:41:54,777] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 19.0, 24.96092214430261, 0.2406718753162538, 0.0, 1.0, 65.0, 65325.46746372504], 
current ob forecast is [], 
actual action is [-10.0, 65.0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 19.0, 24.86330011099401, 0.2296079136832184, 0.0, 1.0, 65.0, 65598.38288750256], 
processed observation next is [1.0, 0.08695652173913043, 0.04709141274238226, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5719416759161676, 0.5765359712277395, 0.0, 1.0, 1.0, 0.31237325184525033], 
reward next is 0.6876, 
noisyNet noise sample is [array([0.5554885], dtype=float32), 0.2015631]. 
=============================================
[2019-04-08 15:41:54,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[65.580635]
 [65.51125 ]
 [65.445496]
 [65.611305]
 [65.78346 ]], R is [[65.53814697]
 [65.57169342]
 [65.60644531]
 [65.64144897]
 [65.67711639]].
[2019-04-08 15:41:54,858] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.49799588e-14 9.15178489e-10 1.74034039e-06 1.05557037e-05
 1.07456388e-09 4.35382459e-12 5.34630473e-08 9.45610235e-13
 1.17250155e-17 7.98765635e-18 9.99987602e-01], sum to 1.0000
[2019-04-08 15:41:54,858] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2431
[2019-04-08 15:41:54,887] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.6, 90.0, 32.0, 607.5, 22.5, 25.01922807065834, 0.2544585692938962, 1.0, 1.0, 65.0, 61279.02272034045], 
current ob forecast is [], 
actual action is [-10.6, 65.0], 
sim time this is 378000.0000, 
sim time next is 378600.0000, 
raw observation next is [-15.41666666666667, 86.0, 34.33333333333334, 651.0, 22.5, 25.13360160652796, 0.258742146571789, 1.0, 1.0, 65.0, 58951.01326400454], 
processed observation next is [1.0, 0.391304347826087, 0.0355493998153277, 0.86, 0.11444444444444447, 0.7193370165745856, 0.375, 0.5944668005439967, 0.5862473821905964, 1.0, 1.0, 1.0, 0.280719110780974], 
reward next is 0.7193, 
noisyNet noise sample is [array([0.6455682], dtype=float32), 0.097782925]. 
=============================================
[2019-04-08 15:41:55,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5952372e-13 2.1348967e-08 1.6547778e-05 1.0408880e-05 2.8977251e-08
 1.6453131e-10 1.6822975e-07 3.9720757e-12 1.5565391e-14 5.0957211e-17
 9.9997282e-01], sum to 1.0000
[2019-04-08 15:41:55,025] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4293
[2019-04-08 15:41:55,041] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 24.30642707728287, 0.1301899119780824, 0.0, 1.0, 65.0, 65827.94682559524], 
current ob forecast is [], 
actual action is [-10.6, 65.0], 
sim time this is 361800.0000, 
sim time next is 362400.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 24.33528959031, 0.1291057742508872, 0.0, 1.0, 65.0, 65675.46597109204], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.5279407991925001, 0.5430352580836291, 0.0, 1.0, 1.0, 0.3127403141480573], 
reward next is 0.6873, 
noisyNet noise sample is [array([-1.0079525], dtype=float32), -1.7832247]. 
=============================================
[2019-04-08 15:41:55,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5402134e-13 3.1656548e-08 1.0539148e-05 3.2357522e-05 8.3312621e-08
 1.2827547e-10 3.6399961e-06 4.7967616e-11 1.5362843e-14 6.2365538e-16
 9.9995339e-01], sum to 1.0000
[2019-04-08 15:41:55,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1622
[2019-04-08 15:41:55,267] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-14.83333333333333, 69.0, 0.0, 0.0, 19.0, 24.9464937981856, 0.2653187294255764, 0.0, 1.0, 65.0, 64666.48469435863], 
current ob forecast is [], 
actual action is [-9.83333333333333, 65.0], 
sim time this is 351600.0000, 
sim time next is 352200.0000, 
raw observation next is [-14.91666666666667, 69.0, 0.0, 0.0, 19.0, 24.98265000676811, 0.2658971218515997, 0.0, 1.0, 65.0, 64879.2969364337], 
processed observation next is [1.0, 0.043478260869565216, 0.04939981532779307, 0.69, 0.0, 0.0, 0.08333333333333333, 0.5818875005640093, 0.5886323739505332, 0.0, 1.0, 1.0, 0.30894903303063664], 
reward next is 0.6911, 
noisyNet noise sample is [array([0.15418541], dtype=float32), 0.48181692]. 
=============================================
[2019-04-08 15:41:56,150] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.49547038e-14 2.71579683e-08 5.32939794e-07 1.48792833e-06
 4.46219239e-09 1.06140055e-10 5.72047838e-07 3.72339182e-12
 6.82951654e-16 2.45579283e-16 9.99997377e-01], sum to 1.0000
[2019-04-08 15:41:56,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3089
[2019-04-08 15:41:56,168] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.7, 54.00000000000001, 0.0, 0.0, 19.0, 25.08930222976576, 0.3086109073535479, 0.0, 1.0, 65.0, 61461.8409259385], 
current ob forecast is [], 
actual action is [-6.699999999999999, 65.0], 
sim time this is 429600.0000, 
sim time next is 430200.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 25.06468451945393, 0.3026493859866513, 0.0, 1.0, 65.0, 61530.57676918244], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.5887237099544942, 0.600883128662217, 0.0, 1.0, 1.0, 0.29300274651991637], 
reward next is 0.7070, 
noisyNet noise sample is [array([1.772864], dtype=float32), 0.114638135]. 
=============================================
[2019-04-08 15:41:56,264] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5153451e-13 1.8300574e-08 2.4296262e-06 1.5931986e-05 6.3345897e-09
 8.9011173e-11 3.0982255e-07 4.7069844e-12 1.3118558e-15 6.2695711e-17
 9.9998128e-01], sum to 1.0000
[2019-04-08 15:41:56,265] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4603
[2019-04-08 15:41:56,286] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-16.2, 78.0, 0.0, 0.0, 19.0, 24.14394945237541, 0.06370038007809616, 0.0, 1.0, 65.0, 65549.82232177508], 
current ob forecast is [], 
actual action is [-11.2, 65.0], 
sim time this is 370800.0000, 
sim time next is 371400.0000, 
raw observation next is [-16.28333333333333, 78.5, 0.0, 0.0, 22.5, 24.11022942623336, 0.04979118182331969, 0.0, 1.0, 65.0, 65655.2878869511], 
processed observation next is [1.0, 0.30434782608695654, 0.011542012927054512, 0.785, 0.0, 0.0, 0.375, 0.5091857855194467, 0.5165970606077732, 0.0, 1.0, 1.0, 0.3126442280331005], 
reward next is 0.6874, 
noisyNet noise sample is [array([0.34020373], dtype=float32), 1.5178177]. 
=============================================
[2019-04-08 15:41:56,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0871823e-14 8.5275991e-09 1.2825370e-06 1.7379074e-06 8.7398382e-09
 3.2278016e-10 7.6919747e-07 4.0266028e-12 4.3877518e-15 4.3253443e-17
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:41:56,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1300
[2019-04-08 15:41:56,318] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.9, 37.0, 21.0, 403.0, 22.5, 26.72004477537797, 0.3331354811433646, 1.0, 1.0, 65.0, 46048.66360112569], 
current ob forecast is [], 
actual action is [-3.9000000000000004, 65.0], 
sim time this is 405000.0000, 
sim time next is 405600.0000, 
raw observation next is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 22.5, 26.7516934413603, 0.6462429061252457, 1.0, 1.0, 65.0, 56921.88960197137], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3666666666666667, 0.058333333333333334, 0.37421731123388585, 0.375, 0.729307786780025, 0.7154143020417486, 1.0, 1.0, 1.0, 0.2710566171522446], 
reward next is 0.7289, 
noisyNet noise sample is [array([1.1143849], dtype=float32), 1.123371]. 
=============================================
[2019-04-08 15:41:56,369] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3047152e-15 2.3670296e-10 8.6911484e-07 6.6665925e-06 7.5456119e-10
 2.1589098e-12 4.7560590e-08 8.4467719e-13 2.4912962e-16 9.6311358e-19
 9.9999237e-01], sum to 1.0000
[2019-04-08 15:41:56,370] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6080
[2019-04-08 15:41:56,383] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.05, 63.0, 89.0, 38.0, 19.0, 26.03511381982047, 0.5171425494528585, 0.0, 1.0, 65.0, 57401.8997683913], 
current ob forecast is [], 
actual action is [1.9500000000000002, 65.0], 
sim time this is 646200.0000, 
sim time next is 646800.0000, 
raw observation next is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 19.0, 26.04287638742426, 0.5199798299213422, 0.0, 1.0, 65.0, 57229.89115024733], 
processed observation next is [0.0, 0.4782608695652174, 0.38134810710988, 0.6233333333333333, 0.30944444444444447, 0.05340699815837937, 0.08333333333333333, 0.6702396989520217, 0.6733266099737807, 0.0, 1.0, 1.0, 0.2725232911916539], 
reward next is 0.7275, 
noisyNet noise sample is [array([0.8915031], dtype=float32), 0.59256005]. 
=============================================
[2019-04-08 15:41:56,645] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.8005517e-14 1.5592809e-08 4.7892840e-06 8.0369500e-06 2.8286444e-09
 5.2570148e-10 2.7321926e-06 6.0145160e-11 4.2310380e-15 1.8398413e-16
 9.9998438e-01], sum to 1.0000
[2019-04-08 15:41:56,647] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9601
[2019-04-08 15:41:56,677] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 19.0, 24.47139088922869, 0.148152749392265, 0.0, 1.0, 65.0, 65974.09520671573], 
current ob forecast is [], 
actual action is [-10.6, 65.0], 
sim time this is 360000.0000, 
sim time next is 360600.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 19.0, 24.41309055529569, 0.1377133972667836, 0.0, 1.0, 65.0, 65964.17072593699], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.08333333333333333, 0.5344242129413074, 0.5459044657555946, 0.0, 1.0, 1.0, 0.31411509869493803], 
reward next is 0.6859, 
noisyNet noise sample is [array([-0.3066667], dtype=float32), 1.0599498]. 
=============================================
[2019-04-08 15:41:57,161] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8296664e-15 2.6440903e-09 1.5076964e-06 7.4167582e-07 1.3980822e-08
 6.4045561e-11 4.0737781e-08 1.2290303e-12 1.0570379e-16 1.3051716e-18
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:41:57,162] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3406
[2019-04-08 15:41:57,193] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 54.0, 55.0, 26.5, 19.0, 26.31457014995746, 0.5720571656892199, 0.0, 1.0, 65.0, 52383.46234998008], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 662400.0000, 
sim time next is 663000.0000, 
raw observation next is [-0.7, 54.5, 45.66666666666666, 22.66666666666666, 19.0, 26.33241233667103, 0.5685630959448459, 0.0, 1.0, 65.0, 51982.83677217553], 
processed observation next is [0.0, 0.6956521739130435, 0.443213296398892, 0.545, 0.1522222222222222, 0.02504604051565377, 0.08333333333333333, 0.6943676947225859, 0.6895210319816153, 0.0, 1.0, 1.0, 0.24753731796274062], 
reward next is 0.7525, 
noisyNet noise sample is [array([0.6910852], dtype=float32), 1.6648095]. 
=============================================
[2019-04-08 15:41:57,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.350746]
 [71.395615]
 [71.49804 ]
 [71.55802 ]
 [71.645195]], R is [[71.34933472]
 [71.38639832]
 [71.4236145 ]
 [71.46046448]
 [71.49723053]].
[2019-04-08 15:41:57,648] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9074395e-15 2.9362004e-08 5.5138014e-07 2.5440718e-06 3.3525327e-10
 1.5192771e-11 6.5957991e-08 4.1629105e-12 2.8192330e-16 8.6112662e-18
 9.9999678e-01], sum to 1.0000
[2019-04-08 15:41:57,649] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1663
[2019-04-08 15:41:57,671] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.5, 40.0, 42.5, 769.5, 22.5, 26.68317140418785, 0.626157987442051, 1.0, 1.0, 65.0, 44632.04591918257], 
current ob forecast is [], 
actual action is [-4.5, 65.0], 
sim time this is 399600.0000, 
sim time next is 400200.0000, 
raw observation next is [-9.4, 39.66666666666667, 40.66666666666666, 748.6666666666666, 22.5, 26.69383246438721, 0.63457603669575, 1.0, 1.0, 65.0, 44609.12337880874], 
processed observation next is [1.0, 0.6521739130434783, 0.20221606648199447, 0.3966666666666667, 0.1355555555555555, 0.8272559852670349, 0.375, 0.7244860386989341, 0.71152534556525, 1.0, 1.0, 1.0, 0.21242439704194638], 
reward next is 0.7876, 
noisyNet noise sample is [array([0.23893555], dtype=float32), -0.075698026]. 
=============================================
[2019-04-08 15:41:57,846] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.8703959e-16 3.0629885e-09 1.4582777e-07 4.4874869e-07 1.8301995e-11
 3.4934745e-13 3.6590571e-09 2.4067822e-13 1.5355792e-17 5.8875490e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:41:57,847] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7792
[2019-04-08 15:41:57,866] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.05, 78.0, 39.0, 738.0, 22.5, 25.25087125446115, 0.3067230193022559, 1.0, 1.0, 65.0, 58085.51631969197], 
current ob forecast is [], 
actual action is [-10.05, 65.0], 
sim time this is 379800.0000, 
sim time next is 380400.0000, 
raw observation next is [-14.86666666666667, 74.0, 44.33333333333333, 736.5, 22.5, 25.42571279616214, 0.3325460710747498, 1.0, 1.0, 65.0, 56205.39691487602], 
processed observation next is [1.0, 0.391304347826087, 0.05078485687903958, 0.74, 0.14777777777777776, 0.8138121546961326, 0.375, 0.6188093996801785, 0.61084869035825, 1.0, 1.0, 1.0, 0.26764474721369536], 
reward next is 0.7324, 
noisyNet noise sample is [array([-0.6277901], dtype=float32), -0.32024673]. 
=============================================
[2019-04-08 15:41:58,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2451114e-15 1.3761630e-08 1.6708379e-06 4.7182291e-07 4.8917287e-10
 4.8458237e-11 2.1155095e-08 1.2653866e-12 8.7823997e-16 3.0802072e-17
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:41:58,549] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8850
[2019-04-08 15:41:58,564] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.8, 51.0, 0.0, 0.0, 19.0, 25.00833008718044, 0.2576189065917595, 0.0, 1.0, 65.0, 62367.51140102761], 
current ob forecast is [], 
actual action is [-5.800000000000001, 65.0], 
sim time this is 441600.0000, 
sim time next is 442200.0000, 
raw observation next is [-10.7, 50.0, 0.0, 0.0, 19.0, 24.96595598839549, 0.2576855252586644, 0.0, 1.0, 65.0, 62328.07465017949], 
processed observation next is [1.0, 0.08695652173913043, 0.1662049861495845, 0.5, 0.0, 0.0, 0.08333333333333333, 0.5804963323662907, 0.5858951750862215, 0.0, 1.0, 1.0, 0.2968003554770452], 
reward next is 0.7032, 
noisyNet noise sample is [array([0.51454055], dtype=float32), -0.52983874]. 
=============================================
[2019-04-08 15:41:58,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6512375e-14 6.4654597e-09 6.2544785e-07 4.5458145e-07 3.1247929e-09
 1.5813508e-11 5.6039895e-08 2.0887961e-12 1.3633422e-15 3.7214995e-17
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:41:58,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0164
[2019-04-08 15:41:58,844] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.1, 52.0, 0.0, 0.0, 19.0, 24.76451806076291, 0.2056867404559213, 0.0, 1.0, 65.0, 62928.80316624675], 
current ob forecast is [], 
actual action is [-6.1, 65.0], 
sim time this is 447000.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 19.0, 24.72394226889489, 0.2048255135863034, 0.0, 1.0, 65.0, 63026.3356600507], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.08333333333333333, 0.5603285224079076, 0.5682751711954345, 0.0, 1.0, 1.0, 0.30012540790500336], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.8817242], dtype=float32), -1.7609235]. 
=============================================
[2019-04-08 15:41:59,075] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0631981e-15 3.0985983e-10 1.3975565e-06 2.0707198e-07 5.7481602e-09
 3.1418490e-12 7.7206384e-09 9.1054323e-13 1.1960152e-15 5.6511652e-18
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:41:59,077] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6879
[2019-04-08 15:41:59,099] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 19.0, 25.27327970217622, 0.3555223793874862, 0.0, 1.0, 65.0, 61769.07441724466], 
current ob forecast is [], 
actual action is [-5.6, 65.0], 
sim time this is 424800.0000, 
sim time next is 425400.0000, 
raw observation next is [-10.78333333333333, 49.83333333333334, 0.0, 0.0, 19.0, 25.2561535758909, 0.3500064255750299, 0.0, 1.0, 65.0, 61564.12302072298], 
processed observation next is [1.0, 0.9565217391304348, 0.1638965835641737, 0.4983333333333334, 0.0, 0.0, 0.08333333333333333, 0.6046794646575749, 0.6166688085250099, 0.0, 1.0, 1.0, 0.2931624905748713], 
reward next is 0.7068, 
noisyNet noise sample is [array([1.0138202], dtype=float32), -1.1905189]. 
=============================================
[2019-04-08 15:41:59,103] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1870142e-13 2.2239540e-06 9.5646326e-05 1.7420898e-05 2.3086875e-08
 2.7871438e-10 2.9423350e-06 4.2655421e-11 1.1457105e-14 4.3610836e-16
 9.9988174e-01], sum to 1.0000
[2019-04-08 15:41:59,107] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8832
[2019-04-08 15:41:59,153] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 40.0, 11.5, 0.0, 22.5, 24.92842780944907, 0.2208271166956715, 1.0, 1.0, 65.0, 61286.11533368972], 
current ob forecast is [], 
actual action is [-2.8, 65.0], 
sim time this is 460800.0000, 
sim time next is 461400.0000, 
raw observation next is [-7.533333333333333, 38.83333333333334, 15.33333333333334, 0.0, 22.5, 24.93149757296318, 0.2268672536569814, 0.0, 1.0, 65.0, 61596.30815882051], 
processed observation next is [1.0, 0.34782608695652173, 0.25392428439519854, 0.3883333333333334, 0.05111111111111113, 0.0, 0.375, 0.5776247977469318, 0.5756224178856605, 0.0, 1.0, 1.0, 0.2933157531372405], 
reward next is 0.7067, 
noisyNet noise sample is [array([-1.043535], dtype=float32), -0.886999]. 
=============================================
[2019-04-08 15:41:59,442] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.9864661e-15 2.8097762e-09 4.4087790e-07 4.8259776e-06 1.7552069e-09
 1.7723275e-12 6.4081752e-07 5.4233652e-13 1.4874985e-16 2.8802270e-18
 9.9999404e-01], sum to 1.0000
[2019-04-08 15:41:59,442] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7573
[2019-04-08 15:41:59,462] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.900000000000002, 37.33333333333334, 23.66666666666666, 453.6666666666667, 22.5, 26.51393446634986, 0.6176055711307856, 1.0, 1.0, 65.00000000000001, 47004.91556702378], 
current ob forecast is [], 
actual action is [-3.900000000000002, 65.0], 
sim time this is 404400.0000, 
sim time next is 405000.0000, 
raw observation next is [-8.9, 37.0, 21.0, 403.0, 22.5, 26.71957750916156, 0.3330010371595749, 1.0, 1.0, 65.0, 46053.64558515043], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.37, 0.07, 0.4453038674033149, 0.375, 0.7266314590967967, 0.6110003457198583, 1.0, 1.0, 1.0, 0.21930307421500203], 
reward next is 0.7807, 
noisyNet noise sample is [array([1.368648], dtype=float32), -0.12959565]. 
=============================================
[2019-04-08 15:41:59,469] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[70.96045 ]
 [71.13385 ]
 [71.319145]
 [71.43641 ]
 [71.65664 ]], R is [[70.73751068]
 [70.80630493]
 [70.83981323]
 [70.85772705]
 [70.93657684]].
[2019-04-08 15:41:59,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7393049e-14 6.1795163e-08 7.7880541e-06 9.8832252e-06 1.2791422e-08
 5.5048677e-10 4.5595871e-07 3.0336979e-11 1.5783858e-15 2.5938330e-16
 9.9998176e-01], sum to 1.0000
[2019-04-08 15:41:59,949] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7557
[2019-04-08 15:41:59,977] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.8, 51.0, 0.0, 0.0, 19.0, 25.00765672444259, 0.2574615581009538, 0.0, 1.0, 65.0, 62369.84283858578], 
current ob forecast is [], 
actual action is [-5.800000000000001, 65.0], 
sim time this is 441600.0000, 
sim time next is 442200.0000, 
raw observation next is [-10.7, 50.0, 0.0, 0.0, 19.0, 24.96528096153814, 0.2575234502618611, 0.0, 1.0, 65.0, 62329.08897830358], 
processed observation next is [1.0, 0.08695652173913043, 0.1662049861495845, 0.5, 0.0, 0.0, 0.08333333333333333, 0.5804400801281785, 0.585841150087287, 0.0, 1.0, 1.0, 0.2968051856109694], 
reward next is 0.7032, 
noisyNet noise sample is [array([-1.075437], dtype=float32), -2.3161793]. 
=============================================
[2019-04-08 15:42:00,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8904854e-15 1.6515448e-09 1.3819159e-06 3.0101316e-06 1.1053339e-09
 1.2932216e-11 3.1807201e-07 7.1099485e-12 9.0105637e-15 9.0493413e-17
 9.9999523e-01], sum to 1.0000
[2019-04-08 15:42:00,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3073
[2019-04-08 15:42:00,239] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.2, 52.0, 0.0, 0.0, 19.0, 24.81048409832425, 0.2129560777131382, 0.0, 1.0, 65.0, 62805.8888150287], 
current ob forecast is [], 
actual action is [-6.199999999999999, 65.0], 
sim time this is 446400.0000, 
sim time next is 447000.0000, 
raw observation next is [-11.1, 52.0, 0.0, 0.0, 19.0, 24.76385009375709, 0.2055201893927348, 0.0, 1.0, 65.0, 62929.16885246091], 
processed observation next is [1.0, 0.17391304347826086, 0.1551246537396122, 0.52, 0.0, 0.0, 0.08333333333333333, 0.5636541744797574, 0.5685067297975782, 0.0, 1.0, 1.0, 0.29966270882124246], 
reward next is 0.7003, 
noisyNet noise sample is [array([-2.9196553], dtype=float32), -2.0676658]. 
=============================================
[2019-04-08 15:42:00,244] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[68.83176]
 [68.86927]
 [68.91347]
 [68.93249]
 [68.98607]], R is [[68.79468536]
 [68.80767059]
 [68.82106781]
 [68.83478546]
 [68.84798431]].
[2019-04-08 15:42:00,485] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [4.2070504e-15 7.5937665e-09 5.2916613e-07 1.3336262e-06 1.8617171e-09
 1.0860531e-11 6.2251274e-08 5.3552470e-13 9.9361750e-16 2.2945249e-16
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:42:00,485] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0524
[2019-04-08 15:42:00,505] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.3, 38.66666666666666, 0.0, 0.0, 22.5, 25.57185167215005, 0.490353606683052, 1.0, 1.0, 65.0, 64693.21004438344], 
current ob forecast is [], 
actual action is [-4.300000000000001, 65.0], 
sim time this is 409200.0000, 
sim time next is 409800.0000, 
raw observation next is [-9.4, 39.33333333333334, 0.0, 0.0, 22.5, 25.97633215451066, 0.5079389146198651, 1.0, 1.0, 65.0, 58051.73287388668], 
processed observation next is [1.0, 0.7391304347826086, 0.20221606648199447, 0.3933333333333334, 0.0, 0.0, 0.375, 0.6646943462092217, 0.669312971539955, 1.0, 1.0, 1.0, 0.27643682320898416], 
reward next is 0.7236, 
noisyNet noise sample is [array([-1.5987759], dtype=float32), 0.8626288]. 
=============================================
[2019-04-08 15:42:00,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.8794573e-15 5.4628376e-08 5.4607804e-06 1.7524187e-06 2.0733673e-09
 2.8571728e-11 1.2353109e-06 5.9413932e-11 1.1575880e-15 4.6960421e-18
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:42:00,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7900
[2019-04-08 15:42:00,625] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.7, 54.0, 0.0, 0.0, 19.0, 25.06401803200547, 0.302478554818186, 0.0, 1.0, 65.0, 61530.69329078049], 
current ob forecast is [], 
actual action is [-6.699999999999999, 65.0], 
sim time this is 430200.0000, 
sim time next is 430800.0000, 
raw observation next is [-11.7, 54.0, 0.0, 0.0, 19.0, 25.03927944195019, 0.303112976993279, 0.0, 1.0, 65.0, 61586.78807423877], 
processed observation next is [1.0, 1.0, 0.13850415512465375, 0.54, 0.0, 0.0, 0.08333333333333333, 0.5866066201625157, 0.6010376589977596, 0.0, 1.0, 1.0, 0.293270419401137], 
reward next is 0.7067, 
noisyNet noise sample is [array([-0.55115795], dtype=float32), 0.24953447]. 
=============================================
[2019-04-08 15:42:00,994] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.2606038e-15 9.3238057e-09 3.0818678e-06 2.8897400e-06 2.0417248e-09
 1.9740046e-11 7.4421763e-07 4.6721568e-12 2.5823515e-16 7.7953149e-19
 9.9999332e-01], sum to 1.0000
[2019-04-08 15:42:00,994] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5911
[2019-04-08 15:42:01,025] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.0, 42.0, 0.0, 0.0, 22.5, 25.59373018539729, 0.419794122421383, 0.0, 1.0, 65.0, 62247.14096834274], 
current ob forecast is [], 
actual action is [-5.0, 65.0], 
sim time this is 417600.0000, 
sim time next is 418200.0000, 
raw observation next is [-10.1, 42.83333333333334, 0.0, 0.0, 19.0, 25.54909685432772, 0.4123644965497846, 0.0, 1.0, 65.0, 62428.52946939046], 
processed observation next is [1.0, 0.8695652173913043, 0.18282548476454297, 0.42833333333333345, 0.0, 0.0, 0.08333333333333333, 0.6290914045273098, 0.6374548321832615, 0.0, 1.0, 1.0, 0.29727871175900217], 
reward next is 0.7027, 
noisyNet noise sample is [array([0.4510276], dtype=float32), -3.2457178]. 
=============================================
[2019-04-08 15:42:01,745] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.5419889e-15 2.8828846e-09 2.5859824e-06 3.1178627e-06 8.0433198e-09
 8.1120987e-12 4.3300691e-08 8.2327260e-13 4.1315061e-15 1.4824614e-18
 9.9999428e-01], sum to 1.0000
[2019-04-08 15:42:01,745] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9769
[2019-04-08 15:42:01,771] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 51.0, 0.0, 0.0, 19.0, 24.88140316020727, 0.2299193878726014, 0.0, 1.0, 65.0, 62595.00238040934], 
current ob forecast is [], 
actual action is [-6.0, 65.0], 
sim time this is 445200.0000, 
sim time next is 445800.0000, 
raw observation next is [-11.1, 51.5, 0.0, 0.0, 19.0, 24.85663326241573, 0.2228107201641153, 0.0, 1.0, 65.0, 62689.46147348], 
processed observation next is [1.0, 0.13043478260869565, 0.1551246537396122, 0.515, 0.0, 0.0, 0.08333333333333333, 0.5713861052013108, 0.5742702400547052, 0.0, 1.0, 1.0, 0.2985212451118095], 
reward next is 0.7015, 
noisyNet noise sample is [array([0.818086], dtype=float32), -0.35435873]. 
=============================================
[2019-04-08 15:42:01,937] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.0764582e-14 3.3049552e-09 1.5003232e-07 1.0312136e-06 7.9614332e-10
 3.9659272e-11 1.2283375e-07 4.7306300e-12 2.4052589e-15 1.9926665e-17
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:42:01,937] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9583
[2019-04-08 15:42:01,953] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.45, 54.5, 0.0, 0.0, 19.0, 25.08135727802567, 0.2924690288067207, 0.0, 1.0, 65.0, 61868.64556529837], 
current ob forecast is [], 
actual action is [-6.449999999999999, 65.0], 
sim time this is 433800.0000, 
sim time next is 434400.0000, 
raw observation next is [-11.36666666666667, 54.66666666666667, 0.0, 0.0, 19.0, 25.05141259079905, 0.2868772061831907, 0.0, 1.0, 65.0, 61852.94293540381], 
processed observation next is [1.0, 0.0, 0.14773776546629722, 0.5466666666666667, 0.0, 0.0, 0.08333333333333333, 0.5876177158999208, 0.5956257353943969, 0.0, 1.0, 1.0, 0.29453782350192287], 
reward next is 0.7055, 
noisyNet noise sample is [array([1.4468248], dtype=float32), -0.5913668]. 
=============================================
[2019-04-08 15:42:02,470] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7799577e-17 3.5787043e-10 4.2914058e-07 7.5073063e-07 1.3517042e-10
 5.4237973e-12 7.4459464e-08 3.1782939e-14 1.3469581e-18 5.8286387e-19
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:42:02,477] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7995
[2019-04-08 15:42:02,489] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.9166666666666667, 34.5, 44.0, 0.0, 22.5, 26.58958773716726, 0.6038080336258266, 1.0, 1.0, 65.0, 39760.26935682816], 
current ob forecast is [], 
actual action is [5.916666666666667, 65.0], 
sim time this is 489000.0000, 
sim time next is 489600.0000, 
raw observation next is [1.1, 34.0, 38.0, 0.0, 22.5, 26.88510071645558, 0.6166483932108161, 1.0, 1.0, 65.0, 34555.06159154153], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.34, 0.12666666666666668, 0.0, 0.375, 0.7404250597046316, 0.7055494644036053, 1.0, 1.0, 1.0, 0.16454791234067398], 
reward next is 0.8355, 
noisyNet noise sample is [array([-0.1704235], dtype=float32), -0.4563869]. 
=============================================
[2019-04-08 15:42:03,111] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1373142e-17 5.0786909e-11 1.2474763e-08 3.7397276e-09 1.0176133e-11
 9.2034141e-14 3.3093581e-08 9.5803630e-16 4.4421067e-19 1.3498124e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:03,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2207
[2019-04-08 15:42:03,137] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 85.0, 77.0, 141.0, 19.0, 26.31083423671647, 0.6314821701867249, 0.0, 1.0, 65.0, 49926.11351648045], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 556200.0000, 
sim time next is 556800.0000, 
raw observation next is [-0.6, 84.33333333333333, 79.0, 140.0, 19.0, 26.34900396698156, 0.631889182737556, 0.0, 1.0, 65.0, 48728.43474981027], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.8433333333333333, 0.2633333333333333, 0.15469613259668508, 0.08333333333333333, 0.6957503305817966, 0.7106297275791853, 0.0, 1.0, 1.0, 0.23204016547528702], 
reward next is 0.7680, 
noisyNet noise sample is [array([0.01715544], dtype=float32), 0.17322032]. 
=============================================
[2019-04-08 15:42:03,428] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0163081e-18 9.7108563e-12 3.1382132e-08 1.0830615e-08 1.3169290e-12
 2.3208438e-14 1.7765739e-10 5.6029260e-16 1.9453777e-20 8.1375503e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:03,429] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5631
[2019-04-08 15:42:03,451] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 22.5, 26.17208242392726, 0.5559149530413605, 1.0, 1.0, 65.0, 53315.62590486876], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 502800.0000, 
sim time next is 503400.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 22.5, 26.19421371927793, 0.5553068523686776, 1.0, 1.0, 65.0, 52310.40375076715], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.375, 0.6828511432731608, 0.6851022841228925, 1.0, 1.0, 1.0, 0.24909716071793883], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.29695755], dtype=float32), -0.9247448]. 
=============================================
[2019-04-08 15:42:03,503] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.80783279e-17 1.04506515e-10 1.20363325e-07 7.64896342e-08
 2.80773217e-11 9.11055615e-16 1.49552337e-09 2.74484044e-16
 1.39466012e-19 3.41576560e-21 9.99999762e-01], sum to 1.0000
[2019-04-08 15:42:03,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7317
[2019-04-08 15:42:03,526] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 19.0, 26.24383895394442, 0.6046846253926205, 0.0, 1.0, 65.0, 51372.18847053092], 
current ob forecast is [], 
actual action is [5.9, 65.0], 
sim time this is 541200.0000, 
sim time next is 541800.0000, 
raw observation next is [0.8, 90.0, 0.0, 0.0, 19.0, 26.40518405627301, 0.5932677501307666, 0.0, 1.0, 65.0, 48468.84775455498], 
processed observation next is [0.0, 0.2608695652173913, 0.4847645429362882, 0.9, 0.0, 0.0, 0.08333333333333333, 0.7004320046894176, 0.6977559167102555, 0.0, 1.0, 1.0, 0.2308040369264523], 
reward next is 0.7692, 
noisyNet noise sample is [array([-1.5085026], dtype=float32), 0.17870452]. 
=============================================
[2019-04-08 15:42:03,736] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.4292973e-19 3.4425084e-11 2.9515086e-09 1.5652371e-07 4.8763142e-13
 6.4961392e-15 6.1602518e-10 3.1663989e-16 6.4995684e-20 3.3036218e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:03,736] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9553
[2019-04-08 15:42:03,755] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.966666666666667, 94.66666666666666, 0.0, 0.0, 19.0, 26.16643504355124, 0.5570181815942389, 0.0, 1.0, 65.0, 51874.0631542056], 
current ob forecast is [], 
actual action is [6.966666666666667, 65.0], 
sim time this is 508800.0000, 
sim time next is 509400.0000, 
raw observation next is [2.15, 94.0, 0.0, 0.0, 19.0, 26.16851140960933, 0.5582151103170011, 0.0, 1.0, 65.0, 51719.27712601829], 
processed observation next is [1.0, 0.9130434782608695, 0.5221606648199446, 0.94, 0.0, 0.0, 0.08333333333333333, 0.6807092841341108, 0.6860717034390004, 0.0, 1.0, 1.0, 0.24628227202865854], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.444285], dtype=float32), 1.0823392]. 
=============================================
[2019-04-08 15:42:04,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0356318e-18 3.7342639e-11 3.8479019e-08 8.5851958e-08 4.4268387e-12
 6.7170666e-15 3.4877861e-08 3.1618740e-15 2.3844745e-18 2.6074736e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:04,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1887
[2019-04-08 15:42:04,186] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.2, 89.66666666666667, 125.6666666666667, 103.1666666666667, 19.0, 26.25547047569277, 0.6164085932883262, 0.0, 1.0, 65.0, 50446.04946108237], 
current ob forecast is [], 
actual action is [4.8, 65.0], 
sim time this is 552000.0000, 
sim time next is 552600.0000, 
raw observation next is [-0.3, 89.0, 144.0, 103.0, 19.0, 26.26552955850793, 0.620717487218805, 0.0, 1.0, 65.0, 50200.91040331431], 
processed observation next is [0.0, 0.391304347826087, 0.4542936288088643, 0.89, 0.48, 0.1138121546961326, 0.08333333333333333, 0.6887941298756607, 0.706905829072935, 0.0, 1.0, 1.0, 0.2390519543014967], 
reward next is 0.7609, 
noisyNet noise sample is [array([-0.40878883], dtype=float32), 0.35564482]. 
=============================================
[2019-04-08 15:42:04,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6329183e-16 4.5726924e-09 1.4017656e-06 2.8200695e-06 1.2931094e-10
 3.1493929e-13 1.9744832e-08 1.0914815e-14 1.2041189e-16 4.5871001e-19
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:42:04,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7745
[2019-04-08 15:42:04,275] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.7000000000000001, 70.33333333333334, 0.0, 0.0, 22.5, 25.23541882254113, 0.4958419789608913, 1.0, 1.0, 65.0, 53031.90362040733], 
current ob forecast is [], 
actual action is [5.7, 65.0], 
sim time this is 495600.0000, 
sim time next is 496200.0000, 
raw observation next is [0.6, 77.16666666666667, 0.0, 0.0, 22.5, 26.37401039245986, 0.5635948340467084, 1.0, 1.0, 65.0, 45586.42088347387], 
processed observation next is [1.0, 0.7391304347826086, 0.479224376731302, 0.7716666666666667, 0.0, 0.0, 0.375, 0.6978341993716549, 0.6878649446822361, 1.0, 1.0, 1.0, 0.21707819468320888], 
reward next is 0.7829, 
noisyNet noise sample is [array([0.01142709], dtype=float32), 0.20152001]. 
=============================================
[2019-04-08 15:42:04,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.7960482e-17 5.8884275e-09 6.4783649e-07 6.6391578e-07 5.0050436e-10
 1.7064128e-12 6.0190750e-08 1.0672141e-13 1.7067926e-17 7.4665251e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:42:04,810] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9683
[2019-04-08 15:42:04,826] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 19.0, 25.94622307848744, 0.4909526742246224, 0.0, 1.0, 65.0, 55635.62866619514], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 802800.0000, 
sim time next is 803400.0000, 
raw observation next is [-6.700000000000001, 68.33333333333334, 0.0, 0.0, 22.5, 25.93926841796505, 0.4902628902252915, 0.0, 1.0, 65.0, 55216.9026400083], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.6833333333333335, 0.0, 0.0, 0.375, 0.6616057014970874, 0.6634209634084305, 0.0, 1.0, 1.0, 0.2629376316190872], 
reward next is 0.7371, 
noisyNet noise sample is [array([-0.52388674], dtype=float32), 0.16533898]. 
=============================================
[2019-04-08 15:42:05,105] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6848913e-18 1.2721968e-10 5.5020287e-08 2.8674976e-06 2.0712709e-10
 1.5824629e-13 5.7072864e-08 3.1733398e-14 1.1323785e-18 1.4829512e-19
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:42:05,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7199
[2019-04-08 15:42:05,125] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 81.0, 128.8333333333333, 488.3333333333333, 19.0, 26.61417483408707, 0.7307619816364586, 0.0, 1.0, 65.0, 44582.95914213257], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 570000.0000, 
sim time next is 570600.0000, 
raw observation next is [-1.2, 81.5, 127.0, 467.0, 19.0, 26.62328204406063, 0.7395254795720859, 0.0, 1.0, 65.0, 44494.4971533152], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.815, 0.42333333333333334, 0.5160220994475138, 0.08333333333333333, 0.7186068370050526, 0.7465084931906953, 0.0, 1.0, 1.0, 0.21187855787292953], 
reward next is 0.7881, 
noisyNet noise sample is [array([1.5250016], dtype=float32), -0.11987521]. 
=============================================
[2019-04-08 15:42:05,280] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6419958e-19 1.2087984e-11 7.5786566e-10 6.8702959e-09 1.0952473e-12
 6.6545060e-15 2.4978128e-10 8.4107958e-16 1.1860867e-20 2.2669848e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:05,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0221
[2019-04-08 15:42:05,311] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 19.0, 26.23285098465827, 0.5714657547302101, 0.0, 1.0, 65.0, 49966.34317862582], 
current ob forecast is [], 
actual action is [8.383333333333333, 65.0], 
sim time this is 515400.0000, 
sim time next is 516000.0000, 
raw observation next is [3.466666666666667, 96.33333333333333, 0.0, 0.0, 19.0, 26.2386816881382, 0.5726943160748775, 0.0, 1.0, 65.0, 49803.02738122876], 
processed observation next is [1.0, 1.0, 0.5586334256694367, 0.9633333333333333, 0.0, 0.0, 0.08333333333333333, 0.68655680734485, 0.6908981053582925, 0.0, 1.0, 1.0, 0.2371572732439465], 
reward next is 0.7628, 
noisyNet noise sample is [array([1.1631445], dtype=float32), -0.67737925]. 
=============================================
[2019-04-08 15:42:05,341] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[87.25953 ]
 [87.18984 ]
 [87.008156]
 [86.96883 ]
 [86.827286]], R is [[87.29564667]
 [87.18475342]
 [87.07332611]
 [86.96199036]
 [86.85088348]].
[2019-04-08 15:42:05,694] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9495200e-18 1.5324504e-11 1.2584191e-07 4.1225312e-07 6.8469961e-12
 4.8614720e-13 7.0457872e-08 8.3440884e-14 6.0196777e-19 1.7831088e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:42:05,696] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8450
[2019-04-08 15:42:05,718] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 19.0, 26.21922349181219, 0.5765233558858455, 0.0, 1.0, 65.0, 52297.6576563683], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 546000.0000, 
sim time next is 546600.0000, 
raw observation next is [0.5, 92.0, 12.0, 37.99999999999999, 19.0, 26.20962085688618, 0.5901733623803463, 0.0, 1.0, 65.0, 52359.20440934551], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.04, 0.04198895027624309, 0.08333333333333333, 0.6841350714071815, 0.6967244541267821, 0.0, 1.0, 1.0, 0.2493295448064072], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.9171126], dtype=float32), -0.6783324]. 
=============================================
[2019-04-08 15:42:06,152] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6571635e-17 3.9910358e-10 1.2046198e-07 1.3140378e-07 4.2008279e-11
 3.3128773e-14 4.5211790e-09 4.1167876e-15 4.2802385e-17 1.7255555e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:42:06,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4757
[2019-04-08 15:42:06,186] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 89.0, 144.0, 103.0, 19.0, 26.26572057342457, 0.6207732251927337, 0.0, 1.0, 65.0, 50198.64075409118], 
current ob forecast is [], 
actual action is [4.7, 65.0], 
sim time this is 552600.0000, 
sim time next is 553200.0000, 
raw observation next is [-0.4, 88.33333333333334, 132.8333333333333, 109.3333333333333, 19.0, 26.27620641423153, 0.6223700439692196, 0.0, 1.0, 65.0, 49997.32408858748], 
processed observation next is [0.0, 0.391304347826087, 0.45152354570637127, 0.8833333333333334, 0.4427777777777776, 0.12081031307550641, 0.08333333333333333, 0.6896838678526276, 0.7074566813230732, 0.0, 1.0, 1.0, 0.23808249565994036], 
reward next is 0.7619, 
noisyNet noise sample is [array([1.1242262], dtype=float32), -0.719958]. 
=============================================
[2019-04-08 15:42:06,580] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2478699e-17 3.5792472e-10 2.7712908e-08 3.4250539e-07 2.3183967e-10
 1.6474987e-14 1.8606338e-09 4.1528755e-15 2.0470694e-19 2.9656992e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:42:06,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2693
[2019-04-08 15:42:06,605] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 81.0, 128.8333333333333, 488.3333333333333, 19.0, 26.61397886052146, 0.730694380819075, 0.0, 1.0, 65.0, 44585.32389352743], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 570000.0000, 
sim time next is 570600.0000, 
raw observation next is [-1.2, 81.5, 127.0, 467.0, 19.0, 26.62308735288593, 0.7394581706804128, 0.0, 1.0, 65.0, 44496.80884490854], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.815, 0.42333333333333334, 0.5160220994475138, 0.08333333333333333, 0.7185906127404941, 0.7464860568934709, 0.0, 1.0, 1.0, 0.2118895659281359], 
reward next is 0.7881, 
noisyNet noise sample is [array([1.1963451], dtype=float32), -1.0124266]. 
=============================================
[2019-04-08 15:42:06,803] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1708387e-18 4.0159126e-11 2.4114996e-08 2.8764102e-08 2.0516321e-11
 5.0034396e-13 5.2188853e-09 5.5237888e-16 2.1760214e-19 4.5826202e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:06,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3925
[2019-04-08 15:42:06,843] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.066666666666667, 80.33333333333333, 131.3333333333333, 429.1666666666666, 19.0, 26.45715691189666, 0.6899175720347902, 0.0, 1.0, 65.0, 45863.75487940556], 
current ob forecast is [], 
actual action is [3.9333333333333327, 65.0], 
sim time this is 564000.0000, 
sim time next is 564600.0000, 
raw observation next is [-1.133333333333333, 80.16666666666667, 132.6666666666667, 462.3333333333334, 19.0, 26.54003788552761, 0.6919080408029724, 0.0, 1.0, 65.0, 44124.76741913459], 
processed observation next is [0.0, 0.5217391304347826, 0.43120960295475536, 0.8016666666666667, 0.4422222222222224, 0.5108655616942911, 0.08333333333333333, 0.7116698237939675, 0.7306360136009907, 0.0, 1.0, 1.0, 0.2101179400911171], 
reward next is 0.7899, 
noisyNet noise sample is [array([-0.71410185], dtype=float32), 1.6044898]. 
=============================================
[2019-04-08 15:42:07,308] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6619059e-16 1.3523648e-09 5.0311615e-07 2.7741225e-06 5.4722636e-11
 9.6736985e-13 5.7150839e-08 3.9220346e-14 7.8192597e-18 3.5795752e-19
 9.9999666e-01], sum to 1.0000
[2019-04-08 15:42:07,320] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3705
[2019-04-08 15:42:07,352] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.466666666666667, 87.0, 0.0, 0.0, 19.0, 26.50686237495867, 0.654510776042338, 0.0, 1.0, 65.0, 48989.09945305366], 
current ob forecast is [], 
actual action is [2.533333333333333, 65.0], 
sim time this is 584400.0000, 
sim time next is 585000.0000, 
raw observation next is [-2.55, 87.0, 0.0, 0.0, 19.0, 26.49068812081114, 0.6496029991583071, 0.0, 1.0, 65.0, 49371.16404233113], 
processed observation next is [0.0, 0.782608695652174, 0.3919667590027701, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7075573434009282, 0.716534333052769, 0.0, 1.0, 1.0, 0.23510078115395777], 
reward next is 0.7649, 
noisyNet noise sample is [array([-1.6244161], dtype=float32), 1.2238915]. 
=============================================
[2019-04-08 15:42:07,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.77364]
 [80.69834]
 [80.64676]
 [80.57161]
 [80.52602]], R is [[80.74113464]
 [80.70043945]
 [80.65610504]
 [80.61367798]
 [80.57495117]].
[2019-04-08 15:42:07,655] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0883473e-17 5.5068577e-10 3.3386277e-07 1.6179854e-06 2.2271827e-11
 2.7835390e-12 6.5962602e-08 3.8314780e-14 6.0428661e-17 2.0393497e-19
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:42:07,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6554
[2019-04-08 15:42:07,670] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 87.0, 20.5, 22.5, 19.0, 26.61424511380209, 0.6883277008121222, 0.0, 1.0, 65.0, 46745.99824536552], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 579600.0000, 
sim time next is 580200.0000, 
raw observation next is [-1.8, 87.0, 0.0, 0.0, 19.0, 26.59888352839952, 0.6777407171658442, 0.0, 1.0, 65.0, 47163.06668627934], 
processed observation next is [0.0, 0.7391304347826086, 0.41274238227146814, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7165736273666266, 0.7259135723886146, 0.0, 1.0, 1.0, 0.2245860318394254], 
reward next is 0.7754, 
noisyNet noise sample is [array([1.0219268], dtype=float32), -1.909628]. 
=============================================
[2019-04-08 15:42:07,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2047826e-17 5.6676691e-10 3.3989841e-07 1.6555335e-06 2.3734568e-11
 2.8784238e-12 7.0423646e-08 4.0138384e-14 6.2773102e-17 2.1837837e-19
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:42:07,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2164
[2019-04-08 15:42:07,702] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.9, 87.0, 0.0, 0.0, 19.0, 26.57118171236743, 0.6738020089298549, 0.0, 1.0, 65.0, 48359.34924463269], 
current ob forecast is [], 
actual action is [3.1, 65.0], 
sim time this is 580800.0000, 
sim time next is 581400.0000, 
raw observation next is [-2.0, 87.0, 0.0, 0.0, 19.0, 26.54478828268226, 0.672658527290826, 0.0, 1.0, 65.0, 48881.66152476478], 
processed observation next is [0.0, 0.7391304347826086, 0.40720221606648205, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7120656902235215, 0.724219509096942, 0.0, 1.0, 1.0, 0.23276981678459419], 
reward next is 0.7672, 
noisyNet noise sample is [array([1.0219268], dtype=float32), -1.909628]. 
=============================================
[2019-04-08 15:42:08,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9887353e-17 8.4514694e-11 3.5177035e-08 4.5478203e-08 2.9226555e-11
 6.3150911e-13 5.4835190e-09 1.8364289e-14 6.2492689e-18 4.2697936e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:08,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9711
[2019-04-08 15:42:08,320] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 81.5, 127.0, 467.0, 19.0, 26.62328204406063, 0.7395254795720859, 0.0, 1.0, 65.0, 44494.4971533152], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 570600.0000, 
sim time next is 571200.0000, 
raw observation next is [-1.2, 82.0, 122.5, 401.3333333333334, 19.0, 26.67399218305878, 0.7363220320049145, 0.0, 1.0, 65.0, 43271.76182941104], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.82, 0.4083333333333333, 0.443462246777164, 0.08333333333333333, 0.7228326819215649, 0.7454406773349715, 0.0, 1.0, 1.0, 0.20605600871148116], 
reward next is 0.7939, 
noisyNet noise sample is [array([0.63712627], dtype=float32), 0.46999526]. 
=============================================
[2019-04-08 15:42:09,472] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.2333422e-17 2.1686530e-10 6.3724553e-07 1.0176879e-05 3.1359262e-10
 1.5006228e-12 1.4461135e-08 9.9730879e-13 3.7980180e-18 1.4199562e-20
 9.9998915e-01], sum to 1.0000
[2019-04-08 15:42:09,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6796
[2019-04-08 15:42:09,503] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.7333333333333334, 81.66666666666667, 95.83333333333333, 178.5, 19.0, 26.38310558941797, 0.6431786381769694, 0.0, 1.0, 65.0, 49403.30068647101], 
current ob forecast is [], 
actual action is [4.266666666666667, 65.0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [-0.7666666666666667, 81.33333333333333, 102.6666666666667, 222.0, 19.0, 26.36900900639157, 0.6471055822779216, 0.0, 1.0, 65.0, 49278.56876058678], 
processed observation next is [0.0, 0.4782608695652174, 0.44136657433056325, 0.8133333333333332, 0.3422222222222223, 0.24530386740331492, 0.08333333333333333, 0.6974174171992974, 0.7157018607593072, 0.0, 1.0, 1.0, 0.23465985124088942], 
reward next is 0.7653, 
noisyNet noise sample is [array([0.9715131], dtype=float32), 0.39643893]. 
=============================================
[2019-04-08 15:42:09,523] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.62164 ]
 [77.68601 ]
 [77.858505]
 [78.00548 ]
 [78.192184]], R is [[77.57072449]
 [77.55976868]
 [77.55467224]
 [77.55330658]
 [77.54463959]].
[2019-04-08 15:42:09,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9655452e-15 7.8733536e-10 4.0761262e-07 3.7524562e-06 5.7727634e-10
 8.8261586e-12 3.5213381e-07 6.4004319e-13 2.9593317e-16 3.4318883e-17
 9.9999547e-01], sum to 1.0000
[2019-04-08 15:42:09,632] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6173
[2019-04-08 15:42:09,668] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 59.0, 147.0, 96.5, 19.0, 26.15645564800501, 0.5534295544598279, 0.0, 1.0, 65.0, 54142.54345623285], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 651600.0000, 
sim time next is 652200.0000, 
raw observation next is [-2.116666666666667, 59.16666666666667, 158.6666666666667, 95.33333333333334, 19.0, 26.17209538294351, 0.5582080701566773, 0.0, 1.0, 65.0, 53694.4791860948], 
processed observation next is [0.0, 0.5652173913043478, 0.40397045244690677, 0.5916666666666667, 0.5288888888888891, 0.10534069981583795, 0.08333333333333333, 0.6810079485786259, 0.6860693567188925, 0.0, 1.0, 1.0, 0.25568799612426096], 
reward next is 0.7443, 
noisyNet noise sample is [array([0.3554121], dtype=float32), -0.89875025]. 
=============================================
[2019-04-08 15:42:10,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3205100e-15 1.2835720e-10 8.1433143e-08 8.7584624e-07 7.2000667e-10
 1.0933641e-11 3.5566888e-08 6.1427046e-14 1.4623230e-16 1.8976472e-18
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:42:10,069] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6702
[2019-04-08 15:42:10,092] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 54.0, 55.0, 26.5, 19.0, 26.31432324497513, 0.5719800959507754, 0.0, 1.0, 65.0, 52386.50161836555], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 662400.0000, 
sim time next is 663000.0000, 
raw observation next is [-0.7, 54.5, 45.66666666666666, 22.66666666666666, 19.0, 26.3321664075215, 0.5684863957825413, 0.0, 1.0, 65.0, 51985.86449217796], 
processed observation next is [0.0, 0.6956521739130435, 0.443213296398892, 0.545, 0.1522222222222222, 0.02504604051565377, 0.08333333333333333, 0.6943472006267916, 0.6894954652608472, 0.0, 1.0, 1.0, 0.24755173567703792], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.7310837], dtype=float32), -1.078719]. 
=============================================
[2019-04-08 15:42:10,122] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.31211]
 [72.35439]
 [72.44913]
 [72.5006 ]
 [72.57697]], R is [[72.29546356]
 [72.32305145]
 [72.35088348]
 [72.37844086]
 [72.40601349]].
[2019-04-08 15:42:11,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8676492e-16 2.2503568e-10 1.7939854e-08 2.5726627e-07 7.6087331e-11
 6.6486449e-14 5.1293288e-08 1.8310247e-14 2.0906977e-17 4.5746242e-19
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:42:11,112] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6839
[2019-04-08 15:42:11,129] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.40756871213874, 0.6304028631152528, 0.0, 1.0, 65.0, 51088.28833073062], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 588600.0000, 
sim time next is 589200.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 19.0, 26.3932994651806, 0.6277086911811666, 0.0, 1.0, 65.0, 51610.3550459606], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6994416220983833, 0.7092362303937222, 0.0, 1.0, 1.0, 0.24576359545695523], 
reward next is 0.7542, 
noisyNet noise sample is [array([-0.43766496], dtype=float32), -0.2801265]. 
=============================================
[2019-04-08 15:42:11,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.8057531e-16 4.3804707e-09 1.3886161e-07 8.0266673e-06 6.9376294e-10
 7.5991253e-12 1.9432697e-07 1.2945194e-12 1.4400915e-16 1.1842153e-18
 9.9999166e-01], sum to 1.0000
[2019-04-08 15:42:11,868] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6085
[2019-04-08 15:42:11,888] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.483333333333333, 71.83333333333333, 0.0, 0.0, 19.0, 26.11909837646714, 0.4913969702934748, 0.0, 1.0, 65.0, 51842.23913312926], 
current ob forecast is [], 
actual action is [1.516666666666667, 65.0], 
sim time this is 694200.0000, 
sim time next is 694800.0000, 
raw observation next is [-3.4, 72.0, 0.0, 0.0, 19.0, 26.11193894583603, 0.4942321092941788, 0.0, 1.0, 65.0, 52093.38101676817], 
processed observation next is [1.0, 0.043478260869565216, 0.368421052631579, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6759949121530026, 0.6647440364313929, 0.0, 1.0, 1.0, 0.2480637191274675], 
reward next is 0.7519, 
noisyNet noise sample is [array([0.5104741], dtype=float32), 0.5835531]. 
=============================================
[2019-04-08 15:42:12,510] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.7736310e-15 5.6874383e-10 2.2147522e-07 6.4250262e-06 2.1419468e-09
 6.8143532e-13 1.3889648e-08 2.4235503e-12 1.2565947e-16 1.0301028e-18
 9.9999332e-01], sum to 1.0000
[2019-04-08 15:42:12,520] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0161
[2019-04-08 15:42:12,559] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 65.0, 0.0, 0.0, 19.0, 26.01520646782391, 0.5017187481470776, 0.0, 1.0, 65.0, 56589.67285743921], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 626400.0000, 
sim time next is 627000.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 19.0, 26.00505944858634, 0.4991540075549454, 0.0, 1.0, 65.0, 56830.39009551642], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.655, 0.0, 0.0, 0.08333333333333333, 0.667088287382195, 0.6663846691849818, 0.0, 1.0, 1.0, 0.27062090521674487], 
reward next is 0.7294, 
noisyNet noise sample is [array([-0.9139116], dtype=float32), 0.07526731]. 
=============================================
[2019-04-08 15:42:12,578] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[72.50215 ]
 [72.59709 ]
 [72.716446]
 [72.8322  ]
 [72.991325]], R is [[72.40773773]
 [72.41418457]
 [72.41572571]
 [72.41686249]
 [72.41928101]].
[2019-04-08 15:42:13,270] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3497510e-16 6.2362426e-10 3.2710591e-07 5.3628687e-06 1.2494207e-09
 9.2877525e-11 3.7411857e-07 1.1486915e-12 5.7713384e-16 4.9868854e-18
 9.9999392e-01], sum to 1.0000
[2019-04-08 15:42:13,272] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0535
[2019-04-08 15:42:13,290] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.566666666666667, 69.66666666666667, 0.0, 0.0, 19.0, 26.13246188169699, 0.5036297309895688, 0.0, 1.0, 65.0, 54117.41147243065], 
current ob forecast is [], 
actual action is [1.4333333333333331, 65.0], 
sim time this is 685200.0000, 
sim time next is 685800.0000, 
raw observation next is [-3.65, 70.0, 0.0, 0.0, 19.0, 26.11104527282282, 0.5076759961606406, 0.0, 1.0, 65.0, 54187.10429801129], 
processed observation next is [0.0, 0.9565217391304348, 0.3614958448753463, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6759204394019017, 0.6692253320535468, 0.0, 1.0, 1.0, 0.25803382999052993], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.40265033], dtype=float32), 0.07280357]. 
=============================================
[2019-04-08 15:42:14,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9527251e-17 4.5009133e-10 4.5312643e-08 2.1436720e-06 3.1137934e-10
 1.8716247e-13 3.0287207e-08 7.3673428e-14 6.8584130e-18 4.4235805e-19
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:42:14,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0849
[2019-04-08 15:42:14,767] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 56.0, 81.33333333333333, 53.0, 19.0, 26.28974615757584, 0.5689328307471012, 0.0, 1.0, 65.0, 52276.77323490181], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 657600.0000, 
sim time next is 658200.0000, 
raw observation next is [-0.7, 55.0, 81.66666666666667, 50.0, 19.0, 26.28941128429592, 0.5696579631617011, 0.0, 1.0, 65.0, 52301.04282877723], 
processed observation next is [0.0, 0.6086956521739131, 0.443213296398892, 0.55, 0.27222222222222225, 0.055248618784530384, 0.08333333333333333, 0.6907842736913267, 0.689885987720567, 0.0, 1.0, 1.0, 0.2490525848989392], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.8117003], dtype=float32), 1.0658797]. 
=============================================
[2019-04-08 15:42:14,977] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.5201513e-16 5.7641403e-09 2.5590003e-07 2.2060710e-06 9.7229991e-10
 1.6399753e-12 1.5221845e-07 4.6363426e-14 4.4421870e-16 4.3715260e-19
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:42:14,981] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6132
[2019-04-08 15:42:14,995] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.35, 55.0, 0.0, 0.0, 22.5, 27.45306605225695, 0.8129194392867682, 1.0, 1.0, 65.0, 39225.70810938847], 
current ob forecast is [], 
actual action is [1.65, 65.0], 
sim time this is 754200.0000, 
sim time next is 754800.0000, 
raw observation next is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 22.5, 27.25025427939564, 0.7961029477735296, 1.0, 1.0, 65.0, 41783.38758214426], 
processed observation next is [1.0, 0.7391304347826086, 0.36472760849492153, 0.5533333333333332, 0.0, 0.0, 0.375, 0.7708545232829701, 0.7653676492578433, 1.0, 1.0, 1.0, 0.19896851229592508], 
reward next is 0.8010, 
noisyNet noise sample is [array([1.0354651], dtype=float32), 1.0207108]. 
=============================================
[2019-04-08 15:42:15,077] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7177021e-18 1.4046513e-10 1.5669785e-09 7.7597004e-09 2.0263448e-12
 3.3622705e-14 3.3582541e-09 2.2377713e-16 5.7772238e-19 6.3979416e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:15,078] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2637
[2019-04-08 15:42:15,108] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 55.0, 0.0, 0.0, 22.5, 26.97346739068669, 0.7534282800820185, 1.0, 1.0, 65.0, 46319.85168673685], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 757200.0000, 
sim time next is 757800.0000, 
raw observation next is [-3.9, 54.5, 0.0, 0.0, 22.5, 26.81227898836848, 0.745690848304018, 1.0, 1.0, 65.0, 48399.96933778455], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.545, 0.0, 0.0, 0.375, 0.73435658236404, 0.7485636161013393, 1.0, 1.0, 1.0, 0.2304760444656407], 
reward next is 0.7695, 
noisyNet noise sample is [array([-0.10060559], dtype=float32), 0.028393412]. 
=============================================
[2019-04-08 15:42:15,517] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.6604081e-16 2.7671293e-10 4.7830940e-07 4.7185048e-07 9.2754127e-10
 5.0405714e-13 6.3788597e-09 5.4263079e-14 5.6271621e-18 4.7648237e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:42:15,519] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6347
[2019-04-08 15:42:15,537] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.1, 75.0, 0.0, 0.0, 19.0, 26.2332167672955, 0.4873735549909739, 0.0, 1.0, 65.0, 51408.29230809564], 
current ob forecast is [], 
actual action is [1.9, 65.0], 
sim time this is 703800.0000, 
sim time next is 704400.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 19.0, 26.1649537880108, 0.4906212883370884, 0.0, 1.0, 65.0, 53803.90968136514], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.75, 0.0, 0.0, 0.08333333333333333, 0.6804128156675665, 0.6635404294456961, 0.0, 1.0, 1.0, 0.2562090937207864], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.09416858], dtype=float32), -0.53323525]. 
=============================================
[2019-04-08 15:42:15,635] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.9622004e-15 6.7622667e-09 2.4712406e-06 3.7266964e-06 2.0291850e-09
 3.8803814e-11 3.0721884e-07 5.2064126e-13 6.5857690e-16 4.0791283e-18
 9.9999344e-01], sum to 1.0000
[2019-04-08 15:42:15,635] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9387
[2019-04-08 15:42:15,655] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.7, 61.5, 0.0, 0.0, 19.0, 26.35079575080867, 0.6604739701968062, 0.0, 1.0, 65.0, 53412.99408848365], 
current ob forecast is [], 
actual action is [-0.7000000000000002, 65.0], 
sim time this is 767400.0000, 
sim time next is 768000.0000, 
raw observation next is [-5.8, 62.0, 0.0, 0.0, 19.0, 26.37279105422731, 0.6541770792379407, 0.0, 1.0, 65.0, 52531.82800137807], 
processed observation next is [1.0, 0.9130434782608695, 0.30193905817174516, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6977325878522759, 0.7180590264126469, 0.0, 1.0, 1.0, 0.25015156191132415], 
reward next is 0.7498, 
noisyNet noise sample is [array([0.378565], dtype=float32), 1.546587]. 
=============================================
[2019-04-08 15:42:15,684] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[76.89292]
 [77.22192]
 [77.47777]
 [77.96522]
 [78.42326]], R is [[77.0574646 ]
 [77.03253937]
 [77.00897217]
 [76.98682404]
 [76.96626282]].
[2019-04-08 15:42:15,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0443157e-18 2.5164963e-09 2.1591451e-07 1.2273961e-07 7.6511730e-10
 1.6682079e-13 1.2073021e-08 1.9300424e-13 3.6553860e-18 1.2096458e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:42:15,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3479
[2019-04-08 15:42:15,964] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 57.0, 107.5, 614.0, 22.5, 27.50936457690441, 0.8135345233091066, 1.0, 1.0, 65.0, 30153.14140860389], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 734400.0000, 
sim time next is 735000.0000, 
raw observation next is [-0.4166666666666667, 55.83333333333333, 115.3333333333333, 559.0, 22.5, 27.5303520622175, 0.8133234218974188, 1.0, 1.0, 65.0, 30006.70381386443], 
processed observation next is [1.0, 0.5217391304347826, 0.45106186518928904, 0.5583333333333332, 0.3844444444444443, 0.6176795580110497, 0.375, 0.7941960051847916, 0.7711078072991396, 1.0, 1.0, 1.0, 0.14288906578030683], 
reward next is 0.8571, 
noisyNet noise sample is [array([1.4895755], dtype=float32), 0.06448337]. 
=============================================
[2019-04-08 15:42:15,978] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.46714]
 [84.55621]
 [84.54615]
 [84.45919]
 [84.25605]], R is [[84.36552429]
 [84.37828827]
 [84.39330292]
 [84.40653992]
 [84.41639709]].
[2019-04-08 15:42:16,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7316420e-16 3.3352427e-09 9.5783264e-07 6.1164582e-07 2.3357527e-10
 1.7388185e-11 4.1597026e-08 1.6951120e-14 2.3018271e-17 1.4023296e-19
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:42:16,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-08 15:42:16,123] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.15513970954449, 0.5111646452711618, 0.0, 1.0, 65.0, 53914.20149606653], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.13523843664971, 0.5092374655709729, 0.0, 1.0, 65.0, 54276.11770834186], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.677936536387476, 0.669745821856991, 0.0, 1.0, 1.0, 0.25845770337305646], 
reward next is 0.7415, 
noisyNet noise sample is [array([-0.7766913], dtype=float32), 0.2627831]. 
=============================================
[2019-04-08 15:42:16,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.6618751e-16 2.0334328e-09 7.1303937e-08 2.0332796e-07 1.9874024e-11
 1.1531891e-13 1.1122698e-08 3.9740257e-15 3.5774722e-18 9.8777825e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:42:16,824] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2540
[2019-04-08 15:42:16,864] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.55, 74.5, 0.0, 0.0, 19.0, 26.11375192456954, 0.5375344269990933, 0.0, 1.0, 65.0, 52813.06363139812], 
current ob forecast is [], 
actual action is [-2.55, 65.0], 
sim time this is 790200.0000, 
sim time next is 790800.0000, 
raw observation next is [-7.466666666666667, 74.66666666666667, 0.0, 0.0, 19.0, 26.08483377800539, 0.53238472420081, 0.0, 1.0, 65.0, 53585.33706669908], 
processed observation next is [1.0, 0.13043478260869565, 0.25577100646352724, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6737361481671158, 0.6774615747336034, 0.0, 1.0, 1.0, 0.2551682717461861], 
reward next is 0.7448, 
noisyNet noise sample is [array([1.2482604], dtype=float32), -0.77797294]. 
=============================================
[2019-04-08 15:42:16,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5520943e-18 3.1019079e-10 6.5484809e-09 4.8275342e-07 9.6534742e-12
 7.9307676e-13 4.1132280e-09 4.2847996e-15 1.1908028e-17 7.8676483e-22
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:42:16,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4193
[2019-04-08 15:42:16,935] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.88505351621595, 1.067827127798844, 0.0, 1.0, 65.0, 22424.83643616176], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1038600.0000, 
sim time next is 1039200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.88212306199978, 1.065545629508719, 0.0, 1.0, 65.0, 23996.94499043359], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8235102551666484, 0.8551818765029063, 0.0, 1.0, 1.0, 0.11427116662111232], 
reward next is 0.8857, 
noisyNet noise sample is [array([-0.58138514], dtype=float32), 2.128944]. 
=============================================
[2019-04-08 15:42:18,138] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.1059028e-16 4.9633653e-10 6.3009902e-06 1.1139103e-06 2.2590020e-10
 6.1498224e-12 1.5051620e-07 5.4048313e-13 2.1562513e-16 5.1482811e-18
 9.9999249e-01], sum to 1.0000
[2019-04-08 15:42:18,140] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4292
[2019-04-08 15:42:18,162] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 73.5, 0.0, 0.0, 19.0, 26.02897453733315, 0.5384789988209491, 0.0, 1.0, 65.0, 53978.64818173124], 
current ob forecast is [], 
actual action is [-2.8, 65.0], 
sim time this is 787800.0000, 
sim time next is 788400.0000, 
raw observation next is [-7.8, 74.0, 0.0, 0.0, 19.0, 26.02191884525102, 0.5478819118164622, 0.0, 1.0, 65.0, 54132.31188845581], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.74, 0.0, 0.0, 0.08333333333333333, 0.6684932371042516, 0.6826273039388208, 0.0, 1.0, 1.0, 0.25777291375455147], 
reward next is 0.7422, 
noisyNet noise sample is [array([1.0240469], dtype=float32), -0.24925794]. 
=============================================
[2019-04-08 15:42:18,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1448970e-17 6.4905248e-10 1.4577745e-07 3.2183302e-06 2.0189830e-10
 7.1415447e-12 1.5722971e-08 4.5708541e-14 5.2985756e-17 7.3329132e-19
 9.9999666e-01], sum to 1.0000
[2019-04-08 15:42:18,275] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1943
[2019-04-08 15:42:18,290] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.55, 74.5, 0.0, 0.0, 19.0, 26.11387014475918, 0.5375678510236024, 0.0, 1.0, 65.0, 52811.63384898823], 
current ob forecast is [], 
actual action is [-2.55, 65.0], 
sim time this is 790200.0000, 
sim time next is 790800.0000, 
raw observation next is [-7.466666666666667, 74.66666666666667, 0.0, 0.0, 19.0, 26.08494907783027, 0.532418381731332, 0.0, 1.0, 65.0, 53584.1768717279], 
processed observation next is [1.0, 0.13043478260869565, 0.25577100646352724, 0.7466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6737457564858559, 0.677472793910444, 0.0, 1.0, 1.0, 0.2551627470082281], 
reward next is 0.7448, 
noisyNet noise sample is [array([-0.5053371], dtype=float32), 1.1926517]. 
=============================================
[2019-04-08 15:42:18,298] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.81787237e-16 1.95704133e-10 1.34948515e-08 1.16441674e-06
 5.58643909e-10 2.58442573e-12 8.77659545e-09 6.60334485e-14
 1.53703247e-16 9.14719424e-21 9.99998808e-01], sum to 1.0000
[2019-04-08 15:42:18,299] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0605
[2019-04-08 15:42:18,326] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 22.5, 27.2502678999814, 0.7961068485335147, 1.0, 1.0, 65.0, 41783.23302927874], 
current ob forecast is [], 
actual action is [1.4666666666666668, 65.0], 
sim time this is 754800.0000, 
sim time next is 755400.0000, 
raw observation next is [-3.716666666666666, 55.66666666666667, 0.0, 0.0, 22.5, 27.12923139387754, 0.7929207874957594, 1.0, 1.0, 65.0, 42975.26083421744], 
processed observation next is [1.0, 0.7391304347826086, 0.3596491228070176, 0.5566666666666668, 0.0, 0.0, 0.375, 0.7607692828231283, 0.7643069291652531, 1.0, 1.0, 1.0, 0.20464409921055926], 
reward next is 0.7954, 
noisyNet noise sample is [array([-0.4736041], dtype=float32), 0.0019122487]. 
=============================================
[2019-04-08 15:42:18,328] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.7937907e-17 1.4711768e-11 1.9294182e-08 5.3008907e-08 1.2963079e-10
 8.9039567e-13 7.0751174e-09 1.0779630e-15 8.0603865e-19 5.8494214e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:18,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3953
[2019-04-08 15:42:18,340] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 71.0, 106.1666666666667, 0.0, 22.5, 26.97031602653001, 0.6720869796592196, 1.0, 1.0, 65.0, 38561.54283832622], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 818400.0000, 
sim time next is 819000.0000, 
raw observation next is [-4.5, 71.0, 110.0, 0.0, 22.5, 27.00092530348169, 0.6763547358842641, 1.0, 1.0, 65.0, 37768.51583299538], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.36666666666666664, 0.0, 0.375, 0.7500771086234742, 0.7254515786280881, 1.0, 1.0, 1.0, 0.17985007539521608], 
reward next is 0.8201, 
noisyNet noise sample is [array([-1.3012294], dtype=float32), -0.037230086]. 
=============================================
[2019-04-08 15:42:18,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.77133]
 [78.65038]
 [78.55898]
 [78.46681]
 [78.36766]], R is [[78.85463715]
 [78.88246918]
 [78.90847778]
 [78.93386078]
 [78.95217896]].
[2019-04-08 15:42:18,501] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.64810611e-15 2.25709715e-10 1.13815844e-07 2.01113423e-07
 2.40695730e-09 5.03983411e-13 1.71129422e-08 1.46126384e-13
 2.97347640e-17 1.95366257e-19 9.99999642e-01], sum to 1.0000
[2019-04-08 15:42:18,501] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4413
[2019-04-08 15:42:18,521] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.300000000000001, 73.66666666666667, 0.0, 0.0, 19.0, 26.07994813680822, 0.5241292167580304, 0.0, 1.0, 65.0, 53512.54015266618], 
current ob forecast is [], 
actual action is [-2.3000000000000007, 65.0], 
sim time this is 793200.0000, 
sim time next is 793800.0000, 
raw observation next is [-7.3, 73.0, 0.0, 0.0, 19.0, 26.03964301099032, 0.5226128889901173, 0.0, 1.0, 65.0, 54076.06593942887], 
processed observation next is [1.0, 0.17391304347826086, 0.26038781163434904, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6699702509158602, 0.6742042963300391, 0.0, 1.0, 1.0, 0.25750507590204225], 
reward next is 0.7425, 
noisyNet noise sample is [array([1.6370019], dtype=float32), -0.13922584]. 
=============================================
[2019-04-08 15:42:18,689] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.9729069e-17 1.4337854e-09 1.5884998e-06 4.3797669e-07 3.7044487e-10
 3.8868114e-12 9.9754068e-08 3.3090884e-14 1.1732205e-17 4.7470587e-19
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:42:18,695] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-08 15:42:18,710] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 19.0, 25.9563938787144, 0.5045440657545739, 0.0, 1.0, 65.0, 55559.00078720091], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 797400.0000, 
sim time next is 798000.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 25.95452940311344, 0.5029563232512492, 0.0, 1.0, 65.0, 54918.67502056562], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6628774502594533, 0.6676521077504164, 0.0, 1.0, 1.0, 0.2615175000979315], 
reward next is 0.7385, 
noisyNet noise sample is [array([-0.08125725], dtype=float32), -1.287696]. 
=============================================
[2019-04-08 15:42:18,720] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.17558 ]
 [77.197266]
 [77.17937 ]
 [77.18258 ]
 [77.18213 ]], R is [[77.1491394 ]
 [77.11308289]
 [77.07588959]
 [77.04222107]
 [77.01794434]].
[2019-04-08 15:42:19,128] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.6884599e-16 1.7482018e-09 5.5028636e-07 8.9819406e-07 3.9870060e-10
 1.0888574e-12 2.0851695e-08 8.0640566e-14 2.5817826e-17 8.7979178e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:42:19,131] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0769
[2019-04-08 15:42:19,150] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 63.0, 0.0, 0.0, 19.0, 26.33447235746015, 0.644122650287303, 0.0, 1.0, 65.0, 53677.08907608312], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 769200.0000, 
sim time next is 769800.0000, 
raw observation next is [-6.1, 63.5, 0.0, 0.0, 19.0, 26.3108843998809, 0.6397730768653037, 0.0, 1.0, 65.0, 53940.71550802851], 
processed observation next is [1.0, 0.9130434782608695, 0.29362880886426596, 0.635, 0.0, 0.0, 0.08333333333333333, 0.6925736999900751, 0.7132576922884346, 0.0, 1.0, 1.0, 0.256860550038231], 
reward next is 0.7431, 
noisyNet noise sample is [array([0.68305427], dtype=float32), 1.3547415]. 
=============================================
[2019-04-08 15:42:19,246] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0137615e-16 4.7205007e-10 1.3374357e-06 8.2243823e-06 2.5681088e-09
 3.4507556e-13 8.0951095e-09 1.4957633e-13 1.6933488e-16 6.4435209e-19
 9.9999046e-01], sum to 1.0000
[2019-04-08 15:42:19,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6878
[2019-04-08 15:42:19,273] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.466666666666667, 71.0, 0.0, 0.0, 19.0, 26.14962085683462, 0.5731022617242091, 0.0, 1.0, 65.0, 53327.01956105851], 
current ob forecast is [], 
actual action is [-2.466666666666667, 65.0], 
sim time this is 782400.0000, 
sim time next is 783000.0000, 
raw observation next is [-7.55, 71.0, 0.0, 0.0, 19.0, 26.15307205568984, 0.5709749153632473, 0.0, 1.0, 65.0, 53173.34011723401], 
processed observation next is [1.0, 0.043478260869565216, 0.25346260387811637, 0.71, 0.0, 0.0, 0.08333333333333333, 0.6794226713074867, 0.690324971787749, 0.0, 1.0, 1.0, 0.2532063815106381], 
reward next is 0.7468, 
noisyNet noise sample is [array([-2.09362], dtype=float32), 0.33089867]. 
=============================================
[2019-04-08 15:42:19,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.53277]
 [75.53962]
 [75.6436 ]
 [75.66189]
 [75.65792]], R is [[75.70989227]
 [75.69886017]
 [75.69001007]
 [75.68280029]
 [75.67986298]].
[2019-04-08 15:42:20,056] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.46945868e-17 1.22359284e-11 6.96444644e-08 2.60964242e-07
 1.69862509e-11 1.13584970e-13 2.42524312e-09 1.13149854e-14
 2.18543848e-18 3.17010184e-20 9.99999642e-01], sum to 1.0000
[2019-04-08 15:42:20,056] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8282
[2019-04-08 15:42:20,076] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 79.0, 95.0, 0.0, 22.5, 26.82075267054916, 0.6638422664066655, 1.0, 1.0, 65.0, 40559.30448356787], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 824400.0000, 
sim time next is 825000.0000, 
raw observation next is [-4.4, 79.00000000000001, 93.66666666666666, 0.0, 22.5, 26.82013337362212, 0.6725344436525794, 1.0, 1.0, 65.0, 41075.62973555187], 
processed observation next is [1.0, 0.5652173913043478, 0.3407202216066482, 0.7900000000000001, 0.3122222222222222, 0.0, 0.375, 0.73501111446851, 0.7241781478841931, 1.0, 1.0, 1.0, 0.19559823683596128], 
reward next is 0.8044, 
noisyNet noise sample is [array([0.26261464], dtype=float32), -0.43466637]. 
=============================================
[2019-04-08 15:42:20,090] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.77087 ]
 [79.74375 ]
 [79.71746 ]
 [79.675125]
 [79.74351 ]], R is [[79.82987976]
 [79.83843994]
 [79.85258484]
 [79.8589325 ]
 [79.82779694]].
[2019-04-08 15:42:21,018] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6890030e-16 4.1246415e-09 4.6298251e-06 5.6047879e-06 3.7647138e-10
 3.0409497e-12 2.2005935e-07 7.4576565e-13 4.1479038e-18 2.5654986e-19
 9.9998951e-01], sum to 1.0000
[2019-04-08 15:42:21,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3237
[2019-04-08 15:42:21,039] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.066666666666666, 72.33333333333333, 90.83333333333333, 0.0, 22.5, 26.86590654765438, 0.6426782361395207, 1.0, 1.0, 65.0, 40657.31607423039], 
current ob forecast is [], 
actual action is [-0.06666666666666643, 65.0], 
sim time this is 816000.0000, 
sim time next is 816600.0000, 
raw observation next is [-4.783333333333333, 71.66666666666667, 94.66666666666666, 0.0, 22.5, 26.88265227213079, 0.653105750326766, 1.0, 1.0, 65.0, 40392.96890476644], 
processed observation next is [1.0, 0.43478260869565216, 0.3301015697137581, 0.7166666666666667, 0.31555555555555553, 0.0, 0.375, 0.740221022677566, 0.7177019167755887, 1.0, 1.0, 1.0, 0.1923474709750783], 
reward next is 0.8077, 
noisyNet noise sample is [array([0.9285452], dtype=float32), 1.6930166]. 
=============================================
[2019-04-08 15:42:21,640] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9341478e-17 1.2298624e-09 1.8175818e-06 3.6211674e-07 1.2658222e-10
 2.3061834e-11 8.4269324e-08 5.1433687e-15 2.5975270e-17 3.7495477e-20
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:42:21,650] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4565
[2019-04-08 15:42:21,687] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8999999999999999, 74.0, 0.0, 0.0, 19.0, 26.25041169992909, 0.5664960145674504, 0.0, 1.0, 65.0, 50118.41208758055], 
current ob forecast is [], 
actual action is [4.1, 65.0], 
sim time this is 880200.0000, 
sim time next is 880800.0000, 
raw observation next is [-0.8, 73.33333333333334, 0.0, 0.0, 19.0, 26.23539694802021, 0.5615213110344935, 0.0, 1.0, 65.0, 49550.74636940395], 
processed observation next is [1.0, 0.17391304347826086, 0.4404432132963989, 0.7333333333333334, 0.0, 0.0, 0.08333333333333333, 0.6862830790016842, 0.6871737703448312, 0.0, 1.0, 1.0, 0.23595593509239976], 
reward next is 0.7640, 
noisyNet noise sample is [array([1.0677502], dtype=float32), 1.0715393]. 
=============================================
[2019-04-08 15:42:21,874] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7079097e-17 3.3228595e-10 2.1515002e-07 1.0832082e-05 8.9218755e-10
 1.0700804e-12 3.6288082e-08 4.8991823e-14 1.0744635e-19 1.6932781e-20
 9.9998891e-01], sum to 1.0000
[2019-04-08 15:42:21,875] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5463
[2019-04-08 15:42:21,888] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 26.45696702947839, 0.579410814415752, 1.0, 1.0, 65.0, 45377.78603255877], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 26.39822105977337, 0.5756597475507735, 1.0, 1.0, 65.0, 46949.43323103065], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.6998517549811142, 0.6918865825169246, 1.0, 1.0, 1.0, 0.2235687296715745], 
reward next is 0.7764, 
noisyNet noise sample is [array([0.22019327], dtype=float32), 0.22047697]. 
=============================================
[2019-04-08 15:42:22,933] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9288177e-17 1.6087122e-10 1.2340508e-08 1.4731616e-08 9.8259484e-11
 1.5942677e-13 5.7617591e-09 4.2481033e-14 1.3649653e-18 3.0472762e-19
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:22,933] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1164
[2019-04-08 15:42:22,952] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 79.5, 0.0, 0.0, 19.0, 26.24858175000817, 0.5790154003600122, 0.0, 1.0, 65.0, 49362.71076163082], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 869400.0000, 
sim time next is 870000.0000, 
raw observation next is [-1.9, 79.33333333333334, 0.0, 0.0, 19.0, 26.29930921342515, 0.5669224744527027, 0.0, 1.0, 65.0, 48844.2500631293], 
processed observation next is [1.0, 0.043478260869565216, 0.4099722991689751, 0.7933333333333334, 0.0, 0.0, 0.08333333333333333, 0.6916091011187625, 0.6889741581509009, 0.0, 1.0, 1.0, 0.2325916669672824], 
reward next is 0.7674, 
noisyNet noise sample is [array([1.9335872], dtype=float32), -0.019478172]. 
=============================================
[2019-04-08 15:42:22,959] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.73022 ]
 [80.52703 ]
 [80.535126]
 [80.63707 ]
 [80.6616  ]], R is [[80.63904572]
 [80.59759521]
 [80.56111908]
 [80.52359772]
 [80.48603058]].
[2019-04-08 15:42:23,979] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3405500e-17 2.0854329e-11 1.1867904e-06 6.9057087e-08 8.8104142e-11
 1.2306212e-13 1.9607279e-07 5.7944427e-15 3.0170381e-18 7.8286990e-20
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:42:23,988] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8278
[2019-04-08 15:42:24,002] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 19.0, 26.39465467929805, 0.5720732182926277, 0.0, 1.0, 65.0, 46113.63221033836], 
current ob forecast is [], 
actual action is [4.7, 65.0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 19.0, 26.33170096523063, 0.5672377407724861, 0.0, 1.0, 65.0, 48142.96700695487], 
processed observation next is [1.0, 0.21739130434782608, 0.4570637119113574, 0.72, 0.0, 0.0, 0.08333333333333333, 0.6943084137692193, 0.689079246924162, 0.0, 1.0, 1.0, 0.2292522238426422], 
reward next is 0.7707, 
noisyNet noise sample is [array([0.65124714], dtype=float32), -0.23373552]. 
=============================================
[2019-04-08 15:42:24,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0476617e-18 3.3052464e-11 6.7240098e-09 1.2306872e-08 6.8710752e-11
 4.4836578e-14 5.4734044e-09 2.3625763e-15 4.1238709e-18 1.1069590e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:24,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0433
[2019-04-08 15:42:24,171] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.55, 76.0, 29.0, 0.0, 22.5, 26.64972592828052, 0.636914338432866, 1.0, 1.0, 65.0, 37552.29328532371], 
current ob forecast is [], 
actual action is [5.55, 65.0], 
sim time this is 894600.0000, 
sim time next is 895200.0000, 
raw observation next is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 22.5, 26.84594002870368, 0.654650674735627, 1.0, 1.0, 65.00000000000003, 35500.52992749248], 
processed observation next is [1.0, 0.34782608695652173, 0.4829178208679595, 0.7733333333333333, 0.10722222222222219, 0.0, 0.375, 0.73716166905864, 0.7182168915785424, 1.0, 1.0, 1.0000000000000007, 0.16905014251186892], 
reward next is 0.8309, 
noisyNet noise sample is [array([0.10889498], dtype=float32), -0.07031828]. 
=============================================
[2019-04-08 15:42:24,345] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9295218e-17 1.2124270e-09 6.5795611e-08 2.4301254e-08 1.9716160e-12
 8.9147719e-15 2.7199183e-09 7.1510947e-16 6.5062169e-20 5.0855346e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:24,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1378
[2019-04-08 15:42:24,359] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 86.0, 14.5, 0.0, 22.5, 27.1067421581275, 0.7165029345964733, 1.0, 1.0, 65.0, 37975.93495315689], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 838800.0000, 
sim time next is 839400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 22.5, 27.09035180759498, 0.7161282519302711, 1.0, 1.0, 65.0, 38534.11699078308], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.375, 0.7575293172995817, 0.7387094173100904, 1.0, 1.0, 1.0, 0.18349579519420514], 
reward next is 0.8165, 
noisyNet noise sample is [array([-2.0155027], dtype=float32), -0.30143204]. 
=============================================
[2019-04-08 15:42:24,372] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9842256e-18 9.5041960e-12 2.6029940e-09 5.2698095e-09 3.4915202e-12
 2.1851864e-13 7.0327066e-10 1.7680574e-15 1.9560784e-20 9.8132509e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:24,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5998
[2019-04-08 15:42:24,414] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.8, 93.0, 96.0, 0.0, 22.5, 27.38812765602535, 0.8332106139090789, 1.0, 1.0, 65.0, 32419.47683201768], 
current ob forecast is [], 
actual action is [8.8, 65.0], 
sim time this is 912600.0000, 
sim time next is 913200.0000, 
raw observation next is [3.8, 93.0, 95.0, 0.0, 22.5, 27.46727617119011, 0.718115491057317, 1.0, 1.0, 65.0, 42618.37501824396], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.31666666666666665, 0.0, 0.375, 0.7889396809325092, 0.739371830352439, 1.0, 1.0, 1.0, 0.20294464294401887], 
reward next is 0.7971, 
noisyNet noise sample is [array([0.63488126], dtype=float32), 0.13955119]. 
=============================================
[2019-04-08 15:42:24,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8620083e-18 1.3556536e-10 9.5277155e-09 2.0288356e-08 2.3062899e-11
 2.8039677e-13 5.9484679e-09 1.2978088e-15 5.9894429e-20 4.2685888e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:24,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2654
[2019-04-08 15:42:24,993] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.7, 97.0, 100.5, 0.0, 22.5, 27.40447848483724, 0.7937264176444119, 1.0, 1.0, 65.0, 31802.28279458009], 
current ob forecast is [], 
actual action is [7.7, 65.0], 
sim time this is 907200.0000, 
sim time next is 907800.0000, 
raw observation next is [2.883333333333334, 96.33333333333334, 101.6666666666667, 0.0, 22.5, 27.39681450232151, 0.8009456446892694, 1.0, 1.0, 65.0, 31699.29089986736], 
processed observation next is [1.0, 0.5217391304347826, 0.5424746075715605, 0.9633333333333334, 0.338888888888889, 0.0, 0.375, 0.7830678751934593, 0.7669818815630899, 1.0, 1.0, 1.0, 0.15094900428508265], 
reward next is 0.8491, 
noisyNet noise sample is [array([0.04232801], dtype=float32), 0.40485737]. 
=============================================
[2019-04-08 15:42:26,638] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3759778e-18 6.8923822e-10 1.0393548e-09 8.8452898e-09 1.4254296e-11
 2.5404437e-15 2.1617108e-09 3.1194855e-16 7.5548869e-21 1.0226524e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:26,645] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9672
[2019-04-08 15:42:26,659] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.8, 100.0, 0.0, 0.0, 22.5, 26.88174000021367, 0.8174962715251614, 0.0, 1.0, 65.0, 41918.5996597544], 
current ob forecast is [], 
actual action is [9.8, 65.0], 
sim time this is 934800.0000, 
sim time next is 935400.0000, 
raw observation next is [4.9, 100.0, 0.0, 0.0, 22.5, 26.83405042387868, 0.8151889520465443, 1.0, 1.0, 65.0, 42149.14990740416], 
processed observation next is [1.0, 0.8260869565217391, 0.5983379501385043, 1.0, 0.0, 0.0, 0.375, 0.7361708686565566, 0.7717296506821815, 1.0, 1.0, 1.0, 0.2007102376543055], 
reward next is 0.7993, 
noisyNet noise sample is [array([0.77622783], dtype=float32), 0.16204648]. 
=============================================
[2019-04-08 15:42:27,820] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9320522e-18 1.3354048e-10 2.1162853e-07 9.3826152e-08 2.5586007e-11
 4.6676835e-14 1.2050061e-08 3.6808583e-15 2.3192341e-20 1.0358807e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:42:27,821] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7270
[2019-04-08 15:42:27,838] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.0, 83.0, 0.0, 0.0, 19.0, 27.22194101867252, 0.8552343898551039, 0.0, 1.0, 65.0, 32765.77454563978], 
current ob forecast is [], 
actual action is [14.0, 65.0], 
sim time this is 972600.0000, 
sim time next is 973200.0000, 
raw observation next is [9.200000000000001, 83.0, 0.0, 0.0, 19.0, 27.24304085674844, 0.8502318847294186, 0.0, 1.0, 65.0, 34003.70601012749], 
processed observation next is [1.0, 0.2608695652173913, 0.7174515235457064, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7702534047290367, 0.7834106282431396, 0.0, 1.0, 1.0, 0.16192240957203566], 
reward next is 0.8381, 
noisyNet noise sample is [array([0.47197017], dtype=float32), 0.08631403]. 
=============================================
[2019-04-08 15:42:28,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.2786915e-20 2.7450462e-11 8.2741941e-08 1.2255236e-07 2.2005119e-12
 4.4167025e-15 1.5036788e-08 1.4995921e-15 2.0302715e-19 6.7192420e-23
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:42:28,957] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7591
[2019-04-08 15:42:28,973] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 22.5, 27.98859820295849, 1.122077314960637, 1.0, 1.0, 65.0, 24399.13890993032], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1017600.0000, 
sim time next is 1018200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 22.5, 27.98834544065399, 1.12186745377177, 1.0, 1.0, 65.0, 23988.48230323087], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.375, 0.8323621200544992, 0.8739558179239234, 1.0, 1.0, 1.0, 0.11423086811062319], 
reward next is 0.8858, 
noisyNet noise sample is [array([0.07744726], dtype=float32), -1.3426056]. 
=============================================
[2019-04-08 15:42:29,062] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4692479e-18 1.5922853e-10 7.4059841e-08 8.8965258e-08 2.0536911e-11
 4.6832187e-14 2.0402984e-08 8.4827772e-16 9.8379964e-20 1.1252061e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:42:29,064] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5313
[2019-04-08 15:42:29,077] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.516666666666667, 80.33333333333334, 0.0, 0.0, 19.0, 26.97923620445219, 0.816716113510719, 0.0, 1.0, 65.0, 39137.02033073321], 
current ob forecast is [], 
actual action is [12.516666666666666, 65.0], 
sim time this is 960600.0000, 
sim time next is 961200.0000, 
raw observation next is [7.7, 80.0, 0.0, 0.0, 19.0, 26.93711551195494, 0.8230161626751095, 0.0, 1.0, 65.0, 40014.64204605568], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.8, 0.0, 0.0, 0.08333333333333333, 0.7447596259962449, 0.7743387208917031, 0.0, 1.0, 1.0, 0.19054591450502703], 
reward next is 0.8095, 
noisyNet noise sample is [array([1.2784065], dtype=float32), 1.7837932]. 
=============================================
[2019-04-08 15:42:29,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0454931e-19 6.2240726e-11 5.6264870e-08 3.5381846e-09 2.3142729e-12
 3.3364594e-15 3.1622863e-10 9.4903001e-16 1.5998279e-19 1.2390638e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:29,168] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3587
[2019-04-08 15:42:29,183] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.33333333333333, 92.66666666666667, 66.0, 0.0, 22.5, 27.93701097827141, 0.9939502020108693, 1.0, 1.0, 65.0, 22691.80812928069], 
current ob forecast is [], 
actual action is [15.33333333333333, 65.0], 
sim time this is 985200.0000, 
sim time next is 985800.0000, 
raw observation next is [10.41666666666667, 92.83333333333333, 72.0, 0.0, 22.5, 27.95368824310047, 1.005972160753657, 1.0, 1.0, 65.0, 23379.28844910821], 
processed observation next is [1.0, 0.391304347826087, 0.7511542012927056, 0.9283333333333332, 0.24, 0.0, 0.375, 0.8294740202583725, 0.8353240535845523, 1.0, 1.0, 1.0, 0.11132994499575338], 
reward next is 0.8887, 
noisyNet noise sample is [array([-0.99166566], dtype=float32), -0.5884696]. 
=============================================
[2019-04-08 15:42:29,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1410051e-20 1.4830472e-10 7.9951841e-09 6.1346967e-08 8.4420036e-13
 3.7701018e-16 2.5216224e-10 5.9574130e-17 5.7762919e-21 4.8955427e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:29,645] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7117
[2019-04-08 15:42:29,696] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 81.0, 81.66666666666667, 0.0, 22.5, 28.57838358862483, 0.9932384144507828, 1.0, 1.0, 65.0, 24138.24582682284], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1003800.0000, 
sim time next is 1004400.0000, 
raw observation next is [14.4, 81.0, 75.5, 0.0, 22.5, 27.64362851633229, 1.166131944413273, 1.0, 1.0, 65.0, 47549.54178743843], 
processed observation next is [1.0, 0.6521739130434783, 0.8614958448753465, 0.81, 0.25166666666666665, 0.0, 0.375, 0.8036357096943574, 0.8887106481377577, 1.0, 1.0, 1.0, 0.22642638946399252], 
reward next is 0.7736, 
noisyNet noise sample is [array([-1.1293002], dtype=float32), 1.7042798]. 
=============================================
[2019-04-08 15:42:29,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4113587e-19 2.1713523e-12 9.9706101e-09 2.1839835e-08 2.6375594e-12
 3.5570897e-17 4.5835225e-09 1.3873242e-16 4.8798490e-21 3.1179756e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:29,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0978
[2019-04-08 15:42:29,815] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.75441510533132, 1.076199055429346, 0.0, 1.0, 65.0, 25155.70616635578], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1038000.0000, 
sim time next is 1038600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.88503231746786, 1.067820885220839, 0.0, 1.0, 65.0, 22425.08096554588], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8237526931223217, 0.855940295073613, 0.0, 1.0, 1.0, 0.10678609983593276], 
reward next is 0.8932, 
noisyNet noise sample is [array([1.080725], dtype=float32), 0.31754383]. 
=============================================
[2019-04-08 15:42:29,864] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3228431e-18 5.2924561e-11 4.6667093e-08 2.9030181e-07 5.8137679e-11
 3.1786910e-14 2.0808562e-09 5.2923066e-14 4.3552710e-20 1.9359308e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:42:29,864] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2038
[2019-04-08 15:42:29,887] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.5, 89.0, 0.0, 0.0, 19.0, 26.7815671062322, 0.8040759481426417, 0.0, 1.0, 65.0, 42241.39754719174], 
current ob forecast is [], 
actual action is [10.5, 65.0], 
sim time this is 954000.0000, 
sim time next is 954600.0000, 
raw observation next is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 19.0, 26.80230126242343, 0.8047925627895265, 0.0, 1.0, 65.0, 41206.8304991761], 
processed observation next is [1.0, 0.043478260869565216, 0.6200369344413666, 0.8783333333333334, 0.0, 0.0, 0.08333333333333333, 0.7335251052019526, 0.7682641875965088, 0.0, 1.0, 1.0, 0.19622300237702905], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.36505076], dtype=float32), 0.6642083]. 
=============================================
[2019-04-08 15:42:29,994] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9335640e-18 3.2889913e-10 8.4010814e-08 6.0363853e-08 9.8491201e-11
 9.7657834e-15 2.4838462e-09 6.7051583e-16 3.7516160e-20 1.0673826e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:29,997] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9228
[2019-04-08 15:42:30,013] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.3, 75.5, 0.0, 0.0, 19.0, 27.8790764941164, 1.074288341725499, 0.0, 1.0, 65.0, 23506.81649099082], 
current ob forecast is [], 
actual action is [19.3, 65.0], 
sim time this is 1041000.0000, 
sim time next is 1041600.0000, 
raw observation next is [14.2, 76.0, 0.0, 0.0, 19.0, 27.89886725544971, 1.071718256633929, 0.0, 1.0, 65.0, 23282.01405171474], 
processed observation next is [1.0, 0.043478260869565216, 0.8559556786703602, 0.76, 0.0, 0.0, 0.08333333333333333, 0.8249056046208091, 0.8572394188779763, 0.0, 1.0, 1.0, 0.110866733579594], 
reward next is 0.8891, 
noisyNet noise sample is [array([0.68922037], dtype=float32), -0.74663264]. 
=============================================
[2019-04-08 15:42:30,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9780353e-20 1.1230571e-11 4.5831655e-08 7.4123157e-08 1.4854376e-12
 8.3813012e-15 2.1580336e-11 1.6459929e-16 3.6037949e-20 1.1902689e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:30,632] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6764
[2019-04-08 15:42:30,647] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1029135e-19 4.8650634e-13 9.0977430e-08 1.7995778e-07 3.7625805e-12
 4.7825030e-15 3.7524810e-09 3.0639500e-15 3.5905387e-20 1.4104682e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:42:30,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3043
[2019-04-08 15:42:30,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1038360e-18 3.9687243e-11 1.2469860e-08 9.6078317e-08 8.9950400e-13
 1.3185494e-13 2.6111340e-09 1.5631169e-15 3.7058864e-20 1.3983333e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:30,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0214
[2019-04-08 15:42:30,662] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 31.5, 0.0, 22.5, 27.81841675895908, 1.041813861563634, 1.0, 1.0, 65.0, 30382.61221377956], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1328400.0000, 
sim time next is 1329000.0000, 
raw observation next is [0.5, 92.0, 36.0, 0.0, 22.5, 27.79823771308565, 1.037916038046247, 1.0, 1.0, 65.0, 30837.20157603143], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.12, 0.0, 0.375, 0.8165198094238043, 0.8459720126820823, 1.0, 1.0, 1.0, 0.1468438170287211], 
reward next is 0.8532, 
noisyNet noise sample is [array([-0.38292253], dtype=float32), 1.3102658]. 
=============================================
[2019-04-08 15:42:30,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[88.61642]
 [88.84648]
 [89.03838]
 [89.3371 ]
 [89.32437]], R is [[88.35922241]
 [88.33095551]
 [88.31095123]
 [88.29750061]
 [88.26576233]].
[2019-04-08 15:42:30,666] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 80.33333333333333, 0.0, 0.0, 22.5, 27.14941247491863, 1.09550973820341, 1.0, 1.0, 65.0, 60214.93438410194], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1019400.0000, 
sim time next is 1020000.0000, 
raw observation next is [14.4, 79.66666666666667, 0.0, 0.0, 22.5, 26.26860121005854, 0.9508674369930015, 1.0, 1.0, 65.0, 37945.3523709664], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7966666666666667, 0.0, 0.0, 0.375, 0.6890501008382118, 0.8169558123310005, 1.0, 1.0, 1.0, 0.18069215414745904], 
reward next is 0.8193, 
noisyNet noise sample is [array([0.44919357], dtype=float32), -0.7316441]. 
=============================================
[2019-04-08 15:42:30,671] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.5, 75.0, 41.0, 0.0, 22.5, 28.6916621522229, 1.182992706556249, 1.0, 1.0, 65.0, 18849.44491182255], 
current ob forecast is [], 
actual action is [20.5, 65.0], 
sim time this is 1008000.0000, 
sim time next is 1008600.0000, 
raw observation next is [15.5, 75.5, 35.66666666666666, 0.0, 22.5, 28.66214850095304, 1.190921188023255, 1.0, 1.0, 65.0, 18849.29285153338], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.755, 0.11888888888888886, 0.0, 0.375, 0.88851237507942, 0.896973729341085, 1.0, 1.0, 1.0, 0.08975853738825419], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.5448072], dtype=float32), 0.7928629]. 
=============================================
[2019-04-08 15:42:30,684] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[90.903534]
 [90.71033 ]
 [91.0403  ]
 [91.03743 ]
 [91.06526 ]], R is [[90.68825531]
 [90.49463654]
 [90.40514374]
 [90.38686371]
 [90.36681366]].
[2019-04-08 15:42:30,780] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9322537e-20 1.2968840e-10 1.7268566e-07 2.8021244e-08 1.9660055e-12
 4.5616014e-14 3.2638811e-10 9.7964046e-17 2.5772453e-20 9.5541229e-23
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:42:30,784] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9922
[2019-04-08 15:42:30,809] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.5, 77.5, 16.66666666666667, 0.0, 22.5, 27.26681180126782, 1.094426133479367, 1.0, 1.0, 65.0, 19519.32085911783], 
current ob forecast is [], 
actual action is [20.5, 65.0], 
sim time this is 1011000.0000, 
sim time next is 1011600.0000, 
raw observation next is [15.5, 78.0, 12.5, 0.0, 22.5, 28.50926764203356, 1.174776139889429, 1.0, 1.0, 65.0, 18847.09258390724], 
processed observation next is [1.0, 0.7391304347826086, 0.8919667590027703, 0.78, 0.041666666666666664, 0.0, 0.375, 0.8757723035027967, 0.8915920466298096, 1.0, 1.0, 1.0, 0.08974805992336782], 
reward next is 0.9103, 
noisyNet noise sample is [array([-0.88764733], dtype=float32), -0.94017154]. 
=============================================
[2019-04-08 15:42:31,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.57457698e-19 1.14379575e-10 2.77740888e-08 1.12061915e-09
 1.59874031e-10 1.42320861e-13 1.54159652e-09 2.76874450e-16
 1.64062460e-19 2.74360774e-22 1.00000000e+00], sum to 1.0000
[2019-04-08 15:42:31,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4415
[2019-04-08 15:42:31,021] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 92.0, 110.3333333333333, 0.0, 22.5, 28.10232035172272, 1.106517840483778, 1.0, 1.0, 65.0, 26190.67573141968], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 1335000.0000, 
sim time next is 1335600.0000, 
raw observation next is [1.1, 92.0, 114.5, 0.0, 22.5, 28.1256447396709, 1.110971914540902, 1.0, 1.0, 65.0, 25656.04101391546], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.38166666666666665, 0.0, 0.375, 0.8438037283059083, 0.8703239715136339, 1.0, 1.0, 1.0, 0.12217162387578791], 
reward next is 0.8778, 
noisyNet noise sample is [array([-0.3740073], dtype=float32), -0.55499417]. 
=============================================
[2019-04-08 15:42:31,045] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8123587e-19 5.8062837e-13 1.7237521e-09 6.5799597e-09 1.3455175e-13
 9.3746608e-15 6.6558904e-11 6.7975876e-17 3.2687463e-22 3.9654219e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:31,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6831
[2019-04-08 15:42:31,059] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.58333333333333, 80.5, 0.0, 0.0, 22.5, 28.42660530825174, 1.153985432608718, 1.0, 1.0, 65.0, 18847.64822945271], 
current ob forecast is [], 
actual action is [19.58333333333333, 65.0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 22.5, 28.37916941845244, 1.139888873517129, 1.0, 1.0, 65.0, 18847.18639731103], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.375, 0.8649307848710368, 0.879962957839043, 1.0, 1.0, 1.0, 0.08974850665386205], 
reward next is 0.9103, 
noisyNet noise sample is [array([1.3765767], dtype=float32), -0.5191967]. 
=============================================
[2019-04-08 15:42:31,795] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1525568e-19 4.4285804e-11 1.2947574e-08 1.1279604e-07 2.8374636e-13
 9.3995831e-15 4.7036059e-09 4.8413317e-16 2.3427270e-20 4.3915702e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:42:31,796] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3029
[2019-04-08 15:42:31,816] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 22.5, 28.24889876599954, 1.144044327453865, 1.0, 1.0, 65.0, 18845.88967470847], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1015800.0000, 
sim time next is 1016400.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 22.5, 28.21127421202315, 1.119713468278368, 1.0, 1.0, 65.0, 18844.89393434339], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.375, 0.8509395176685958, 0.873237822759456, 1.0, 1.0, 1.0, 0.08973759016353995], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.74541426], dtype=float32), 0.13937294]. 
=============================================
[2019-04-08 15:42:31,870] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7291158e-17 5.0347560e-12 2.6999684e-08 5.4858105e-09 2.7518895e-10
 8.2586092e-14 1.2291663e-09 4.9655057e-17 3.0869140e-20 5.3834367e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:42:31,879] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4522
[2019-04-08 15:42:31,891] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.2, 76.0, 0.0, 0.0, 19.0, 27.8988775550577, 1.071721166104709, 0.0, 1.0, 65.0, 23281.90095356195], 
current ob forecast is [], 
actual action is [19.2, 65.0], 
sim time this is 1041600.0000, 
sim time next is 1042200.0000, 
raw observation next is [14.1, 76.5, 0.0, 0.0, 19.0, 27.88947201807542, 1.069141571627387, 0.0, 1.0, 65.0, 24369.31633118623], 
processed observation next is [1.0, 0.043478260869565216, 0.8531855955678671, 0.765, 0.0, 0.0, 0.08333333333333333, 0.8241226681729517, 0.8563805238757958, 0.0, 1.0, 1.0, 0.11604436348183919], 
reward next is 0.8840, 
noisyNet noise sample is [array([-0.96578264], dtype=float32), -1.099765]. 
=============================================
[2019-04-08 15:42:32,036] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-08 15:42:32,036] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:42:32,036] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:42:32,038] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:42:32,039] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:42:32,040] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:42:32,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:42:32,044] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run16
[2019-04-08 15:42:32,087] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run16
[2019-04-08 15:42:32,114] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run16
[2019-04-08 15:42:56,490] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06244296]
[2019-04-08 15:42:56,491] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-2.8, 83.66666666666667, 0.0, 0.0, 19.0, 26.37117908663759, 0.6097581131924257, 0.0, 1.0, 65.0, 51489.53514174096]
[2019-04-08 15:42:56,491] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:42:56,491] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [1.25287370e-16 5.30699318e-10 2.07838838e-07 3.96957830e-07
 2.25650693e-10 9.65875383e-13 3.14209245e-08 5.94385584e-14
 1.25471925e-17 1.06407344e-19 9.99999404e-01], sampled 0.4815071295629859
[2019-04-08 15:43:16,678] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06244296]
[2019-04-08 15:43:16,678] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [2.3, 78.66666666666667, 0.0, 0.0, 19.0, 27.44076338246352, 1.022267973546858, 0.0, 1.0, 65.0, 28280.35377481017]
[2019-04-08 15:43:16,678] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:43:16,679] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [2.21211240e-17 2.13632070e-10 9.10559024e-08 1.56200542e-07
 6.95165661e-11 2.80590384e-13 1.11398535e-08 1.34701617e-14
 2.26126364e-18 1.27388852e-20 9.99999762e-01], sampled 0.7476702845915614
[2019-04-08 15:44:05,416] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06244296]
[2019-04-08 15:44:05,417] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [21.83333333333334, 44.0, 188.8333333333333, 600.5, 22.5, 30.34327278981262, 1.793597199616456, 1.0, 1.0, 65.0, 0.0]
[2019-04-08 15:44:05,417] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:44:05,417] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [9.9723996e-20 2.0421569e-11 1.3310455e-08 1.6920932e-08 2.7383833e-12
 5.6298172e-15 6.7048089e-10 1.9698605e-16 1.2703221e-20 4.8968506e-23
 1.0000000e+00], sampled 0.3164997979080515
[2019-04-08 15:44:10,979] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.4974 316155535.9166 2958.1653
[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,001] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:11,111] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,301] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.0306 355943583.6214 2370.6695
[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,323] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:17,433] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,070] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.4921 342826658.0633 2768.0391
[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,090] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:21,210] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:44:22,093] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 300000, evaluation results [300000.0, 6863.49210446026, 342826658.06334865, 2768.0391246356376, 6990.497448015987, 316155535.9166481, 2958.1652844170026, 6801.030554183986, 355943583.62136, 2370.669476478192]
[2019-04-08 15:44:22,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8897083e-19 6.9798545e-10 3.6606529e-08 1.9196287e-08 9.8047698e-12
 2.4007681e-13 2.6252567e-09 3.7242466e-16 9.0238224e-21 2.4981405e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:22,139] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9872
[2019-04-08 15:44:22,161] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.74624944351221, 1.052674966722583, 0.0, 1.0, 65.0, 25744.99577631659], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1030200.0000, 
sim time next is 1030800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.74466416202797, 1.05530864517958, 0.0, 1.0, 65.0, 25899.83362570739], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8120553468356642, 0.8517695483931934, 0.0, 1.0, 1.0, 0.12333254107479709], 
reward next is 0.8767, 
noisyNet noise sample is [array([0.15991133], dtype=float32), 1.3739444]. 
=============================================
[2019-04-08 15:44:22,312] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.0995488e-19 2.7512712e-13 1.3766698e-08 1.0453998e-07 3.3101146e-12
 4.0069063e-15 3.4187511e-10 5.1308329e-17 2.9137586e-20 1.2023794e-24
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:22,313] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4093
[2019-04-08 15:44:22,358] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.5, 76.5, 25.0, 0.0, 22.5, 28.71544285850139, 1.027609932555864, 1.0, 1.0, 65.0, 23659.36453978639], 
current ob forecast is [], 
actual action is [20.5, 65.0], 
sim time this is 1009800.0000, 
sim time next is 1010400.0000, 
raw observation next is [15.5, 77.0, 20.83333333333334, 0.0, 22.5, 27.87701261982306, 1.213323111389323, 1.0, 1.0, 65.0, 46892.43036115597], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.77, 0.06944444444444446, 0.0, 0.375, 0.8230843849852549, 0.9044410371297743, 1.0, 1.0, 1.0, 0.22329728743407606], 
reward next is 0.7767, 
noisyNet noise sample is [array([0.6969224], dtype=float32), 0.37174731]. 
=============================================
[2019-04-08 15:44:22,400] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.05867091e-17 1.07195985e-09 1.04032270e-05 1.58291755e-07
 1.05819624e-10 9.79733742e-12 2.14736353e-08 2.29809881e-14
 2.99903831e-20 3.93789283e-20 9.99989390e-01], sum to 1.0000
[2019-04-08 15:44:22,406] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7998
[2019-04-08 15:44:22,419] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 19.0, 27.88504583223381, 1.067824752361096, 0.0, 1.0, 65.0, 22424.93019447989], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1038600.0000, 
sim time next is 1039200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 19.0, 27.88211532255987, 1.0655432560194, 0.0, 1.0, 65.0, 23997.04116590511], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.08333333333333333, 0.8235096102133225, 0.8551810853398001, 0.0, 1.0, 1.0, 0.11427162459954814], 
reward next is 0.8857, 
noisyNet noise sample is [array([-1.1494395], dtype=float32), -0.14140207]. 
=============================================
[2019-04-08 15:44:23,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7297913e-17 1.7838095e-09 3.6266464e-08 3.2496791e-07 1.4603492e-10
 1.3295705e-12 7.9867412e-08 9.7791214e-16 5.8476911e-17 2.9275731e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:44:23,255] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6462
[2019-04-08 15:44:23,269] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.0, 77.0, 0.0, 0.0, 19.0, 27.85653303106999, 1.062570686678013, 0.0, 1.0, 65.0, 25346.55451984759], 
current ob forecast is [], 
actual action is [19.0, 65.0], 
sim time this is 1042800.0000, 
sim time next is 1043400.0000, 
raw observation next is [13.9, 77.5, 0.0, 0.0, 19.0, 27.79707012312211, 1.064265220080304, 0.0, 1.0, 65.0, 26556.75610759163], 
processed observation next is [1.0, 0.043478260869565216, 0.847645429362881, 0.775, 0.0, 0.0, 0.08333333333333333, 0.816422510260176, 0.8547550733601014, 0.0, 1.0, 1.0, 0.12646074336948396], 
reward next is 0.8735, 
noisyNet noise sample is [array([1.2009609], dtype=float32), 0.014230234]. 
=============================================
[2019-04-08 15:44:23,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1556717e-19 1.0025305e-11 8.4722345e-09 8.0984304e-09 2.7959119e-11
 6.0757766e-15 3.9110244e-09 3.9223354e-16 4.0660212e-21 1.1783955e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:23,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7852
[2019-04-08 15:44:23,693] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.4, 75.0, 114.0, 0.0, 22.5, 28.94192948483557, 1.279289929241177, 1.0, 1.0, 65.0, 18849.17043032084], 
current ob forecast is [], 
actual action is [19.4, 65.0], 
sim time this is 1074600.0000, 
sim time next is 1075200.0000, 
raw observation next is [14.76666666666667, 73.33333333333334, 137.3333333333333, 35.83333333333333, 22.5, 28.97366300921609, 1.295548711641725, 1.0, 1.0, 65.0, 18849.32570531819], 
processed observation next is [1.0, 0.43478260869565216, 0.8716528162511544, 0.7333333333333334, 0.4577777777777776, 0.03959484346224677, 0.375, 0.9144719174346742, 0.9318495705472417, 1.0, 1.0, 1.0, 0.08975869383484852], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.68030244], dtype=float32), -0.94298124]. 
=============================================
[2019-04-08 15:44:23,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1209704e-20 3.9742452e-11 4.1632896e-07 2.8660912e-08 1.7654712e-12
 1.0577991e-14 9.5144621e-11 1.6354213e-16 6.5013977e-21 2.0256165e-24
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:44:23,717] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0268
[2019-04-08 15:44:23,732] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.36666666666667, 52.5, 0.0, 0.0, 22.5, 28.95723168981475, 1.398842316676917, 1.0, 1.0, 65.0, 18849.27787546058], 
current ob forecast is [], 
actual action is [21.36666666666667, 65.0], 
sim time this is 1101000.0000, 
sim time next is 1101600.0000, 
raw observation next is [16.1, 53.0, 0.0, 0.0, 22.5, 29.00041519756963, 1.382520091198103, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.9085872576177286, 0.53, 0.0, 0.0, 0.375, 0.9167012664641359, 0.9608400303993676, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47925362], dtype=float32), -1.3932505]. 
=============================================
[2019-04-08 15:44:23,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9916896e-18 5.2168718e-09 1.9169430e-07 3.4669901e-07 1.4738226e-10
 1.0549672e-12 2.3840398e-08 1.7146611e-13 1.0361711e-17 8.2126711e-22
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:44:23,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5067
[2019-04-08 15:44:23,895] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.8, 63.66666666666666, 0.0, 0.0, 19.0, 28.1004152414313, 1.250138777513662, 0.0, 1.0, 65.0, 23083.63759910408], 
current ob forecast is [], 
actual action is [17.8, 65.0], 
sim time this is 1115400.0000, 
sim time next is 1116000.0000, 
raw observation next is [12.7, 64.0, 0.0, 0.0, 19.0, 28.0865064674126, 1.246494811707328, 0.0, 1.0, 65.0, 23421.42248306497], 
processed observation next is [1.0, 0.9565217391304348, 0.8144044321329641, 0.64, 0.0, 0.0, 0.08333333333333333, 0.8405422056177168, 0.9154982705691094, 0.0, 1.0, 1.0, 0.11153058325269033], 
reward next is 0.8885, 
noisyNet noise sample is [array([0.8729655], dtype=float32), -0.19008942]. 
=============================================
[2019-04-08 15:44:23,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.46047 ]
 [87.420715]
 [87.04718 ]
 [87.21995 ]
 [87.00558 ]], R is [[87.49378204]
 [87.50891876]
 [87.52440643]
 [87.53935242]
 [87.55522919]].
[2019-04-08 15:44:24,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9600651e-19 7.0292695e-11 2.2809504e-08 3.2345405e-08 1.7366669e-11
 1.2070316e-14 4.8177302e-09 1.4544356e-16 1.9385029e-20 1.4776007e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:24,666] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3992
[2019-04-08 15:44:24,673] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 22.5, 28.91011304372725, 1.38555992953395, 1.0, 1.0, 65.0, 18848.54925338488], 
current ob forecast is [], 
actual action is [20.91666666666667, 65.0], 
sim time this is 1102200.0000, 
sim time next is 1102800.0000, 
raw observation next is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 22.5, 28.88190552472637, 1.378210662508285, 1.0, 1.0, 65.0, 18849.20139561585], 
processed observation next is [1.0, 0.782608695652174, 0.8984302862419208, 0.5433333333333334, 0.0, 0.0, 0.375, 0.9068254603938642, 0.9594035541694282, 1.0, 1.0, 1.0, 0.08975810188388501], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.12297401], dtype=float32), 0.38955477]. 
=============================================
[2019-04-08 15:44:24,897] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0004397e-20 8.7789587e-10 1.3434402e-07 1.8517810e-07 1.2835459e-10
 8.2909929e-14 1.9433454e-08 7.6521321e-15 5.3612633e-21 4.9886160e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:24,900] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7061
[2019-04-08 15:44:24,922] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.1, 53.0, 0.0, 0.0, 22.5, 29.00040529288411, 1.382517382385372, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [21.1, 65.0], 
sim time this is 1101600.0000, 
sim time next is 1102200.0000, 
raw observation next is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 22.5, 28.9100967727087, 1.385555424486947, 1.0, 1.0, 65.0, 18848.54897192212], 
processed observation next is [1.0, 0.782608695652174, 0.9035087719298247, 0.5366666666666667, 0.0, 0.0, 0.375, 0.9091747310590584, 0.9618518081623156, 1.0, 1.0, 1.0, 0.08975499510439104], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.40361544], dtype=float32), 0.35781863]. 
=============================================
[2019-04-08 15:44:24,968] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2597560e-21 3.4036321e-13 5.0203003e-10 9.2480974e-09 5.4007189e-13
 2.2847394e-15 2.1644068e-11 1.1860064e-18 7.1338576e-21 4.9382274e-25
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:24,975] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5910
[2019-04-08 15:44:24,986] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.55, 55.0, 0.0, 0.0, 22.5, 28.85207937558355, 1.364619104587201, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [20.55, 65.0], 
sim time this is 1103400.0000, 
sim time next is 1104000.0000, 
raw observation next is [15.36666666666667, 55.66666666666667, 0.0, 0.0, 22.5, 28.77162761201966, 1.364621582858744, 1.0, 1.0, 65.0, 18849.16881108763], 
processed observation next is [1.0, 0.782608695652174, 0.8882733148661128, 0.5566666666666668, 0.0, 0.0, 0.375, 0.8976356343349717, 0.9548738609529147, 1.0, 1.0, 1.0, 0.08975794671946491], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.2175432], dtype=float32), 0.9705632]. 
=============================================
[2019-04-08 15:44:24,990] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[90.180725]
 [90.32576 ]
 [90.29165 ]
 [90.35365 ]
 [90.42827 ]], R is [[90.06005859]
 [90.15946198]
 [90.16810608]
 [90.17667389]
 [90.27490997]].
[2019-04-08 15:44:25,164] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1927292e-20 9.1063680e-13 4.7137103e-09 5.9796257e-09 5.0790179e-12
 2.3105063e-16 1.5960568e-09 2.8720826e-17 1.3282324e-20 8.0097352e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:25,171] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6465
[2019-04-08 15:44:25,214] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [19.4, 49.0, 108.0, 0.0, 22.5, 29.77213325814104, 1.441784689234951, 1.0, 1.0, 65.0, 16951.01613236378], 
current ob forecast is [], 
actual action is [24.4, 65.0], 
sim time this is 1091400.0000, 
sim time next is 1092000.0000, 
raw observation next is [19.4, 49.0, 100.0, 0.0, 22.5, 29.1170529018934, 1.552191203339209, 1.0, 1.0, 65.0, 30907.79643221183], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.3333333333333333, 0.0, 0.375, 0.9264210751577832, 1.0173970677797364, 1.0, 1.0, 1.0, 0.14717998301053253], 
reward next is 0.8528, 
noisyNet noise sample is [array([0.65991443], dtype=float32), 1.1069387]. 
=============================================
[2019-04-08 15:44:25,231] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.004364]
 [91.03756 ]
 [91.081764]
 [91.02208 ]
 [91.026924]], R is [[90.99123383]
 [91.00060272]
 [91.09059906]
 [91.17969513]
 [91.26789856]].
[2019-04-08 15:44:25,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7977012e-18 3.7913109e-10 3.2836968e-07 5.8764704e-07 4.0122721e-11
 1.0520412e-12 2.1019766e-09 1.4659758e-16 3.4135375e-20 3.5407102e-22
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:44:25,444] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6863
[2019-04-08 15:44:25,456] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.28333333333333, 65.66666666666666, 0.0, 0.0, 19.0, 28.02435984031866, 1.229844878895421, 0.0, 1.0, 65.0, 24251.9059028671], 
current ob forecast is [], 
actual action is [17.28333333333333, 65.0], 
sim time this is 1119000.0000, 
sim time next is 1119600.0000, 
raw observation next is [12.2, 66.0, 0.0, 0.0, 19.0, 28.01391321532476, 1.226697417070825, 0.0, 1.0, 65.0, 24401.32510340674], 
processed observation next is [1.0, 1.0, 0.8005540166204987, 0.66, 0.0, 0.0, 0.08333333333333333, 0.83449276794373, 0.9088991390236082, 0.0, 1.0, 1.0, 0.11619678620669877], 
reward next is 0.8838, 
noisyNet noise sample is [array([0.60834634], dtype=float32), -0.2859772]. 
=============================================
[2019-04-08 15:44:25,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1552519e-18 3.9238557e-10 7.5282441e-07 1.0936647e-07 8.2397613e-12
 3.6677272e-13 6.1478436e-09 6.8447864e-16 5.5420989e-18 1.8664232e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:44:25,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5328
[2019-04-08 15:44:25,698] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.7, 82.66666666666667, 0.0, 0.0, 19.0, 27.80728306700261, 1.132838637733816, 0.0, 1.0, 65.0, 26731.09508532346], 
current ob forecast is [], 
actual action is [17.7, 65.0], 
sim time this is 1150800.0000, 
sim time next is 1151400.0000, 
raw observation next is [12.7, 83.33333333333333, 11.0, 0.6666666666666667, 19.0, 27.81419596447763, 1.132872177612978, 0.0, 1.0, 65.0, 26544.64755498224], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8333333333333333, 0.03666666666666667, 0.0007366482504604052, 0.08333333333333333, 0.8178496637064692, 0.877624059204326, 0.0, 1.0, 1.0, 0.1264030835951535], 
reward next is 0.8736, 
noisyNet noise sample is [array([-0.09223954], dtype=float32), 0.3382717]. 
=============================================
[2019-04-08 15:44:25,906] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.24023387e-18 1.14809946e-10 2.40431657e-08 8.01242006e-09
 2.39953925e-11 4.49904328e-14 4.44067227e-09 4.08416925e-14
 9.74533326e-19 3.82625075e-21 1.00000000e+00], sum to 1.0000
[2019-04-08 15:44:25,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2218
[2019-04-08 15:44:25,920] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.3, 64.66666666666667, 9.666666666666664, 0.0, 19.0, 28.3732237632064, 1.27828923971418, 0.0, 1.0, 65.0, 18849.1242545601], 
current ob forecast is [], 
actual action is [23.3, 65.0], 
sim time this is 1185000.0000, 
sim time next is 1185600.0000, 
raw observation next is [18.3, 64.33333333333334, 0.0, 0.0, 19.0, 28.36609178468174, 1.276725990183768, 0.0, 1.0, 65.0, 18848.10051712214], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.6433333333333334, 0.0, 0.0, 0.08333333333333333, 0.8638409820568116, 0.9255753300612559, 0.0, 1.0, 1.0, 0.08975285960534353], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.13066967], dtype=float32), -2.7260318]. 
=============================================
[2019-04-08 15:44:26,101] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0195466e-18 2.2477270e-11 1.5398539e-09 1.4813574e-09 2.1633605e-13
 5.1465603e-15 8.8069822e-11 8.8037812e-17 1.0951112e-20 8.0568358e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:26,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7690
[2019-04-08 15:44:26,121] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.61666666666667, 64.33333333333334, 0.0, 0.0, 19.0, 28.07207055881685, 1.24312252138882, 0.0, 1.0, 65.0, 23660.74691449958], 
current ob forecast is [], 
actual action is [17.61666666666667, 65.0], 
sim time this is 1116600.0000, 
sim time next is 1117200.0000, 
raw observation next is [12.53333333333333, 64.66666666666667, 0.0, 0.0, 19.0, 28.05857780311574, 1.239703379711492, 0.0, 1.0, 65.0, 23804.45151051548], 
processed observation next is [1.0, 0.9565217391304348, 0.8097876269621421, 0.6466666666666667, 0.0, 0.0, 0.08333333333333333, 0.8382148169263116, 0.9132344599038307, 0.0, 1.0, 1.0, 0.11335453100245466], 
reward next is 0.8866, 
noisyNet noise sample is [array([-0.716393], dtype=float32), -0.521203]. 
=============================================
[2019-04-08 15:44:26,150] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.20239204e-17 2.21699829e-11 2.00572288e-08 1.92014340e-06
 1.25949362e-10 5.09695605e-14 1.77808240e-10 4.10381407e-15
 1.04772361e-18 1.15709385e-20 9.99998093e-01], sum to 1.0000
[2019-04-08 15:44:26,152] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6545
[2019-04-08 15:44:26,173] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.25, 78.0, 0.0, 0.0, 19.0, 27.85614717566841, 1.180390790055712, 0.0, 1.0, 65.0, 27017.84404108968], 
current ob forecast is [], 
actual action is [15.25, 65.0], 
sim time this is 1128600.0000, 
sim time next is 1129200.0000, 
raw observation next is [10.16666666666667, 78.33333333333333, 0.0, 0.0, 19.0, 27.85170765720124, 1.176557741059262, 0.0, 1.0, 65.0, 26986.28153959334], 
processed observation next is [0.0, 0.043478260869565216, 0.7442289935364729, 0.7833333333333333, 0.0, 0.0, 0.08333333333333333, 0.8209756381001032, 0.8921859136864206, 0.0, 1.0, 1.0, 0.1285061025694921], 
reward next is 0.8715, 
noisyNet noise sample is [array([0.18823814], dtype=float32), -0.91806895]. 
=============================================
[2019-04-08 15:44:26,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1676469e-18 4.5763793e-10 7.5599149e-07 2.3842940e-08 7.2654710e-10
 1.5446893e-12 5.7050894e-08 1.5277576e-14 4.7871287e-18 2.1239980e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:44:26,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3657
[2019-04-08 15:44:26,220] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.55, 78.0, 0.0, 0.0, 19.0, 27.79599397241184, 1.163200597269762, 0.0, 1.0, 65.0, 27896.25957993307], 
current ob forecast is [], 
actual action is [15.55, 65.0], 
sim time this is 1132200.0000, 
sim time next is 1132800.0000, 
raw observation next is [10.73333333333333, 77.66666666666666, 0.0, 0.0, 19.0, 27.79231710031042, 1.161325259173341, 0.0, 1.0, 65.0, 27932.73834374964], 
processed observation next is [0.0, 0.08695652173913043, 0.7599261311172669, 0.7766666666666666, 0.0, 0.0, 0.08333333333333333, 0.8160264250258683, 0.8871084197244471, 0.0, 1.0, 1.0, 0.13301303973214113], 
reward next is 0.8670, 
noisyNet noise sample is [array([0.77923334], dtype=float32), -0.7784242]. 
=============================================
[2019-04-08 15:44:27,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3705585e-18 3.3887312e-10 8.2820053e-09 2.1850279e-07 4.1630974e-12
 8.3518933e-14 9.3391179e-09 1.2654595e-15 6.5471749e-19 5.4894253e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:27,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8082
[2019-04-08 15:44:27,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.33262048948134, 1.26899670912087, 0.0, 1.0, 65.0, 18848.83015145153], 
current ob forecast is [], 
actual action is [22.7, 65.0], 
sim time this is 1192800.0000, 
sim time next is 1193400.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 28.32777230283259, 1.268291502005662, 0.0, 1.0, 65.0, 18848.79709400366], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8606476919027157, 0.9227638340018873, 0.0, 1.0, 1.0, 0.08975617663811267], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.172631], dtype=float32), -0.33638927]. 
=============================================
[2019-04-08 15:44:27,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.6477971e-19 6.3369962e-11 4.5696652e-10 4.1098960e-07 8.8691944e-12
 6.6084121e-15 7.7348244e-10 1.4966252e-16 3.9280310e-20 9.1981036e-23
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:27,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7913
[2019-04-08 15:44:27,771] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.71666666666667, 63.33333333333333, 167.3333333333333, 0.0, 19.0, 28.22621252072377, 1.243191425797514, 0.0, 1.0, 65.0, 18843.76933453819], 
current ob forecast is [], 
actual action is [23.71666666666667, 65.0], 
sim time this is 1167000.0000, 
sim time next is 1167600.0000, 
raw observation next is [18.63333333333333, 63.66666666666667, 169.1666666666667, 0.0, 19.0, 28.23591648565573, 1.247354655025054, 0.0, 1.0, 65.0, 18843.82713614282], 
processed observation next is [0.0, 0.5217391304347826, 0.9787626962142197, 0.6366666666666667, 0.563888888888889, 0.0, 0.08333333333333333, 0.8529930404713107, 0.9157848850083514, 0.0, 1.0, 1.0, 0.08973251017210866], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.15923893], dtype=float32), -0.76382947]. 
=============================================
[2019-04-08 15:44:28,084] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1917168e-18 7.8517123e-11 4.4199535e-08 1.1329257e-07 5.2929583e-12
 1.8730103e-13 1.7614448e-07 3.9042601e-14 2.4288047e-19 4.2372090e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:28,089] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2681
[2019-04-08 15:44:28,117] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.3433384214748, 1.269727891924897, 0.0, 1.0, 65.0, 18848.91770189499], 
current ob forecast is [], 
actual action is [22.7, 65.0], 
sim time this is 1192200.0000, 
sim time next is 1192800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 28.33262926657791, 1.26899939004553, 0.0, 1.0, 65.0, 18848.83116445833], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8610524388814925, 0.9229997966818434, 0.0, 1.0, 1.0, 0.089756338878373], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.7059369], dtype=float32), -0.7140649]. 
=============================================
[2019-04-08 15:44:28,372] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3552004e-17 1.4640252e-09 4.5479382e-08 7.3611432e-07 1.6176699e-11
 3.9131299e-14 2.2061673e-08 3.5096594e-15 4.6202551e-19 1.5419791e-22
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:44:28,377] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0784
[2019-04-08 15:44:28,387] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.1, 77.0, 0.0, 0.0, 19.0, 27.78888699272468, 1.156371719802255, 0.0, 1.0, 65.0, 27990.00768950045], 
current ob forecast is [], 
actual action is [16.1, 65.0], 
sim time this is 1134600.0000, 
sim time next is 1135200.0000, 
raw observation next is [11.1, 77.0, 0.0, 0.0, 19.0, 27.78778027163387, 1.155602206746154, 0.0, 1.0, 65.0, 28027.8438919905], 
processed observation next is [0.0, 0.13043478260869565, 0.7700831024930749, 0.77, 0.0, 0.0, 0.08333333333333333, 0.8156483559694893, 0.8852007355820514, 0.0, 1.0, 1.0, 0.13346592329519286], 
reward next is 0.8665, 
noisyNet noise sample is [array([-0.01494387], dtype=float32), 1.0279825]. 
=============================================
[2019-04-08 15:44:28,966] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.0262810e-20 4.9751290e-11 8.9169521e-07 2.9613518e-09 2.2223699e-10
 2.2981693e-14 1.8531887e-08 9.6737117e-15 7.5259719e-21 8.4700053e-22
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:44:28,970] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2731
[2019-04-08 15:44:28,990] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.15, 81.5, 0.0, 0.0, 19.0, 27.76365193694413, 1.132349229123628, 0.0, 1.0, 65.0, 28318.19779458504], 
current ob forecast is [], 
actual action is [17.15, 65.0], 
sim time this is 1146600.0000, 
sim time next is 1147200.0000, 
raw observation next is [12.33333333333333, 81.0, 0.0, 0.0, 19.0, 27.7694232859569, 1.131801234353081, 0.0, 1.0, 65.0, 28062.16322669929], 
processed observation next is [0.0, 0.2608695652173913, 0.8042474607571561, 0.81, 0.0, 0.0, 0.08333333333333333, 0.814118607163075, 0.8772670781176938, 0.0, 1.0, 1.0, 0.13362934869856805], 
reward next is 0.8664, 
noisyNet noise sample is [array([0.9655889], dtype=float32), 0.009196008]. 
=============================================
[2019-04-08 15:44:29,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0681017e-20 3.1180537e-11 8.5080538e-09 1.3601506e-07 3.5826184e-12
 1.2782761e-14 1.7371853e-09 1.1802808e-15 8.0348105e-20 7.7260759e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:29,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7486
[2019-04-08 15:44:29,246] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.35681350483815, 1.271950523602852, 0.0, 1.0, 65.0, 18848.71823311247], 
current ob forecast is [], 
actual action is [22.7, 65.0], 
sim time this is 1198200.0000, 
sim time next is 1198800.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 28.37504199148891, 1.269126540790173, 0.0, 1.0, 65.0, 18848.92935425281], 
processed observation next is [0.0, 0.9130434782608695, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8645868326240759, 0.923042180263391, 0.0, 1.0, 1.0, 0.0897568064488229], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.54923713], dtype=float32), 1.428863]. 
=============================================
[2019-04-08 15:44:29,757] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1498010e-19 9.2941495e-11 2.6724123e-07 2.3375677e-08 2.3996619e-11
 8.5851018e-14 1.8955527e-08 8.6380483e-16 2.4374459e-19 1.6077960e-23
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:29,758] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4616
[2019-04-08 15:44:29,768] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 19.0, 28.3211902797722, 1.267185376841973, 0.0, 1.0, 65.0, 18848.64577932504], 
current ob forecast is [], 
actual action is [22.7, 65.0], 
sim time this is 1194600.0000, 
sim time next is 1195200.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 19.0, 28.31838231737673, 1.269699415316037, 0.0, 1.0, 65.0, 18848.55794250532], 
processed observation next is [0.0, 0.8695652173913043, 0.9529085872576178, 0.67, 0.0, 0.0, 0.08333333333333333, 0.8598651931147275, 0.9232331384386789, 0.0, 1.0, 1.0, 0.0897550378214539], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.85416895], dtype=float32), -0.9736951]. 
=============================================
[2019-04-08 15:44:30,215] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [8.2171362e-19 5.5060349e-11 1.5824600e-07 1.3240295e-07 5.3146172e-12
 3.0288664e-15 2.4355240e-10 3.7572375e-16 6.5548189e-20 1.6954018e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:30,219] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5834
[2019-04-08 15:44:30,232] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [18.3, 65.0, 159.0, 0.0, 19.0, 28.30390464571385, 1.267915458429875, 0.0, 1.0, 65.0, 18845.06867957908], 
current ob forecast is [], 
actual action is [23.3, 65.0], 
sim time this is 1171800.0000, 
sim time next is 1172400.0000, 
raw observation next is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 28.30825548741291, 1.271518084821669, 0.0, 1.0, 65.0, 18845.28961717703], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5127777777777777, 0.0, 0.08333333333333333, 0.8590212906177426, 0.923839361607223, 0.0, 1.0, 1.0, 0.08973947436750966], 
reward next is 0.9103, 
noisyNet noise sample is [array([1.64481], dtype=float32), 0.028678648]. 
=============================================
[2019-04-08 15:44:31,255] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1347160e-18 5.9208026e-13 4.1191495e-09 2.4726544e-08 3.6899807e-13
 1.3398890e-16 2.4733501e-10 6.0188375e-17 1.0893477e-22 8.5322448e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:31,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1972
[2019-04-08 15:44:31,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.97812554e-21 6.07147362e-13 1.62019576e-09 1.46012304e-08
 1.80148917e-11 5.61392262e-16 2.56965088e-10 3.83682566e-17
 8.58389555e-21 4.94733722e-24 1.00000000e+00], sum to 1.0000
[2019-04-08 15:44:31,272] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 100.0, 74.66666666666667, 0.0, 19.0, 28.15554767662393, 1.265928815528048, 0.0, 1.0, 65.0, 20312.42712414052], 
current ob forecast is [], 
actual action is [20.0, 65.0], 
sim time this is 1244400.0000, 
sim time next is 1245000.0000, 
raw observation next is [15.0, 100.0, 75.33333333333333, 0.0, 19.0, 28.16047134877332, 1.26772535833594, 0.0, 1.0, 65.0, 20073.61394961209], 
processed observation next is [0.0, 0.391304347826087, 0.8781163434903049, 1.0, 0.2511111111111111, 0.0, 0.08333333333333333, 0.84670594573111, 0.9225751194453133, 0.0, 1.0, 1.0, 0.09558863785529566], 
reward next is 0.9044, 
noisyNet noise sample is [array([1.384292], dtype=float32), -0.008908905]. 
=============================================
[2019-04-08 15:44:31,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5712
[2019-04-08 15:44:31,287] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.1, 79.0, 0.0, 0.0, 19.0, 28.22060037716433, 1.254147294780551, 0.0, 1.0, 65.0, 20368.94103941112], 
current ob forecast is [], 
actual action is [21.1, 65.0], 
sim time this is 1211400.0000, 
sim time next is 1212000.0000, 
raw observation next is [16.1, 79.33333333333333, 0.0, 0.0, 19.0, 28.24948349521489, 1.257865226033916, 0.0, 1.0, 65.0, 20313.2029667584], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7933333333333333, 0.0, 0.0, 0.08333333333333333, 0.8541236246012408, 0.9192884086779719, 0.0, 1.0, 1.0, 0.09672953793694476], 
reward next is 0.9033, 
noisyNet noise sample is [array([-0.07080981], dtype=float32), 0.33808646]. 
=============================================
[2019-04-08 15:44:31,289] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[94.02266 ]
 [93.93791 ]
 [93.85241 ]
 [93.709946]
 [93.59437 ]], R is [[94.03145599]
 [93.99441528]
 [93.95662689]
 [93.91848755]
 [93.87950134]].
[2019-04-08 15:44:31,304] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[90.674324]
 [90.738335]
 [91.24304 ]
 [91.4952  ]
 [91.84875 ]], R is [[90.46523285]
 [90.46359253]
 [90.46886444]
 [90.47444153]
 [90.47995758]].
[2019-04-08 15:44:31,314] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9329624e-20 2.8270108e-13 7.5033030e-10 1.0111271e-09 1.2888028e-11
 1.7950870e-13 8.3614760e-10 1.7586786e-17 1.6729995e-22 2.6596512e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:31,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8918
[2019-04-08 15:44:31,335] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.8, 100.0, 83.0, 0.0, 19.0, 28.20013065289962, 1.296033106684632, 0.0, 1.0, 65.0, 19804.66279117823], 
current ob forecast is [], 
actual action is [18.8, 65.0], 
sim time this is 1260600.0000, 
sim time next is 1261200.0000, 
raw observation next is [13.8, 100.0, 80.0, 0.0, 19.0, 28.19660450716758, 1.295912877057391, 0.0, 1.0, 65.0, 19858.87786827972], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.26666666666666666, 0.0, 0.08333333333333333, 0.8497170422639652, 0.9319709590191304, 0.0, 1.0, 1.0, 0.09456608508704628], 
reward next is 0.9054, 
noisyNet noise sample is [array([-0.7621826], dtype=float32), 1.6958762]. 
=============================================
[2019-04-08 15:44:31,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0274808e-18 2.8915065e-10 5.3099882e-08 1.0117280e-07 1.2323277e-10
 3.5707514e-14 7.3107093e-10 4.8327909e-15 5.4157831e-19 5.2311782e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:31,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0080
[2019-04-08 15:44:31,672] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.0, 50.66666666666666, 78.66666666666667, 403.0000000000001, 22.5, 28.48796682217247, 1.296532029453032, 1.0, 1.0, 65.0, 32133.59158049732], 
current ob forecast is [], 
actual action is [17.0, 65.0], 
sim time this is 1525200.0000, 
sim time next is 1525800.0000, 
raw observation next is [12.1, 50.33333333333334, 80.33333333333333, 328.0, 22.5, 28.12464731359337, 1.239916912663941, 1.0, 1.0, 65.00000000000009, 19285.61991338556], 
processed observation next is [1.0, 0.6521739130434783, 0.7977839335180056, 0.5033333333333334, 0.2677777777777778, 0.3624309392265193, 0.375, 0.8437206094661143, 0.913305637554647, 1.0, 1.0, 1.0000000000000018, 0.09183628530183599], 
reward next is 0.9082, 
noisyNet noise sample is [array([-2.5309553], dtype=float32), 0.30462873]. 
=============================================
[2019-04-08 15:44:32,064] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3947742e-21 2.8517945e-12 8.8763192e-09 2.3664892e-09 1.1029298e-13
 1.5487468e-16 5.4243863e-11 1.7169487e-18 1.0303991e-21 6.5323414e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:32,068] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1542
[2019-04-08 15:44:32,077] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [14.1, 98.0, 101.0, 0.0, 19.0, 28.19710843602786, 1.289787584193254, 0.0, 1.0, 65.0, 19448.38882890679], 
current ob forecast is [], 
actual action is [19.1, 65.0], 
sim time this is 1254600.0000, 
sim time next is 1255200.0000, 
raw observation next is [14.0, 98.66666666666666, 100.0, 0.0, 19.0, 28.19495282680181, 1.290243768201074, 0.0, 1.0, 65.0, 19477.9190686787], 
processed observation next is [0.0, 0.5217391304347826, 0.8504155124653741, 0.9866666666666666, 0.3333333333333333, 0.0, 0.08333333333333333, 0.8495794022334842, 0.9300812560670245, 0.0, 1.0, 1.0, 0.09275199556513666], 
reward next is 0.9072, 
noisyNet noise sample is [array([-1.2514633], dtype=float32), -0.5045839]. 
=============================================
[2019-04-08 15:44:32,440] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9300387e-19 1.3519021e-09 2.1617329e-07 1.4137956e-07 2.1076967e-11
 2.9306454e-13 1.6344988e-08 4.8382310e-15 4.3348982e-21 3.3905898e-23
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:32,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5567
[2019-04-08 15:44:32,457] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 19.0, 28.13622568955821, 1.241548429535829, 0.0, 1.0, 65.0, 22472.98706193257], 
current ob forecast is [], 
actual action is [20.0, 65.0], 
sim time this is 1234200.0000, 
sim time next is 1234800.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 19.0, 28.12726334125178, 1.24121779043146, 0.0, 1.0, 65.0, 22539.96824559803], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8439386117709816, 0.9137392634771534, 0.0, 1.0, 1.0, 0.10733318212189538], 
reward next is 0.8927, 
noisyNet noise sample is [array([0.22346872], dtype=float32), 1.015607]. 
=============================================
[2019-04-08 15:44:32,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6028794e-20 1.5006111e-11 2.3056632e-09 2.6572678e-08 5.8887126e-14
 2.8613673e-16 1.5572104e-11 5.6298541e-17 6.2726661e-21 8.4373091e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:32,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7786
[2019-04-08 15:44:32,515] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.533333333333333, 73.66666666666666, 0.0, 0.0, 19.0, 27.83238491139603, 1.134007439551343, 0.0, 1.0, 65.0, 22878.7059981283], 
current ob forecast is [], 
actual action is [12.533333333333333, 65.0], 
sim time this is 1543200.0000, 
sim time next is 1543800.0000, 
raw observation next is [7.616666666666667, 73.83333333333334, 0.0, 0.0, 19.0, 27.80869789895868, 1.130659083440533, 0.0, 1.0, 65.0, 24286.49465372378], 
processed observation next is [1.0, 0.8695652173913043, 0.6735918744228995, 0.7383333333333334, 0.0, 0.0, 0.08333333333333333, 0.81739149157989, 0.8768863611468444, 0.0, 1.0, 1.0, 0.11564997454154181], 
reward next is 0.8844, 
noisyNet noise sample is [array([-1.3825068], dtype=float32), -1.1350509]. 
=============================================
[2019-04-08 15:44:32,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.44224362e-19 1.00487136e-11 6.87643151e-08 4.31856364e-07
 1.42863525e-11 1.88332270e-14 1.05920757e-08 8.03331738e-17
 2.10912538e-21 2.26703557e-23 9.99999523e-01], sum to 1.0000
[2019-04-08 15:44:32,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8167
[2019-04-08 15:44:32,568] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 19.0, 27.61725369928074, 1.165109090198326, 0.0, 1.0, 65.0, 32630.84316562708], 
current ob forecast is [], 
actual action is [10.5, 65.0], 
sim time this is 1285800.0000, 
sim time next is 1286400.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 19.0, 27.60895467553393, 1.161251816807918, 0.0, 1.0, 65.0, 32561.29483126511], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8007462229611608, 0.8870839389359727, 0.0, 1.0, 1.0, 0.15505378491078622], 
reward next is 0.8449, 
noisyNet noise sample is [array([-0.92044896], dtype=float32), 1.3058664]. 
=============================================
[2019-04-08 15:44:32,581] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.8605257e-18 1.4665704e-11 2.7140318e-08 3.8143551e-07 3.1460323e-12
 2.2739360e-14 1.1297142e-08 2.5350876e-16 2.2387818e-20 1.3006434e-22
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:44:32,589] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7747
[2019-04-08 15:44:32,600] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 19.0, 28.219822022891, 1.247076311880104, 0.0, 1.0, 65.0, 21378.12549270788], 
current ob forecast is [], 
actual action is [20.5, 65.0], 
sim time this is 1221600.0000, 
sim time next is 1222200.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 19.0, 28.20946036841509, 1.245695490544956, 0.0, 1.0, 65.0, 21518.69456639295], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.08333333333333333, 0.8507883640345909, 0.9152318301816521, 0.0, 1.0, 1.0, 0.10246997412568072], 
reward next is 0.8975, 
noisyNet noise sample is [array([-0.5915933], dtype=float32), -0.41538924]. 
=============================================
[2019-04-08 15:44:32,766] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7972474e-18 3.5549316e-10 7.3111597e-09 3.5512215e-08 1.4587880e-10
 8.6777902e-14 2.8673444e-08 8.9604884e-16 1.8952236e-20 3.2281491e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:32,770] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0017
[2019-04-08 15:44:32,779] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.8, 100.0, 89.0, 0.0, 19.0, 28.20863403287033, 1.295434411244908, 0.0, 1.0, 65.0, 19129.49185146083], 
current ob forecast is [], 
actual action is [18.8, 65.0], 
sim time this is 1259400.0000, 
sim time next is 1260000.0000, 
raw observation next is [13.8, 100.0, 86.0, 0.0, 19.0, 28.20477300436845, 1.295846972165069, 0.0, 1.0, 65.0, 19592.51640291279], 
processed observation next is [0.0, 0.6086956521739131, 0.844875346260388, 1.0, 0.2866666666666667, 0.0, 0.08333333333333333, 0.8503977503640376, 0.9319489907216897, 0.0, 1.0, 1.0, 0.09329769715672757], 
reward next is 0.9067, 
noisyNet noise sample is [array([-0.4821587], dtype=float32), 0.5635364]. 
=============================================
[2019-04-08 15:44:32,803] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[92.73229 ]
 [92.79228 ]
 [92.83031 ]
 [92.89458 ]
 [92.918564]], R is [[92.67632294]
 [92.65846252]
 [92.640625  ]
 [92.62031555]
 [92.60030365]].
[2019-04-08 15:44:32,937] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7295097e-19 7.6876500e-12 8.5578078e-09 4.1677939e-08 3.7053447e-11
 1.1516297e-15 1.0670004e-08 1.4082361e-16 1.5115508e-21 5.8719063e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:32,938] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6510
[2019-04-08 15:44:32,951] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.73333333333333, 100.0, 15.83333333333333, 0.0, 19.0, 28.15406237247329, 1.282743973135712, 0.0, 1.0, 65.0, 21324.29488428298], 
current ob forecast is [], 
actual action is [17.73333333333333, 65.0], 
sim time this is 1269600.0000, 
sim time next is 1270200.0000, 
raw observation next is [12.46666666666667, 100.0, 12.66666666666667, 0.0, 19.0, 28.1379226366784, 1.279483825237516, 0.0, 1.0, 65.0, 21743.55697514931], 
processed observation next is [0.0, 0.6956521739130435, 0.8079409048938138, 1.0, 0.04222222222222223, 0.0, 0.08333333333333333, 0.8448268863898667, 0.9264946084125053, 0.0, 1.0, 1.0, 0.103540747500711], 
reward next is 0.8965, 
noisyNet noise sample is [array([1.1437274], dtype=float32), 0.4869791]. 
=============================================
[2019-04-08 15:44:32,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4953914e-19 6.2858239e-11 3.1840049e-08 4.4724136e-08 2.2422533e-12
 2.4355485e-13 6.6786560e-10 1.8048489e-15 3.9039683e-19 5.3260985e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:32,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2360
[2019-04-08 15:44:33,016] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.133333333333334, 98.66666666666667, 0.0, 0.0, 19.0, 27.55789234141824, 1.126572892807087, 0.0, 1.0, 65.0, 33691.6798184136], 
current ob forecast is [], 
actual action is [10.133333333333333, 65.0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [4.95, 98.0, 0.0, 0.0, 19.0, 27.53899878474907, 1.121925492144544, 0.0, 1.0, 65.0, 34187.55820888748], 
processed observation next is [0.0, 1.0, 0.5997229916897507, 0.98, 0.0, 0.0, 0.08333333333333333, 0.7949165653957557, 0.8739751640481813, 0.0, 1.0, 1.0, 0.16279789623279753], 
reward next is 0.8372, 
noisyNet noise sample is [array([-1.1801434], dtype=float32), -1.4776926]. 
=============================================
[2019-04-08 15:44:33,591] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.1754098e-20 2.2365136e-11 2.2274406e-08 1.6377109e-07 1.0362038e-12
 7.9943207e-15 4.5551793e-09 3.7761156e-17 5.2775815e-21 5.0093263e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:33,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5002
[2019-04-08 15:44:33,610] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 110.6666666666667, 0.0, 22.5, 28.00251449163543, 1.116931698887357, 1.0, 1.0, 65.0, 28822.05109038335], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1342200.0000, 
sim time next is 1342800.0000, 
raw observation next is [1.1, 92.0, 109.5, 0.0, 22.5, 28.00190810643732, 1.111734154745016, 1.0, 1.0, 65.0, 28154.75601096051], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.365, 0.0, 0.375, 0.83349234220311, 0.870578051581672, 1.0, 1.0, 1.0, 0.13407026671885958], 
reward next is 0.8659, 
noisyNet noise sample is [array([-0.50156087], dtype=float32), -0.3595782]. 
=============================================
[2019-04-08 15:44:34,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.09983529e-16 1.16356376e-10 8.00924269e-08 2.26290453e-07
 3.23080562e-10 1.17403293e-13 1.30599815e-07 1.58915589e-15
 1.07719169e-19 4.66678026e-23 9.99999523e-01], sum to 1.0000
[2019-04-08 15:44:34,384] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6974
[2019-04-08 15:44:34,395] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [16.26666666666667, 77.0, 0.0, 0.0, 19.0, 28.27131347875515, 1.253692073816392, 0.0, 1.0, 65.0, 18845.53568415577], 
current ob forecast is [], 
actual action is [21.26666666666667, 65.0], 
sim time this is 1208400.0000, 
sim time next is 1209000.0000, 
raw observation next is [16.18333333333333, 77.5, 0.0, 0.0, 19.0, 28.2583840620668, 1.252933257877401, 0.0, 1.0, 65.0, 18845.07834838361], 
processed observation next is [0.0, 1.0, 0.9108956602031394, 0.775, 0.0, 0.0, 0.08333333333333333, 0.8548653385055666, 0.917644419292467, 0.0, 1.0, 1.0, 0.08973846832563624], 
reward next is 0.9103, 
noisyNet noise sample is [array([-2.6115024], dtype=float32), 0.11760116]. 
=============================================
[2019-04-08 15:44:34,405] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.79992]
 [86.74325]
 [86.721  ]
 [86.65337]
 [86.61593]], R is [[86.87585449]
 [86.9173584 ]
 [86.95844269]
 [86.99911499]
 [87.03938293]].
[2019-04-08 15:44:34,557] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4444108e-19 1.8109865e-12 5.0566871e-09 1.8818643e-08 7.1203952e-11
 8.2031550e-15 1.7950085e-09 4.8332679e-16 4.9994438e-20 7.0249954e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:34,563] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3455
[2019-04-08 15:44:34,577] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.1, 92.0, 0.0, 0.0, 19.0, 27.41161460580062, 1.068589310345295, 0.0, 1.0, 65.0, 36398.11052353142], 
current ob forecast is [], 
actual action is [8.1, 65.0], 
sim time this is 1304400.0000, 
sim time next is 1305000.0000, 
raw observation next is [3.0, 92.0, 0.0, 0.0, 19.0, 27.39676736937241, 1.067029240753856, 0.0, 1.0, 65.0, 36287.51587916716], 
processed observation next is [1.0, 0.08695652173913043, 0.5457063711911359, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7830639474477007, 0.8556764135846185, 0.0, 1.0, 1.0, 0.17279769466270076], 
reward next is 0.8272, 
noisyNet noise sample is [array([0.04532963], dtype=float32), 1.891777]. 
=============================================
[2019-04-08 15:44:34,581] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.08862]
 [86.37843]
 [86.46268]
 [86.30534]
 [86.46808]], R is [[86.03130341]
 [85.99766541]
 [85.9586792 ]
 [85.92206573]
 [85.88778687]].
[2019-04-08 15:44:34,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9932260e-19 4.7337221e-12 4.4792003e-08 7.3710076e-08 3.4616046e-10
 1.6770893e-14 3.6367986e-10 2.3322826e-15 8.5567925e-21 5.2834612e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:34,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6862
[2019-04-08 15:44:34,759] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.466666666666667, 96.0, 0.0, 0.0, 19.0, 27.72811057077444, 1.197202747340634, 0.0, 1.0, 65.0, 30658.06913465235], 
current ob forecast is [], 
actual action is [11.466666666666667, 65.0], 
sim time this is 1280400.0000, 
sim time next is 1281000.0000, 
raw observation next is [6.283333333333333, 96.0, 0.0, 0.0, 19.0, 27.71165317113962, 1.193001500170934, 0.0, 1.0, 65.0, 30938.31097406412], 
processed observation next is [0.0, 0.8260869565217391, 0.6366574330563252, 0.96, 0.0, 0.0, 0.08333333333333333, 0.8093044309283016, 0.8976671667236445, 0.0, 1.0, 1.0, 0.1473252903526863], 
reward next is 0.8527, 
noisyNet noise sample is [array([-0.36033118], dtype=float32), -0.39394686]. 
=============================================
[2019-04-08 15:44:34,769] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.92256]
 [87.06062]
 [87.23019]
 [87.34886]
 [87.48124]], R is [[86.74156189]
 [86.72815704]
 [86.71637726]
 [86.7063446 ]
 [86.69793701]].
[2019-04-08 15:44:34,855] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1716932e-17 2.3378646e-10 7.9618459e-08 3.9846967e-08 4.0889687e-11
 4.2983093e-14 4.3429013e-10 2.8453094e-14 5.5168304e-19 7.7477468e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:34,855] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6258
[2019-04-08 15:44:34,877] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 96.66666666666666, 0.0, 0.0, 19.0, 27.67957106837774, 1.184697514002668, 0.0, 1.0, 65.0, 31537.79208334472], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 1282200.0000, 
sim time next is 1282800.0000, 
raw observation next is [5.9, 97.33333333333333, 0.0, 0.0, 19.0, 27.66449448330079, 1.181044411111181, 0.0, 1.0, 65.0, 31804.36963162541], 
processed observation next is [0.0, 0.8695652173913043, 0.626038781163435, 0.9733333333333333, 0.0, 0.0, 0.08333333333333333, 0.8053745402750657, 0.8936814703703937, 0.0, 1.0, 1.0, 0.15144937919821624], 
reward next is 0.8486, 
noisyNet noise sample is [array([-0.42194545], dtype=float32), -1.3150815]. 
=============================================
[2019-04-08 15:44:35,228] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.80861701e-17 1.01116504e-10 6.46945466e-08 2.55045371e-08
 3.64589348e-12 2.68112626e-14 8.73567085e-09 3.61770186e-15
 8.07457008e-20 4.17326882e-22 9.99999881e-01], sum to 1.0000
[2019-04-08 15:44:35,229] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9246
[2019-04-08 15:44:35,258] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.766666666666667, 97.33333333333334, 0.0, 0.0, 19.0, 27.51687464235532, 1.117325585239629, 0.0, 1.0, 65.0, 34987.99710336644], 
current ob forecast is [], 
actual action is [9.766666666666666, 65.0], 
sim time this is 1294800.0000, 
sim time next is 1295400.0000, 
raw observation next is [4.583333333333334, 96.66666666666666, 0.0, 0.0, 19.0, 27.49196709312697, 1.112640524631879, 0.0, 1.0, 65.0, 35431.3584157349], 
processed observation next is [0.0, 1.0, 0.5895660203139428, 0.9666666666666666, 0.0, 0.0, 0.08333333333333333, 0.7909972577605808, 0.8708801748772931, 0.0, 1.0, 1.0, 0.1687207543606424], 
reward next is 0.8313, 
noisyNet noise sample is [array([2.4035673], dtype=float32), 1.295042]. 
=============================================
[2019-04-08 15:44:35,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.41877948e-20 1.16440295e-11 2.65396540e-08 2.63588351e-09
 1.42212663e-12 1.23141433e-13 4.05286582e-10 7.27643865e-16
 3.63805226e-19 9.51314351e-22 1.00000000e+00], sum to 1.0000
[2019-04-08 15:44:35,315] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7949
[2019-04-08 15:44:35,328] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 19.0, 27.40385296261006, 1.04384174457574, 0.0, 1.0, 65.0, 37690.73325774436], 
current ob forecast is [], 
actual action is [7.2, 65.0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [2.1, 92.0, 0.0, 0.0, 19.0, 27.38121340009933, 1.040636727277204, 0.0, 1.0, 65.0, 37524.93469891273], 
processed observation next is [1.0, 0.17391304347826086, 0.5207756232686982, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7817677833416109, 0.8468789090924013, 0.0, 1.0, 1.0, 0.17869016523291775], 
reward next is 0.8213, 
noisyNet noise sample is [array([-0.1521834], dtype=float32), 0.9803359]. 
=============================================
[2019-04-08 15:44:35,332] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.89159]
 [85.94072]
 [86.0151 ]
 [86.06888]
 [86.16399]], R is [[85.79650879]
 [85.75906372]
 [85.72995758]
 [85.70295715]
 [85.67010498]].
[2019-04-08 15:44:35,452] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8884849e-18 2.9261333e-09 6.0549624e-08 6.1886638e-08 6.1725243e-12
 2.7660496e-14 5.7671894e-09 1.4674941e-15 5.7414316e-20 2.8875017e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:35,457] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8174
[2019-04-08 15:44:35,473] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 27.23033643936737, 0.9823899326314446, 1.0, 1.0, 65.0, 40295.43313547304], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1322400.0000, 
sim time next is 1323000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 27.24767139200927, 0.9872691336102687, 1.0, 1.0, 65.0, 39071.60951196637], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7706392826674392, 0.8290897112034229, 1.0, 1.0, 1.0, 0.18605528339031607], 
reward next is 0.8139, 
noisyNet noise sample is [array([-0.3276278], dtype=float32), -1.2679385]. 
=============================================
[2019-04-08 15:44:35,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.553925]
 [83.93784 ]
 [83.75404 ]
 [83.78837 ]
 [83.84161 ]], R is [[84.90285492]
 [84.86194611]
 [84.81729889]
 [84.78173065]
 [84.75231934]].
[2019-04-08 15:44:35,711] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2897768e-18 2.4721551e-11 9.8053627e-09 2.7672032e-08 9.5350264e-12
 2.1917492e-12 5.8605607e-08 1.5350486e-15 1.3159424e-19 3.6323351e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:35,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7230
[2019-04-08 15:44:35,731] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 27.22417805389025, 0.9823608767406391, 0.0, 1.0, 65.0, 41166.97069669787], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1321800.0000, 
sim time next is 1322400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 27.2303349658906, 0.9823894702917183, 1.0, 1.0, 65.0, 40295.45006119819], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7691945804908832, 0.8274631567639061, 1.0, 1.0, 1.0, 0.1918830955295152], 
reward next is 0.8081, 
noisyNet noise sample is [array([-2.266605], dtype=float32), 0.0031864182]. 
=============================================
[2019-04-08 15:44:35,916] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.3480740e-17 8.7937352e-10 3.3146819e-07 5.6027329e-07 2.3519287e-10
 1.1080805e-12 2.8038492e-08 4.7408667e-15 3.3490356e-19 9.9898661e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:44:35,929] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7224
[2019-04-08 15:44:35,943] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.65155672952377, 1.07479143239709, 1.0, 1.0, 65.0, 28790.47411064595], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1360800.0000, 
sim time next is 1361400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.68663401642826, 1.085336238122813, 1.0, 1.0, 65.0, 30184.70264725033], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.8072195013690218, 0.8617787460409376, 1.0, 1.0, 1.0, 0.14373667927262063], 
reward next is 0.8563, 
noisyNet noise sample is [array([-0.5058313], dtype=float32), 0.43592042]. 
=============================================
[2019-04-08 15:44:36,015] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6359003e-18 4.9878429e-10 1.5096388e-06 3.4171371e-06 9.2259747e-09
 1.6891379e-11 1.6570172e-07 4.2063090e-14 7.4627714e-18 1.3838633e-19
 9.9999487e-01], sum to 1.0000
[2019-04-08 15:44:36,016] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-08 15:44:36,035] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 92.0, 0.0, 0.0, 19.0, 27.3967652079904, 1.067028503356102, 0.0, 1.0, 65.0, 36287.54145332066], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 1305000.0000, 
sim time next is 1305600.0000, 
raw observation next is [2.9, 92.0, 0.0, 0.0, 19.0, 27.42770703958265, 1.069500910495583, 0.0, 1.0, 65.0, 36303.01753119723], 
processed observation next is [1.0, 0.08695652173913043, 0.5429362880886427, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7856422532985542, 0.8565003034985277, 0.0, 1.0, 1.0, 0.17287151205332013], 
reward next is 0.8271, 
noisyNet noise sample is [array([-1.6337602], dtype=float32), -0.9292453]. 
=============================================
[2019-04-08 15:44:36,062] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3116428e-18 3.6071049e-11 1.5461406e-08 1.8012777e-07 2.4018313e-12
 5.0422069e-14 1.5765205e-09 1.2614783e-15 7.8746541e-20 2.9247564e-23
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:36,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7531
[2019-04-08 15:44:36,075] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 31.5, 0.0, 22.5, 27.81841299790183, 1.041812791464499, 1.0, 1.0, 65.0, 30382.65558789313], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1328400.0000, 
sim time next is 1329000.0000, 
raw observation next is [0.5, 92.0, 36.0, 0.0, 22.5, 27.79823393754086, 1.037914967213096, 1.0, 1.0, 65.0, 30837.24444542645], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.12, 0.0, 0.375, 0.8165194947950717, 0.8459716557376987, 1.0, 1.0, 1.0, 0.14684402116869738], 
reward next is 0.8532, 
noisyNet noise sample is [array([-0.37605664], dtype=float32), 0.5921333]. 
=============================================
[2019-04-08 15:44:36,084] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.45914 ]
 [82.7118  ]
 [82.93549 ]
 [83.23817 ]
 [83.271736]], R is [[82.25770569]
 [82.29045105]
 [82.33084869]
 [82.37719727]
 [82.40466309]].
[2019-04-08 15:44:36,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9312214e-17 2.3054504e-10 2.1908704e-07 2.5841180e-08 7.9262673e-12
 2.3725038e-13 8.0194518e-09 4.9230188e-14 4.6356190e-18 3.0242143e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:36,241] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9765
[2019-04-08 15:44:36,256] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 73.5, 0.0, 22.5, 27.97527453823735, 1.078169060300845, 1.0, 1.0, 65.0, 28691.73402617948], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1332000.0000, 
sim time next is 1332600.0000, 
raw observation next is [0.6000000000000001, 92.0, 83.00000000000001, 0.0, 22.5, 28.002333314394, 1.085347315430153, 1.0, 1.0, 65.0, 27957.20642600279], 
processed observation next is [1.0, 0.43478260869565216, 0.479224376731302, 0.92, 0.2766666666666667, 0.0, 0.375, 0.8335277761995, 0.8617824384767175, 1.0, 1.0, 1.0, 0.1331295544095371], 
reward next is 0.8669, 
noisyNet noise sample is [array([-0.60534436], dtype=float32), 0.68210596]. 
=============================================
[2019-04-08 15:44:36,475] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4708482e-18 2.0849772e-11 3.3832296e-07 2.7908649e-07 2.5710391e-12
 3.3571403e-15 8.4672980e-10 4.9809100e-16 7.5490958e-20 4.4069910e-22
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:44:36,480] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0496
[2019-04-08 15:44:36,494] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 92.0, 73.5, 0.0, 22.5, 27.97527681332864, 1.078169691460519, 1.0, 1.0, 65.0, 28691.70861003441], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1332000.0000, 
sim time next is 1332600.0000, 
raw observation next is [0.6000000000000001, 92.0, 83.00000000000001, 0.0, 22.5, 28.0023355887615, 1.085347947167232, 1.0, 1.0, 65.0, 27957.18130862122], 
processed observation next is [1.0, 0.43478260869565216, 0.479224376731302, 0.92, 0.2766666666666667, 0.0, 0.375, 0.833527965730125, 0.8617826490557441, 1.0, 1.0, 1.0, 0.1331294348029582], 
reward next is 0.8669, 
noisyNet noise sample is [array([-0.6962031], dtype=float32), 0.69507784]. 
=============================================
[2019-04-08 15:44:36,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9957880e-17 1.9498590e-11 9.1127838e-08 6.1701826e-08 3.1522639e-11
 5.8440703e-13 3.1503256e-09 5.1516247e-16 6.1614488e-18 2.2343205e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:36,648] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4730
[2019-04-08 15:44:36,673] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 22.5, 27.60044052328371, 1.05229245162625, 1.0, 1.0, 65.0, 33881.29936486659], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1362600.0000, 
sim time next is 1363200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 22.5, 27.45803157502336, 1.044276824982594, 1.0, 1.0, 65.0, 36618.77883368969], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.375, 0.7881692979186132, 0.848092274994198, 1.0, 1.0, 1.0, 0.17437513730328424], 
reward next is 0.8256, 
noisyNet noise sample is [array([-0.1076074], dtype=float32), 1.2652433]. 
=============================================
[2019-04-08 15:44:37,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5932830e-17 6.1988947e-10 6.8014350e-07 2.9649721e-07 5.8127444e-11
 4.6139720e-13 2.5991531e-08 4.6932709e-16 1.0086487e-17 2.5954367e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:44:37,250] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2801
[2019-04-08 15:44:37,270] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.6, 92.0, 27.0, 0.0, 22.5, 27.81145407077467, 1.040123090437584, 1.0, 1.0, 65.0, 28705.65725951305], 
current ob forecast is [], 
actual action is [5.6, 65.0], 
sim time this is 1327800.0000, 
sim time next is 1328400.0000, 
raw observation next is [0.5, 92.0, 31.5, 0.0, 22.5, 27.81841242053476, 1.041812597914502, 1.0, 1.0, 65.0, 30382.66230781224], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.105, 0.0, 0.375, 0.8182010350445633, 0.8472708659715007, 1.0, 1.0, 1.0, 0.14467934432291543], 
reward next is 0.8553, 
noisyNet noise sample is [array([0.43495843], dtype=float32), 0.67689425]. 
=============================================
[2019-04-08 15:44:37,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3053282e-19 9.0811310e-13 9.9197872e-10 9.4523624e-09 2.4894392e-12
 4.8865699e-15 5.6132654e-10 3.7711236e-16 8.9148138e-21 1.1498478e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:37,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3331
[2019-04-08 15:44:37,398] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.2, 100.0, 9.5, 0.0, 19.0, 28.12075143195509, 1.275979418874317, 0.0, 1.0, 65.0, 22198.20175448693], 
current ob forecast is [], 
actual action is [17.2, 65.0], 
sim time this is 1270800.0000, 
sim time next is 1271400.0000, 
raw observation next is [11.55, 99.33333333333334, 6.333333333333332, 0.0, 19.0, 28.10231540573876, 1.271263748891906, 0.0, 1.0, 65.0, 22702.89039739329], 
processed observation next is [0.0, 0.7391304347826086, 0.7825484764542937, 0.9933333333333334, 0.02111111111111111, 0.0, 0.08333333333333333, 0.8418596171448968, 0.9237545829639687, 0.0, 1.0, 1.0, 0.108109001892349], 
reward next is 0.8919, 
noisyNet noise sample is [array([-0.34860462], dtype=float32), 0.8356872]. 
=============================================
[2019-04-08 15:44:38,328] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2673900e-18 9.9691158e-11 2.8071318e-08 5.7781371e-08 1.0539561e-12
 8.3635796e-14 3.1908001e-09 3.6486080e-15 4.1178162e-19 5.8378943e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:38,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1527
[2019-04-08 15:44:38,364] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.7000000000000001, 92.0, 92.5, 0.0, 22.5, 28.03879128535691, 1.089574138776994, 1.0, 1.0, 65.0, 26707.34026883758], 
current ob forecast is [], 
actual action is [5.7, 65.0], 
sim time this is 1333200.0000, 
sim time next is 1333800.0000, 
raw observation next is [0.8, 92.0, 102.0, 0.0, 22.5, 28.06308117457986, 1.094881632045265, 1.0, 1.0, 65.0, 26594.88469115878], 
processed observation next is [1.0, 0.43478260869565216, 0.4847645429362882, 0.92, 0.34, 0.0, 0.375, 0.8385900978816551, 0.8649605440150884, 1.0, 1.0, 1.0, 0.12664230805313703], 
reward next is 0.8734, 
noisyNet noise sample is [array([-1.5026102], dtype=float32), 0.11809293]. 
=============================================
[2019-04-08 15:44:39,675] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7884323e-17 1.2806523e-11 1.7828881e-07 1.7209905e-07 4.9526837e-11
 1.9946155e-13 9.3925125e-09 4.7480008e-15 2.4358378e-19 4.0336081e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:39,678] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6836
[2019-04-08 15:44:39,691] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 19.0, 27.10993116564336, 0.9952453879426327, 0.0, 1.0, 65.0, 38659.43741085353], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 1371600.0000, 
sim time next is 1372200.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 19.0, 27.1038188497561, 0.9927982570658527, 0.0, 1.0, 65.0, 38630.59972468291], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7586515708130083, 0.8309327523552842, 0.0, 1.0, 1.0, 0.1839552367842043], 
reward next is 0.8160, 
noisyNet noise sample is [array([-1.4324894], dtype=float32), -0.5494123]. 
=============================================
[2019-04-08 15:44:39,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0471174e-18 1.5594549e-10 7.1259073e-08 1.3912494e-07 5.5004716e-11
 6.1435796e-13 6.2208501e-09 9.4142535e-16 1.6402942e-17 3.4230050e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:39,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9495
[2019-04-08 15:44:40,005] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 19.0, 26.8547108808368, 0.8767548993192013, 0.0, 1.0, 65.0, 43369.55918255486], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 1389600.0000, 
sim time next is 1390200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 19.0, 26.83215392763694, 0.8784502788417795, 0.0, 1.0, 65.0, 43537.39370688993], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.08333333333333333, 0.7360128273030785, 0.7928167596139265, 0.0, 1.0, 1.0, 0.20732092241376157], 
reward next is 0.7927, 
noisyNet noise sample is [array([0.5040375], dtype=float32), 0.23002164]. 
=============================================
[2019-04-08 15:44:40,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.5031340e-17 8.8469475e-11 2.1608885e-06 2.6230657e-07 4.7930132e-11
 4.0527052e-12 1.0311780e-08 3.0413177e-15 4.7325929e-19 2.9611970e-19
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:44:40,107] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6035
[2019-04-08 15:44:40,125] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 22.5, 27.24766996468367, 0.9872686876903639, 1.0, 1.0, 65.0, 39071.62606818829], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1323000.0000, 
sim time next is 1323600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 27.31190753876378, 0.9809332106320156, 1.0, 1.0, 65.0, 37720.6423683012], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7759922948969816, 0.8269777368773386, 1.0, 1.0, 1.0, 0.17962210651572003], 
reward next is 0.8204, 
noisyNet noise sample is [array([0.43034512], dtype=float32), -1.0438813]. 
=============================================
[2019-04-08 15:44:40,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2447953e-18 2.2775408e-09 1.2685494e-07 6.6911575e-08 2.6988979e-11
 3.3492828e-14 2.9107807e-09 6.0492923e-16 4.2228061e-19 1.2960687e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:40,799] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4157
[2019-04-08 15:44:40,837] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.516666666666667, 82.16666666666667, 37.66666666666667, 0.0, 22.5, 28.09419354472885, 1.069754580067154, 1.0, 1.0, 65.0, 26930.07780727848], 
current ob forecast is [], 
actual action is [6.5166666666666675, 65.0], 
sim time this is 1699800.0000, 
sim time next is 1700400.0000, 
raw observation next is [1.433333333333334, 83.33333333333334, 33.83333333333333, 0.0, 22.5, 28.1090225097042, 0.9272777420089823, 1.0, 1.0, 65.0, 42399.49617431409], 
processed observation next is [1.0, 0.6956521739130435, 0.502308402585411, 0.8333333333333335, 0.11277777777777777, 0.0, 0.375, 0.84241854247535, 0.8090925806696608, 1.0, 1.0, 1.0, 0.201902362734829], 
reward next is 0.7981, 
noisyNet noise sample is [array([0.54195225], dtype=float32), 0.2077947]. 
=============================================
[2019-04-08 15:44:41,275] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.53620752e-18 8.61454311e-11 1.78080327e-07 8.93608316e-08
 1.09070504e-11 4.77694429e-14 5.22628518e-09 6.70521234e-15
 1.44615108e-19 7.62875903e-22 9.99999762e-01], sum to 1.0000
[2019-04-08 15:44:41,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7710
[2019-04-08 15:44:41,300] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.87116227631378, 0.8432557151746826, 0.0, 1.0, 65.0, 42109.71473706514], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 1400400.0000, 
sim time next is 1401000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 26.81388739083534, 0.8465406417703929, 0.0, 1.0, 65.0, 43633.45854684644], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7344906159029451, 0.7821802139234643, 0.0, 1.0, 1.0, 0.2077783740326021], 
reward next is 0.7922, 
noisyNet noise sample is [array([-1.983323], dtype=float32), 0.5750246]. 
=============================================
[2019-04-08 15:44:41,308] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.53885 ]
 [82.525276]
 [82.526   ]
 [82.50523 ]
 [82.52237 ]], R is [[82.51400757]
 [82.48834229]
 [82.47006226]
 [82.44981384]
 [82.42958069]].
[2019-04-08 15:44:41,928] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.73099029e-18 2.49611376e-09 2.21419967e-08 1.82355325e-06
 3.68030745e-10 1.13264826e-13 1.50075286e-09 2.86564502e-15
 1.80602843e-17 8.37014126e-21 9.99998093e-01], sum to 1.0000
[2019-04-08 15:44:41,940] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7592354e-18 2.0336907e-10 3.2017311e-08 4.6950933e-07 1.0909314e-11
 4.1015169e-12 6.2710534e-08 4.5697140e-15 4.8139029e-18 1.1475216e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:44:41,940] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3677
[2019-04-08 15:44:41,957] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8701
[2019-04-08 15:44:41,964] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 22.66666666666666, 0.0, 22.5, 27.69463024708919, 0.9583922577053521, 1.0, 1.0, 65.0, 22595.52451506146], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1441200.0000, 
sim time next is 1441800.0000, 
raw observation next is [1.1, 92.0, 18.0, 0.0, 22.5, 27.7955174265079, 0.961783810075253, 1.0, 1.0, 65.0, 26466.00348368534], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.06, 0.0, 0.375, 0.8162931188756583, 0.8205946033584177, 1.0, 1.0, 1.0, 0.12602858801754924], 
reward next is 0.8740, 
noisyNet noise sample is [array([0.21212487], dtype=float32), -0.6563587]. 
=============================================
[2019-04-08 15:44:42,000] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 92.0, 12.0, 0.0, 22.5, 27.75055483021837, 0.9810429077775665, 1.0, 1.0, 65.0, 28990.82311369763], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1443000.0000, 
sim time next is 1443600.0000, 
raw observation next is [1.1, 92.0, 9.0, 0.0, 22.5, 27.79618857378441, 0.9836347156168287, 1.0, 1.0, 65.0, 27011.65025751177], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.92, 0.03, 0.0, 0.375, 0.8163490478153674, 0.8278782385389429, 1.0, 1.0, 1.0, 0.1286269059881513], 
reward next is 0.8714, 
noisyNet noise sample is [array([-1.2860036], dtype=float32), -0.79659104]. 
=============================================
[2019-04-08 15:44:42,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7071022e-17 7.5832567e-09 1.7377744e-06 3.8448538e-06 2.1769275e-10
 5.0714463e-12 3.4820579e-07 1.9266808e-13 6.7084679e-17 6.3840624e-20
 9.9999404e-01], sum to 1.0000
[2019-04-08 15:44:42,148] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4439
[2019-04-08 15:44:42,162] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 99.16666666666667, 36.66666666666667, 0.0, 22.5, 27.52553486340448, 0.9081136731319838, 1.0, 1.0, 65.0, 29215.21628038166], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 1415400.0000, 
sim time next is 1416000.0000, 
raw observation next is [-0.4, 98.33333333333334, 41.33333333333334, 0.0, 22.5, 27.48832675053061, 0.9208288088046243, 1.0, 1.0, 65.0, 31196.69754362983], 
processed observation next is [1.0, 0.391304347826087, 0.45152354570637127, 0.9833333333333334, 0.1377777777777778, 0.0, 0.375, 0.7906938958775509, 0.8069429362682081, 1.0, 1.0, 1.0, 0.14855570258871348], 
reward next is 0.8514, 
noisyNet noise sample is [array([0.11405022], dtype=float32), -0.0019928182]. 
=============================================
[2019-04-08 15:44:42,166] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.67884 ]
 [83.71657 ]
 [84.107475]
 [83.96935 ]
 [84.57614 ]], R is [[83.51359558]
 [83.53933716]
 [83.56109619]
 [83.57500458]
 [83.59511566]].
[2019-04-08 15:44:42,732] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.53546943e-17 4.32195973e-10 1.16958304e-07 4.36085173e-07
 5.73329093e-11 1.40354568e-12 8.69571970e-09 2.09601989e-13
 8.77584119e-18 1.82145009e-20 9.99999404e-01], sum to 1.0000
[2019-04-08 15:44:42,735] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6798
[2019-04-08 15:44:42,749] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 95.0, 92.0, 0.0, 22.5, 27.67678099548019, 0.9835897119980732, 1.0, 1.0, 65.0, 28031.72549135372], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 1425000.0000, 
sim time next is 1425600.0000, 
raw observation next is [0.0, 95.0, 93.0, 0.0, 22.5, 27.71380732062899, 0.9898448686070451, 1.0, 1.0, 65.0, 27558.90753933734], 
processed observation next is [1.0, 0.5217391304347826, 0.46260387811634357, 0.95, 0.31, 0.0, 0.375, 0.8094839433857492, 0.8299482895356817, 1.0, 1.0, 1.0, 0.1312328930444635], 
reward next is 0.8688, 
noisyNet noise sample is [array([-0.07243263], dtype=float32), -0.06408617]. 
=============================================
[2019-04-08 15:44:42,754] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5748562e-16 2.3296551e-10 1.6410904e-07 5.0655939e-07 2.1369352e-10
 6.2354581e-13 3.1329421e-08 6.6166047e-16 1.3669451e-17 6.2336874e-20
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:44:42,760] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1430
[2019-04-08 15:44:42,779] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.45, 91.83333333333334, 0.0, 0.0, 19.0, 26.79235880233234, 0.8068217922258961, 0.0, 1.0, 65.0, 47527.71670325106], 
current ob forecast is [], 
actual action is [5.45, 65.0], 
sim time this is 1732200.0000, 
sim time next is 1732800.0000, 
raw observation next is [0.4, 91.66666666666667, 0.0, 0.0, 19.0, 26.77534374407103, 0.8038784229153527, 0.0, 1.0, 65.0, 47705.69409491544], 
processed observation next is [0.0, 0.043478260869565216, 0.4736842105263158, 0.9166666666666667, 0.0, 0.0, 0.08333333333333333, 0.7312786453392525, 0.7679594743051176, 0.0, 1.0, 1.0, 0.22716997188054971], 
reward next is 0.7728, 
noisyNet noise sample is [array([-0.8348505], dtype=float32), 1.8335247]. 
=============================================
[2019-04-08 15:44:43,118] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4288901e-18 3.5088328e-11 4.0961801e-08 2.2846680e-09 9.5390475e-12
 1.2876853e-14 1.1570335e-10 1.0521930e-15 5.4518065e-20 8.3974616e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:43,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0114
[2019-04-08 15:44:43,157] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.0, 26.86317791388924, 0.8072184357279415, 0.0, 1.0, 65.0, 39240.2905142649], 
current ob forecast is [], 
actual action is [7.2, 65.0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 26.86928664101851, 0.8112698885179649, 0.0, 1.0, 65.0, 40010.95316632123], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.7391072200848757, 0.7704232961726549, 0.0, 1.0, 1.0, 0.19052834841105348], 
reward next is 0.8095, 
noisyNet noise sample is [array([0.40051308], dtype=float32), 0.8395953]. 
=============================================
[2019-04-08 15:44:43,162] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.56906167e-17 7.06025294e-10 2.12300364e-07 5.61369745e-07
 8.64856797e-11 3.96622785e-13 1.85657734e-09 1.03814723e-13
 1.19592375e-17 2.53789723e-20 9.99999166e-01], sum to 1.0000
[2019-04-08 15:44:43,163] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4852
[2019-04-08 15:44:43,186] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 19.0, 27.01776773394939, 0.9094708261525231, 0.0, 1.0, 65.0, 37104.77172135037], 
current ob forecast is [], 
actual action is [6.2666666666666675, 65.0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 19.0, 27.00607894595347, 0.9084608586312486, 0.0, 1.0, 65.0, 37300.93457166613], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.905, 0.0, 0.0, 0.08333333333333333, 0.7505065788294557, 0.8028202862104162, 0.0, 1.0, 1.0, 0.17762349796031487], 
reward next is 0.8224, 
noisyNet noise sample is [array([1.532157], dtype=float32), -0.19296706]. 
=============================================
[2019-04-08 15:44:43,936] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2830212e-18 2.7157792e-11 3.3006337e-08 2.0514661e-09 1.0484470e-11
 8.5814661e-15 6.4915984e-10 2.1970521e-16 2.6577686e-20 7.2244501e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:43,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6187
[2019-04-08 15:44:43,956] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 19.0, 26.83361578528228, 0.8388432015195719, 0.0, 1.0, 65.0, 39895.72563195766], 
current ob forecast is [], 
actual action is [6.6, 65.0], 
sim time this is 1469400.0000, 
sim time next is 1470000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 19.0, 26.8286356152746, 0.8365753308527316, 0.0, 1.0, 65.0, 39864.73490388966], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7357196346062166, 0.7788584436175773, 0.0, 1.0, 1.0, 0.18983207097090313], 
reward next is 0.8102, 
noisyNet noise sample is [array([1.5630068], dtype=float32), -0.7712427]. 
=============================================
[2019-04-08 15:44:43,971] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[82.01048 ]
 [82.280556]
 [82.659874]
 [82.66529 ]
 [82.65164 ]], R is [[81.47350311]
 [81.46878815]
 [81.46375275]
 [81.45826721]
 [81.452034  ]].
[2019-04-08 15:44:44,145] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.8791096e-17 7.7658741e-11 1.7017116e-07 7.9156302e-07 5.6569263e-11
 7.6666565e-13 3.8021120e-08 6.3053306e-15 1.0029085e-18 1.2480624e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:44:44,146] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4958
[2019-04-08 15:44:44,165] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.8, 92.0, 0.0, 0.0, 19.0, 27.02294380446483, 0.8451985815927906, 0.0, 1.0, 65.0, 34905.45378323428], 
current ob forecast is [], 
actual action is [6.8, 65.0], 
sim time this is 1473600.0000, 
sim time next is 1474200.0000, 
raw observation next is [1.9, 92.0, 0.0, 0.0, 19.0, 27.01522326001819, 0.8445459105070591, 0.0, 1.0, 65.0, 36120.64090603284], 
processed observation next is [1.0, 0.043478260869565216, 0.515235457063712, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7512686050015157, 0.7815153035023531, 0.0, 1.0, 1.0, 0.1720030519334897], 
reward next is 0.8280, 
noisyNet noise sample is [array([1.9758077], dtype=float32), 0.92217237]. 
=============================================
[2019-04-08 15:44:45,063] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6941427e-20 1.2287503e-11 2.8074373e-08 1.7685899e-08 1.3924175e-11
 3.7348478e-15 4.6450563e-10 7.5701055e-17 4.7416193e-20 1.6325506e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:45,063] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1338
[2019-04-08 15:44:45,080] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.266666666666667, 100.0, 15.0, 0.0, 22.5, 26.8782366999724, 0.8350548091741432, 1.0, 1.0, 65.0, 37882.64344795104], 
current ob forecast is [], 
actual action is [6.2666666666666675, 65.0], 
sim time this is 1498800.0000, 
sim time next is 1499400.0000, 
raw observation next is [1.35, 100.0, 18.0, 0.0, 22.5, 27.0986470201367, 0.8448796014467979, 1.0, 1.0, 65.0, 30717.33399134013], 
processed observation next is [1.0, 0.34782608695652173, 0.5000000000000001, 1.0, 0.06, 0.0, 0.375, 0.7582205850113916, 0.7816265338155993, 1.0, 1.0, 1.0, 0.14627301900638157], 
reward next is 0.8537, 
noisyNet noise sample is [array([0.58812535], dtype=float32), -0.45833305]. 
=============================================
[2019-04-08 15:44:45,094] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3368075e-20 1.0129889e-11 2.1923601e-08 1.4154644e-08 1.0673877e-11
 2.8322215e-15 3.4786812e-10 5.5622271e-17 3.0256373e-20 1.1349663e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:45,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5915
[2019-04-08 15:44:45,112] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.35, 100.0, 18.0, 0.0, 22.5, 27.0986470201367, 0.8448796014467979, 1.0, 1.0, 65.0, 30717.33399134013], 
current ob forecast is [], 
actual action is [6.35, 65.0], 
sim time this is 1499400.0000, 
sim time next is 1500000.0000, 
raw observation next is [1.433333333333333, 100.0, 22.83333333333333, 0.0, 22.5, 27.26464463436459, 0.8532210066720847, 1.0, 1.0, 65.0, 29480.34133585677], 
processed observation next is [1.0, 0.34782608695652173, 0.502308402585411, 1.0, 0.0761111111111111, 0.0, 0.375, 0.7720537195303825, 0.7844070022240283, 1.0, 1.0, 1.0, 0.14038257778979416], 
reward next is 0.8596, 
noisyNet noise sample is [array([0.58812535], dtype=float32), -0.45833305]. 
=============================================
[2019-04-08 15:44:45,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[86.35858]
 [85.82051]
 [86.15408]
 [85.6224 ]
 [85.43623]], R is [[85.74697876]
 [85.74324036]
 [85.70541382]
 [85.66007233]
 [85.61088562]].
[2019-04-08 15:44:45,260] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5071252e-17 1.2198859e-10 2.8932973e-07 1.3177861e-07 1.6159629e-10
 8.4945267e-13 1.5502419e-08 2.1365645e-14 8.0234177e-19 1.8481862e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:44:45,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4721
[2019-04-08 15:44:45,288] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 19.0, 26.84868775488958, 0.8037298638693281, 0.0, 1.0, 65.0, 39856.05475092844], 
current ob forecast is [], 
actual action is [7.2, 65.0], 
sim time this is 1487400.0000, 
sim time next is 1488000.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 19.0, 26.82048758239302, 0.8136731116674999, 0.0, 1.0, 65.0, 40919.66135996813], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.08333333333333333, 0.735040631866085, 0.7712243705558333, 0.0, 1.0, 1.0, 0.19485553028556252], 
reward next is 0.8051, 
noisyNet noise sample is [array([-0.19540028], dtype=float32), -1.1167918]. 
=============================================
[2019-04-08 15:44:45,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.09713]
 [84.07275]
 [84.03153]
 [84.0077 ]
 [83.96267]], R is [[84.10069275]
 [84.06990051]
 [84.04061127]
 [84.00795746]
 [83.97715759]].
[2019-04-08 15:44:45,487] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.1638780e-18 3.4975058e-11 4.0148525e-08 4.0555841e-07 2.1574350e-12
 2.4487205e-13 4.7459596e-09 1.2505014e-15 1.5654413e-18 3.5791039e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:44:45,490] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5383
[2019-04-08 15:44:45,505] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 91.33333333333334, 0.0, 0.0, 22.5, 27.28600645901409, 0.9432685746129335, 1.0, 1.0, 65.0, 32641.47120885214], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1450200.0000, 
sim time next is 1450800.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 22.5, 27.2578942157098, 0.9370063270214289, 1.0, 1.0, 65.0, 32716.54211562201], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.375, 0.7714911846424833, 0.8123354423404763, 1.0, 1.0, 1.0, 0.15579305769343815], 
reward next is 0.8442, 
noisyNet noise sample is [array([-0.76857424], dtype=float32), 0.71634007]. 
=============================================
[2019-04-08 15:44:45,830] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8647522e-16 4.5035423e-10 3.1732071e-08 1.3893734e-07 2.2021968e-11
 4.7757355e-13 4.5607087e-09 2.0371516e-13 1.4525378e-18 8.0823010e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:45,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1322
[2019-04-08 15:44:45,840] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.8, 60.33333333333334, 0.0, 0.0, 22.5, 28.5115389893563, 1.217630824266814, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [14.8, 65.0], 
sim time this is 1534800.0000, 
sim time next is 1535400.0000, 
raw observation next is [9.7, 60.5, 0.0, 0.0, 22.5, 28.43237338619846, 1.214165835585094, 1.0, 1.0, 65.0, 18848.76940634078], 
processed observation next is [1.0, 0.782608695652174, 0.7313019390581719, 0.605, 0.0, 0.0, 0.375, 0.8693644488498716, 0.9047219451950314, 1.0, 1.0, 1.0, 0.08975604479209895], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.51471007], dtype=float32), 0.5937512]. 
=============================================
[2019-04-08 15:44:46,021] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.9854098e-19 1.6167141e-12 9.0446960e-08 2.5752074e-08 1.8484598e-12
 6.6077372e-14 6.3952960e-10 4.2159748e-15 1.1857162e-19 1.0166245e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:46,023] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0097
[2019-04-08 15:44:46,040] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.2, 93.0, 0.0, 0.0, 19.0, 27.01656466971194, 0.8349333646804254, 0.0, 1.0, 65.0, 35049.40041957169], 
current ob forecast is [], 
actual action is [7.2, 65.0], 
sim time this is 1477800.0000, 
sim time next is 1478400.0000, 
raw observation next is [2.2, 93.33333333333334, 0.0, 0.0, 19.0, 26.98664324832561, 0.8332589997839808, 0.0, 1.0, 65.0, 37069.5582545665], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9333333333333335, 0.0, 0.0, 0.08333333333333333, 0.7488869373604675, 0.7777529999279936, 0.0, 1.0, 1.0, 0.1765217059741262], 
reward next is 0.8235, 
noisyNet noise sample is [array([-0.71170783], dtype=float32), 2.0046215]. 
=============================================
[2019-04-08 15:44:46,148] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.2357171e-18 7.6362877e-10 6.5747550e-08 3.0047599e-07 5.7236663e-11
 2.5603459e-13 2.9478566e-09 5.8615927e-14 3.7566149e-19 2.7372836e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:46,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8075
[2019-04-08 15:44:46,163] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.75, 83.66666666666667, 0.0, 0.0, 19.0, 27.33193147456489, 0.945435694254963, 0.0, 1.0, 65.0, 33104.1568055784], 
current ob forecast is [], 
actual action is [9.75, 65.0], 
sim time this is 1573800.0000, 
sim time next is 1574400.0000, 
raw observation next is [4.800000000000001, 83.33333333333334, 0.0, 0.0, 19.0, 27.32487383218805, 0.9481109469365306, 0.0, 1.0, 65.0, 33459.95359759407], 
processed observation next is [1.0, 0.21739130434782608, 0.5955678670360112, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.7770728193490042, 0.8160369823121769, 0.0, 1.0, 1.0, 0.15933311236949554], 
reward next is 0.8407, 
noisyNet noise sample is [array([0.25227895], dtype=float32), -1.3307709]. 
=============================================
[2019-04-08 15:44:46,294] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.38595195e-17 5.10546494e-10 2.63246932e-07 8.18885269e-07
 4.21831181e-11 9.26588450e-13 1.00680815e-08 6.03907598e-15
 1.58165319e-18 2.12524449e-21 9.99998927e-01], sum to 1.0000
[2019-04-08 15:44:46,298] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5659
[2019-04-08 15:44:46,312] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 19.0, 27.44327357931166, 0.9608872012663724, 0.0, 1.0, 65.0, 30827.70118451087], 
current ob forecast is [], 
actual action is [10.0, 65.0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 19.0, 27.50426229063599, 0.9514865510688056, 0.0, 1.0, 65.0, 30019.97987137462], 
processed observation next is [1.0, 0.2608695652173913, 0.6034164358264081, 0.8150000000000002, 0.0, 0.0, 0.08333333333333333, 0.792021857552999, 0.8171621836896019, 0.0, 1.0, 1.0, 0.1429522851017839], 
reward next is 0.8570, 
noisyNet noise sample is [array([0.5856211], dtype=float32), 1.570463]. 
=============================================
[2019-04-08 15:44:46,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2947893e-18 1.0894616e-11 9.3811252e-09 3.2038074e-07 2.1798416e-12
 7.1507690e-15 5.6669974e-10 5.7917309e-15 8.9560254e-20 2.8349561e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:46,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8519
[2019-04-08 15:44:46,903] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.9, 100.0, 47.0, 0.0, 22.5, 27.55281821556455, 0.9102753170468433, 1.0, 1.0, 65.0, 25597.470089439], 
current ob forecast is [], 
actual action is [6.9, 65.0], 
sim time this is 1503000.0000, 
sim time next is 1503600.0000, 
raw observation next is [2.0, 100.0, 51.33333333333334, 0.0, 22.5, 27.66868016137561, 0.92030916285266, 1.0, 1.0, 65.0, 24443.19762772387], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 1.0, 0.17111111111111113, 0.0, 0.375, 0.805723346781301, 0.8067697209508866, 1.0, 1.0, 1.0, 0.11639617917963747], 
reward next is 0.8836, 
noisyNet noise sample is [array([-0.8933957], dtype=float32), 0.5544659]. 
=============================================
[2019-04-08 15:44:46,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0037706e-18 3.0422492e-10 1.1634700e-08 2.9300766e-09 4.6730193e-11
 1.7161443e-13 4.3922950e-09 1.3907910e-14 7.8236946e-20 1.2198122e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:46,913] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8186
[2019-04-08 15:44:46,932] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.333333333333333, 70.66666666666666, 129.1666666666667, 128.0, 22.5, 28.13416881536197, 1.077858397295683, 1.0, 1.0, 65.0, 18843.81569820656], 
current ob forecast is [], 
actual action is [12.333333333333332, 65.0], 
sim time this is 1590000.0000, 
sim time next is 1590600.0000, 
raw observation next is [7.516666666666667, 69.33333333333334, 143.3333333333333, 120.0, 22.5, 28.19629462203009, 1.093771308241488, 1.0, 1.0, 65.0, 18843.8378292891], 
processed observation next is [1.0, 0.391304347826087, 0.6708217913204063, 0.6933333333333335, 0.47777777777777763, 0.13259668508287292, 0.375, 0.8496912185025076, 0.8645904360804959, 1.0, 1.0, 1.0, 0.08973256109185285], 
reward next is 0.9103, 
noisyNet noise sample is [array([-2.3608735], dtype=float32), -0.07949407]. 
=============================================
[2019-04-08 15:44:47,623] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5233317e-17 9.1093566e-10 1.2708001e-07 5.1015309e-06 7.8270368e-10
 9.4525809e-13 3.8050121e-08 2.1610014e-14 7.6032324e-17 1.2965919e-19
 9.9999475e-01], sum to 1.0000
[2019-04-08 15:44:47,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4401
[2019-04-08 15:44:47,628] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5426098e-19 5.2747608e-11 8.8809976e-08 2.4996021e-08 2.6671346e-11
 2.9915489e-14 2.6308606e-09 5.8707631e-16 1.0719534e-19 2.0268716e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:47,629] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3509
[2019-04-08 15:44:47,642] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.683333333333334, 81.0, 0.0, 0.0, 19.0, 27.5419987616217, 1.045404372555395, 0.0, 1.0, 65.0, 32657.33330025961], 
current ob forecast is [], 
actual action is [10.683333333333334, 65.0], 
sim time this is 1551000.0000, 
sim time next is 1551600.0000, 
raw observation next is [5.5, 82.0, 0.0, 0.0, 19.0, 27.50057398765311, 1.050355628144829, 0.0, 1.0, 65.0, 33216.03164961359], 
processed observation next is [1.0, 1.0, 0.6149584487534627, 0.82, 0.0, 0.0, 0.08333333333333333, 0.7917144989710924, 0.850118542714943, 0.0, 1.0, 1.0, 0.15817157928387426], 
reward next is 0.8418, 
noisyNet noise sample is [array([-0.05392595], dtype=float32), -0.70952237]. 
=============================================
[2019-04-08 15:44:47,655] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.5, 78.16666666666666, 0.0, 0.0, 19.0, 26.09984194646965, 0.5673545736970877, 0.0, 1.0, 65.0, 59162.59714074324], 
current ob forecast is [], 
actual action is [-0.5, 65.0], 
sim time this is 1817400.0000, 
sim time next is 1818000.0000, 
raw observation next is [-5.6, 78.0, 0.0, 0.0, 19.0, 26.1067440665402, 0.5639788267185036, 0.0, 1.0, 65.0, 58921.50062351258], 
processed observation next is [0.0, 0.043478260869565216, 0.30747922437673136, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6755620055450168, 0.6879929422395011, 0.0, 1.0, 1.0, 0.2805785743976789], 
reward next is 0.7194, 
noisyNet noise sample is [array([-0.37320703], dtype=float32), -0.63656855]. 
=============================================
[2019-04-08 15:44:47,673] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.631714]
 [74.63915 ]
 [74.93059 ]
 [75.055244]
 [75.65022 ]], R is [[74.51785278]
 [74.49095154]
 [74.46521759]
 [74.44063568]
 [74.4171524 ]].
[2019-04-08 15:44:47,736] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5506447e-18 7.7086344e-11 1.8028605e-07 6.7927175e-08 1.8829740e-11
 2.2295262e-13 1.1862792e-08 3.4479022e-15 1.1216936e-18 1.6535667e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:47,736] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3850
[2019-04-08 15:44:47,746] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.8, 49.0, 122.6666666666667, 0.0, 22.5, 28.22358072275549, 1.243755011798823, 1.0, 1.0, 64.9999999999999, 20023.69074889853], 
current ob forecast is [], 
actual action is [18.8, 65.0], 
sim time this is 1608600.0000, 
sim time next is 1609200.0000, 
raw observation next is [13.8, 49.0, 111.5, 0.0, 22.5, 28.70363471583651, 1.253265656344956, 1.0, 1.0, 65.0, 18848.31077093925], 
processed observation next is [1.0, 0.6521739130434783, 0.844875346260388, 0.49, 0.37166666666666665, 0.0, 0.375, 0.8919695596530426, 0.917755218781652, 1.0, 1.0, 1.0, 0.08975386081399643], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.2616367], dtype=float32), 0.099172875]. 
=============================================
[2019-04-08 15:44:48,027] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1901715e-18 1.2175100e-11 5.7161129e-09 5.2821587e-08 1.9210357e-11
 1.6686752e-14 1.8922686e-09 4.3596860e-16 4.3318596e-20 2.2551347e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:48,030] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6112
[2019-04-08 15:44:48,058] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.666666666666667, 95.0, 85.5, 590.0, 22.5, 27.93117801365331, 1.043437677700718, 1.0, 1.0, 64.99999999999997, 20839.71941137508], 
current ob forecast is [], 
actual action is [8.666666666666668, 65.0], 
sim time this is 1509600.0000, 
sim time next is 1510200.0000, 
raw observation next is [3.85, 94.5, 88.0, 708.0, 22.5, 28.0151673398788, 1.063062091374226, 1.0, 1.0, 65.0, 19106.11722265198], 
processed observation next is [1.0, 0.4782608695652174, 0.569252077562327, 0.945, 0.29333333333333333, 0.7823204419889502, 0.375, 0.8345972783232334, 0.8543540304580753, 1.0, 1.0, 1.0, 0.09098151058405705], 
reward next is 0.9090, 
noisyNet noise sample is [array([0.54625237], dtype=float32), -0.2486623]. 
=============================================
[2019-04-08 15:44:48,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0468569e-17 2.0059952e-11 6.8766177e-08 1.2804933e-08 1.2455483e-11
 3.6328745e-14 2.1322555e-09 4.8084409e-16 2.5842467e-19 6.3043517e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:48,249] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9527
[2019-04-08 15:44:48,266] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.91666666666667, 51.33333333333334, 83.66666666666667, 177.9999999999999, 22.5, 28.92415387006498, 1.283798357392421, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.91666666666667, 65.0], 
sim time this is 1527000.0000, 
sim time next is 1527600.0000, 
raw observation next is [11.63333333333333, 52.66666666666667, 85.33333333333333, 103.0, 22.5, 29.07380003821889, 1.286659235131228, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7848568790397045, 0.5266666666666667, 0.28444444444444444, 0.1138121546961326, 0.375, 0.9228166698515743, 0.9288864117104093, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2548263], dtype=float32), -1.257303]. 
=============================================
[2019-04-08 15:44:48,723] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3835082e-14 6.5957462e-09 1.3786818e-07 1.0066390e-06 4.9884834e-09
 4.3234531e-12 3.8145611e-08 8.5213119e-13 2.8808811e-16 1.9940649e-18
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:44:48,728] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6459
[2019-04-08 15:44:48,746] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.199999999999999, 79.0, 0.0, 0.0, 19.0, 25.9996017642832, 0.5143098867923681, 0.0, 1.0, 65.0, 60016.68709319232], 
current ob forecast is [], 
actual action is [-1.1999999999999993, 65.0], 
sim time this is 1834800.0000, 
sim time next is 1835400.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 19.0, 25.98361832866534, 0.5120444753694638, 0.0, 1.0, 65.0, 61306.43378514084], 
processed observation next is [0.0, 0.21739130434782608, 0.2908587257617729, 0.79, 0.0, 0.0, 0.08333333333333333, 0.6653015273887783, 0.6706814917898213, 0.0, 1.0, 1.0, 0.29193539897686116], 
reward next is 0.7081, 
noisyNet noise sample is [array([-0.543486], dtype=float32), 1.7080616]. 
=============================================
[2019-04-08 15:44:48,836] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7202538e-17 2.9889587e-11 2.4982727e-08 2.0962903e-08 5.1831057e-12
 1.2968359e-12 5.5717925e-10 6.8388904e-17 1.9111887e-20 7.0630481e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:48,840] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5569
[2019-04-08 15:44:48,850] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.9, 60.16666666666666, 0.0, 0.0, 22.5, 28.481751564968, 1.227366127229615, 1.0, 1.0, 65.0, 18849.44218565794], 
current ob forecast is [], 
actual action is [14.9, 65.0], 
sim time this is 1534200.0000, 
sim time next is 1534800.0000, 
raw observation next is [9.8, 60.33333333333334, 0.0, 0.0, 22.5, 28.51149457939299, 1.217620295337472, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7340720221606649, 0.6033333333333334, 0.0, 0.0, 0.375, 0.8759578816160826, 0.9058734317791574, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8924314], dtype=float32), 0.43374282]. 
=============================================
[2019-04-08 15:44:49,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7322442e-18 1.9954587e-11 1.0615544e-07 1.9120448e-07 8.3226577e-12
 2.0426021e-13 1.4365253e-09 7.9260235e-16 1.3354056e-20 1.3644466e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:49,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4740
[2019-04-08 15:44:49,837] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.49202707791365, 1.029183495958669, 0.0, 1.0, 65.0, 33090.08069459575], 
current ob forecast is [], 
actual action is [11.6, 65.0], 
sim time this is 1656000.0000, 
sim time next is 1656600.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.49677359795234, 1.033634444274807, 0.0, 1.0, 65.0, 32241.4109194536], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7913977998293618, 0.8445448147582691, 0.0, 1.0, 1.0, 0.15353052818787427], 
reward next is 0.8465, 
noisyNet noise sample is [array([0.8183426], dtype=float32), -0.88483125]. 
=============================================
[2019-04-08 15:44:50,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.16130639e-19 2.26596554e-11 1.46358605e-08 5.43043743e-09
 1.53135683e-13 8.35394238e-16 2.70458522e-09 4.14192494e-17
 9.31197864e-20 6.44996342e-23 1.00000000e+00], sum to 1.0000
[2019-04-08 15:44:50,687] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5275
[2019-04-08 15:44:50,703] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.2, 82.66666666666667, 0.0, 0.0, 19.0, 27.58347060227844, 1.090399262885027, 0.0, 1.0, 65.0, 34468.14540183133], 
current ob forecast is [], 
actual action is [12.2, 65.0], 
sim time this is 1638600.0000, 
sim time next is 1639200.0000, 
raw observation next is [7.200000000000001, 83.33333333333334, 0.0, 0.0, 19.0, 27.62530552330195, 1.081739992418999, 0.0, 1.0, 65.0, 32760.42984976728], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.8021087936084959, 0.8605799974729997, 0.0, 1.0, 1.0, 0.1560020469036537], 
reward next is 0.8440, 
noisyNet noise sample is [array([0.32710257], dtype=float32), 1.2369385]. 
=============================================
[2019-04-08 15:44:50,782] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2902660e-20 1.7034959e-11 4.0499315e-09 8.1083220e-09 2.9180811e-12
 2.9336747e-15 5.9053151e-10 8.2127868e-18 7.3003898e-20 4.9440783e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:50,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5979
[2019-04-08 15:44:50,792] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.55, 64.5, 200.0, 88.0, 22.5, 28.42737279268278, 1.135518596087345, 1.0, 1.0, 65.0, 18845.84933702851], 
current ob forecast is [], 
actual action is [13.55, 65.0], 
sim time this is 1593000.0000, 
sim time next is 1593600.0000, 
raw observation next is [8.833333333333334, 63.33333333333333, 202.6666666666667, 114.8333333333333, 22.5, 28.42762731518877, 1.150387131636354, 1.0, 1.0, 65.0, 18846.2626356414], 
processed observation next is [1.0, 0.43478260869565216, 0.7072945521698984, 0.6333333333333333, 0.6755555555555557, 0.12688766114180475, 0.375, 0.8689689429323973, 0.8834623772121181, 1.0, 1.0, 1.0, 0.08974410778876857], 
reward next is 0.9103, 
noisyNet noise sample is [array([2.2751362], dtype=float32), 1.2182171]. 
=============================================
[2019-04-08 15:44:50,942] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7706889e-15 2.1203634e-09 3.4770829e-07 1.1709816e-07 1.1037285e-10
 6.5144325e-13 7.7736511e-09 6.2642925e-14 2.6461981e-17 3.1973441e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:44:50,943] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1676
[2019-04-08 15:44:50,958] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 79.0, 29.0, 0.0, 19.0, 26.25565758023261, 0.5684687718000002, 0.0, 1.0, 65.0, 53793.23186495101], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 1873800.0000, 
sim time next is 1874400.0000, 
raw observation next is [-4.5, 80.33333333333333, 24.33333333333334, 0.0, 19.0, 26.24807512931464, 0.5651186277725873, 0.0, 1.0, 65.0, 53945.63328185459], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.8033333333333332, 0.08111111111111113, 0.0, 0.08333333333333333, 0.6873395941095533, 0.6883728759241957, 0.0, 1.0, 1.0, 0.2568839680088314], 
reward next is 0.7431, 
noisyNet noise sample is [array([1.0503502], dtype=float32), -0.38135698]. 
=============================================
[2019-04-08 15:44:51,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7365751e-20 7.6355797e-12 4.2581433e-07 3.8081307e-08 9.8828281e-13
 2.1455436e-15 1.3981633e-10 1.4631831e-16 9.0196889e-21 7.3352381e-24
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:44:51,303] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4482
[2019-04-08 15:44:51,316] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.8, 96.66666666666666, 0.0, 0.0, 19.0, 27.71531577459334, 1.065281053208195, 0.0, 1.0, 65.0, 28421.83821843093], 
current ob forecast is [], 
actual action is [11.8, 65.0], 
sim time this is 1651200.0000, 
sim time next is 1651800.0000, 
raw observation next is [6.699999999999999, 96.83333333333334, 0.0, 0.0, 19.0, 27.74640309841895, 1.053600642923749, 0.0, 1.0, 65.0, 28495.91908403463], 
processed observation next is [1.0, 0.08695652173913043, 0.6481994459833795, 0.9683333333333334, 0.0, 0.0, 0.08333333333333333, 0.812200258201579, 0.8512002143079164, 0.0, 1.0, 1.0, 0.1356948527811173], 
reward next is 0.8643, 
noisyNet noise sample is [array([0.433478], dtype=float32), -1.7451215]. 
=============================================
[2019-04-08 15:44:52,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8931547e-14 4.0317554e-09 7.8876161e-07 1.0861298e-06 6.6693127e-09
 7.7627184e-12 2.5534534e-06 8.5129881e-12 3.5185328e-16 2.0484223e-17
 9.9999559e-01], sum to 1.0000
[2019-04-08 15:44:52,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0282
[2019-04-08 15:44:52,358] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.300000000000001, 80.0, 0.0, 0.0, 19.0, 25.90076386808899, 0.4728314004645296, 0.0, 1.0, 65.0, 59276.33721064134], 
current ob forecast is [], 
actual action is [-2.3000000000000007, 65.0], 
sim time this is 1898400.0000, 
sim time next is 1899000.0000, 
raw observation next is [-7.3, 80.5, 0.0, 0.0, 19.0, 25.88808114427454, 0.4702216487263661, 0.0, 1.0, 65.0, 59310.21876408714], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.805, 0.0, 0.0, 0.08333333333333333, 0.6573400953562117, 0.6567405495754554, 0.0, 1.0, 1.0, 0.28242961316231974], 
reward next is 0.7176, 
noisyNet noise sample is [array([0.5146298], dtype=float32), -0.59283847]. 
=============================================
[2019-04-08 15:44:52,387] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.42673]
 [77.50782]
 [77.60922]
 [77.70479]
 [77.8132 ]], R is [[77.25321198]
 [77.19841766]
 [77.14483643]
 [77.09226227]
 [77.0400238 ]].
[2019-04-08 15:44:52,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6640666e-18 2.5590593e-12 3.8420849e-09 2.9310005e-08 2.1141958e-11
 1.2882626e-14 1.6594879e-09 2.3037208e-16 5.8469042e-20 2.2582156e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:52,582] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7658
[2019-04-08 15:44:52,594] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.54954311141465, 1.043073888668901, 0.0, 1.0, 65.0, 33009.79435932432], 
current ob forecast is [], 
actual action is [11.6, 65.0], 
sim time this is 1653600.0000, 
sim time next is 1654200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.56557986557835, 1.034921488474917, 0.0, 1.0, 65.0, 31783.72594736571], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7971316554648625, 0.844973829491639, 0.0, 1.0, 1.0, 0.1513510759398367], 
reward next is 0.8486, 
noisyNet noise sample is [array([-0.8788152], dtype=float32), 1.3181832]. 
=============================================
[2019-04-08 15:44:52,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6304246e-18 2.5364192e-12 3.7979464e-09 2.9095450e-08 2.0928674e-11
 1.2665639e-14 1.6356212e-09 2.2513511e-16 5.6662357e-20 2.1665001e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:52,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7991
[2019-04-08 15:44:52,642] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.53158406039621, 1.026455000291351, 0.0, 1.0, 65.0, 32187.31186009599], 
current ob forecast is [], 
actual action is [11.6, 65.0], 
sim time this is 1654800.0000, 
sim time next is 1655400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.47806120124801, 1.031200267715657, 0.0, 1.0, 65.0, 33995.06462160819], 
processed observation next is [1.0, 0.13043478260869565, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7898384334373342, 0.8437334225718857, 0.0, 1.0, 1.0, 0.16188126010289614], 
reward next is 0.8381, 
noisyNet noise sample is [array([-0.8788152], dtype=float32), 1.3181832]. 
=============================================
[2019-04-08 15:44:52,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2572625e-19 3.6950484e-11 5.3023959e-09 2.0421989e-08 2.1553601e-11
 2.5765462e-14 4.2833581e-10 1.0246285e-15 5.8919275e-21 1.1345594e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:52,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9823
[2019-04-08 15:44:52,897] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.8, 49.0, 111.5, 0.0, 22.5, 28.70363375676101, 1.253265362246131, 1.0, 1.0, 65.0, 18848.31076171614], 
current ob forecast is [], 
actual action is [18.8, 65.0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 22.5, 28.87632529021757, 1.27032236430173, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8425669436749772, 0.4933333333333334, 0.3344444444444443, 0.0, 0.375, 0.906360440851464, 0.9234407881005767, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27689433], dtype=float32), 1.0120875]. 
=============================================
[2019-04-08 15:44:52,926] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4658843e-18 1.7533360e-11 6.8435845e-08 4.9969753e-06 5.6371494e-11
 1.4927849e-14 3.5262759e-09 1.1265483e-15 3.3936473e-19 1.1874007e-23
 9.9999487e-01], sum to 1.0000
[2019-04-08 15:44:52,926] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7833
[2019-04-08 15:44:52,940] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 19.0, 27.51489056559373, 1.028255923705318, 0.0, 1.0, 65.0, 33177.12483821545], 
current ob forecast is [], 
actual action is [11.6, 65.0], 
sim time this is 1659000.0000, 
sim time next is 1659600.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 19.0, 27.52836979895001, 1.029283268847437, 0.0, 1.0, 65.0, 32249.5567049678], 
processed observation next is [1.0, 0.21739130434782608, 0.6454293628808865, 0.97, 0.0, 0.0, 0.08333333333333333, 0.7940308165791675, 0.8430944229491457, 0.0, 1.0, 1.0, 0.15356931764270382], 
reward next is 0.8464, 
noisyNet noise sample is [array([-0.14876874], dtype=float32), 0.35286075]. 
=============================================
[2019-04-08 15:44:53,797] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.9520449e-18 2.7888275e-10 4.2342219e-07 3.6807242e-07 1.0689036e-09
 1.3195245e-13 3.8957265e-10 1.3882449e-15 2.8805053e-18 1.3212216e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:44:53,799] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4596
[2019-04-08 15:44:53,821] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.95, 63.5, 0.0, 0.0, 22.5, 28.48921671330179, 1.240124516114881, 1.0, 1.0, 65.0, 18848.31173589056], 
current ob forecast is [], 
actual action is [14.95, 65.0], 
sim time this is 1621800.0000, 
sim time next is 1622400.0000, 
raw observation next is [9.766666666666667, 64.33333333333334, 0.0, 0.0, 22.5, 28.41024158059683, 1.230724135996775, 0.0, 1.0, 65.0, 18848.17999020386], 
processed observation next is [1.0, 0.782608695652174, 0.7331486611265007, 0.6433333333333334, 0.0, 0.0, 0.375, 0.8675201317164024, 0.9102413786655917, 0.0, 1.0, 1.0, 0.0897532380485898], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.35514557], dtype=float32), 0.53598064]. 
=============================================
[2019-04-08 15:44:54,017] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.6171755e-18 3.6036842e-13 1.5680858e-08 6.6825689e-10 8.0026860e-14
 4.3501033e-15 1.6024138e-09 3.6153255e-17 2.1986406e-20 1.2209515e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:44:54,019] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0235
[2019-04-08 15:44:54,031] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.266666666666666, 71.33333333333333, 0.0, 0.0, 22.5, 28.03549741570916, 1.193983213684548, 1.0, 1.0, 65.0, 18844.52102288105], 
current ob forecast is [], 
actual action is [13.266666666666666, 65.0], 
sim time this is 1626000.0000, 
sim time next is 1626600.0000, 
raw observation next is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 22.5, 28.02545330366688, 1.18912705269919, 1.0, 1.0, 65.0, 18915.14397664489], 
processed observation next is [1.0, 0.8260869565217391, 0.6837488457987074, 0.7266666666666667, 0.0, 0.0, 0.375, 0.83545444197224, 0.8963756842330634, 1.0, 1.0, 1.0, 0.09007211417449948], 
reward next is 0.9099, 
noisyNet noise sample is [array([-0.43227905], dtype=float32), 1.0709207]. 
=============================================
[2019-04-08 15:44:55,600] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.2219124e-18 1.9633990e-10 3.5046163e-08 3.5200267e-07 6.8253133e-12
 1.5732797e-12 9.5411510e-09 3.9273536e-14 9.4693045e-20 5.8546887e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:44:55,600] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3591
[2019-04-08 15:44:55,638] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 22.5, 27.62745836826449, 0.8633186882775462, 1.0, 1.0, 65.0, 47817.06113926692], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1708200.0000, 
sim time next is 1708800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 22.5, 26.65823537722203, 0.9655293006759549, 1.0, 1.0, 65.0, 69625.64813880344], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.375, 0.7215196147685026, 0.8218431002253183, 1.0, 1.0, 1.0, 0.33155070542287357], 
reward next is 0.6684, 
noisyNet noise sample is [array([-0.06103827], dtype=float32), -0.24477133]. 
=============================================
[2019-04-08 15:44:56,142] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.8709712e-18 1.3939008e-10 1.3730091e-08 6.7262825e-08 5.5611189e-11
 9.6495425e-15 6.3874119e-09 7.4638731e-15 2.9063448e-19 1.2254736e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:44:56,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4700
[2019-04-08 15:44:56,165] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.35, 84.5, 53.0, 0.0, 22.5, 27.92717224917414, 1.047180148601129, 1.0, 1.0, 65.0, 29164.05144156832], 
current ob forecast is [], 
actual action is [6.35, 65.0], 
sim time this is 1697400.0000, 
sim time next is 1698000.0000, 
raw observation next is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 22.5, 27.97243625395449, 1.062835406607968, 1.0, 1.0, 65.0, 29284.53135579418], 
processed observation next is [1.0, 0.6521739130434783, 0.502308402585411, 0.8333333333333335, 0.16388888888888892, 0.0, 0.375, 0.8310363544962076, 0.8542784688693227, 1.0, 1.0, 1.0, 0.13945014931330563], 
reward next is 0.8605, 
noisyNet noise sample is [array([-1.1121616], dtype=float32), 1.4686595]. 
=============================================
[2019-04-08 15:44:56,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.279106]
 [84.38123 ]
 [84.38535 ]
 [84.47976 ]
 [84.47917 ]], R is [[84.2249527 ]
 [84.24382782]
 [84.28408813]
 [84.28425598]
 [84.15516663]].
[2019-04-08 15:44:56,236] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4890141e-18 2.4086703e-11 4.4910262e-08 1.2076315e-06 2.2700982e-11
 6.1158641e-13 1.2779689e-08 6.4360206e-15 5.6708158e-19 3.7660346e-21
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:44:56,238] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9374
[2019-04-08 15:44:56,265] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.516666666666667, 82.16666666666666, 45.33333333333334, 0.0, 22.5, 28.00805755796714, 1.064183370123288, 1.0, 1.0, 65.0, 27708.55950916924], 
current ob forecast is [], 
actual action is [6.5166666666666675, 65.0], 
sim time this is 1698600.0000, 
sim time next is 1699200.0000, 
raw observation next is [1.6, 81.0, 41.5, 0.0, 22.5, 28.05077824888785, 1.067838115588007, 1.0, 1.0, 65.0, 26864.94796596727], 
processed observation next is [1.0, 0.6956521739130435, 0.5069252077562327, 0.81, 0.13833333333333334, 0.0, 0.375, 0.8375648540739874, 0.8559460385293356, 1.0, 1.0, 1.0, 0.12792832364746318], 
reward next is 0.8721, 
noisyNet noise sample is [array([1.1927896], dtype=float32), -0.0160446]. 
=============================================
[2019-04-08 15:44:58,643] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0486319e-16 6.5019412e-10 3.7990901e-08 5.9969534e-07 4.7573549e-11
 1.2650939e-12 3.9115456e-08 1.2715832e-14 6.3293937e-19 3.8672915e-21
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:44:58,644] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1519
[2019-04-08 15:44:58,659] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 88.0, 15.5, 0.0, 22.5, 28.03629384548486, 1.030529164987769, 1.0, 1.0, 65.0, 24934.86307074362], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 1702800.0000, 
sim time next is 1703400.0000, 
raw observation next is [1.1, 88.00000000000001, 10.66666666666666, 0.0, 22.5, 28.07228464085642, 1.046492008535986, 1.0, 1.0, 65.0, 27304.76430708369], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8800000000000001, 0.035555555555555535, 0.0, 0.375, 0.8393570534047017, 0.8488306695119953, 1.0, 1.0, 1.0, 0.130022687176589], 
reward next is 0.8700, 
noisyNet noise sample is [array([-0.73946005], dtype=float32), -1.155432]. 
=============================================
[2019-04-08 15:44:59,308] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0196446e-16 7.0465050e-11 1.1738812e-07 9.4693270e-08 4.6136389e-10
 7.2558257e-13 8.4103586e-09 4.7171238e-14 1.4927075e-17 1.4756876e-18
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:44:59,308] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3706
[2019-04-08 15:44:59,359] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.616666666666667, 87.0, 0.0, 0.0, 19.0, 26.53160960462588, 0.7250046848538236, 0.0, 1.0, 65.0, 51957.10142411337], 
current ob forecast is [], 
actual action is [3.383333333333333, 65.0], 
sim time this is 1752600.0000, 
sim time next is 1753200.0000, 
raw observation next is [-1.7, 87.0, 0.0, 0.0, 19.0, 26.52349162166757, 0.7252315734459062, 0.0, 1.0, 65.0, 52054.153857024], 
processed observation next is [0.0, 0.30434782608695654, 0.4155124653739613, 0.87, 0.0, 0.0, 0.08333333333333333, 0.7102909684722976, 0.741743857815302, 0.0, 1.0, 1.0, 0.24787692312868573], 
reward next is 0.7521, 
noisyNet noise sample is [array([0.49613804], dtype=float32), -0.666018]. 
=============================================
[2019-04-08 15:44:59,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2624735e-16 4.9138610e-10 8.4840522e-08 6.9358532e-07 7.7288446e-11
 1.3708055e-11 2.5911445e-08 1.1992131e-13 2.3870313e-17 3.3097008e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:44:59,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2300
[2019-04-08 15:44:59,889] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 83.66666666666667, 0.0, 0.0, 19.0, 26.61303172591403, 0.7531328849583678, 0.0, 1.0, 65.0, 50349.34216095111], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 1745400.0000, 
sim time next is 1746000.0000, 
raw observation next is [-0.6, 83.0, 0.0, 0.0, 19.0, 26.60772325958445, 0.7510461670296408, 0.0, 1.0, 65.0, 50452.66240595845], 
processed observation next is [0.0, 0.21739130434782608, 0.44598337950138506, 0.83, 0.0, 0.0, 0.08333333333333333, 0.7173102716320375, 0.7503487223432136, 0.0, 1.0, 1.0, 0.24025077336170692], 
reward next is 0.7597, 
noisyNet noise sample is [array([0.04292145], dtype=float32), -0.8078857]. 
=============================================
[2019-04-08 15:44:59,930] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.07417 ]
 [75.16914 ]
 [75.245865]
 [75.35242 ]
 [75.41393 ]], R is [[75.00260162]
 [75.01281738]
 [75.02342987]
 [75.03443146]
 [75.04585266]].
[2019-04-08 15:45:00,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.84181136e-17 4.15736445e-10 1.71620727e-08 2.16393389e-07
 2.77425912e-11 1.73821331e-12 4.60203164e-09 2.20789200e-14
 1.23113195e-17 1.02675080e-19 9.99999762e-01], sum to 1.0000
[2019-04-08 15:45:00,202] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6500
[2019-04-08 15:45:00,227] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.2, 87.0, 59.66666666666666, 0.0, 19.0, 26.54005106233368, 0.7019196301835128, 0.0, 1.0, 65.0, 51484.49928300514], 
current ob forecast is [], 
actual action is [1.7999999999999998, 65.0], 
sim time this is 1784400.0000, 
sim time next is 1785000.0000, 
raw observation next is [-3.3, 87.0, 53.33333333333334, 0.0, 19.0, 26.52515619750081, 0.6973256772879816, 0.0, 1.0, 65.0, 52222.47034251919], 
processed observation next is [0.0, 0.6521739130434783, 0.37119113573407203, 0.87, 0.1777777777777778, 0.0, 0.08333333333333333, 0.7104296831250675, 0.7324418924293271, 0.0, 1.0, 1.0, 0.24867843020247235], 
reward next is 0.7513, 
noisyNet noise sample is [array([0.91427475], dtype=float32), 0.42417103]. 
=============================================
[2019-04-08 15:45:00,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.44172 ]
 [72.45693 ]
 [72.49887 ]
 [72.498924]
 [72.52367 ]], R is [[72.43614197]
 [72.4666214 ]
 [72.4932251 ]
 [72.5174408 ]
 [72.54180908]].
[2019-04-08 15:45:00,892] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.06248097e-18 2.44038696e-11 4.51388331e-08 6.97818408e-08
 5.48329090e-12 1.04515734e-13 3.80309251e-09 4.92161108e-14
 3.40733634e-19 9.86986899e-21 9.99999881e-01], sum to 1.0000
[2019-04-08 15:45:00,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6259
[2019-04-08 15:45:00,913] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.166666666666667, 86.83333333333333, 19.66666666666667, 0.0, 22.5, 26.09693393934389, 0.5030294285657891, 1.0, 1.0, 65.0, 52679.82937413593], 
current ob forecast is [], 
actual action is [-1.166666666666667, 65.0], 
sim time this is 2016600.0000, 
sim time next is 2017200.0000, 
raw observation next is [-6.133333333333334, 86.66666666666667, 24.33333333333334, 0.0, 22.5, 26.07376594649046, 0.5413486581033354, 1.0, 1.0, 64.99999999999997, 51151.31112969956], 
processed observation next is [1.0, 0.34782608695652173, 0.2927054478301016, 0.8666666666666667, 0.08111111111111113, 0.0, 0.375, 0.672813828874205, 0.6804495527011118, 1.0, 1.0, 0.9999999999999994, 0.24357767204618838], 
reward next is 0.7564, 
noisyNet noise sample is [array([2.4358974], dtype=float32), -0.29015854]. 
=============================================
[2019-04-08 15:45:01,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4503098e-17 1.7636430e-09 1.5928563e-06 7.3846235e-07 3.2190509e-10
 4.2404964e-12 6.9595991e-08 8.8682208e-14 7.9920121e-16 7.6980664e-19
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:45:01,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7824
[2019-04-08 15:45:01,082] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.8, 83.0, 109.0, 0.0, 19.0, 26.58449682450519, 0.7170357070417995, 0.0, 1.0, 65.0, 50239.82590514031], 
current ob forecast is [], 
actual action is [2.2, 65.0], 
sim time this is 1778400.0000, 
sim time next is 1779000.0000, 
raw observation next is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 19.0, 26.56565300303398, 0.7157120293153459, 0.0, 1.0, 65.0, 51649.87674680192], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8366666666666667, 0.3522222222222223, 0.0, 0.08333333333333333, 0.7138044169194983, 0.7385706764384486, 0.0, 1.0, 1.0, 0.2459517940323901], 
reward next is 0.7540, 
noisyNet noise sample is [array([0.8361234], dtype=float32), -0.26179376]. 
=============================================
[2019-04-08 15:45:01,109] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.94303]
 [75.01917]
 [75.10788]
 [75.18356]
 [75.29902]], R is [[74.86763763]
 [74.87973022]
 [74.89002991]
 [74.89261627]
 [74.8957901 ]].
[2019-04-08 15:45:02,296] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9196934e-16 3.2418899e-09 5.8744804e-06 1.3361727e-06 1.7563430e-09
 2.4044430e-10 1.4334307e-07 2.0394548e-12 1.4165987e-16 5.4027463e-18
 9.9999261e-01], sum to 1.0000
[2019-04-08 15:45:02,296] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1768
[2019-04-08 15:45:02,315] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 19.0, 26.39923785782982, 0.6670672264473692, 0.0, 1.0, 65.0, 54741.62740537209], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 1790400.0000, 
sim time next is 1791000.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 26.39029583043875, 0.6644977514786576, 0.0, 1.0, 65.0, 54883.80450234954], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.6991913192032291, 0.721499250492886, 0.0, 1.0, 1.0, 0.2613514500111883], 
reward next is 0.7386, 
noisyNet noise sample is [array([-0.58127934], dtype=float32), 0.07805409]. 
=============================================
[2019-04-08 15:45:02,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.55043 ]
 [70.5811  ]
 [70.617935]
 [70.65241 ]
 [70.69593 ]], R is [[70.52522278]
 [70.55929565]
 [70.59371185]
 [70.62852478]
 [70.66374207]].
[2019-04-08 15:45:02,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7383540e-14 2.5342322e-09 1.2035503e-06 8.0900156e-07 7.4564649e-10
 5.0558710e-11 5.5468579e-08 3.6233058e-13 1.3492005e-16 2.0821779e-17
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:45:02,657] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8910
[2019-04-08 15:45:02,688] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.933333333333333, 82.16666666666667, 0.0, 0.0, 19.0, 26.07789615541269, 0.55373337092834, 0.0, 1.0, 65.0, 59936.40512094965], 
current ob forecast is [], 
actual action is [-0.9333333333333327, 65.0], 
sim time this is 1821000.0000, 
sim time next is 1821600.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 19.0, 26.06549246721561, 0.5512963122164726, 0.0, 1.0, 65.0, 60312.76497266443], 
processed observation next is [0.0, 0.08695652173913043, 0.296398891966759, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6721243722679674, 0.6837654374054908, 0.0, 1.0, 1.0, 0.28720364272697346], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.4564156], dtype=float32), 0.4753589]. 
=============================================
[2019-04-08 15:45:02,751] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-08 15:45:02,752] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:45:02,752] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:45:02,753] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:45:02,753] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:45:02,756] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:45:02,757] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:45:02,763] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run17
[2019-04-08 15:45:02,785] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run17
[2019-04-08 15:45:02,801] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run17
[2019-04-08 15:45:22,435] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:45:22,435] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [-9.8, 77.0, 0.0, 0.0, 19.0, 25.58619257905702, 0.4542134312846678, 0.0, 1.0, 65.0, 62709.16997948364]
[2019-04-08 15:45:22,435] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:45:22,436] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [8.7697082e-15 4.0176129e-09 1.0257289e-06 1.9133959e-06 2.3196964e-09
 3.0294010e-11 2.0825519e-07 2.2049925e-12 8.0780436e-16 1.7231580e-17
 9.9999690e-01], sampled 0.7174712651793729
[2019-04-08 15:45:22,557] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:45:22,557] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-9.919411509, 57.92541309, 0.0, 0.0, 19.0, 24.9432837367999, 0.2770885413348103, 0.0, 1.0, 65.0, 61836.98014221769]
[2019-04-08 15:45:22,557] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:45:22,558] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [1.8401379e-13 3.7599385e-08 5.4359330e-06 8.1240696e-06 1.4826513e-08
 3.2290853e-10 6.9143755e-07 3.3211302e-11 3.3469433e-14 7.9961858e-16
 9.9998569e-01], sampled 0.5196562665213259
[2019-04-08 15:45:29,041] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:45:29,041] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-4.833374238, 68.09231304, 0.4059002505, 56.9609867, 19.0, 26.20697332087221, 0.55421517335667, 0.0, 1.0, 65.0, 55669.5813041968]
[2019-04-08 15:45:29,041] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:45:29,042] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [7.2130488e-15 5.3289422e-09 1.1412209e-06 1.8545106e-06 2.1168947e-09
 2.6773915e-11 1.5830409e-07 2.2373590e-12 9.8230993e-16 1.6666669e-17
 9.9999678e-01], sampled 0.7496010934823566
[2019-04-08 15:45:43,340] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:45:43,341] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [2.682276308, 100.0, 0.0, 0.0, 19.0, 26.70049105476192, 0.773401535149539, 0.0, 1.0, 65.0, 42577.27959906319]
[2019-04-08 15:45:43,341] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:45:43,342] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [2.1649368e-16 4.7121318e-10 2.0370655e-07 3.9020995e-07 2.4982783e-10
 1.7687336e-12 3.5790041e-08 9.6728337e-14 1.5925100e-17 2.0417969e-19
 9.9999940e-01], sampled 0.5097085104042974
[2019-04-08 15:45:59,847] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:45:59,847] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-6.7, 78.0, 0.0, 0.0, 19.0, 25.9438981466038, 0.5468666724248115, 0.0, 1.0, 65.0, 58252.01009038781]
[2019-04-08 15:45:59,848] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:45:59,848] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [6.2786499e-15 5.1528062e-09 1.1971691e-06 1.8437343e-06 2.0082573e-09
 2.0910171e-11 1.4007161e-07 1.8135972e-12 8.2538183e-16 1.3762707e-17
 9.9999678e-01], sampled 0.718412557723386
[2019-04-08 15:46:25,795] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:46:25,796] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [4.233333333333333, 58.66666666666667, 0.0, 0.0, 19.0, 27.16696174014258, 0.8422662608373704, 0.0, 1.0, 65.0, 39344.90310517055]
[2019-04-08 15:46:25,796] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:46:25,797] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [1.0258083e-15 1.5921690e-09 4.7500717e-07 7.4149835e-07 7.6279971e-10
 5.4863076e-12 7.2742111e-08 4.2646778e-13 1.1496832e-16 1.9577576e-18
 9.9999869e-01], sampled 0.9500881680200438
[2019-04-08 15:46:31,523] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:46:31,523] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [1.0, 51.0, 0.0, 0.0, 22.5, 27.86172200447292, 1.038804318213125, 1.0, 1.0, 65.0, 25049.21519212214]
[2019-04-08 15:46:31,523] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:46:31,524] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [1.8510821e-15 4.2392196e-09 8.3435356e-07 9.3993901e-07 8.1618817e-10
 9.6926859e-12 5.1372915e-08 7.0392596e-13 2.7674512e-16 4.1124897e-18
 9.9999821e-01], sampled 0.8223226725267986
[2019-04-08 15:46:31,748] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.060989957]
[2019-04-08 15:46:31,748] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-1.0, 59.16666666666666, 0.0, 0.0, 19.0, 27.13338193870474, 0.8682085978736098, 0.0, 1.0, 65.0, 39945.1176161418]
[2019-04-08 15:46:31,749] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:46:31,750] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [1.7387184e-15 2.6432290e-09 7.0744102e-07 1.0633872e-06 9.8146025e-10
 9.3778943e-12 7.6403040e-08 7.0592918e-13 2.5490287e-16 3.3060920e-18
 9.9999809e-01], sampled 0.14335026277827934
[2019-04-08 15:46:41,521] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.3526 316185964.4079 2958.0493
[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,546] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:41,668] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,399] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.1041 355928145.3215 2370.6014
[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,439] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:48,549] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:52,888] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.5223 342820311.4356 2767.9935
[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:52,917] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,029] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:46:53,920] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 320000, evaluation results [320000.0, 6863.522326496907, 342820311.4356497, 2767.993481239964, 6990.352550438706, 316185964.4078659, 2958.049253090378, 6801.104069897809, 355928145.3214582, 2370.601379178921]
[2019-04-08 15:46:53,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4033264e-17 8.4574409e-10 1.5348171e-06 2.0737102e-06 1.1162418e-09
 4.1383268e-12 1.8389541e-07 4.7527161e-14 6.8442929e-17 4.7782274e-19
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:46:53,992] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7158
[2019-04-08 15:46:54,010] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 85.33333333333334, 0.0, 0.0, 19.0, 26.23729790411429, 0.600972470363168, 0.0, 1.0, 65.0, 57955.31377460847], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 1807800.0000, 
sim time next is 1808400.0000, 
raw observation next is [-5.0, 84.66666666666667, 0.0, 0.0, 19.0, 26.21234091392197, 0.5976171723238118, 0.0, 1.0, 65.0, 57967.71571720323], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8466666666666667, 0.0, 0.0, 0.08333333333333333, 0.6843617428268308, 0.6992057241079372, 0.0, 1.0, 1.0, 0.27603674151049157], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.14781688], dtype=float32), 0.5783227]. 
=============================================
[2019-04-08 15:46:54,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8128924e-14 5.4222400e-09 5.0058935e-07 8.6821689e-07 1.1952386e-09
 2.3582185e-11 4.8459225e-07 4.3668276e-12 1.8294186e-15 2.2669602e-18
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:46:54,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9846
[2019-04-08 15:46:54,179] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 83.0, 0.0, 0.0, 19.0, 26.06549179627216, 0.5512961054126841, 0.0, 1.0, 65.0, 60312.77133716598], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 1821600.0000, 
sim time next is 1822200.0000, 
raw observation next is [-6.033333333333333, 83.66666666666667, 0.0, 0.0, 19.0, 26.05226588008793, 0.55191545854091, 0.0, 1.0, 65.0, 60280.42637583478], 
processed observation next is [0.0, 0.08695652173913043, 0.29547553093259465, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6710221566739941, 0.6839718195136366, 0.0, 1.0, 1.0, 0.28704964940873706], 
reward next is 0.7130, 
noisyNet noise sample is [array([-0.945968], dtype=float32), 0.81338036]. 
=============================================
[2019-04-08 15:46:54,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8081545e-14 4.9684155e-09 4.9665488e-07 8.7200078e-07 1.1369470e-09
 2.4235213e-11 5.2709908e-07 4.0062480e-12 1.7375448e-15 2.2021473e-18
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:46:54,236] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9913
[2019-04-08 15:46:54,260] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.066666666666666, 84.33333333333334, 0.0, 0.0, 19.0, 26.06389504993788, 0.5477284368615969, 0.0, 1.0, 65.0, 59717.95440682187], 
current ob forecast is [], 
actual action is [-1.0666666666666664, 65.0], 
sim time this is 1822800.0000, 
sim time next is 1823400.0000, 
raw observation next is [-6.1, 85.0, 0.0, 0.0, 19.0, 26.05043573657954, 0.544285912830332, 0.0, 1.0, 65.0, 60063.02340956838], 
processed observation next is [0.0, 0.08695652173913043, 0.29362880886426596, 0.85, 0.0, 0.0, 0.08333333333333333, 0.6708696447149617, 0.6814286376101107, 0.0, 1.0, 1.0, 0.28601439718842087], 
reward next is 0.7140, 
noisyNet noise sample is [array([-0.945968], dtype=float32), 0.81338036]. 
=============================================
[2019-04-08 15:46:55,570] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.09261609e-14 8.40014014e-09 1.52633368e-06 9.50370350e-07
 1.07216085e-08 6.04172753e-11 3.15788839e-07 1.34815260e-12
 1.20555976e-15 3.12319153e-16 9.99997139e-01], sum to 1.0000
[2019-04-08 15:46:55,575] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7694
[2019-04-08 15:46:55,626] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 86.33333333333333, 0.0, 0.0, 19.0, 25.99861211451179, 0.5347004426318427, 0.0, 1.0, 65.0, 60914.58551620231], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 65.0], 
sim time this is 1825800.0000, 
sim time next is 1826400.0000, 
raw observation next is [-6.2, 85.66666666666667, 0.0, 0.0, 19.0, 25.98862012633362, 0.5432023390602599, 0.0, 1.0, 65.0, 60903.21068022685], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8566666666666667, 0.0, 0.0, 0.08333333333333333, 0.6657183438611348, 0.68106744635342, 0.0, 1.0, 1.0, 0.2900152889534612], 
reward next is 0.7100, 
noisyNet noise sample is [array([1.1925272], dtype=float32), 0.43407634]. 
=============================================
[2019-04-08 15:46:55,813] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6588644e-16 7.0706989e-11 6.3943510e-08 5.5847249e-08 5.1823167e-11
 6.1652620e-13 3.6375014e-09 2.0848915e-14 4.9504481e-18 6.1592805e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:46:55,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2681
[2019-04-08 15:46:55,846] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.06290927205697, 0.5118581328114683, 0.0, 1.0, 65.0, 58229.32304398099], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 1890000.0000, 
sim time next is 1890600.0000, 
raw observation next is [-5.7, 81.66666666666667, 0.0, 0.0, 19.0, 26.05223426226709, 0.5089309380006843, 0.0, 1.0, 65.0, 58033.14709487084], 
processed observation next is [0.0, 0.9130434782608695, 0.30470914127423826, 0.8166666666666668, 0.0, 0.0, 0.08333333333333333, 0.6710195218555907, 0.6696436460002282, 0.0, 1.0, 1.0, 0.27634831949938493], 
reward next is 0.7237, 
noisyNet noise sample is [array([-1.3871746], dtype=float32), 0.7163034]. 
=============================================
[2019-04-08 15:46:56,154] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0863589e-16 5.3580640e-10 3.0662061e-08 1.0725939e-07 3.2179089e-11
 6.3268128e-12 2.3876041e-08 1.9252410e-13 9.7722530e-18 1.9627325e-18
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:46:56,154] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0786
[2019-04-08 15:46:56,176] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.2, 86.33333333333333, 75.66666666666666, 0.0, 19.0, 26.55277662504053, 0.7161571180128621, 0.0, 1.0, 65.0, 50735.42876759545], 
current ob forecast is [], 
actual action is [2.8, 65.0], 
sim time this is 1763400.0000, 
sim time next is 1764000.0000, 
raw observation next is [-2.3, 87.0, 81.0, 0.0, 19.0, 26.54070088048564, 0.7127460158775446, 0.0, 1.0, 65.0, 50909.60732791791], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.27, 0.0, 0.08333333333333333, 0.7117250733738034, 0.7375820052925149, 0.0, 1.0, 1.0, 0.24242670156151386], 
reward next is 0.7576, 
noisyNet noise sample is [array([0.44005814], dtype=float32), 1.0619255]. 
=============================================
[2019-04-08 15:46:56,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.46879 ]
 [72.474915]
 [72.46235 ]
 [72.48026 ]
 [72.44347 ]], R is [[72.50769043]
 [72.54101562]
 [72.56639099]
 [72.59487915]
 [72.62351227]].
[2019-04-08 15:46:56,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6269864e-13 2.4837263e-08 1.7320126e-05 3.3457989e-05 5.0009181e-08
 7.5757284e-10 1.8628651e-06 5.0676416e-12 7.6752357e-15 2.4197571e-16
 9.9994731e-01], sum to 1.0000
[2019-04-08 15:46:56,431] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6351
[2019-04-08 15:46:56,454] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 19.0, 26.29489080457486, 0.6276921266671751, 0.0, 1.0, 65.0, 56581.2011383899], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 1800000.0000, 
sim time next is 1800600.0000, 
raw observation next is [-4.583333333333333, 83.5, 0.0, 0.0, 19.0, 26.280849112407, 0.6251193873281515, 0.0, 1.0, 65.0, 56837.85246044327], 
processed observation next is [0.0, 0.8695652173913043, 0.3356417359187443, 0.835, 0.0, 0.0, 0.08333333333333333, 0.69007075936725, 0.7083731291093839, 0.0, 1.0, 1.0, 0.2706564402878251], 
reward next is 0.7293, 
noisyNet noise sample is [array([0.43177253], dtype=float32), -0.57944036]. 
=============================================
[2019-04-08 15:46:56,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0062800e-16 1.7963252e-09 1.7156152e-06 1.2347454e-06 2.9587544e-09
 1.3003480e-10 7.0548305e-09 1.1502544e-11 1.4739916e-16 1.2331733e-17
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:46:56,946] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2970
[2019-04-08 15:46:56,968] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.716666666666667, 77.5, 0.0, 0.0, 19.0, 25.95801926279972, 0.4589087274236217, 0.0, 1.0, 65.0, 57969.55078037445], 
current ob forecast is [], 
actual action is [-2.716666666666667, 65.0], 
sim time this is 1907400.0000, 
sim time next is 1908000.0000, 
raw observation next is [-7.8, 78.0, 0.0, 0.0, 19.0, 25.94627484226806, 0.4576470899867207, 0.0, 1.0, 65.0, 58483.34182175577], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.78, 0.0, 0.0, 0.08333333333333333, 0.662189570189005, 0.6525490299955735, 0.0, 1.0, 1.0, 0.27849210391312273], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.87595904], dtype=float32), -0.37383828]. 
=============================================
[2019-04-08 15:46:56,976] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.309715]
 [71.32791 ]
 [71.345024]
 [71.04038 ]
 [71.01542 ]], R is [[71.59651184]
 [71.60449982]
 [71.60991669]
 [71.61093903]
 [71.61108398]].
[2019-04-08 15:46:58,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6731471e-14 1.8439248e-08 1.6025753e-06 3.5353037e-06 3.5858447e-09
 2.6193972e-11 4.0293023e-07 3.5604254e-12 1.7890321e-15 1.1427697e-16
 9.9999440e-01], sum to 1.0000
[2019-04-08 15:46:58,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2163
[2019-04-08 15:46:58,351] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.75, 77.0, 0.0, 0.0, 19.0, 25.96649732589604, 0.4874303298942939, 0.0, 1.0, 65.0, 58892.83092096535], 
current ob forecast is [], 
actual action is [-1.75, 65.0], 
sim time this is 1895400.0000, 
sim time next is 1896000.0000, 
raw observation next is [-6.933333333333334, 77.66666666666667, 0.0, 0.0, 19.0, 25.95247091905705, 0.4843038340856105, 0.0, 1.0, 65.0, 58994.32113480612], 
processed observation next is [0.0, 0.9565217391304348, 0.270544783010157, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6627059099214208, 0.6614346113618702, 0.0, 1.0, 1.0, 0.280925338737172], 
reward next is 0.7191, 
noisyNet noise sample is [array([2.495071], dtype=float32), 0.46339914]. 
=============================================
[2019-04-08 15:46:58,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[70.28343 ]
 [70.406624]
 [70.48933 ]
 [70.59701 ]
 [70.696526]], R is [[70.2156601 ]
 [70.23306274]
 [70.25088501]
 [70.26869965]
 [70.28603363]].
[2019-04-08 15:46:58,957] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2291008e-15 2.0462659e-10 1.9434866e-07 2.9196789e-07 4.5849072e-10
 6.9335049e-13 2.4079003e-07 2.7152267e-12 2.4856272e-16 1.2683808e-18
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:46:58,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2780
[2019-04-08 15:46:58,979] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.916666666666667, 85.5, 0.0, 0.0, 19.0, 26.20194269578318, 0.5500686829943185, 0.0, 1.0, 65.0, 54749.44848600415], 
current ob forecast is [], 
actual action is [0.08333333333333304, 65.0], 
sim time this is 1878600.0000, 
sim time next is 1879200.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 26.18708212234621, 0.5480201371498747, 0.0, 1.0, 65.0, 54843.23085587427], 
processed observation next is [0.0, 0.782608695652174, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.682256843528851, 0.6826733790499583, 0.0, 1.0, 1.0, 0.2611582421708299], 
reward next is 0.7388, 
noisyNet noise sample is [array([-2.7047532], dtype=float32), 0.6488712]. 
=============================================
[2019-04-08 15:47:00,397] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9921583e-16 1.5077639e-09 1.3277879e-07 2.4965416e-06 7.2038997e-10
 3.7914831e-11 9.5439724e-08 2.3045883e-13 1.0651617e-15 4.0029019e-18
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:47:00,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7175
[2019-04-08 15:47:00,416] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.1, 76.33333333333333, 0.0, 0.0, 19.0, 26.0119111665593, 0.4991228906916018, 0.0, 1.0, 65.0, 58731.26809239177], 
current ob forecast is [], 
actual action is [-1.0999999999999996, 65.0], 
sim time this is 1893000.0000, 
sim time next is 1893600.0000, 
raw observation next is [-6.2, 75.0, 0.0, 0.0, 19.0, 26.00281261333269, 0.496625844340226, 0.0, 1.0, 65.0, 58794.60201613462], 
processed observation next is [0.0, 0.9565217391304348, 0.2908587257617729, 0.75, 0.0, 0.0, 0.08333333333333333, 0.6669010511110575, 0.6655419481134087, 0.0, 1.0, 1.0, 0.27997429531492674], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.16000687], dtype=float32), -1.0098436]. 
=============================================
[2019-04-08 15:47:00,850] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.4017421e-15 5.4471661e-10 3.7148396e-08 1.3228380e-07 2.7106528e-09
 2.3207306e-12 1.8068391e-08 1.0392420e-12 2.3580275e-16 1.4318194e-18
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:47:00,851] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2271
[2019-04-08 15:47:00,869] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.583333333333333, 71.0, 136.6666666666667, 13.33333333333333, 19.0, 26.11272406915815, 0.5574966574192203, 0.0, 1.0, 65.0, 56823.19316645405], 
current ob forecast is [], 
actual action is [0.41666666666666696, 65.0], 
sim time this is 1860600.0000, 
sim time next is 1861200.0000, 
raw observation next is [-4.5, 71.0, 145.0, 20.0, 19.0, 26.11889484662773, 0.5606457413563591, 0.0, 1.0, 65.0, 56639.55047219351], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.48333333333333334, 0.022099447513812154, 0.08333333333333333, 0.6765745705523107, 0.686881913785453, 0.0, 1.0, 1.0, 0.26971214510568337], 
reward next is 0.7303, 
noisyNet noise sample is [array([-1.6439588], dtype=float32), 1.1736115]. 
=============================================
[2019-04-08 15:47:01,668] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.5584214e-15 4.9834237e-10 5.9016031e-07 1.6608750e-07 1.1428446e-09
 1.3887244e-11 8.8063338e-08 9.0525542e-13 3.5040792e-16 1.8078777e-17
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:47:01,672] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3327
[2019-04-08 15:47:01,690] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 83.0, 15.0, 0.0, 19.0, 26.22213704219779, 0.5614446293905762, 0.0, 1.0, 65.0, 54702.18040011189], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [-4.583333333333333, 83.5, 10.33333333333333, 0.0, 19.0, 26.2171971559752, 0.5584638584974387, 0.0, 1.0, 65.0, 54525.26249871706], 
processed observation next is [0.0, 0.7391304347826086, 0.3356417359187443, 0.835, 0.03444444444444444, 0.0, 0.08333333333333333, 0.6847664296646, 0.6861546194991462, 0.0, 1.0, 1.0, 0.2596441071367479], 
reward next is 0.7404, 
noisyNet noise sample is [array([0.18261509], dtype=float32), 0.4261366]. 
=============================================
[2019-04-08 15:47:02,116] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.4174773e-17 9.1210506e-09 1.5972169e-06 4.8800407e-06 5.8749801e-09
 1.7181328e-11 7.7339585e-08 1.3105127e-13 1.0973686e-16 1.9852424e-18
 9.9999344e-01], sum to 1.0000
[2019-04-08 15:47:02,119] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6025
[2019-04-08 15:47:02,151] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.566666666666666, 80.16666666666667, 113.6666666666667, 444.6666666666667, 22.5, 26.62000615726078, 0.5861380908677826, 1.0, 1.0, 65.0, 43446.0925049364], 
current ob forecast is [], 
actual action is [-2.5666666666666664, 65.0], 
sim time this is 1936200.0000, 
sim time next is 1936800.0000, 
raw observation next is [-7.3, 79.0, 128.0, 392.5, 22.5, 26.65040809236068, 0.6075071952345699, 1.0, 1.0, 65.0, 42635.14991512454], 
processed observation next is [1.0, 0.43478260869565216, 0.26038781163434904, 0.79, 0.4266666666666667, 0.43370165745856354, 0.375, 0.7208673410300568, 0.7025023984115233, 1.0, 1.0, 1.0, 0.20302452340535493], 
reward next is 0.7970, 
noisyNet noise sample is [array([0.69825935], dtype=float32), 2.9674957]. 
=============================================
[2019-04-08 15:47:02,466] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3308960e-15 1.7890718e-09 5.3747303e-06 5.8321448e-06 1.2151402e-10
 3.1363975e-10 2.6758274e-08 4.2213393e-13 1.6982753e-16 8.7481058e-18
 9.9998879e-01], sum to 1.0000
[2019-04-08 15:47:02,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5504
[2019-04-08 15:47:02,487] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 19.0, 25.94620049775752, 0.4408728884994003, 0.0, 1.0, 65.0, 57738.05311985681], 
current ob forecast is [], 
actual action is [-3.4000000000000004, 65.0], 
sim time this is 1913400.0000, 
sim time next is 1914000.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 19.0, 25.9051025192143, 0.4411541877995253, 0.0, 1.0, 65.0, 59052.01546019726], 
processed observation next is [1.0, 0.13043478260869565, 0.2299168975069252, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6587585432678583, 0.6470513959331751, 0.0, 1.0, 1.0, 0.28120007361998695], 
reward next is 0.7188, 
noisyNet noise sample is [array([-0.71431094], dtype=float32), -0.6093302]. 
=============================================
[2019-04-08 15:47:02,520] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.61758 ]
 [71.679535]
 [71.6975  ]
 [71.54513 ]
 [71.605064]], R is [[71.58916473]
 [71.59833527]
 [71.60237885]
 [71.59997559]
 [71.61875153]].
[2019-04-08 15:47:02,521] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7737087e-16 1.7540898e-10 5.2486399e-08 1.0939121e-06 1.2013809e-09
 1.4605556e-12 4.0978065e-09 4.1634939e-14 5.3107282e-17 5.4154879e-19
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:47:02,521] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8844
[2019-04-08 15:47:02,536] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.55, 76.5, 0.0, 0.0, 19.0, 25.91691682227848, 0.4616482191382802, 0.0, 1.0, 65.0, 59405.78680441773], 
current ob forecast is [], 
actual action is [-2.55, 65.0], 
sim time this is 1906200.0000, 
sim time next is 1906800.0000, 
raw observation next is [-7.633333333333333, 77.0, 0.0, 0.0, 19.0, 25.92373246487378, 0.4635765035280294, 0.0, 1.0, 65.0, 58494.4320834648], 
processed observation next is [1.0, 0.043478260869565216, 0.2511542012927055, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6603110387394816, 0.6545255011760098, 0.0, 1.0, 1.0, 0.2785449146831657], 
reward next is 0.7215, 
noisyNet noise sample is [array([-1.2043886], dtype=float32), -0.30188277]. 
=============================================
[2019-04-08 15:47:02,711] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.4297772e-16 1.2682768e-08 3.4722511e-06 4.1557777e-07 3.4605546e-10
 7.9621596e-13 1.9425935e-08 6.3619146e-13 5.1223234e-18 1.2636819e-19
 9.9999607e-01], sum to 1.0000
[2019-04-08 15:47:02,715] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5127
[2019-04-08 15:47:02,726] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.633333333333334, 65.0, 227.8333333333333, 5.0, 22.5, 27.09542591120888, 0.6815848434987783, 1.0, 1.0, 65.0, 39080.15767313181], 
current ob forecast is [], 
actual action is [0.36666666666666625, 65.0], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [-4.45, 65.0, 227.0, 4.0, 22.5, 26.98202434858058, 0.6904845246575668, 1.0, 1.0, 65.0, 42704.26067924088], 
processed observation next is [1.0, 0.5217391304347826, 0.3393351800554017, 0.65, 0.7566666666666667, 0.004419889502762431, 0.375, 0.7485020290483817, 0.7301615082191889, 1.0, 1.0, 1.0, 0.2033536222820994], 
reward next is 0.7966, 
noisyNet noise sample is [array([0.9541158], dtype=float32), 1.7869185]. 
=============================================
[2019-04-08 15:47:02,762] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.4857751e-16 2.9915035e-09 1.4385185e-07 3.6746820e-07 4.9076916e-09
 3.2369207e-12 5.1020262e-08 4.2584791e-13 1.1442418e-15 9.9087304e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:47:02,763] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0712
[2019-04-08 15:47:02,787] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.566666666666666, 76.33333333333334, 0.0, 0.0, 19.0, 25.98025202311914, 0.4905625175079593, 0.0, 1.0, 65.0, 58767.68076604715], 
current ob forecast is [], 
actual action is [-1.5666666666666664, 65.0], 
sim time this is 1894800.0000, 
sim time next is 1895400.0000, 
raw observation next is [-6.75, 77.0, 0.0, 0.0, 19.0, 25.96649703113249, 0.4874302460863167, 0.0, 1.0, 65.0, 58892.83238624985], 
processed observation next is [0.0, 0.9565217391304348, 0.275623268698061, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6638747525943742, 0.6624767486954389, 0.0, 1.0, 1.0, 0.28044205898214214], 
reward next is 0.7196, 
noisyNet noise sample is [array([1.768826], dtype=float32), 0.37063545]. 
=============================================
[2019-04-08 15:47:03,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0586816e-15 7.8560364e-10 1.4000340e-06 1.5443518e-07 1.3514879e-09
 7.5684424e-11 1.6429173e-08 1.0432603e-12 3.8186409e-17 5.8782748e-19
 9.9999845e-01], sum to 1.0000
[2019-04-08 15:47:03,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7741
[2019-04-08 15:47:03,542] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.683333333333334, 73.0, 0.0, 0.0, 22.5, 26.59333330655964, 0.6811935055939923, 1.0, 1.0, 65.0, 43636.98102268148], 
current ob forecast is [], 
actual action is [0.31666666666666643, 65.0], 
sim time this is 1969800.0000, 
sim time next is 1970400.0000, 
raw observation next is [-4.866666666666667, 75.0, 0.0, 0.0, 22.5, 26.6282030254922, 0.6717050822126055, 1.0, 1.0, 65.0, 44564.75680324888], 
processed observation next is [1.0, 0.8260869565217391, 0.3277931671283472, 0.75, 0.0, 0.0, 0.375, 0.7190169187910168, 0.7239016940708685, 1.0, 1.0, 1.0, 0.21221312763451847], 
reward next is 0.7878, 
noisyNet noise sample is [array([0.42967018], dtype=float32), 1.4220092]. 
=============================================
[2019-04-08 15:47:03,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8675837e-17 2.3639627e-11 8.2426260e-07 1.2111734e-07 1.4614297e-10
 8.5998515e-13 4.2576822e-09 1.8310664e-13 1.3489331e-18 2.9202658e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:47:03,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9132
[2019-04-08 15:47:03,713] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.083333333333333, 75.66666666666667, 13.33333333333333, 1.333333333333333, 22.5, 27.2608906213829, 0.7472557468649245, 1.0, 1.0, 65.0, 37846.58647557384], 
current ob forecast is [], 
actual action is [0.916666666666667, 65.0], 
sim time this is 1962600.0000, 
sim time next is 1963200.0000, 
raw observation next is [-4.266666666666667, 76.33333333333334, 9.166666666666666, 1.666666666666667, 22.5, 27.23776511663201, 0.7227248730990694, 1.0, 1.0, 65.0, 39111.43585355027], 
processed observation next is [1.0, 0.7391304347826086, 0.3444136657433057, 0.7633333333333334, 0.030555555555555555, 0.0018416206261510132, 0.375, 0.7698137597193341, 0.7409082910330231, 1.0, 1.0, 1.0, 0.18624493263595365], 
reward next is 0.8138, 
noisyNet noise sample is [array([-2.0110044], dtype=float32), -0.5498451]. 
=============================================
[2019-04-08 15:47:03,713] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0109250e-16 5.5314259e-10 4.4404638e-07 5.6461381e-06 2.8066788e-10
 1.2309313e-11 2.8985324e-08 3.4772200e-14 2.5724031e-16 1.4100862e-18
 9.9999392e-01], sum to 1.0000
[2019-04-08 15:47:03,721] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6903
[2019-04-08 15:47:03,739] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.03193703839252, 0.5514744505157053, 0.0, 1.0, 65.0, 55547.42436054315], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 19.0, 26.03714171030806, 0.5485412878615684, 0.0, 1.0, 65.0, 55181.09811190563], 
processed observation next is [1.0, 0.0, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.08333333333333333, 0.6697618091923383, 0.6828470959538562, 0.0, 1.0, 1.0, 0.2627671338662173], 
reward next is 0.7372, 
noisyNet noise sample is [array([-0.6130908], dtype=float32), 1.3046691]. 
=============================================
[2019-04-08 15:47:04,115] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6783580e-15 5.2470286e-09 5.0726862e-07 1.9630468e-06 1.1426988e-09
 1.2080929e-10 5.3753762e-08 5.1699877e-13 9.1557487e-18 6.1258783e-19
 9.9999750e-01], sum to 1.0000
[2019-04-08 15:47:04,120] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1619
[2019-04-08 15:47:04,135] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 26.15226306746933, 0.5303760503369456, 0.0, 1.0, 65.0, 52130.36048396631], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 65.0], 
sim time this is 2005200.0000, 
sim time next is 2005800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 26.18196398715553, 0.5262615082530346, 0.0, 1.0, 65.0, 50725.2963257547], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6818303322629609, 0.6754205027510115, 0.0, 1.0, 1.0, 0.24154903012264145], 
reward next is 0.7585, 
noisyNet noise sample is [array([-0.8001893], dtype=float32), 0.019163534]. 
=============================================
[2019-04-08 15:47:04,234] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4034583e-15 1.4241561e-09 9.6244669e-08 1.3585468e-07 3.5966161e-10
 1.5850781e-12 2.6469227e-08 3.6020343e-13 4.9816122e-17 3.7835957e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:47:04,235] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.3295
[2019-04-08 15:47:04,251] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.1, 82.5, 85.0, 549.0, 22.5, 26.57891106729118, 0.5680163715241965, 1.0, 1.0, 65.0, 43378.87248632176], 
current ob forecast is [], 
actual action is [-3.0999999999999996, 65.0], 
sim time this is 1935000.0000, 
sim time next is 1935600.0000, 
raw observation next is [-7.833333333333334, 81.33333333333334, 99.33333333333334, 496.8333333333333, 22.5, 26.58871618176607, 0.5770767445227043, 1.0, 1.0, 65.0, 43727.04813970367], 
processed observation next is [1.0, 0.391304347826087, 0.2456140350877193, 0.8133333333333335, 0.33111111111111113, 0.548987108655617, 0.375, 0.7157263484805059, 0.6923589148409014, 1.0, 1.0, 1.0, 0.20822403876049367], 
reward next is 0.7918, 
noisyNet noise sample is [array([0.02771717], dtype=float32), -0.8299614]. 
=============================================
[2019-04-08 15:47:05,278] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.17587455e-17 6.01803607e-11 3.41748709e-08 7.70674944e-07
 6.15660634e-10 2.15252027e-14 7.66635111e-09 4.64724965e-14
 1.04624749e-17 4.53156261e-20 9.99999166e-01], sum to 1.0000
[2019-04-08 15:47:05,281] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7103
[2019-04-08 15:47:05,304] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.100000000000001, 83.0, 0.0, 0.0, 19.0, 26.21303672363519, 0.5771472631750889, 0.0, 1.0, 65.0, 51780.45734481505], 
current ob forecast is [], 
actual action is [-1.1000000000000014, 65.0], 
sim time this is 1980600.0000, 
sim time next is 1981200.0000, 
raw observation next is [-6.0, 83.0, 0.0, 0.0, 19.0, 26.21133406883171, 0.5789628601126454, 0.0, 1.0, 65.0, 55523.61065585359], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6842778390693093, 0.6929876200375484, 0.0, 1.0, 1.0, 0.2643981459802552], 
reward next is 0.7356, 
noisyNet noise sample is [array([2.361037], dtype=float32), 0.031766694]. 
=============================================
[2019-04-08 15:47:05,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3147339e-17 2.1621777e-11 1.4153294e-07 3.7986655e-07 6.8364425e-11
 9.1340875e-14 1.6230158e-09 1.5973039e-14 4.7254070e-18 1.0068650e-18
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:47:05,457] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4985
[2019-04-08 15:47:05,482] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.15906500957556, 0.5440165668675576, 0.0, 1.0, 65.0, 51096.00169850267], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 1996200.0000, 
sim time next is 1996800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.17866680879035, 0.5444332576068635, 0.0, 1.0, 65.0, 50967.81120303038], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6815555673991959, 0.6814777525356212, 0.0, 1.0, 1.0, 0.24270386287157325], 
reward next is 0.7573, 
noisyNet noise sample is [array([-0.21284188], dtype=float32), 1.3764473]. 
=============================================
[2019-04-08 15:47:05,613] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2135660e-15 3.1111272e-10 4.2323137e-07 1.4124270e-08 1.3197510e-10
 2.7467150e-12 7.8067712e-09 3.5830100e-14 6.6987356e-17 1.6501273e-18
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:47:05,616] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3952
[2019-04-08 15:47:05,635] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.700000000000001, 77.5, 0.0, 0.0, 19.0, 25.93157099913645, 0.5456498039391262, 0.0, 1.0, 65.0, 58305.13595042156], 
current ob forecast is [], 
actual action is [-1.700000000000001, 65.0], 
sim time this is 2247000.0000, 
sim time next is 2247600.0000, 
raw observation next is [-6.700000000000001, 77.0, 0.0, 0.0, 19.0, 25.93596313159281, 0.5482476404066644, 0.0, 1.0, 65.0, 57969.45715382948], 
processed observation next is [1.0, 0.0, 0.2770083102493075, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6613302609660675, 0.6827492134688882, 0.0, 1.0, 1.0, 0.2760450340658547], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.4550779], dtype=float32), 0.9412936]. 
=============================================
[2019-04-08 15:47:07,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2136862e-17 1.0242048e-10 1.6688479e-07 1.5788103e-07 4.3593136e-11
 6.8726643e-13 2.1083744e-09 1.4850485e-13 5.8462615e-18 2.7664574e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:47:07,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7422
[2019-04-08 15:47:07,671] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 102.0, 0.0, 22.5, 26.81743543343005, 0.6350274884672144, 1.0, 1.0, 65.0, 43096.25438499868], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 2025000.0000, 
sim time next is 2025600.0000, 
raw observation next is [-5.6, 83.0, 109.5, 0.0, 22.5, 26.84514574188893, 0.6368313800852428, 1.0, 1.0, 65.0, 43076.48315900752], 
processed observation next is [1.0, 0.43478260869565216, 0.30747922437673136, 0.83, 0.365, 0.0, 0.375, 0.7370954784907443, 0.7122771266950809, 1.0, 1.0, 1.0, 0.20512611028098818], 
reward next is 0.7949, 
noisyNet noise sample is [array([-0.69893444], dtype=float32), 0.28164297]. 
=============================================
[2019-04-08 15:47:07,696] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5933676e-17 7.6201941e-11 4.9697633e-08 6.1077849e-08 8.3197747e-11
 1.4192216e-12 3.2204769e-10 3.1690709e-15 8.0928601e-19 3.0179271e-19
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:47:07,696] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3045
[2019-04-08 15:47:07,715] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.566666666666666, 71.33333333333333, 175.1666666666667, 60.0, 22.5, 26.78037803725581, 0.6384677539140957, 1.0, 1.0, 65.0, 40662.54035581883], 
current ob forecast is [], 
actual action is [-0.5666666666666664, 65.0], 
sim time this is 2284800.0000, 
sim time next is 2285400.0000, 
raw observation next is [-5.283333333333333, 69.66666666666667, 172.3333333333333, 70.0, 22.5, 26.82288200609624, 0.6492444333418124, 1.0, 1.0, 65.0, 39922.11338358382], 
processed observation next is [1.0, 0.43478260869565216, 0.31625115420129274, 0.6966666666666668, 0.5744444444444443, 0.07734806629834254, 0.375, 0.7352401671746867, 0.7164148111139376, 1.0, 1.0, 1.0, 0.1901053018265896], 
reward next is 0.8099, 
noisyNet noise sample is [array([0.9495143], dtype=float32), -0.9398857]. 
=============================================
[2019-04-08 15:47:08,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5224284e-16 4.4545131e-11 3.2371412e-07 3.3032416e-06 9.1706358e-09
 3.2685575e-12 9.6816146e-09 8.3501917e-14 1.1457400e-16 3.9006325e-20
 9.9999630e-01], sum to 1.0000
[2019-04-08 15:47:08,019] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9744
[2019-04-08 15:47:08,041] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 77.66666666666667, 154.6666666666667, 0.0, 22.5, 26.7918589377467, 0.6840836778146425, 1.0, 1.0, 65.0, 46436.03288124465], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2032800.0000, 
sim time next is 2033400.0000, 
raw observation next is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 22.5, 26.81727629989125, 0.6945204913347673, 1.0, 1.0, 65.0, 43784.37089344974], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7833333333333334, 0.511111111111111, 0.0, 0.375, 0.7347730249909376, 0.7315068304449225, 1.0, 1.0, 1.0, 0.20849700425452258], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.02981721], dtype=float32), 0.13734764]. 
=============================================
[2019-04-08 15:47:09,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7727341e-17 1.9368850e-10 8.6441804e-07 1.0483642e-07 6.1322752e-10
 8.7895169e-12 9.9555306e-09 4.0724741e-14 2.5845329e-17 8.0378958e-21
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:47:09,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8666
[2019-04-08 15:47:09,423] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 75.0, 151.5, 0.0, 22.5, 27.01995908776797, 0.694334452874799, 1.0, 1.0, 65.0, 40368.24101933655], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2030400.0000, 
sim time next is 2031000.0000, 
raw observation next is [-4.5, 75.66666666666667, 153.0, 0.0, 22.5, 27.04300416286051, 0.700619772240597, 1.0, 1.0, 65.0, 39877.69158354455], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7566666666666667, 0.51, 0.0, 0.375, 0.7535836802383757, 0.733539924080199, 1.0, 1.0, 1.0, 0.18989376944545022], 
reward next is 0.8101, 
noisyNet noise sample is [array([0.8442168], dtype=float32), -1.0986344]. 
=============================================
[2019-04-08 15:47:09,446] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.5053 ]
 [78.52184]
 [78.53824]
 [78.52048]
 [78.51051]], R is [[78.49913788]
 [78.52191925]
 [78.54174042]
 [78.56134033]
 [78.57688904]].
[2019-04-08 15:47:09,748] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.1370970e-17 5.8275794e-11 2.4059457e-07 3.1799445e-08 8.2412280e-12
 1.0593775e-12 2.4097334e-08 6.2389635e-15 2.6951910e-17 1.6340777e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:47:09,749] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-08 15:47:09,767] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 22.5, 26.8599857216443, 0.7040776124664946, 1.0, 1.0, 65.0, 43958.1404030628], 
current ob forecast is [], 
actual action is [0.6999999999999993, 65.0], 
sim time this is 2035200.0000, 
sim time next is 2035800.0000, 
raw observation next is [-4.2, 79.0, 148.0, 0.0, 22.5, 26.92441989509304, 0.7151951579451074, 1.0, 1.0, 65.0, 41115.32695033561], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.49333333333333335, 0.0, 0.375, 0.7437016579244199, 0.7383983859817024, 1.0, 1.0, 1.0, 0.19578727119207434], 
reward next is 0.8042, 
noisyNet noise sample is [array([1.0030848], dtype=float32), 0.052197956]. 
=============================================
[2019-04-08 15:47:09,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1089047e-17 2.4355057e-10 3.8233718e-08 1.9592530e-06 2.4632982e-10
 6.0107957e-13 9.1891756e-09 4.1210346e-13 6.4174541e-16 5.3832697e-19
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:47:09,797] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2415
[2019-04-08 15:47:09,823] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.916666666666667, 86.0, 0.0, 0.0, 19.0, 26.09105111845214, 0.5564919876465851, 0.0, 1.0, 65.0, 56307.84225196003], 
current ob forecast is [], 
actual action is [0.08333333333333304, 65.0], 
sim time this is 2083800.0000, 
sim time next is 2084400.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 19.0, 26.09303239789725, 0.5547841433703251, 0.0, 1.0, 65.0, 55875.92226734436], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6744193664914375, 0.6849280477901084, 0.0, 1.0, 1.0, 0.26607582032068744], 
reward next is 0.7339, 
noisyNet noise sample is [array([0.58933324], dtype=float32), -0.2497927]. 
=============================================
[2019-04-08 15:47:09,824] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.0766121e-17 2.9678035e-10 6.5225095e-07 1.6278826e-06 3.3101549e-10
 1.0816963e-12 1.2234412e-07 1.6379626e-12 1.3416247e-16 1.8955527e-19
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:47:09,839] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3135
[2019-04-08 15:47:09,856] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 91.0, 0.0, 0.0, 19.0, 26.04056797214365, 0.5848868681198046, 0.0, 1.0, 65.0, 55748.71941927969], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2074800.0000, 
sim time next is 2075400.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 19.0, 26.15303255675753, 0.5773730880818652, 0.0, 1.0, 65.0, 51678.04760596005], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.08333333333333333, 0.6794193797297942, 0.6924576960272883, 0.0, 1.0, 1.0, 0.24608594098076214], 
reward next is 0.7539, 
noisyNet noise sample is [array([1.0245105], dtype=float32), -1.500266]. 
=============================================
[2019-04-08 15:47:09,893] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.7614054e-17 9.7823993e-10 4.4947246e-08 1.0913107e-07 4.7285287e-10
 4.0037418e-12 3.9306933e-08 2.9257294e-14 2.6912241e-18 6.7060615e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:47:09,896] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1512
[2019-04-08 15:47:09,915] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.9, 79.0, 126.0, 0.0, 22.5, 25.93685899338511, 0.6341610239039644, 1.0, 1.0, 65.0, 43226.76010209893], 
current ob forecast is [], 
actual action is [1.1, 65.0], 
sim time this is 2037600.0000, 
sim time next is 2038200.0000, 
raw observation next is [-4.0, 80.16666666666667, 118.6666666666667, 0.0, 22.5, 26.99284763406836, 0.7134096615481432, 1.0, 1.0, 65.0, 35362.56386177072], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.8016666666666667, 0.39555555555555566, 0.0, 0.375, 0.7494039695056968, 0.7378032205160477, 1.0, 1.0, 1.0, 0.16839316124652726], 
reward next is 0.8316, 
noisyNet noise sample is [array([-0.1975391], dtype=float32), 0.17584586]. 
=============================================
[2019-04-08 15:47:09,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8132907e-15 3.6306020e-09 6.6588345e-06 1.6317259e-06 3.8360606e-10
 9.0287888e-10 4.0193563e-06 9.8040272e-13 4.5495533e-15 1.6227230e-18
 9.9998772e-01], sum to 1.0000
[2019-04-08 15:47:09,982] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0488
[2019-04-08 15:47:10,007] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 19.0, 26.17153251473543, 0.5170791401651282, 0.0, 1.0, 65.0, 51552.53236136155], 
current ob forecast is [], 
actual action is [-1.1999999999999993, 65.0], 
sim time this is 2007600.0000, 
sim time next is 2008200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 26.1284838935541, 0.5183464639526393, 0.0, 1.0, 65.0, 52866.00075409517], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6773736577961751, 0.6727821546508798, 0.0, 1.0, 1.0, 0.25174286073378654], 
reward next is 0.7483, 
noisyNet noise sample is [array([-2.0960948], dtype=float32), 1.1432046]. 
=============================================
[2019-04-08 15:47:10,153] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2427063e-16 1.0388540e-10 4.6800849e-07 2.6139799e-07 1.3810418e-09
 3.8189281e-13 6.2369601e-08 3.2113320e-12 5.0353511e-18 2.8404659e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:47:10,154] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3256
[2019-04-08 15:47:10,186] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 86.0, 0.0, 0.0, 19.0, 26.10171290939795, 0.5688365590769299, 0.0, 1.0, 65.0, 55388.21161591078], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2080800.0000, 
sim time next is 2081400.0000, 
raw observation next is [-4.583333333333333, 86.0, 0.0, 0.0, 19.0, 26.13941310269428, 0.5724048069299602, 0.0, 1.0, 65.0, 53879.53638483665], 
processed observation next is [1.0, 0.08695652173913043, 0.3356417359187443, 0.86, 0.0, 0.0, 0.08333333333333333, 0.6782844252245234, 0.6908016023099868, 0.0, 1.0, 1.0, 0.25656922088017453], 
reward next is 0.7434, 
noisyNet noise sample is [array([-1.6368988], dtype=float32), -1.9599026]. 
=============================================
[2019-04-08 15:47:11,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1011568e-16 2.4684835e-10 3.8263468e-07 2.5758168e-06 1.1520292e-10
 1.6464006e-11 1.2080176e-08 1.2724592e-13 3.7778778e-17 5.5865318e-19
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:47:11,969] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8277
[2019-04-08 15:47:11,984] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 22.5, 26.96381848707141, 0.7152870627214069, 1.0, 1.0, 65.0, 38416.10829786741], 
current ob forecast is [], 
actual action is [-2.383333333333333, 65.0], 
sim time this is 2112600.0000, 
sim time next is 2113200.0000, 
raw observation next is [-7.3, 75.0, 250.5, 80.5, 22.5, 26.9899943766051, 0.7206355313356948, 1.0, 1.0, 65.0, 37814.33125018883], 
processed observation next is [1.0, 0.4782608695652174, 0.26038781163434904, 0.75, 0.835, 0.08895027624309393, 0.375, 0.7491661980504251, 0.7402118437785649, 1.0, 1.0, 1.0, 0.18006824404851823], 
reward next is 0.8199, 
noisyNet noise sample is [array([0.04222471], dtype=float32), 0.21352825]. 
=============================================
[2019-04-08 15:47:13,381] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6802314e-15 5.9011702e-09 6.9743005e-06 8.1998189e-07 1.6580225e-08
 2.3145858e-11 5.9756189e-07 1.9554629e-11 1.1633157e-16 3.3693225e-18
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:47:13,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5996
[2019-04-08 15:47:13,397] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 19.0, 26.01670922776602, 0.5353268457220036, 0.0, 1.0, 65.0, 55623.17553441087], 
current ob forecast is [], 
actual action is [-2.1999999999999993, 65.0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 19.0, 25.96940837720943, 0.536907363809763, 0.0, 1.0, 65.0, 56555.62242284061], 
processed observation next is [1.0, 0.043478260869565216, 0.26592797783933514, 0.7866666666666667, 0.0, 0.0, 0.08333333333333333, 0.6641173647674524, 0.678969121269921, 0.0, 1.0, 1.0, 0.26931248772781247], 
reward next is 0.7307, 
noisyNet noise sample is [array([0.2945439], dtype=float32), 0.1007622]. 
=============================================
[2019-04-08 15:47:13,715] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6951225e-15 1.2047555e-09 6.8577936e-07 6.9687617e-06 9.2052359e-11
 8.7677747e-12 2.8536611e-08 1.2746141e-11 1.7826782e-16 5.3944046e-18
 9.9999237e-01], sum to 1.0000
[2019-04-08 15:47:13,717] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2214
[2019-04-08 15:47:13,737] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.2, 77.0, 0.0, 0.0, 19.0, 25.89853967233893, 0.4993414140313036, 0.0, 1.0, 65.0, 55780.15075580127], 
current ob forecast is [], 
actual action is [-1.2000000000000002, 65.0], 
sim time this is 2179800.0000, 
sim time next is 2180400.0000, 
raw observation next is [-6.199999999999999, 77.66666666666667, 0.0, 0.0, 19.0, 25.93447720663739, 0.4910470784427645, 0.0, 1.0, 65.0, 54335.31612937626], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.6612064338864491, 0.6636823594809215, 0.0, 1.0, 1.0, 0.2587396006160774], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.10656343], dtype=float32), -1.1385334]. 
=============================================
[2019-04-08 15:47:13,949] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.9667595e-17 1.1488049e-09 1.4132281e-07 2.2562247e-07 1.2179820e-10
 3.4923514e-12 2.9786993e-09 8.3495146e-15 5.4815919e-17 4.1255959e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:47:13,952] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6005
[2019-04-08 15:47:13,974] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.8, 82.0, 185.3333333333333, 98.66666666666667, 22.5, 26.85506799064642, 0.674062981770768, 1.0, 1.0, 65.0, 39097.1949146292], 
current ob forecast is [], 
actual action is [-2.8, 65.0], 
sim time this is 2109000.0000, 
sim time next is 2109600.0000, 
raw observation next is [-7.8, 82.0, 191.0, 89.0, 22.5, 26.88549463908668, 0.6841026055256081, 1.0, 1.0, 65.0, 38981.59870723331], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.82, 0.6366666666666667, 0.09834254143646409, 0.375, 0.7404578865905567, 0.7280342018418694, 1.0, 1.0, 1.0, 0.1856266605106348], 
reward next is 0.8144, 
noisyNet noise sample is [array([0.1942858], dtype=float32), -0.69390666]. 
=============================================
[2019-04-08 15:47:13,988] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4512196e-15 2.4543229e-10 3.1794804e-07 1.2950653e-06 6.3735829e-11
 1.9793826e-12 1.3562710e-08 5.5739406e-14 1.2778407e-17 1.1785719e-18
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:47:13,989] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8229
[2019-04-08 15:47:14,010] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 19.0, 26.28364428161111, 0.6505897562475453, 0.0, 1.0, 65.0, 48932.9259019482], 
current ob forecast is [], 
actual action is [-0.5999999999999996, 65.0], 
sim time this is 2149200.0000, 
sim time next is 2149800.0000, 
raw observation next is [-5.783333333333333, 83.0, 0.0, 0.0, 19.0, 26.27355805651213, 0.6495805538689704, 0.0, 1.0, 65.0, 49096.30519776606], 
processed observation next is [1.0, 0.9130434782608695, 0.3024007386888274, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6894631713760108, 0.7165268512896569, 0.0, 1.0, 1.0, 0.2337919295131717], 
reward next is 0.7662, 
noisyNet noise sample is [array([0.49933177], dtype=float32), 0.7511436]. 
=============================================
[2019-04-08 15:47:14,096] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.4019955e-18 8.3187086e-12 1.0837062e-08 1.8481005e-08 6.4985330e-11
 2.3780846e-14 1.9038209e-09 4.0623148e-14 9.3927235e-18 7.8390418e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:47:14,099] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2285
[2019-04-08 15:47:14,124] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.466666666666667, 77.33333333333334, 222.1666666666667, 66.83333333333333, 22.5, 26.93710647487721, 0.706124168226491, 1.0, 1.0, 65.0, 38627.14971078058], 
current ob forecast is [], 
actual action is [-2.466666666666667, 65.0], 
sim time this is 2112000.0000, 
sim time next is 2112600.0000, 
raw observation next is [-7.383333333333333, 76.16666666666666, 236.3333333333333, 73.66666666666666, 22.5, 26.96381848707141, 0.7152870627214069, 1.0, 1.0, 65.0, 38416.10829786741], 
processed observation next is [1.0, 0.43478260869565216, 0.25807940904893817, 0.7616666666666666, 0.7877777777777776, 0.08139963167587476, 0.375, 0.7469848739226176, 0.7384290209071356, 1.0, 1.0, 1.0, 0.18293384903746385], 
reward next is 0.8171, 
noisyNet noise sample is [array([0.96030486], dtype=float32), 0.4923455]. 
=============================================
[2019-04-08 15:47:14,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.9039719e-16 4.6654575e-10 6.3772958e-08 6.1388869e-07 2.0471091e-10
 1.0167307e-13 5.3504654e-09 4.3183930e-13 1.0914082e-18 8.8610689e-21
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:47:14,828] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8944
[2019-04-08 15:47:14,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.36723191e-17 2.15627766e-10 4.85136162e-08 1.15793355e-07
 2.43586193e-11 1.72348941e-12 5.66566385e-08 2.78982888e-14
 2.21420616e-17 1.54461326e-20 9.99999762e-01], sum to 1.0000
[2019-04-08 15:47:14,832] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8639
[2019-04-08 15:47:14,843] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.600000000000001, 75.0, 28.0, 174.6666666666667, 22.5, 26.16865136883251, 0.5233457132012008, 1.0, 1.0, 65.0, 51473.46252515441], 
current ob forecast is [], 
actual action is [-0.6000000000000014, 65.0], 
sim time this is 2189400.0000, 
sim time next is 2190000.0000, 
raw observation next is [-5.6, 75.0, 34.50000000000001, 218.3333333333333, 22.5, 26.15254576498618, 0.5501189227332738, 1.0, 1.0, 65.0, 48356.09903542878], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.11500000000000002, 0.24125230202578263, 0.375, 0.6793788137488482, 0.6833729742444246, 1.0, 1.0, 1.0, 0.23026713826394657], 
reward next is 0.7697, 
noisyNet noise sample is [array([0.7211276], dtype=float32), -0.09188198]. 
=============================================
[2019-04-08 15:47:14,846] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.533333333333333, 68.66666666666667, 0.0, 0.0, 22.5, 26.7688330215524, 0.7151529540635136, 1.0, 1.0, 65.0, 43530.57383525389], 
current ob forecast is [], 
actual action is [0.4666666666666668, 65.0], 
sim time this is 2226000.0000, 
sim time next is 2226600.0000, 
raw observation next is [-4.55, 69.0, 0.0, 0.0, 22.5, 26.74858711981695, 0.712527805146074, 1.0, 1.0, 65.0, 44814.25844115258], 
processed observation next is [1.0, 0.782608695652174, 0.3365650969529086, 0.69, 0.0, 0.0, 0.375, 0.7290489266514125, 0.7375092683820247, 1.0, 1.0, 1.0, 0.21340123067215516], 
reward next is 0.7866, 
noisyNet noise sample is [array([-0.02378629], dtype=float32), 0.42441404]. 
=============================================
[2019-04-08 15:47:14,865] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.71723 ]
 [79.32882 ]
 [79.031456]
 [78.47657 ]
 [77.974174]], R is [[79.69941711]
 [79.65731049]
 [79.61560822]
 [79.57884216]
 [79.52091217]].
[2019-04-08 15:47:15,269] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3004319e-16 5.3808830e-10 8.2847254e-07 1.0163279e-06 8.3422641e-10
 1.8813076e-12 9.0967916e-08 2.0148590e-13 9.5511475e-17 1.0341593e-18
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:47:15,271] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-08 15:47:15,286] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 19.0, 26.091337706174, 0.5218194095649321, 0.0, 1.0, 65.0, 55259.40541336405], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 2172600.0000, 
sim time next is 2173200.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 26.02571658261877, 0.5240379179975784, 0.0, 1.0, 65.0, 56435.26695468451], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6688097152182308, 0.6746793059991928, 0.0, 1.0, 1.0, 0.26873936645087865], 
reward next is 0.7313, 
noisyNet noise sample is [array([-1.3824073], dtype=float32), 0.6817151]. 
=============================================
[2019-04-08 15:47:15,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3020296e-16 5.4139798e-10 8.3451658e-07 1.0167410e-06 8.3479784e-10
 1.8841768e-12 9.1083891e-08 2.0216032e-13 9.6483736e-17 1.0377119e-18
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:47:15,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9231
[2019-04-08 15:47:15,312] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 26.02571658261877, 0.5240379179975784, 0.0, 1.0, 65.0, 56435.26695468451], 
current ob forecast is [], 
actual action is [-1.700000000000001, 65.0], 
sim time this is 2173200.0000, 
sim time next is 2173800.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 26.03002826114779, 0.5135426463563241, 0.0, 1.0, 65.0, 54692.1066515703], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.669169021762316, 0.6711808821187747, 0.0, 1.0, 1.0, 0.2604386031027157], 
reward next is 0.7396, 
noisyNet noise sample is [array([-1.3824073], dtype=float32), 0.6817151]. 
=============================================
[2019-04-08 15:47:15,646] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0675975e-16 4.1983067e-10 1.2047380e-07 6.0362467e-08 2.4928337e-10
 2.4640508e-13 3.4870589e-09 1.2181469e-14 3.4066901e-17 1.3489231e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:47:15,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7145
[2019-04-08 15:47:15,665] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.2, 77.0, 0.0, 0.0, 22.5, 26.70618731613748, 0.700721242056786, 0.0, 1.0, 65.0, 41469.04892801652], 
current ob forecast is [], 
actual action is [-0.20000000000000018, 65.0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 22.5, 26.63616423740979, 0.689627879037698, 0.0, 1.0, 65.0, 44343.87076772487], 
processed observation next is [1.0, 0.8260869565217391, 0.31578947368421056, 0.785, 0.0, 0.0, 0.375, 0.7196803531174826, 0.7298759596792327, 0.0, 1.0, 1.0, 0.2111612893701184], 
reward next is 0.7888, 
noisyNet noise sample is [array([-0.07725794], dtype=float32), -0.069393456]. 
=============================================
[2019-04-08 15:47:15,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4887016e-17 2.7802785e-10 1.2209164e-08 1.3485057e-07 4.5978683e-11
 9.6245169e-13 1.9725857e-09 6.1020228e-15 1.9708205e-18 4.0756800e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:47:15,701] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6749
[2019-04-08 15:47:15,727] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 67.5, 18.0, 0.0, 22.5, 26.56412735499717, 0.7421666514582409, 1.0, 1.0, 65.0, 64562.11957754519], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2134200.0000, 
sim time next is 2134800.0000, 
raw observation next is [-4.5, 68.0, 14.0, 0.0, 22.5, 26.05181243698924, 0.6592131702602887, 1.0, 1.0, 65.0, 43205.20837237663], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.68, 0.04666666666666667, 0.0, 0.375, 0.6709843697491035, 0.7197377234200962, 1.0, 1.0, 1.0, 0.20573908748750777], 
reward next is 0.7943, 
noisyNet noise sample is [array([-0.5031979], dtype=float32), -0.038890466]. 
=============================================
[2019-04-08 15:47:17,441] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.3046052e-15 2.1568929e-10 5.4497673e-07 2.3544085e-06 2.6257382e-10
 6.7512523e-11 5.8863666e-08 1.8753443e-12 1.4981283e-17 1.6008437e-18
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:47:17,446] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8187
[2019-04-08 15:47:17,458] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 19.0, 26.19289477110284, 0.5281224998633469, 0.0, 1.0, 65.0, 52726.78941493192], 
current ob forecast is [], 
actual action is [-1.700000000000001, 65.0], 
sim time this is 2172000.0000, 
sim time next is 2172600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 19.0, 26.09133763404532, 0.5218193882851004, 0.0, 1.0, 65.0, 55259.40629466586], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.08333333333333333, 0.6742781361704434, 0.6739397960950334, 0.0, 1.0, 1.0, 0.26314002997459934], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.663955], dtype=float32), 0.2646162]. 
=============================================
[2019-04-08 15:47:17,690] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3333177e-17 2.5854805e-09 7.8190254e-07 8.7939469e-08 2.3679128e-10
 1.7440239e-12 1.9052552e-08 5.0929967e-14 1.4884810e-18 9.7196242e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:47:17,690] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4914
[2019-04-08 15:47:17,706] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.8, 76.33333333333334, 0.0, 0.0, 19.0, 25.86343038333099, 0.4842952636688698, 0.0, 1.0, 65.0, 55021.62526719823], 
current ob forecast is [], 
actual action is [-0.7999999999999998, 65.0], 
sim time this is 2184000.0000, 
sim time next is 2184600.0000, 
raw observation next is [-5.7, 75.66666666666666, 0.0, 0.0, 19.0, 25.88686102619315, 0.4793144561129561, 0.0, 1.0, 65.0, 54546.53742945652], 
processed observation next is [1.0, 0.2608695652173913, 0.30470914127423826, 0.7566666666666666, 0.0, 0.0, 0.08333333333333333, 0.657238418849429, 0.6597714853709854, 0.0, 1.0, 1.0, 0.2597454163307453], 
reward next is 0.7403, 
noisyNet noise sample is [array([-0.75016826], dtype=float32), 0.09965057]. 
=============================================
[2019-04-08 15:47:17,852] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.47720124e-17 7.95852384e-10 2.22672313e-07 2.80666086e-07
 2.39789660e-10 1.30249015e-11 1.38521699e-07 1.07519202e-12
 1.00491617e-17 2.08200512e-18 9.99999404e-01], sum to 1.0000
[2019-04-08 15:47:17,854] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2912
[2019-04-08 15:47:17,868] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 19.0, 25.94598661219493, 0.5191994509327386, 0.0, 1.0, 65.0, 57521.60929453516], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 2253600.0000, 
sim time next is 2254200.0000, 
raw observation next is [-7.383333333333333, 82.66666666666667, 0.0, 0.0, 19.0, 25.91393388022182, 0.5227174879717112, 0.0, 1.0, 65.0, 57681.10445863436], 
processed observation next is [1.0, 0.08695652173913043, 0.25807940904893817, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.6594944900184849, 0.674239162657237, 0.0, 1.0, 1.0, 0.274671925993497], 
reward next is 0.7253, 
noisyNet noise sample is [array([-0.42093644], dtype=float32), -0.45050701]. 
=============================================
[2019-04-08 15:47:17,937] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3182298e-16 1.0097313e-09 1.7310360e-07 3.7284548e-07 1.3195268e-10
 3.1503613e-12 3.4744370e-09 3.6190340e-14 5.2491543e-19 6.2616890e-21
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:47:17,945] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2315
[2019-04-08 15:47:17,962] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 69.5, 0.0, 0.0, 22.5, 26.61240806086662, 0.6770756256399878, 1.0, 1.0, 65.0, 46564.97828952473], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2223000.0000, 
sim time next is 2223600.0000, 
raw observation next is [-4.5, 69.0, 0.0, 0.0, 22.5, 26.74355827701125, 0.7148946826172972, 1.0, 1.0, 65.00000000000003, 45177.13952682116], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.69, 0.0, 0.0, 0.375, 0.7286298564176041, 0.738298227539099, 1.0, 1.0, 1.0000000000000007, 0.21512923584200555], 
reward next is 0.7849, 
noisyNet noise sample is [array([0.27356905], dtype=float32), -1.4197415]. 
=============================================
[2019-04-08 15:47:18,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6148686e-18 7.5997431e-11 2.2125111e-08 9.6930695e-09 3.1294381e-12
 2.1255595e-13 3.9248096e-10 7.2471063e-15 4.5685568e-19 2.0098429e-20
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:47:18,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0049
[2019-04-08 15:47:18,249] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.8, 76.33333333333334, 0.0, 0.0, 19.0, 25.86343029535394, 0.484295240338081, 0.0, 1.0, 65.0, 55021.62573339842], 
current ob forecast is [], 
actual action is [-0.7999999999999998, 65.0], 
sim time this is 2184000.0000, 
sim time next is 2184600.0000, 
raw observation next is [-5.7, 75.66666666666666, 0.0, 0.0, 19.0, 25.88686093897935, 0.4793144329219931, 0.0, 1.0, 65.0, 54546.53786269756], 
processed observation next is [1.0, 0.2608695652173913, 0.30470914127423826, 0.7566666666666666, 0.0, 0.0, 0.08333333333333333, 0.6572384115816124, 0.6597714776406643, 0.0, 1.0, 1.0, 0.2597454183937979], 
reward next is 0.7403, 
noisyNet noise sample is [array([0.6258423], dtype=float32), -1.0677102]. 
=============================================
[2019-04-08 15:47:18,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0990317e-16 1.0405010e-07 3.3521928e-07 7.4072443e-07 4.0181258e-09
 1.7654967e-12 1.2956384e-07 2.8831278e-13 1.4052937e-16 1.2646399e-19
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:47:18,566] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0894
[2019-04-08 15:47:18,580] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 69.5, 0.0, 0.0, 19.0, 26.30204653684271, 0.6533502111433491, 0.0, 1.0, 65.0, 52049.80938643312], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2233800.0000, 
sim time next is 2234400.0000, 
raw observation next is [-5.0, 69.0, 0.0, 0.0, 19.0, 26.28711536956337, 0.6508939314847965, 0.0, 1.0, 65.0, 52177.05149212384], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6905929474636142, 0.7169646438282654, 0.0, 1.0, 1.0, 0.2484621499624945], 
reward next is 0.7515, 
noisyNet noise sample is [array([1.9737643], dtype=float32), -0.011631542]. 
=============================================
[2019-04-08 15:47:18,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.37445614e-17 1.35692535e-09 3.99596956e-07 8.46194439e-07
 1.12275224e-10 1.14334662e-11 3.37822499e-08 2.90374448e-14
 1.94775330e-18 4.88282454e-20 9.99998689e-01], sum to 1.0000
[2019-04-08 15:47:18,658] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2846
[2019-04-08 15:47:18,672] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.5, 74.33333333333333, 81.66666666666669, 388.0, 22.5, 26.65648282200673, 0.6299903309550677, 1.0, 1.0, 65.0, 41555.66918755616], 
current ob forecast is [], 
actual action is [-0.5, 65.0], 
sim time this is 2193000.0000, 
sim time next is 2193600.0000, 
raw observation next is [-5.4, 73.66666666666667, 91.83333333333333, 419.5, 22.5, 26.68538323410165, 0.6475806069250601, 1.0, 1.0, 65.0, 41371.92275577808], 
processed observation next is [1.0, 0.391304347826087, 0.31301939058171746, 0.7366666666666667, 0.3061111111111111, 0.46353591160220997, 0.375, 0.7237819361751375, 0.7158602023083533, 1.0, 1.0, 1.0, 0.19700915597989563], 
reward next is 0.8030, 
noisyNet noise sample is [array([0.3283102], dtype=float32), -0.34045988]. 
=============================================
[2019-04-08 15:47:18,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1836907e-15 1.9633102e-08 3.9015478e-07 2.8566473e-07 2.1214550e-09
 8.2013615e-12 1.2571437e-07 7.8678015e-13 4.0264526e-16 1.3846840e-17
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:47:18,739] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1689
[2019-04-08 15:47:18,762] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 35.0, 0.0, 0.0, 19.0, 26.72029792517174, 0.6382869093795697, 0.0, 1.0, 65.0, 43971.03437628618], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 2496600.0000, 
sim time next is 2497200.0000, 
raw observation next is [-1.2, 34.33333333333334, 0.0, 0.0, 19.0, 26.76347446125806, 0.6312900742329622, 0.0, 1.0, 65.0, 43022.89772600995], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.34333333333333343, 0.0, 0.0, 0.08333333333333333, 0.7302895384381717, 0.7104300247443208, 0.0, 1.0, 1.0, 0.20487094155242833], 
reward next is 0.7951, 
noisyNet noise sample is [array([0.7776586], dtype=float32), 1.5883592]. 
=============================================
[2019-04-08 15:47:20,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1928765e-15 5.3617294e-10 1.3440838e-07 7.6926489e-07 3.8404011e-10
 2.3442644e-11 1.4959888e-07 9.7337381e-14 6.0523275e-18 1.1628832e-19
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:47:20,304] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8629
[2019-04-08 15:47:20,319] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.300000000000001, 70.0, 138.6666666666667, 0.0, 22.5, 26.90400682634527, 0.6874283054088696, 1.0, 1.0, 65.0, 42510.64583377534], 
current ob forecast is [], 
actual action is [0.6999999999999993, 65.0], 
sim time this is 2200800.0000, 
sim time next is 2201400.0000, 
raw observation next is [-4.2, 69.5, 143.0, 0.0, 22.5, 26.93789356207484, 0.69789105494712, 1.0, 1.0, 65.0, 41778.2940528308], 
processed observation next is [1.0, 0.4782608695652174, 0.34626038781163443, 0.695, 0.4766666666666667, 0.0, 0.375, 0.7448244635062368, 0.73263035164904, 1.0, 1.0, 1.0, 0.1989442573944324], 
reward next is 0.8011, 
noisyNet noise sample is [array([-0.7332115], dtype=float32), -0.681275]. 
=============================================
[2019-04-08 15:47:21,033] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.2864166e-16 4.2190648e-10 2.7296401e-06 1.3367886e-06 3.5852545e-09
 3.5878162e-11 2.2842248e-07 1.5958398e-13 2.6443125e-16 1.0860835e-19
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:47:21,034] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4757
[2019-04-08 15:47:21,051] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666666, 70.16666666666667, 0.0, 0.0, 22.5, 26.5951006811455, 0.691569383616045, 1.0, 1.0, 65.0, 47557.31233993738], 
current ob forecast is [], 
actual action is [0.3333333333333339, 65.0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [-4.733333333333333, 70.33333333333334, 0.0, 0.0, 22.5, 26.55846234846157, 0.6831354928909156, 1.0, 1.0, 65.0, 48724.51123941952], 
processed observation next is [1.0, 0.8260869565217391, 0.3314866112650046, 0.7033333333333335, 0.0, 0.0, 0.375, 0.713205195705131, 0.7277118309636386, 1.0, 1.0, 1.0, 0.2320214820924739], 
reward next is 0.7680, 
noisyNet noise sample is [array([-1.5705236], dtype=float32), 0.7340219]. 
=============================================
[2019-04-08 15:47:21,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9937011e-18 5.6760915e-11 1.1021714e-07 3.9086174e-07 2.1264202e-11
 1.4386756e-12 1.7167830e-08 6.6399476e-15 6.8921797e-18 2.8620710e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:47:21,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5969
[2019-04-08 15:47:21,066] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.8, 72.33333333333334, 0.0, 0.0, 19.0, 26.20427926346586, 0.5930419894139837, 0.0, 1.0, 65.0, 57858.05546806299], 
current ob forecast is [], 
actual action is [-0.7999999999999998, 65.0], 
sim time this is 2240400.0000, 
sim time next is 2241000.0000, 
raw observation next is [-5.9, 73.0, 0.0, 0.0, 19.0, 26.13963919387163, 0.5887725780134515, 0.0, 1.0, 65.0, 58679.2374834676], 
processed observation next is [1.0, 0.9565217391304348, 0.2991689750692521, 0.73, 0.0, 0.0, 0.08333333333333333, 0.6783032661559693, 0.6962575260044838, 0.0, 1.0, 1.0, 0.2794249403974648], 
reward next is 0.7206, 
noisyNet noise sample is [array([-1.2878504], dtype=float32), -0.73230565]. 
=============================================
[2019-04-08 15:47:21,068] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.77299]
 [79.84844]
 [79.93203]
 [79.80132]
 [79.7256 ]], R is [[79.84542084]
 [79.77145386]
 [79.71811676]
 [79.67178345]
 [79.62593842]].
[2019-04-08 15:47:21,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9711889e-16 3.2695375e-09 3.2465175e-07 7.2250037e-07 3.3470691e-11
 2.5628309e-13 7.9004625e-09 5.2767805e-14 1.2841887e-18 1.0334852e-19
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:47:21,212] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1862
[2019-04-08 15:47:21,231] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.383333333333333, 82.66666666666667, 0.0, 0.0, 19.0, 25.91393393899702, 0.5227175025183087, 0.0, 1.0, 65.0, 57681.10416073723], 
current ob forecast is [], 
actual action is [-2.383333333333333, 65.0], 
sim time this is 2254200.0000, 
sim time next is 2254800.0000, 
raw observation next is [-7.466666666666667, 83.33333333333334, 0.0, 0.0, 19.0, 25.94868286336205, 0.5346382967658965, 0.0, 1.0, 65.0, 56915.92452766871], 
processed observation next is [1.0, 0.08695652173913043, 0.25577100646352724, 0.8333333333333335, 0.0, 0.0, 0.08333333333333333, 0.662390238613504, 0.6782127655886322, 0.0, 1.0, 1.0, 0.27102821203651767], 
reward next is 0.7290, 
noisyNet noise sample is [array([1.7685806], dtype=float32), 0.29911694]. 
=============================================
[2019-04-08 15:47:21,513] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7155737e-17 1.0863256e-09 8.4106368e-08 4.3704940e-08 6.2138357e-11
 3.8248501e-13 1.7814786e-08 3.7612693e-15 9.6535720e-19 3.1199311e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:47:21,517] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8806
[2019-04-08 15:47:21,537] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.95, 56.83333333333334, 228.3333333333333, 86.0, 22.5, 27.0004336946799, 0.7122198173235272, 1.0, 1.0, 65.0, 36785.85129322312], 
current ob forecast is [], 
actual action is [2.05, 65.0], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [-2.7, 55.66666666666667, 245.1666666666667, 80.0, 22.5, 27.02670672665684, 0.7213440651698756, 1.0, 1.0, 65.0, 35944.48526596506], 
processed observation next is [1.0, 0.5217391304347826, 0.38781163434903054, 0.5566666666666668, 0.8172222222222224, 0.08839779005524862, 0.375, 0.7522255605547367, 0.7404480217232918, 1.0, 1.0, 1.0, 0.17116421555221456], 
reward next is 0.8288, 
noisyNet noise sample is [array([2.0905921], dtype=float32), -1.4584495]. 
=============================================
[2019-04-08 15:47:23,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.6669709e-16 9.4549488e-09 4.9808784e-07 5.7718893e-07 4.3593876e-10
 1.8640832e-11 3.3187728e-08 8.2338520e-12 1.2258688e-16 1.4798911e-19
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:47:23,004] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6405
[2019-04-08 15:47:23,024] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 19.0, 26.22755637277633, 0.5724003030931778, 0.0, 1.0, 65.0, 50750.03368392208], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2348400.0000, 
sim time next is 2349000.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 19.0, 26.20566465276823, 0.5702424718569854, 0.0, 1.0, 65.0, 51999.37237893695], 
processed observation next is [0.0, 0.17391304347826086, 0.37673130193905824, 0.67, 0.0, 0.0, 0.08333333333333333, 0.6838053877306859, 0.6900808239523285, 0.0, 1.0, 1.0, 0.2476160589473188], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.11581635], dtype=float32), 0.72066486]. 
=============================================
[2019-04-08 15:47:23,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.728935]
 [72.69478 ]
 [72.68839 ]
 [72.672325]
 [72.685005]], R is [[72.74971008]
 [72.7805481 ]
 [72.81304169]
 [72.83982849]
 [72.8677063 ]].
[2019-04-08 15:47:23,501] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0869437e-16 9.9581161e-09 9.7324524e-08 5.8892914e-07 1.5849580e-10
 7.0235359e-13 2.7509037e-08 5.1472780e-13 3.8382599e-16 4.0964437e-19
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:47:23,506] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6993
[2019-04-08 15:47:23,520] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.3070830879919, 0.6061854658829582, 0.0, 1.0, 65.0, 48664.93821848596], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 2338800.0000, 
sim time next is 2339400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.28924882719387, 0.607444702682749, 0.0, 1.0, 65.0, 48714.69849582326], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6907707355994891, 0.7024815675609163, 0.0, 1.0, 1.0, 0.2319747547420155], 
reward next is 0.7680, 
noisyNet noise sample is [array([0.77999085], dtype=float32), -0.34047243]. 
=============================================
[2019-04-08 15:47:23,819] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6925897e-17 9.4538288e-10 5.5797733e-07 1.0291015e-07 4.1592582e-10
 2.1502862e-12 8.5286667e-09 7.2450836e-15 2.2205986e-18 1.2574676e-20
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:47:23,822] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5250
[2019-04-08 15:47:23,842] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.2500000000000001, 44.0, 121.0, 60.0, 22.5, 27.36465468003161, 0.7817201981325916, 1.0, 1.0, 65.0, 30235.8796301766], 
current ob forecast is [], 
actual action is [5.25, 65.0], 
sim time this is 2298600.0000, 
sim time next is 2299200.0000, 
raw observation next is [0.5333333333333334, 43.66666666666666, 123.8333333333333, 57.0, 22.5, 27.36499149631653, 0.7947146322872197, 1.0, 1.0, 65.0, 31250.51993869485], 
processed observation next is [1.0, 0.6086956521739131, 0.4773776546629733, 0.4366666666666666, 0.4127777777777777, 0.06298342541436464, 0.375, 0.7804159580263775, 0.7649048774290733, 1.0, 1.0, 1.0, 0.1488119997080707], 
reward next is 0.8512, 
noisyNet noise sample is [array([-0.49283504], dtype=float32), 1.1821513]. 
=============================================
[2019-04-08 15:47:25,317] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3321885e-14 4.2067794e-09 1.0943507e-08 6.4447482e-07 1.9664828e-10
 2.2297943e-12 8.4273912e-09 1.4089475e-12 2.2973204e-17 2.3420748e-19
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:47:25,318] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8179
[2019-04-08 15:47:25,335] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.15, 44.5, 0.0, 0.0, 19.0, 26.69516787582719, 0.6766834705254406, 0.0, 1.0, 65.0, 45683.52269823941], 
current ob forecast is [], 
actual action is [3.85, 65.0], 
sim time this is 2395800.0000, 
sim time next is 2396400.0000, 
raw observation next is [-1.333333333333333, 44.33333333333334, 0.0, 0.0, 19.0, 26.6740855051252, 0.672096407218858, 0.0, 1.0, 65.0, 46227.3301779289], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.4433333333333334, 0.0, 0.0, 0.08333333333333333, 0.7228404587604332, 0.7240321357396193, 0.0, 1.0, 1.0, 0.22013014370442335], 
reward next is 0.7799, 
noisyNet noise sample is [array([-0.07315046], dtype=float32), 0.2753381]. 
=============================================
[2019-04-08 15:47:26,055] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.4814929e-16 1.3670033e-09 1.6566459e-07 1.2743099e-07 1.4046990e-10
 1.2541384e-13 4.3595531e-08 7.4535044e-14 2.5525632e-17 2.5881328e-19
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:47:26,056] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7149
[2019-04-08 15:47:26,077] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.34786943985555, 0.5985713047491897, 0.0, 1.0, 65.0, 47692.38947304754], 
current ob forecast is [], 
actual action is [2.7, 65.0], 
sim time this is 2341800.0000, 
sim time next is 2342400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 19.0, 26.31010159774597, 0.5958959297642644, 0.0, 1.0, 65.0, 49563.20381483999], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.08333333333333333, 0.6925084664788308, 0.6986319765880881, 0.0, 1.0, 1.0, 0.2360152562611428], 
reward next is 0.7640, 
noisyNet noise sample is [array([-0.8429882], dtype=float32), 0.09995751]. 
=============================================
[2019-04-08 15:47:26,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6314257e-16 1.3729397e-09 6.7520807e-07 1.2024564e-06 1.0727145e-10
 3.6984075e-12 6.5455583e-07 7.8197302e-14 1.0904918e-16 3.0617051e-19
 9.9999750e-01], sum to 1.0000
[2019-04-08 15:47:26,848] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9891
[2019-04-08 15:47:26,871] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 19.0, 26.48701065588137, 0.6009609298455546, 0.0, 1.0, 65.0, 49476.7353387618], 
current ob forecast is [], 
actual action is [1.416666666666667, 65.0], 
sim time this is 2409000.0000, 
sim time next is 2409600.0000, 
raw observation next is [-3.766666666666667, 42.66666666666667, 0.0, 0.0, 19.0, 26.46348772979419, 0.5940638184845103, 0.0, 1.0, 65.0, 50477.85445297323], 
processed observation next is [0.0, 0.9130434782608695, 0.358264081255771, 0.4266666666666667, 0.0, 0.0, 0.08333333333333333, 0.7052906441495157, 0.6980212728281701, 0.0, 1.0, 1.0, 0.24037073549034874], 
reward next is 0.7596, 
noisyNet noise sample is [array([0.56639105], dtype=float32), -0.37659007]. 
=============================================
[2019-04-08 15:47:26,902] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9685722e-13 1.3794277e-08 1.0461793e-05 1.2913976e-06 1.4253943e-08
 3.3789343e-11 6.9171706e-06 2.2270520e-11 1.8002230e-14 1.1910583e-17
 9.9998128e-01], sum to 1.0000
[2019-04-08 15:47:26,904] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7723
[2019-04-08 15:47:26,925] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.9385907e-15 4.1053436e-09 8.4436337e-08 2.5004852e-07 1.7168242e-10
 2.5399490e-11 7.3588822e-08 5.4545252e-14 1.4958831e-16 6.7209469e-19
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:47:26,928] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.7127
[2019-04-08 15:47:26,936] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.75, 50.5, 0.0, 0.0, 19.0, 26.13249670032138, 0.5081815542998332, 0.0, 1.0, 65.0, 55566.03976191446], 
current ob forecast is [], 
actual action is [-1.75, 65.0], 
sim time this is 2424600.0000, 
sim time next is 2425200.0000, 
raw observation next is [-6.933333333333334, 51.33333333333333, 0.0, 0.0, 19.0, 26.10970698432221, 0.5034817407132065, 0.0, 1.0, 65.0, 56054.57342169838], 
processed observation next is [0.0, 0.043478260869565216, 0.270544783010157, 0.5133333333333333, 0.0, 0.0, 0.08333333333333333, 0.6758089153601841, 0.6678272469044022, 0.0, 1.0, 1.0, 0.2669265401033256], 
reward next is 0.7331, 
noisyNet noise sample is [array([1.86507], dtype=float32), 1.890546]. 
=============================================
[2019-04-08 15:47:26,960] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.12902246661279, 0.543320482199773, 0.0, 1.0, 65.0, 53683.52058388331], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 2359200.0000, 
sim time next is 2359800.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 19.0, 26.1123225064323, 0.540882561412753, 0.0, 1.0, 65.0, 53940.69629809502], 
processed observation next is [0.0, 0.30434782608695654, 0.368421052631579, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6760268755360249, 0.6802941871375844, 0.0, 1.0, 1.0, 0.25686045856235723], 
reward next is 0.7431, 
noisyNet noise sample is [array([2.792013], dtype=float32), 0.039911635]. 
=============================================
[2019-04-08 15:47:27,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4542798e-16 4.6696025e-09 1.3634241e-06 4.8990302e-07 1.8888540e-09
 3.3195419e-12 2.6425397e-08 2.5415886e-14 1.0320794e-16 1.8122862e-19
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:47:27,009] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7564
[2019-04-08 15:47:27,024] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.7, 55.66666666666667, 0.0, 0.0, 19.0, 26.57789663601122, 0.7044394798025341, 0.0, 1.0, 65.0, 40778.07058896901], 
current ob forecast is [], 
actual action is [3.3, 65.0], 
sim time this is 2325000.0000, 
sim time next is 2325600.0000, 
raw observation next is [-1.7, 56.0, 0.0, 0.0, 19.0, 26.57332254706527, 0.703637942769879, 0.0, 1.0, 65.0, 40888.08932188286], 
processed observation next is [1.0, 0.9565217391304348, 0.4155124653739613, 0.56, 0.0, 0.0, 0.08333333333333333, 0.7144435455887725, 0.734545980923293, 0.0, 1.0, 1.0, 0.19470518724706123], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.17965592], dtype=float32), 1.2659464]. 
=============================================
[2019-04-08 15:47:27,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.87695503e-16 5.79759485e-09 1.84181135e-06 5.62236835e-07
 2.19581442e-09 3.69753360e-12 2.95022016e-08 2.81288918e-14
 1.08314954e-16 2.38458619e-19 9.99997616e-01], sum to 1.0000
[2019-04-08 15:47:27,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4926
[2019-04-08 15:47:27,113] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.9, 57.0, 0.0, 0.0, 19.0, 26.55358800828484, 0.6636809382193333, 0.0, 1.0, 65.0, 46118.06348851204], 
current ob forecast is [], 
actual action is [3.1, 65.0], 
sim time this is 2326800.0000, 
sim time next is 2327400.0000, 
raw observation next is [-2.0, 57.5, 0.0, 0.0, 19.0, 26.49675712765014, 0.6587233405096439, 0.0, 1.0, 65.0, 46792.0650001013], 
processed observation next is [1.0, 0.9565217391304348, 0.40720221606648205, 0.575, 0.0, 0.0, 0.08333333333333333, 0.7080630939708451, 0.7195744468365479, 0.0, 1.0, 1.0, 0.2228193571433395], 
reward next is 0.7772, 
noisyNet noise sample is [array([-0.17965592], dtype=float32), 1.2659464]. 
=============================================
[2019-04-08 15:47:27,202] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4040683e-15 1.4873135e-09 2.2681181e-07 3.8182766e-06 5.3022213e-09
 2.0359410e-11 1.9236275e-07 6.1852232e-13 1.3914371e-15 6.2071426e-18
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:47:27,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9615
[2019-04-08 15:47:27,219] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.100000000000001, 41.33333333333334, 0.0, 0.0, 19.0, 26.24051642223422, 0.5508531294763517, 0.0, 1.0, 65.0, 54601.2812863248], 
current ob forecast is [], 
actual action is [-0.10000000000000142, 65.0], 
sim time this is 2416200.0000, 
sim time next is 2416800.0000, 
raw observation next is [-5.2, 41.66666666666667, 0.0, 0.0, 19.0, 26.22507661833156, 0.5525406971465706, 0.0, 1.0, 65.0, 54797.24403851435], 
processed observation next is [0.0, 1.0, 0.31855955678670367, 0.41666666666666674, 0.0, 0.0, 0.08333333333333333, 0.68542305152763, 0.6841802323821903, 0.0, 1.0, 1.0, 0.2609392573262588], 
reward next is 0.7391, 
noisyNet noise sample is [array([-1.1421739], dtype=float32), 0.95526797]. 
=============================================
[2019-04-08 15:47:27,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7189127e-15 7.8382034e-10 1.9128814e-07 3.7644659e-07 4.8729558e-09
 7.9307665e-13 6.2966421e-08 1.4382638e-12 2.1524108e-17 1.4659190e-18
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:47:27,333] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1866
[2019-04-08 15:47:27,358] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.733333333333333, 42.66666666666667, 0.0, 0.0, 19.0, 26.52226476779419, 0.6324843340819417, 0.0, 1.0, 65.0, 49438.36507438334], 
current ob forecast is [], 
actual action is [2.266666666666667, 65.0], 
sim time this is 2402400.0000, 
sim time next is 2403000.0000, 
raw observation next is [-2.9, 42.5, 0.0, 0.0, 19.0, 26.50593273611457, 0.6283019248887637, 0.0, 1.0, 65.0, 49753.21965992824], 
processed observation next is [0.0, 0.8260869565217391, 0.38227146814404434, 0.425, 0.0, 0.0, 0.08333333333333333, 0.7088277280095475, 0.7094339749629213, 0.0, 1.0, 1.0, 0.2369200936187059], 
reward next is 0.7631, 
noisyNet noise sample is [array([-0.5776896], dtype=float32), 0.0030639837]. 
=============================================
[2019-04-08 15:47:27,386] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.61283 ]
 [73.6059  ]
 [73.61348 ]
 [73.62381 ]
 [73.651726]], R is [[73.57557678]
 [73.60440063]
 [73.63461304]
 [73.66682434]
 [73.70013428]].
[2019-04-08 15:47:27,447] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.2384888e-16 2.2496607e-10 2.1134646e-07 2.5162208e-06 4.1112555e-10
 2.8631652e-13 8.3525311e-09 9.4847111e-14 9.5922122e-18 4.0106646e-19
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:47:27,447] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7246
[2019-04-08 15:47:27,464] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.583333333333333, 42.33333333333334, 0.0, 0.0, 19.0, 26.48701066700937, 0.6009609332813081, 0.0, 1.0, 65.0, 49476.73520269529], 
current ob forecast is [], 
actual action is [1.416666666666667, 65.0], 
sim time this is 2409000.0000, 
sim time next is 2409600.0000, 
raw observation next is [-3.766666666666667, 42.66666666666667, 0.0, 0.0, 19.0, 26.46348774093, 0.5940638219036048, 0.0, 1.0, 65.0, 50477.85431646187], 
processed observation next is [0.0, 0.9130434782608695, 0.358264081255771, 0.4266666666666667, 0.0, 0.0, 0.08333333333333333, 0.7052906450775, 0.6980212739678683, 0.0, 1.0, 1.0, 0.24037073484029461], 
reward next is 0.7596, 
noisyNet noise sample is [array([0.90730464], dtype=float32), -0.7363112]. 
=============================================
[2019-04-08 15:47:27,962] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.4674458e-17 4.0474681e-12 3.1318217e-07 2.9603612e-08 7.8510989e-11
 9.3980257e-14 5.0188365e-09 1.8883046e-14 2.2781568e-17 2.2834868e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:47:27,966] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.0306
[2019-04-08 15:47:27,984] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.6246928e-14 1.5613365e-08 3.3164324e-06 1.2385029e-06 6.6380368e-10
 4.4196580e-11 8.4118817e-08 1.0197712e-11 9.3136238e-16 6.1134723e-18
 9.9999535e-01], sum to 1.0000
[2019-04-08 15:47:27,987] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0169
[2019-04-08 15:47:27,990] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 51.66666666666667, 241.8333333333333, 353.3333333333334, 19.0, 26.66212249944512, 0.7176704032079733, 0.0, 1.0, 65.0, 41499.89746750324], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 2378400.0000, 
sim time next is 2379000.0000, 
raw observation next is [-0.7, 52.83333333333333, 231.6666666666667, 282.6666666666667, 19.0, 26.68762830345401, 0.7166158525862029, 0.0, 1.0, 65.0, 42154.16244935748], 
processed observation next is [0.0, 0.5217391304347826, 0.443213296398892, 0.5283333333333333, 0.7722222222222224, 0.3123388581952118, 0.08333333333333333, 0.723969025287834, 0.7388719508620677, 0.0, 1.0, 1.0, 0.20073410690170226], 
reward next is 0.7993, 
noisyNet noise sample is [array([-0.9315035], dtype=float32), -0.8679943]. 
=============================================
[2019-04-08 15:47:28,000] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 46.33333333333334, 0.0, 0.0, 19.0, 26.22686934904764, 0.5272181706247765, 0.0, 1.0, 65.0, 54170.04862827577], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2421600.0000, 
sim time next is 2422200.0000, 
raw observation next is [-6.1, 47.16666666666666, 0.0, 0.0, 19.0, 26.19631769350047, 0.522415072732172, 0.0, 1.0, 65.0, 55607.4636876269], 
processed observation next is [0.0, 0.0, 0.29362880886426596, 0.47166666666666657, 0.0, 0.0, 0.08333333333333333, 0.6830264744583724, 0.6741383575773906, 0.0, 1.0, 1.0, 0.26479744613155665], 
reward next is 0.7352, 
noisyNet noise sample is [array([0.42556134], dtype=float32), -1.4898646]. 
=============================================
[2019-04-08 15:47:28,014] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[78.439415]
 [78.45176 ]
 [78.35645 ]
 [78.259544]
 [78.23343 ]], R is [[78.30912781]
 [78.32842255]
 [78.34117889]
 [78.34745026]
 [78.35127258]].
[2019-04-08 15:47:28,593] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2989176e-15 1.3146297e-08 4.1179734e-07 8.5572694e-08 6.8535233e-10
 1.8895061e-12 7.1913857e-09 8.5712429e-14 3.2183721e-16 1.8946249e-18
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:47:28,595] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1847
[2019-04-08 15:47:28,617] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.066666666666666, 42.33333333333334, 0.0, 0.0, 19.0, 26.49021037100852, 0.6241745446545126, 0.0, 1.0, 65.0, 50091.02820102275], 
current ob forecast is [], 
actual action is [1.933333333333334, 65.0], 
sim time this is 2403600.0000, 
sim time next is 2404200.0000, 
raw observation next is [-3.233333333333333, 42.16666666666666, 0.0, 0.0, 19.0, 26.47454327391522, 0.620032494998564, 0.0, 1.0, 65.0, 50435.98671829316], 
processed observation next is [0.0, 0.8260869565217391, 0.3730378578024008, 0.4216666666666666, 0.0, 0.0, 0.08333333333333333, 0.706211939492935, 0.7066774983328546, 0.0, 1.0, 1.0, 0.24017136532520553], 
reward next is 0.7598, 
noisyNet noise sample is [array([1.063779], dtype=float32), 0.32721102]. 
=============================================
[2019-04-08 15:47:29,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2189822e-15 2.9062435e-09 1.2928167e-06 5.0735309e-07 2.1946652e-08
 2.5631379e-12 3.0290568e-08 4.6888734e-12 1.3775713e-15 3.8771304e-18
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:47:29,056] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6413
[2019-04-08 15:47:29,071] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.166666666666667, 43.33333333333333, 0.0, 0.0, 19.0, 26.5805087211858, 0.6494189977569614, 0.0, 1.0, 65.0, 48206.01402563672], 
current ob forecast is [], 
actual action is [2.833333333333333, 65.0], 
sim time this is 2400000.0000, 
sim time next is 2400600.0000, 
raw observation next is [-2.283333333333333, 43.16666666666667, 0.0, 0.0, 19.0, 26.57185252161502, 0.6449915874448312, 0.0, 1.0, 65.0, 48304.79050074499], 
processed observation next is [0.0, 0.782608695652174, 0.399353647276085, 0.4316666666666667, 0.0, 0.0, 0.08333333333333333, 0.7143210434679185, 0.7149971958149437, 0.0, 1.0, 1.0, 0.23002281190830948], 
reward next is 0.7700, 
noisyNet noise sample is [array([-0.44708145], dtype=float32), 2.111906]. 
=============================================
[2019-04-08 15:47:29,134] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1658886e-16 1.2098089e-10 2.2592779e-07 1.0814660e-07 2.7274905e-10
 2.3380611e-13 1.2022810e-08 4.7635961e-14 6.7153414e-17 6.7448553e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:47:29,134] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4698
[2019-04-08 15:47:29,149] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.3, 60.66666666666667, 0.0, 0.0, 19.0, 25.66824522651, 0.3980997439631417, 0.0, 1.0, 65.0, 58478.70869841352], 
current ob forecast is [], 
actual action is [-4.300000000000001, 65.0], 
sim time this is 2443200.0000, 
sim time next is 2443800.0000, 
raw observation next is [-9.4, 60.83333333333334, 0.0, 0.0, 19.0, 25.64544154285834, 0.3932452067381489, 0.0, 1.0, 65.0, 58483.99141250479], 
processed observation next is [0.0, 0.2608695652173913, 0.20221606648199447, 0.6083333333333334, 0.0, 0.0, 0.08333333333333333, 0.6371201285715283, 0.631081735579383, 0.0, 1.0, 1.0, 0.27849519720240373], 
reward next is 0.7215, 
noisyNet noise sample is [array([-0.53217393], dtype=float32), -0.48958343]. 
=============================================
[2019-04-08 15:47:29,619] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5264218e-16 1.1017676e-10 1.2637605e-07 8.6705177e-07 3.2916425e-10
 4.9682229e-12 5.8165899e-08 9.7513170e-14 6.3105933e-17 2.2351635e-19
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:47:29,620] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7999
[2019-04-08 15:47:29,640] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 19.0, 26.44334088536645, 0.6176174110733149, 0.0, 1.0, 65.0, 51105.4651018787], 
current ob forecast is [], 
actual action is [1.6, 65.0], 
sim time this is 2405400.0000, 
sim time next is 2406000.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 19.0, 26.4660091784592, 0.6122591986944871, 0.0, 1.0, 65.0, 50360.41972567478], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7055007648716, 0.704086399564829, 0.0, 1.0, 1.0, 0.23981152250321325], 
reward next is 0.7602, 
noisyNet noise sample is [array([0.11633288], dtype=float32), 0.67597616]. 
=============================================
[2019-04-08 15:47:29,650] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[73.46165]
 [73.5281 ]
 [73.58255]
 [73.62676]
 [73.67057]], R is [[73.4471283 ]
 [73.46929169]
 [73.49279785]
 [73.5177002 ]
 [73.54399872]].
[2019-04-08 15:47:29,664] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.80426536e-16 1.26081723e-09 6.67903521e-07 3.94552700e-07
 3.99970634e-09 3.15252967e-12 1.99868637e-08 1.03000914e-13
 2.45295024e-16 8.11511413e-18 9.99998927e-01], sum to 1.0000
[2019-04-08 15:47:29,667] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0451
[2019-04-08 15:47:29,691] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.833333333333333, 27.0, 0.0, 0.0, 19.0, 27.01553942290315, 0.7121761998606804, 0.0, 1.0, 65.0, 38462.84878708121], 
current ob forecast is [], 
actual action is [6.833333333333333, 65.0], 
sim time this is 2482800.0000, 
sim time next is 2483400.0000, 
raw observation next is [1.466666666666667, 27.5, 0.0, 0.0, 19.0, 26.9972917416677, 0.7082264635726824, 0.0, 1.0, 65.0, 39105.73117515909], 
processed observation next is [0.0, 0.7391304347826086, 0.5032317636195753, 0.275, 0.0, 0.0, 0.08333333333333333, 0.7497743118056416, 0.7360754878575607, 0.0, 1.0, 1.0, 0.18621776750075758], 
reward next is 0.8138, 
noisyNet noise sample is [array([-1.4294688], dtype=float32), 1.421766]. 
=============================================
[2019-04-08 15:47:30,168] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1678228e-17 8.9876745e-11 4.9200377e-09 3.8352848e-08 4.0002404e-11
 1.6106070e-13 1.7535617e-08 2.7477454e-14 6.0427165e-17 1.6481259e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:47:30,168] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4667
[2019-04-08 15:47:30,181] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.033333333333333, 34.33333333333333, 0.0, 0.0, 19.0, 26.74762523282524, 0.6443347073143852, 0.0, 1.0, 65.0, 43365.22939290364], 
current ob forecast is [], 
actual action is [3.966666666666667, 65.0], 
sim time this is 2493600.0000, 
sim time next is 2494200.0000, 
raw observation next is [-1.116666666666667, 35.66666666666667, 0.0, 0.0, 19.0, 26.73464762723919, 0.6407683928270992, 0.0, 1.0, 65.0, 43652.98154863829], 
processed observation next is [0.0, 0.8695652173913043, 0.43167128347183753, 0.3566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7278873022699326, 0.7135894642756998, 0.0, 1.0, 1.0, 0.20787134070780136], 
reward next is 0.7921, 
noisyNet noise sample is [array([-0.4151735], dtype=float32), -0.18838853]. 
=============================================
[2019-04-08 15:47:30,441] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7322776e-17 1.2669062e-10 4.5340184e-07 1.7108276e-07 3.8309939e-10
 3.1131638e-13 3.6051691e-08 2.2818832e-13 8.9971219e-18 5.1475082e-19
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:47:30,443] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8552
[2019-04-08 15:47:30,459] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 33.66666666666667, 0.0, 0.0, 19.0, 26.66074381275408, 0.6154432382274398, 0.0, 1.0, 65.0, 45939.57282241652], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 2499600.0000, 
sim time next is 2500200.0000, 
raw observation next is [-0.8999999999999999, 34.0, 0.0, 0.0, 19.0, 26.6496231704172, 0.6131040714493778, 0.0, 1.0, 65.0, 46053.01470419409], 
processed observation next is [0.0, 0.9565217391304348, 0.43767313019390586, 0.34, 0.0, 0.0, 0.08333333333333333, 0.7208019308681001, 0.7043680238164592, 0.0, 1.0, 1.0, 0.21930007001997184], 
reward next is 0.7807, 
noisyNet noise sample is [array([0.42308363], dtype=float32), -2.189267]. 
=============================================
[2019-04-08 15:47:30,466] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.23989027e-16 3.11186721e-09 2.63509173e-06 4.64588265e-06
 1.77103097e-08 4.39451904e-12 1.30460425e-08 1.52385548e-12
 2.55750799e-16 1.75904734e-18 9.99992728e-01], sum to 1.0000
[2019-04-08 15:47:30,468] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1405
[2019-04-08 15:47:30,479] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.55, 54.0, 0.0, 0.0, 19.0, 26.01389697470712, 0.48445288099527, 0.0, 1.0, 65.0, 57236.7106593522], 
current ob forecast is [], 
actual action is [-2.55, 65.0], 
sim time this is 2428200.0000, 
sim time next is 2428800.0000, 
raw observation next is [-7.633333333333333, 54.33333333333333, 0.0, 0.0, 19.0, 25.99612955478303, 0.4793223852203929, 0.0, 1.0, 65.0, 57311.60694844017], 
processed observation next is [0.0, 0.08695652173913043, 0.2511542012927055, 0.5433333333333333, 0.0, 0.0, 0.08333333333333333, 0.6663441295652524, 0.6597741284067976, 0.0, 1.0, 1.0, 0.2729124140401913], 
reward next is 0.7271, 
noisyNet noise sample is [array([-0.82137245], dtype=float32), 0.9992043]. 
=============================================
[2019-04-08 15:47:31,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7807291e-16 7.3966735e-09 6.7636739e-07 3.6138918e-08 2.3249187e-10
 3.7391254e-13 3.9754462e-09 2.1833433e-13 2.0529122e-17 5.8984512e-19
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:47:31,079] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3998
[2019-04-08 15:47:31,164] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.3, 25.66666666666667, 50.33333333333333, 345.8333333333333, 19.0, 27.10888885881016, 0.7641566506421892, 0.0, 1.0, 65.0, 34121.29263626422], 
current ob forecast is [], 
actual action is [8.3, 65.0], 
sim time this is 2478000.0000, 
sim time next is 2478600.0000, 
raw observation next is [3.3, 25.5, 48.0, 279.0, 19.0, 27.10915689541037, 0.7695472641315281, 0.0, 1.0, 65.0, 32596.54611018001], 
processed observation next is [0.0, 0.6956521739130435, 0.554016620498615, 0.255, 0.16, 0.3082872928176796, 0.08333333333333333, 0.7590964079508641, 0.7565157547105094, 0.0, 1.0, 1.0, 0.15522164814371434], 
reward next is 0.8448, 
noisyNet noise sample is [array([-1.1374751], dtype=float32), 0.45306733]. 
=============================================
[2019-04-08 15:47:31,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9276121e-16 1.4790743e-09 2.4507887e-07 1.0449047e-06 1.1108476e-09
 4.6307680e-12 2.0454006e-08 4.8393178e-14 1.2467457e-17 1.4366544e-18
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:47:31,682] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1023
[2019-04-08 15:47:31,711] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1333333333333333, 30.0, 89.33333333333333, 842.3333333333334, 19.0, 26.55985521280485, 0.6460151659731134, 0.0, 1.0, 65.0, 41892.25188989178], 
current ob forecast is [], 
actual action is [5.133333333333333, 65.0], 
sim time this is 2463600.0000, 
sim time next is 2464200.0000, 
raw observation next is [0.5, 29.5, 90.0, 845.0, 19.0, 26.57448023441949, 0.6532709853343844, 0.0, 1.0, 65.0, 41176.80766135637], 
processed observation next is [0.0, 0.5217391304347826, 0.4764542936288089, 0.295, 0.3, 0.9337016574585635, 0.08333333333333333, 0.7145400195349575, 0.7177569951114614, 0.0, 1.0, 1.0, 0.19608003648264938], 
reward next is 0.8039, 
noisyNet noise sample is [array([1.230647], dtype=float32), -0.08167531]. 
=============================================
[2019-04-08 15:47:31,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4638094e-18 7.7027558e-11 1.5407923e-07 1.8910102e-08 7.1431652e-11
 6.0935843e-13 9.3925134e-09 3.7230564e-14 5.3963971e-19 1.9221912e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:47:31,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7354
[2019-04-08 15:47:31,946] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.3, 29.0, 121.5, 338.3333333333333, 22.5, 28.017528980653, 0.9537370760688931, 1.0, 1.0, 65.0, 21343.35364219879], 
current ob forecast is [], 
actual action is [8.3, 65.0], 
sim time this is 2560800.0000, 
sim time next is 2561400.0000, 
raw observation next is [3.3, 29.0, 114.0, 351.0, 22.5, 28.00569744731366, 0.9672918441076147, 1.0, 1.0, 65.0, 22087.05577197978], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.38, 0.3878453038674033, 0.375, 0.8338081206094717, 0.8224306147025383, 1.0, 1.0, 1.0, 0.10517645605704658], 
reward next is 0.8948, 
noisyNet noise sample is [array([0.08298158], dtype=float32), 1.4638008]. 
=============================================
[2019-04-08 15:47:32,453] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.8372089e-16 3.2250780e-09 4.6744614e-07 1.0870707e-06 5.2990584e-10
 6.8073641e-12 3.4427234e-09 4.3620049e-14 1.5929945e-17 1.5627666e-18
 9.9999845e-01], sum to 1.0000
[2019-04-08 15:47:32,455] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.4506
[2019-04-08 15:47:32,502] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 40.66666666666667, 73.5, 758.3333333333333, 19.0, 26.08441021167803, 0.5390343505203209, 0.0, 1.0, 65.0, 49936.21946325229], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2456400.0000, 
sim time next is 2457000.0000, 
raw observation next is [-3.95, 39.5, 76.0, 777.0, 19.0, 26.1260654434048, 0.5501484112495133, 0.0, 1.0, 65.0, 49072.69910940114], 
processed observation next is [0.0, 0.43478260869565216, 0.3531855955678671, 0.395, 0.25333333333333335, 0.8585635359116022, 0.08333333333333333, 0.6771721202837334, 0.6833828037498377, 0.0, 1.0, 1.0, 0.23367951956857685], 
reward next is 0.7663, 
noisyNet noise sample is [array([-2.781086], dtype=float32), -0.63487]. 
=============================================
[2019-04-08 15:47:32,515] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[75.16021 ]
 [75.159485]
 [75.21921 ]
 [75.243484]
 [75.23978 ]], R is [[75.11925507]
 [75.13027191]
 [75.13774109]
 [75.14151001]
 [75.14092255]].
[2019-04-08 15:47:33,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.06245265e-16 1.38074763e-09 1.05304835e-07 8.99454733e-08
 5.61891714e-11 1.49813061e-12 5.49495383e-08 1.01318608e-14
 3.63510013e-17 6.54640094e-19 9.99999762e-01], sum to 1.0000
[2019-04-08 15:47:33,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7047
[2019-04-08 15:47:33,340] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.1, 28.0, 0.0, 0.0, 19.0, 26.95403147609885, 0.7014305176879984, 0.0, 1.0, 65.0, 39643.37549116257], 
current ob forecast is [], 
actual action is [6.1, 65.0], 
sim time this is 2484000.0000, 
sim time next is 2484600.0000, 
raw observation next is [0.9166666666666667, 28.33333333333334, 0.0, 0.0, 19.0, 26.94171514727336, 0.6983108074281003, 0.0, 1.0, 65.0, 40025.93103541956], 
processed observation next is [0.0, 0.782608695652174, 0.48799630655586346, 0.2833333333333334, 0.0, 0.0, 0.08333333333333333, 0.7451429289394467, 0.7327702691427, 0.0, 1.0, 1.0, 0.190599671597236], 
reward next is 0.8094, 
noisyNet noise sample is [array([-0.31189433], dtype=float32), -0.31720605]. 
=============================================
[2019-04-08 15:47:33,420] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8664876e-19 1.7530632e-12 5.5531085e-10 6.6236815e-08 3.0921470e-12
 6.2293778e-16 3.6633307e-10 1.0299462e-15 2.3145250e-19 4.1368650e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:47:33,426] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8722
[2019-04-08 15:47:33,448] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.25, 50.0, 0.0, 0.0, 19.0, 26.94515363680988, 0.7894844898271245, 0.0, 1.0, 65.0, 40066.50744838314], 
current ob forecast is [], 
actual action is [2.75, 65.0], 
sim time this is 2579400.0000, 
sim time next is 2580000.0000, 
raw observation next is [-2.433333333333333, 52.0, 0.0, 0.0, 19.0, 26.93389161274668, 0.7896261146084159, 0.0, 1.0, 65.0, 39982.98222752901], 
processed observation next is [1.0, 0.8695652173913043, 0.3951985226223454, 0.52, 0.0, 0.0, 0.08333333333333333, 0.7444909677288901, 0.7632087048694719, 0.0, 1.0, 1.0, 0.19039515346442387], 
reward next is 0.8096, 
noisyNet noise sample is [array([-1.3897635], dtype=float32), 1.0034835]. 
=============================================
[2019-04-08 15:47:33,452] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.57855 ]
 [83.69208 ]
 [83.61357 ]
 [83.94545 ]
 [83.961945]], R is [[83.73558807]
 [83.70743561]
 [83.6805191 ]
 [83.65608978]
 [83.63779449]].
[2019-04-08 15:47:34,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4143998e-18 4.9178484e-10 6.9632520e-08 1.4949018e-07 2.9541792e-12
 2.8612342e-13 2.6658349e-09 1.9628176e-15 7.2496337e-19 9.1801619e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:47:34,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5584
[2019-04-08 15:47:34,134] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.533333333333333, 26.66666666666667, 167.0, 413.0, 22.5, 27.979763447807, 0.9013597359045266, 1.0, 1.0, 65.0, 21980.27349628082], 
current ob forecast is [], 
actual action is [8.533333333333333, 65.0], 
sim time this is 2555400.0000, 
sim time next is 2556000.0000, 
raw observation next is [3.8, 26.0, 165.0, 378.5, 22.5, 27.88563509623835, 0.9121269573421978, 1.0, 1.0, 65.0, 25522.49632036958], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.26, 0.55, 0.41823204419889504, 0.375, 0.8238029246865292, 0.8040423191140659, 1.0, 1.0, 1.0, 0.12153569676366467], 
reward next is 0.8785, 
noisyNet noise sample is [array([0.91745764], dtype=float32), 0.43261722]. 
=============================================
[2019-04-08 15:47:34,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.63218 ]
 [87.72762 ]
 [87.759895]
 [87.71016 ]
 [87.66332 ]], R is [[87.5113678 ]
 [87.53158569]
 [87.55196381]
 [87.56117249]
 [87.55366516]].
[2019-04-08 15:47:35,944] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-08 15:47:35,944] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:47:35,944] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:47:35,944] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:47:35,945] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:47:35,946] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:47:35,948] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:47:35,949] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run18
[2019-04-08 15:47:35,987] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run18
[2019-04-08 15:47:36,040] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run18
[2019-04-08 15:48:19,746] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06506711]
[2019-04-08 15:48:19,747] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [10.26666666666667, 61.16666666666666, 78.33333333333334, 675.6666666666667, 22.5, 28.48436303695974, 1.21911433561091, 1.0, 1.0, 65.0, 9424.41252427531]
[2019-04-08 15:48:19,747] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:48:19,747] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [1.9202505e-20 6.0237683e-12 3.6356291e-09 5.8595528e-09 7.9405948e-13
 1.0866290e-15 2.4268901e-10 3.6085602e-17 2.1242337e-21 5.2428003e-24
 1.0000000e+00], sampled 0.6734296051239788
[2019-04-08 15:48:49,044] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06506711]
[2019-04-08 15:48:49,045] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [16.8, 61.0, 209.5, 336.5, 22.5, 28.73182621372934, 1.30769281215868, 1.0, 1.0, 65.0, 18847.38652162353]
[2019-04-08 15:48:49,045] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:48:49,046] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [2.5090197e-21 1.9955077e-12 1.3520547e-09 1.9460120e-09 2.2987688e-13
 2.1527827e-16 9.0635596e-11 7.1402942e-18 2.6573503e-22 5.0656042e-25
 1.0000000e+00], sampled 0.8027469826316603
[2019-04-08 15:48:51,586] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06506711]
[2019-04-08 15:48:51,586] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-1.833333333333333, 100.0, 0.0, 0.0, 19.0, 27.01026449778824, 0.9072679744824645, 0.0, 1.0, 65.0, 42347.87581588675]
[2019-04-08 15:48:51,587] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:48:51,587] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [4.2452416e-19 1.3787544e-11 8.1609821e-09 2.0482375e-08 6.4531496e-12
 8.5272645e-15 1.8739832e-09 3.8404741e-16 3.0052082e-20 1.1089153e-22
 1.0000000e+00], sampled 0.5698211740425937
[2019-04-08 15:48:53,126] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06506711]
[2019-04-08 15:48:53,126] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation this: [-7.666666666666666, 74.66666666666666, 0.0, 0.0, 19.0, 26.59584885350978, 0.7196168211958042, 0.0, 1.0, 65.0, 51494.9803567854]
[2019-04-08 15:48:53,126] A3C_EVAL-Part4-Light-Pit-Train-v2 DEBUG:Observation forecast: []
[2019-04-08 15:48:53,127] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Softmax [1.1782281e-17 1.3575295e-10 4.3126693e-08 8.8309484e-08 6.2665026e-11
 9.2744985e-14 6.8264585e-09 7.7984756e-15 1.3050001e-18 6.8836323e-21
 9.9999988e-01], sampled 0.7944742553643429
[2019-04-08 15:49:13,881] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.06506711]
[2019-04-08 15:49:13,881] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-1.007439215833333, 72.415957005, 88.96554263, 0.0, 22.5, 26.84702231557594, 0.779789882659152, 1.0, 1.0, 65.0, 40439.60205614755]
[2019-04-08 15:49:13,881] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:49:13,882] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [8.8281202e-19 3.7592120e-11 1.9039746e-08 3.0632545e-08 1.1476523e-11
 1.5113143e-14 2.4186104e-09 8.4210369e-16 9.5335700e-20 4.9212349e-22
 1.0000000e+00], sampled 0.9685156152051068
[2019-04-08 15:49:16,510] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.6864 316115857.2297 2958.0261
[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,538] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:16,676] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,243] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6800.8527 355980929.4075 2370.5733
[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,263] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:23,375] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,197] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.4651 342832329.1818 2768.0760
[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,223] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:25,342] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:49:26,226] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 340000, evaluation results [340000.0, 6863.465099134063, 342832329.1818459, 2768.0759519131266, 6990.6863941440715, 316115857.2297444, 2958.0261175185124, 6800.852717107343, 355980929.40745914, 2370.573292155841]
[2019-04-08 15:49:27,112] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2347692e-17 2.8811276e-09 4.0417663e-07 2.6631915e-06 2.1598350e-11
 7.6231577e-13 1.6884064e-08 5.9600605e-15 9.0481780e-18 8.6388926e-21
 9.9999690e-01], sum to 1.0000
[2019-04-08 15:49:27,114] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7551
[2019-04-08 15:49:27,124] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.3, 29.0, 136.5, 313.0, 22.5, 27.93760142716447, 0.9392730325453619, 1.0, 1.0, 65.0, 23923.60520978479], 
current ob forecast is [], 
actual action is [8.3, 65.0], 
sim time this is 2559600.0000, 
sim time next is 2560200.0000, 
raw observation next is [3.3, 29.0, 129.0, 325.6666666666667, 22.5, 27.94405306571633, 0.9536727981771286, 1.0, 1.0, 65.0, 23474.60790838907], 
processed observation next is [1.0, 0.6521739130434783, 0.554016620498615, 0.29, 0.43, 0.3598526703499079, 0.375, 0.8286710888096941, 0.8178909327257095, 1.0, 1.0, 1.0, 0.1117838471828051], 
reward next is 0.8882, 
noisyNet noise sample is [array([0.2890767], dtype=float32), -0.7036381]. 
=============================================
[2019-04-08 15:49:27,523] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4892402e-15 4.6132316e-09 1.3464140e-06 1.7912826e-06 4.3509987e-09
 5.5156097e-13 5.7566247e-08 4.5123172e-13 1.4895070e-16 7.8686855e-20
 9.9999678e-01], sum to 1.0000
[2019-04-08 15:49:27,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0889
[2019-04-08 15:49:27,544] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 36.33333333333334, 0.0, 0.0, 19.0, 26.70850962674538, 0.6366023491348182, 0.0, 1.0, 65.0, 44277.19089157471], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 2495400.0000, 
sim time next is 2496000.0000, 
raw observation next is [-1.2, 35.66666666666667, 0.0, 0.0, 19.0, 26.71770619405332, 0.6342661689545112, 0.0, 1.0, 65.0, 44035.74231751567], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.3566666666666667, 0.0, 0.0, 0.08333333333333333, 0.7264755161711101, 0.7114220563181703, 0.0, 1.0, 1.0, 0.2096940110357889], 
reward next is 0.7903, 
noisyNet noise sample is [array([-0.02447282], dtype=float32), -0.9669309]. 
=============================================
[2019-04-08 15:49:27,565] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.7442 ]
 [78.73493]
 [78.71685]
 [78.73238]
 [78.76207]], R is [[78.7702179 ]
 [78.77166748]
 [78.77462769]
 [78.77901459]
 [78.78472137]].
[2019-04-08 15:49:27,682] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6280274e-18 4.7851327e-09 2.4755357e-06 3.2655836e-07 9.0901303e-10
 1.5191522e-12 1.0152758e-08 3.3150005e-13 3.2277577e-16 5.8312065e-20
 9.9999714e-01], sum to 1.0000
[2019-04-08 15:49:27,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6076
[2019-04-08 15:49:27,707] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.2, 33.0, 0.0, 0.0, 19.0, 26.70737030492064, 0.6219449585103397, 0.0, 1.0, 65.0, 45488.37388306077], 
current ob forecast is [], 
actual action is [3.8, 65.0], 
sim time this is 2498400.0000, 
sim time next is 2499000.0000, 
raw observation next is [-1.1, 33.33333333333334, 0.0, 0.0, 19.0, 26.67958001562882, 0.6186439215712861, 0.0, 1.0, 65.0, 45880.27149916216], 
processed observation next is [0.0, 0.9565217391304348, 0.4321329639889197, 0.3333333333333334, 0.0, 0.0, 0.08333333333333333, 0.7232983346357349, 0.706214640523762, 0.0, 1.0, 1.0, 0.21847748332934364], 
reward next is 0.7815, 
noisyNet noise sample is [array([0.50616896], dtype=float32), -0.49020126]. 
=============================================
[2019-04-08 15:49:27,737] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.96793]
 [77.97632]
 [78.00577]
 [78.0064 ]
 [78.0292 ]], R is [[77.96810913]
 [77.97181702]
 [77.9828949 ]
 [77.99819183]
 [78.00881958]].
[2019-04-08 15:49:27,887] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.33705992e-18 1.15587095e-10 9.60669553e-08 2.35884254e-08
 1.29416035e-11 5.28055410e-13 3.45369866e-09 2.70154509e-15
 6.48495642e-20 3.85736332e-21 9.99999881e-01], sum to 1.0000
[2019-04-08 15:49:27,887] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3183
[2019-04-08 15:49:27,903] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.1, 82.16666666666667, 0.0, 0.0, 19.0, 26.34133563754223, 0.5699014472622493, 0.0, 1.0, 65.0, 52135.68963249401], 
current ob forecast is [], 
actual action is [-1.0999999999999996, 65.0], 
sim time this is 2609400.0000, 
sim time next is 2610000.0000, 
raw observation next is [-6.2, 83.0, 0.0, 0.0, 19.0, 26.29849817280746, 0.5622334132981047, 0.0, 1.0, 65.0, 53892.58111133354], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6915415144006216, 0.6874111377660349, 0.0, 1.0, 1.0, 0.2566313386253978], 
reward next is 0.7434, 
noisyNet noise sample is [array([-0.14739174], dtype=float32), 2.4515965]. 
=============================================
[2019-04-08 15:49:27,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.49091 ]
 [86.45373 ]
 [86.39596 ]
 [86.36164 ]
 [86.283424]], R is [[86.41429138]
 [86.30187988]
 [86.19618225]
 [86.08364105]
 [85.96504974]].
[2019-04-08 15:49:29,235] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2759769e-18 5.9030164e-10 2.0394067e-07 5.9691551e-08 8.4682050e-10
 1.2118719e-13 1.7249686e-09 3.6949922e-14 1.1027079e-18 4.3954601e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:49:29,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0036
[2019-04-08 15:49:29,285] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 59.0, 0.0, 0.0, 19.0, 26.72485531550395, 0.7186637229798029, 0.0, 1.0, 65.0, 49205.89960277535], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2847000.0000, 
sim time next is 2847600.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 26.71289289348434, 0.7159725559798648, 0.0, 1.0, 65.0, 48860.53783914555], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7260744077903617, 0.7386575186599549, 0.0, 1.0, 1.0, 0.23266922780545501], 
reward next is 0.7673, 
noisyNet noise sample is [array([-1.6477808], dtype=float32), 0.42485312]. 
=============================================
[2019-04-08 15:49:29,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0474704e-17 7.3115437e-11 4.4197186e-08 7.5563023e-09 2.6954106e-11
 3.6743400e-13 1.2854482e-09 5.0745045e-16 1.9482230e-19 2.3657538e-20
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:29,684] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3235
[2019-04-08 15:49:29,698] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.54425030705502, 0.6234180241157864, 0.0, 1.0, 65.0, 48986.84607975614], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2601000.0000, 
sim time next is 2601600.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.49983620387142, 0.6274362538553944, 0.0, 1.0, 65.0, 49619.13048061132], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7083196836559518, 0.7091454179517981, 0.0, 1.0, 1.0, 0.23628157371719674], 
reward next is 0.7637, 
noisyNet noise sample is [array([0.7757289], dtype=float32), 1.5766174]. 
=============================================
[2019-04-08 15:49:29,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3557987e-19 1.1893815e-11 1.8606912e-09 3.3018912e-09 1.1690718e-11
 1.8812259e-15 3.8758080e-10 2.6020466e-16 1.9587889e-20 2.5835124e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:29,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0962
[2019-04-08 15:49:29,872] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.7, 75.0, 85.0, 45.5, 22.5, 26.78209130190831, 0.6159231428858539, 1.0, 1.0, 65.0, 44444.93843317486], 
current ob forecast is [], 
actual action is [-1.7000000000000002, 65.0], 
sim time this is 2624400.0000, 
sim time next is 2625000.0000, 
raw observation next is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 22.5, 26.76397689465565, 0.6281057085416306, 1.0, 1.0, 65.0, 46225.99152784333], 
processed observation next is [1.0, 0.391304347826087, 0.2848568790397045, 0.7333333333333333, 0.2922222222222222, 0.06703499079189687, 0.375, 0.7303314078879707, 0.7093685695138768, 1.0, 1.0, 1.0, 0.22012376918020635], 
reward next is 0.7799, 
noisyNet noise sample is [array([-0.10599762], dtype=float32), -0.7218496]. 
=============================================
[2019-04-08 15:49:29,884] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[88.07331 ]
 [87.90725 ]
 [88.37865 ]
 [88.138565]
 [88.24947 ]], R is [[87.73993683]
 [87.65090179]
 [87.5637207 ]
 [87.46981049]
 [87.36096954]].
[2019-04-08 15:49:30,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9026548e-18 1.2872650e-10 2.5196471e-07 1.5446614e-07 2.1692714e-10
 6.2757017e-14 1.1776176e-08 2.0510149e-13 2.9651377e-18 4.7039458e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:30,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2050
[2019-04-08 15:49:30,613] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666667, 64.0, 0.0, 0.0, 19.0, 26.45411429815753, 0.6482117944451197, 0.0, 1.0, 65.0, 50462.65926471799], 
current ob forecast is [], 
actual action is [0.33333333333333304, 65.0], 
sim time this is 2593200.0000, 
sim time next is 2593800.0000, 
raw observation next is [-4.75, 65.0, 0.0, 0.0, 19.0, 26.43564274857121, 0.6599973512971069, 0.0, 1.0, 65.0, 50633.90772207979], 
processed observation next is [1.0, 0.0, 0.3310249307479225, 0.65, 0.0, 0.0, 0.08333333333333333, 0.702970229047601, 0.7199991170990355, 0.0, 1.0, 1.0, 0.24111384629561805], 
reward next is 0.7589, 
noisyNet noise sample is [array([-1.1146868], dtype=float32), 0.02332158]. 
=============================================
[2019-04-08 15:49:30,619] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2872792e-17 7.4782368e-11 2.5714610e-07 1.9421839e-06 6.6807879e-12
 2.2512293e-13 3.4702406e-08 3.5657398e-15 5.4108386e-18 1.6360568e-21
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:49:30,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4788
[2019-04-08 15:49:30,650] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.666666666666666, 69.5, 0.0, 0.0, 19.0, 26.31854289420194, 0.6152016680937789, 0.0, 1.0, 65.0, 54568.10986548705], 
current ob forecast is [], 
actual action is [-3.666666666666666, 65.0], 
sim time this is 2681400.0000, 
sim time next is 2682000.0000, 
raw observation next is [-9.0, 69.0, 0.0, 0.0, 19.0, 26.29738358154735, 0.6166520769905682, 0.0, 1.0, 65.0, 54603.52121624865], 
processed observation next is [1.0, 0.043478260869565216, 0.21329639889196678, 0.69, 0.0, 0.0, 0.08333333333333333, 0.6914486317956126, 0.7055506923301894, 0.0, 1.0, 1.0, 0.2600167676964221], 
reward next is 0.7400, 
noisyNet noise sample is [array([0.39294147], dtype=float32), 1.2775785]. 
=============================================
[2019-04-08 15:49:30,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.61038 ]
 [82.65918 ]
 [83.040955]
 [83.20082 ]
 [83.86095 ]], R is [[82.39833069]
 [82.3144989 ]
 [82.23168945]
 [82.15196991]
 [82.06702423]].
[2019-04-08 15:49:31,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4997898e-18 1.3188862e-09 2.6813947e-08 2.6950761e-07 4.5199754e-11
 8.2797091e-14 6.4986153e-08 6.4460766e-15 1.1777741e-17 8.4380549e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:49:31,235] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4939
[2019-04-08 15:49:31,252] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.58148525879242, 0.6311224050764127, 0.0, 1.0, 65.0, 48720.90852411197], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2600400.0000, 
sim time next is 2601000.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 19.0, 26.5442503070093, 0.6234180241025994, 0.0, 1.0, 65.0, 48986.84608029834], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.08333333333333333, 0.7120208589174416, 0.7078060080341998, 0.0, 1.0, 1.0, 0.2332706956204683], 
reward next is 0.7667, 
noisyNet noise sample is [array([0.19728374], dtype=float32), -0.4556819]. 
=============================================
[2019-04-08 15:49:31,260] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.75505 ]
 [80.74753 ]
 [80.55233 ]
 [80.330894]
 [80.289604]], R is [[80.97885895]
 [80.93706512]
 [80.90145874]
 [80.87380981]
 [80.83971405]].
[2019-04-08 15:49:31,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9022671e-17 1.0440877e-10 3.2688526e-08 1.2374387e-06 4.8328936e-12
 2.3066144e-13 3.8437676e-08 5.4101428e-14 8.1836915e-18 3.1378189e-20
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:49:31,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1481
[2019-04-08 15:49:31,609] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.366666666666667, 81.33333333333334, 0.0, 0.0, 19.0, 26.18248086585407, 0.5492052561086848, 0.0, 1.0, 65.0, 56002.58144078313], 
current ob forecast is [], 
actual action is [-1.3666666666666671, 65.0], 
sim time this is 2611200.0000, 
sim time next is 2611800.0000, 
raw observation next is [-6.45, 80.5, 0.0, 0.0, 19.0, 26.14469448487321, 0.5441990152835602, 0.0, 1.0, 65.0, 56433.1358800591], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.805, 0.0, 0.0, 0.08333333333333333, 0.6787245404061008, 0.6813996717611867, 0.0, 1.0, 1.0, 0.2687292184764719], 
reward next is 0.7313, 
noisyNet noise sample is [array([-1.9484159], dtype=float32), -1.2859018]. 
=============================================
[2019-04-08 15:49:31,637] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2059082e-18 1.1939863e-11 4.1305594e-08 6.2802492e-08 4.1395304e-10
 6.1912154e-15 7.0552408e-09 1.2887846e-16 1.1182326e-19 4.7903425e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:31,640] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1480
[2019-04-08 15:49:31,659] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.8, 79.66666666666667, 0.0, 0.0, 19.0, 26.28307608935674, 0.5826583612396804, 0.0, 1.0, 65.0, 54129.89697051659], 
current ob forecast is [], 
actual action is [-0.7999999999999998, 65.0], 
sim time this is 2607600.0000, 
sim time next is 2608200.0000, 
raw observation next is [-5.9, 80.5, 0.0, 0.0, 19.0, 26.31512989927582, 0.5841013741810523, 0.0, 1.0, 65.0, 52621.88547324672], 
processed observation next is [1.0, 0.17391304347826086, 0.2991689750692521, 0.805, 0.0, 0.0, 0.08333333333333333, 0.6929274916063184, 0.6947004580603507, 0.0, 1.0, 1.0, 0.2505804070154606], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.4873088], dtype=float32), -0.31115204]. 
=============================================
[2019-04-08 15:49:32,419] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2352524e-17 2.2506699e-10 2.3517114e-07 1.4608598e-07 1.9444740e-10
 1.8056628e-13 5.5354736e-08 8.2785663e-15 5.0568513e-19 2.5495034e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:32,437] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6250
[2019-04-08 15:49:32,458] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.3, 79.0, 0.0, 0.0, 22.5, 26.14640936313234, 0.5270868629713239, 1.0, 1.0, 65.0, 57494.49375701643], 
current ob forecast is [], 
actual action is [-2.3, 65.0], 
sim time this is 2619000.0000, 
sim time next is 2619600.0000, 
raw observation next is [-7.3, 79.0, 18.66666666666666, 6.666666666666667, 22.5, 26.14477746014574, 0.53464142006158, 0.0, 1.0, 65.0, 57043.68312209913], 
processed observation next is [1.0, 0.30434782608695654, 0.26038781163434904, 0.79, 0.0622222222222222, 0.007366482504604052, 0.375, 0.6787314550121449, 0.6782138066871933, 0.0, 1.0, 1.0, 0.27163658629571014], 
reward next is 0.7284, 
noisyNet noise sample is [array([-1.8637567], dtype=float32), -0.6192747]. 
=============================================
[2019-04-08 15:49:32,944] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0762970e-17 6.5241236e-11 8.8573074e-08 7.1195143e-07 2.7591072e-11
 1.8839086e-13 3.2435516e-08 2.1021481e-14 3.5621110e-17 9.7038096e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:49:32,949] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7446
[2019-04-08 15:49:32,983] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.833333333333334, 64.0, 113.6666666666667, 745.3333333333334, 22.5, 27.04240522591557, 0.4150984590275191, 1.0, 1.0, 65.0, 37347.97319720381], 
current ob forecast is [], 
actual action is [-3.833333333333334, 65.0], 
sim time this is 2718600.0000, 
sim time next is 2719200.0000, 
raw observation next is [-8.666666666666668, 64.0, 112.8333333333333, 763.1666666666667, 22.5, 26.85109362731372, 0.7728394759508931, 1.0, 1.0, 65.0, 52095.46679141602], 
processed observation next is [1.0, 0.4782608695652174, 0.22253000923361033, 0.64, 0.376111111111111, 0.8432780847145489, 0.375, 0.7375911356094766, 0.7576131586502978, 1.0, 1.0, 1.0, 0.24807365138769535], 
reward next is 0.7519, 
noisyNet noise sample is [array([-0.48078397], dtype=float32), 1.1239936]. 
=============================================
[2019-04-08 15:49:33,330] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0985594e-17 5.2512800e-10 2.3898669e-06 5.9992385e-06 5.9467438e-11
 7.7842816e-14 5.0355204e-08 6.3971086e-14 2.7371208e-17 1.8672157e-18
 9.9999154e-01], sum to 1.0000
[2019-04-08 15:49:33,335] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5747
[2019-04-08 15:49:33,375] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.6, 85.66666666666667, 0.0, 0.0, 19.0, 25.94198020918265, 0.5264815944472699, 0.0, 1.0, 65.0, 58794.09570047292], 
current ob forecast is [], 
actual action is [-8.6, 65.0], 
sim time this is 2690400.0000, 
sim time next is 2691000.0000, 
raw observation next is [-13.95, 87.0, 0.0, 0.0, 19.0, 25.91936325834265, 0.514413122158076, 0.0, 1.0, 65.0, 58427.18330345902], 
processed observation next is [1.0, 0.13043478260869565, 0.07617728531855956, 0.87, 0.0, 0.0, 0.08333333333333333, 0.6599469381952208, 0.6714710407193586, 0.0, 1.0, 1.0, 0.27822468239742393], 
reward next is 0.7218, 
noisyNet noise sample is [array([1.2154788], dtype=float32), 0.17878859]. 
=============================================
[2019-04-08 15:49:33,383] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.59655 ]
 [77.63997 ]
 [77.659485]
 [77.77836 ]
 [77.84011 ]], R is [[77.46317291]
 [77.40857697]
 [77.35490417]
 [77.30176544]
 [77.24810791]].
[2019-04-08 15:49:33,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9811986e-15 1.2671997e-09 1.6693149e-06 4.3776194e-07 2.0003676e-10
 3.6808356e-12 2.6530898e-07 3.4570518e-13 4.9214097e-15 3.0344178e-18
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:49:33,477] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6353
[2019-04-08 15:49:33,497] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.2417429e-18 5.5237498e-10 1.4846479e-07 2.6038472e-07 3.5610091e-11
 6.2649613e-14 8.9262684e-08 1.2467303e-13 1.5483546e-18 4.9195110e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:33,497] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5994
[2019-04-08 15:49:33,501] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-15.33333333333333, 83.0, 0.0, 0.0, 19.0, 25.63051773511588, 0.4215103498370915, 0.0, 1.0, 65.0, 56494.22798919848], 
current ob forecast is [], 
actual action is [-10.33333333333333, 65.0], 
sim time this is 2702400.0000, 
sim time next is 2703000.0000, 
raw observation next is [-15.16666666666667, 83.0, 0.0, 0.0, 19.0, 25.57290302781455, 0.4144153629893029, 0.0, 1.0, 65.0, 57685.99417822452], 
processed observation next is [1.0, 0.2608695652173913, 0.04247460757156039, 0.83, 0.0, 0.0, 0.08333333333333333, 0.6310752523178792, 0.6381384543297677, 0.0, 1.0, 1.0, 0.2746952103724977], 
reward next is 0.7253, 
noisyNet noise sample is [array([-0.9246311], dtype=float32), -0.16781591]. 
=============================================
[2019-04-08 15:49:33,507] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[75.54644 ]
 [75.640564]
 [75.77632 ]
 [75.87872 ]
 [75.99311 ]], R is [[75.39791107]
 [75.37491608]
 [75.34934998]
 [75.32241058]
 [75.29573059]].
[2019-04-08 15:49:33,518] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 22.5, 26.7639768931079, 0.6281057080917556, 1.0, 1.0, 65.0, 46225.99154674191], 
current ob forecast is [], 
actual action is [-1.416666666666667, 65.0], 
sim time this is 2625000.0000, 
sim time next is 2625600.0000, 
raw observation next is [-6.133333333333335, 71.66666666666667, 90.33333333333333, 75.83333333333334, 22.5, 26.80943563098587, 0.6444683776074057, 1.0, 1.0, 65.0, 45250.70457336451], 
processed observation next is [1.0, 0.391304347826087, 0.29270544783010155, 0.7166666666666667, 0.3011111111111111, 0.0837937384898711, 0.375, 0.7341196359154892, 0.7148227925358018, 1.0, 1.0, 1.0, 0.21547954558745006], 
reward next is 0.7845, 
noisyNet noise sample is [array([2.9556239], dtype=float32), 0.7020301]. 
=============================================
[2019-04-08 15:49:33,612] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.8731909e-18 1.9381163e-11 2.4541844e-08 1.5716848e-08 2.0436209e-12
 1.9141706e-15 1.4692660e-09 3.1278998e-15 2.5516576e-18 4.9938376e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:33,614] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7417
[2019-04-08 15:49:33,651] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 77.0, 77.0, 0.0, 22.5, 26.3386859616123, 0.5835882841168459, 1.0, 1.0, 65.0, 49170.42194336836], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 2622600.0000, 
sim time next is 2623200.0000, 
raw observation next is [-6.9, 76.33333333333334, 79.66666666666667, 15.16666666666666, 22.5, 26.56524543229927, 0.606550512977796, 1.0, 1.0, 65.0, 45838.40139508487], 
processed observation next is [1.0, 0.34782608695652173, 0.27146814404432135, 0.7633333333333334, 0.26555555555555554, 0.01675874769797421, 0.375, 0.7137704526916059, 0.702183504325932, 1.0, 1.0, 1.0, 0.21827810188135652], 
reward next is 0.7817, 
noisyNet noise sample is [array([0.8719554], dtype=float32), -0.5536316]. 
=============================================
[2019-04-08 15:49:34,148] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.4130045e-17 1.0639287e-10 3.5762763e-08 6.9426314e-08 3.2582678e-12
 2.4116969e-12 3.3936896e-08 1.0645314e-14 3.3839429e-19 2.8386482e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:34,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6730
[2019-04-08 15:49:34,173] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.5, 50.0, 41.0, 103.0, 22.5, 26.60395237040253, 0.8041301997805888, 1.0, 1.0, 65.0, 33860.75944788058], 
current ob forecast is [], 
actual action is [5.5, 65.0], 
sim time this is 2653200.0000, 
sim time next is 2653800.0000, 
raw observation next is [0.3166666666666667, 50.66666666666667, 29.66666666666666, 96.0, 22.5, 27.69417509898049, 0.8855919297620645, 1.0, 1.0, 65.0, 25252.60272251028], 
processed observation next is [1.0, 0.7391304347826086, 0.47137580794090495, 0.5066666666666667, 0.09888888888888887, 0.10607734806629834, 0.375, 0.8078479249150409, 0.7951973099206882, 1.0, 1.0, 1.0, 0.12025048915481086], 
reward next is 0.8797, 
noisyNet noise sample is [array([-1.1203243], dtype=float32), 0.08045591]. 
=============================================
[2019-04-08 15:49:34,311] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [7.2472569e-18 1.3646055e-10 4.9493826e-07 7.0511646e-08 1.4905570e-12
 6.0821096e-13 5.3485070e-09 1.0314167e-14 1.6587917e-19 1.9257070e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:34,311] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8839
[2019-04-08 15:49:34,329] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.833333333333333, 56.66666666666667, 227.5, 167.0, 22.5, 27.28365991839661, 0.7774735467549144, 1.0, 1.0, 65.0, 38030.01960517713], 
current ob forecast is [], 
actual action is [2.166666666666667, 65.0], 
sim time this is 2634000.0000, 
sim time next is 2634600.0000, 
raw observation next is [-2.566666666666666, 55.33333333333333, 231.0, 163.0, 22.5, 27.30257406940185, 0.7905876670870541, 1.0, 1.0, 65.0, 38048.93688785905], 
processed observation next is [1.0, 0.4782608695652174, 0.39150507848568794, 0.5533333333333332, 0.77, 0.18011049723756906, 0.375, 0.7752145057834875, 0.7635292223623513, 1.0, 1.0, 1.0, 0.18118541375170974], 
reward next is 0.8188, 
noisyNet noise sample is [array([-0.17726946], dtype=float32), 0.3852826]. 
=============================================
[2019-04-08 15:49:34,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.03626626e-16 1.27358879e-09 4.27570058e-07 3.64770870e-07
 8.40627151e-11 2.93213284e-12 4.06220764e-08 1.14067759e-13
 4.18930650e-17 4.92589622e-19 9.99999166e-01], sum to 1.0000
[2019-04-08 15:49:34,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1606
[2019-04-08 15:49:34,716] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-10.66666666666667, 74.83333333333333, 0.0, 0.0, 19.0, 26.09745350783879, 0.5719066315370477, 0.0, 1.0, 65.0, 58435.26177727281], 
current ob forecast is [], 
actual action is [-5.66666666666667, 65.0], 
sim time this is 2685000.0000, 
sim time next is 2685600.0000, 
raw observation next is [-11.0, 76.0, 0.0, 0.0, 19.0, 26.06170602000549, 0.5818981191622353, 0.0, 1.0, 65.0, 58503.17926337759], 
processed observation next is [1.0, 0.08695652173913043, 0.15789473684210528, 0.76, 0.0, 0.0, 0.08333333333333333, 0.6718088350004576, 0.6939660397207451, 0.0, 1.0, 1.0, 0.2785865679208457], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.8432787], dtype=float32), 0.9941566]. 
=============================================
[2019-04-08 15:49:35,724] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [5.2826454e-17 1.5961159e-10 4.9459210e-08 6.5036380e-07 2.7657476e-10
 3.2788120e-13 8.2256063e-10 1.6690395e-15 4.4721985e-19 4.1488217e-22
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:49:35,724] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7790
[2019-04-08 15:49:35,758] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 56.00000000000001, 0.0, 0.0, 22.5, 26.5990261460366, 0.8111401747348045, 1.0, 1.0, 65.0, 68560.68552908562], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 2658000.0000, 
sim time next is 2658600.0000, 
raw observation next is [-0.8999999999999999, 57.0, 0.0, 0.0, 22.5, 25.62329019880148, 0.7039496915317017, 1.0, 1.0, 65.0, 49374.56042319092], 
processed observation next is [1.0, 0.782608695652174, 0.43767313019390586, 0.57, 0.0, 0.0, 0.375, 0.6352741832334567, 0.734649897177234, 1.0, 1.0, 1.0, 0.23511695439614722], 
reward next is 0.7649, 
noisyNet noise sample is [array([0.25663164], dtype=float32), 0.5733759]. 
=============================================
[2019-04-08 15:49:36,607] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7823057e-17 2.3089271e-10 7.1145948e-08 7.9698225e-08 8.7441426e-12
 2.8107142e-13 1.2174509e-09 1.7261846e-15 9.1698875e-19 7.9050800e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:36,611] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4831
[2019-04-08 15:49:36,646] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 56.5, 106.6666666666667, 769.6666666666666, 22.5, 27.48488472451011, 0.6904944289123253, 1.0, 1.0, 65.0, 37545.8008308498], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2728200.0000, 
sim time next is 2728800.0000, 
raw observation next is [-4.8, 56.0, 105.5, 760.5, 22.5, 26.72599892605682, 0.8404631617304018, 1.0, 1.0, 65.0, 55375.80070960423], 
processed observation next is [1.0, 0.6086956521739131, 0.3296398891966759, 0.56, 0.3516666666666667, 0.8403314917127072, 0.375, 0.7271665771714018, 0.7801543872434672, 1.0, 1.0, 1.0, 0.26369428909335346], 
reward next is 0.7363, 
noisyNet noise sample is [array([-0.34587404], dtype=float32), -1.9388295]. 
=============================================
[2019-04-08 15:49:37,577] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.2174266e-17 5.4097260e-10 1.9933580e-08 2.7289781e-07 9.0494605e-11
 6.0214773e-13 6.1810623e-09 1.6865905e-14 6.9733964e-18 1.0678432e-19
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:49:37,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4443
[2019-04-08 15:49:37,615] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.77332278822095, 0.7325099972382064, 0.0, 1.0, 65.0, 65596.98142763031], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2748000.0000, 
sim time next is 2748600.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.71468396087124, 0.7257356488590755, 1.0, 1.0, 65.0, 66479.94063630345], 
processed observation next is [1.0, 0.8260869565217391, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.7262236634059368, 0.7419118829530252, 1.0, 1.0, 1.0, 0.3165711458871593], 
reward next is 0.6834, 
noisyNet noise sample is [array([1.2655841], dtype=float32), -1.5340539]. 
=============================================
[2019-04-08 15:49:38,153] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.0671475e-18 6.3396323e-11 4.2781849e-08 1.5216888e-07 1.4061453e-12
 4.2976944e-14 3.4505181e-09 1.9928303e-15 1.9292685e-18 4.1905493e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:49:38,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8734
[2019-04-08 15:49:38,202] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 72.0, 113.6666666666667, 663.6666666666667, 22.5, 26.88210104707413, 0.6804638552660377, 1.0, 1.0, 65.0, 36143.62418383696], 
current ob forecast is [], 
actual action is [-6.0, 65.0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [-10.5, 70.0, 117.0, 674.0, 22.5, 26.87969202014629, 0.6975345559114324, 1.0, 1.0, 65.0, 35704.92996539381], 
processed observation next is [1.0, 0.43478260869565216, 0.17174515235457063, 0.7, 0.39, 0.7447513812154696, 0.375, 0.7399743350121909, 0.7325115186371441, 1.0, 1.0, 1.0, 0.1700234760256848], 
reward next is 0.8300, 
noisyNet noise sample is [array([1.0601242], dtype=float32), 1.2207996]. 
=============================================
[2019-04-08 15:49:38,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5599130e-17 6.0622798e-11 7.2325452e-09 8.0580641e-08 2.8992309e-11
 5.6140886e-14 3.9518888e-10 5.4875121e-14 4.0072038e-19 4.2259792e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:38,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3200
[2019-04-08 15:49:38,255] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 49.00000000000001, 141.3333333333333, 678.0, 22.5, 27.4098213086704, 0.8649119380996066, 1.0, 1.0, 65.0, 50695.78494581833], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 2805000.0000, 
sim time next is 2805600.0000, 
raw observation next is [-1.110223024625157e-16, 48.0, 133.1666666666667, 720.5, 22.5, 27.0105625569039, 0.7918078436163033, 1.0, 1.0, 65.0, 44670.85731881797], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.48, 0.44388888888888905, 0.7961325966850828, 0.375, 0.7508802130753249, 0.7639359478721012, 1.0, 1.0, 1.0, 0.2127183681848475], 
reward next is 0.7873, 
noisyNet noise sample is [array([0.08306544], dtype=float32), -0.31374264]. 
=============================================
[2019-04-08 15:49:38,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2941892e-15 2.1314126e-09 2.2491048e-07 3.3302567e-07 1.1989866e-10
 1.2125865e-12 1.1228440e-08 2.8376773e-14 4.3916276e-18 3.3185786e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:38,335] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3375
[2019-04-08 15:49:38,403] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.166666666666667, 54.83333333333333, 0.0, 0.0, 22.5, 27.30667567785711, 0.8208726380971715, 1.0, 1.0, 65.0, 48050.1630080979], 
current ob forecast is [], 
actual action is [0.833333333333333, 65.0], 
sim time this is 2743800.0000, 
sim time next is 2744400.0000, 
raw observation next is [-4.333333333333334, 55.66666666666667, 0.0, 0.0, 22.5, 27.32837336307099, 0.777181020404386, 1.0, 1.0, 64.99999999999997, 53219.11020597925], 
processed observation next is [1.0, 0.782608695652174, 0.3425669436749769, 0.5566666666666668, 0.0, 0.0, 0.375, 0.7773644469225826, 0.7590603401347953, 1.0, 1.0, 0.9999999999999994, 0.25342433431418687], 
reward next is 0.7466, 
noisyNet noise sample is [array([0.1049863], dtype=float32), 3.0509906]. 
=============================================
[2019-04-08 15:49:39,165] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.51653976e-15 9.37846578e-10 2.86019723e-08 9.02720785e-07
 1.59976210e-08 8.88972760e-13 6.67170497e-08 1.03760655e-13
 1.07333295e-16 2.19316227e-18 9.99998927e-01], sum to 1.0000
[2019-04-08 15:49:39,165] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7071
[2019-04-08 15:49:39,200] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.26503167738285, 0.5599059972575725, 0.0, 1.0, 65.0, 51251.24450326199], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.25552394833964, 0.5560441513791953, 0.0, 1.0, 65.0, 51651.76775304506], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6879603290283033, 0.6853480504597318, 0.0, 1.0, 1.0, 0.2459607988240241], 
reward next is 0.7540, 
noisyNet noise sample is [array([-0.03838073], dtype=float32), -1.6782638]. 
=============================================
[2019-04-08 15:49:39,952] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.2300304e-19 3.4500874e-11 8.8194518e-08 4.2060464e-08 5.5159345e-12
 1.2619122e-14 4.2697867e-09 1.1338143e-15 2.6401165e-19 4.8936594e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:39,952] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9214
[2019-04-08 15:49:39,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9135366e-16 4.5450171e-10 1.1091437e-06 2.5667950e-07 3.6806509e-11
 1.8575393e-13 1.5621599e-07 9.1059269e-15 1.6691438e-18 3.1077408e-19
 9.9999845e-01], sum to 1.0000
[2019-04-08 15:49:39,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9955
[2019-04-08 15:49:39,987] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 44.0, 151.5, 724.0, 22.5, 26.20352388720361, 0.743189948518058, 1.0, 1.0, 65.0, 55309.58999212085], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2808000.0000, 
sim time next is 2808600.0000, 
raw observation next is [2.333333333333333, 42.50000000000001, 160.3333333333333, 711.0, 22.5, 27.30153447483146, 0.8341325770122526, 1.0, 1.0, 65.0, 45732.80221212318], 
processed observation next is [1.0, 0.5217391304347826, 0.5272391505078486, 0.42500000000000004, 0.5344444444444443, 0.7856353591160221, 0.375, 0.7751278729026216, 0.7780441923374175, 1.0, 1.0, 1.0, 0.21777524862915798], 
reward next is 0.7822, 
noisyNet noise sample is [array([-1.7812757], dtype=float32), -0.78615856]. 
=============================================
[2019-04-08 15:49:40,003] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 64.0, 0.0, 0.0, 19.0, 26.54995540450658, 0.7002891766829019, 0.0, 1.0, 65.0, 57919.35199005387], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2754000.0000, 
sim time next is 2754600.0000, 
raw observation next is [-6.0, 63.16666666666667, 0.0, 0.0, 19.0, 26.53431059293602, 0.6971434261225609, 0.0, 1.0, 65.0, 57371.77691809794], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6316666666666667, 0.0, 0.0, 0.08333333333333333, 0.711192549411335, 0.7323811420408536, 0.0, 1.0, 1.0, 0.2731989377052283], 
reward next is 0.7268, 
noisyNet noise sample is [array([0.07595795], dtype=float32), 0.74561006]. 
=============================================
[2019-04-08 15:49:40,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1463375e-16 6.0252536e-10 1.6171785e-07 4.6091935e-07 3.5354084e-10
 1.5800486e-12 5.8230278e-09 1.0812412e-14 7.3976741e-17 1.7622427e-18
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:40,711] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8871
[2019-04-08 15:49:40,764] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.22751894665274, 0.5957813585893642, 0.0, 1.0, 65.0, 55435.96795450627], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2764800.0000, 
sim time next is 2765400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.21909752048532, 0.5924948818311461, 0.0, 1.0, 65.0, 55264.02760882329], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6849247933737767, 0.6974982939437154, 0.0, 1.0, 1.0, 0.26316203623249185], 
reward next is 0.7368, 
noisyNet noise sample is [array([-2.2092853], dtype=float32), 1.7215252]. 
=============================================
[2019-04-08 15:49:41,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5896239e-17 5.2579308e-12 5.9794196e-08 6.2876416e-06 4.4942015e-11
 5.1432283e-12 1.3554759e-08 1.3779943e-14 2.4332624e-17 3.6508056e-19
 9.9999368e-01], sum to 1.0000
[2019-04-08 15:49:41,041] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2503
[2019-04-08 15:49:41,055] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.666666666666666, 24.83333333333334, 87.0, 25.33333333333333, 22.5, 27.88508882067596, 0.8971413428510392, 1.0, 1.0, 65.0, 39028.54423428889], 
current ob forecast is [], 
actual action is [11.666666666666666, 65.0], 
sim time this is 2821800.0000, 
sim time next is 2822400.0000, 
raw observation next is [6.6, 25.0, 83.0, 38.0, 22.5, 27.95655598703509, 0.9076784266048216, 1.0, 1.0, 65.0, 38191.58526474734], 
processed observation next is [1.0, 0.6956521739130435, 0.6454293628808865, 0.25, 0.27666666666666667, 0.041988950276243095, 0.375, 0.8297129989195909, 0.8025594755349404, 1.0, 1.0, 1.0, 0.18186469173689207], 
reward next is 0.8181, 
noisyNet noise sample is [array([0.14883702], dtype=float32), 0.25185153]. 
=============================================
[2019-04-08 15:49:41,071] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1845939e-17 3.1133234e-09 6.5409438e-07 4.2076772e-07 1.3931639e-09
 2.3691565e-12 1.5520754e-08 6.5431131e-15 6.5322048e-18 1.5268263e-20
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:49:41,073] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4460
[2019-04-08 15:49:41,097] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.5, 29.0, 5.0, 46.0, 22.5, 27.89517504272034, 0.881053607104147, 1.0, 1.0, 65.0, 33922.18501835458], 
current ob forecast is [], 
actual action is [10.5, 65.0], 
sim time this is 2827800.0000, 
sim time next is 2828400.0000, 
raw observation next is [5.333333333333333, 29.33333333333334, 0.0, 0.0, 22.5, 27.84013988181485, 0.905677578578956, 1.0, 1.0, 65.0, 35103.91899830972], 
processed observation next is [1.0, 0.7391304347826086, 0.6103416435826409, 0.2933333333333334, 0.0, 0.0, 0.375, 0.8200116568179041, 0.8018925261929853, 1.0, 1.0, 1.0, 0.16716151903957008], 
reward next is 0.8328, 
noisyNet noise sample is [array([-0.16831312], dtype=float32), -0.295147]. 
=============================================
[2019-04-08 15:49:41,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1576501e-16 4.7574328e-10 5.8052901e-08 6.0554015e-08 3.2038736e-10
 2.7533656e-13 2.6388424e-08 3.8281617e-14 1.8535566e-17 1.3941450e-18
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:41,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3035
[2019-04-08 15:49:41,126] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.5, 64.0, 0.0, 0.0, 22.5, 25.67807196187166, 0.4412882321471263, 1.0, 1.0, 65.0, 82358.4239938328], 
current ob forecast is [], 
actual action is [-1.5, 65.0], 
sim time this is 2791800.0000, 
sim time next is 2792400.0000, 
raw observation next is [-6.333333333333334, 64.0, 18.0, 34.49999999999999, 22.5, 25.73320981583937, 0.4524182980459936, 1.0, 1.0, 65.0, 80813.23653337172], 
processed observation next is [1.0, 0.30434782608695654, 0.28716528162511545, 0.64, 0.06, 0.038121546961325956, 0.375, 0.6444341513199475, 0.6508060993486645, 1.0, 1.0, 1.0, 0.38482493587319866], 
reward next is 0.6152, 
noisyNet noise sample is [array([-0.60842323], dtype=float32), -0.85357124]. 
=============================================
[2019-04-08 15:49:41,143] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0011324e-18 9.8756246e-11 1.6211665e-07 5.3071711e-09 4.1722101e-11
 4.6367943e-15 9.0254554e-10 6.7195230e-16 2.2638176e-19 1.9431430e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:41,147] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9360
[2019-04-08 15:49:41,171] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 44.0, 151.5, 724.0, 22.5, 26.20352689438967, 0.7431908433136681, 1.0, 1.0, 65.0, 55309.52577932377], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2808000.0000, 
sim time next is 2808600.0000, 
raw observation next is [2.333333333333333, 42.50000000000001, 160.3333333333333, 711.0, 22.5, 27.30153718313, 0.8341334459865283, 1.0, 1.0, 65.0, 45732.74084618433], 
processed observation next is [1.0, 0.5217391304347826, 0.5272391505078486, 0.42500000000000004, 0.5344444444444443, 0.7856353591160221, 0.375, 0.7751280985941668, 0.7780444819955094, 1.0, 1.0, 1.0, 0.21777495641040157], 
reward next is 0.7822, 
noisyNet noise sample is [array([-1.2401462], dtype=float32), 0.269452]. 
=============================================
[2019-04-08 15:49:41,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7784563e-16 3.4489303e-10 2.6990395e-07 6.3010525e-08 3.3382813e-10
 1.7867593e-12 8.3547027e-09 3.5803459e-14 8.8647117e-16 4.5415375e-18
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:49:41,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1157
[2019-04-08 15:49:41,251] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.33652549818914, 0.5688901622969578, 0.0, 1.0, 65.0, 51945.61809975237], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 2773200.0000, 
sim time next is 2773800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 19.0, 26.29819386549259, 0.5725492237499413, 0.0, 1.0, 65.0, 51712.72580130806], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.08333333333333333, 0.6915161554577157, 0.6908497412499804, 0.0, 1.0, 1.0, 0.24625107524432407], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.3928594], dtype=float32), 1.1603585]. 
=============================================
[2019-04-08 15:49:41,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8709533e-18 2.1080160e-10 3.5649426e-09 1.7538964e-08 2.3665996e-11
 2.5468405e-13 1.5259348e-08 6.7793633e-15 4.6018947e-19 1.6986334e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:41,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3091
[2019-04-08 15:49:41,307] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 83.66666666666666, 0.0, 0.0, 19.0, 26.61934651113517, 0.6624970367890809, 0.0, 1.0, 65.0, 63530.73356940253], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2860800.0000, 
sim time next is 2861400.0000, 
raw observation next is [1.0, 84.83333333333334, 0.0, 0.0, 19.0, 26.58113535138435, 0.6690763076878811, 0.0, 1.0, 65.0, 63474.44410431787], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8483333333333334, 0.0, 0.0, 0.08333333333333333, 0.7150946126153626, 0.7230254358959604, 0.0, 1.0, 1.0, 0.30225925763960887], 
reward next is 0.6977, 
noisyNet noise sample is [array([-0.5260685], dtype=float32), 0.17839622]. 
=============================================
[2019-04-08 15:49:42,103] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6818236e-17 7.0550021e-10 4.5155593e-06 2.2899383e-07 1.3186358e-10
 2.1754376e-12 4.6615025e-08 4.8559971e-15 2.8198938e-17 3.3443119e-19
 9.9999523e-01], sum to 1.0000
[2019-04-08 15:49:42,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3171
[2019-04-08 15:49:42,127] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 22.5, 25.74047517483605, 0.7068107224022407, 1.0, 1.0, 65.0, 66303.38142806858], 
current ob forecast is [], 
actual action is [1.333333333333333, 65.0], 
sim time this is 2742000.0000, 
sim time next is 2742600.0000, 
raw observation next is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 22.5, 26.94113323466373, 0.791153161363872, 1.0, 1.0, 65.0, 54081.63789076015], 
processed observation next is [1.0, 0.7391304347826086, 0.3564173591874424, 0.5333333333333333, 0.0, 0.0, 0.375, 0.7450944362219776, 0.763717720454624, 1.0, 1.0, 1.0, 0.25753160900361977], 
reward next is 0.7425, 
noisyNet noise sample is [array([-1.9210079], dtype=float32), 0.98105085]. 
=============================================
[2019-04-08 15:49:42,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4350521e-16 4.9157785e-09 1.8320253e-07 3.8273665e-06 2.8291923e-11
 5.8009917e-13 3.8436472e-08 8.8084115e-14 8.0821310e-17 2.3887611e-19
 9.9999595e-01], sum to 1.0000
[2019-04-08 15:49:42,451] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6724
[2019-04-08 15:49:42,483] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.5941608377327, 0.7224910685843228, 1.0, 1.0, 65.0, 67613.30133967393], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 22.5, 26.58689116494812, 0.7206536841766812, 0.0, 1.0, 65.0, 65737.62337559008], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.375, 0.7155742637456767, 0.7402178947255603, 0.0, 1.0, 1.0, 0.31303630178852415], 
reward next is 0.6870, 
noisyNet noise sample is [array([-0.1364493], dtype=float32), -0.79800785]. 
=============================================
[2019-04-08 15:49:42,938] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2019007e-16 2.2509674e-09 3.1961932e-07 1.7459926e-07 1.7762544e-10
 2.2819023e-12 3.9228883e-08 1.5106053e-13 1.3607542e-17 4.1250153e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:42,938] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3445
[2019-04-08 15:49:42,954] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 26.84470092689219, 0.7753457582123681, 0.0, 1.0, 65.0, 54075.36378022667], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 26.85350660460718, 0.7753994660952892, 0.0, 1.0, 65.0, 52993.66220827552], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.7377922170505983, 0.7584664886984297, 0.0, 1.0, 1.0, 0.25235077242035964], 
reward next is 0.7476, 
noisyNet noise sample is [array([1.2780019], dtype=float32), -0.90193874]. 
=============================================
[2019-04-08 15:49:43,210] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0229261e-19 6.8250551e-11 1.6122454e-08 6.3010179e-07 2.5073853e-11
 9.3059597e-15 1.4613132e-10 4.2497358e-15 5.7629764e-20 8.7987226e-21
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:43,210] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4046
[2019-04-08 15:49:43,222] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.833333333333333, 100.0, 171.3333333333333, 0.0, 22.5, 27.4174036656329, 0.8618122410373766, 1.0, 1.0, 65.0, 30243.51688238091], 
current ob forecast is [], 
actual action is [6.833333333333333, 65.0], 
sim time this is 2897400.0000, 
sim time next is 2898000.0000, 
raw observation next is [2.0, 100.0, 169.5, 0.0, 22.5, 27.57759391455201, 0.888439379422187, 1.0, 1.0, 65.0, 31201.00668142676], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 1.0, 0.565, 0.0, 0.375, 0.7981328262126676, 0.7961464598073956, 1.0, 1.0, 1.0, 0.1485762222925084], 
reward next is 0.8514, 
noisyNet noise sample is [array([-2.6806529], dtype=float32), 0.005247882]. 
=============================================
[2019-04-08 15:49:43,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[88.01822 ]
 [87.993835]
 [87.874306]
 [87.70719 ]
 [87.57151 ]], R is [[88.03377533]
 [88.00941467]
 [87.93201447]
 [87.79624939]
 [87.73303223]].
[2019-04-08 15:49:43,435] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2687208e-17 3.9622829e-09 3.2726345e-07 1.0516190e-07 2.1314607e-10
 7.4285847e-13 7.6445534e-09 9.1640325e-14 2.7742817e-17 9.0350166e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:43,436] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1186
[2019-04-08 15:49:43,475] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.666666666666667, 34.66666666666666, 0.0, 0.0, 22.5, 27.21141331821185, 0.8086509833884175, 1.0, 1.0, 65.0, 54347.04150193403], 
current ob forecast is [], 
actual action is [8.666666666666668, 65.0], 
sim time this is 2832000.0000, 
sim time next is 2832600.0000, 
raw observation next is [3.333333333333333, 35.83333333333334, 0.0, 0.0, 22.5, 27.09909756803554, 0.7977488338932069, 1.0, 1.0, 65.0, 56795.28921696867], 
processed observation next is [1.0, 0.782608695652174, 0.5549399815327793, 0.35833333333333345, 0.0, 0.0, 0.375, 0.7582581306696282, 0.7659162779644023, 1.0, 1.0, 1.0, 0.2704537581760413], 
reward next is 0.7295, 
noisyNet noise sample is [array([-2.0832093], dtype=float32), 0.3007748]. 
=============================================
[2019-04-08 15:49:43,576] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4860816e-18 7.6215606e-10 2.4096468e-07 2.4910000e-07 1.4872948e-11
 6.9499473e-14 1.0946448e-09 9.2688325e-15 2.2901887e-18 5.7685827e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:43,577] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6715
[2019-04-08 15:49:43,594] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.166666666666667, 70.33333333333333, 0.0, 0.0, 19.0, 26.66281483309308, 0.684853698234245, 0.0, 1.0, 65.0, 53317.03861032322], 
current ob forecast is [], 
actual action is [6.166666666666667, 65.0], 
sim time this is 2850600.0000, 
sim time next is 2851200.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 19.0, 26.60829105524829, 0.6793117281897242, 0.0, 1.0, 65.0, 62001.96157718453], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7173575879373576, 0.7264372427299081, 0.0, 1.0, 1.0, 0.29524743608183107], 
reward next is 0.7048, 
noisyNet noise sample is [array([0.29311773], dtype=float32), -0.25650245]. 
=============================================
[2019-04-08 15:49:43,671] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.56448555e-17 1.08384704e-07 9.24987773e-07 1.07204414e-06
 1.07948608e-10 1.57264462e-11 1.96872989e-08 2.75359623e-13
 2.63614748e-18 4.27945871e-19 9.99997854e-01], sum to 1.0000
[2019-04-08 15:49:43,675] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4702
[2019-04-08 15:49:43,681] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.0204081e-16 1.2533127e-10 2.0738882e-07 1.4713953e-07 4.3765668e-11
 2.4682085e-13 6.4170870e-08 4.6431930e-14 1.7105486e-17 4.5210859e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:43,688] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.4560
[2019-04-08 15:49:43,691] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 41.0, 169.1666666666667, 698.0, 22.5, 27.53624561720059, 0.8457202259814706, 1.0, 1.0, 65.0, 49464.88499466168], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 2809200.0000, 
sim time next is 2809800.0000, 
raw observation next is [3.0, 39.5, 178.0, 685.0, 22.5, 27.51961614141367, 0.8559841835531001, 1.0, 1.0, 65.0, 49033.20131167072], 
processed observation next is [1.0, 0.5217391304347826, 0.5457063711911359, 0.395, 0.5933333333333334, 0.7569060773480663, 0.375, 0.7933013451178059, 0.7853280611843667, 1.0, 1.0, 1.0, 0.2334914348174796], 
reward next is 0.7665, 
noisyNet noise sample is [array([1.7073255], dtype=float32), -0.40500787]. 
=============================================
[2019-04-08 15:49:43,704] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 59.5, 152.0, 233.0, 22.5, 26.87651713577662, 0.6668146104321964, 1.0, 1.0, 65.0, 48210.64909363699], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 2799000.0000, 
sim time next is 2799600.0000, 
raw observation next is [-4.0, 58.0, 155.6666666666667, 278.8333333333333, 22.5, 26.97254521024401, 0.6869538025737395, 1.0, 1.0, 65.0, 44453.64256516653], 
processed observation next is [1.0, 0.391304347826087, 0.3518005540166205, 0.58, 0.5188888888888891, 0.30810313075506446, 0.375, 0.7477121008536676, 0.7289846008579132, 1.0, 1.0, 1.0, 0.2116840122150787], 
reward next is 0.7883, 
noisyNet noise sample is [array([0.7219833], dtype=float32), 2.236143]. 
=============================================
[2019-04-08 15:49:44,022] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1979161e-17 6.3305701e-11 1.4917582e-08 4.1190649e-08 1.0720191e-11
 8.7879729e-14 3.6955893e-08 5.0099373e-15 1.7369819e-18 3.8287489e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:44,024] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5181
[2019-04-08 15:49:44,042] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 26.84470092321007, 0.7753457603662564, 0.0, 1.0, 65.0, 54075.36364340033], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2838600.0000, 
sim time next is 2839200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 26.85350660107698, 0.7753994682416883, 0.0, 1.0, 65.0, 52993.66207697136], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.7377922167564149, 0.758466489413896, 0.0, 1.0, 1.0, 0.25235077179510174], 
reward next is 0.7476, 
noisyNet noise sample is [array([-1.5295893], dtype=float32), -2.043103]. 
=============================================
[2019-04-08 15:49:44,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0467995e-16 3.2052605e-10 1.4636410e-07 1.0592609e-07 6.9142699e-12
 7.8282243e-14 1.2944947e-09 1.0780169e-14 1.1799540e-19 5.3137416e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:49:44,431] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3713
[2019-04-08 15:49:44,446] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 92.5, 0.0, 0.0, 22.5, 27.16735764801287, 0.8921368587585059, 1.0, 1.0, 65.0, 37281.50304684817], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 2917800.0000, 
sim time next is 2918400.0000, 
raw observation next is [-0.3333333333333333, 92.33333333333333, 0.0, 0.0, 22.5, 27.14591077396457, 0.8884229471894364, 1.0, 1.0, 65.0, 37943.88055175845], 
processed observation next is [1.0, 0.782608695652174, 0.4533702677747, 0.9233333333333333, 0.0, 0.0, 0.375, 0.7621592311637141, 0.7961409823964788, 1.0, 1.0, 1.0, 0.18068514548456405], 
reward next is 0.8193, 
noisyNet noise sample is [array([0.3639378], dtype=float32), 0.8151511]. 
=============================================
[2019-04-08 15:49:44,632] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.0453381e-20 1.6067181e-11 2.8040816e-08 5.1212357e-08 4.4316430e-13
 4.9360112e-15 1.5971706e-10 1.9210464e-15 1.4952251e-19 4.9404175e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:44,634] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8431
[2019-04-08 15:49:44,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.35564235e-17 7.38477446e-10 1.06377179e-07 1.92709066e-07
 2.03641895e-11 3.89615369e-13 8.83941309e-10 4.50627288e-14
 1.23046516e-17 5.64870977e-20 9.99999642e-01], sum to 1.0000
[2019-04-08 15:49:44,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6182
[2019-04-08 15:49:44,646] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 97.66666666666666, 101.6666666666667, 0.0, 22.5, 27.54668291121038, 0.8465927625340782, 1.0, 1.0, 65.0, 36944.13724274125], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2893200.0000, 
sim time next is 2893800.0000, 
raw observation next is [1.0, 98.83333333333334, 116.3333333333333, 0.0, 22.5, 27.57049449125708, 0.852024444653937, 1.0, 1.0, 65.0, 35670.58733659243], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.9883333333333334, 0.38777777777777767, 0.0, 0.375, 0.7975412076047567, 0.7840081482179789, 1.0, 1.0, 1.0, 0.16985993969805918], 
reward next is 0.8301, 
noisyNet noise sample is [array([1.1139616], dtype=float32), -1.2852019]. 
=============================================
[2019-04-08 15:49:44,662] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 72.0, 0.0, 0.0, 19.0, 26.60829105381651, 0.6793117301836918, 0.0, 1.0, 65.0, 62001.96151733497], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2851200.0000, 
sim time next is 2851800.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 19.0, 26.54297294393892, 0.6711737465389193, 0.0, 1.0, 65.0, 65229.60271864167], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.08333333333333333, 0.71191441199491, 0.7237245821796398, 0.0, 1.0, 1.0, 0.31061715580305554], 
reward next is 0.6894, 
noisyNet noise sample is [array([0.03518559], dtype=float32), -0.86190176]. 
=============================================
[2019-04-08 15:49:44,814] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0445684e-19 1.7092712e-13 4.3634479e-08 4.1936521e-09 1.3982886e-11
 2.3180259e-14 9.8680002e-11 4.6234568e-16 3.9922518e-19 1.4803379e-20
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:44,815] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2651
[2019-04-08 15:49:44,843] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 97.66666666666666, 0.0, 0.0, 19.0, 26.67382632216905, 0.6397957113296425, 0.0, 1.0, 65.0, 61376.95163835206], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2871600.0000, 
sim time next is 2872200.0000, 
raw observation next is [1.0, 98.83333333333334, 0.0, 0.0, 19.0, 26.58077210621793, 0.6521583387839226, 0.0, 1.0, 65.0, 65676.77076973874], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9883333333333334, 0.0, 0.0, 0.08333333333333333, 0.7150643421848274, 0.7173861129279743, 0.0, 1.0, 1.0, 0.31274652747494636], 
reward next is 0.6873, 
noisyNet noise sample is [array([0.1154108], dtype=float32), -0.37598762]. 
=============================================
[2019-04-08 15:49:45,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3415605e-17 2.0769579e-10 1.4833119e-08 2.2672701e-08 1.3089786e-11
 1.5592952e-13 2.4942470e-10 4.3469019e-15 3.6871341e-19 1.6244924e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:45,144] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7047
[2019-04-08 15:49:45,174] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 19.0, 26.62031004501554, 0.6654561546323886, 0.0, 1.0, 65.0, 64984.69031751248], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2859600.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 19.0, 26.61333053430981, 0.6645132368450356, 0.0, 1.0, 65.0, 64327.50526254113], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 0.08333333333333333, 0.7177775445258175, 0.7215044122816785, 0.0, 1.0, 1.0, 0.30632145363114827], 
reward next is 0.6937, 
noisyNet noise sample is [array([0.9513675], dtype=float32), -1.1605543]. 
=============================================
[2019-04-08 15:49:45,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7226660e-19 3.2274782e-12 1.1496323e-08 1.9943894e-08 8.4756362e-11
 1.7757920e-13 6.5461734e-09 1.4342819e-15 9.7825009e-20 2.4477123e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:45,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4351
[2019-04-08 15:49:45,465] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 98.83333333333334, 0.0, 0.0, 19.0, 26.58077390366507, 0.6521590010459739, 0.0, 1.0, 65.0, 65676.73985869053], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2872200.0000, 
sim time next is 2872800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 26.61778847693073, 0.6411283477687381, 0.0, 1.0, 65.0, 62787.68803399551], 
processed observation next is [1.0, 0.2608695652173913, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7181490397442275, 0.713709449256246, 0.0, 1.0, 1.0, 0.2989889906380739], 
reward next is 0.7010, 
noisyNet noise sample is [array([1.1518433], dtype=float32), 0.44208017]. 
=============================================
[2019-04-08 15:49:45,507] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.43492814e-19 3.04778121e-12 1.11354375e-08 1.90682385e-08
 8.03028199e-11 1.64790233e-13 6.23235952e-09 1.30883118e-15
 8.69095017e-20 2.14095951e-21 1.00000000e+00], sum to 1.0000
[2019-04-08 15:49:45,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4391
[2019-04-08 15:49:45,529] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 19.0, 26.57310343849121, 0.6343926166434586, 0.0, 1.0, 65.0, 63694.94322154787], 
current ob forecast is [], 
actual action is [6.166666666666667, 65.0], 
sim time this is 2873400.0000, 
sim time next is 2874000.0000, 
raw observation next is [1.333333333333333, 97.66666666666667, 0.0, 0.0, 19.0, 26.52018959376291, 0.631832118416556, 0.0, 1.0, 65.0, 66194.06915084508], 
processed observation next is [1.0, 0.2608695652173913, 0.4995383194829178, 0.9766666666666667, 0.0, 0.0, 0.08333333333333333, 0.7100157994802426, 0.7106107061388519, 0.0, 1.0, 1.0, 0.3152098530992623], 
reward next is 0.6848, 
noisyNet noise sample is [array([1.1518433], dtype=float32), 0.44208017]. 
=============================================
[2019-04-08 15:49:45,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.54991 ]
 [85.444565]
 [85.31438 ]
 [85.20207 ]
 [85.08058 ]], R is [[85.48173523]
 [85.3236084 ]
 [85.17138672]
 [85.00692749]
 [84.86458588]].
[2019-04-08 15:49:45,668] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5387241e-16 6.6532624e-10 3.0913930e-07 2.6217876e-07 2.5066310e-10
 1.2256621e-11 4.2587175e-08 6.4540990e-14 6.1220450e-16 6.5473972e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:45,669] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8793
[2019-04-08 15:49:45,685] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 26.86123392471887, 0.7688501340448307, 0.0, 1.0, 65.0, 46247.99938971373], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2844000.0000, 
sim time next is 2844600.0000, 
raw observation next is [2.0, 47.0, 0.0, 0.0, 19.0, 26.85859698638062, 0.7265163445468029, 0.0, 1.0, 65.0, 46957.07550758257], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.47, 0.0, 0.0, 0.08333333333333333, 0.7382164155317182, 0.7421721148489343, 0.0, 1.0, 1.0, 0.22360512146467892], 
reward next is 0.7764, 
noisyNet noise sample is [array([-0.46942893], dtype=float32), -1.8486744]. 
=============================================
[2019-04-08 15:49:45,827] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.2854206e-17 5.1732756e-09 2.8453533e-06 1.0200263e-06 1.2657837e-10
 4.1521855e-12 1.3353178e-07 8.4119537e-15 5.3566930e-18 1.4692834e-19
 9.9999595e-01], sum to 1.0000
[2019-04-08 15:49:45,830] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.5625
[2019-04-08 15:49:45,848] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 39.33333333333334, 0.0, 0.0, 22.5, 26.98575567190329, 0.7832970954985274, 0.0, 1.0, 65.0, 56582.62730880309], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 2834400.0000, 
sim time next is 2835000.0000, 
raw observation next is [2.5, 40.5, 0.0, 0.0, 22.5, 26.93615428940417, 0.7744279213156261, 1.0, 1.0, 65.0, 57449.00745935646], 
processed observation next is [1.0, 0.8260869565217391, 0.5318559556786704, 0.405, 0.0, 0.0, 0.375, 0.744679524117014, 0.7581426404385421, 1.0, 1.0, 1.0, 0.2735667021874117], 
reward next is 0.7264, 
noisyNet noise sample is [array([-0.8414694], dtype=float32), 0.7805978]. 
=============================================
[2019-04-08 15:49:45,867] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[77.44902 ]
 [77.791985]
 [77.74942 ]
 [77.69311 ]
 [77.65264 ]], R is [[77.6530304 ]
 [77.60706329]
 [77.55283356]
 [77.5019455 ]
 [77.4564743 ]].
[2019-04-08 15:49:46,345] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.1673283e-18 6.5641667e-12 5.5245902e-08 3.2278230e-08 1.5597280e-11
 4.4988366e-14 3.1887801e-09 1.9112445e-15 1.2569037e-18 4.4999881e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:46,345] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5602
[2019-04-08 15:49:46,362] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 26.53570223427776, 0.647185949129079, 0.0, 1.0, 65.0, 65442.29953227124], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2869200.0000, 
sim time next is 2869800.0000, 
raw observation next is [1.0, 94.16666666666666, 0.0, 0.0, 19.0, 26.55128811630376, 0.6406075369401141, 0.0, 1.0, 65.0, 64468.88925237473], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9416666666666665, 0.0, 0.0, 0.08333333333333333, 0.7126073430253133, 0.7135358456467046, 0.0, 1.0, 1.0, 0.3069947107255939], 
reward next is 0.6930, 
noisyNet noise sample is [array([-0.29380548], dtype=float32), -0.22140017]. 
=============================================
[2019-04-08 15:49:46,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6406449e-17 2.5266522e-11 3.0606304e-09 7.0742580e-08 3.2833968e-12
 3.7293698e-13 2.7906772e-09 2.5040647e-15 8.1821051e-19 5.9330434e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:46,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9661
[2019-04-08 15:49:46,508] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 97.66666666666666, 0.0, 0.0, 19.0, 26.67382773085932, 0.6397962057444384, 0.0, 1.0, 65.0, 61376.92751501592], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2871600.0000, 
sim time next is 2872200.0000, 
raw observation next is [1.0, 98.83333333333334, 0.0, 0.0, 19.0, 26.58077351192493, 0.6521588281005447, 0.0, 1.0, 65.0, 65676.74683531898], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.9883333333333334, 0.0, 0.0, 0.08333333333333333, 0.7150644593270776, 0.7173862760335149, 0.0, 1.0, 1.0, 0.31274641350151894], 
reward next is 0.6873, 
noisyNet noise sample is [array([-2.9439044], dtype=float32), 0.40524498]. 
=============================================
[2019-04-08 15:49:46,774] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1229770e-17 5.9472577e-10 2.3652519e-06 2.3242680e-07 2.1105822e-09
 1.4037549e-11 1.2826148e-07 5.7329968e-14 3.5202805e-18 2.0450575e-20
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:49:46,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3584
[2019-04-08 15:49:46,792] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 26.55730322055948, 0.6435704872111869, 0.0, 1.0, 65.0, 61177.61221891735], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2876400.0000, 
sim time next is 2877000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 22.5, 26.54796375490422, 0.6479885272193618, 0.0, 1.0, 65.0, 60853.59293051606], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.375, 0.712330312908685, 0.7159961757397872, 0.0, 1.0, 1.0, 0.2897790139548384], 
reward next is 0.7102, 
noisyNet noise sample is [array([0.887941], dtype=float32), 0.8286202]. 
=============================================
[2019-04-08 15:49:46,801] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[84.383675]
 [84.363014]
 [84.33202 ]
 [84.253426]
 [84.194786]], R is [[84.51182556]
 [84.3753891 ]
 [84.24364471]
 [84.09955597]
 [83.94210052]].
[2019-04-08 15:49:46,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.25608815e-17 2.74993972e-10 5.75988750e-08 1.09053519e-06
 5.56566529e-11 6.44777417e-13 2.86920692e-08 1.10955319e-14
 4.25237051e-18 3.54818995e-20 9.99998808e-01], sum to 1.0000
[2019-04-08 15:49:46,842] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1209
[2019-04-08 15:49:46,866] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 26.49429482397871, 0.635118682869651, 0.0, 1.0, 65.0, 66083.92576770451], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2867400.0000, 
sim time next is 2868000.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 26.4552618828206, 0.6488354113606472, 0.0, 1.0, 65.0, 67158.90399506563], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7046051569017168, 0.716278470453549, 0.0, 1.0, 1.0, 0.31980430473840776], 
reward next is 0.6802, 
noisyNet noise sample is [array([-0.40648356], dtype=float32), 1.8795617]. 
=============================================
[2019-04-08 15:49:46,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.473045]
 [80.3528  ]
 [80.17774 ]
 [80.02094 ]
 [79.8592  ]], R is [[80.49531555]
 [80.37567902]
 [80.26392365]
 [80.15048218]
 [80.03142548]].
[2019-04-08 15:49:47,034] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4771290e-19 3.7957117e-11 7.3049020e-09 2.0404546e-08 3.8044418e-13
 7.8765043e-16 3.9053644e-10 1.6719775e-16 1.6112075e-19 4.4519075e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:47,036] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5659
[2019-04-08 15:49:47,058] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 19.0, 26.80373797003107, 0.8358483882675749, 0.0, 1.0, 65.0, 44760.13808320933], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 2925000.0000, 
sim time next is 2925600.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 19.0, 26.78508849731734, 0.8408619459934136, 0.0, 1.0, 65.0, 44892.12171841846], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.08333333333333333, 0.7320907081097783, 0.7802873153311379, 0.0, 1.0, 1.0, 0.21377200818294503], 
reward next is 0.7862, 
noisyNet noise sample is [array([0.232664], dtype=float32), -0.35840362]. 
=============================================
[2019-04-08 15:49:47,769] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.0834719e-17 4.1651734e-11 2.7199404e-08 2.1623386e-08 1.6498138e-10
 1.1764857e-13 1.6278474e-08 7.9534657e-16 5.0167925e-19 5.7843181e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:47,771] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.1647
[2019-04-08 15:49:47,799] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 19.0, 26.48104643084479, 0.6479285524840149, 0.0, 1.0, 65.0, 66685.79181945589], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2865600.0000, 
sim time next is 2866200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 19.0, 26.51183859968938, 0.6458696428345295, 0.0, 1.0, 65.0, 65269.78978045567], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7093198833074483, 0.7152898809448431, 0.0, 1.0, 1.0, 0.3108085227640746], 
reward next is 0.6892, 
noisyNet noise sample is [array([-1.1878737], dtype=float32), -0.3102523]. 
=============================================
[2019-04-08 15:49:47,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.3408788e-17 8.7805284e-11 4.1483464e-08 3.5464203e-07 8.5186629e-11
 1.8370819e-12 3.5592476e-09 9.6684788e-15 4.9142486e-17 7.8433209e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:49:47,958] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0865
[2019-04-08 15:49:47,978] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.31602856652252, 0.6663602130993597, 0.0, 1.0, 65.0, 52743.330400251], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2953200.0000, 
sim time next is 2953800.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.30627285096859, 0.6630067340394533, 0.0, 1.0, 65.0, 52925.35442660264], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6921894042473825, 0.7210022446798178, 0.0, 1.0, 1.0, 0.2520254972695364], 
reward next is 0.7480, 
noisyNet noise sample is [array([-0.02064502], dtype=float32), -1.7031745]. 
=============================================
[2019-04-08 15:49:48,518] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5667099e-16 6.8118029e-12 6.5588864e-08 8.7271488e-08 1.4673488e-11
 8.8775699e-13 1.6897572e-09 3.5542736e-14 1.0168438e-17 1.4750616e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:48,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4094
[2019-04-08 15:49:48,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0313211e-18 6.5398124e-11 1.9912575e-07 1.7507266e-07 5.1767215e-12
 5.3824528e-13 2.9359384e-09 6.3492387e-15 1.5200413e-18 9.5401151e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:49:48,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6242
[2019-04-08 15:49:48,545] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 83.83333333333334, 0.0, 0.0, 19.0, 26.8229663429837, 0.8350705287242363, 0.0, 1.0, 65.0, 43487.31495266781], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 2926200.0000, 
sim time next is 2926800.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 19.0, 26.80918932564602, 0.830589730288168, 0.0, 1.0, 65.0, 43619.56560309158], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7340991104705017, 0.7768632434293893, 0.0, 1.0, 1.0, 0.20771221715757895], 
reward next is 0.7923, 
noisyNet noise sample is [array([-0.27550057], dtype=float32), 0.052440535]. 
=============================================
[2019-04-08 15:49:48,547] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 19.0, 26.67401146603364, 0.6662388457115104, 0.0, 1.0, 65.0, 62650.58744908514], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 2858400.0000, 
sim time next is 2859000.0000, 
raw observation next is [1.0, 80.16666666666667, 0.0, 0.0, 19.0, 26.62780109004985, 0.6659058595097421, 0.0, 1.0, 65.0, 64410.20253710156], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8016666666666667, 0.0, 0.0, 0.08333333333333333, 0.7189834241708208, 0.7219686198365807, 0.0, 1.0, 1.0, 0.3067152501766741], 
reward next is 0.6933, 
noisyNet noise sample is [array([0.31563094], dtype=float32), 0.13337128]. 
=============================================
[2019-04-08 15:49:48,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.32367 ]
 [78.99112 ]
 [78.800735]
 [78.594574]
 [78.1568  ]], R is [[79.52389526]
 [79.43032074]
 [79.3341217 ]
 [79.24520874]
 [79.1778717 ]].
[2019-04-08 15:49:48,582] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.9848357e-16 7.1008621e-10 3.0858425e-07 4.6310461e-07 1.2518171e-09
 3.8034901e-12 1.2477327e-07 5.3537611e-13 1.7203506e-17 2.8635209e-20
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:49:48,586] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7951
[2019-04-08 15:49:48,603] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 119.0, 90.5, 19.0, 26.15118245556103, 0.6301002108014907, 0.0, 1.0, 65.0, 65453.42777062808], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 2970000.0000, 
sim time next is 2970600.0000, 
raw observation next is [-4.0, 71.0, 130.6666666666667, 104.3333333333333, 19.0, 26.16169588612818, 0.6345944912171807, 0.0, 1.0, 65.0, 64184.6883197327], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.4355555555555557, 0.11528545119705337, 0.08333333333333333, 0.6801413238440149, 0.7115314970723935, 0.0, 1.0, 1.0, 0.3056413729511081], 
reward next is 0.6944, 
noisyNet noise sample is [array([0.41229716], dtype=float32), -1.1963372]. 
=============================================
[2019-04-08 15:49:49,315] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1764711e-16 6.4536743e-10 2.5428398e-07 3.3190558e-06 2.2732484e-10
 2.9485022e-11 2.3104720e-08 2.0252701e-13 1.7884275e-16 4.7998954e-19
 9.9999642e-01], sum to 1.0000
[2019-04-08 15:49:49,326] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1337
[2019-04-08 15:49:49,356] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.666666666666667, 69.0, 174.0, 42.0, 19.0, 26.24341464254758, 0.6548493680389558, 0.0, 1.0, 65.0, 57637.79393078384], 
current ob forecast is [], 
actual action is [1.333333333333333, 65.0], 
sim time this is 2974800.0000, 
sim time next is 2975400.0000, 
raw observation next is [-3.5, 68.0, 178.0, 24.0, 19.0, 26.25358135668878, 0.6546259706814737, 0.0, 1.0, 65.0, 57034.3288605195], 
processed observation next is [0.0, 0.43478260869565216, 0.36565096952908593, 0.68, 0.5933333333333334, 0.026519337016574586, 0.08333333333333333, 0.6877984463907317, 0.7182086568938245, 0.0, 1.0, 1.0, 0.27159204219294997], 
reward next is 0.7284, 
noisyNet noise sample is [array([-0.26199302], dtype=float32), 0.6547209]. 
=============================================
[2019-04-08 15:49:49,405] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.3263620e-17 7.2710865e-11 4.0871807e-08 1.3650039e-07 1.1297068e-10
 3.7613947e-13 2.2536367e-08 1.6692775e-13 4.7312154e-18 1.4652709e-18
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:49:49,408] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6943
[2019-04-08 15:49:49,419] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9255676e-16 1.5401527e-09 7.8498353e-08 4.6384028e-07 9.3144015e-10
 5.8862897e-12 9.9164815e-08 9.7547870e-13 1.8059343e-17 1.9896524e-17
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:49,421] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4384
[2019-04-08 15:49:49,437] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.56571407659087, 0.7432756955557461, 0.0, 1.0, 65.0, 49502.84575661366], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 2937000.0000, 
sim time next is 2937600.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 19.0, 26.54599923449054, 0.7360164874445313, 0.0, 1.0, 65.0, 49867.02577894091], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.08333333333333333, 0.7121666028742117, 0.7453388291481771, 0.0, 1.0, 1.0, 0.23746202751876622], 
reward next is 0.7625, 
noisyNet noise sample is [array([-0.77967364], dtype=float32), 0.13473247]. 
=============================================
[2019-04-08 15:49:49,458] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.666666666666667, 63.33333333333334, 121.8333333333333, 781.8333333333334, 19.0, 26.69278710127258, 0.7951572009675324, 0.0, 1.0, 65.0, 43064.88030736103], 
current ob forecast is [], 
actual action is [2.333333333333333, 65.0], 
sim time this is 2985600.0000, 
sim time next is 2986200.0000, 
raw observation next is [-2.5, 62.5, 110.0, 800.0, 19.0, 26.73314142854496, 0.7985270495799394, 0.0, 1.0, 65.0, 41915.30595023301], 
processed observation next is [0.0, 0.5652173913043478, 0.39335180055401664, 0.625, 0.36666666666666664, 0.8839779005524862, 0.08333333333333333, 0.7277617857120801, 0.7661756831933131, 0.0, 1.0, 1.0, 0.19959669500110955], 
reward next is 0.8004, 
noisyNet noise sample is [array([1.7696031], dtype=float32), 0.7693623]. 
=============================================
[2019-04-08 15:49:49,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1259809e-17 8.3751027e-11 7.2305602e-09 9.8375281e-09 7.6246578e-11
 1.4156956e-13 2.0499378e-08 1.3188532e-14 8.6993422e-20 1.1814316e-19
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:49,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0076
[2019-04-08 15:49:49,696] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 17.5, 48.49999999999999, 22.5, 26.53136138415685, 0.6566698721280205, 1.0, 1.0, 65.0, 57857.75662447941], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 2878800.0000, 
sim time next is 2879400.0000, 
raw observation next is [2.0, 93.0, 34.99999999999999, 69.99999999999999, 22.5, 26.56506786823746, 0.6577896627971652, 1.0, 1.0, 65.0, 57867.7520144033], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.11666666666666664, 0.07734806629834252, 0.375, 0.7137556556864549, 0.7192632209323885, 1.0, 1.0, 1.0, 0.2755607238781109], 
reward next is 0.7244, 
noisyNet noise sample is [array([0.5375698], dtype=float32), 0.067728646]. 
=============================================
[2019-04-08 15:49:50,004] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9895252e-16 4.6521290e-10 3.0895950e-07 2.1516500e-07 7.4888584e-10
 8.6047770e-13 1.2964223e-08 3.8984996e-14 1.1646919e-17 8.9663470e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:50,004] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5625
[2019-04-08 15:49:50,032] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.01006050738127, 0.5907858609323631, 0.0, 1.0, 65.0, 77999.8680541896], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 2963400.0000, 
sim time next is 2964000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 25.99513214089808, 0.5865882889780466, 0.0, 1.0, 65.0, 77906.9438015003], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6662610117415065, 0.6955294296593489, 0.0, 1.0, 1.0, 0.37098544667381095], 
reward next is 0.6290, 
noisyNet noise sample is [array([0.7292675], dtype=float32), -0.34341165]. 
=============================================
[2019-04-08 15:49:50,048] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[73.706985]
 [73.718704]
 [73.73128 ]
 [73.75988 ]
 [73.78407 ]], R is [[73.60193634]
 [73.49449158]
 [73.38790131]
 [73.28308105]
 [73.18305206]].
[2019-04-08 15:49:50,256] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8316503e-16 1.8990567e-09 5.8960984e-07 7.2656104e-07 5.5827531e-10
 1.8606229e-11 2.6318812e-09 3.7657818e-14 6.6536020e-17 1.6931146e-19
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:49:50,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0682
[2019-04-08 15:49:50,285] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.29973301331213, 0.6705747185606435, 0.0, 1.0, 65.0, 53379.87018873238], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2952600.0000, 
sim time next is 2953200.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.31602902629897, 0.6663603567607651, 0.0, 1.0, 65.0, 52743.32464390153], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6930024188582475, 0.722120118920255, 0.0, 1.0, 1.0, 0.2511586887804835], 
reward next is 0.7488, 
noisyNet noise sample is [array([-1.0212644], dtype=float32), -2.449804]. 
=============================================
[2019-04-08 15:49:50,749] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8700476e-16 6.5595231e-11 3.6625778e-08 5.6057218e-07 3.2492520e-10
 5.4681720e-13 9.4558580e-08 1.6445540e-13 9.9135413e-17 1.5142236e-19
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:49:50,755] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5361
[2019-04-08 15:49:50,781] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.29276161327153, 0.6610603918975805, 0.0, 1.0, 65.0, 53525.16232237098], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 2954400.0000, 
sim time next is 2955000.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 19.0, 26.28076819085067, 0.6588660686221379, 0.0, 1.0, 65.0, 53588.21491942146], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.08333333333333333, 0.6900640159042224, 0.719622022874046, 0.0, 1.0, 1.0, 0.2551819758067688], 
reward next is 0.7448, 
noisyNet noise sample is [array([0.33169454], dtype=float32), 1.5809639]. 
=============================================
[2019-04-08 15:49:50,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.7802  ]
 [73.817924]
 [73.886345]
 [73.910515]
 [73.95646 ]], R is [[73.73422241]
 [73.74199677]
 [73.75255585]
 [73.76387024]
 [73.77204132]].
[2019-04-08 15:49:51,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2549098e-15 6.3378160e-09 5.6688691e-07 2.9525729e-06 1.7227608e-08
 1.9372438e-12 2.4769694e-08 2.5639261e-12 3.8316055e-17 2.5101730e-18
 9.9999642e-01], sum to 1.0000
[2019-04-08 15:49:51,735] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1338
[2019-04-08 15:49:51,752] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 62.5, 110.0, 800.0, 19.0, 26.73314157158434, 0.7985270989061597, 0.0, 1.0, 65.0, 41915.30421321958], 
current ob forecast is [], 
actual action is [2.5, 65.0], 
sim time this is 2986200.0000, 
sim time next is 2986800.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 108.5, 791.8333333333334, 19.0, 26.75300423953562, 0.8035999043169756, 0.0, 1.0, 65.0, 41586.74231183127], 
processed observation next is [0.0, 0.5652173913043478, 0.3979686057248385, 0.6166666666666667, 0.3616666666666667, 0.8749539594843463, 0.08333333333333333, 0.7294170199613017, 0.7678666347723252, 0.0, 1.0, 1.0, 0.19803210624681558], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.17616852], dtype=float32), 0.23453514]. 
=============================================
[2019-04-08 15:49:51,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7565907e-15 2.6076240e-08 1.7912497e-07 2.5061688e-06 2.8695066e-09
 3.2111796e-11 2.1341231e-07 2.1045557e-13 6.6452747e-16 4.5740383e-18
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:49:51,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0201
[2019-04-08 15:49:51,962] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.833333333333333, 76.0, 0.0, 0.0, 19.0, 26.24545749632233, 0.5724015714751437, 0.0, 1.0, 65.0, 50133.98734645932], 
current ob forecast is [], 
actual action is [-0.833333333333333, 65.0], 
sim time this is 3034200.0000, 
sim time next is 3034800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.22950863513595, 0.5686337861955968, 0.0, 1.0, 65.0, 50492.18586525026], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6857923862613292, 0.6895445953985323, 0.0, 1.0, 1.0, 0.24043898031071553], 
reward next is 0.7596, 
noisyNet noise sample is [array([0.24408202], dtype=float32), 0.17361943]. 
=============================================
[2019-04-08 15:49:52,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1933136e-15 3.5896225e-10 4.8718405e-08 9.9310171e-07 8.3988683e-10
 5.2398971e-12 1.1102049e-07 5.4747965e-13 1.7955684e-16 1.5785978e-17
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:49:52,374] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5011
[2019-04-08 15:49:52,386] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.21407960062768, 0.569874825765368, 0.0, 1.0, 65.0, 50848.03339601376], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3035400.0000, 
sim time next is 3036000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.23147331071517, 0.5650587824967839, 0.0, 1.0, 65.0, 50269.3968939923], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6859561092262642, 0.688352927498928, 0.0, 1.0, 1.0, 0.23937808044758238], 
reward next is 0.7606, 
noisyNet noise sample is [array([-2.3017857], dtype=float32), -0.329805]. 
=============================================
[2019-04-08 15:49:52,398] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[71.03888]
 [70.97225]
 [71.00248]
 [70.9039 ]
 [70.9118 ]], R is [[71.12760925]
 [71.17420197]
 [71.22202301]
 [71.27107239]
 [71.32106781]].
[2019-04-08 15:49:52,742] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6557613e-16 1.4826516e-09 1.2189059e-06 3.3673095e-07 3.8547729e-10
 5.4734434e-13 6.9617408e-07 2.1047498e-13 2.2608548e-17 1.8224844e-18
 9.9999774e-01], sum to 1.0000
[2019-04-08 15:49:52,742] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1929
[2019-04-08 15:49:52,755] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.13129155117659, 0.5179224568271835, 0.0, 1.0, 65.0, 51218.76961246778], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3049200.0000, 
sim time next is 3049800.0000, 
raw observation next is [-6.0, 74.83333333333334, 0.0, 0.0, 19.0, 26.10448305980643, 0.5161353279838713, 0.0, 1.0, 65.0, 52695.43783197975], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.7483333333333334, 0.0, 0.0, 0.08333333333333333, 0.6753735883172025, 0.6720451093279571, 0.0, 1.0, 1.0, 0.2509306563427607], 
reward next is 0.7491, 
noisyNet noise sample is [array([-0.09982605], dtype=float32), -0.73400426]. 
=============================================
[2019-04-08 15:49:53,001] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0554996e-15 1.0081281e-10 7.1950410e-08 1.5239804e-07 8.3648877e-10
 4.6031391e-12 2.9765715e-08 4.9719213e-13 1.0095421e-18 1.0162033e-18
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:49:53,002] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5272
[2019-04-08 15:49:53,023] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 72.33333333333333, 0.0, 0.0, 19.0, 26.12409888838463, 0.5310374153940138, 0.0, 1.0, 65.0, 51707.05813615878], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3044400.0000, 
sim time next is 3045000.0000, 
raw observation next is [-6.0, 71.16666666666667, 0.0, 0.0, 19.0, 26.11085690430277, 0.5290287239689626, 0.0, 1.0, 65.0, 52324.76867613913], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7116666666666667, 0.0, 0.0, 0.08333333333333333, 0.675904742025231, 0.6763429079896541, 0.0, 1.0, 1.0, 0.24916556512447205], 
reward next is 0.7508, 
noisyNet noise sample is [array([0.9198526], dtype=float32), 0.9503126]. 
=============================================
[2019-04-08 15:49:53,034] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.4292 ]
 [72.45936]
 [72.50415]
 [72.49748]
 [72.49931]], R is [[72.39141083]
 [72.42127228]
 [72.45153046]
 [72.47820282]
 [72.50469971]].
[2019-04-08 15:49:53,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5943129e-15 4.7719739e-10 1.8304845e-06 1.9031849e-06 3.3548464e-10
 7.6623985e-12 9.1563770e-08 3.9695910e-13 3.9649543e-16 3.0163749e-18
 9.9999619e-01], sum to 1.0000
[2019-04-08 15:49:53,930] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5709
[2019-04-08 15:49:53,955] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.13129119629649, 0.5179223556270475, 0.0, 1.0, 65.0, 51218.77404402903], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3049200.0000, 
sim time next is 3049800.0000, 
raw observation next is [-6.0, 74.83333333333334, 0.0, 0.0, 19.0, 26.10448271191716, 0.5161352301528075, 0.0, 1.0, 65.0, 52695.44195810796], 
processed observation next is [0.0, 0.30434782608695654, 0.296398891966759, 0.7483333333333334, 0.0, 0.0, 0.08333333333333333, 0.6753735593264301, 0.6720450767176024, 0.0, 1.0, 1.0, 0.2509306759909903], 
reward next is 0.7491, 
noisyNet noise sample is [array([-1.2258223], dtype=float32), 1.0804253]. 
=============================================
[2019-04-08 15:49:54,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [4.7445022e-17 6.6223568e-11 3.9730391e-08 1.5076326e-07 1.8349816e-11
 8.6506095e-13 1.8800870e-09 7.5119632e-14 3.8537799e-18 5.9888930e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:49:54,003] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9795
[2019-04-08 15:49:54,021] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.666666666666667, 69.0, 174.0, 42.0, 19.0, 26.24341541954995, 0.6548496166802665, 0.0, 1.0, 65.0, 57637.78365768752], 
current ob forecast is [], 
actual action is [1.333333333333333, 65.0], 
sim time this is 2974800.0000, 
sim time next is 2975400.0000, 
raw observation next is [-3.5, 68.0, 178.0, 24.0, 19.0, 26.25358212716124, 0.6546262179487008, 0.0, 1.0, 65.0, 57034.31873526864], 
processed observation next is [0.0, 0.43478260869565216, 0.36565096952908593, 0.68, 0.5933333333333334, 0.026519337016574586, 0.08333333333333333, 0.6877985105967701, 0.7182087393162336, 0.0, 1.0, 1.0, 0.2715919939774697], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.50585693], dtype=float32), -1.0161289]. 
=============================================
[2019-04-08 15:49:54,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0329564e-16 1.4009641e-10 2.8658899e-08 5.3409380e-08 9.6674675e-11
 1.8099286e-13 3.2602763e-08 1.4094018e-13 1.5231826e-17 1.5512579e-18
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:54,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8935
[2019-04-08 15:49:54,722] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.21841697973733, 0.5608652111845319, 0.0, 1.0, 65.0, 50590.26005602856], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3036600.0000, 
sim time next is 3037200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.20053897883396, 0.5582617141804311, 0.0, 1.0, 65.0, 51430.64563132045], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.6833782482361634, 0.6860872380601437, 0.0, 1.0, 1.0, 0.24490783633962118], 
reward next is 0.7551, 
noisyNet noise sample is [array([0.22563195], dtype=float32), 0.89453864]. 
=============================================
[2019-04-08 15:49:54,905] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.9666186e-15 1.8688686e-09 8.7458693e-07 6.3255283e-07 2.4029176e-09
 2.9838428e-12 1.2806996e-07 5.1879508e-13 7.8685030e-16 2.9080275e-18
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:49:54,906] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6985
[2019-04-08 15:49:54,923] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 66.16666666666667, 28.33333333333333, 166.3333333333333, 19.0, 26.03792238427243, 0.5261199991519547, 0.0, 1.0, 65.0, 53033.379332901], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [-6.0, 64.0, 42.0, 214.5, 19.0, 26.04197695972297, 0.5392267665674771, 0.0, 1.0, 65.0, 52358.43595440391], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.14, 0.23701657458563535, 0.08333333333333333, 0.6701647466435808, 0.6797422555224925, 0.0, 1.0, 1.0, 0.24932588549716148], 
reward next is 0.7507, 
noisyNet noise sample is [array([-0.25880858], dtype=float32), -0.27328172]. 
=============================================
[2019-04-08 15:49:55,524] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4345649e-17 2.1329755e-10 3.1738554e-07 1.0997968e-06 1.3426028e-10
 4.2356500e-12 6.1738007e-08 9.1368709e-13 4.1476468e-15 1.3186139e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:49:55,524] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4131
[2019-04-08 15:49:55,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.06080036e-17 2.40582598e-09 5.47483296e-07 7.69599296e-07
 1.72360355e-11 1.05829180e-12 9.27760802e-09 5.75091407e-13
 1.49973189e-17 1.01858846e-19 9.99998689e-01], sum to 1.0000
[2019-04-08 15:49:55,530] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1149
[2019-04-08 15:49:55,541] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 39.83333333333334, 75.0, 610.6666666666667, 19.0, 27.25216581300604, 0.8629323396095246, 0.0, 1.0, 65.0, 30434.02269321471], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 3081000.0000, 
sim time next is 3081600.0000, 
raw observation next is [1.0, 40.0, 70.5, 579.5, 19.0, 27.27257598890511, 0.862859371722906, 0.0, 1.0, 65.0, 30601.47376668931], 
processed observation next is [0.0, 0.6956521739130435, 0.4903047091412743, 0.4, 0.235, 0.6403314917127072, 0.08333333333333333, 0.7727146657420926, 0.787619790574302, 0.0, 1.0, 1.0, 0.14572130365090147], 
reward next is 0.8543, 
noisyNet noise sample is [array([1.8819274], dtype=float32), 0.4701483]. 
=============================================
[2019-04-08 15:49:55,544] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.333333333333333, 54.66666666666667, 111.5, 807.0, 19.0, 26.6640715197253, 0.7212854072769176, 0.0, 1.0, 65.0, 37293.97346638671], 
current ob forecast is [], 
actual action is [1.666666666666667, 65.0], 
sim time this is 3066000.0000, 
sim time next is 3066600.0000, 
raw observation next is [-3.166666666666667, 54.83333333333334, 112.0, 809.0, 19.0, 26.66728626541966, 0.7253223240660539, 0.0, 1.0, 65.0, 36829.63080950013], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.5483333333333335, 0.37333333333333335, 0.8939226519337017, 0.08333333333333333, 0.7222738554516382, 0.7417741080220179, 0.0, 1.0, 1.0, 0.175379194330953], 
reward next is 0.8246, 
noisyNet noise sample is [array([1.3246465], dtype=float32), 1.0563904]. 
=============================================
[2019-04-08 15:49:55,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2257460e-15 2.4059696e-09 2.8618041e-07 1.9068425e-06 5.3583289e-09
 7.3604378e-12 4.3402389e-07 7.8692469e-12 3.5851334e-15 1.1826737e-18
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:49:55,810] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4205
[2019-04-08 15:49:55,829] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 54.0, 102.5, 697.0, 19.0, 26.41256594606214, 0.6503547669281619, 0.0, 1.0, 65.0, 41220.61900588607], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3060000.0000, 
sim time next is 3060600.0000, 
raw observation next is [-4.0, 54.0, 103.6666666666667, 717.6666666666667, 19.0, 26.44235647123413, 0.6624814697717789, 0.0, 1.0, 65.0, 39735.41669708495], 
processed observation next is [0.0, 0.43478260869565216, 0.3518005540166205, 0.54, 0.34555555555555567, 0.7930018416206263, 0.08333333333333333, 0.7035297059361776, 0.7208271565905929, 0.0, 1.0, 1.0, 0.1892162699861188], 
reward next is 0.8108, 
noisyNet noise sample is [array([1.390316], dtype=float32), -2.2079105]. 
=============================================
[2019-04-08 15:49:56,280] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0107485e-15 5.4005855e-10 4.9486613e-07 8.0983648e-07 2.4292961e-09
 8.8212678e-13 6.4141190e-08 3.3413365e-13 1.6708840e-17 1.3635575e-19
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:49:56,287] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8022
[2019-04-08 15:49:56,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.27432851e-16 1.21027507e-10 4.30739036e-08 2.89807218e-07
 1.65339598e-09 8.72595887e-13 2.46783238e-09 7.16032272e-15
 1.22507905e-17 7.66300443e-21 9.99999642e-01], sum to 1.0000
[2019-04-08 15:49:56,298] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8332
[2019-04-08 15:49:56,322] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 87.0, 0.0, 0.0, 19.0, 26.97683762196673, 0.7635119486777499, 0.0, 1.0, 65.0, 41670.01492873847], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 3090600.0000, 
sim time next is 3091200.0000, 
raw observation next is [-0.8666666666666667, 88.66666666666666, 0.0, 0.0, 19.0, 26.97621115483847, 0.7609520420553814, 0.0, 1.0, 65.0, 42062.24330054046], 
processed observation next is [0.0, 0.782608695652174, 0.4385964912280702, 0.8866666666666666, 0.0, 0.0, 0.08333333333333333, 0.7480175962365392, 0.7536506806851272, 0.0, 1.0, 1.0, 0.2002963966692403], 
reward next is 0.7997, 
noisyNet noise sample is [array([1.0612588], dtype=float32), -1.272449]. 
=============================================
[2019-04-08 15:49:56,342] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 45.33333333333334, 66.0, 548.3333333333334, 19.0, 27.27768500680044, 0.8609282564818547, 0.0, 1.0, 65.0, 31703.69695807061], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 3082200.0000, 
sim time next is 3082800.0000, 
raw observation next is [0.6666666666666667, 50.66666666666667, 61.5, 517.1666666666666, 19.0, 27.27532082335444, 0.8580606249697734, 0.0, 1.0, 65.0, 32075.3620216305], 
processed observation next is [0.0, 0.6956521739130435, 0.4810710987996307, 0.5066666666666667, 0.205, 0.5714548802946593, 0.08333333333333333, 0.7729434019462035, 0.7860202083232578, 0.0, 1.0, 1.0, 0.15273981915062143], 
reward next is 0.8473, 
noisyNet noise sample is [array([-0.93840903], dtype=float32), -1.9943507]. 
=============================================
[2019-04-08 15:49:57,175] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3977072e-17 9.5758923e-10 2.6517876e-08 6.3885420e-07 4.6182766e-10
 2.5369739e-12 2.4692486e-09 1.9082511e-13 6.2435940e-18 4.7797527e-20
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:49:57,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5935
[2019-04-08 15:49:57,192] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.4, 78.66666666666667, 0.0, 0.0, 19.0, 27.10734055888336, 0.7883879852132624, 0.0, 1.0, 65.0, 38294.39693305299], 
current ob forecast is [], 
actual action is [4.6, 65.0], 
sim time this is 3087600.0000, 
sim time next is 3088200.0000, 
raw observation next is [-0.5, 80.33333333333334, 0.0, 0.0, 19.0, 27.06693155703423, 0.7818978430713779, 0.0, 1.0, 65.0, 39526.8319850107], 
processed observation next is [0.0, 0.7391304347826086, 0.44875346260387816, 0.8033333333333335, 0.0, 0.0, 0.08333333333333333, 0.7555776297528526, 0.7606326143571259, 0.0, 1.0, 1.0, 0.1882230094524319], 
reward next is 0.8118, 
noisyNet noise sample is [array([-0.34124768], dtype=float32), -0.9039294]. 
=============================================
[2019-04-08 15:49:57,514] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3999443e-16 2.3801327e-08 1.5239134e-07 3.0141246e-07 4.4161055e-10
 6.9130682e-12 1.8133015e-08 4.1831982e-14 4.2971681e-18 6.0105431e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:49:57,534] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1206
[2019-04-08 15:49:57,557] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 19.0, 27.24733387228815, 0.8411319344220819, 0.0, 1.0, 65.0, 33610.24089540946], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 3084600.0000, 
sim time next is 3085200.0000, 
raw observation next is [0.0, 72.0, 32.0, 287.0, 19.0, 27.23893673295989, 0.8319861778392971, 0.0, 1.0, 65.0, 34571.60945992974], 
processed observation next is [0.0, 0.7391304347826086, 0.46260387811634357, 0.72, 0.10666666666666667, 0.31712707182320443, 0.08333333333333333, 0.7699113944133241, 0.7773287259464324, 0.0, 1.0, 1.0, 0.16462671171395116], 
reward next is 0.8354, 
noisyNet noise sample is [array([0.04027186], dtype=float32), -0.44074628]. 
=============================================
[2019-04-08 15:49:57,851] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.0584799e-22 4.9317469e-12 9.3143404e-09 2.7577549e-09 2.6033391e-11
 5.5651378e-16 6.9170190e-11 3.3349166e-17 8.8249036e-23 1.5751541e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:57,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1582
[2019-04-08 15:49:57,859] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 22.5, 28.58901341222781, 1.354197045030085, 1.0, 1.0, 65.00000000000003, 18846.53859928286], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 3174000.0000, 
sim time next is 3174600.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 22.5, 28.71450953129893, 1.356237156984289, 1.0, 1.0, 65.0, 18848.75546500158], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0, 0.0, 0.375, 0.8928757942749108, 0.9520790523280963, 1.0, 1.0, 1.0, 0.08975597840476943], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.2295254], dtype=float32), -1.2142751]. 
=============================================
[2019-04-08 15:49:58,004] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8587946e-19 1.6143679e-10 4.1661412e-08 5.9808003e-09 6.1545265e-12
 1.4020333e-14 6.4392136e-10 1.6336123e-14 5.4974165e-20 1.8443186e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:58,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2971
[2019-04-08 15:49:58,020] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.666666666666666, 100.0, 0.0, 0.0, 22.5, 28.39960759566096, 1.293861913781112, 0.0, 1.0, 65.0, 18848.50613795241], 
current ob forecast is [], 
actual action is [9.666666666666666, 65.0], 
sim time this is 3177600.0000, 
sim time next is 3178200.0000, 
raw observation next is [4.333333333333333, 100.0, 0.0, 0.0, 22.5, 28.33031256763885, 1.272394422882298, 1.0, 1.0, 65.0, 18848.91157010726], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 1.0, 0.0, 0.0, 0.375, 0.8608593806365707, 0.9241314742940995, 1.0, 1.0, 1.0, 0.08975672176241552], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.41791338], dtype=float32), 1.3371149]. 
=============================================
[2019-04-08 15:49:58,068] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5129588e-18 5.6115385e-10 1.9805773e-07 1.8046471e-07 8.1730516e-12
 2.3193734e-14 2.4171288e-07 6.5165383e-15 6.4258770e-20 7.3078731e-22
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:49:58,070] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4432
[2019-04-08 15:49:58,083] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 19.0, 26.92866718909609, 0.6862734159102993, 0.0, 1.0, 65.0, 40741.10099225793], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3115800.0000, 
sim time next is 3116400.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 26.87251158999211, 0.682557006883652, 0.0, 1.0, 65.0, 42419.05459828727], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.739375965832676, 0.7275190022945507, 0.0, 1.0, 1.0, 0.20199549808708225], 
reward next is 0.7980, 
noisyNet noise sample is [array([0.6401952], dtype=float32), -0.90575504]. 
=============================================
[2019-04-08 15:49:58,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.7255989e-17 2.7905481e-10 5.6391002e-08 5.2559887e-08 8.1517473e-11
 1.0356363e-13 1.9084755e-08 5.7249724e-13 3.6285733e-18 3.5494798e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:58,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9124
[2019-04-08 15:49:58,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 71.16666666666667, 0.0, 0.0, 19.0, 26.11085740502627, 0.5290288721357767, 0.0, 1.0, 65.0, 52324.76306106098], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3045000.0000, 
sim time next is 3045600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 19.0, 26.09785216555342, 0.5267336630760743, 0.0, 1.0, 65.0, 52479.03615485873], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.08333333333333333, 0.6748210137961183, 0.6755778876920248, 0.0, 1.0, 1.0, 0.24990017216599397], 
reward next is 0.7501, 
noisyNet noise sample is [array([-1.4074137], dtype=float32), 0.8417772]. 
=============================================
[2019-04-08 15:49:58,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5049574e-18 5.8479260e-10 4.6119546e-08 2.8208127e-08 1.2175362e-10
 1.7665723e-13 1.8456099e-08 1.1510489e-13 2.5771108e-18 3.3038739e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:49:58,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6615
[2019-04-08 15:49:58,566] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.97815456359218, 0.7531037244056419, 0.0, 1.0, 65.0, 41956.88464422614], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3093600.0000, 
sim time next is 3094200.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 19.0, 26.98150056617894, 0.7479262026671876, 0.0, 1.0, 65.0, 41847.67601403652], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7484583805149118, 0.7493087342223959, 0.0, 1.0, 1.0, 0.1992746476858882], 
reward next is 0.8007, 
noisyNet noise sample is [array([-1.1138796], dtype=float32), 0.8380135]. 
=============================================
[2019-04-08 15:49:58,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5273662e-17 4.7138693e-10 4.2385096e-08 1.0166099e-08 3.1294262e-12
 3.8297761e-14 1.5300607e-09 5.0686621e-16 1.6182666e-19 5.2267279e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:58,842] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0601
[2019-04-08 15:49:58,859] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.90906246390093, 0.7207439506753057, 0.0, 1.0, 65.0, 43535.89993553141], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3099600.0000, 
sim time next is 3100200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 26.88611400424314, 0.7196648874454228, 0.0, 1.0, 65.0, 44158.91402604502], 
processed observation next is [0.0, 0.9130434782608695, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.740509500353595, 0.7398882958151409, 0.0, 1.0, 1.0, 0.21028054298116677], 
reward next is 0.7897, 
noisyNet noise sample is [array([-1.9606898], dtype=float32), -0.04680528]. 
=============================================
[2019-04-08 15:49:59,105] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4492060e-20 2.7321919e-11 3.4336871e-08 6.4744263e-09 1.4568605e-11
 2.1742999e-15 4.7031219e-09 4.6660515e-16 8.3229985e-22 7.4404438e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:59,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3351
[2019-04-08 15:49:59,128] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 27.76802379742696, 1.181778096720535, 0.0, 1.0, 65.0, 26587.67211287344], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3186000.0000, 
sim time next is 3186600.0000, 
raw observation next is [2.833333333333333, 100.0, 0.0, 0.0, 19.0, 27.7595553311599, 1.176323462016066, 0.0, 1.0, 65.0, 26540.69105996883], 
processed observation next is [1.0, 0.9130434782608695, 0.541089566020314, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8132962775966582, 0.8921078206720221, 0.0, 1.0, 1.0, 0.1263842431427087], 
reward next is 0.8736, 
noisyNet noise sample is [array([-0.53649753], dtype=float32), -1.567641]. 
=============================================
[2019-04-08 15:49:59,135] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6525401e-20 1.9258640e-11 1.8352116e-09 2.5546676e-10 1.4737677e-13
 7.0531576e-16 1.6353630e-12 2.1249350e-17 1.3336084e-21 1.1889563e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:59,142] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1237
[2019-04-08 15:49:59,151] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.666666666666666, 100.0, 0.0, 0.0, 22.5, 28.39958488932657, 1.293856504007322, 0.0, 1.0, 65.0, 18848.50555303168], 
current ob forecast is [], 
actual action is [9.666666666666666, 65.0], 
sim time this is 3177600.0000, 
sim time next is 3178200.0000, 
raw observation next is [4.333333333333333, 100.0, 0.0, 0.0, 22.5, 28.33029088351115, 1.272388452147541, 1.0, 1.0, 65.0, 18848.91091630508], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 1.0, 0.0, 0.0, 0.375, 0.8608575736259292, 0.9241294840491804, 1.0, 1.0, 1.0, 0.08975671864907182], 
reward next is 0.9102, 
noisyNet noise sample is [array([-1.2097812], dtype=float32), -1.4299715]. 
=============================================
[2019-04-08 15:49:59,275] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.93502240e-18 5.02010655e-10 3.41377358e-07 1.72487461e-07
 1.46824133e-11 2.90779262e-14 4.17942081e-09 1.06821654e-14
 1.06816955e-18 7.20029058e-21 9.99999523e-01], sum to 1.0000
[2019-04-08 15:49:59,276] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8071113e-20 4.5442044e-12 1.7376902e-08 1.0217984e-08 1.0829498e-12
 3.1239170e-15 6.3441730e-10 3.9827946e-17 1.1389440e-20 3.1386339e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:59,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7920
[2019-04-08 15:49:59,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8604
[2019-04-08 15:49:59,294] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 26.76753030302644, 0.6946217139210041, 0.0, 1.0, 65.0, 43496.72146027758], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 3113400.0000, 
sim time next is 3114000.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 19.0, 26.88604382414192, 0.6893403082484292, 0.0, 1.0, 65.0, 39183.05577714081], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7405036520118268, 0.7297801027494764, 0.0, 1.0, 1.0, 0.18658597989114672], 
reward next is 0.8134, 
noisyNet noise sample is [array([-0.26428887], dtype=float32), -0.7273851]. 
=============================================
[2019-04-08 15:49:59,306] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.166666666666666, 100.0, 55.66666666666668, 288.6666666666667, 22.5, 27.17486110873547, 0.7639624231375385, 1.0, 1.0, 65.0, 40313.89736576181], 
current ob forecast is [], 
actual action is [11.166666666666666, 65.0], 
sim time this is 3139800.0000, 
sim time next is 3140400.0000, 
raw observation next is [6.333333333333333, 100.0, 69.33333333333334, 340.3333333333334, 22.5, 27.23415819615537, 0.7911640519038422, 1.0, 1.0, 65.0, 39357.45019594448], 
processed observation next is [1.0, 0.34782608695652173, 0.6380424746075716, 1.0, 0.23111111111111116, 0.3760589318600369, 0.375, 0.7695131830129475, 0.7637213506346141, 1.0, 1.0, 1.0, 0.18741642950449752], 
reward next is 0.8126, 
noisyNet noise sample is [array([-1.3576725], dtype=float32), 0.50339836]. 
=============================================
[2019-04-08 15:49:59,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.2405 ]
 [85.78495]
 [85.6796 ]
 [85.17303]
 [85.43256]], R is [[86.4440155 ]
 [86.37244415]
 [86.30052185]
 [86.2292099 ]
 [86.15843964]].
[2019-04-08 15:49:59,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0550410e-20 9.4134360e-11 2.7311682e-09 1.5416679e-08 2.9018910e-12
 1.7997521e-14 6.6459227e-10 7.4453256e-17 7.5813898e-21 6.0055627e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:49:59,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7682
[2019-04-08 15:49:59,797] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.984915333694, 0.6775778658226717, 0.0, 1.0, 65.0, 55354.03937144795], 
current ob forecast is [], 
actual action is [8.666666666666668, 65.0], 
sim time this is 3130800.0000, 
sim time next is 3131400.0000, 
raw observation next is [3.833333333333333, 100.0, 0.0, 0.0, 19.0, 26.93777874035825, 0.6839279780589567, 0.0, 1.0, 65.0, 57814.58902325476], 
processed observation next is [1.0, 0.21739130434782608, 0.5687903970452447, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7448148950298542, 0.7279759926863189, 0.0, 1.0, 1.0, 0.27530756677740365], 
reward next is 0.7247, 
noisyNet noise sample is [array([0.6686276], dtype=float32), 0.52737546]. 
=============================================
[2019-04-08 15:50:00,040] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7234986e-21 5.5130111e-12 2.6649091e-10 7.6357258e-09 2.6602923e-14
 3.0173333e-16 5.1070623e-12 9.1807428e-18 8.3530478e-24 1.3868786e-25
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:50:00,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0927
[2019-04-08 15:50:00,052] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.166666666666667, 98.83333333333334, 112.6666666666667, 817.3333333333334, 22.5, 28.67241438115431, 1.215491535118705, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [12.166666666666668, 65.0], 
sim time this is 3156600.0000, 
sim time next is 3157200.0000, 
raw observation next is [7.0, 100.0, 112.5, 814.5, 22.5, 28.67440760303869, 1.228924647231565, 1.0, 1.0, 65.0, 18848.88463777088], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.375, 0.9, 0.375, 0.889533966919891, 0.9096415490771883, 1.0, 1.0, 1.0, 0.08975659351319466], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.81375206], dtype=float32), -0.22379065]. 
=============================================
[2019-04-08 15:50:00,072] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4756315e-18 5.1870241e-11 3.1082017e-08 1.1748714e-07 1.6623806e-12
 3.9488598e-14 1.0383521e-09 1.1884130e-15 1.1652112e-19 2.2318990e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:50:00,078] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2701
[2019-04-08 15:50:00,092] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.23737266696745, 0.9671150037921233, 0.0, 1.0, 65.0, 40057.92067296257], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3207600.0000, 
sim time next is 3208200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.18388674074693, 0.9786451569809249, 0.0, 1.0, 65.0, 39904.15136956819], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7653238950622443, 0.826215052326975, 0.0, 1.0, 1.0, 0.19001976842651522], 
reward next is 0.8100, 
noisyNet noise sample is [array([-0.8788847], dtype=float32), 0.5538135]. 
=============================================
[2019-04-08 15:50:00,128] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9067553e-19 8.7645984e-12 7.1816162e-09 4.8347353e-08 4.7182251e-13
 9.8711759e-16 4.2189460e-10 1.6609853e-17 9.4166697e-22 4.3970835e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:50:00,132] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0131
[2019-04-08 15:50:00,143] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 27.47294527865632, 1.033853176042837, 0.0, 1.0, 65.0, 32294.1498932716], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 3201000.0000, 
sim time next is 3201600.0000, 
raw observation next is [0.6666666666666667, 100.0, 0.0, 0.0, 19.0, 27.47535849520056, 1.031792732669821, 0.0, 1.0, 65.0, 33384.75203691513], 
processed observation next is [1.0, 0.043478260869565216, 0.4810710987996307, 1.0, 0.0, 0.0, 0.08333333333333333, 0.78961320793338, 0.8439309108899403, 0.0, 1.0, 1.0, 0.15897500969959585], 
reward next is 0.8410, 
noisyNet noise sample is [array([-0.64366597], dtype=float32), -0.21429974]. 
=============================================
[2019-04-08 15:50:00,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.0247607e-19 5.2829347e-12 3.2860461e-09 1.6837163e-08 2.8576983e-11
 4.9955864e-15 3.5641674e-09 2.4956145e-16 2.6812412e-20 2.1457251e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:50:00,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4996
[2019-04-08 15:50:00,230] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.30895671536813, 0.9478344502044925, 0.0, 1.0, 65.0, 37896.39703816792], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3210600.0000, 
sim time next is 3211200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.20810651404287, 0.9446232319174084, 0.0, 1.0, 65.0, 40781.51791614766], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7673422095035726, 0.8148744106391361, 0.0, 1.0, 1.0, 0.19419770436260791], 
reward next is 0.8058, 
noisyNet noise sample is [array([-0.5993015], dtype=float32), -0.91854244]. 
=============================================
[2019-04-08 15:50:00,353] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1215682e-18 1.2874553e-10 1.1881074e-06 2.1188740e-07 1.8046135e-10
 1.7903462e-12 5.3571323e-08 1.1655949e-14 4.7667536e-18 2.5744799e-21
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:50:00,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2228
[2019-04-08 15:50:00,378] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 100.0, 0.0, 0.0, 19.0, 27.06021828218049, 0.9207873544532251, 0.0, 1.0, 65.0, 42164.94087364952], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 3213000.0000, 
sim time next is 3213600.0000, 
raw observation next is [-1.666666666666667, 100.0, 0.0, 0.0, 19.0, 27.03292955880995, 0.9158516671241711, 0.0, 1.0, 65.0, 42511.0273943017], 
processed observation next is [1.0, 0.17391304347826086, 0.4164358264081256, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7527441299008292, 0.8052838890413904, 0.0, 1.0, 1.0, 0.20243346378238905], 
reward next is 0.7976, 
noisyNet noise sample is [array([-0.8832924], dtype=float32), 0.5956822]. 
=============================================
[2019-04-08 15:50:00,596] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2655011e-18 8.6657834e-12 6.2960019e-08 1.0288445e-06 6.8599369e-11
 1.7644324e-13 4.0207004e-10 2.3729609e-15 1.5082275e-19 2.9768372e-22
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:50:00,596] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4078
[2019-04-08 15:50:00,624] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 19.0, 27.20810812832111, 0.9446240333631466, 0.0, 1.0, 65.0, 40781.49408733205], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3211200.0000, 
sim time next is 3211800.0000, 
raw observation next is [-1.166666666666667, 100.0, 0.0, 0.0, 19.0, 27.15997569785143, 0.9368214856228567, 0.0, 1.0, 65.0, 41032.06864521806], 
processed observation next is [1.0, 0.17391304347826086, 0.43028624192059095, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7633313081542857, 0.8122738285409522, 0.0, 1.0, 1.0, 0.19539080307246695], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.73182595], dtype=float32), -0.6037067]. 
=============================================
[2019-04-08 15:50:00,694] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5407401e-18 5.2172669e-11 1.7648214e-09 6.3968272e-08 5.1784015e-12
 5.1009989e-15 1.5829174e-09 1.9802726e-16 2.5162465e-21 4.6587067e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:50:00,699] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7861
[2019-04-08 15:50:00,726] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 19.0, 27.32366376539619, 1.045821525036114, 0.0, 1.0, 65.0, 35858.62453084462], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3196200.0000, 
sim time next is 3196800.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 19.0, 27.30702461246883, 1.04090490439801, 0.0, 1.0, 65.0, 36220.46104608148], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.08333333333333333, 0.7755853843724024, 0.8469683014660033, 0.0, 1.0, 1.0, 0.17247838593372133], 
reward next is 0.8275, 
noisyNet noise sample is [array([-1.1589223], dtype=float32), 1.0206453]. 
=============================================
[2019-04-08 15:50:01,411] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.3646985e-19 9.5031809e-11 9.0461427e-08 3.2844531e-07 1.6917308e-12
 1.3312115e-13 7.0412702e-09 3.3122495e-15 1.2084057e-19 1.9775549e-23
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:50:01,412] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-08 15:50:01,421] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 100.0, 108.0, 746.0, 22.5, 28.37946576076605, 1.06392537351139, 1.0, 1.0, 65.0, 18846.39285339547], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 3148200.0000, 
sim time next is 3148800.0000, 
raw observation next is [7.0, 100.0, 109.0, 755.8333333333334, 22.5, 28.43389151188416, 1.08023226349922, 1.0, 1.0, 65.0, 18847.56844195725], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36333333333333334, 0.8351749539594844, 0.375, 0.8694909593236799, 0.8600774211664067, 1.0, 1.0, 1.0, 0.08975032591408215], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.58124185], dtype=float32), 0.89056176]. 
=============================================
[2019-04-08 15:50:02,447] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.34968037e-18 4.85919811e-11 1.09087726e-08 2.08972381e-07
 1.49550771e-11 4.28102886e-14 2.69253531e-09 5.56501197e-15
 4.05589298e-19 1.39341442e-21 9.99999762e-01], sum to 1.0000
[2019-04-08 15:50:02,448] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6633
[2019-04-08 15:50:02,464] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6666666666666666, 100.0, 0.0, 0.0, 19.0, 27.33739322634695, 0.9901829416686297, 0.0, 1.0, 65.0, 35436.18034563951], 
current ob forecast is [], 
actual action is [4.333333333333333, 65.0], 
sim time this is 3206400.0000, 
sim time next is 3207000.0000, 
raw observation next is [-0.8333333333333334, 100.0, 0.0, 0.0, 19.0, 27.34513315660586, 0.9701649113785636, 0.0, 1.0, 65.0, 36258.2455618814], 
processed observation next is [1.0, 0.08695652173913043, 0.43951985226223456, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7787610963838217, 0.8233883037928545, 0.0, 1.0, 1.0, 0.17265831219943523], 
reward next is 0.8273, 
noisyNet noise sample is [array([-1.3559417], dtype=float32), 1.1437554]. 
=============================================
[2019-04-08 15:50:02,477] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.7261 ]
 [85.78411]
 [85.60865]
 [85.72105]
 [85.66946]], R is [[85.68164062]
 [85.65607452]
 [85.62327576]
 [85.58529663]
 [85.5472641 ]].
[2019-04-08 15:50:02,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5147921e-18 3.4489078e-12 6.0295044e-08 3.3696939e-09 8.4299720e-12
 1.4114348e-14 8.2868373e-10 4.4723591e-17 7.0422816e-20 2.4622720e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:50:02,767] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8794
[2019-04-08 15:50:02,775] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 19.0, 27.81407552932574, 1.190638146377527, 0.0, 1.0, 65.0, 26270.22936108449], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3184800.0000, 
sim time next is 3185400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 19.0, 27.7885998832503, 1.185276742222646, 0.0, 1.0, 65.0, 26403.70781489325], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.08333333333333333, 0.8157166569375249, 0.8950922474075487, 0.0, 1.0, 1.0, 0.12573194197568213], 
reward next is 0.8743, 
noisyNet noise sample is [array([-1.5138223], dtype=float32), 2.2588644]. 
=============================================
[2019-04-08 15:50:02,975] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.4550777e-18 7.4771066e-12 2.3936911e-08 6.1362641e-08 3.6256855e-12
 6.8345600e-13 1.3550778e-07 3.8304964e-15 8.5925144e-20 3.6218484e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:50:02,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5369
[2019-04-08 15:50:02,990] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.666666666666667, 100.0, 0.0, 0.0, 19.0, 26.9389015665095, 0.8857813583986918, 0.0, 1.0, 65.0, 43486.75683147183], 
current ob forecast is [], 
actual action is [2.333333333333333, 65.0], 
sim time this is 3217200.0000, 
sim time next is 3217800.0000, 
raw observation next is [-2.833333333333333, 100.0, 0.0, 0.0, 19.0, 26.91100647329589, 0.8843685955012096, 0.0, 1.0, 65.0, 43549.73728450637], 
processed observation next is [1.0, 0.21739130434782608, 0.3841181902123731, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7425838727746573, 0.7947895318337365, 0.0, 1.0, 1.0, 0.20737970135479225], 
reward next is 0.7926, 
noisyNet noise sample is [array([-0.6802671], dtype=float32), 0.88257915]. 
=============================================
[2019-04-08 15:50:03,374] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8226051e-17 5.1670317e-09 2.5916435e-07 1.3998999e-07 1.6675772e-10
 1.3778093e-12 3.4388567e-08 8.1447998e-14 5.2069973e-18 1.0232549e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:50:03,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7163
[2019-04-08 15:50:03,394] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.5, 76.0, 0.0, 0.0, 19.0, 27.1912281663398, 0.9241815280526268, 0.0, 1.0, 65.0, 37970.9662431039], 
current ob forecast is [], 
actual action is [0.5, 65.0], 
sim time this is 3270600.0000, 
sim time next is 3271200.0000, 
raw observation next is [-4.666666666666666, 77.66666666666667, 0.0, 0.0, 19.0, 27.17835714030454, 0.9199550097616512, 0.0, 1.0, 65.0, 38338.5251924945], 
processed observation next is [1.0, 0.8695652173913043, 0.33333333333333337, 0.7766666666666667, 0.0, 0.0, 0.08333333333333333, 0.7648630950253784, 0.8066516699205505, 0.0, 1.0, 1.0, 0.18256440567854523], 
reward next is 0.8174, 
noisyNet noise sample is [array([-1.0047622], dtype=float32), 0.8513865]. 
=============================================
[2019-04-08 15:50:03,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8616825e-17 7.8843307e-11 1.7141151e-08 1.1335360e-06 4.9324017e-11
 1.4790545e-13 2.0417465e-10 6.9057389e-15 6.6376757e-19 3.3310915e-21
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:50:03,527] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9833
[2019-04-08 15:50:03,536] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 71.0, 85.0, 686.0, 22.5, 28.54759570684908, 1.21235261170949, 1.0, 1.0, 65.0, 18846.02693383587], 
current ob forecast is [], 
actual action is [2.5, 65.0], 
sim time this is 3252600.0000, 
sim time next is 3253200.0000, 
raw observation next is [-2.666666666666667, 71.0, 80.66666666666667, 656.8333333333334, 22.5, 28.59015353494586, 1.219423461246272, 1.0, 1.0, 65.0, 18846.43920356467], 
processed observation next is [1.0, 0.6521739130434783, 0.38873499538319484, 0.71, 0.2688888888888889, 0.7257826887661142, 0.375, 0.8825127945788216, 0.9064744870820908, 1.0, 1.0, 1.0, 0.08974494858840319], 
reward next is 0.9103, 
noisyNet noise sample is [array([-2.2760625], dtype=float32), 0.88125205]. 
=============================================
[2019-04-08 15:50:03,822] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6052521e-18 7.3178075e-11 7.3415398e-09 1.6056699e-08 1.9146199e-11
 9.5699951e-15 6.7371264e-10 8.5061097e-16 4.7074125e-20 5.3617153e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:50:03,826] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0883
[2019-04-08 15:50:03,839] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666666, 100.0, 0.0, 0.0, 19.0, 27.3251888381142, 1.003180157639226, 0.0, 1.0, 65.0, 37608.84995284228], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 3203400.0000, 
sim time next is 3204000.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 19.0, 27.31514143390865, 0.9930799228879966, 0.0, 1.0, 65.0, 37609.75872951275], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7762617861590542, 0.8310266409626655, 0.0, 1.0, 1.0, 0.17909408918815597], 
reward next is 0.8209, 
noisyNet noise sample is [array([-0.6003765], dtype=float32), -1.8009479]. 
=============================================
[2019-04-08 15:50:03,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.47064]
 [83.52852]
 [83.59347]
 [83.41146]
 [83.36407]], R is [[83.57474518]
 [83.55990601]
 [83.55757141]
 [83.56213379]
 [83.56754303]].
[2019-04-08 15:50:04,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2641398e-19 1.2944096e-11 6.2178408e-08 3.2696654e-09 1.5756975e-12
 5.5870912e-15 1.8797240e-10 3.0805893e-17 1.5752275e-21 8.2468586e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:50:04,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1257
[2019-04-08 15:50:04,181] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [7.0, 100.0, 92.5, 721.0, 22.5, 29.1061708783545, 1.361892175498664, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [12.0, 65.0], 
sim time this is 3164400.0000, 
sim time next is 3165000.0000, 
raw observation next is [6.933333333333334, 99.83333333333334, 89.66666666666666, 707.0, 22.5, 29.16918890500698, 1.370757593421879, 1.0, 1.0, 65.0, 18849.38232304973], 
processed observation next is [1.0, 0.6521739130434783, 0.6546629732225301, 0.9983333333333334, 0.29888888888888887, 0.7812154696132597, 0.375, 0.9307657420839149, 0.956919197807293, 1.0, 1.0, 1.0, 0.08975896344309395], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.69334096], dtype=float32), 0.68629056]. 
=============================================
[2019-04-08 15:50:04,204] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[91.39978 ]
 [91.46585 ]
 [91.41556 ]
 [91.36918 ]
 [91.360634]], R is [[91.44798279]
 [91.53350067]
 [91.61816406]
 [91.64214325]
 [91.61844635]].
[2019-04-08 15:50:04,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.76914081e-18 4.95456731e-10 3.91837283e-08 1.13654046e-07
 4.90008251e-12 4.35491378e-13 3.40169803e-09 1.39220199e-15
 8.39113675e-19 2.60454359e-20 9.99999881e-01], sum to 1.0000
[2019-04-08 15:50:04,255] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9741
[2019-04-08 15:50:04,274] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.166666666666667, 100.0, 0.0, 0.0, 19.0, 27.15997504722044, 0.9368211983582251, 0.0, 1.0, 65.0, 41032.07456099577], 
current ob forecast is [], 
actual action is [3.833333333333333, 65.0], 
sim time this is 3211800.0000, 
sim time next is 3212400.0000, 
raw observation next is [-1.333333333333333, 100.0, 0.0, 0.0, 19.0, 27.11521638011646, 0.9249123726610841, 0.0, 1.0, 65.0, 40977.11058049664], 
processed observation next is [1.0, 0.17391304347826086, 0.42566943674976926, 1.0, 0.0, 0.0, 0.08333333333333333, 0.7596013650097051, 0.8083041242203614, 0.0, 1.0, 1.0, 0.19512909800236497], 
reward next is 0.8049, 
noisyNet noise sample is [array([1.1684538], dtype=float32), -0.5271346]. 
=============================================
[2019-04-08 15:50:05,006] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8205828e-17 3.7967269e-09 5.1545595e-07 1.3281009e-07 1.2225300e-09
 2.7723629e-12 1.3469245e-08 6.2219347e-14 4.0396666e-17 6.8368920e-20
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:50:05,008] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9475
[2019-04-08 15:50:05,020] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 19.0, 26.60155271385308, 0.7138860415602813, 0.0, 1.0, 65.0, 50620.36987918364], 
current ob forecast is [], 
actual action is [-3.0, 65.0], 
sim time this is 3291600.0000, 
sim time next is 3292200.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 19.0, 26.62822318368326, 0.712818134556747, 0.0, 1.0, 65.0, 50447.00371880106], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7190185986402717, 0.737606044852249, 0.0, 1.0, 1.0, 0.240223827232386], 
reward next is 0.7598, 
noisyNet noise sample is [array([-1.023705], dtype=float32), 0.6845346]. 
=============================================
[2019-04-08 15:50:05,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.3031508e-17 6.8651008e-11 2.9051582e-08 6.9221763e-08 7.9292357e-11
 3.8329183e-14 7.4713841e-10 6.8932181e-15 1.1196508e-17 3.3486901e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:50:05,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4348
[2019-04-08 15:50:05,183] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.833333333333334, 62.33333333333334, 114.3333333333333, 778.6666666666667, 22.5, 27.56652118385544, 0.5868834070266008, 1.0, 1.0, 65.0, 32483.02946307777], 
current ob forecast is [], 
actual action is [-1.833333333333334, 65.0], 
sim time this is 3323400.0000, 
sim time next is 3324000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666667, 115.1666666666667, 788.3333333333333, 22.5, 27.40278411212071, 0.9284514281806735, 1.0, 1.0, 65.0, 49129.8482920138], 
processed observation next is [1.0, 0.4782608695652174, 0.27793167128347185, 0.6066666666666667, 0.383888888888889, 0.871086556169429, 0.375, 0.7835653426767258, 0.8094838093935578, 1.0, 1.0, 1.0, 0.23395165853339903], 
reward next is 0.7660, 
noisyNet noise sample is [array([0.65295553], dtype=float32), 0.081547014]. 
=============================================
[2019-04-08 15:50:05,218] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.003   ]
 [77.07176 ]
 [77.054474]
 [77.04933 ]
 [76.987206]], R is [[77.04325867]
 [77.1181488 ]
 [77.21420288]
 [77.30815887]
 [77.39672089]].
[2019-04-08 15:50:05,340] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.9310116e-16 9.8829112e-10 1.2249907e-07 2.9695743e-06 1.7082093e-10
 5.7276666e-12 2.6136686e-08 1.4534185e-13 6.4297940e-17 2.5088721e-18
 9.9999690e-01], sum to 1.0000
[2019-04-08 15:50:05,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5733
[2019-04-08 15:50:05,366] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 80.0, 2.0, 94.0, 22.5, 26.10885720728663, 0.5726683622879453, 1.0, 1.0, 65.0, 57111.84771451898], 
current ob forecast is [], 
actual action is [-6.0, 65.0], 
sim time this is 3310200.0000, 
sim time next is 3310800.0000, 
raw observation next is [-11.0, 81.33333333333334, 16.0, 144.3333333333333, 22.5, 26.14534876638432, 0.580981484260579, 1.0, 1.0, 65.0, 55049.58163479222], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8133333333333335, 0.05333333333333334, 0.15948434622467766, 0.375, 0.67877906386536, 0.6936604947535264, 1.0, 1.0, 1.0, 0.26214086492758204], 
reward next is 0.7379, 
noisyNet noise sample is [array([-1.8804536], dtype=float32), 1.4542087]. 
=============================================
[2019-04-08 15:50:06,119] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.77575941e-18 1.60645930e-09 1.78479489e-08 1.40750895e-08
 4.84045313e-13 1.72507169e-12 1.74526171e-10 1.90029618e-15
 2.19581926e-17 4.00243773e-19 1.00000000e+00], sum to 1.0000
[2019-04-08 15:50:06,120] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0382
[2019-04-08 15:50:06,136] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.833333333333334, 69.0, 108.6666666666667, 698.3333333333333, 22.5, 27.37815784929087, 0.8181415834125337, 1.0, 1.0, 65.0, 31820.43439315759], 
current ob forecast is [], 
actual action is [-2.833333333333334, 65.0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 22.5, 27.33790167798636, 0.836330292236764, 1.0, 1.0, 65.0, 31555.90041765253], 
processed observation next is [1.0, 0.43478260869565216, 0.2502308402585411, 0.68, 0.366111111111111, 0.7946593001841622, 0.375, 0.7781584731655299, 0.7787767640789213, 1.0, 1.0, 1.0, 0.15026619246501205], 
reward next is 0.8497, 
noisyNet noise sample is [array([-0.08537594], dtype=float32), 0.8606475]. 
=============================================
[2019-04-08 15:50:06,151] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0553171e-16 2.1419748e-10 2.7245275e-07 5.9888239e-06 6.0104571e-10
 5.9168642e-11 1.7687600e-07 4.0888313e-12 5.3548851e-17 6.5614894e-18
 9.9999356e-01], sum to 1.0000
[2019-04-08 15:50:06,152] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3400
[2019-04-08 15:50:06,170] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.600000000000001, 77.0, 0.0, 0.0, 19.0, 26.50425949192166, 0.6739008136016501, 0.0, 1.0, 65.0, 50801.05155567887], 
current ob forecast is [], 
actual action is [-3.6000000000000014, 65.0], 
sim time this is 3296400.0000, 
sim time next is 3297000.0000, 
raw observation next is [-8.75, 77.0, 0.0, 0.0, 19.0, 26.50717223280474, 0.6779652196462645, 0.0, 1.0, 65.0, 51207.58361000819], 
processed observation next is [1.0, 0.13043478260869565, 0.22022160664819945, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7089310194003952, 0.7259884065487548, 0.0, 1.0, 1.0, 0.2438456362381342], 
reward next is 0.7562, 
noisyNet noise sample is [array([-1.0002558], dtype=float32), 0.18694776]. 
=============================================
[2019-04-08 15:50:06,180] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[74.14444 ]
 [74.1925  ]
 [74.275536]
 [74.33542 ]
 [74.3549  ]], R is [[74.09608459]
 [74.11321259]
 [74.12284088]
 [74.12747955]
 [74.134552  ]].
[2019-04-08 15:50:06,579] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-08 15:50:06,579] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:50:06,579] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:50:06,579] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:50:06,580] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:50:06,580] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:50:06,580] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:50:06,589] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run19
[2019-04-08 15:50:06,589] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run19
[2019-04-08 15:50:06,621] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run19
[2019-04-08 15:51:47,285] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6989.7397 316314656.2109 2958.2603
[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,321] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:47,445] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,134] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.2630 355894776.9810 2370.4173
[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,170] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:55,286] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,253] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3510 342856294.4587 2768.2918
[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,273] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:57,388] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:51:58,276] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 360000, evaluation results [360000.0, 6863.350978767962, 342856294.4587231, 2768.2918065107797, 6989.739732329105, 316314656.21088964, 2958.260328491998, 6801.2629667572, 355894776.9809919, 2370.417253531819]
[2019-04-08 15:51:58,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8846617e-16 1.7705764e-09 1.0100754e-07 2.8697744e-07 3.0485927e-09
 2.1191558e-12 5.3061875e-09 1.4892409e-13 1.0458933e-16 1.0492816e-17
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:51:58,341] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6484
[2019-04-08 15:51:58,357] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 65.5, 0.0, 0.0, 19.0, 26.93542521294274, 0.8303806784268977, 0.0, 1.0, 65.0, 44644.10165246558], 
current ob forecast is [], 
actual action is [2.5, 65.0], 
sim time this is 3547800.0000, 
sim time next is 3548400.0000, 
raw observation next is [-2.666666666666667, 67.33333333333333, 0.0, 0.0, 19.0, 26.91925977706067, 0.8250595576229122, 0.0, 1.0, 65.0, 44578.47551495282], 
processed observation next is [0.0, 0.043478260869565216, 0.38873499538319484, 0.6733333333333333, 0.0, 0.0, 0.08333333333333333, 0.7432716480883892, 0.7750198525409707, 0.0, 1.0, 1.0, 0.21227845483310867], 
reward next is 0.7877, 
noisyNet noise sample is [array([0.2550363], dtype=float32), 0.4389615]. 
=============================================
[2019-04-08 15:51:58,396] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3352997e-17 1.5659454e-10 4.9870528e-07 5.5334408e-07 1.5838514e-10
 1.7616246e-13 9.7971420e-10 3.9436075e-15 3.0645040e-18 1.7176993e-19
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:51:58,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2573
[2019-04-08 15:51:58,433] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.833333333333333, 54.0, 117.3333333333333, 806.6666666666667, 22.5, 27.49344368531676, 0.9137655213350051, 1.0, 1.0, 65.0, 24170.86494522334], 
current ob forecast is [], 
actual action is [-0.833333333333333, 65.0], 
sim time this is 3327000.0000, 
sim time next is 3327600.0000, 
raw observation next is [-5.666666666666666, 54.00000000000001, 117.6666666666667, 808.8333333333334, 22.5, 27.69348855805188, 0.6264632068113919, 1.0, 1.0, 65.0, 29747.39898639524], 
processed observation next is [1.0, 0.5217391304347826, 0.3056325023084026, 0.54, 0.3922222222222223, 0.8937384898710866, 0.375, 0.8077907131709899, 0.7088210689371306, 1.0, 1.0, 1.0, 0.14165428088759638], 
reward next is 0.8583, 
noisyNet noise sample is [array([1.3209254], dtype=float32), 1.1492543]. 
=============================================
[2019-04-08 15:51:58,747] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4819579e-17 2.3198242e-10 9.6658887e-08 8.5075283e-07 2.1501311e-11
 6.6105330e-13 4.4157438e-09 7.2370746e-14 7.4758235e-18 1.4312224e-19
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:51:58,749] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3028
[2019-04-08 15:51:58,793] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.166666666666667, 50.66666666666667, 111.3333333333333, 784.0, 22.5, 27.96765141502875, 0.9896157576784116, 1.0, 1.0, 65.0, 20555.33342588835], 
current ob forecast is [], 
actual action is [0.833333333333333, 65.0], 
sim time this is 3333000.0000, 
sim time next is 3333600.0000, 
raw observation next is [-4.0, 50.0, 110.0, 776.0, 22.5, 27.98748157442772, 0.8265087785505575, 1.0, 1.0, 65.0, 37558.28721645821], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.5, 0.36666666666666664, 0.8574585635359117, 0.375, 0.8322901312023099, 0.7755029261835191, 1.0, 1.0, 1.0, 0.17884898674503907], 
reward next is 0.8212, 
noisyNet noise sample is [array([0.21481816], dtype=float32), 0.25748485]. 
=============================================
[2019-04-08 15:51:59,951] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6232936e-16 9.1816155e-10 3.5110608e-07 2.2044024e-07 1.6902939e-10
 6.3984554e-13 4.5576023e-09 1.3517333e-14 3.2749933e-17 7.2632985e-21
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:51:59,954] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7312
[2019-04-08 15:51:59,976] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.333333333333333, 66.0, 111.8333333333333, 749.6666666666667, 22.5, 27.41940612223929, 0.8614606160427792, 1.0, 1.0, 65.0, 29055.20856156481], 
current ob forecast is [], 
actual action is [-2.333333333333333, 65.0], 
sim time this is 3321600.0000, 
sim time next is 3322200.0000, 
raw observation next is [-7.166666666666667, 65.0, 112.6666666666667, 759.3333333333333, 22.5, 27.4800945775834, 0.8768268373397707, 1.0, 1.0, 65.0, 28120.90451179036], 
processed observation next is [1.0, 0.43478260869565216, 0.26408125577100644, 0.65, 0.37555555555555564, 0.8390423572744014, 0.375, 0.7900078814652834, 0.7922756124465903, 1.0, 1.0, 1.0, 0.1339090691037636], 
reward next is 0.8661, 
noisyNet noise sample is [array([1.6252365], dtype=float32), 0.91737247]. 
=============================================
[2019-04-08 15:52:00,074] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.0798071e-17 1.2026460e-10 5.6319169e-07 4.9830447e-07 1.9995797e-11
 2.7104151e-11 9.4014494e-09 1.6725164e-14 6.4142964e-18 2.5768723e-18
 9.9999893e-01], sum to 1.0000
[2019-04-08 15:52:00,075] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5932
[2019-04-08 15:52:00,091] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.333333333333334, 51.33333333333333, 112.6666666666667, 792.0, 22.5, 27.87109218304884, 0.9709245062781421, 1.0, 1.0, 65.00000000000003, 19530.0166012309], 
current ob forecast is [], 
actual action is [0.6666666666666661, 65.0], 
sim time this is 3332400.0000, 
sim time next is 3333000.0000, 
raw observation next is [-4.166666666666667, 50.66666666666667, 111.3333333333333, 784.0, 22.5, 27.96765530727, 0.9896166444170219, 1.0, 1.0, 65.0, 20555.28493955821], 
processed observation next is [1.0, 0.5652173913043478, 0.3471837488457987, 0.5066666666666667, 0.371111111111111, 0.8662983425414365, 0.375, 0.8306379422725, 0.829872214805674, 1.0, 1.0, 1.0, 0.09788230923599148], 
reward next is 0.9021, 
noisyNet noise sample is [array([0.22804865], dtype=float32), -1.5646429]. 
=============================================
[2019-04-08 15:52:00,120] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.63453 ]
 [77.611465]
 [77.66653 ]
 [77.65611 ]
 [77.6535  ]], R is [[77.69124603]
 [77.82133484]
 [77.93748474]
 [77.99447632]
 [77.99280548]].
[2019-04-08 15:52:01,406] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3323126e-17 3.7913500e-10 3.4607101e-08 1.6886885e-08 2.8614614e-11
 1.1553231e-12 1.6572237e-08 9.1686819e-15 1.3137511e-18 8.9030923e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:01,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6651
[2019-04-08 15:52:01,456] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 60.0, 93.0, 540.0, 22.5, 27.28764094257148, 0.7668859050976874, 1.0, 1.0, 65.0, 29250.35303269713], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3402000.0000, 
sim time next is 3402600.0000, 
raw observation next is [-0.5, 58.0, 95.0, 579.3333333333334, 22.5, 27.32332160937452, 0.7932370856110763, 1.0, 1.0, 65.0, 30411.7028681447], 
processed observation next is [1.0, 0.391304347826087, 0.44875346260387816, 0.58, 0.31666666666666665, 0.6401473296500921, 0.375, 0.7769434674478767, 0.7644123618703588, 1.0, 1.0, 1.0, 0.14481763270545095], 
reward next is 0.8552, 
noisyNet noise sample is [array([1.1446248], dtype=float32), 0.21661723]. 
=============================================
[2019-04-08 15:52:01,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.05957929e-17 1.46724313e-10 4.53292479e-08 1.15844394e-07
 1.03460331e-10 1.62091640e-12 5.27560173e-09 1.26358859e-14
 5.26628518e-19 2.74731788e-20 9.99999881e-01], sum to 1.0000
[2019-04-08 15:52:01,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1503
[2019-04-08 15:52:01,990] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 47.66666666666667, 114.0, 797.3333333333333, 22.5, 27.87490991342982, 0.8428346259408341, 1.0, 1.0, 65.0, 56680.15945515323], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3410400.0000, 
sim time next is 3411000.0000, 
raw observation next is [3.0, 47.0, 115.0, 804.0, 22.5, 27.33198024669726, 0.9745150959040298, 1.0, 1.0, 65.0, 68874.87033879817], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.47, 0.38333333333333336, 0.8883977900552487, 0.375, 0.7776650205581049, 0.8248383653013432, 1.0, 1.0, 1.0, 0.327975573041896], 
reward next is 0.6720, 
noisyNet noise sample is [array([-1.2178065], dtype=float32), -0.05739819]. 
=============================================
[2019-04-08 15:52:02,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.69051]
 [82.52057]
 [82.37303]
 [82.22293]
 [82.11751]], R is [[82.67112732]
 [82.57450867]
 [82.55381775]
 [82.56328583]
 [82.61206818]].
[2019-04-08 15:52:02,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5419597e-16 5.3329119e-09 3.1704761e-07 1.4503273e-06 5.7490444e-09
 6.6830682e-12 6.5332921e-07 7.8989593e-13 2.6150361e-16 2.6164904e-18
 9.9999762e-01], sum to 1.0000
[2019-04-08 15:52:02,689] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2351
[2019-04-08 15:52:02,715] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9904536e-18 1.4747963e-10 5.1876455e-08 2.9604070e-08 6.1711205e-13
 3.2746484e-15 2.8822793e-09 1.4777462e-15 3.0478457e-19 2.7095870e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:02,715] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5196
[2019-04-08 15:52:02,736] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 42.5, 0.0, 0.0, 19.0, 26.92679013181642, 0.7559372722790508, 0.0, 1.0, 65.0, 39915.33489732046], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3627000.0000, 
sim time next is 3627600.0000, 
raw observation next is [5.0, 36.66666666666667, 0.0, 0.0, 19.0, 26.96949967796467, 0.7619167525965577, 0.0, 1.0, 65.0, 38941.44764441501], 
processed observation next is [0.0, 1.0, 0.6011080332409973, 0.3666666666666667, 0.0, 0.0, 0.08333333333333333, 0.7474583064970558, 0.7539722508655192, 0.0, 1.0, 1.0, 0.18543546497340482], 
reward next is 0.8146, 
noisyNet noise sample is [array([0.0544815], dtype=float32), 1.4503354]. 
=============================================
[2019-04-08 15:52:02,740] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 108.0, 790.5, 22.5, 28.34715268293226, 1.086835511795094, 1.0, 1.0, 65.0, 30676.86408648665], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3420000.0000, 
sim time next is 3420600.0000, 
raw observation next is [3.0, 50.5, 106.3333333333333, 785.3333333333334, 22.5, 28.40425998110174, 1.098467780688914, 1.0, 1.0, 65.0, 28407.77637484822], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.505, 0.35444444444444434, 0.8677716390423573, 0.375, 0.8670216650918118, 0.8661559268963046, 1.0, 1.0, 1.0, 0.13527512559451535], 
reward next is 0.8647, 
noisyNet noise sample is [array([-0.73544616], dtype=float32), -0.6720183]. 
=============================================
[2019-04-08 15:52:03,351] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6223015e-17 1.3582441e-09 7.7429178e-08 6.7170129e-07 1.0108675e-10
 1.4052457e-13 2.2523423e-08 6.2668004e-14 7.7796800e-18 1.1371421e-19
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:52:03,353] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9807
[2019-04-08 15:52:03,365] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.833333333333334, 70.0, 0.0, 0.0, 19.0, 26.72458869451064, 0.7258663957314516, 0.0, 1.0, 65.0, 46660.87703271444], 
current ob forecast is [], 
actual action is [0.16666666666666607, 65.0], 
sim time this is 3365400.0000, 
sim time next is 3366000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 19.0, 26.68555665000328, 0.7195323716451059, 0.0, 1.0, 65.0, 47222.32572220756], 
processed observation next is [1.0, 1.0, 0.32409972299168976, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7237963875002734, 0.739844123881702, 0.0, 1.0, 1.0, 0.22486821772479793], 
reward next is 0.7751, 
noisyNet noise sample is [array([0.28477302], dtype=float32), -0.6298857]. 
=============================================
[2019-04-08 15:52:03,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.65552]
 [77.62412]
 [77.79951]
 [77.75956]
 [77.5955 ]], R is [[77.67734528]
 [77.67837524]
 [77.68350983]
 [77.68718719]
 [77.68901825]].
[2019-04-08 15:52:03,754] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1167977e-17 1.8524070e-10 3.7441444e-07 3.4866410e-08 4.7312140e-11
 8.3200027e-13 8.0836082e-10 2.1235275e-14 8.6588233e-18 1.2008237e-19
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:52:03,756] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4094
[2019-04-08 15:52:03,785] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 26.35621388735138, 0.5840280905768188, 0.0, 1.0, 65.0, 52808.09454167995], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3394800.0000, 
sim time next is 3395400.0000, 
raw observation next is [-2.833333333333333, 64.16666666666667, 0.0, 0.0, 22.5, 26.34584742666048, 0.5852927238658897, 0.0, 1.0, 65.0, 52562.48093198548], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.6416666666666667, 0.0, 0.0, 0.375, 0.69548728555504, 0.6950975746219633, 0.0, 1.0, 1.0, 0.2502975282475499], 
reward next is 0.7497, 
noisyNet noise sample is [array([1.1779531], dtype=float32), -0.14176196]. 
=============================================
[2019-04-08 15:52:03,877] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2624502e-19 5.5455358e-12 7.8717113e-09 3.5470300e-09 3.7695420e-12
 9.2683412e-15 1.3121014e-10 7.1594719e-17 2.6683045e-19 8.3892006e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:03,878] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9175
[2019-04-08 15:52:03,892] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 19.0, 27.3851849215504, 0.9650118460656314, 1.0, 1.0, 65.0, 35634.39542186912], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3442200.0000, 
sim time next is 3442800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 19.0, 27.41168174435172, 0.9628561792423492, 0.0, 1.0, 65.0, 34942.76218335239], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.79, 0.0, 0.0, 0.08333333333333333, 0.7843068120293101, 0.8209520597474498, 0.0, 1.0, 1.0, 0.16639410563501136], 
reward next is 0.8336, 
noisyNet noise sample is [array([0.2566528], dtype=float32), 2.756935]. 
=============================================
[2019-04-08 15:52:03,920] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2430902e-16 1.4701472e-10 4.3290338e-08 2.4580900e-06 5.9138631e-11
 1.9246057e-12 8.4720391e-08 9.8163773e-13 1.1988449e-17 1.1782914e-19
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:52:03,920] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1109
[2019-04-08 15:52:03,935] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 22.5, 26.49125160555135, 0.6162075287481686, 1.0, 1.0, 65.0, 48247.88344126454], 
current ob forecast is [], 
actual action is [2.666666666666667, 65.0], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 22.5, 26.5364083587713, 0.6357321637387047, 1.0, 1.0, 65.0, 47300.46610141743], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.6083333333333333, 0.1011111111111111, 0.23425414364640884, 0.375, 0.7113673632309417, 0.7119107212462349, 1.0, 1.0, 1.0, 0.22524031476865441], 
reward next is 0.7748, 
noisyNet noise sample is [array([-0.07570046], dtype=float32), 0.39133543]. 
=============================================
[2019-04-08 15:52:04,519] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0980897e-18 1.9681483e-10 1.0110346e-07 8.6845449e-07 1.0007801e-10
 9.5971164e-14 1.2996818e-09 1.4649599e-14 4.3370667e-19 2.0953230e-21
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:52:04,522] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8372
[2019-04-08 15:52:04,536] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 67.0, 72.5, 608.5, 22.5, 28.67773901286437, 1.161963835923885, 1.0, 1.0, 65.0, 18844.83241668039], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3427200.0000, 
sim time next is 3427800.0000, 
raw observation next is [2.0, 67.0, 68.66666666666666, 576.6666666666667, 22.5, 28.68986356172799, 1.156632829360685, 1.0, 1.0, 65.0, 18844.52481237732], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.22888888888888886, 0.6372007366482505, 0.375, 0.8908219634773324, 0.8855442764535617, 1.0, 1.0, 1.0, 0.089735832439892], 
reward next is 0.9103, 
noisyNet noise sample is [array([-1.1353363], dtype=float32), -0.91921717]. 
=============================================
[2019-04-08 15:52:05,153] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.13030550e-18 1.94306429e-11 6.07100006e-08 4.43171444e-07
 5.21739162e-12 6.83249572e-14 1.15368675e-08 1.97705079e-14
 4.22241866e-19 2.86407927e-20 9.99999523e-01], sum to 1.0000
[2019-04-08 15:52:05,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5443
[2019-04-08 15:52:05,182] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.5, 73.0, 0.0, 0.0, 22.5, 27.3094436046157, 0.988069118785456, 1.0, 1.0, 65.0, 31688.02547760732], 
current ob forecast is [], 
actual action is [6.5, 65.0], 
sim time this is 3436200.0000, 
sim time next is 3436800.0000, 
raw observation next is [1.333333333333333, 75.0, 0.0, 0.0, 22.5, 27.5082906350647, 1.01261751734325, 1.0, 1.0, 65.00000000000003, 29347.49726228921], 
processed observation next is [1.0, 0.782608695652174, 0.4995383194829178, 0.75, 0.0, 0.0, 0.375, 0.7923575529220583, 0.8375391724477499, 1.0, 1.0, 1.0000000000000007, 0.13974998696328195], 
reward next is 0.8603, 
noisyNet noise sample is [array([-1.7404404], dtype=float32), 0.47158232]. 
=============================================
[2019-04-08 15:52:05,197] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.07842308e-15 7.20515203e-10 2.25206325e-08 5.44308023e-07
 1.48283719e-10 2.72037991e-12 1.24804600e-09 3.80897760e-13
 1.84047868e-17 1.01248495e-17 9.99999404e-01], sum to 1.0000
[2019-04-08 15:52:05,200] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.7277
[2019-04-08 15:52:05,215] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.55799327394982, 0.6841726164618435, 0.0, 1.0, 65.0, 48377.05240926593], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3370200.0000, 
sim time next is 3370800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 19.0, 26.53520946099493, 0.6824434676449168, 0.0, 1.0, 65.0, 48621.86498430722], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7112674550829109, 0.7274811558816389, 0.0, 1.0, 1.0, 0.23153269040146296], 
reward next is 0.7685, 
noisyNet noise sample is [array([0.5488362], dtype=float32), -0.5475443]. 
=============================================
[2019-04-08 15:52:05,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.40724227e-16 7.56946172e-11 1.06361355e-07 3.01284018e-07
 1.50633152e-11 1.21125942e-13 2.21026109e-09 4.83638576e-15
 9.38386981e-19 3.42590300e-22 9.99999642e-01], sum to 1.0000
[2019-04-08 15:52:05,357] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3009
[2019-04-08 15:52:05,375] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 27.21611162348841, 0.8860888182725776, 0.0, 1.0, 65.0, 42832.08974568108], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3450600.0000, 
sim time next is 3451200.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 27.18933491674785, 0.8774762170451357, 0.0, 1.0, 65.0, 41391.9153403744], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.7657779097289875, 0.7924920723483786, 0.0, 1.0, 1.0, 0.19710435876368762], 
reward next is 0.8029, 
noisyNet noise sample is [array([-0.16649109], dtype=float32), 0.7400327]. 
=============================================
[2019-04-08 15:52:05,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7588251e-15 1.5630485e-08 4.0505313e-07 1.0282005e-06 1.3077044e-10
 2.9627141e-11 2.0112141e-07 1.4606386e-12 8.5073202e-18 1.0347864e-19
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:52:05,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0872
[2019-04-08 15:52:05,532] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.333333333333333, 73.0, 0.0, 0.0, 19.0, 26.51567558573384, 0.6110079057114777, 0.0, 1.0, 65.0, 48966.30958531554], 
current ob forecast is [], 
actual action is [-0.33333333333333304, 65.0], 
sim time this is 3386400.0000, 
sim time next is 3387000.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.4623754171572, 0.6138977626507195, 0.0, 1.0, 65.0, 51038.00350902277], 
processed observation next is [1.0, 0.17391304347826086, 0.31948291782086796, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7051979514297667, 0.7046325875502398, 0.0, 1.0, 1.0, 0.24303811194772748], 
reward next is 0.7570, 
noisyNet noise sample is [array([0.28998888], dtype=float32), 0.6937945]. 
=============================================
[2019-04-08 15:52:05,546] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.89627 ]
 [79.919106]
 [79.96044 ]
 [79.95568 ]
 [79.96528 ]], R is [[79.81017303]
 [79.77890015]
 [79.75105286]
 [79.71873474]
 [79.68872833]].
[2019-04-08 15:52:05,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.12865635e-17 7.75059530e-11 1.21209247e-08 1.08445271e-08
 7.24243278e-12 2.12163356e-14 3.89833887e-10 2.95485564e-16
 3.66715946e-19 3.66425916e-22 1.00000000e+00], sum to 1.0000
[2019-04-08 15:52:05,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3657
[2019-04-08 15:52:05,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4617920e-18 2.5942219e-12 1.4181862e-07 5.4740723e-09 1.5504167e-12
 9.1813560e-14 1.1856951e-09 1.2686594e-15 1.9095435e-18 5.8028828e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:05,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3789
[2019-04-08 15:52:05,594] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3609520e-17 3.3949017e-11 9.3961701e-09 1.2303157e-07 9.8438003e-12
 1.5458058e-14 4.8041104e-10 6.1710993e-16 2.0386935e-18 1.5314877e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:05,595] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2417
[2019-04-08 15:52:05,603] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 22.5, 27.83010717471507, 0.9403503881019893, 1.0, 1.0, 65.0, 22478.71999795672], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 3408000.0000, 
sim time next is 3408600.0000, 
raw observation next is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 22.5, 27.92832656608678, 0.9352195396209306, 1.0, 1.0, 65.0, 26373.05138799926], 
processed observation next is [1.0, 0.43478260869565216, 0.541089566020314, 0.48833333333333345, 0.37, 0.8589318600368323, 0.375, 0.8273605471738984, 0.8117398465403102, 1.0, 1.0, 1.0, 0.12558595899047267], 
reward next is 0.8744, 
noisyNet noise sample is [array([0.07873751], dtype=float32), -0.39787847]. 
=============================================
[2019-04-08 15:52:05,611] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666666, 71.16666666666666, 0.0, 0.0, 19.0, 26.87097330401709, 0.7627315186006474, 0.0, 1.0, 65.0, 45374.0614887963], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 3473400.0000, 
sim time next is 3474000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 26.83549156137803, 0.7581770688314159, 0.0, 1.0, 65.0, 46129.63376285683], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7362909634481692, 0.752725689610472, 0.0, 1.0, 1.0, 0.2196649226802706], 
reward next is 0.7803, 
noisyNet noise sample is [array([-0.34879294], dtype=float32), 0.95429546]. 
=============================================
[2019-04-08 15:52:05,613] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.5, 52.5, 11.0, 133.0, 22.5, 27.8729347239918, 0.9477614265730683, 1.0, 1.0, 65.0, 30708.68162689354], 
current ob forecast is [], 
actual action is [2.5, 65.0], 
sim time this is 3346200.0000, 
sim time next is 3346800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 22.5, 27.6770393204101, 0.9609231024240131, 1.0, 1.0, 65.00000000000003, 32728.30729778211], 
processed observation next is [1.0, 0.7391304347826086, 0.38873499538319484, 0.5333333333333333, 0.030555555555555558, 0.12246777163904232, 0.375, 0.8064199433675082, 0.8203077008080043, 1.0, 1.0, 1.0000000000000007, 0.155849082370391], 
reward next is 0.8442, 
noisyNet noise sample is [array([-0.4465011], dtype=float32), 0.04369205]. 
=============================================
[2019-04-08 15:52:05,628] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.14133 ]
 [82.23435 ]
 [82.31774 ]
 [82.442375]
 [82.52063 ]], R is [[82.01850891]
 [81.98226166]
 [81.94715118]
 [81.91601562]
 [81.88784027]].
[2019-04-08 15:52:07,057] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.1126955e-17 2.3811647e-11 6.1729564e-09 2.9495806e-07 9.1221586e-11
 4.3265210e-14 8.4038915e-10 3.5649575e-15 5.7593107e-19 6.9610544e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:52:07,057] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0714
[2019-04-08 15:52:07,080] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.333333333333333, 60.0, 89.0, 461.3333333333333, 22.5, 27.03359370965511, 0.7276263595457136, 1.0, 1.0, 65.00000000000003, 35413.84951081123], 
current ob forecast is [], 
actual action is [3.666666666666667, 65.0], 
sim time this is 3400800.0000, 
sim time next is 3401400.0000, 
raw observation next is [-1.166666666666667, 60.0, 91.0, 500.6666666666666, 22.5, 27.12639781470767, 0.7643784385825197, 1.0, 1.0, 65.0, 34620.21220981203], 
processed observation next is [1.0, 0.34782608695652173, 0.43028624192059095, 0.6, 0.30333333333333334, 0.5532228360957642, 0.375, 0.7605331512256391, 0.75479281286084, 1.0, 1.0, 1.0, 0.1648581533800573], 
reward next is 0.8351, 
noisyNet noise sample is [array([-1.0161439], dtype=float32), 0.17440236]. 
=============================================
[2019-04-08 15:52:07,606] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7274432e-19 8.8479120e-11 1.3249232e-08 5.5527330e-08 1.2938913e-11
 1.8924840e-14 2.6178111e-09 4.5208356e-16 1.0072610e-18 1.9416474e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:07,612] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4521
[2019-04-08 15:52:07,617] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6142615e-17 1.4735077e-10 1.2567459e-07 3.7216233e-07 3.8288667e-11
 6.7528421e-14 9.5440292e-08 3.9796393e-14 8.0843401e-19 4.4305435e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:52:07,619] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2163
[2019-04-08 15:52:07,636] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 19.0, 27.0911704284449, 0.8387353958934757, 0.0, 1.0, 65.0, 41099.22265555491], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3458400.0000, 
sim time next is 3459000.0000, 
raw observation next is [1.0, 80.16666666666667, 0.0, 0.0, 19.0, 27.07867893468718, 0.8387727761906089, 0.0, 1.0, 65.0, 41609.53590919183], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8016666666666667, 0.0, 0.0, 0.08333333333333333, 0.7565565778905983, 0.7795909253968696, 0.0, 1.0, 1.0, 0.19814064718662774], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.5559834], dtype=float32), 2.0714595]. 
=============================================
[2019-04-08 15:52:07,641] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 55.0, 99.83333333333334, 763.1666666666667, 22.5, 28.4521501669565, 1.113447355120249, 1.0, 1.0, 65.0, 27101.39900594616], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3422400.0000, 
sim time next is 3423000.0000, 
raw observation next is [3.0, 56.5, 96.66666666666666, 751.3333333333334, 22.5, 28.44867999232343, 1.122213532975085, 1.0, 1.0, 65.0, 25705.09723470723], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.565, 0.3222222222222222, 0.8302025782688767, 0.375, 0.8707233326936192, 0.8740711776583616, 1.0, 1.0, 1.0, 0.12240522492717729], 
reward next is 0.8776, 
noisyNet noise sample is [array([1.4596864], dtype=float32), -1.3186158]. 
=============================================
[2019-04-08 15:52:07,648] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.168686]
 [86.10377 ]
 [86.0835  ]
 [86.05129 ]
 [86.02993 ]], R is [[86.25241089]
 [86.26083374]
 [86.26522064]
 [86.2741394 ]
 [86.27612305]].
[2019-04-08 15:52:07,654] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.24717 ]
 [83.407684]
 [83.3859  ]
 [83.898476]
 [84.1358  ]], R is [[83.27643585]
 [83.24796295]
 [83.22254944]
 [83.19523621]
 [83.1655426 ]].
[2019-04-08 15:52:08,763] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0678156e-18 1.7298732e-11 5.1959443e-08 1.7272110e-09 3.2144187e-11
 5.6316886e-14 3.6761963e-10 6.9817109e-16 1.6534606e-19 4.5023041e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:08,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6135
[2019-04-08 15:52:08,778] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 77.0, 105.5, 722.0, 22.5, 27.91776965195583, 0.9304104532260403, 1.0, 1.0, 65.0, 23819.70256102963], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3751200.0000, 
sim time next is 3751800.0000, 
raw observation next is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 22.5, 27.98325582986378, 0.9460115589653909, 1.0, 1.0, 65.0, 23220.64382930808], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.76, 0.3577777777777777, 0.8151012891344384, 0.375, 0.8319379858219816, 0.815337186321797, 1.0, 1.0, 1.0, 0.11057449442527657], 
reward next is 0.8894, 
noisyNet noise sample is [array([0.49263248], dtype=float32), 2.0682757]. 
=============================================
[2019-04-08 15:52:08,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7588525e-17 3.5382028e-10 4.1799975e-08 1.7594623e-07 2.5240279e-10
 5.5346192e-12 5.3743401e-09 1.9541812e-15 8.2375096e-18 3.4306262e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:08,885] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6389
[2019-04-08 15:52:08,899] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 19.0, 27.1446879319566, 0.8583982797070243, 0.0, 1.0, 65.0, 40235.1992438977], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3454200.0000, 
sim time next is 3454800.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 19.0, 27.11848655717272, 0.8524752078537877, 0.0, 1.0, 65.0, 41246.98750245389], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.86, 0.0, 0.0, 0.08333333333333333, 0.7598738797643932, 0.7841584026179292, 0.0, 1.0, 1.0, 0.1964142262021614], 
reward next is 0.8036, 
noisyNet noise sample is [array([-1.1912909], dtype=float32), 0.47854167]. 
=============================================
[2019-04-08 15:52:09,867] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9363447e-17 6.4946111e-11 8.4740686e-09 9.7863915e-07 9.5434424e-12
 4.5994827e-13 2.1207924e-08 2.7361434e-14 6.3833430e-19 5.0196741e-21
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:52:09,868] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5819
[2019-04-08 15:52:09,889] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 83.66666666666667, 660.0, 22.5, 28.90777373722803, 1.249273309257294, 1.0, 1.0, 65.0, 18848.93267537914], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3512400.0000, 
sim time next is 3513000.0000, 
raw observation next is [3.0, 49.0, 79.33333333333333, 633.0, 22.5, 28.94665333477713, 1.255837101122133, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.2644444444444444, 0.6994475138121546, 0.375, 0.9122211112314277, 0.918612367040711, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30401257], dtype=float32), 0.4676171]. 
=============================================
[2019-04-08 15:52:09,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.40167 ]
 [83.465416]
 [83.56916 ]
 [83.624306]
 [83.69348 ]], R is [[83.46183014]
 [83.5374527 ]
 [83.61232758]
 [83.77620697]
 [83.84868622]].
[2019-04-08 15:52:10,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5081568e-19 1.5966498e-10 3.5733976e-09 4.2648324e-08 1.3344720e-12
 8.8424500e-14 1.6336624e-10 4.3567792e-15 2.1597629e-19 9.1058742e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:10,361] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3581
[2019-04-08 15:52:10,379] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 22.5, 28.10752772831671, 1.072257937928007, 1.0, 1.0, 65.0, 21767.80318475225], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 3496800.0000, 
sim time next is 3497400.0000, 
raw observation next is [1.5, 59.0, 115.0, 810.0, 22.5, 28.18095174410964, 0.8486557360411178, 1.0, 1.0, 65.0, 27455.4489508214], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.59, 0.38333333333333336, 0.8950276243093923, 0.375, 0.8484126453424702, 0.7828852453470393, 1.0, 1.0, 1.0, 0.13074023309914953], 
reward next is 0.8693, 
noisyNet noise sample is [array([0.47927427], dtype=float32), -0.41069874]. 
=============================================
[2019-04-08 15:52:10,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4392145e-18 1.2439773e-10 5.1310717e-08 2.2692081e-08 7.1332909e-12
 4.3351648e-13 9.3373922e-09 1.1754158e-13 3.7973655e-17 8.1324372e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:10,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2545
[2019-04-08 15:52:10,883] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 19.0, 26.5846973845274, 0.7635000886999924, 0.0, 1.0, 65.0, 44186.56626212329], 
current ob forecast is [], 
actual action is [-0.33333333333333304, 65.0], 
sim time this is 3577200.0000, 
sim time next is 3577800.0000, 
raw observation next is [-5.166666666666667, 65.83333333333334, 103.6666666666667, 703.3333333333334, 19.0, 26.62215954710208, 0.7707087504454745, 0.0, 1.0, 65.0, 43471.5816382491], 
processed observation next is [0.0, 0.391304347826087, 0.31948291782086796, 0.6583333333333334, 0.34555555555555567, 0.7771639042357275, 0.08333333333333333, 0.71851329559184, 0.7569029168151582, 0.0, 1.0, 1.0, 0.20700753161071], 
reward next is 0.7930, 
noisyNet noise sample is [array([0.9940695], dtype=float32), -0.042405855]. 
=============================================
[2019-04-08 15:52:11,137] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.91969933e-18 3.55984200e-11 6.57488073e-08 1.38819175e-07
 6.00211686e-11 1.08885604e-13 3.83477028e-09 2.63408453e-16
 8.84247596e-19 3.07684983e-20 9.99999762e-01], sum to 1.0000
[2019-04-08 15:52:11,141] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5087
[2019-04-08 15:52:11,161] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 83.66666666666667, 660.0, 22.5, 28.90777202348047, 1.249272847853392, 1.0, 1.0, 65.0, 18848.93265593849], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3512400.0000, 
sim time next is 3513000.0000, 
raw observation next is [3.0, 49.0, 79.33333333333333, 633.0, 22.5, 28.94665164339758, 1.255836641418928, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.2644444444444444, 0.6994475138121546, 0.375, 0.9122209702831316, 0.9186122138063094, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17996219], dtype=float32), -0.35997146]. 
=============================================
[2019-04-08 15:52:11,199] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.92407]
 [83.99182]
 [84.09511]
 [84.15226]
 [84.22383]], R is [[83.97655487]
 [84.04702759]
 [84.11680603]
 [84.2756424 ]
 [84.34312439]].
[2019-04-08 15:52:11,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5845963e-17 3.3330370e-11 1.5387687e-07 1.7038465e-07 5.2793971e-12
 4.6143506e-14 1.2324814e-08 2.7243538e-15 2.0828625e-19 1.8601044e-21
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:52:11,375] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5595
[2019-04-08 15:52:11,388] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8333333333333334, 69.16666666666667, 95.33333333333334, 579.6666666666667, 22.5, 27.45467782622762, 0.9059340419163134, 1.0, 1.0, 65.0, 36957.93504555848], 
current ob forecast is [], 
actual action is [4.166666666666667, 65.0], 
sim time this is 3489000.0000, 
sim time next is 3489600.0000, 
raw observation next is [-0.6666666666666667, 67.33333333333334, 97.16666666666666, 624.8333333333334, 22.5, 27.59302081406288, 0.911312509895224, 1.0, 1.0, 65.0, 33459.06363804176], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6733333333333335, 0.32388888888888884, 0.6904235727440148, 0.375, 0.7994184011719065, 0.8037708366317413, 1.0, 1.0, 1.0, 0.15932887446686553], 
reward next is 0.8407, 
noisyNet noise sample is [array([0.29573873], dtype=float32), -0.7621502]. 
=============================================
[2019-04-08 15:52:11,775] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1521997e-17 8.3867233e-11 5.0355624e-08 8.3773514e-08 3.8880054e-12
 8.0670090e-13 6.2640920e-10 1.0137739e-14 1.4471067e-18 3.1817559e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:11,776] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5459
[2019-04-08 15:52:11,791] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 28.11178391583147, 1.106590123725748, 1.0, 1.0, 65.0, 24632.5388057663], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 3523800.0000, 
sim time next is 3524400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 28.00327110080163, 1.103362640823897, 1.0, 1.0, 65.0, 27320.19791469556], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.8336059250668025, 0.8677875469412989, 1.0, 1.0, 1.0, 0.13009618054616934], 
reward next is 0.8699, 
noisyNet noise sample is [array([1.1235045], dtype=float32), -0.07320468]. 
=============================================
[2019-04-08 15:52:11,929] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0971736e-17 5.3519222e-10 2.9617437e-08 6.4109099e-07 2.8984164e-11
 2.6383372e-12 1.4952709e-10 1.2501345e-14 3.7240395e-19 1.0942204e-21
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:52:11,933] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4343
[2019-04-08 15:52:11,950] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3218053e-17 5.3239779e-10 1.1987687e-08 4.8414989e-08 8.4825896e-11
 2.0096083e-13 1.7432162e-09 1.2595155e-13 1.4601537e-19 4.4090824e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:11,951] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5812
[2019-04-08 15:52:11,953] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 22.5, 28.10752713223469, 1.072257765833595, 1.0, 1.0, 65.0, 21767.81013226258], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 3496800.0000, 
sim time next is 3497400.0000, 
raw observation next is [1.5, 59.0, 115.0, 810.0, 22.5, 28.18095114515222, 0.8486555599317459, 1.0, 1.0, 65.0, 27455.45497221191], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.59, 0.38333333333333336, 0.8950276243093923, 0.375, 0.8484125954293518, 0.7828851866439153, 1.0, 1.0, 1.0, 0.13074026177243767], 
reward next is 0.8693, 
noisyNet noise sample is [array([-0.8354069], dtype=float32), -0.20885333]. 
=============================================
[2019-04-08 15:52:11,969] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 37.5, 338.0, 22.5, 28.79244549706527, 1.221726102000387, 1.0, 1.0, 65.0, 18849.29018272844], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3517200.0000, 
sim time next is 3517800.0000, 
raw observation next is [2.833333333333333, 49.5, 29.33333333333333, 275.6666666666666, 22.5, 28.85971690452689, 1.222498221789657, 1.0, 1.0, 65.0, 9424.677007643773], 
processed observation next is [1.0, 0.7391304347826086, 0.541089566020314, 0.495, 0.09777777777777776, 0.30460405156537745, 0.375, 0.9049764087105743, 0.9074994072632189, 1.0, 1.0, 1.0, 0.044879414322113204], 
reward next is 0.9551, 
noisyNet noise sample is [array([0.10503005], dtype=float32), 1.3230895]. 
=============================================
[2019-04-08 15:52:12,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3964120e-17 3.2646580e-11 3.0154865e-08 2.0396712e-07 9.3939911e-11
 1.5925046e-13 2.0847615e-09 2.2186050e-15 2.9787640e-19 1.0243326e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:12,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0947
[2019-04-08 15:52:12,076] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.833333333333333, 57.66666666666666, 115.3333333333333, 814.3333333333334, 22.5, 27.67913884309074, 1.070228426423082, 1.0, 1.0, 65.00000000000009, 28648.25858239447], 
current ob forecast is [], 
actual action is [6.833333333333333, 65.0], 
sim time this is 3498600.0000, 
sim time next is 3499200.0000, 
raw observation next is [2.0, 57.0, 115.5, 816.5, 22.5, 28.09195479042167, 0.8674827800578343, 1.0, 1.0, 65.0, 24364.89531096944], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.57, 0.385, 0.9022099447513812, 0.375, 0.8409962325351392, 0.7891609266859447, 1.0, 1.0, 1.0, 0.11602331100461638], 
reward next is 0.8840, 
noisyNet noise sample is [array([1.3112066], dtype=float32), 0.5492334]. 
=============================================
[2019-04-08 15:52:12,366] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0530704e-16 6.1911926e-10 5.0302930e-07 6.3456646e-08 5.1218897e-11
 1.4227036e-12 2.6757894e-08 2.6014274e-14 1.4189664e-16 8.0595831e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:52:12,367] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-08 15:52:12,401] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 102.0, 788.0, 19.0, 27.26092478489525, 0.9369686244559784, 0.0, 1.0, 65.0, 30973.9488883027], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3594600.0000, 
sim time next is 3595200.0000, 
raw observation next is [-1.0, 42.0, 99.33333333333333, 773.1666666666667, 19.0, 27.28969012112326, 0.9382327426402023, 0.0, 1.0, 65.0, 30485.67671495051], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.3311111111111111, 0.854327808471455, 0.08333333333333333, 0.7741408434269385, 0.8127442475467341, 0.0, 1.0, 1.0, 0.14516988911881196], 
reward next is 0.8548, 
noisyNet noise sample is [array([0.5845542], dtype=float32), 1.7311443]. 
=============================================
[2019-04-08 15:52:12,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0233563e-16 7.0701844e-10 3.6709156e-07 5.5571286e-07 5.8427818e-10
 1.0435615e-12 6.4009591e-08 1.1416635e-13 6.0762974e-17 1.1173027e-18
 9.9999905e-01], sum to 1.0000
[2019-04-08 15:52:12,744] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7828
[2019-04-08 15:52:12,761] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.14796376842514, 0.8348141188011766, 0.0, 1.0, 65.0, 36146.67957352597], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3613800.0000, 
sim time next is 3614400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 19.0, 27.13436261515839, 0.8307222064705565, 0.0, 1.0, 65.0, 36297.36489815002], 
processed observation next is [0.0, 0.8695652173913043, 0.4349030470914128, 0.42, 0.0, 0.0, 0.08333333333333333, 0.7611968845965326, 0.7769074021568522, 0.0, 1.0, 1.0, 0.17284459475309535], 
reward next is 0.8272, 
noisyNet noise sample is [array([1.3473324], dtype=float32), -1.2915703]. 
=============================================
[2019-04-08 15:52:13,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3166459e-18 1.1057222e-10 3.1219503e-08 2.6843889e-08 1.1112144e-11
 3.8058840e-14 8.4209377e-09 1.6022008e-15 2.8388907e-18 2.9633929e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:13,226] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8843
[2019-04-08 15:52:13,247] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.5, 54.5, 114.0, 816.0, 19.0, 26.88537639522168, 0.8373824400807374, 0.0, 1.0, 65.0, 38000.62045781415], 
current ob forecast is [], 
actual action is [1.5, 65.0], 
sim time this is 3583800.0000, 
sim time next is 3584400.0000, 
raw observation next is [-3.333333333333333, 54.66666666666667, 114.6666666666667, 817.1666666666666, 19.0, 26.90372130657709, 0.8453494057536822, 0.0, 1.0, 65.0, 37060.7092544857], 
processed observation next is [0.0, 0.4782608695652174, 0.37026777469990774, 0.5466666666666667, 0.38222222222222235, 0.9029465930018415, 0.08333333333333333, 0.7419767755480908, 0.7817831352512274, 0.0, 1.0, 1.0, 0.17647956787850333], 
reward next is 0.8235, 
noisyNet noise sample is [array([1.5744792], dtype=float32), -0.5197003]. 
=============================================
[2019-04-08 15:52:14,263] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [3.40505590e-17 3.53745699e-10 8.94922096e-07 2.08762162e-06
 1.08968095e-11 1.72659706e-12 6.33579162e-08 9.36359280e-14
 2.53358126e-17 5.36069906e-20 9.99996901e-01], sum to 1.0000
[2019-04-08 15:52:14,267] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6966
[2019-04-08 15:52:14,297] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 70.0, 0.0, 0.0, 19.0, 27.25968987654912, 0.9205307616119135, 0.0, 1.0, 65.0, 42141.00822476792], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3537600.0000, 
sim time next is 3538200.0000, 
raw observation next is [-1.0, 68.0, 0.0, 0.0, 19.0, 27.20498740065183, 0.9122772369455383, 0.0, 1.0, 65.0, 42540.81368891957], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.68, 0.0, 0.0, 0.08333333333333333, 0.7670822833876526, 0.8040924123151795, 0.0, 1.0, 1.0, 0.20257530328056936], 
reward next is 0.7974, 
noisyNet noise sample is [array([0.82069945], dtype=float32), 2.340076]. 
=============================================
[2019-04-08 15:52:14,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3189466e-17 5.6752384e-11 2.2070065e-08 1.1975250e-07 2.5526997e-10
 1.7046056e-13 1.7187093e-09 4.6016952e-14 1.6239626e-18 2.0934016e-19
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:14,320] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8655
[2019-04-08 15:52:14,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6074298e-18 7.1842116e-11 3.5217049e-08 1.8329457e-08 2.1170799e-12
 1.4477199e-14 6.7486627e-10 1.3831615e-14 1.2333833e-18 2.4744215e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:14,334] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1065
[2019-04-08 15:52:14,339] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 19.0, 27.01505863991272, 0.8520809206055523, 0.0, 1.0, 65.0, 44443.40070272046], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3545400.0000, 
sim time next is 3546000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 19.0, 27.00235151979529, 0.8461997053443922, 0.0, 1.0, 65.0, 44217.07449451776], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.08333333333333333, 0.7501959599829409, 0.7820665684481307, 0.0, 1.0, 1.0, 0.2105574975929417], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.60105765], dtype=float32), -1.2322053]. 
=============================================
[2019-04-08 15:52:14,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.8679  ]
 [78.132286]
 [78.61203 ]
 [79.21247 ]
 [80.035065]], R is [[77.55132294]
 [77.56417847]
 [77.58351135]
 [77.60388947]
 [77.61724091]].
[2019-04-08 15:52:14,353] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 43.0, 64.0, 551.0, 22.5, 28.69195486784425, 1.243436446253092, 1.0, 1.0, 65.0, 28705.21556551749], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 3861000.0000, 
sim time next is 3861600.0000, 
raw observation next is [3.0, 42.33333333333334, 56.33333333333334, 489.0, 22.5, 28.35311531382288, 1.190466062291613, 1.0, 1.0, 65.00000000000006, 19406.5834855483], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.42333333333333345, 0.18777777777777782, 0.5403314917127072, 0.375, 0.86275960948524, 0.896822020763871, 1.0, 1.0, 1.000000000000001, 0.09241230231213476], 
reward next is 0.9076, 
noisyNet noise sample is [array([-2.068964], dtype=float32), 0.8166698]. 
=============================================
[2019-04-08 15:52:14,939] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5184789e-15 2.9436920e-10 1.9853910e-07 3.4489275e-08 2.3575625e-10
 1.0017709e-12 1.6402142e-08 1.7572457e-14 7.1821702e-18 7.9447156e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:14,951] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.2196
[2019-04-08 15:52:14,968] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 68.0, 0.0, 0.0, 19.0, 26.87809885670181, 0.8033297450509105, 0.0, 1.0, 65.0, 44860.93213873418], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3551400.0000, 
sim time next is 3552000.0000, 
raw observation next is [-3.0, 67.0, 0.0, 0.0, 19.0, 26.84720520270714, 0.797917663986056, 0.0, 1.0, 65.0, 45318.41908126904], 
processed observation next is [0.0, 0.08695652173913043, 0.3795013850415513, 0.67, 0.0, 0.0, 0.08333333333333333, 0.737267100225595, 0.7659725546620186, 0.0, 1.0, 1.0, 0.21580199562509067], 
reward next is 0.7842, 
noisyNet noise sample is [array([0.62523514], dtype=float32), -0.16341206]. 
=============================================
[2019-04-08 15:52:14,972] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4265693e-16 1.3640850e-09 1.8981526e-08 1.0687694e-06 2.3680823e-11
 6.3225927e-13 5.0282342e-08 5.1618042e-14 8.5206733e-18 1.0800464e-19
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:52:14,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8698
[2019-04-08 15:52:14,986] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.5, 46.0, 114.0, 812.0, 19.0, 27.18800834294398, 0.9056464784532242, 0.0, 1.0, 65.0, 31888.13464652855], 
current ob forecast is [], 
actual action is [3.5, 65.0], 
sim time this is 3591000.0000, 
sim time next is 3591600.0000, 
raw observation next is [-1.333333333333333, 44.66666666666667, 112.0, 808.0, 19.0, 27.20064780511159, 0.9126091538737304, 0.0, 1.0, 65.0, 32313.66570362079], 
processed observation next is [0.0, 0.5652173913043478, 0.42566943674976926, 0.4466666666666667, 0.37333333333333335, 0.8928176795580111, 0.08333333333333333, 0.7667206504259658, 0.8042030512912435, 0.0, 1.0, 1.0, 0.15387459858867042], 
reward next is 0.8461, 
noisyNet noise sample is [array([-0.41919798], dtype=float32), 0.8363425]. 
=============================================
[2019-04-08 15:52:14,988] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[77.05405]
 [76.64569]
 [76.56814]
 [76.36069]
 [76.28565]], R is [[76.992836  ]
 [77.00928497]
 [77.03211212]
 [77.05603027]
 [77.07245636]].
[2019-04-08 15:52:15,194] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5590355e-16 9.9606394e-09 2.4013684e-07 2.6328423e-06 2.2099128e-10
 3.9147063e-13 9.7390974e-08 1.3589425e-14 2.0589622e-17 9.9337226e-19
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:52:15,195] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4397
[2019-04-08 15:52:15,207] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 96.66666666666667, 758.3333333333333, 19.0, 27.29041611740027, 0.9399053979214752, 0.0, 1.0, 65.0, 30634.82590509272], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3595800.0000, 
sim time next is 3596400.0000, 
raw observation next is [-1.0, 42.0, 94.0, 743.5, 19.0, 27.28603720178292, 0.9430998076761675, 0.0, 1.0, 65.0, 31226.5878176995], 
processed observation next is [0.0, 0.6521739130434783, 0.4349030470914128, 0.42, 0.31333333333333335, 0.8215469613259668, 0.08333333333333333, 0.7738364334819101, 0.8143666025587225, 0.0, 1.0, 1.0, 0.14869803722714048], 
reward next is 0.8513, 
noisyNet noise sample is [array([-0.11866756], dtype=float32), 2.2114718]. 
=============================================
[2019-04-08 15:52:15,480] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.5883746e-18 8.6194100e-11 6.7472246e-09 4.0327190e-09 1.4976688e-11
 2.0643578e-15 7.8759577e-10 1.6778647e-13 7.3915255e-18 1.1129722e-20
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:15,483] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9862
[2019-04-08 15:52:15,499] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8333333333333334, 42.16666666666666, 91.33333333333334, 728.6666666666667, 19.0, 27.28594665589168, 0.9529023398190363, 0.0, 1.0, 65.0, 31390.38146266669], 
current ob forecast is [], 
actual action is [4.166666666666667, 65.0], 
sim time this is 3597000.0000, 
sim time next is 3597600.0000, 
raw observation next is [-0.6666666666666667, 42.33333333333334, 88.66666666666667, 713.8333333333333, 19.0, 27.32681379619992, 0.9632686610786068, 0.0, 1.0, 65.0, 30387.72089385413], 
processed observation next is [0.0, 0.6521739130434783, 0.44413665743305636, 0.42333333333333345, 0.29555555555555557, 0.7887661141804787, 0.08333333333333333, 0.7772344830166601, 0.821089553692869, 0.0, 1.0, 1.0, 0.14470343282787682], 
reward next is 0.8553, 
noisyNet noise sample is [array([-0.9102278], dtype=float32), 1.446715]. 
=============================================
[2019-04-08 15:52:15,894] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2161199e-16 7.8127664e-09 4.4696429e-07 4.7235931e-06 5.8990701e-10
 9.2719625e-13 1.2162808e-07 2.3625752e-13 1.4839970e-16 4.3972368e-19
 9.9999475e-01], sum to 1.0000
[2019-04-08 15:52:15,895] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0565
[2019-04-08 15:52:15,908] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.466666666666667, 26.33333333333334, 0.0, 0.0, 19.0, 27.16498347184537, 0.7618026997415166, 0.0, 1.0, 65.0, 35260.55498805466], 
current ob forecast is [], 
actual action is [13.466666666666667, 65.0], 
sim time this is 3638400.0000, 
sim time next is 3639000.0000, 
raw observation next is [8.333333333333332, 26.66666666666666, 0.0, 0.0, 19.0, 27.15765944650643, 0.7601002720210301, 0.0, 1.0, 65.0, 35378.21307168689], 
processed observation next is [0.0, 0.08695652173913043, 0.6934441366574331, 0.2666666666666666, 0.0, 0.0, 0.08333333333333333, 0.7631382872088691, 0.7533667573403434, 0.0, 1.0, 1.0, 0.16846768129374712], 
reward next is 0.8315, 
noisyNet noise sample is [array([0.8707067], dtype=float32), 0.15395443]. 
=============================================
[2019-04-08 15:52:15,918] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.09232 ]
 [77.15559 ]
 [76.97364 ]
 [77.066414]
 [76.96311 ]], R is [[77.17640686]
 [77.23673248]
 [77.29703522]
 [77.35717773]
 [77.41695404]].
[2019-04-08 15:52:16,855] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1169464e-17 6.8611616e-11 4.5778696e-09 3.9306794e-08 5.6645112e-11
 2.4214272e-12 1.0431566e-08 3.2915186e-15 5.7574014e-19 1.1790359e-20
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:16,856] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6564
[2019-04-08 15:52:16,872] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.666666666666667, 71.0, 0.0, 0.0, 19.0, 26.82032187009566, 0.7178031146143274, 0.0, 1.0, 65.0, 44701.3152422803], 
current ob forecast is [], 
actual action is [0.33333333333333304, 65.0], 
sim time this is 3907200.0000, 
sim time next is 3907800.0000, 
raw observation next is [-5.0, 68.0, 0.0, 0.0, 19.0, 26.76562912205691, 0.7065077514813977, 0.0, 1.0, 65.0, 46547.8861649113], 
processed observation next is [1.0, 0.21739130434782608, 0.32409972299168976, 0.68, 0.0, 0.0, 0.08333333333333333, 0.7304690935047425, 0.7355025838271326, 0.0, 1.0, 1.0, 0.22165660078529192], 
reward next is 0.7783, 
noisyNet noise sample is [array([2.1462297], dtype=float32), -0.14311175]. 
=============================================
[2019-04-08 15:52:16,950] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5805372e-17 1.7589127e-10 1.2194229e-07 1.3506350e-08 6.1134487e-12
 3.1736834e-13 2.0909232e-08 2.4110917e-14 1.1222154e-19 1.0033982e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:16,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4128
[2019-04-08 15:52:16,968] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.0, 26.33333333333334, 97.0, 576.3333333333334, 19.0, 27.46718685062287, 0.8729549825112093, 0.0, 1.0, 65.0, 25764.16356472042], 
current ob forecast is [], 
actual action is [16.0, 65.0], 
sim time this is 3661800.0000, 
sim time next is 3662400.0000, 
raw observation next is [11.0, 26.66666666666667, 99.0, 619.6666666666666, 19.0, 27.52161527893261, 0.8855221786066464, 0.0, 1.0, 65.0, 25315.52331027738], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.2666666666666667, 0.33, 0.6847145488029466, 0.08333333333333333, 0.7934679399110509, 0.7951740595355488, 0.0, 1.0, 1.0, 0.12055011100132085], 
reward next is 0.8794, 
noisyNet noise sample is [array([-0.6244882], dtype=float32), -1.0924231]. 
=============================================
[2019-04-08 15:52:18,739] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0318213e-18 7.1614555e-12 3.9807919e-08 4.8261754e-08 5.8955764e-11
 2.3842213e-13 2.8915463e-09 3.7880828e-14 3.1054124e-18 1.2339849e-19
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:18,742] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1165
[2019-04-08 15:52:18,771] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [10.0, 28.0, 91.0, 446.3333333333334, 19.0, 27.31891215931041, 0.8374725061639907, 0.0, 1.0, 65.0, 29925.68447193642], 
current ob forecast is [], 
actual action is [15.0, 65.0], 
sim time this is 3660000.0000, 
sim time next is 3660600.0000, 
raw observation next is [10.5, 27.0, 93.0, 489.6666666666666, 19.0, 27.3432484061589, 0.8462628644360803, 0.0, 1.0, 65.0, 28919.17160348006], 
processed observation next is [0.0, 0.34782608695652173, 0.7534626038781165, 0.27, 0.31, 0.5410681399631675, 0.08333333333333333, 0.778604033846575, 0.7820876214786935, 0.0, 1.0, 1.0, 0.13771034096895265], 
reward next is 0.8623, 
noisyNet noise sample is [array([-0.50866777], dtype=float32), 0.5616085]. 
=============================================
[2019-04-08 15:52:19,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5992008e-17 6.7937950e-10 8.9584326e-08 6.7906058e-07 9.9104509e-11
 7.1465152e-14 1.0300434e-08 3.4791353e-14 2.6411635e-18 9.4366961e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:52:19,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8339
[2019-04-08 15:52:19,260] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 108.0, 800.0, 19.0, 27.23521632968052, 0.9250672387239302, 0.0, 1.0, 65.0, 31421.56371043446], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3592800.0000, 
sim time next is 3593400.0000, 
raw observation next is [-1.0, 42.0, 106.0, 796.0, 19.0, 27.25833784657096, 0.929261011202937, 0.0, 1.0, 65.0, 30950.85669211031], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.35333333333333333, 0.8795580110497238, 0.08333333333333333, 0.7715281538809133, 0.809753670400979, 0.0, 1.0, 1.0, 0.14738503186719196], 
reward next is 0.8526, 
noisyNet noise sample is [array([-0.32515725], dtype=float32), 1.6968796]. 
=============================================
[2019-04-08 15:52:19,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.68225766e-17 6.91914137e-10 9.06325468e-08 6.79213372e-07
 1.00578240e-10 7.18204386e-14 1.03102815e-08 3.53757923e-14
 2.69321467e-18 9.67995833e-21 9.99999166e-01], sum to 1.0000
[2019-04-08 15:52:19,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4770
[2019-04-08 15:52:19,286] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 42.0, 106.0, 796.0, 19.0, 27.25833784657096, 0.929261011202937, 0.0, 1.0, 65.0, 30950.85669211031], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 3593400.0000, 
sim time next is 3594000.0000, 
raw observation next is [-1.0, 42.0, 104.0, 792.0, 19.0, 27.25868821476138, 0.930607043626424, 0.0, 1.0, 65.0, 30828.80773132834], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.3466666666666667, 0.8751381215469614, 0.08333333333333333, 0.7715573512301152, 0.8102023478754746, 0.0, 1.0, 1.0, 0.14680384633965876], 
reward next is 0.8532, 
noisyNet noise sample is [array([-0.32515725], dtype=float32), 1.6968796]. 
=============================================
[2019-04-08 15:52:19,294] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.570206]
 [84.615875]
 [84.651764]
 [84.70332 ]
 [84.73475 ]], R is [[84.56059265]
 [84.56760406]
 [84.57230377]
 [84.5743103 ]
 [84.57469177]].
[2019-04-08 15:52:19,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.7492586e-19 4.9657452e-11 2.0680694e-07 4.2727464e-07 7.7073091e-11
 3.6508777e-14 1.7112871e-08 2.6298792e-14 5.0656551e-19 9.4960243e-22
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:52:19,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3613
[2019-04-08 15:52:19,433] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.833333333333334, 42.5, 115.6666666666667, 825.3333333333334, 19.0, 27.73067209085846, 1.01266308501882, 0.0, 1.0, 65.0, 23669.29365076624], 
current ob forecast is [], 
actual action is [9.833333333333334, 65.0], 
sim time this is 3675000.0000, 
sim time next is 3675600.0000, 
raw observation next is [5.0, 42.0, 115.0, 823.5, 19.0, 27.740319618259, 1.0174227545164, 0.0, 1.0, 65.0, 23684.44586442709], 
processed observation next is [0.0, 0.5652173913043478, 0.6011080332409973, 0.42, 0.38333333333333336, 0.9099447513812154, 0.08333333333333333, 0.8116933015215834, 0.8391409181721334, 0.0, 1.0, 1.0, 0.1127830755448909], 
reward next is 0.8872, 
noisyNet noise sample is [array([2.238098], dtype=float32), 0.16340406]. 
=============================================
[2019-04-08 15:52:19,660] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [6.68946785e-17 5.28598543e-09 1.93382178e-07 2.76741936e-08
 1.80417270e-09 3.68156264e-14 4.12795167e-08 4.19577669e-14
 1.92648942e-17 1.04676204e-19 9.99999762e-01], sum to 1.0000
[2019-04-08 15:52:19,662] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8150
[2019-04-08 15:52:19,679] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 19.0, 27.17713370240285, 0.7613679458616359, 0.0, 1.0, 65.0, 34473.42127550621], 
current ob forecast is [], 
actual action is [14.833333333333334, 65.0], 
sim time this is 3649800.0000, 
sim time next is 3650400.0000, 
raw observation next is [10.0, 25.0, 0.0, 0.0, 19.0, 27.21087247119016, 0.7666753554006234, 0.0, 1.0, 65.0, 33840.43151941845], 
processed observation next is [0.0, 0.2608695652173913, 0.739612188365651, 0.25, 0.0, 0.0, 0.08333333333333333, 0.7675727059325134, 0.7555584518002078, 0.0, 1.0, 1.0, 0.16114491199723072], 
reward next is 0.8389, 
noisyNet noise sample is [array([-0.649411], dtype=float32), 1.2243512]. 
=============================================
[2019-04-08 15:52:19,682] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.0939886e-19 2.3514016e-10 1.3073339e-08 3.8901482e-09 2.9662585e-11
 5.9776945e-14 1.0951764e-08 1.8583589e-15 4.1785166e-19 2.9196767e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:19,685] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2899
[2019-04-08 15:52:19,702] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.16666666666667, 27.33333333333334, 107.6666666666667, 729.6666666666667, 19.0, 27.61439102307084, 0.9325402414587587, 0.0, 1.0, 65.0, 23247.21075984828], 
current ob forecast is [], 
actual action is [16.16666666666667, 65.0], 
sim time this is 3665400.0000, 
sim time next is 3666000.0000, 
raw observation next is [11.33333333333333, 26.66666666666667, 109.3333333333333, 746.3333333333334, 19.0, 27.63917243393427, 0.9429633860100327, 0.0, 1.0, 65.0, 22674.71690716658], 
processed observation next is [0.0, 0.43478260869565216, 0.7765466297322253, 0.2666666666666667, 0.36444444444444435, 0.8246777163904236, 0.08333333333333333, 0.8032643694945225, 0.8143211286700108, 0.0, 1.0, 1.0, 0.10797484241507896], 
reward next is 0.8920, 
noisyNet noise sample is [array([0.5308234], dtype=float32), -1.7965558]. 
=============================================
[2019-04-08 15:52:19,715] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[86.51053]
 [86.39279]
 [86.27902]
 [86.11299]
 [85.86978]], R is [[86.63150787]
 [86.65448761]
 [86.67515564]
 [86.69348907]
 [86.70908356]].
[2019-04-08 15:52:20,219] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4937033e-16 1.7231472e-10 1.0125370e-06 4.0617115e-07 1.4387744e-10
 3.7080297e-13 6.2337300e-09 1.4647637e-14 2.7868698e-17 1.1689889e-20
 9.9999857e-01], sum to 1.0000
[2019-04-08 15:52:20,220] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8531
[2019-04-08 15:52:20,238] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.666666666666668, 51.66666666666666, 0.0, 0.0, 19.0, 26.62834045412667, 0.7315968411502953, 0.0, 1.0, 65.0, 50800.83339267548], 
current ob forecast is [], 
actual action is [-3.666666666666668, 65.0], 
sim time this is 3969600.0000, 
sim time next is 3970200.0000, 
raw observation next is [-8.833333333333332, 52.33333333333334, 0.0, 0.0, 19.0, 26.58421203279239, 0.7222208430752022, 0.0, 1.0, 65.0, 51077.50262790007], 
processed observation next is [1.0, 0.9565217391304348, 0.2179132040627886, 0.5233333333333334, 0.0, 0.0, 0.08333333333333333, 0.7153510027326991, 0.7407402810250674, 0.0, 1.0, 1.0, 0.2432262029900003], 
reward next is 0.7568, 
noisyNet noise sample is [array([-1.1990141], dtype=float32), 0.10586188]. 
=============================================
[2019-04-08 15:52:20,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4143769e-19 2.6700584e-09 1.6468637e-07 1.4745885e-07 2.4537178e-10
 8.8099389e-14 1.2591846e-08 3.3821164e-15 5.0393311e-18 8.1231257e-22
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:52:20,492] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0721
[2019-04-08 15:52:20,508] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 45.66666666666667, 101.1666666666667, 764.1666666666667, 19.0, 28.07051416908808, 1.08415337775795, 0.0, 1.0, 65.0, 19758.37617893238], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 3681600.0000, 
sim time next is 3682200.0000, 
raw observation next is [6.0, 46.33333333333334, 98.33333333333334, 752.3333333333333, 19.0, 28.0678186168422, 1.085367592423671, 0.0, 1.0, 65.0, 20621.90357744412], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.46333333333333343, 0.32777777777777783, 0.8313075506445672, 0.08333333333333333, 0.83898488473685, 0.861789197474557, 0.0, 1.0, 1.0, 0.09819954084497201], 
reward next is 0.9018, 
noisyNet noise sample is [array([-1.0805829], dtype=float32), 0.6372965]. 
=============================================
[2019-04-08 15:52:20,522] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0076226e-16 3.8060802e-10 5.6320918e-08 7.2077469e-08 9.0019485e-11
 1.4523108e-14 1.3307027e-08 7.1684966e-15 4.2537517e-19 8.8725988e-23
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:20,535] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4339
[2019-04-08 15:52:20,549] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.0, 27.0, 101.0, 663.0, 19.0, 27.54145407572707, 0.8964019726834002, 0.0, 1.0, 65.0, 25308.32196165468], 
current ob forecast is [], 
actual action is [16.0, 65.0], 
sim time this is 3663000.0000, 
sim time next is 3663600.0000, 
raw observation next is [11.0, 27.33333333333333, 102.6666666666667, 679.6666666666666, 19.0, 27.54856931424084, 0.9039131942872797, 0.0, 1.0, 65.0, 24668.36574717645], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.27333333333333326, 0.3422222222222223, 0.751012891344383, 0.08333333333333333, 0.79571410952007, 0.8013043980957599, 0.0, 1.0, 1.0, 0.11746840831988785], 
reward next is 0.8825, 
noisyNet noise sample is [array([-1.0096335], dtype=float32), -1.8550165]. 
=============================================
[2019-04-08 15:52:20,638] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9674144e-19 4.2585938e-10 2.7901612e-09 1.8297207e-09 1.1163251e-12
 1.5227726e-14 5.9929783e-11 3.6943342e-16 6.4858444e-21 1.9772927e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:20,640] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1477
[2019-04-08 15:52:20,656] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 65.33333333333334, 0.0, 0.0, 19.0, 27.55586058302746, 0.9126855100964278, 0.0, 1.0, 65.0, 33946.85310716589], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 3705600.0000, 
sim time next is 3706200.0000, 
raw observation next is [1.0, 67.0, 0.0, 0.0, 19.0, 27.51616841702694, 0.9064295953399683, 0.0, 1.0, 65.0, 35390.43712239865], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.67, 0.0, 0.0, 0.08333333333333333, 0.793014034752245, 0.802143198446656, 0.0, 1.0, 1.0, 0.1685258910590412], 
reward next is 0.8315, 
noisyNet noise sample is [array([-0.75634956], dtype=float32), 1.0798442]. 
=============================================
[2019-04-08 15:52:20,729] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6373546e-15 3.6383161e-09 4.8021468e-07 7.9484880e-07 1.8612024e-10
 1.2177402e-12 3.1743823e-09 2.2673999e-13 4.1476590e-17 8.0898833e-20
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:52:20,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7185
[2019-04-08 15:52:20,768] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.5, 60.5, 0.0, 0.0, 19.0, 26.40661497326754, 0.6329652785717351, 0.0, 1.0, 65.0, 52669.63209552689], 
current ob forecast is [], 
actual action is [-6.5, 65.0], 
sim time this is 3979800.0000, 
sim time next is 3980400.0000, 
raw observation next is [-11.66666666666667, 61.33333333333334, 0.0, 0.0, 19.0, 26.34394754659343, 0.6252404436019218, 0.0, 1.0, 65.0, 54636.18201905236], 
processed observation next is [1.0, 0.043478260869565216, 0.13942751615881802, 0.6133333333333334, 0.0, 0.0, 0.08333333333333333, 0.6953289622161192, 0.7084134812006405, 0.0, 1.0, 1.0, 0.26017229532882075], 
reward next is 0.7398, 
noisyNet noise sample is [array([-0.557546], dtype=float32), -0.116532154]. 
=============================================
[2019-04-08 15:52:21,214] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.44609609e-17 4.16714406e-11 1.25421833e-08 6.63799824e-08
 1.19846745e-11 2.22635936e-14 6.57627339e-08 5.27238665e-15
 4.89366416e-20 5.73046575e-21 9.99999881e-01], sum to 1.0000
[2019-04-08 15:52:21,220] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6471
[2019-04-08 15:52:21,234] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.06373793236621, 0.7476684612691056, 0.0, 1.0, 65.0, 42558.65025298343], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3727200.0000, 
sim time next is 3727800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.03404244580097, 0.7542325635462911, 0.0, 1.0, 65.0, 42846.46871767121], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7528368704834142, 0.7514108545154303, 0.0, 1.0, 1.0, 0.20403080341748195], 
reward next is 0.7960, 
noisyNet noise sample is [array([-0.09308718], dtype=float32), 0.50616616]. 
=============================================
[2019-04-08 15:52:22,107] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9926497e-18 3.6064450e-11 1.8345860e-08 1.0708462e-07 1.3401043e-11
 6.6345821e-15 2.5491296e-09 2.2157655e-13 2.1627731e-18 1.7618107e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:22,114] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1710
[2019-04-08 15:52:22,126] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 62.83333333333333, 0.0, 0.0, 19.0, 27.65360422555056, 0.9486318779698468, 0.0, 1.0, 65.0, 32794.62500091436], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 3701400.0000, 
sim time next is 3702000.0000, 
raw observation next is [2.666666666666667, 62.66666666666667, 0.0, 0.0, 19.0, 27.63106087857557, 0.9477516396431357, 0.0, 1.0, 65.0, 33142.05742475151], 
processed observation next is [0.0, 0.8695652173913043, 0.5364727608494922, 0.6266666666666667, 0.0, 0.0, 0.08333333333333333, 0.8025884065479643, 0.8159172132143785, 0.0, 1.0, 1.0, 0.1578193210702453], 
reward next is 0.8422, 
noisyNet noise sample is [array([0.21390033], dtype=float32), 0.012509573]. 
=============================================
[2019-04-08 15:52:22,134] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.66963]
 [85.6519 ]
 [85.64649]
 [85.63133]
 [85.52928]], R is [[85.67396545]
 [85.66106415]
 [85.6499176 ]
 [85.64060974]
 [85.6333847 ]].
[2019-04-08 15:52:22,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1762176e-18 1.7038014e-10 6.5066637e-08 4.4938298e-07 4.1735476e-10
 1.2181693e-11 1.3108689e-08 1.3587618e-14 2.7808587e-19 3.4072946e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:52:22,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9681
[2019-04-08 15:52:22,613] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.88682040582896, 0.7858143965411855, 0.0, 1.0, 65.0, 46780.00375185426], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3806400.0000, 
sim time next is 3807000.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.92458738726149, 0.7821613933992588, 0.0, 1.0, 65.0, 45825.12001413506], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7437156156051241, 0.7607204644664196, 0.0, 1.0, 1.0, 0.21821485721016692], 
reward next is 0.7818, 
noisyNet noise sample is [array([-0.54427433], dtype=float32), -0.22141096]. 
=============================================
[2019-04-08 15:52:22,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.30775 ]
 [82.26528 ]
 [82.228065]
 [82.216934]
 [82.120094]], R is [[82.42393494]
 [82.37693024]
 [82.32279968]
 [82.26819611]
 [82.22425079]].
[2019-04-08 15:52:22,891] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1355619e-17 4.5616635e-10 7.7458331e-08 9.2702788e-09 5.2767131e-11
 3.0059624e-14 3.4226236e-08 6.6169814e-14 8.8830140e-19 1.0356094e-19
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:22,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7741
[2019-04-08 15:52:22,920] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 27.07789125207309, 0.8036179680750465, 0.0, 1.0, 65.0, 41856.84406109616], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3717000.0000, 
sim time next is 3717600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 19.0, 27.07466473424421, 0.8164674246900283, 0.0, 1.0, 65.0, 41410.8025604513], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7562220611870174, 0.7721558082300094, 0.0, 1.0, 1.0, 0.19719429790691095], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.35899615], dtype=float32), -0.40400946]. 
=============================================
[2019-04-08 15:52:23,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3943926e-17 9.9601195e-12 2.3932346e-08 1.7070069e-07 5.6570061e-11
 9.5549321e-14 1.1334911e-09 3.2714533e-14 1.0777479e-17 4.7958066e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:23,112] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7973
[2019-04-08 15:52:23,127] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.08349198580781, 0.7828496263196284, 0.0, 1.0, 65.0, 41944.94124465901], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3722400.0000, 
sim time next is 3723000.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.1141989055148, 0.7756651611123946, 0.0, 1.0, 65.0, 40499.40949465182], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7595165754595667, 0.7585550537041316, 0.0, 1.0, 1.0, 0.19285433092691345], 
reward next is 0.8071, 
noisyNet noise sample is [array([1.1401975], dtype=float32), -0.09389872]. 
=============================================
[2019-04-08 15:52:23,182] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.59828]
 [81.52573]
 [81.73468]
 [81.96271]
 [81.87257]], R is [[81.45632935]
 [81.44203186]
 [81.42243195]
 [81.41046906]
 [81.40882111]].
[2019-04-08 15:52:23,207] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3713941e-18 1.7773590e-11 2.9708360e-09 5.6158726e-08 4.6991421e-12
 1.2084091e-14 7.3505302e-10 6.9298889e-16 6.5784918e-20 2.5605057e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:23,211] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2906
[2019-04-08 15:52:23,272] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 60.0, 75.5, 625.0, 22.5, 28.8237533255779, 1.082658991619949, 1.0, 1.0, 65.0, 25357.23474011511], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3772800.0000, 
sim time next is 3773400.0000, 
raw observation next is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 22.5, 28.07470644265315, 1.215965470045028, 1.0, 1.0, 65.0, 44725.67840507947], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6, 0.23888888888888885, 0.6589318600368324, 0.375, 0.8395588702210958, 0.9053218233483427, 1.0, 1.0, 1.0, 0.21297942097656888], 
reward next is 0.7870, 
noisyNet noise sample is [array([-0.07117188], dtype=float32), 0.14720798]. 
=============================================
[2019-04-08 15:52:23,690] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3650345e-19 3.0855637e-10 1.4104461e-08 1.1224919e-07 6.6704413e-11
 2.0266411e-13 1.1343633e-08 1.1040193e-14 1.9607634e-19 4.4858655e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:23,691] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4069
[2019-04-08 15:52:23,718] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 71.0, 0.0, 0.0, 22.5, 27.24027015454543, 1.0973283958372, 1.0, 1.0, 65.0, 59131.68178170751], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3780600.0000, 
sim time next is 3781200.0000, 
raw observation next is [-2.0, 71.0, 0.0, 0.0, 22.5, 26.82648447500742, 0.9756672273071897, 1.0, 1.0, 65.0, 29537.43717825404], 
processed observation next is [1.0, 0.782608695652174, 0.40720221606648205, 0.71, 0.0, 0.0, 0.375, 0.7355403729172849, 0.8252224091023965, 1.0, 1.0, 1.0, 0.14065446275359067], 
reward next is 0.8593, 
noisyNet noise sample is [array([0.46665356], dtype=float32), -0.02267943]. 
=============================================
[2019-04-08 15:52:23,890] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1386637e-16 8.5557568e-11 1.2887185e-08 1.9995471e-07 3.3985294e-11
 6.7685804e-13 2.0957052e-09 2.8325032e-14 2.6675930e-18 1.9421517e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:23,891] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9405
[2019-04-08 15:52:23,911] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 24.0, 43.5, 370.5, 22.5, 27.7632875051308, 0.9907806151307237, 1.0, 1.0, 65.0, 23161.83745459087], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 4035600.0000, 
sim time next is 4036200.0000, 
raw observation next is [-2.166666666666667, 24.33333333333333, 35.66666666666666, 311.3333333333333, 22.5, 28.16538134882274, 1.003303427167249, 1.0, 1.0, 65.0, 18845.38662183186], 
processed observation next is [1.0, 0.7391304347826086, 0.4025854108956602, 0.2433333333333333, 0.11888888888888886, 0.3440147329650092, 0.375, 0.8471151124018951, 0.8344344757224164, 1.0, 1.0, 1.0, 0.08973993629443744], 
reward next is 0.9103, 
noisyNet noise sample is [array([-1.0058533], dtype=float32), 0.25063094]. 
=============================================
[2019-04-08 15:52:25,200] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.9870419e-16 1.0709868e-08 2.1321453e-06 2.6341728e-05 1.2114311e-09
 9.8969305e-12 2.0725281e-08 7.2894110e-13 1.7580735e-15 2.7327870e-18
 9.9997151e-01], sum to 1.0000
[2019-04-08 15:52:25,200] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1355
[2019-04-08 15:52:25,213] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.88289412026482, 0.801184053289075, 0.0, 1.0, 65.0, 47312.756402851], 
current ob forecast is [], 
actual action is [1.833333333333333, 65.0], 
sim time this is 3802200.0000, 
sim time next is 3802800.0000, 
raw observation next is [-3.333333333333333, 73.0, 0.0, 0.0, 19.0, 26.85933465676278, 0.7972749582413686, 0.0, 1.0, 65.0, 47787.89598897948], 
processed observation next is [1.0, 0.0, 0.37026777469990774, 0.73, 0.0, 0.0, 0.08333333333333333, 0.738277888063565, 0.7657583194137896, 0.0, 1.0, 1.0, 0.22756140947133086], 
reward next is 0.7724, 
noisyNet noise sample is [array([-0.28029394], dtype=float32), -1.0148557]. 
=============================================
[2019-04-08 15:52:25,444] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.7539608e-19 7.9000237e-11 1.2034565e-07 8.2875083e-08 1.1784121e-10
 3.5026498e-13 7.8201817e-10 4.0001250e-15 5.3317576e-19 3.9538850e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:25,445] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2477
[2019-04-08 15:52:25,460] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 19.0, 26.90612550280464, 0.8070042176508778, 0.0, 1.0, 65.0, 46634.7560226961], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3801600.0000, 
sim time next is 3802200.0000, 
raw observation next is [-3.166666666666667, 72.0, 0.0, 0.0, 19.0, 26.88289576044166, 0.8011847228548863, 0.0, 1.0, 65.0, 47312.73158551974], 
processed observation next is [1.0, 0.0, 0.3748845798707295, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7402413133701383, 0.7670615742849621, 0.0, 1.0, 1.0, 0.22529872183580826], 
reward next is 0.7747, 
noisyNet noise sample is [array([0.11721574], dtype=float32), 1.0703608]. 
=============================================
[2019-04-08 15:52:25,769] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0541595e-19 6.4566005e-11 6.2048184e-09 5.2854347e-08 1.1724252e-12
 2.3165099e-14 1.3940818e-10 1.3198314e-15 8.1892839e-20 3.9817665e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:25,770] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6852
[2019-04-08 15:52:25,794] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 60.0, 71.66666666666666, 596.3333333333333, 22.5, 28.07470721773416, 1.215965848623529, 1.0, 1.0, 65.0, 44725.6229479379], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [0.0, 60.00000000000001, 67.83333333333333, 567.6666666666666, 22.5, 27.53002589285174, 1.133770120122697, 1.0, 1.0, 65.0, 22574.74504887097], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.6000000000000001, 0.2261111111111111, 0.627255985267035, 0.375, 0.7941688244043116, 0.8779233733742323, 1.0, 1.0, 1.0, 0.10749878594700461], 
reward next is 0.8925, 
noisyNet noise sample is [array([-0.02840754], dtype=float32), -0.27212304]. 
=============================================
[2019-04-08 15:52:25,802] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1278931e-16 1.0467452e-09 8.9439077e-08 2.0422226e-07 8.3196977e-10
 1.8535457e-13 5.1771664e-08 8.7295670e-14 2.5083816e-18 1.4387179e-18
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:52:25,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6780
[2019-04-08 15:52:25,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.73754]
 [86.83631]
 [86.8943 ]
 [86.96614]
 [86.98707]], R is [[86.69221497]
 [86.61231232]
 [86.6254425 ]
 [86.66943359]
 [86.71298218]].
[2019-04-08 15:52:25,823] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.06584950152611, 0.7586541422201205, 0.0, 1.0, 65.0, 42438.22424254513], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3724800.0000, 
sim time next is 3725400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.01899317526337, 0.7549855357306221, 0.0, 1.0, 65.0, 43524.30849901969], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7515827646052807, 0.7516618452435407, 0.0, 1.0, 1.0, 0.20725861190009376], 
reward next is 0.7927, 
noisyNet noise sample is [array([-1.0219779], dtype=float32), -0.40638453]. 
=============================================
[2019-04-08 15:52:26,033] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6058828e-17 8.8030505e-10 7.6674823e-08 2.3338205e-06 2.6420563e-10
 2.9184298e-12 1.9817257e-07 4.4147903e-14 1.0474557e-17 2.0775016e-19
 9.9999738e-01], sum to 1.0000
[2019-04-08 15:52:26,034] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9959
[2019-04-08 15:52:26,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2468208e-17 3.6408151e-11 1.4655825e-07 1.0851419e-07 1.1316494e-11
 3.9487988e-14 1.4068617e-08 1.9691144e-14 1.1995511e-19 4.8953628e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:26,045] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-08 15:52:26,047] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 22.5, 27.69363774453531, 1.013700358322817, 0.0, 1.0, 65.0, 29170.08905697337], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 3872400.0000, 
sim time next is 3873000.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 22.5, 27.62883763531783, 1.005293117528859, 0.0, 1.0, 65.0, 29935.70737730572], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.375, 0.8024031362764857, 0.8350977058429531, 0.0, 1.0, 1.0, 0.1425509875109796], 
reward next is 0.8574, 
noisyNet noise sample is [array([-0.9027852], dtype=float32), 0.38269112]. 
=============================================
[2019-04-08 15:52:26,054] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.12903 ]
 [81.38614 ]
 [81.85337 ]
 [81.89782 ]
 [81.883865]], R is [[80.82801056]
 [80.88082123]
 [80.93922424]
 [81.00676727]
 [81.07741547]].
[2019-04-08 15:52:26,073] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.04151744883826, 0.7402938150330755, 0.0, 1.0, 65.0, 42621.74171136814], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3730200.0000, 
sim time next is 3730800.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 19.0, 27.03845592766737, 0.7412719291150122, 0.0, 1.0, 65.0, 42354.44335083805], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7532046606389476, 0.7470906430383374, 0.0, 1.0, 1.0, 0.20168782548018122], 
reward next is 0.7983, 
noisyNet noise sample is [array([-1.2506553], dtype=float32), -1.0091668]. 
=============================================
[2019-04-08 15:52:26,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6360564e-18 9.3399008e-13 9.2353991e-10 2.7526255e-07 6.2149938e-12
 3.8520900e-14 2.0789679e-09 4.7885271e-15 3.3282132e-20 1.3145618e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:26,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5801
[2019-04-08 15:52:26,302] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.833333333333334, 76.0, 62.33333333333334, 347.6666666666667, 22.5, 26.47250805469901, 0.6918536941656029, 1.0, 1.0, 65.0, 49747.4050399202], 
current ob forecast is [], 
actual action is [0.16666666666666607, 65.0], 
sim time this is 3831000.0000, 
sim time next is 3831600.0000, 
raw observation next is [-4.666666666666667, 75.0, 76.66666666666667, 397.3333333333333, 22.5, 26.53804256872697, 0.7312522980380125, 1.0, 1.0, 65.0, 46661.60178251661], 
processed observation next is [1.0, 0.34782608695652173, 0.3333333333333333, 0.75, 0.2555555555555556, 0.43904235727440144, 0.375, 0.7115035473939141, 0.7437507660126709, 1.0, 1.0, 1.0, 0.22219810372626958], 
reward next is 0.7778, 
noisyNet noise sample is [array([-0.01283472], dtype=float32), -0.0070594447]. 
=============================================
[2019-04-08 15:52:26,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6873351e-19 1.3043704e-11 5.1156785e-09 1.3874091e-08 2.2730201e-12
 2.0267167e-15 4.2209929e-09 1.6396769e-15 9.2713589e-20 7.3578084e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:26,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2233
[2019-04-08 15:52:26,695] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.5, 49.5, 113.0, 824.0, 22.5, 28.32973878235675, 1.090456095901335, 1.0, 1.0, 65.0, 18847.81293229991], 
current ob forecast is [], 
actual action is [6.5, 65.0], 
sim time this is 3850200.0000, 
sim time next is 3850800.0000, 
raw observation next is [1.666666666666667, 49.0, 111.8333333333333, 817.0, 22.5, 28.43191294414388, 1.107369702738418, 1.0, 1.0, 65.00000000000003, 18848.20510227893], 
processed observation next is [1.0, 0.5652173913043478, 0.5087719298245615, 0.49, 0.37277777777777765, 0.9027624309392265, 0.375, 0.8693260786786565, 0.8691232342461394, 1.0, 1.0, 1.0000000000000007, 0.08975335762989967], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.5068399], dtype=float32), 0.30256572]. 
=============================================
[2019-04-08 15:52:26,888] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1742327e-17 2.4906760e-10 3.0689191e-09 2.6033885e-07 2.7692208e-11
 8.2289827e-14 1.5670807e-08 1.6148632e-14 4.2790611e-18 1.3545945e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:26,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8626
[2019-04-08 15:52:26,925] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 76.0, 0.0, 0.0, 19.0, 27.21618440989452, 0.917851929875919, 0.0, 1.0, 65.0, 38481.99374755751], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 3793800.0000, 
sim time next is 3794400.0000, 
raw observation next is [-3.0, 77.0, 0.0, 0.0, 19.0, 27.20556561155183, 0.9137591408451812, 0.0, 1.0, 65.0, 38575.00027222319], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7671304676293191, 0.804586380281727, 0.0, 1.0, 1.0, 0.18369047748677708], 
reward next is 0.8163, 
noisyNet noise sample is [array([-1.2231089], dtype=float32), 0.94372004]. 
=============================================
[2019-04-08 15:52:27,184] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5186979e-16 2.5967903e-09 4.1589477e-08 8.5673008e-08 6.7352207e-11
 1.3724591e-12 1.3708895e-08 4.7616055e-15 1.5611438e-16 3.2772395e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:27,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7095
[2019-04-08 15:52:27,206] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.73691844220854, 0.719416811983283, 0.0, 1.0, 65.0, 48558.18938513439], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3814800.0000, 
sim time next is 3815400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.7020777488983, 0.7190974291408773, 0.0, 1.0, 65.0, 50155.23517065727], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.725173145741525, 0.739699143046959, 0.0, 1.0, 1.0, 0.23883445319360608], 
reward next is 0.7612, 
noisyNet noise sample is [array([0.14917766], dtype=float32), -0.18298903]. 
=============================================
[2019-04-08 15:52:27,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5197412e-17 1.2994896e-09 4.7447958e-08 3.0698814e-08 9.3282326e-11
 7.7582038e-13 1.3743558e-08 5.6822162e-14 1.0650496e-17 4.4944490e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:27,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6971
[2019-04-08 15:52:27,327] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.98440733984154, 0.8230322122301623, 0.0, 1.0, 65.0, 41446.38049796961], 
current ob forecast is [], 
actual action is [3.0, 65.0], 
sim time this is 3889200.0000, 
sim time next is 3889800.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 19.0, 26.97055034641354, 0.8185268571863085, 0.0, 1.0, 65.0, 41575.0044880283], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.08333333333333333, 0.7475458622011283, 0.7728422857287695, 0.0, 1.0, 1.0, 0.19797621184775382], 
reward next is 0.8020, 
noisyNet noise sample is [array([-2.1125958], dtype=float32), 1.6397738]. 
=============================================
[2019-04-08 15:52:27,599] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.5510344e-17 2.2042729e-10 6.0224792e-08 4.8487630e-07 7.3310615e-11
 4.7814026e-13 1.0675720e-08 1.9532023e-13 2.0587934e-18 1.4405786e-19
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:52:27,600] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0137
[2019-04-08 15:52:27,615] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.92458743294202, 0.7821616888917634, 0.0, 1.0, 65.0, 45825.11469824663], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 19.0, 26.93797931533584, 0.7858291234275597, 0.0, 1.0, 65.0, 46023.87532434372], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.08333333333333333, 0.7448316096113201, 0.76194304114252, 0.0, 1.0, 1.0, 0.21916131106830342], 
reward next is 0.7808, 
noisyNet noise sample is [array([0.21027675], dtype=float32), 0.9748666]. 
=============================================
[2019-04-08 15:52:28,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.9279805e-17 1.5395883e-10 3.8205503e-08 1.6821711e-07 2.0845932e-10
 5.6597852e-13 8.6947614e-09 1.1032066e-14 1.8812936e-19 9.5497598e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:28,094] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6264
[2019-04-08 15:52:28,110] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.6883522743029, 0.711639798025607, 0.0, 1.0, 65.0, 49520.07046406514], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.68588779573862, 0.7048428554329252, 0.0, 1.0, 65.0, 49743.61220590075], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7238239829782183, 0.7349476184776417, 0.0, 1.0, 1.0, 0.2368743438376226], 
reward next is 0.7631, 
noisyNet noise sample is [array([0.77318084], dtype=float32), -0.4744384]. 
=============================================
[2019-04-08 15:52:28,218] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3375583e-19 7.1557689e-14 2.5959883e-09 1.9174148e-08 1.6864199e-11
 5.2826270e-14 1.1492371e-09 7.1250876e-17 5.3388182e-20 4.3847890e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:28,220] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2100
[2019-04-08 15:52:28,236] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.666666666666667, 69.16666666666667, 97.66666666666667, 602.3333333333334, 22.5, 27.36058048334239, 0.8303771473383148, 1.0, 1.0, 65.0, 32880.55489929837], 
current ob forecast is [], 
actual action is [1.333333333333333, 65.0], 
sim time this is 3834600.0000, 
sim time next is 3835200.0000, 
raw observation next is [-3.333333333333333, 67.33333333333334, 99.33333333333333, 641.1666666666667, 22.5, 27.39635533599891, 0.8508984326634862, 1.0, 1.0, 65.0, 31544.35259979541], 
processed observation next is [1.0, 0.391304347826087, 0.37026777469990774, 0.6733333333333335, 0.3311111111111111, 0.7084714548802947, 0.375, 0.7830296113332423, 0.7836328108878288, 1.0, 1.0, 1.0, 0.15021120285616862], 
reward next is 0.8498, 
noisyNet noise sample is [array([0.5969249], dtype=float32), 0.91335124]. 
=============================================
[2019-04-08 15:52:29,226] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4229450e-17 5.3117323e-11 3.1302861e-08 1.3017447e-07 1.9814301e-11
 7.3243027e-14 7.6429236e-09 5.3383015e-15 1.0364710e-19 2.5127249e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:29,228] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9684
[2019-04-08 15:52:29,239] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 41.83333333333334, 0.0, 0.0, 19.0, 27.66622727688193, 0.9954152056733149, 1.0, 1.0, 65.0, 26495.99025229136], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4133400.0000, 
sim time next is 4134000.0000, 
raw observation next is [1.0, 40.66666666666667, 0.0, 0.0, 19.0, 27.63211444615085, 0.9906429883746127, 0.0, 1.0, 65.0, 26984.55575283766], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.40666666666666673, 0.0, 0.0, 0.08333333333333333, 0.8026762038459042, 0.8302143294582042, 0.0, 1.0, 1.0, 0.1284978845373222], 
reward next is 0.8715, 
noisyNet noise sample is [array([-1.4589937], dtype=float32), -0.03228917]. 
=============================================
[2019-04-08 15:52:29,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.32623]
 [80.3445 ]
 [80.55996]
 [80.35486]
 [80.83098]], R is [[80.0689621 ]
 [80.14209747]
 [80.21728516]
 [80.29345703]
 [80.36942291]].
[2019-04-08 15:52:30,046] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0492677e-16 2.1155919e-09 2.6818805e-07 2.1862347e-06 4.7912980e-09
 6.0744604e-12 3.1203720e-07 3.4936539e-13 3.2025277e-17 1.7296710e-18
 9.9999726e-01], sum to 1.0000
[2019-04-08 15:52:30,047] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0560
[2019-04-08 15:52:30,065] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 39.0, 0.0, 0.0, 19.0, 27.01543934846036, 0.8005392914846164, 0.0, 1.0, 65.0, 40132.86756089787], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4150800.0000, 
sim time next is 4151400.0000, 
raw observation next is [-1.166666666666667, 40.16666666666666, 0.0, 0.0, 19.0, 26.99730149709908, 0.7953473652709633, 0.0, 1.0, 65.0, 40546.17948951383], 
processed observation next is [0.0, 0.043478260869565216, 0.43028624192059095, 0.40166666666666656, 0.0, 0.0, 0.08333333333333333, 0.7497751247582567, 0.7651157884236545, 0.0, 1.0, 1.0, 0.1930770451881611], 
reward next is 0.8069, 
noisyNet noise sample is [array([0.18290815], dtype=float32), 0.3648707]. 
=============================================
[2019-04-08 15:52:30,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7633298e-17 6.6217642e-11 4.6103100e-08 2.1746805e-08 1.1627604e-11
 8.6214341e-14 1.9479280e-09 2.7819969e-14 3.4886182e-19 6.4016307e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:30,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4818
[2019-04-08 15:52:30,754] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.73691978533926, 0.7194173315106324, 0.0, 1.0, 65.0, 48558.16979671571], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 3814800.0000, 
sim time next is 3815400.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.70207907938948, 0.7190979411361599, 0.0, 1.0, 65.0, 50155.21431161846], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7251732566157901, 0.7396993137120532, 0.0, 1.0, 1.0, 0.2388343538648498], 
reward next is 0.7612, 
noisyNet noise sample is [array([0.22562908], dtype=float32), 0.9937963]. 
=============================================
[2019-04-08 15:52:30,754] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.64107637e-16 5.32723643e-10 5.83973758e-08 4.03614536e-07
 1.68663469e-11 1.29020763e-13 1.00032738e-08 1.35109421e-14
 3.09767352e-18 1.00267086e-20 9.99999523e-01], sum to 1.0000
[2019-04-08 15:52:30,758] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2982
[2019-04-08 15:52:30,786] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.5, 61.0, 6.0, 163.0, 22.5, 26.59051751082855, 0.6685053806591353, 1.0, 1.0, 65.0, 48839.70844182469], 
current ob forecast is [], 
actual action is [-2.5, 65.0], 
sim time this is 3915000.0000, 
sim time next is 3915600.0000, 
raw observation next is [-7.666666666666666, 60.0, 20.16666666666666, 213.5, 22.5, 26.53364401008082, 0.6959473179149755, 1.0, 1.0, 65.0, 48294.81437856804], 
processed observation next is [1.0, 0.30434782608695654, 0.25023084025854114, 0.6, 0.0672222222222222, 0.23591160220994475, 0.375, 0.7111370008400684, 0.7319824393049918, 1.0, 1.0, 1.0, 0.2299753065646097], 
reward next is 0.7700, 
noisyNet noise sample is [array([0.05596264], dtype=float32), 0.15181643]. 
=============================================
[2019-04-08 15:52:30,994] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.07641524e-17 3.57572666e-10 1.28408175e-07 4.31189555e-08
 1.99668459e-11 4.57572077e-13 1.23133965e-08 4.18712484e-15
 8.13231607e-19 4.39863475e-19 9.99999762e-01], sum to 1.0000
[2019-04-08 15:52:30,997] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8468
[2019-04-08 15:52:31,011] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.0, 56.33333333333334, 76.83333333333334, 415.5000000000001, 22.5, 26.68918578033499, 0.7428711342541684, 1.0, 1.0, 65.0, 45574.0343592327], 
current ob forecast is [], 
actual action is [-3.0, 65.0], 
sim time this is 3918000.0000, 
sim time next is 3918600.0000, 
raw observation next is [-8.0, 55.5, 91.0, 466.0, 22.5, 26.86258904413104, 0.7516196964659243, 1.0, 1.0, 65.0, 39622.50951227548], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.555, 0.30333333333333334, 0.5149171270718232, 0.375, 0.7385490870109201, 0.7505398988219748, 1.0, 1.0, 1.0, 0.18867861672512132], 
reward next is 0.8113, 
noisyNet noise sample is [array([1.0598036], dtype=float32), -0.46089715]. 
=============================================
[2019-04-08 15:52:32,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1867812e-17 2.4234190e-10 2.0581271e-07 5.8126568e-07 2.1575847e-10
 1.4757665e-13 1.9074360e-09 7.9798688e-16 1.2394093e-19 1.6213779e-21
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:52:32,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0965
[2019-04-08 15:52:32,355] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.666666666666666, 60.0, 20.16666666666666, 213.5, 22.5, 26.53364394192522, 0.6959472818101703, 1.0, 1.0, 65.0, 48294.81594567785], 
current ob forecast is [], 
actual action is [-2.666666666666666, 65.0], 
sim time this is 3915600.0000, 
sim time next is 3916200.0000, 
raw observation next is [-7.833333333333334, 59.0, 34.33333333333333, 264.0, 22.5, 26.63997183715276, 0.6917760084336586, 1.0, 1.0, 65.0, 45366.12860670809], 
processed observation next is [1.0, 0.30434782608695654, 0.2456140350877193, 0.59, 0.11444444444444443, 0.29171270718232045, 0.375, 0.7199976530960633, 0.7305920028112195, 1.0, 1.0, 1.0, 0.21602918384146708], 
reward next is 0.7840, 
noisyNet noise sample is [array([0.6347909], dtype=float32), -0.50790113]. 
=============================================
[2019-04-08 15:52:32,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2488935e-18 2.9246074e-11 1.8546005e-09 2.5537739e-08 8.7963846e-12
 6.7871557e-14 5.0178212e-10 1.3665460e-15 3.6627695e-19 5.7336153e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:32,871] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5058
[2019-04-08 15:52:32,887] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.666666666666667, 39.33333333333333, 144.6666666666667, 540.0, 19.0, 27.58644305396282, 0.9610573348633737, 0.0, 1.0, 65.0, 27239.28230022901], 
current ob forecast is [], 
actual action is [7.666666666666667, 65.0], 
sim time this is 4203600.0000, 
sim time next is 4204200.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 19.0, 27.58639798717769, 0.9627167482329305, 0.0, 1.0, 65.0, 27635.76059118546], 
processed observation next is [0.0, 0.6521739130434783, 0.541089566020314, 0.3816666666666667, 0.43111111111111095, 0.5988950276243094, 0.08333333333333333, 0.7988664989314742, 0.8209055827443102, 0.0, 1.0, 1.0, 0.131598859958026], 
reward next is 0.8684, 
noisyNet noise sample is [array([-0.4733579], dtype=float32), -1.0570723]. 
=============================================
[2019-04-08 15:52:33,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.3290891e-17 2.2410193e-10 1.8404445e-07 1.5339174e-07 1.6332194e-10
 2.4263488e-13 7.6590174e-09 2.1018963e-14 2.0034525e-18 4.6301881e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:52:33,188] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5317
[2019-04-08 15:52:33,209] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.5, 43.0, 0.0, 0.0, 22.5, 27.22915817052668, 0.8919993931486875, 1.0, 1.0, 65.0, 37523.76532028595], 
current ob forecast is [], 
actual action is [-1.5, 65.0], 
sim time this is 3958200.0000, 
sim time next is 3958800.0000, 
raw observation next is [-6.666666666666666, 43.66666666666667, 0.0, 0.0, 22.5, 27.17695898661479, 0.8858248210549523, 1.0, 1.0, 65.0, 38951.90612620614], 
processed observation next is [1.0, 0.8260869565217391, 0.2779316712834719, 0.4366666666666667, 0.0, 0.0, 0.375, 0.764746582217899, 0.7952749403516508, 1.0, 1.0, 1.0, 0.18548526726764827], 
reward next is 0.8145, 
noisyNet noise sample is [array([0.35871327], dtype=float32), 1.2666677]. 
=============================================
[2019-04-08 15:52:33,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1850275e-17 1.3766042e-11 6.5033881e-08 4.0160759e-07 8.9166743e-11
 1.8136680e-12 9.3103703e-10 1.2429680e-15 4.6073580e-18 8.0050654e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:52:33,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1519
[2019-04-08 15:52:33,575] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-8.0, 40.0, 115.5, 798.5, 22.5, 27.13075079258966, 0.4666750367644539, 1.0, 1.0, 65.0, 31984.01788362298], 
current ob forecast is [], 
actual action is [-3.0, 65.0], 
sim time this is 4014000.0000, 
sim time next is 4014600.0000, 
raw observation next is [-7.666666666666667, 39.5, 116.6666666666667, 804.3333333333334, 22.5, 27.06819717954944, 0.8234599116658785, 1.0, 1.0, 65.0, 47056.56665149134], 
processed observation next is [1.0, 0.4782608695652174, 0.2502308402585411, 0.395, 0.388888888888889, 0.8887661141804789, 0.375, 0.7556830982957866, 0.7744866372219595, 1.0, 1.0, 1.0, 0.22407888881662544], 
reward next is 0.7759, 
noisyNet noise sample is [array([-1.015771], dtype=float32), -0.8425315]. 
=============================================
[2019-04-08 15:52:34,059] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8551767e-16 8.1997423e-11 3.0594812e-08 4.5415928e-07 8.0499879e-10
 4.4115108e-13 2.3104837e-08 8.2398069e-13 2.3888436e-18 1.1546005e-19
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:52:34,060] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4695
[2019-04-08 15:52:34,083] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 19.0, 27.12092482800691, 0.779951809491048, 0.0, 1.0, 65.0, 39264.99920299229], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4228200.0000, 
sim time next is 4228800.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 19.0, 27.11758417630116, 0.7736161841218815, 0.0, 1.0, 65.0, 38805.45732560844], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.08333333333333333, 0.7597986813584301, 0.7578720613739605, 0.0, 1.0, 1.0, 0.18478789202670687], 
reward next is 0.8152, 
noisyNet noise sample is [array([1.5007246], dtype=float32), -1.5320828]. 
=============================================
[2019-04-08 15:52:34,362] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5221049e-17 2.4652724e-10 1.1672553e-08 6.7464292e-08 3.2928372e-11
 3.4723908e-13 5.3534905e-09 1.6334160e-14 7.1754891e-18 2.0701011e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:52:34,364] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6122
[2019-04-08 15:52:34,397] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4970732e-16 1.0170725e-10 1.5212793e-08 1.8127077e-08 1.6439015e-11
 9.4910039e-13 4.3467328e-09 3.9906458e-15 4.3092540e-17 1.2371855e-19
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:34,398] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 19.0, 27.0487025482288, 0.8625425605531021, 0.0, 1.0, 65.0, 40472.71918953781], 
current ob forecast is [], 
actual action is [-2.0, 65.0], 
sim time this is 3960600.0000, 
sim time next is 3961200.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 19.0, 27.01613335254651, 0.8561597354125771, 0.0, 1.0, 65.0, 40928.67855702861], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7513444460455426, 0.785386578470859, 0.0, 1.0, 1.0, 0.19489846931918384], 
reward next is 0.8051, 
noisyNet noise sample is [array([0.41511524], dtype=float32), -2.3810678]. 
=============================================
[2019-04-08 15:52:34,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8114
[2019-04-08 15:52:34,428] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.666666666666667, 40.0, 15.83333333333333, 140.8333333333333, 22.5, 27.63023003606965, 1.00485702684754, 1.0, 1.0, 65.00000000000003, 30150.78498635103], 
current ob forecast is [], 
actual action is [-0.666666666666667, 65.0], 
sim time this is 3951600.0000, 
sim time next is 3952200.0000, 
raw observation next is [-5.833333333333333, 40.5, 0.0, 0.0, 22.5, 27.69675946815357, 0.9754138190698791, 1.0, 1.0, 65.0, 23319.90250174875], 
processed observation next is [1.0, 0.7391304347826086, 0.30101569713758086, 0.405, 0.0, 0.0, 0.375, 0.8080632890127974, 0.8251379396899597, 1.0, 1.0, 1.0, 0.11104715477023215], 
reward next is 0.8890, 
noisyNet noise sample is [array([0.6249055], dtype=float32), 0.06997905]. 
=============================================
[2019-04-08 15:52:34,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.24522982e-16 2.72495937e-09 1.12328246e-06 4.78053107e-06
 1.92471816e-10 1.00996529e-11 2.78514335e-07 3.07617918e-13
 4.67728523e-16 1.03249586e-17 9.99993801e-01], sum to 1.0000
[2019-04-08 15:52:34,808] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3900
[2019-04-08 15:52:34,835] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 22.5, 27.40477711588778, 0.920700082201443, 1.0, 1.0, 65.0, 34279.36939301339], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3956400.0000, 
sim time next is 3957000.0000, 
raw observation next is [-6.166666666666666, 41.66666666666667, 0.0, 0.0, 22.5, 27.33651777856283, 0.9144092124781101, 1.0, 1.0, 65.0, 35895.06819285089], 
processed observation next is [1.0, 0.8260869565217391, 0.29178208679593726, 0.41666666666666674, 0.0, 0.0, 0.375, 0.7780431482135691, 0.8048030708260367, 1.0, 1.0, 1.0, 0.17092889615643284], 
reward next is 0.8291, 
noisyNet noise sample is [array([1.74413], dtype=float32), 0.41779086]. 
=============================================
[2019-04-08 15:52:34,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.17071 ]
 [77.24737 ]
 [77.30239 ]
 [77.40136 ]
 [77.487206]], R is [[77.26795197]
 [77.33203125]
 [77.39776611]
 [77.46554565]
 [77.54258728]].
[2019-04-08 15:52:34,960] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4670304e-17 9.4003583e-10 1.5188746e-07 5.8179261e-08 3.1241492e-11
 4.4038175e-14 5.5793872e-09 1.1460381e-14 1.6753026e-17 3.8714299e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:34,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6917
[2019-04-08 15:52:34,977] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 22.5, 25.76780799510792, 0.4779226410920822, 1.0, 1.0, 65.0, 57314.1865816127], 
current ob forecast is [], 
actual action is [-8.33333333333333, 65.0], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 22.5, 25.82250983108868, 0.4914220662679353, 1.0, 1.0, 65.0, 54190.35419768944], 
processed observation next is [1.0, 0.30434782608695654, 0.09787626962142189, 0.64, 0.10333333333333329, 0.16353591160220995, 0.375, 0.65187581925739, 0.663807355422645, 1.0, 1.0, 1.0, 0.25804930570328305], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.12495787], dtype=float32), -0.2763705]. 
=============================================
[2019-04-08 15:52:35,012] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [6.2030000e-19 7.8847318e-11 7.9342977e-09 8.6642853e-09 2.7109323e-13
 1.1077656e-14 4.5345294e-10 2.0238803e-16 1.3159576e-19 9.3974352e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:52:35,013] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5312
[2019-04-08 15:52:35,053] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 49.0, 119.6666666666667, 818.3333333333334, 22.5, 27.71861180775357, 1.019561938793225, 1.0, 1.0, 65.0, 43692.67070118309], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 3930600.0000, 
sim time next is 3931200.0000, 
raw observation next is [-6.0, 49.0, 119.5, 822.5, 22.5, 27.30377102365897, 0.9496029129553688, 1.0, 1.0, 65.0, 32071.18641665576], 
processed observation next is [1.0, 0.5217391304347826, 0.296398891966759, 0.49, 0.3983333333333333, 0.9088397790055248, 0.375, 0.7753142519715809, 0.8165343043184562, 1.0, 1.0, 1.0, 0.15271993531740838], 
reward next is 0.8473, 
noisyNet noise sample is [array([0.68641883], dtype=float32), 0.83811927]. 
=============================================
[2019-04-08 15:52:35,418] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.44673567e-16 4.61178734e-10 1.29687237e-07 9.12282360e-07
 6.16257826e-12 8.53533823e-12 1.57345266e-08 1.12756226e-13
 1.04207287e-17 3.62107360e-19 9.99998927e-01], sum to 1.0000
[2019-04-08 15:52:35,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8049
[2019-04-08 15:52:35,450] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 22.5, 27.41980962958363, 0.8844626039009972, 1.0, 1.0, 65.0, 30372.27748540226], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 4043400.0000, 
sim time next is 4044000.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 22.5, 27.35417820036622, 0.8714136958635489, 0.0, 1.0, 65.0, 32269.65543426703], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.375, 0.7795148500305183, 0.7904712319545163, 0.0, 1.0, 1.0, 0.15366502587746206], 
reward next is 0.8463, 
noisyNet noise sample is [array([0.6086072], dtype=float32), -0.1214249]. 
=============================================
[2019-04-08 15:52:35,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.96223]
 [75.99867]
 [76.00317]
 [76.07804]
 [76.10125]], R is [[75.62822723]
 [75.72731781]
 [75.82342529]
 [75.91719818]
 [76.01259613]].
[2019-04-08 15:52:35,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.11495387e-16 1.02098765e-08 3.86768733e-08 2.60530555e-07
 1.51616442e-10 2.70695277e-13 8.35373015e-09 4.82637137e-14
 1.05583087e-16 1.84107411e-19 9.99999642e-01], sum to 1.0000
[2019-04-08 15:52:35,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-08 15:52:35,670] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-9.0, 53.0, 0.0, 0.0, 19.0, 26.54818975868458, 0.714018409582378, 0.0, 1.0, 65.0, 51704.79988758541], 
current ob forecast is [], 
actual action is [-4.0, 65.0], 
sim time this is 3970800.0000, 
sim time next is 3971400.0000, 
raw observation next is [-9.166666666666666, 53.83333333333334, 0.0, 0.0, 19.0, 26.51528526409263, 0.7066435646129353, 0.0, 1.0, 65.0, 52296.36646757289], 
processed observation next is [1.0, 1.0, 0.208679593721145, 0.5383333333333334, 0.0, 0.0, 0.08333333333333333, 0.7096071053410524, 0.7355478548709784, 0.0, 1.0, 1.0, 0.24903031651225185], 
reward next is 0.7510, 
noisyNet noise sample is [array([-0.72864074], dtype=float32), 0.16511603]. 
=============================================
[2019-04-08 15:52:36,913] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5224421e-14 7.9204765e-10 7.0873784e-07 2.6627181e-06 7.3399042e-09
 1.9641214e-12 8.8912387e-08 3.3837887e-12 4.3315544e-17 7.0279571e-19
 9.9999654e-01], sum to 1.0000
[2019-04-08 15:52:36,915] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.3970
[2019-04-08 15:52:36,943] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-11.0, 58.0, 0.0, 0.0, 19.0, 26.39623764910243, 0.6554715350192772, 0.0, 1.0, 65.0, 52961.05478675665], 
current ob forecast is [], 
actual action is [-6.0, 65.0], 
sim time this is 3978000.0000, 
sim time next is 3978600.0000, 
raw observation next is [-11.16666666666667, 58.83333333333334, 0.0, 0.0, 19.0, 26.41467538342362, 0.654504020553898, 0.0, 1.0, 65.0, 52796.84626464153], 
processed observation next is [1.0, 0.043478260869565216, 0.1532779316712834, 0.5883333333333334, 0.0, 0.0, 0.08333333333333333, 0.7012229486186351, 0.7181680068512993, 0.0, 1.0, 1.0, 0.25141355364115014], 
reward next is 0.7486, 
noisyNet noise sample is [array([0.45048165], dtype=float32), -1.7422153]. 
=============================================
[2019-04-08 15:52:37,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5435026e-15 6.6463119e-09 1.2690850e-06 2.6396217e-05 2.4351038e-10
 5.8669853e-12 1.5356304e-07 1.3338343e-12 1.6211157e-16 5.1351918e-18
 9.9997222e-01], sum to 1.0000
[2019-04-08 15:52:37,129] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5032
[2019-04-08 15:52:37,152] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.5, 66.0, 0.0, 0.0, 19.0, 25.94274508650592, 0.4916785146014755, 0.0, 1.0, 65.0, 55888.15041641339], 
current ob forecast is [], 
actual action is [-8.5, 65.0], 
sim time this is 3997800.0000, 
sim time next is 3998400.0000, 
raw observation next is [-13.66666666666667, 67.0, 0.0, 0.0, 19.0, 25.90401871175538, 0.4918749104717886, 0.0, 1.0, 65.0, 56817.97406601672], 
processed observation next is [1.0, 0.2608695652173913, 0.08402585410895651, 0.67, 0.0, 0.0, 0.08333333333333333, 0.658668225979615, 0.6639583034905961, 0.0, 1.0, 1.0, 0.2705617812667463], 
reward next is 0.7294, 
noisyNet noise sample is [array([1.176161], dtype=float32), -1.1819922]. 
=============================================
[2019-04-08 15:52:37,441] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.08468174e-16 4.41064130e-11 2.48947227e-08 1.73526288e-07
 4.72525352e-11 2.47468577e-13 4.56429596e-08 1.45771406e-14
 9.04765108e-18 1.04598365e-19 9.99999762e-01], sum to 1.0000
[2019-04-08 15:52:37,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4713
[2019-04-08 15:52:37,459] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-13.33333333333333, 65.0, 15.5, 73.99999999999999, 22.5, 25.76780788940059, 0.4779226097799181, 1.0, 1.0, 65.0, 57314.18705751064], 
current ob forecast is [], 
actual action is [-8.33333333333333, 65.0], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [-13.16666666666667, 64.0, 30.99999999999999, 148.0, 22.5, 25.82250972053371, 0.4914220358916532, 1.0, 1.0, 65.0, 54190.35482711504], 
processed observation next is [1.0, 0.30434782608695654, 0.09787626962142189, 0.64, 0.10333333333333329, 0.16353591160220995, 0.375, 0.6518758100444758, 0.6638073452972177, 1.0, 1.0, 1.0, 0.2580493087005478], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.27805808], dtype=float32), 1.0620378]. 
=============================================
[2019-04-08 15:52:38,406] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.7978611e-15 4.6786703e-09 1.8796927e-06 1.6463473e-06 1.1390326e-10
 4.1285751e-12 3.3576416e-08 1.7152055e-13 6.5183520e-17 1.0796367e-18
 9.9999642e-01], sum to 1.0000
[2019-04-08 15:52:38,408] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8023
[2019-04-08 15:52:38,421] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.226271964658, 0.6010138401921649, 0.0, 1.0, 65.0, 54434.7775375275], 
current ob forecast is [], 
actual action is [-7.0, 65.0], 
sim time this is 3985200.0000, 
sim time next is 3985800.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 19.0, 26.2889599868756, 0.5981362739115151, 0.0, 1.0, 65.0, 52163.63490710726], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.63, 0.0, 0.0, 0.08333333333333333, 0.6907466655729667, 0.699378757970505, 0.0, 1.0, 1.0, 0.24839826146241553], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.02497253], dtype=float32), -0.124733105]. 
=============================================
[2019-04-08 15:52:38,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6162193e-16 5.1339244e-10 7.1319718e-08 9.5657015e-08 2.1959982e-10
 2.3507574e-12 2.2179188e-08 7.9403762e-14 1.7655748e-17 3.1712166e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:52:38,671] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3486
[2019-04-08 15:52:38,689] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.0, 31.0, 0.0, 0.0, 19.0, 26.9533438866804, 0.7937342072463909, 0.0, 1.0, 65.0, 38169.13519526967], 
current ob forecast is [], 
actual action is [0.0, 65.0], 
sim time this is 4053600.0000, 
sim time next is 4054200.0000, 
raw observation next is [-5.166666666666667, 32.0, 0.0, 0.0, 19.0, 26.94094751960001, 0.7468499665544409, 0.0, 1.0, 65.0, 39482.0519079942], 
processed observation next is [1.0, 0.9565217391304348, 0.31948291782086796, 0.32, 0.0, 0.0, 0.08333333333333333, 0.7450789599666674, 0.7489499888514803, 0.0, 1.0, 1.0, 0.18800977099044858], 
reward next is 0.8120, 
noisyNet noise sample is [array([-0.8993491], dtype=float32), 1.1945462]. 
=============================================
[2019-04-08 15:52:38,742] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-08 15:52:38,742] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:52:38,742] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:52:38,743] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:52:38,744] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:52:38,746] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run20
[2019-04-08 15:52:38,743] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:52:38,766] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:52:38,779] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run20
[2019-04-08 15:52:38,795] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run20
[2019-04-08 15:53:15,399] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.064866945]
[2019-04-08 15:53:15,399] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [17.48670290666667, 74.66864504666667, 175.26911595, 0.0, 19.0, 28.26216565582334, 1.264084325598972, 0.0, 1.0, 65.0, 18843.96645729787]
[2019-04-08 15:53:15,399] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:53:15,401] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [1.9956404e-20 3.6305165e-12 2.2297100e-09 4.3163624e-09 1.0406128e-12
 8.6688830e-16 3.7260556e-10 4.0771349e-17 2.2560826e-21 5.1738981e-24
 1.0000000e+00], sampled 0.28943428112079916
[2019-04-08 15:54:14,795] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.064866945]
[2019-04-08 15:54:14,795] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation this: [20.6, 40.66666666666667, 0.0, 0.0, 19.0, 31.21489175214795, 2.040170393891871, 0.0, 1.0, 65.0, 0.0]
[2019-04-08 15:54:14,796] A3C_EVAL-Part4-Light-Pit-Test-v3 DEBUG:Observation forecast: []
[2019-04-08 15:54:14,796] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Softmax [3.5641048e-21 1.7684615e-12 1.1905517e-09 1.7392146e-09 3.6450021e-13
 2.5604064e-16 1.3830219e-10 9.2597394e-18 4.0253227e-22 6.4969248e-25
 1.0000000e+00], sampled 0.035016274242415935
[2019-04-08 15:54:17,874] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00935303], dtype=float32), 0.064866945]
[2019-04-08 15:54:17,874] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation this: [-0.3986760506666666, 63.20587235333333, 0.0, 0.0, 22.5, 27.69027299197482, 1.072706007482495, 0.0, 1.0, 65.0, 26510.48091235859]
[2019-04-08 15:54:17,874] A3C_EVAL-Part4-Light-Pit-Test-v4 DEBUG:Observation forecast: []
[2019-04-08 15:54:17,875] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Softmax [6.6081772e-18 1.1996047e-10 2.6699329e-08 5.3793379e-08 2.2490090e-11
 8.4763339e-14 3.1354350e-09 3.9920921e-15 7.1300280e-19 2.7136213e-21
 9.9999988e-01], sampled 0.7883350651928952
[2019-04-08 15:54:20,006] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation: average rewards by now are 6990.3889 316178329.3323 2958.2103
[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,028] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:20,166] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,007] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation: average rewards by now are 6801.0806 355933080.1029 2370.4616
[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,041] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:25,158] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,557] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation: average rewards by now are 6863.3272 342861293.3630 2768.3516
[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,578] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:*** stack smashing detected ***: /home/zhiangz/Documents/HVAC-RL-Control/src/eplus-env/eplus_env/envs/EnergyPlus-8-3-0//energyplus terminated

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:29,695] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread-EPLUSPROCESS_EPI_0 ERROR:Aborted (core dumped)

[2019-04-08 15:54:30,580] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 380000, evaluation results [380000.0, 6863.327174461987, 342861293.36298394, 2768.351608256195, 6990.388907941344, 316178329.33231246, 2958.21033464435, 6801.080570938545, 355933080.10290486, 2370.4616298649803]
[2019-04-08 15:54:32,224] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5128736e-15 5.6006561e-10 5.0280647e-07 1.4990443e-06 1.2051155e-10
 1.4572578e-11 4.4527518e-08 6.9961842e-13 3.3401663e-18 9.9445008e-19
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:54:32,225] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7862
[2019-04-08 15:54:32,245] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 19.0, 26.45024971770934, 0.5936200606522779, 0.0, 1.0, 65.0, 48839.81026051663], 
current ob forecast is [], 
actual action is [0.6666666666666661, 65.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 19.0, 26.4144692627244, 0.6055093913689173, 0.0, 1.0, 65.0, 49516.30465079378], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.08333333333333333, 0.7012057718936999, 0.7018364637896392, 0.0, 1.0, 1.0, 0.23579192690854184], 
reward next is 0.7642, 
noisyNet noise sample is [array([-2.4640076], dtype=float32), -0.30485165]. 
=============================================
[2019-04-08 15:54:33,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1514128e-18 9.0552996e-11 5.0660609e-09 7.1118649e-09 5.0798606e-12
 1.1398428e-14 1.5526180e-09 2.7417149e-14 2.9189449e-19 1.8628469e-20
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:33,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1826
[2019-04-08 15:54:33,121] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 22.5, 28.65733505160696, 1.119626899528148, 1.0, 1.0, 65.0, 18849.30216586641], 
current ob forecast is [], 
actual action is [8.333333333333332, 65.0], 
sim time this is 4112400.0000, 
sim time next is 4113000.0000, 
raw observation next is [3.5, 33.0, 106.0, 794.0, 22.5, 28.68217742774611, 1.114023657343446, 1.0, 1.0, 65.0, 9424.673967063312], 
processed observation next is [1.0, 0.6086956521739131, 0.5595567867036012, 0.33, 0.35333333333333333, 0.8773480662983425, 0.375, 0.8901814523121759, 0.8713412191144819, 1.0, 1.0, 1.0, 0.044879399843158634], 
reward next is 0.9551, 
noisyNet noise sample is [array([0.03177615], dtype=float32), -1.9168452]. 
=============================================
[2019-04-08 15:54:33,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.32732 ]
 [86.10166 ]
 [86.123825]
 [86.000374]
 [85.80926 ]], R is [[86.54679871]
 [86.59156799]
 [86.7256546 ]
 [86.85839844]
 [86.90006256]].
[2019-04-08 15:54:33,484] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.68359925e-17 1.64186539e-10 1.23992848e-07 2.28128094e-08
 1.15863666e-10 4.23213880e-14 1.05803890e-08 1.93904510e-14
 2.09574949e-20 7.03035616e-22 9.99999881e-01], sum to 1.0000
[2019-04-08 15:54:33,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9735
[2019-04-08 15:54:33,509] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 32.0, 117.5, 792.5, 22.5, 27.3705092828206, 0.8999053613788585, 1.0, 1.0, 65.0, 26399.83257704634], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 22.5, 27.76664953390155, 0.6804148517350921, 1.0, 1.0, 65.0, 22721.16065757699], 
processed observation next is [1.0, 0.4782608695652174, 0.44413665743305636, 0.3133333333333334, 0.39555555555555566, 0.8843462246777164, 0.375, 0.8138874611584624, 0.726804950578364, 1.0, 1.0, 1.0, 0.10819600313131901], 
reward next is 0.8918, 
noisyNet noise sample is [array([-0.10703062], dtype=float32), -0.8776959]. 
=============================================
[2019-04-08 15:54:33,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[84.06776]
 [83.89573]
 [83.74685]
 [83.77432]
 [83.91296]], R is [[84.20911407]
 [84.24131012]
 [84.20491028]
 [84.24349976]
 [84.29982758]].
[2019-04-08 15:54:34,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4420240e-16 3.4026124e-09 3.7807402e-07 1.7034869e-06 1.5784339e-09
 5.1305786e-13 5.1290261e-08 2.8877359e-14 1.8914524e-17 7.2656221e-19
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:54:34,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7349
[2019-04-08 15:54:34,263] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.833333333333334, 40.5, 0.0, 0.0, 19.0, 26.40883684906632, 0.5689217350454436, 0.0, 1.0, 65.0, 48147.92260875308], 
current ob forecast is [], 
actual action is [0.16666666666666607, 65.0], 
sim time this is 4085400.0000, 
sim time next is 4086000.0000, 
raw observation next is [-5.0, 41.0, 0.0, 0.0, 19.0, 26.39646895192225, 0.5673034605001551, 0.0, 1.0, 65.0, 48597.12373896732], 
processed observation next is [1.0, 0.30434782608695654, 0.32409972299168976, 0.41, 0.0, 0.0, 0.08333333333333333, 0.699705745993521, 0.6891011535000517, 0.0, 1.0, 1.0, 0.23141487494746343], 
reward next is 0.7686, 
noisyNet noise sample is [array([-0.17164801], dtype=float32), -0.9652258]. 
=============================================
[2019-04-08 15:54:34,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.69708 ]
 [77.73308 ]
 [77.73672 ]
 [77.76107 ]
 [77.745735]], R is [[77.66973114]
 [77.66375732]
 [77.65583801]
 [77.64656067]
 [77.63988495]].
[2019-04-08 15:54:34,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0593924e-15 8.1153390e-11 9.9157695e-08 1.7228626e-06 1.6605764e-10
 4.2187289e-14 1.7105053e-08 6.3459397e-14 3.8510109e-18 2.9863795e-21
 9.9999821e-01], sum to 1.0000
[2019-04-08 15:54:34,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1117
[2019-04-08 15:54:34,692] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 35.5, 0.0, 0.0, 22.5, 27.82991359775797, 1.041044266166612, 1.0, 1.0, 65.0, 18844.71675511317], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4127400.0000, 
sim time next is 4128000.0000, 
raw observation next is [3.0, 36.0, 0.0, 0.0, 22.5, 28.00759601252912, 1.043786003612967, 1.0, 1.0, 65.00000000000003, 18844.52982859938], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.36, 0.0, 0.0, 0.375, 0.8339663343774267, 0.8479286678709889, 1.0, 1.0, 1.0000000000000007, 0.08973585632666373], 
reward next is 0.9103, 
noisyNet noise sample is [array([-1.1869454], dtype=float32), 0.9306768]. 
=============================================
[2019-04-08 15:54:34,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[84.751564]
 [84.88134 ]
 [85.07128 ]
 [85.35034 ]
 [85.70472 ]], R is [[84.74285126]
 [84.80568695]
 [84.81994629]
 [84.7413559 ]
 [84.77417755]].
[2019-04-08 15:54:35,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9369044e-16 3.6975420e-10 1.4116715e-05 2.3683675e-07 3.4526382e-11
 2.4431580e-13 1.0931660e-07 3.4102614e-14 1.4964442e-16 8.7429351e-19
 9.9998558e-01], sum to 1.0000
[2019-04-08 15:54:35,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9986
[2019-04-08 15:54:35,072] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 40.5, 0.0, 0.0, 19.0, 27.02091096588869, 0.8151982555944293, 0.0, 1.0, 65.0, 40052.95476247363], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4149000.0000, 
sim time next is 4149600.0000, 
raw observation next is [-1.0, 40.0, 0.0, 0.0, 19.0, 27.03070344330058, 0.8109365011067462, 0.0, 1.0, 65.0, 39713.80806020986], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.4, 0.0, 0.0, 0.08333333333333333, 0.7525586202750484, 0.770312167035582, 0.0, 1.0, 1.0, 0.18911337171528506], 
reward next is 0.8109, 
noisyNet noise sample is [array([1.9259769], dtype=float32), 0.69674337]. 
=============================================
[2019-04-08 15:54:35,210] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.6459679e-17 1.5760454e-09 6.5065784e-08 1.0424926e-07 3.2501692e-11
 1.0404598e-12 8.7874461e-08 2.7236073e-14 3.6439301e-17 1.9071836e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:54:35,213] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3357
[2019-04-08 15:54:35,222] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 40.5, 0.0, 0.0, 19.0, 27.45314799299561, 0.8967139410779064, 0.0, 1.0, 65.0, 30620.87213524834], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 4140600.0000, 
sim time next is 4141200.0000, 
raw observation next is [0.6666666666666667, 41.0, 0.0, 0.0, 19.0, 27.41908483592662, 0.8950554264618386, 0.0, 1.0, 65.0, 35510.1704643515], 
processed observation next is [1.0, 0.9565217391304348, 0.4810710987996307, 0.41, 0.0, 0.0, 0.08333333333333333, 0.7849237363272182, 0.7983518088206129, 0.0, 1.0, 1.0, 0.16909604983024523], 
reward next is 0.8309, 
noisyNet noise sample is [array([0.91740555], dtype=float32), -1.5429659]. 
=============================================
[2019-04-08 15:54:36,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1344733e-16 3.5186856e-10 2.0744851e-08 2.0440498e-06 8.2496295e-11
 8.3593885e-13 2.3640265e-08 3.7198123e-14 1.8206373e-17 2.6220348e-19
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:54:36,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7939
[2019-04-08 15:54:36,930] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.166666666666667, 49.83333333333334, 0.0, 0.0, 19.0, 26.55428721065459, 0.6641073839476787, 0.0, 1.0, 65.0, 47233.92961892734], 
current ob forecast is [], 
actual action is [0.833333333333333, 65.0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [-4.333333333333334, 49.66666666666667, 0.0, 0.0, 19.0, 26.56385761595025, 0.6583373488931284, 0.0, 1.0, 65.0, 46817.37100254557], 
processed observation next is [0.0, 0.2608695652173913, 0.3425669436749769, 0.4966666666666667, 0.0, 0.0, 0.08333333333333333, 0.7136548013291876, 0.7194457829643762, 0.0, 1.0, 1.0, 0.22293986191688367], 
reward next is 0.7771, 
noisyNet noise sample is [array([0.83698225], dtype=float32), 1.2892559]. 
=============================================
[2019-04-08 15:54:36,937] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.261856]
 [79.32074 ]
 [79.36625 ]
 [79.4281  ]
 [79.46477 ]], R is [[79.20823669]
 [79.19123077]
 [79.17546844]
 [79.16069031]
 [79.14730835]].
[2019-04-08 15:54:37,288] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1742457e-19 1.4034783e-10 7.6909625e-09 7.5663253e-09 1.7104861e-11
 2.3679539e-15 1.4156040e-09 1.0135762e-15 5.4397605e-19 4.3594094e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:37,296] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2103
[2019-04-08 15:54:37,312] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.80893610037719, 0.7328644553170772, 0.0, 1.0, 65.0, 43215.66741025044], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 4159800.0000, 
sim time next is 4160400.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.81207776391271, 0.725475311652715, 0.0, 1.0, 65.0, 43005.2586855616], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7343398136593926, 0.7418251038842384, 0.0, 1.0, 1.0, 0.2047869461217219], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.01062403], dtype=float32), 0.19032691]. 
=============================================
[2019-04-08 15:54:37,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3111344e-18 1.7418147e-11 4.3167305e-09 4.1251232e-07 4.2715404e-11
 3.3529299e-15 4.1614058e-08 1.3128566e-14 1.7555191e-19 2.8423095e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:54:37,365] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7602
[2019-04-08 15:54:37,390] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3333333333333333, 42.66666666666667, 0.0, 0.0, 19.0, 27.12539418473007, 0.8481655764317945, 0.0, 1.0, 65.0, 38359.54515015886], 
current ob forecast is [], 
actual action is [4.666666666666667, 65.0], 
sim time this is 4144800.0000, 
sim time next is 4145400.0000, 
raw observation next is [-0.5, 42.5, 0.0, 0.0, 19.0, 27.11856435675448, 0.8433379074595221, 0.0, 1.0, 65.0, 38641.01093356761], 
processed observation next is [1.0, 1.0, 0.44875346260387816, 0.425, 0.0, 0.0, 0.08333333333333333, 0.7598803630628733, 0.7811126358198407, 0.0, 1.0, 1.0, 0.18400481396936957], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.64090157], dtype=float32), -0.083954796]. 
=============================================
[2019-04-08 15:54:37,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3648917e-15 1.9136541e-09 1.7130509e-07 1.7535801e-06 2.1850395e-10
 1.6086582e-12 4.0970082e-08 1.7473571e-13 1.0087290e-16 1.8997955e-19
 9.9999797e-01], sum to 1.0000
[2019-04-08 15:54:37,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2407
[2019-04-08 15:54:37,682] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 39.5, 0.0, 0.0, 19.0, 27.03043918788967, 0.8054523259742261, 0.0, 1.0, 65.0, 39646.67701534266], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4150200.0000, 
sim time next is 4150800.0000, 
raw observation next is [-1.0, 39.0, 0.0, 0.0, 19.0, 27.01542893572643, 0.8005367302279601, 0.0, 1.0, 65.0, 40132.94633793214], 
processed observation next is [0.0, 0.043478260869565216, 0.4349030470914128, 0.39, 0.0, 0.0, 0.08333333333333333, 0.7512857446438691, 0.7668455767426533, 0.0, 1.0, 1.0, 0.19110926827586733], 
reward next is 0.8089, 
noisyNet noise sample is [array([1.2835492], dtype=float32), 0.3665775]. 
=============================================
[2019-04-08 15:54:37,955] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.55985332e-19 2.90822557e-11 5.14298426e-09 4.29261870e-09
 4.98611569e-11 1.69939136e-14 1.78873016e-09 1.06847270e-16
 2.97606169e-19 1.02316515e-20 1.00000000e+00], sum to 1.0000
[2019-04-08 15:54:37,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8990
[2019-04-08 15:54:37,978] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 27.01251175962018, 0.7277387206584184, 0.0, 1.0, 65.0, 41739.90936091598], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4241400.0000, 
sim time next is 4242000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 27.01121841144168, 0.7257825101035564, 0.0, 1.0, 65.0, 41790.71488244028], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7509348676201398, 0.7419275033678522, 0.0, 1.0, 1.0, 0.19900340420209658], 
reward next is 0.8010, 
noisyNet noise sample is [array([0.07383508], dtype=float32), -1.6320224]. 
=============================================
[2019-04-08 15:54:37,986] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.544334]
 [81.397385]
 [81.23003 ]
 [81.259155]
 [81.2615  ]], R is [[81.47805786]
 [81.46451569]
 [81.45085144]
 [81.43706512]
 [81.423172  ]].
[2019-04-08 15:54:38,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7201836e-18 1.1833760e-11 4.7158144e-09 8.6168050e-09 2.9396909e-11
 1.5044327e-13 6.4497017e-09 2.5915166e-16 1.7369887e-18 1.1630474e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:38,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9462
[2019-04-08 15:54:38,062] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 19.0, 26.99908272260538, 0.7170976928335046, 0.0, 1.0, 65.0, 41624.79868559692], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4245600.0000, 
sim time next is 4246200.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 19.0, 26.99321231094991, 0.7149230637615028, 0.0, 1.0, 65.0, 41803.80786017149], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.08333333333333333, 0.7494343592458259, 0.7383076879205009, 0.0, 1.0, 1.0, 0.19906575171510232], 
reward next is 0.8009, 
noisyNet noise sample is [array([0.2166975], dtype=float32), -1.6849262]. 
=============================================
[2019-04-08 15:54:38,511] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.10775816e-16 2.11085663e-10 1.04988134e-08 1.93463120e-06
 1.06704222e-10 8.31938210e-14 1.09521758e-09 2.77109302e-14
 3.23549758e-19 1.56457174e-20 9.99998093e-01], sum to 1.0000
[2019-04-08 15:54:38,514] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8203
[2019-04-08 15:54:38,555] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 38.33333333333334, 0.0, 0.0, 19.0, 27.58392262744819, 0.9790445386847244, 0.0, 1.0, 65.0, 27436.44293744992], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4135200.0000, 
sim time next is 4135800.0000, 
raw observation next is [1.0, 37.16666666666666, 0.0, 0.0, 19.0, 27.56040956280867, 0.9741177692842, 0.0, 1.0, 65.0, 27815.25937274935], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.3716666666666666, 0.0, 0.0, 0.08333333333333333, 0.7967007969007224, 0.8247059230947333, 0.0, 1.0, 1.0, 0.1324536160607112], 
reward next is 0.8675, 
noisyNet noise sample is [array([-0.994002], dtype=float32), -0.024217088]. 
=============================================
[2019-04-08 15:54:38,775] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0323757e-18 1.0572008e-10 9.3719716e-09 3.7980932e-08 2.6271825e-11
 9.0740403e-15 3.9832666e-09 5.1566417e-15 5.7244641e-19 1.0726933e-20
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:38,776] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7845
[2019-04-08 15:54:38,798] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 19.0, 26.69273004870274, 0.6980425509838706, 0.0, 1.0, 65.0, 45598.03974191991], 
current ob forecast is [], 
actual action is [1.333333333333333, 65.0], 
sim time this is 4164000.0000, 
sim time next is 4164600.0000, 
raw observation next is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 19.0, 26.66934102819782, 0.6948865113448638, 0.0, 1.0, 65.0, 45731.25333205223], 
processed observation next is [0.0, 0.17391304347826086, 0.3564173591874424, 0.5333333333333333, 0.0, 0.0, 0.08333333333333333, 0.7224450856831517, 0.7316288371149545, 0.0, 1.0, 1.0, 0.21776787300977252], 
reward next is 0.7822, 
noisyNet noise sample is [array([1.7964908], dtype=float32), 0.8745458]. 
=============================================
[2019-04-08 15:54:38,909] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7691540e-18 1.8444172e-11 6.2816618e-08 1.1466041e-07 4.1598214e-11
 5.9516237e-12 1.3200968e-09 8.2350712e-15 4.1759342e-19 1.5811332e-21
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:54:38,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2105
[2019-04-08 15:54:38,931] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.05, 72.0, 0.0, 0.0, 19.0, 27.14459701446239, 0.8879571499679734, 0.0, 1.0, 65.0, 40666.83356455177], 
current ob forecast is [], 
actual action is [4.95, 65.0], 
sim time this is 4486200.0000, 
sim time next is 4486800.0000, 
raw observation next is [-0.09999999999999999, 72.0, 0.0, 0.0, 19.0, 27.12759064325262, 0.8892451494899997, 0.0, 1.0, 65.0, 45107.30388653851], 
processed observation next is [1.0, 0.9565217391304348, 0.4598337950138504, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7606325536043851, 0.7964150498299999, 0.0, 1.0, 1.0, 0.2147966851739929], 
reward next is 0.7852, 
noisyNet noise sample is [array([-1.7308878], dtype=float32), -0.8158516]. 
=============================================
[2019-04-08 15:54:39,645] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2766780e-17 2.0863724e-09 4.6760121e-08 2.1942704e-07 2.9466735e-11
 1.2467402e-13 4.3977177e-09 5.4918961e-15 1.3730858e-18 4.8663914e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:54:39,650] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3180
[2019-04-08 15:54:39,664] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.333333333333333, 39.0, 0.0, 0.0, 22.5, 27.88579496300187, 1.030046326907969, 0.0, 1.0, 65.0, 22389.20152305675], 
current ob forecast is [], 
actual action is [7.333333333333333, 65.0], 
sim time this is 4130400.0000, 
sim time next is 4131000.0000, 
raw observation next is [2.0, 40.0, 0.0, 0.0, 22.5, 27.83247169110527, 1.020999605392797, 1.0, 1.0, 65.0, 23857.43332675454], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.4, 0.0, 0.0, 0.375, 0.819372640925439, 0.840333201797599, 1.0, 1.0, 1.0, 0.11360682536549781], 
reward next is 0.8864, 
noisyNet noise sample is [array([-0.46100456], dtype=float32), 0.43523496]. 
=============================================
[2019-04-08 15:54:39,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.4746 ]
 [84.99214]
 [85.00599]
 [84.99448]
 [85.04029]], R is [[84.67582703]
 [84.72245789]
 [84.76803589]
 [84.81647491]
 [84.87254333]].
[2019-04-08 15:54:39,713] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3432653e-18 5.3523188e-12 1.1044799e-08 1.3254365e-08 5.9436726e-12
 3.0284858e-15 1.6385939e-09 3.2426999e-16 8.8875573e-19 6.6976745e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:39,729] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2411
[2019-04-08 15:54:39,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.3093137e-18 3.4892200e-10 3.6056094e-09 9.0908060e-08 3.9772929e-12
 3.2226993e-15 1.1164928e-09 4.7566469e-16 9.5788859e-19 4.7098858e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:39,739] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.166666666666667, 39.5, 53.33333333333334, 421.0, 19.0, 27.77641394451997, 0.9668989311077277, 0.0, 1.0, 65.0, 26569.73556506002], 
current ob forecast is [], 
actual action is [7.166666666666667, 65.0], 
sim time this is 4207800.0000, 
sim time next is 4208400.0000, 
raw observation next is [2.0, 40.0, 46.0, 356.5, 19.0, 27.71859585477354, 0.9580156490043118, 0.0, 1.0, 65.0, 27456.62009734242], 
processed observation next is [0.0, 0.7391304347826086, 0.518005540166205, 0.4, 0.15333333333333332, 0.3939226519337017, 0.08333333333333333, 0.809882987897795, 0.8193385496681039, 0.0, 1.0, 1.0, 0.13074580998734486], 
reward next is 0.8693, 
noisyNet noise sample is [array([-1.1753815], dtype=float32), 0.33167446]. 
=============================================
[2019-04-08 15:54:39,743] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8855
[2019-04-08 15:54:39,761] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 40.0, 200.5, 379.0, 19.0, 27.42442824506785, 0.9242659896837164, 0.0, 1.0, 65.0, 26939.32131836147], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4197600.0000, 
sim time next is 4198200.0000, 
raw observation next is [2.0, 40.66666666666667, 196.0, 282.3333333333333, 19.0, 27.48157082431651, 0.9165467633346691, 0.0, 1.0, 65.0, 27351.06275332748], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.40666666666666673, 0.6533333333333333, 0.31197053406998154, 0.08333333333333333, 0.7901309020263758, 0.8055155877782231, 0.0, 1.0, 1.0, 0.13024315596822608], 
reward next is 0.8698, 
noisyNet noise sample is [array([0.63862276], dtype=float32), -1.2441306]. 
=============================================
[2019-04-08 15:54:40,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0681459e-16 4.0950637e-10 2.3412872e-07 1.8349795e-07 2.7044027e-09
 1.9101592e-13 2.8760340e-07 1.1687971e-14 5.7999039e-17 3.1578864e-20
 9.9999928e-01], sum to 1.0000
[2019-04-08 15:54:40,064] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5920
[2019-04-08 15:54:40,079] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 19.0, 27.58639439695866, 0.962715643701615, 0.0, 1.0, 65.0, 27635.79463638031], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 4204200.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 19.0, 27.58786161535953, 0.9705532357842928, 0.0, 1.0, 65.0, 27686.9593287073], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 0.08333333333333333, 0.7989884679466274, 0.823517745261431, 0.0, 1.0, 1.0, 0.13184266347003476], 
reward next is 0.8682, 
noisyNet noise sample is [array([-1.2866809], dtype=float32), 1.4496307]. 
=============================================
[2019-04-08 15:54:40,181] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [3.2175933e-17 8.7165958e-10 3.7213357e-07 1.2418290e-06 1.2912010e-09
 1.9870944e-13 5.7331810e-09 2.4098098e-14 5.9234745e-19 3.0293047e-20
 9.9999833e-01], sum to 1.0000
[2019-04-08 15:54:40,184] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.8284
[2019-04-08 15:54:40,199] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.166666666666667, 49.83333333333334, 0.0, 0.0, 19.0, 26.55428481626509, 0.6641067345101087, 0.0, 1.0, 65.0, 47233.95291173921], 
current ob forecast is [], 
actual action is [0.833333333333333, 65.0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [-4.333333333333334, 49.66666666666667, 0.0, 0.0, 19.0, 26.56385523340786, 0.6583367043144895, 0.0, 1.0, 65.0, 46817.39440535093], 
processed observation next is [0.0, 0.2608695652173913, 0.3425669436749769, 0.4966666666666667, 0.0, 0.0, 0.08333333333333333, 0.7136546027839884, 0.7194455681048298, 0.0, 1.0, 1.0, 0.22293997335881396], 
reward next is 0.7771, 
noisyNet noise sample is [array([-0.5852549], dtype=float32), 1.371728]. 
=============================================
[2019-04-08 15:54:40,227] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[80.45157]
 [80.51195]
 [80.5576 ]
 [80.61908]
 [80.65596]], R is [[80.38657379]
 [80.35778809]
 [80.33036041]
 [80.30403137]
 [80.27921295]].
[2019-04-08 15:54:40,310] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7060190e-18 1.3093758e-10 2.1878320e-07 2.1920570e-07 6.8283205e-12
 1.0708037e-13 1.2829106e-08 5.5817560e-14 1.1209485e-19 6.7984553e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:54:40,314] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7238
[2019-04-08 15:54:40,332] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.95, 40.16666666666667, 38.66666666666666, 292.0, 19.0, 27.69676533836049, 0.9452175395336501, 0.0, 1.0, 65.0, 28199.73699619577], 
current ob forecast is [], 
actual action is [6.95, 65.0], 
sim time this is 4209000.0000, 
sim time next is 4209600.0000, 
raw observation next is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 19.0, 27.65111068079219, 0.9349360088992613, 0.0, 1.0, 65.0, 28739.86443585446], 
processed observation next is [0.0, 0.7391304347826086, 0.515235457063712, 0.40333333333333343, 0.10444444444444442, 0.2513812154696133, 0.08333333333333333, 0.8042592233993492, 0.8116453362997538, 0.0, 1.0, 1.0, 0.13685649731359267], 
reward next is 0.8631, 
noisyNet noise sample is [array([0.02973365], dtype=float32), -0.77255696]. 
=============================================
[2019-04-08 15:54:40,906] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.0379279e-15 3.9108977e-10 2.1468665e-07 9.0487970e-07 8.1155327e-10
 2.8195558e-12 1.5156232e-07 2.5661271e-13 1.2334905e-17 8.0134964e-20
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:54:40,908] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.9359
[2019-04-08 15:54:40,921] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.666666666666667, 38.33333333333334, 109.0, 679.0, 19.0, 26.7934405281325, 0.7493826633164321, 0.0, 1.0, 65.0, 38220.92322012779], 
current ob forecast is [], 
actual action is [2.333333333333333, 65.0], 
sim time this is 4182000.0000, 
sim time next is 4182600.0000, 
raw observation next is [-2.333333333333333, 36.66666666666666, 110.0, 698.0, 19.0, 26.7874978079185, 0.7565667034763459, 0.0, 1.0, 65.0, 38292.47665678518], 
processed observation next is [0.0, 0.391304347826087, 0.3979686057248385, 0.3666666666666666, 0.36666666666666664, 0.7712707182320442, 0.08333333333333333, 0.7322914839932082, 0.752188901158782, 0.0, 1.0, 1.0, 0.18234512693707228], 
reward next is 0.8177, 
noisyNet noise sample is [array([-2.8205533], dtype=float32), -0.06475042]. 
=============================================
[2019-04-08 15:54:40,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4838198e-18 1.9935126e-10 1.3352175e-09 5.2636713e-09 9.1978647e-11
 3.2044114e-14 9.1944186e-09 1.1704622e-14 5.1715857e-18 1.7153030e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:40,950] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1944
[2019-04-08 15:54:40,963] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 19.0, 27.20618813974949, 0.8112244400230119, 0.0, 1.0, 65.0, 37721.76065282337], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4222200.0000, 
sim time next is 4222800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 19.0, 27.17916101591363, 0.8060092438374267, 0.0, 1.0, 65.0, 37833.40617040379], 
processed observation next is [0.0, 0.9130434782608695, 0.4903047091412743, 0.43, 0.0, 0.0, 0.08333333333333333, 0.764930084659469, 0.7686697479458089, 0.0, 1.0, 1.0, 0.1801590770019228], 
reward next is 0.8198, 
noisyNet noise sample is [array([-0.837258], dtype=float32), 1.8838892]. 
=============================================
[2019-04-08 15:54:41,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5967396e-17 1.8509052e-11 4.3573170e-08 1.2558250e-07 1.1189742e-09
 8.2036273e-14 2.9013536e-08 6.9573768e-14 1.7071649e-18 7.2288354e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:54:41,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2456
[2019-04-08 15:54:41,093] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.78273685091627, 0.7236927600595898, 0.0, 1.0, 65.0, 44197.08034757814], 
current ob forecast is [], 
actual action is [2.0, 65.0], 
sim time this is 4161000.0000, 
sim time next is 4161600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 19.0, 26.7747677284684, 0.7219274871541663, 0.0, 1.0, 65.0, 44317.43646616609], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.5, 0.0, 0.0, 0.08333333333333333, 0.7312306440390334, 0.7406424957180554, 0.0, 1.0, 1.0, 0.21103541174364804], 
reward next is 0.7890, 
noisyNet noise sample is [array([0.33763933], dtype=float32), 0.2623916]. 
=============================================
[2019-04-08 15:54:41,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0597409e-18 8.2155836e-12 1.3699067e-08 5.4474526e-08 1.1246235e-12
 3.0770003e-14 3.6426531e-09 2.6284150e-15 1.7061290e-19 2.4911471e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:41,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1687
[2019-04-08 15:54:41,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 52.33333333333334, 134.0, 79.0, 19.0, 26.94009593312909, 0.7234398165862775, 0.0, 1.0, 65.0, 40198.87149360616], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4265400.0000, 
sim time next is 4266000.0000, 
raw observation next is [3.0, 53.0, 146.0, 92.0, 19.0, 26.96337282679499, 0.7281761136408532, 0.0, 1.0, 65.0, 39851.54652131419], 
processed observation next is [0.0, 0.391304347826087, 0.5457063711911359, 0.53, 0.4866666666666667, 0.10165745856353592, 0.08333333333333333, 0.7469477355662493, 0.7427253712136177, 0.0, 1.0, 1.0, 0.18976926914911518], 
reward next is 0.8102, 
noisyNet noise sample is [array([0.07696475], dtype=float32), 0.0785403]. 
=============================================
[2019-04-08 15:54:41,267] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[84.843864]
 [84.56204 ]
 [84.244026]
 [83.948   ]
 [83.626595]], R is [[85.11289978]
 [85.07035065]
 [85.0273056 ]
 [84.98005676]
 [84.9317627 ]].
[2019-04-08 15:54:42,179] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.5707545e-20 9.3955149e-11 3.9931571e-08 1.2814884e-07 1.6064951e-10
 4.3474982e-15 6.0554441e-09 8.0164569e-16 1.8257440e-19 5.5282978e-24
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:42,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-08 15:54:42,195] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.966666666666667, 57.5, 215.3333333333333, 495.6666666666666, 19.0, 27.82556482128299, 1.01277341556336, 0.0, 1.0, 65.0, 23880.82137031133], 
current ob forecast is [], 
actual action is [11.966666666666667, 65.0], 
sim time this is 4284600.0000, 
sim time next is 4285200.0000, 
raw observation next is [6.933333333333334, 58.00000000000001, 222.1666666666667, 440.3333333333334, 19.0, 27.84764106100436, 1.016056170279135, 0.0, 1.0, 65.0, 24050.75264247652], 
processed observation next is [0.0, 0.6086956521739131, 0.6546629732225301, 0.5800000000000001, 0.7405555555555557, 0.4865561694290977, 0.08333333333333333, 0.8206367550836967, 0.838685390093045, 0.0, 1.0, 1.0, 0.11452739353560247], 
reward next is 0.8855, 
noisyNet noise sample is [array([0.8535804], dtype=float32), 0.7912762]. 
=============================================
[2019-04-08 15:54:42,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0632227e-17 1.8931268e-10 9.1591005e-08 7.5604142e-08 5.3388419e-11
 3.0812858e-15 3.1807967e-10 2.1991997e-15 3.4789169e-19 5.6681375e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:42,446] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2326
[2019-04-08 15:54:42,459] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 19.0, 27.09880716943504, 0.8317762487776288, 0.0, 1.0, 65.0, 30977.45235137198], 
current ob forecast is [], 
actual action is [4.666666666666667, 65.0], 
sim time this is 4188000.0000, 
sim time next is 4188600.0000, 
raw observation next is [0.0, 32.5, 119.0, 822.0, 19.0, 27.11462665445281, 0.8378627221220453, 0.0, 1.0, 65.0, 31352.51012325244], 
processed observation next is [0.0, 0.4782608695652174, 0.46260387811634357, 0.325, 0.39666666666666667, 0.9082872928176795, 0.08333333333333333, 0.7595522212044008, 0.7792875740406817, 0.0, 1.0, 1.0, 0.14929766725358304], 
reward next is 0.8507, 
noisyNet noise sample is [array([0.52970606], dtype=float32), -0.913344]. 
=============================================
[2019-04-08 15:54:42,641] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.7624663e-17 1.1573070e-10 9.0459000e-09 5.1463815e-08 3.4433506e-10
 2.8929143e-13 1.4445219e-08 2.4039593e-15 1.5492186e-17 1.3380430e-19
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:42,650] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6194
[2019-04-08 15:54:42,665] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.5, 46.5, 0.0, 0.0, 19.0, 27.01447795143376, 0.7325468929978989, 0.0, 1.0, 65.0, 41904.05143257799], 
current ob forecast is [], 
actual action is [7.5, 65.0], 
sim time this is 4239000.0000, 
sim time next is 4239600.0000, 
raw observation next is [2.666666666666667, 46.0, 0.0, 0.0, 19.0, 27.01247605741975, 0.7313794802706327, 0.0, 1.0, 65.0, 41900.43018425594], 
processed observation next is [0.0, 0.043478260869565216, 0.5364727608494922, 0.46, 0.0, 0.0, 0.08333333333333333, 0.7510396714516459, 0.7437931600902109, 0.0, 1.0, 1.0, 0.1995258580202664], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.00265862], dtype=float32), 0.53372884]. 
=============================================
[2019-04-08 15:54:42,813] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6362270e-16 7.0441042e-10 2.6140208e-08 2.2745903e-08 1.1598512e-09
 1.3639051e-12 8.0368636e-09 1.1788634e-15 6.4041600e-17 7.7542229e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:42,818] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6534
[2019-04-08 15:54:42,831] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 19.0, 26.88792739446377, 0.6868607449915048, 0.0, 1.0, 65.0, 42466.65346817528], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4258800.0000, 
sim time next is 4259400.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.0, 26.88314821853661, 0.6857377615882253, 0.0, 1.0, 65.0, 42511.90491163877], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7402623515447176, 0.7285792538627418, 0.0, 1.0, 1.0, 0.20243764243637508], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.1897539], dtype=float32), 0.2530331]. 
=============================================
[2019-04-08 15:54:43,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4609004e-20 1.2931365e-11 7.4548341e-09 8.7923393e-09 5.1888771e-14
 1.6428709e-14 9.2944591e-10 2.5404133e-16 1.0664828e-21 2.3107096e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:43,018] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4321
[2019-04-08 15:54:43,027] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.866666666666667, 57.33333333333334, 77.66666666666666, 572.5, 19.0, 28.03964008712195, 1.068645106433735, 0.0, 1.0, 65.0, 22604.73582232391], 
current ob forecast is [], 
actual action is [11.866666666666667, 65.0], 
sim time this is 4292400.0000, 
sim time next is 4293000.0000, 
raw observation next is [6.8, 58.0, 70.0, 556.0, 19.0, 28.04665437161505, 1.068614237914287, 0.0, 1.0, 65.0, 22837.19057721344], 
processed observation next is [0.0, 0.6956521739130435, 0.6509695290858727, 0.58, 0.23333333333333334, 0.6143646408839779, 0.08333333333333333, 0.8372211976345874, 0.8562047459714289, 0.0, 1.0, 1.0, 0.10874852655815924], 
reward next is 0.8913, 
noisyNet noise sample is [array([0.11723369], dtype=float32), 0.08877047]. 
=============================================
[2019-04-08 15:54:43,051] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[90.70922 ]
 [90.85436 ]
 [91.01014 ]
 [91.149284]
 [91.25092 ]], R is [[90.50132751]
 [90.48867035]
 [90.47806549]
 [90.46679688]
 [90.45137024]].
[2019-04-08 15:54:43,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9580716e-19 9.5798203e-11 5.8714921e-08 3.9675875e-08 5.0139337e-11
 3.5012059e-14 1.3067454e-08 3.4806098e-15 7.3191867e-19 4.9234877e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:43,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1577
[2019-04-08 15:54:43,765] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.6, 71.66666666666667, 0.0, 0.0, 19.0, 27.7038451372688, 0.95282198480942, 0.0, 1.0, 65.0, 31356.00999287303], 
current ob forecast is [], 
actual action is [10.6, 65.0], 
sim time this is 4304400.0000, 
sim time next is 4305000.0000, 
raw observation next is [5.500000000000001, 72.33333333333333, 0.0, 0.0, 19.0, 27.68677275560348, 0.9486219580815495, 0.0, 1.0, 65.0, 31625.52213611687], 
processed observation next is [0.0, 0.8260869565217391, 0.6149584487534627, 0.7233333333333333, 0.0, 0.0, 0.08333333333333333, 0.8072310629669568, 0.8162073193605165, 0.0, 1.0, 1.0, 0.15059772445769937], 
reward next is 0.8494, 
noisyNet noise sample is [array([-0.8991208], dtype=float32), -0.29394355]. 
=============================================
[2019-04-08 15:54:43,799] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[88.020004]
 [87.85724 ]
 [87.80936 ]
 [87.67486 ]
 [87.62135 ]], R is [[88.11257172]
 [88.08213043]
 [88.05334473]
 [88.02628326]
 [88.0009613 ]].
[2019-04-08 15:54:44,059] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.09907087e-19 6.41557432e-11 3.45375839e-09 9.96108707e-09
 1.07822944e-13 7.85217436e-16 4.65028654e-10 1.14297660e-16
 1.03673551e-20 3.53721325e-24 1.00000000e+00], sum to 1.0000
[2019-04-08 15:54:44,061] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3958
[2019-04-08 15:54:44,074] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.8, 58.0, 70.0, 556.0, 19.0, 28.04665455434085, 1.068614290268624, 0.0, 1.0, 65.0, 22837.18897981583], 
current ob forecast is [], 
actual action is [11.8, 65.0], 
sim time this is 4293000.0000, 
sim time next is 4293600.0000, 
raw observation next is [6.733333333333333, 58.66666666666667, 62.33333333333334, 501.3333333333334, 19.0, 28.05360165826878, 1.065051710492302, 0.0, 1.0, 65.0, 22871.70632512272], 
processed observation next is [0.0, 0.6956521739130435, 0.649122807017544, 0.5866666666666667, 0.2077777777777778, 0.5539594843462248, 0.08333333333333333, 0.8378001381890648, 0.8550172368307672, 0.0, 1.0, 1.0, 0.10891288726248914], 
reward next is 0.8911, 
noisyNet noise sample is [array([0.01753841], dtype=float32), 0.7224322]. 
=============================================
[2019-04-08 15:54:44,617] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [1.1475281e-17 4.7208237e-10 9.2329252e-08 6.5149479e-08 8.8327740e-10
 1.9997556e-12 8.4269494e-08 2.2448867e-14 7.8974387e-17 1.3591552e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:54:44,620] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5232
[2019-04-08 15:54:44,637] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 48.33333333333334, 0.0, 0.0, 19.0, 26.93334767116274, 0.6962835830198454, 0.0, 1.0, 65.0, 42159.67458085183], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4254600.0000, 
sim time next is 4255200.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.0, 26.92720011994048, 0.6945323897052704, 0.0, 1.0, 65.0, 42219.74242926925], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7439333433283734, 0.7315107965684234, 0.0, 1.0, 1.0, 0.20104639252032977], 
reward next is 0.7990, 
noisyNet noise sample is [array([-0.07949986], dtype=float32), -0.2750098]. 
=============================================
[2019-04-08 15:54:44,699] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9007336e-20 5.0137693e-12 6.9025252e-10 1.9050606e-08 3.2262717e-12
 1.2251228e-15 1.0421703e-08 5.5100196e-18 9.6572101e-21 1.3961322e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:44,700] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0444
[2019-04-08 15:54:44,720] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.3, 38.0, 115.0, 780.0, 22.5, 28.88489542357835, 1.164988960905053, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [16.3, 65.0], 
sim time this is 4357800.0000, 
sim time next is 4358400.0000, 
raw observation next is [11.73333333333333, 36.66666666666666, 115.8333333333333, 788.0, 22.5, 28.88788455034224, 1.188245280253619, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7876269621421976, 0.3666666666666666, 0.386111111111111, 0.8707182320441988, 0.375, 0.90732371252852, 0.8960817600845395, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.49278], dtype=float32), -0.4789911]. 
=============================================
[2019-04-08 15:54:44,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2798581e-19 2.4304189e-10 1.8631782e-07 2.8512213e-07 2.2676637e-11
 4.0457729e-14 2.0135953e-08 6.4123740e-16 1.0867050e-19 4.2008856e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:54:44,907] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3899
[2019-04-08 15:54:44,921] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.333333333333333, 67.33333333333334, 157.1666666666667, 452.6666666666667, 22.5, 27.5021017918179, 0.9042928317692273, 1.0, 1.0, 65.0, 26767.11656757599], 
current ob forecast is [], 
actual action is [3.666666666666667, 65.0], 
sim time this is 4612800.0000, 
sim time next is 4613400.0000, 
raw observation next is [-1.0, 65.5, 164.0, 509.0, 22.5, 27.58468416409653, 0.928814145062819, 1.0, 1.0, 65.0, 22198.02970218093], 
processed observation next is [1.0, 0.391304347826087, 0.4349030470914128, 0.655, 0.5466666666666666, 0.5624309392265193, 0.375, 0.7987236803413774, 0.8096047150209397, 1.0, 1.0, 1.0, 0.10570490334371872], 
reward next is 0.8943, 
noisyNet noise sample is [array([0.68808204], dtype=float32), 1.117159]. 
=============================================
[2019-04-08 15:54:44,943] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6086233e-19 2.7844539e-11 1.3873456e-08 1.6901032e-08 3.9128908e-12
 8.5908216e-14 1.1477782e-09 1.0870685e-15 2.6723690e-19 5.3658634e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:44,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6824
[2019-04-08 15:54:44,953] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.033333333333333, 68.16666666666666, 0.0, 0.0, 19.0, 27.79463985923788, 0.9746729564655795, 0.0, 1.0, 65.0, 29775.68612627094], 
current ob forecast is [], 
actual action is [11.033333333333333, 65.0], 
sim time this is 4301400.0000, 
sim time next is 4302000.0000, 
raw observation next is [6.0, 69.0, 0.0, 0.0, 19.0, 27.77476795779654, 0.97026463036004, 0.0, 1.0, 65.0, 30140.17875199663], 
processed observation next is [0.0, 0.8260869565217391, 0.6288088642659281, 0.69, 0.0, 0.0, 0.08333333333333333, 0.8145639964830451, 0.8234215434533466, 0.0, 1.0, 1.0, 0.14352466072379347], 
reward next is 0.8565, 
noisyNet noise sample is [array([-1.7002012], dtype=float32), 0.35841802]. 
=============================================
[2019-04-08 15:54:44,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.54679 ]
 [88.533394]
 [88.517204]
 [88.57544 ]
 [88.703896]], R is [[88.63792419]
 [88.60975647]
 [88.58377838]
 [88.55993652]
 [88.53850555]].
[2019-04-08 15:54:45,574] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9076643e-19 2.4274815e-11 1.3228730e-08 3.8719530e-09 1.6719194e-13
 2.0404417e-13 1.4351758e-09 9.3494330e-18 4.0855982e-20 1.4779263e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:45,578] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6749
[2019-04-08 15:54:45,593] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.25, 61.16666666666667, 0.0, 0.0, 19.0, 28.30062850199031, 1.195843052374678, 0.0, 1.0, 65.0, 18848.10007001788], 
current ob forecast is [], 
actual action is [14.25, 65.0], 
sim time this is 4399800.0000, 
sim time next is 4400400.0000, 
raw observation next is [9.100000000000001, 61.33333333333334, 0.0, 0.0, 19.0, 28.26737583011728, 1.195359362834469, 0.0, 1.0, 65.0, 18845.89540030219], 
processed observation next is [1.0, 0.9565217391304348, 0.7146814404432135, 0.6133333333333334, 0.0, 0.0, 0.08333333333333333, 0.8556146525097734, 0.898453120944823, 0.0, 1.0, 1.0, 0.08974235904905804], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.8687576], dtype=float32), -0.7040326]. 
=============================================
[2019-04-08 15:54:45,838] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6877143e-17 8.3979884e-10 8.0604354e-07 4.9119598e-07 5.7008585e-11
 2.2162001e-12 1.6553064e-06 7.0069813e-13 2.1645766e-16 8.3086758e-20
 9.9999702e-01], sum to 1.0000
[2019-04-08 15:54:45,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2966
[2019-04-08 15:54:45,858] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 19.0, 26.90357312900879, 0.6905224482995359, 0.0, 1.0, 65.0, 42368.41349875929], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4257000.0000, 
sim time next is 4257600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 19.0, 26.89826727643927, 0.6892820055338778, 0.0, 1.0, 65.0, 42393.53122427643], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.08333333333333333, 0.7415222730366059, 0.7297606685112926, 0.0, 1.0, 1.0, 0.20187395821084012], 
reward next is 0.7981, 
noisyNet noise sample is [array([-0.5264169], dtype=float32), 0.9047193]. 
=============================================
[2019-04-08 15:54:46,204] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3600235e-17 6.1129830e-11 4.0736577e-06 8.3070994e-08 2.9237751e-11
 3.2020756e-14 9.0190603e-08 1.8561899e-15 2.7735606e-18 3.2651927e-22
 9.9999571e-01], sum to 1.0000
[2019-04-08 15:54:46,204] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9138
[2019-04-08 15:54:46,216] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.933333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 27.35655997237534, 0.8247694216919602, 0.0, 1.0, 65.0, 37682.80558044199], 
current ob forecast is [], 
actual action is [8.933333333333334, 65.0], 
sim time this is 4333200.0000, 
sim time next is 4333800.0000, 
raw observation next is [3.916666666666667, 70.16666666666667, 0.0, 0.0, 19.0, 27.3192935871664, 0.8221742358309978, 0.0, 1.0, 65.0, 38359.14548304581], 
processed observation next is [1.0, 0.13043478260869565, 0.5710987996306557, 0.7016666666666667, 0.0, 0.0, 0.08333333333333333, 0.7766077989305332, 0.7740580786103326, 0.0, 1.0, 1.0, 0.18266259753831338], 
reward next is 0.8173, 
noisyNet noise sample is [array([-0.1730964], dtype=float32), 0.92402273]. 
=============================================
[2019-04-08 15:54:47,452] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0222110e-20 2.4887259e-13 1.4824202e-09 2.8839404e-09 6.0939345e-13
 1.2599835e-14 6.8809077e-12 2.2676070e-16 2.1997061e-20 5.8648527e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:47,455] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3084
[2019-04-08 15:54:47,467] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.916666666666666, 54.5, 102.0, 615.0, 22.5, 28.03122606227769, 0.9850806814528043, 1.0, 1.0, 65.0, 21090.48416009058], 
current ob forecast is [], 
actual action is [11.916666666666666, 65.0], 
sim time this is 4353000.0000, 
sim time next is 4353600.0000, 
raw observation next is [7.533333333333333, 52.00000000000001, 104.5, 646.0, 22.5, 28.09000918400331, 1.01192529707644, 1.0, 1.0, 65.0, 18844.40927622184], 
processed observation next is [1.0, 0.391304347826087, 0.6712834718374886, 0.52, 0.34833333333333333, 0.7138121546961326, 0.375, 0.8408340986669426, 0.8373084323588134, 1.0, 1.0, 1.0, 0.08973528226772304], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.16649576], dtype=float32), -1.6545291]. 
=============================================
[2019-04-08 15:54:47,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0474572e-19 2.1664873e-12 1.8006628e-09 6.1288254e-08 3.5243852e-12
 2.5041506e-15 3.1672964e-10 9.1182409e-16 4.5981740e-19 1.7381250e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:47,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9936
[2019-04-08 15:54:47,552] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.5, 43.0, 6.000000000000001, 0.0, 22.5, 29.33844998152653, 1.409121918538626, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [17.5, 65.0], 
sim time this is 4384200.0000, 
sim time next is 4384800.0000, 
raw observation next is [12.4, 44.0, 0.0, 0.0, 22.5, 29.36431921174576, 1.399459354232209, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.806094182825485, 0.44, 0.0, 0.0, 0.375, 0.9470266009788132, 0.9664864514107364, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20053898], dtype=float32), -0.028408427]. 
=============================================
[2019-04-08 15:54:47,721] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5450640e-20 5.2846247e-13 8.9094877e-11 2.2127453e-10 2.1903399e-13
 1.8149222e-15 5.9299998e-10 8.8935300e-18 8.3089501e-21 6.7382314e-25
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:47,725] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1684
[2019-04-08 15:54:47,735] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.0, 33.0, 118.3333333333333, 812.0, 22.5, 29.03896616467532, 1.231993362169495, 1.0, 1.0, 65.0, 9424.691143288097], 
current ob forecast is [], 
actual action is [18.0, 65.0], 
sim time this is 4360200.0000, 
sim time next is 4360800.0000, 
raw observation next is [13.4, 32.0, 119.1666666666667, 820.0, 22.5, 29.03371554331152, 1.25238165907288, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8337950138504157, 0.32, 0.3972222222222223, 0.9060773480662984, 0.375, 0.91947629527596, 0.9174605530242933, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03257173], dtype=float32), 0.9134289]. 
=============================================
[2019-04-08 15:54:48,041] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3369832e-16 4.4899189e-11 2.0043733e-07 1.1903185e-07 9.9161417e-12
 9.0279704e-13 2.9225071e-09 2.0574880e-13 3.4904010e-19 1.8684126e-19
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:54:48,042] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8952
[2019-04-08 15:54:48,063] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 62.0, 0.0, 0.0, 19.0, 27.31940576889348, 0.9244592010617522, 0.0, 1.0, 65.0, 34448.23951180456], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4673400.0000, 
sim time next is 4674000.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 19.0, 27.36639061761302, 0.923438755122874, 0.0, 1.0, 65.0, 32632.60484351184], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.08333333333333333, 0.7805325514677518, 0.8078129183742914, 0.0, 1.0, 1.0, 0.15539335639767543], 
reward next is 0.8446, 
noisyNet noise sample is [array([-0.642086], dtype=float32), 0.6782729]. 
=============================================
[2019-04-08 15:54:48,082] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.94817 ]
 [82.83771 ]
 [82.66137 ]
 [82.64822 ]
 [82.651115]], R is [[82.91764832]
 [82.92443085]
 [82.93398285]
 [82.9453125 ]
 [82.95843506]].
[2019-04-08 15:54:48,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1709039e-19 8.2538497e-11 1.3009243e-07 1.0579259e-08 5.6978172e-12
 9.3982249e-14 7.3402786e-09 1.3365712e-15 5.5139689e-19 5.8999308e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:48,251] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.03614046e-16 8.26364237e-12 1.15693915e-07 5.04895965e-08
 3.33556078e-12 7.64447845e-14 8.96522145e-09 1.60223143e-15
 2.34329152e-19 2.64323901e-20 9.99999881e-01], sum to 1.0000
[2019-04-08 15:54:48,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8041
[2019-04-08 15:54:48,263] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0954
[2019-04-08 15:54:48,265] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.2, 62.33333333333334, 0.0, 0.0, 19.0, 27.97615793415468, 1.154627087503324, 0.0, 1.0, 65.0, 27519.79093032698], 
current ob forecast is [], 
actual action is [13.2, 65.0], 
sim time this is 4404000.0000, 
sim time next is 4404600.0000, 
raw observation next is [8.05, 62.5, 0.0, 0.0, 19.0, 27.95214636200343, 1.149134628581781, 0.0, 1.0, 65.0, 27804.20413336201], 
processed observation next is [1.0, 1.0, 0.6855955678670361, 0.625, 0.0, 0.0, 0.08333333333333333, 0.8293455301669524, 0.883044876193927, 0.0, 1.0, 1.0, 0.13240097206362864], 
reward next is 0.8676, 
noisyNet noise sample is [array([1.0914896], dtype=float32), -0.1636469]. 
=============================================
[2019-04-08 15:54:48,277] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [8.95, 61.5, 0.0, 0.0, 19.0, 28.17811101735844, 1.188458994978123, 0.0, 1.0, 65.0, 18844.26355173874], 
current ob forecast is [], 
actual action is [13.95, 65.0], 
sim time this is 4401000.0000, 
sim time next is 4401600.0000, 
raw observation next is [8.8, 61.66666666666666, 0.0, 0.0, 19.0, 28.11063916990689, 1.180334769881904, 0.0, 1.0, 65.0, 21740.17526826383], 
processed observation next is [1.0, 0.9565217391304348, 0.7063711911357342, 0.6166666666666666, 0.0, 0.0, 0.08333333333333333, 0.8425532641589074, 0.8934449232939681, 0.0, 1.0, 1.0, 0.10352464413458966], 
reward next is 0.8965, 
noisyNet noise sample is [array([0.52095985], dtype=float32), -0.9251807]. 
=============================================
[2019-04-08 15:54:48,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.6097889e-20 1.1628693e-11 1.5793839e-08 5.8015024e-09 4.2167376e-12
 9.8542500e-15 6.1199307e-10 5.1302297e-16 4.8120126e-19 5.0574469e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:48,355] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7459
[2019-04-08 15:54:48,372] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [13.0, 35.5, 60.33333333333333, 0.0, 22.5, 29.53276946962644, 1.42823685087343, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [18.0, 65.0], 
sim time this is 4378200.0000, 
sim time next is 4378800.0000, 
raw observation next is [13.0, 36.0, 49.66666666666666, 0.0, 22.5, 29.69919400903398, 1.438676280689188, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.36, 0.1655555555555555, 0.0, 0.375, 0.974932834086165, 0.9795587602297293, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23202711], dtype=float32), -0.025371592]. 
=============================================
[2019-04-08 15:54:48,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4879680e-19 6.0033961e-12 2.3700075e-07 2.4795586e-08 3.3367439e-12
 8.4117006e-14 1.2061668e-09 6.4364695e-15 1.6207126e-19 4.5545582e-22
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:54:48,395] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6853
[2019-04-08 15:54:48,404] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [12.5, 43.0, 6.000000000000001, 0.0, 22.5, 29.33845015545302, 1.409121987293202, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [17.5, 65.0], 
sim time this is 4384200.0000, 
sim time next is 4384800.0000, 
raw observation next is [12.4, 44.0, 0.0, 0.0, 22.5, 29.3643193861774, 1.399459427810568, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.806094182825485, 0.44, 0.0, 0.0, 0.375, 0.9470266155147833, 0.966486475936856, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03981883], dtype=float32), 2.028631]. 
=============================================
[2019-04-08 15:54:48,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5980540e-18 5.7028549e-11 1.3172960e-08 2.6144034e-09 3.4276491e-11
 1.9980726e-14 1.4980818e-09 7.6204907e-17 2.0538507e-20 1.5941723e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:48,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5567
[2019-04-08 15:54:48,934] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.77622835923486, 1.046592777915167, 1.0, 1.0, 65.0, 29006.93213569355], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4471200.0000, 
sim time next is 4471800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.84243124622904, 1.042479403281567, 1.0, 1.0, 65.0, 29679.04890559494], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.82020260385242, 0.847493134427189, 1.0, 1.0, 1.0, 0.14132880431235686], 
reward next is 0.8587, 
noisyNet noise sample is [array([0.4383219], dtype=float32), 0.5421474]. 
=============================================
[2019-04-08 15:54:49,248] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0705350e-20 1.5721493e-12 9.4219377e-10 6.6115963e-10 6.4798275e-12
 3.2561262e-15 1.8941457e-10 3.9950762e-16 1.6714949e-21 6.4629184e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:49,250] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4735
[2019-04-08 15:54:49,258] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.33333333333333, 55.33333333333333, 0.0, 0.0, 22.5, 28.78012705062855, 1.318294643803165, 1.0, 1.0, 65.0, 18849.12125782467], 
current ob forecast is [], 
actual action is [16.33333333333333, 65.0], 
sim time this is 4390800.0000, 
sim time next is 4391400.0000, 
raw observation next is [11.16666666666667, 56.66666666666667, 0.0, 0.0, 22.5, 28.70472914592371, 1.30885397810802, 1.0, 1.0, 65.0, 18849.10156286983], 
processed observation next is [1.0, 0.8260869565217391, 0.7719298245614037, 0.5666666666666668, 0.0, 0.0, 0.375, 0.8920607621603093, 0.93628465936934, 1.0, 1.0, 1.0, 0.08975762648985633], 
reward next is 0.9102, 
noisyNet noise sample is [array([0.33525115], dtype=float32), 1.1429086]. 
=============================================
[2019-04-08 15:54:49,457] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0132125e-19 3.0256787e-12 2.6111442e-09 2.0527500e-09 1.1595038e-13
 8.3467995e-14 1.5234758e-10 6.1469239e-16 1.6724661e-19 3.2536188e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:49,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6017
[2019-04-08 15:54:49,491] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.8141620690327, 1.040795161173065, 1.0, 1.0, 65.0, 32125.62095243506], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4472400.0000, 
sim time next is 4473000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.79054782282287, 1.032492911395231, 1.0, 1.0, 65.0, 32900.77630157712], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.8158789852352392, 0.8441643037984102, 1.0, 1.0, 1.0, 0.15667036334084342], 
reward next is 0.8433, 
noisyNet noise sample is [array([0.28095078], dtype=float32), -1.0924467]. 
=============================================
[2019-04-08 15:54:49,501] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.16431]
 [87.40058]
 [87.6971 ]
 [88.02896]
 [88.18708]], R is [[86.95682526]
 [86.9342804 ]
 [86.9236145 ]
 [86.91625214]
 [86.89247131]].
[2019-04-08 15:54:49,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2004708e-18 3.7902024e-09 1.2983233e-08 1.5324163e-07 9.5021331e-11
 5.7151176e-14 2.9589828e-09 4.2465620e-15 1.6638760e-18 8.7033868e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:49,537] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4179
[2019-04-08 15:54:49,549] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [9.8, 60.0, 0.0, 0.0, 19.0, 28.40130054572599, 1.260517459111558, 0.0, 1.0, 65.0, 18848.79942982513], 
current ob forecast is [], 
actual action is [14.8, 65.0], 
sim time this is 4397400.0000, 
sim time next is 4398000.0000, 
raw observation next is [9.666666666666668, 60.33333333333333, 0.0, 0.0, 19.0, 28.37278035027302, 1.254723858415191, 0.0, 1.0, 65.0, 18848.78216944285], 
processed observation next is [1.0, 0.9130434782608695, 0.7303785780240075, 0.6033333333333333, 0.0, 0.0, 0.08333333333333333, 0.8643983625227515, 0.918241286138397, 0.0, 1.0, 1.0, 0.08975610556877547], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.5417171], dtype=float32), 1.3722286]. 
=============================================
[2019-04-08 15:54:49,563] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.48845 ]
 [86.57237 ]
 [86.29654 ]
 [86.616295]
 [86.38771 ]], R is [[86.85227203]
 [86.89398956]
 [86.9352951 ]
 [86.97618866]
 [87.01667786]].
[2019-04-08 15:54:49,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1171538e-17 2.7697242e-10 1.4443957e-07 1.9157749e-06 1.2556867e-10
 1.3376010e-12 3.8444611e-08 1.1431199e-13 2.3311091e-18 1.8693920e-20
 9.9999785e-01], sum to 1.0000
[2019-04-08 15:54:49,870] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0653
[2019-04-08 15:54:49,885] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.7, 69.33333333333334, 0.0, 0.0, 19.0, 27.41593710983963, 0.8196705828723152, 0.0, 1.0, 65.0, 36495.69350042796], 
current ob forecast is [], 
actual action is [8.7, 65.0], 
sim time this is 4336800.0000, 
sim time next is 4337400.0000, 
raw observation next is [3.65, 69.16666666666666, 0.0, 0.0, 19.0, 27.33136419684468, 0.8195150731250158, 0.0, 1.0, 65.0, 37673.67480723945], 
processed observation next is [1.0, 0.17391304347826086, 0.5637119113573408, 0.6916666666666665, 0.0, 0.0, 0.08333333333333333, 0.7776136830703901, 0.7731716910416719, 0.0, 1.0, 1.0, 0.17939845146304498], 
reward next is 0.8206, 
noisyNet noise sample is [array([1.5096204], dtype=float32), -0.6603954]. 
=============================================
[2019-04-08 15:54:50,539] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.09443543e-18 8.81185333e-11 1.12916034e-07 1.72818957e-07
 4.42677249e-11 1.33634745e-14 5.80690340e-09 5.72085078e-15
 5.72892223e-18 7.53479194e-22 9.99999762e-01], sum to 1.0000
[2019-04-08 15:54:50,557] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1202
[2019-04-08 15:54:50,581] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 22.5, 27.81416245441187, 1.040795272624622, 1.0, 1.0, 65.0, 32125.61619987779], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4472400.0000, 
sim time next is 4473000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 22.5, 27.79054820714114, 1.032493024134553, 1.0, 1.0, 65.0, 32900.77168393922], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.72, 0.0, 0.0, 0.375, 0.8158790172617616, 0.8441643413781842, 1.0, 1.0, 1.0, 0.15667034135209154], 
reward next is 0.8433, 
noisyNet noise sample is [array([1.2069978], dtype=float32), -0.70308304]. 
=============================================
[2019-04-08 15:54:50,596] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[85.47068]
 [85.70213]
 [85.99242]
 [86.32697]
 [86.48362]], R is [[85.28723907]
 [85.28138733]
 [85.2872467 ]
 [85.29624176]
 [85.28866577]].
[2019-04-08 15:54:50,645] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3841244e-16 5.3062915e-10 2.6507196e-06 2.2766344e-06 1.3132591e-10
 2.5466931e-12 1.3564610e-07 3.2373616e-13 1.4350450e-17 4.0879028e-19
 9.9999499e-01], sum to 1.0000
[2019-04-08 15:54:50,648] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2502
[2019-04-08 15:54:50,662] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6666666666666666, 73.0, 0.0, 0.0, 19.0, 26.93023704419149, 0.827022790465758, 0.0, 1.0, 65.0, 46009.81075668551], 
current ob forecast is [], 
actual action is [4.333333333333333, 65.0], 
sim time this is 4500600.0000, 
sim time next is 4501200.0000, 
raw observation next is [-0.7333333333333334, 73.0, 0.0, 0.0, 19.0, 26.95096451741766, 0.8160622350617279, 0.0, 1.0, 65.0, 44515.05068188019], 
processed observation next is [1.0, 0.08695652173913043, 0.44228993536472766, 0.73, 0.0, 0.0, 0.08333333333333333, 0.745913709784805, 0.7720207450205759, 0.0, 1.0, 1.0, 0.21197643181847708], 
reward next is 0.7880, 
noisyNet noise sample is [array([0.33165872], dtype=float32), -1.326375]. 
=============================================
[2019-04-08 15:54:51,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.5917792e-17 4.4271367e-10 1.5387588e-08 4.1095583e-07 5.9426644e-11
 2.2499981e-12 5.8303748e-09 9.5841331e-14 2.0946879e-18 3.7873745e-21
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:54:51,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4514
[2019-04-08 15:54:51,178] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.133333333333333, 68.0, 0.0, 0.0, 19.0, 27.52523144122466, 0.9958396362768425, 0.0, 1.0, 65.0, 36603.00377546349], 
current ob forecast is [], 
actual action is [8.133333333333333, 65.0], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [3.0, 68.0, 0.0, 0.0, 19.0, 27.48336155711979, 0.9949596055021809, 0.0, 1.0, 65.0, 36723.50136923928], 
processed observation next is [1.0, 0.2608695652173913, 0.5457063711911359, 0.68, 0.0, 0.0, 0.08333333333333333, 0.7902801297599824, 0.8316532018340603, 0.0, 1.0, 1.0, 0.17487381604399657], 
reward next is 0.8251, 
noisyNet noise sample is [array([-0.29198998], dtype=float32), -0.59279716]. 
=============================================
[2019-04-08 15:54:51,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[84.120125]
 [84.19931 ]
 [84.27484 ]
 [84.36167 ]
 [84.39729 ]], R is [[84.04512024]
 [84.03036499]
 [84.02154541]
 [84.02394867]
 [84.0284729 ]].
[2019-04-08 15:54:52,251] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.51316265e-17 1.51055543e-10 3.85105386e-06 5.97862368e-07
 1.43616244e-10 3.94265024e-13 1.13285189e-07 7.81556962e-14
 9.70811013e-19 1.00612705e-19 9.99995470e-01], sum to 1.0000
[2019-04-08 15:54:52,252] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.8549
[2019-04-08 15:54:52,264] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 22.5, 27.44303517528688, 0.977357574907933, 1.0, 1.0, 65.0, 37255.95513293487], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4432800.0000, 
sim time next is 4433400.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 22.5, 27.51097804534645, 0.978217586527489, 1.0, 1.0, 65.0, 35076.51869872398], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.375, 0.7925815037788709, 0.8260725288424964, 1.0, 1.0, 1.0, 0.16703104142249514], 
reward next is 0.8330, 
noisyNet noise sample is [array([2.0071225], dtype=float32), -1.8643748]. 
=============================================
[2019-04-08 15:54:52,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2244721e-17 1.3461562e-10 2.4176524e-08 2.6975801e-08 1.2171648e-10
 5.0433556e-13 3.1786094e-09 3.6654063e-14 4.9660741e-17 1.2869995e-19
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:54:52,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8582
[2019-04-08 15:54:52,284] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 19.0, 27.25392916310613, 0.9617146945646872, 0.0, 1.0, 65.0, 39528.75245140449], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4479600.0000, 
sim time next is 4480200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 19.0, 27.23404604107248, 0.9587018877420096, 0.0, 1.0, 65.0, 39614.55881179631], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7695038367560402, 0.8195672959140032, 0.0, 1.0, 1.0, 0.1886407562466491], 
reward next is 0.8114, 
noisyNet noise sample is [array([-0.3441324], dtype=float32), -0.06319289]. 
=============================================
[2019-04-08 15:54:52,357] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3054390e-17 7.2958709e-11 1.4025055e-08 4.3614421e-08 1.0064677e-11
 6.1661878e-14 4.6752344e-09 1.0927525e-14 4.0160642e-19 1.5419987e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:52,362] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1521
[2019-04-08 15:54:52,370] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.833333333333333, 57.66666666666666, 186.6666666666667, 12.0, 22.5, 27.89524657745147, 0.9676179740455041, 1.0, 1.0, 65.0, 25221.80700915], 
current ob forecast is [], 
actual action is [6.833333333333333, 65.0], 
sim time this is 4531800.0000, 
sim time next is 4532400.0000, 
raw observation next is [2.0, 57.0, 169.5, 9.0, 22.5, 27.92711757409939, 0.9716075217565612, 1.0, 1.0, 65.0, 25099.19983588916], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.57, 0.565, 0.009944751381215469, 0.375, 0.827259797841616, 0.8238691739188537, 1.0, 1.0, 1.0, 0.11951999921851982], 
reward next is 0.8805, 
noisyNet noise sample is [array([1.1621013], dtype=float32), 0.9061089]. 
=============================================
[2019-04-08 15:54:52,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2200326e-18 1.2892409e-10 4.4227459e-08 6.9017418e-08 3.8918136e-11
 2.2292586e-13 2.9360471e-08 7.9971536e-16 2.0747110e-19 8.2593976e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:52,766] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2964
[2019-04-08 15:54:52,775] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [11.0, 58.0, 0.0, 0.0, 22.5, 28.63869116672311, 1.304309109744553, 0.0, 1.0, 65.0, 18848.43431259199], 
current ob forecast is [], 
actual action is [16.0, 65.0], 
sim time this is 4392000.0000, 
sim time next is 4392600.0000, 
raw observation next is [10.86666666666667, 58.16666666666667, 0.0, 0.0, 19.0, 28.6069065824592, 1.296869337680614, 1.0, 1.0, 65.0, 18848.77977719842], 
processed observation next is [1.0, 0.8695652173913043, 0.7636195752539245, 0.5816666666666667, 0.0, 0.0, 0.08333333333333333, 0.8839088818716, 0.9322897792268714, 1.0, 1.0, 1.0, 0.08975609417713533], 
reward next is 0.9102, 
noisyNet noise sample is [array([-0.12492056], dtype=float32), 0.27477986]. 
=============================================
[2019-04-08 15:54:52,841] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [2.8709294e-19 1.9108701e-10 8.5692902e-09 8.2647944e-08 3.6827139e-11
 7.2630087e-14 1.6395825e-08 6.1905985e-16 1.6472711e-20 9.7385817e-24
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:52,841] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.6979
[2019-04-08 15:54:52,853] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 86.0, 251.0, 146.0, 22.5, 28.46394491790121, 1.171049782978567, 1.0, 1.0, 65.0, 19238.91892827322], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4444200.0000, 
sim time next is 4444800.0000, 
raw observation next is [1.0, 86.0, 232.8333333333333, 121.6666666666667, 22.5, 28.48582291055128, 1.170035951705134, 1.0, 1.0, 65.0, 19277.35536442491], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.776111111111111, 0.134438305709024, 0.375, 0.8738185758792735, 0.8900119839017112, 1.0, 1.0, 1.0, 0.09179693030678529], 
reward next is 0.9082, 
noisyNet noise sample is [array([-0.49510285], dtype=float32), 0.19404374]. 
=============================================
[2019-04-08 15:54:52,930] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4127440e-16 2.5149199e-10 4.3383679e-07 4.1119836e-07 2.3224497e-10
 2.1879300e-12 2.5110880e-08 1.0131894e-14 2.5313373e-16 4.9301634e-19
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:54:52,932] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0011
[2019-04-08 15:54:52,952] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.09999999999999999, 72.0, 0.0, 0.0, 19.0, 27.1275901161766, 0.8892449819819009, 0.0, 1.0, 65.0, 45107.31010075717], 
current ob forecast is [], 
actual action is [4.9, 65.0], 
sim time this is 4486800.0000, 
sim time next is 4487400.0000, 
raw observation next is [-0.15, 72.0, 0.0, 0.0, 19.0, 27.05687606978987, 0.8839119183731668, 0.0, 1.0, 65.0, 46251.89423532684], 
processed observation next is [1.0, 0.9565217391304348, 0.458448753462604, 0.72, 0.0, 0.0, 0.08333333333333333, 0.7547396724824891, 0.7946373061243889, 0.0, 1.0, 1.0, 0.2202471154063183], 
reward next is 0.7798, 
noisyNet noise sample is [array([0.5010249], dtype=float32), 0.5844798]. 
=============================================
[2019-04-08 15:54:53,274] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [5.40725237e-18 2.92127628e-10 5.97142114e-09 1.50114806e-06
 1.06007446e-10 3.49820541e-13 6.39762066e-09 1.27282218e-14
 8.28397892e-18 7.50778770e-21 9.99998450e-01], sum to 1.0000
[2019-04-08 15:54:53,276] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.2664
[2019-04-08 15:54:53,296] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 68.0, 0.0, 0.0, 19.0, 27.48336155711979, 0.9949596055021809, 0.0, 1.0, 65.0, 36723.50136923928], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4428000.0000, 
sim time next is 4428600.0000, 
raw observation next is [2.833333333333333, 70.0, 0.0, 0.0, 19.0, 27.51716123042086, 0.9912843363207356, 0.0, 1.0, 65.0, 35683.02142989783], 
processed observation next is [1.0, 0.2608695652173913, 0.541089566020314, 0.7, 0.0, 0.0, 0.08333333333333333, 0.7930967692017384, 0.8304281121069118, 0.0, 1.0, 1.0, 0.16991914966618016], 
reward next is 0.8301, 
noisyNet noise sample is [array([1.0310042], dtype=float32), 0.6438185]. 
=============================================
[2019-04-08 15:54:53,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.23932162e-17 1.76534320e-09 1.02132333e-07 6.05086399e-08
 1.08382547e-09 1.67652405e-13 2.19455520e-09 4.27494208e-14
 1.09240904e-17 9.83833563e-20 9.99999881e-01], sum to 1.0000
[2019-04-08 15:54:53,501] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9558
[2019-04-08 15:54:53,534] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.04400508583215, 0.8363601519701319, 0.0, 1.0, 65.0, 44347.02622292087], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 4498200.0000, 
sim time next is 4498800.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 26.99247332419115, 0.8399172028170883, 0.0, 1.0, 65.0, 45537.91110177446], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7493727770159291, 0.7799724009390294, 0.0, 1.0, 1.0, 0.2168471957227355], 
reward next is 0.7832, 
noisyNet noise sample is [array([-0.37931865], dtype=float32), 0.53099227]. 
=============================================
[2019-04-08 15:54:53,896] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.88454357e-17 2.49488452e-09 3.49500198e-07 1.01440961e-07
 1.11930853e-10 2.70258289e-13 6.70476155e-08 1.67418759e-13
 4.97887555e-17 6.61477232e-21 9.99999523e-01], sum to 1.0000
[2019-04-08 15:54:53,896] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8042
[2019-04-08 15:54:53,910] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.0, 61.0, 172.0, 9.0, 22.5, 27.71991279127325, 0.9251809975592552, 1.0, 1.0, 65.0, 27362.7432705943], 
current ob forecast is [], 
actual action is [6.0, 65.0], 
sim time this is 4528800.0000, 
sim time next is 4529400.0000, 
raw observation next is [1.166666666666667, 60.33333333333334, 188.3333333333333, 12.0, 22.5, 27.77787453704375, 0.9335141658004381, 1.0, 1.0, 65.0, 26752.96963439195], 
processed observation next is [1.0, 0.43478260869565216, 0.49492151431209613, 0.6033333333333334, 0.6277777777777777, 0.013259668508287293, 0.375, 0.8148228780869792, 0.811171388600146, 1.0, 1.0, 1.0, 0.1273950934971045], 
reward next is 0.8726, 
noisyNet noise sample is [array([-0.1113009], dtype=float32), 0.6094964]. 
=============================================
[2019-04-08 15:54:53,996] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2101419e-16 2.7997646e-10 7.3975393e-08 1.6859642e-07 1.0937107e-11
 3.9211594e-12 3.4356262e-07 5.8902258e-14 2.4120206e-17 1.3769812e-20
 9.9999940e-01], sum to 1.0000
[2019-04-08 15:54:53,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9871
[2019-04-08 15:54:54,025] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.5, 84.5, 124.0, 419.0, 19.0, 26.30316894243097, 0.6798403623511025, 0.0, 1.0, 65.0, 48517.19082286004], 
current ob forecast is [], 
actual action is [-0.5, 65.0], 
sim time this is 4782600.0000, 
sim time next is 4783200.0000, 
raw observation next is [-5.333333333333333, 82.0, 132.3333333333333, 419.3333333333334, 19.0, 26.32019358761572, 0.6855810231788085, 0.0, 1.0, 65.0, 48075.29065664755], 
processed observation next is [0.0, 0.34782608695652173, 0.3148661126500462, 0.82, 0.44111111111111095, 0.46335174953959496, 0.08333333333333333, 0.6933494656346433, 0.7285270077262695, 0.0, 1.0, 1.0, 0.22892995550784548], 
reward next is 0.7711, 
noisyNet noise sample is [array([-0.3032852], dtype=float32), 0.8738584]. 
=============================================
[2019-04-08 15:54:54,086] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.9170519e-17 1.4852045e-09 7.1368419e-08 2.3684098e-07 3.4650782e-10
 9.2364766e-13 1.9521380e-09 2.3037675e-13 1.8349129e-17 1.0499417e-18
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:54:54,087] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3903
[2019-04-08 15:54:54,104] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 19.0, 26.82775723592704, 0.7654880899141361, 0.0, 1.0, 65.0, 44460.19660822078], 
current ob forecast is [], 
actual action is [4.0, 65.0], 
sim time this is 4515000.0000, 
sim time next is 4515600.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 19.0, 26.80706024639594, 0.7606260114003982, 0.0, 1.0, 65.0, 45525.03450981875], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7339216871996618, 0.7535420038001327, 0.0, 1.0, 1.0, 0.21678587861818455], 
reward next is 0.7832, 
noisyNet noise sample is [array([-0.02524127], dtype=float32), -1.4483346]. 
=============================================
[2019-04-08 15:54:54,138] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0151111e-16 1.0709888e-10 2.4280528e-08 7.6427506e-07 1.6087323e-10
 3.9105858e-14 1.3404735e-08 7.4354253e-14 8.9524393e-19 1.8636494e-19
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:54:54,139] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2091
[2019-04-08 15:54:54,151] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.8, 71.0, 0.0, 0.0, 19.0, 26.85564166055443, 0.7872051271504, 0.0, 1.0, 65.0, 44853.50702727278], 
current ob forecast is [], 
actual action is [4.2, 65.0], 
sim time this is 4510800.0000, 
sim time next is 4511400.0000, 
raw observation next is [-0.8333333333333334, 71.0, 0.0, 0.0, 19.0, 26.886494982613, 0.778080571558395, 0.0, 1.0, 65.0, 44412.08389235698], 
processed observation next is [1.0, 0.21739130434782608, 0.43951985226223456, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7405412485510835, 0.759360190519465, 0.0, 1.0, 1.0, 0.21148611377312848], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.5981166], dtype=float32), 0.9779775]. 
=============================================
[2019-04-08 15:54:54,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.57012529e-17 2.87688429e-10 8.15581380e-09 2.76088059e-07
 1.96019902e-11 8.17358117e-13 3.03493608e-09 3.25440087e-14
 1.34526046e-17 7.52724405e-20 9.99999762e-01], sum to 1.0000
[2019-04-08 15:54:54,803] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7928
[2019-04-08 15:54:54,824] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.04400491760295, 0.8363600997061068, 0.0, 1.0, 65.0, 44347.02828678521], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 4498200.0000, 
sim time next is 4498800.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 26.99247315679523, 0.8399171464173832, 0.0, 1.0, 65.0, 45537.91315672536], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7493727630662693, 0.7799723821391278, 0.0, 1.0, 1.0, 0.21684720550821598], 
reward next is 0.7832, 
noisyNet noise sample is [array([-0.39754382], dtype=float32), 1.2497228]. 
=============================================
[2019-04-08 15:54:55,126] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4575669e-18 7.3949499e-11 1.3222623e-08 8.5475513e-08 5.4895199e-11
 2.0332765e-14 3.8798681e-10 7.3194811e-15 2.1601911e-19 2.6750585e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:54:55,127] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7831
[2019-04-08 15:54:55,139] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 62.83333333333333, 155.6666666666667, 5.999999999999998, 22.5, 27.67646395016003, 0.9168061442873544, 1.0, 1.0, 65.0, 28932.69868692055], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 4528200.0000, 
sim time next is 4528800.0000, 
raw observation next is [1.0, 61.0, 172.0, 9.0, 22.5, 27.71991265409551, 0.925180960531391, 1.0, 1.0, 65.0, 27362.74479242056], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.61, 0.5733333333333334, 0.009944751381215469, 0.375, 0.8099927211746257, 0.8083936535104637, 1.0, 1.0, 1.0, 0.1302987847258122], 
reward next is 0.8697, 
noisyNet noise sample is [array([0.12075159], dtype=float32), 1.1231256]. 
=============================================
[2019-04-08 15:54:55,630] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.3426321e-16 1.4624030e-10 1.2178334e-07 7.2350173e-07 6.2386840e-12
 1.3778639e-12 5.5375171e-09 1.4026554e-14 3.7934785e-19 1.8882440e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:54:55,635] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.6398
[2019-04-08 15:54:55,672] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 22.5, 27.90988772845813, 1.047683311703714, 1.0, 1.0, 65.0, 35006.73759371802], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4470000.0000, 
sim time next is 4470600.0000, 
raw observation next is [0.0, 73.0, 16.66666666666667, 36.66666666666667, 22.5, 27.69344223693798, 1.055224534853511, 1.0, 1.0, 65.0, 32469.38227341097], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.73, 0.05555555555555557, 0.04051565377532229, 0.375, 0.807786853078165, 0.851741511617837, 1.0, 1.0, 1.0, 0.15461610606386175], 
reward next is 0.8454, 
noisyNet noise sample is [array([-0.46505144], dtype=float32), -0.08797031]. 
=============================================
[2019-04-08 15:54:56,479] A3C_AGENT_WORKER-Thread-7 DEBUG:Policy network output: [1.5100444e-16 4.8449824e-11 8.0491532e-08 2.6277382e-07 1.0944586e-10
 7.1746846e-13 5.1034657e-09 6.3645645e-15 2.0684766e-18 1.1509507e-19
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:54:56,480] A3C_AGENT_WORKER-Thread-7 DEBUG:Softmax action selection sampled number: 0.0334
[2019-04-08 15:54:56,507] A3C_AGENT_WORKER-Thread-7 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.95, 73.0, 0.0, 0.0, 19.0, 26.92570057347264, 0.8086682759033191, 0.0, 1.0, 65.0, 45512.15236401816], 
current ob forecast is [], 
actual action is [4.05, 65.0], 
sim time this is 4505400.0000, 
sim time next is 4506000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 19.0, 26.93484075800939, 0.8035360830718385, 0.0, 1.0, 65.0, 44440.4061445066], 
processed observation next is [1.0, 0.13043478260869565, 0.4367497691597415, 0.73, 0.0, 0.0, 0.08333333333333333, 0.7445700631674491, 0.7678453610239462, 0.0, 1.0, 1.0, 0.21162098164050763], 
reward next is 0.7884, 
noisyNet noise sample is [array([-0.5185476], dtype=float32), 0.29053536]. 
=============================================
[2019-04-08 15:54:56,538] A3C_AGENT_WORKER-Thread-7 DEBUG:Value prediction is [[77.304016]
 [77.35387 ]
 [77.36544 ]
 [77.29553 ]
 [77.357635]], R is [[77.29802704]
 [77.30831909]
 [77.32393646]
 [77.34550476]
 [77.36843872]].
[2019-04-08 15:54:57,110] A3C_AGENT_WORKER-Thread-8 DEBUG:Policy network output: [2.6673719e-16 1.9245168e-10 7.0562265e-08 2.0083267e-07 1.9066930e-11
 4.0758201e-13 1.2686008e-08 4.8314471e-14 1.7466036e-17 2.6705748e-19
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:54:57,111] A3C_AGENT_WORKER-Thread-8 DEBUG:Softmax action selection sampled number: 0.5859
[2019-04-08 15:54:57,137] A3C_AGENT_WORKER-Thread-8 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 19.0, 27.00824706099924, 0.8271203527771043, 0.0, 1.0, 65.0, 43922.86774685429], 
current ob forecast is [], 
actual action is [4.4, 65.0], 
sim time this is 4499400.0000, 
sim time next is 4500000.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 19.0, 26.95399977257496, 0.8230549582304475, 0.0, 1.0, 65.0, 44977.31386563056], 
processed observation next is [1.0, 0.08695652173913043, 0.44598337950138506, 0.73, 0.0, 0.0, 0.08333333333333333, 0.74616664771458, 0.7743516527434825, 0.0, 1.0, 1.0, 0.21417768507443125], 
reward next is 0.7858, 
noisyNet noise sample is [array([-0.11102153], dtype=float32), -2.681839]. 
=============================================
[2019-04-08 15:54:57,147] A3C_AGENT_WORKER-Thread-8 DEBUG:Value prediction is [[77.84139 ]
 [77.85229 ]
 [77.850365]
 [77.613884]
 [77.498314]], R is [[77.97786713]
 [77.98892975]
 [77.9921875 ]
 [78.001091  ]
 [78.02108002]].
[2019-04-08 15:54:57,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3226237e-15 5.2675411e-09 2.9618394e-07 5.6448431e-07 3.1583778e-09
 1.5904647e-11 4.4373439e-07 1.0026072e-12 3.8584135e-16 3.0732699e-19
 9.9999869e-01], sum to 1.0000
[2019-04-08 15:54:57,889] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3778
[2019-04-08 15:54:57,911] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 19.0, 26.63897675018143, 0.6677377285308843, 0.0, 1.0, 65.0, 45722.71718764989], 
current ob forecast is [], 
actual action is [1.833333333333333, 65.0], 
sim time this is 4853400.0000, 
sim time next is 4854000.0000, 
raw observation next is [-3.333333333333333, 63.66666666666667, 0.0, 0.0, 19.0, 26.62529138578252, 0.6637707257925338, 0.0, 1.0, 65.0, 45838.26479056749], 
processed observation next is [0.0, 0.17391304347826086, 0.37026777469990774, 0.6366666666666667, 0.0, 0.0, 0.08333333333333333, 0.7187742821485434, 0.7212569085975113, 0.0, 1.0, 1.0, 0.2182774513836547], 
reward next is 0.7817, 
noisyNet noise sample is [array([-0.606261], dtype=float32), 0.09616744]. 
=============================================
[2019-04-08 15:54:57,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.09605 ]
 [75.138016]
 [75.18121 ]
 [75.24363 ]
 [75.29111 ]], R is [[75.12032318]
 [75.15139008]
 [75.18276215]
 [75.21578979]
 [75.24913025]].
[2019-04-08 15:54:59,275] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8719680e-16 2.9399101e-09 1.5334838e-07 5.0457675e-07 4.5796539e-10
 1.2998214e-12 1.4536008e-07 6.8375806e-13 1.3673993e-16 1.3974712e-19
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:54:59,276] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7023
[2019-04-08 15:54:59,304] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-2.866666666666667, 76.0, 0.0, 0.0, 19.0, 26.71380955920956, 0.7306887514083935, 0.0, 1.0, 65.0, 42213.18528980389], 
current ob forecast is [], 
actual action is [2.133333333333333, 65.0], 
sim time this is 4603200.0000, 
sim time next is 4603800.0000, 
raw observation next is [-2.933333333333333, 76.5, 0.0, 0.0, 19.0, 26.71289209435589, 0.7316870775401364, 0.0, 1.0, 65.0, 41372.67390671706], 
processed observation next is [1.0, 0.2608695652173913, 0.38134810710988, 0.765, 0.0, 0.0, 0.08333333333333333, 0.7260743411963242, 0.7438956925133788, 0.0, 1.0, 1.0, 0.19701273288912885], 
reward next is 0.8030, 
noisyNet noise sample is [array([-0.29564175], dtype=float32), -0.14774767]. 
=============================================
[2019-04-08 15:54:59,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0378715e-17 1.2698305e-10 7.4933371e-07 4.7219069e-07 1.1448770e-10
 4.5728980e-13 1.2667813e-08 9.9843225e-16 2.7090910e-19 1.3250383e-19
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:54:59,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1880
[2019-04-08 15:54:59,884] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [1.333333333333333, 54.66666666666667, 127.8333333333333, 778.0, 22.5, 28.1017873343472, 0.813889056622549, 1.0, 1.0, 65.0, 21372.73515905981], 
current ob forecast is [], 
actual action is [6.333333333333333, 65.0], 
sim time this is 4617600.0000, 
sim time next is 4618200.0000, 
raw observation next is [1.666666666666667, 53.33333333333334, 126.6666666666667, 789.0, 22.5, 28.00329509713984, 1.083035591224711, 1.0, 1.0, 65.0, 36179.490327232], 
processed observation next is [1.0, 0.43478260869565216, 0.5087719298245615, 0.5333333333333334, 0.42222222222222233, 0.8718232044198895, 0.375, 0.8336079247616534, 0.8610118637415702, 1.0, 1.0, 1.0, 0.17228328727253334], 
reward next is 0.8277, 
noisyNet noise sample is [array([0.31494534], dtype=float32), -1.5106268]. 
=============================================
[2019-04-08 15:55:01,453] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8406892e-16 4.0066370e-10 5.7449327e-08 9.3654035e-07 2.0959110e-09
 2.7194080e-12 1.5628653e-07 3.5169930e-13 1.4769371e-17 2.9319884e-18
 9.9999881e-01], sum to 1.0000
[2019-04-08 15:55:01,454] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1382
[2019-04-08 15:55:01,475] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666666, 38.5, 0.0, 0.0, 19.0, 27.10313588602065, 0.7676440886365663, 0.0, 1.0, 65.0, 38980.58387923497], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 4920600.0000, 
sim time next is 4921200.0000, 
raw observation next is [0.0, 39.0, 0.0, 0.0, 19.0, 27.10726890797675, 0.7616656761451225, 0.0, 1.0, 65.0, 38410.47869370314], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 0.39, 0.0, 0.0, 0.08333333333333333, 0.7589390756647291, 0.7538885587150408, 0.0, 1.0, 1.0, 0.18290704139858638], 
reward next is 0.8171, 
noisyNet noise sample is [array([1.3271396], dtype=float32), -0.52500653]. 
=============================================
[2019-04-08 15:55:01,549] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9865836e-17 2.2528306e-10 2.1985326e-07 8.7865736e-08 8.3679785e-11
 7.5166781e-14 6.2304473e-08 1.0046151e-14 7.2448540e-18 1.5383036e-20
 9.9999964e-01], sum to 1.0000
[2019-04-08 15:55:01,551] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0753
[2019-04-08 15:55:01,562] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 19.0, 27.4797968404763, 0.9361609834871731, 0.0, 1.0, 65.0, 31712.37753079968], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4671000.0000, 
sim time next is 4671600.0000, 
raw observation next is [2.0, 58.66666666666666, 0.0, 0.0, 19.0, 27.43052703661737, 0.932686024151419, 0.0, 1.0, 65.0, 33059.93019381849], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.5866666666666666, 0.0, 0.0, 0.08333333333333333, 0.7858772530514475, 0.8108953413838064, 0.0, 1.0, 1.0, 0.15742823901818329], 
reward next is 0.8426, 
noisyNet noise sample is [array([-1.7368064], dtype=float32), -0.8180462]. 
=============================================
[2019-04-08 15:55:01,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2418776e-19 7.7186374e-12 3.6831043e-09 3.0992748e-08 5.5619917e-14
 1.4075979e-15 1.8038762e-10 4.5894525e-17 3.4092074e-20 7.1432182e-23
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:01,679] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2430
[2019-04-08 15:55:01,696] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.0, 92.0, 71.66666666666666, 0.0, 22.5, 27.64406605796795, 0.8866853907669245, 1.0, 1.0, 65.00000000000003, 26352.55575441475], 
current ob forecast is [], 
actual action is [5.0, 65.0], 
sim time this is 4696800.0000, 
sim time next is 4697400.0000, 
raw observation next is [0.0, 92.0, 80.33333333333333, 0.0, 22.5, 27.61896120944351, 0.8946841882481639, 1.0, 1.0, 65.0, 27974.12403386156], 
processed observation next is [1.0, 0.34782608695652173, 0.46260387811634357, 0.92, 0.2677777777777778, 0.0, 0.375, 0.8015801007869591, 0.798228062749388, 1.0, 1.0, 1.0, 0.13321011444695982], 
reward next is 0.8668, 
noisyNet noise sample is [array([1.0617504], dtype=float32), -0.659234]. 
=============================================
[2019-04-08 15:55:02,042] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5329640e-17 3.1657912e-11 3.1079409e-08 2.1603764e-08 2.4064507e-11
 1.1473578e-13 1.6073083e-09 4.8576509e-15 7.4283249e-19 4.0926623e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:02,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4277
[2019-04-08 15:55:02,056] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 22.5, 27.61885142709937, 0.9855848706960503, 1.0, 1.0, 65.0, 28593.3009064483], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4561800.0000, 
sim time next is 4562400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 27.59859867843809, 0.9744818535662348, 0.0, 1.0, 65.0, 29309.40811210952], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.7998832232031742, 0.8248272845220783, 0.0, 1.0, 1.0, 0.13956861005766438], 
reward next is 0.8604, 
noisyNet noise sample is [array([0.9583471], dtype=float32), -0.38385937]. 
=============================================
[2019-04-08 15:55:02,095] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.3270838e-17 1.2777658e-10 1.9030942e-08 7.7492524e-07 3.3676206e-10
 7.8296825e-14 4.6381427e-08 6.7841956e-15 7.7973568e-18 3.4382379e-20
 9.9999917e-01], sum to 1.0000
[2019-04-08 15:55:02,101] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3780
[2019-04-08 15:55:02,123] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.5, 96.0, 0.0, 0.0, 19.0, 27.3445340274984, 0.8835030451892818, 0.0, 1.0, 65.0, 33985.43372467533], 
current ob forecast is [], 
actual action is [4.5, 65.0], 
sim time this is 4681800.0000, 
sim time next is 4682400.0000, 
raw observation next is [-0.6666666666666666, 97.33333333333333, 0.0, 0.0, 19.0, 27.33046844866892, 0.8837284945813252, 0.0, 1.0, 65.0, 33814.47839316581], 
processed observation next is [1.0, 0.17391304347826086, 0.44413665743305636, 0.9733333333333333, 0.0, 0.0, 0.08333333333333333, 0.7775390373890767, 0.7945761648604418, 0.0, 1.0, 1.0, 0.16102132568174196], 
reward next is 0.8390, 
noisyNet noise sample is [array([0.57661116], dtype=float32), -1.5332654]. 
=============================================
[2019-04-08 15:55:02,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.6074399e-20 1.1110832e-11 2.2416498e-09 4.8876128e-08 3.5006290e-12
 1.9783636e-15 3.6478523e-10 4.5468285e-17 1.4442219e-19 9.2352301e-24
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:02,195] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-08 15:55:02,215] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.8333333333333334, 87.0, 178.0, 4.0, 22.5, 28.0390952355834, 1.004353594979394, 1.0, 1.0, 65.0, 40159.8802441865], 
current ob forecast is [], 
actual action is [5.833333333333333, 65.0], 
sim time this is 4708200.0000, 
sim time next is 4708800.0000, 
raw observation next is [1.0, 86.0, 160.5, 3.0, 22.5, 28.04531317331718, 1.004788907675769, 1.0, 1.0, 65.0, 41636.29081809365], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.535, 0.0033149171270718232, 0.375, 0.837109431109765, 0.8349296358919229, 1.0, 1.0, 1.0, 0.19826805151473167], 
reward next is 0.8017, 
noisyNet noise sample is [array([-0.33612114], dtype=float32), 0.283487]. 
=============================================
[2019-04-08 15:55:02,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6075658e-18 3.0628294e-10 4.5470273e-07 6.0132370e-09 6.6603695e-11
 5.5755419e-14 6.4357550e-08 8.4269132e-16 2.4775962e-19 3.0548850e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:55:02,378] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3483
[2019-04-08 15:55:02,401] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.35, 49.0, 139.0, 813.0, 22.5, 28.07937098064228, 1.21774788645942, 1.0, 1.0, 65.0, 33021.48008493535], 
current ob forecast is [], 
actual action is [9.35, 65.0], 
sim time this is 4627800.0000, 
sim time next is 4628400.0000, 
raw observation next is [4.466666666666667, 49.0, 149.6666666666667, 777.3333333333333, 22.5, 27.70181995752268, 1.158813751904616, 1.0, 1.0, 65.00000000000014, 20274.3847425941], 
processed observation next is [1.0, 0.5652173913043478, 0.5863342566943676, 0.49, 0.49888888888888905, 0.8589318600368323, 0.375, 0.8084849964602233, 0.8862712506348721, 1.0, 1.0, 1.0000000000000029, 0.09654468925044808], 
reward next is 0.9035, 
noisyNet noise sample is [array([-0.776461], dtype=float32), -0.23251602]. 
=============================================
[2019-04-08 15:55:02,450] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3977189e-18 5.9650840e-11 2.0385951e-08 1.4724706e-08 7.6060436e-13
 6.1074409e-14 2.1462336e-10 5.8077926e-16 2.0261096e-19 7.8185547e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:02,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8198
[2019-04-08 15:55:02,498] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [5.866666666666667, 43.5, 143.0, 141.0, 22.5, 29.07291545949125, 1.113287734535444, 1.0, 1.0, 65.0, 9423.414795221432], 
current ob forecast is [], 
actual action is [10.866666666666667, 65.0], 
sim time this is 4637400.0000, 
sim time next is 4638000.0000, 
raw observation next is [5.733333333333333, 44.0, 130.0, 144.0, 22.5, 29.13867470658869, 1.33336703966651, 1.0, 1.0, 65.0, 19531.67911897767], 
processed observation next is [1.0, 0.6956521739130435, 0.6214219759926132, 0.44, 0.43333333333333335, 0.1591160220994475, 0.375, 0.9282228922157243, 0.9444556798888367, 1.0, 1.0, 1.0, 0.09300799580465557], 
reward next is 0.9070, 
noisyNet noise sample is [array([0.84950405], dtype=float32), -0.555488]. 
=============================================
[2019-04-08 15:55:02,513] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.6646  ]
 [86.977844]
 [87.191635]
 [87.5381  ]
 [87.825775]], R is [[86.47964478]
 [86.56997681]
 [86.61453247]
 [86.64199066]
 [86.73070526]].
[2019-04-08 15:55:03,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6296977e-19 4.7888755e-11 2.6189599e-09 1.5004123e-07 1.4589710e-11
 5.0944675e-14 5.1509059e-09 3.8801173e-15 3.3910098e-20 8.7062375e-22
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:55:03,053] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1081
[2019-04-08 15:55:03,066] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.166666666666667, 52.16666666666667, 0.0, 0.0, 22.5, 27.95718050411616, 1.120715866207904, 0.0, 1.0, 65.0, 18845.37777085626], 
current ob forecast is [], 
actual action is [7.166666666666667, 65.0], 
sim time this is 4650600.0000, 
sim time next is 4651200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 22.5, 27.90351865591344, 1.112937954166959, 0.0, 1.0, 65.0, 18844.20096024276], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.375, 0.82529322132612, 0.8709793180556531, 0.0, 1.0, 1.0, 0.08973429028687029], 
reward next is 0.9103, 
noisyNet noise sample is [array([0.16009562], dtype=float32), -0.8195893]. 
=============================================
[2019-04-08 15:55:03,404] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.37630931e-19 1.08011024e-10 6.93380064e-09 1.52302619e-07
 1.11844355e-12 9.17993708e-14 3.14236664e-10 1.93682998e-15
 1.38069602e-19 2.62601887e-23 9.99999881e-01], sum to 1.0000
[2019-04-08 15:55:03,405] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7654
[2019-04-08 15:55:03,438] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [4.7, 49.0, 171.0, 706.0, 22.5, 28.7375363411916, 1.23091032252759, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [9.7, 65.0], 
sim time this is 4629600.0000, 
sim time next is 4630200.0000, 
raw observation next is [4.75, 49.16666666666667, 181.6666666666667, 670.3333333333333, 22.5, 28.90466036857976, 1.253961969858823, 1.0, 1.0, 65.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5941828254847646, 0.4916666666666667, 0.6055555555555557, 0.7406998158379373, 0.375, 0.9087216973816465, 0.9179873232862743, 1.0, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.89002943], dtype=float32), -1.2525569]. 
=============================================
[2019-04-08 15:55:03,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0709188e-16 1.3358789e-10 1.4555821e-08 8.0734175e-08 1.1055297e-11
 2.1393556e-13 3.6488279e-09 2.2736018e-15 5.0970787e-19 1.0069604e-20
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:55:03,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0885
[2019-04-08 15:55:03,691] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-1.75, 70.0, 0.0, 0.0, 19.0, 26.90048982993374, 0.7796757040628499, 0.0, 1.0, 65.0, 38980.82526577231], 
current ob forecast is [], 
actual action is [3.25, 65.0], 
sim time this is 4595400.0000, 
sim time next is 4596000.0000, 
raw observation next is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 19.0, 26.92738521229212, 0.7732267964409063, 0.0, 1.0, 65.0, 37889.52902093325], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 0.7033333333333333, 0.0, 0.0, 0.08333333333333333, 0.7439487676910099, 0.7577422654803021, 0.0, 1.0, 1.0, 0.18042632867111072], 
reward next is 0.8196, 
noisyNet noise sample is [array([0.05280722], dtype=float32), 0.811321]. 
=============================================
[2019-04-08 15:55:03,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.14572 ]
 [82.183716]
 [82.18885 ]
 [82.20601 ]
 [82.21776 ]], R is [[82.14666748]
 [82.13957977]
 [82.13433075]
 [82.13037872]
 [82.13178253]].
[2019-04-08 15:55:03,740] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9919407e-17 9.9844932e-10 4.2750824e-07 7.9500978e-08 1.3062054e-10
 7.4334750e-13 9.6666257e-09 1.7022387e-14 9.5199836e-18 1.3888456e-20
 9.9999952e-01], sum to 1.0000
[2019-04-08 15:55:03,740] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2082
[2019-04-08 15:55:03,751] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 22.5, 28.22684061144511, 1.160758384580275, 0.0, 1.0, 65.0, 18848.38372618015], 
current ob forecast is [], 
actual action is [7.833333333333333, 65.0], 
sim time this is 4648200.0000, 
sim time next is 4648800.0000, 
raw observation next is [2.666666666666667, 52.66666666666667, 0.0, 0.0, 22.5, 28.16255757806577, 1.147290159042891, 0.0, 1.0, 65.0, 18848.08579790538], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.5266666666666667, 0.0, 0.0, 0.375, 0.8468797981721474, 0.8824300530142969, 0.0, 1.0, 1.0, 0.08975278951383514], 
reward next is 0.9102, 
noisyNet noise sample is [array([1.0865978], dtype=float32), -0.028612955]. 
=============================================
[2019-04-08 15:55:04,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3739021e-17 3.4257985e-11 4.7366665e-08 1.6279117e-07 7.7075657e-10
 1.4160498e-12 2.0269379e-10 1.0175402e-13 2.1022119e-19 3.9490352e-20
 9.9999976e-01], sum to 1.0000
[2019-04-08 15:55:04,569] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8250
[2019-04-08 15:55:04,594] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-3.833333333333333, 82.83333333333333, 0.0, 0.0, 19.0, 26.64820256198843, 0.7846583796241418, 0.0, 1.0, 65.0, 47731.24479101703], 
current ob forecast is [], 
actual action is [1.166666666666667, 65.0], 
sim time this is 4751400.0000, 
sim time next is 4752000.0000, 
raw observation next is [-4.0, 84.0, 0.0, 0.0, 19.0, 26.6877872761391, 0.7757347912857092, 0.0, 1.0, 65.0, 46357.21039199109], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.84, 0.0, 0.0, 0.08333333333333333, 0.7239822730115918, 0.7585782637619031, 0.0, 1.0, 1.0, 0.22074862091424327], 
reward next is 0.7793, 
noisyNet noise sample is [array([0.38414344], dtype=float32), -0.15969113]. 
=============================================
[2019-04-08 15:55:04,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.506035]
 [84.58868 ]
 [84.64968 ]
 [84.751976]
 [84.91442 ]], R is [[83.81060028]
 [83.74520111]
 [83.68123627]
 [83.61911774]
 [83.55988312]].
[2019-04-08 15:55:04,783] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7839405e-17 1.0923254e-10 7.2234876e-09 1.1965331e-07 2.1312195e-12
 4.5516184e-13 1.9284707e-10 9.7679814e-15 6.9636918e-19 1.3600461e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:55:04,785] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5173
[2019-04-08 15:55:04,817] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.1666666666666666, 77.0, 19.33333333333334, 18.66666666666667, 22.5, 27.54255263516964, 1.015712434231716, 1.0, 1.0, 65.00000000000009, 51724.5601982533], 
current ob forecast is [], 
actual action is [5.166666666666667, 65.0], 
sim time this is 4729800.0000, 
sim time next is 4730400.0000, 
raw observation next is [0.0, 78.0, 0.0, 0.0, 22.5, 27.1268590492093, 0.9538086861883522, 1.0, 1.0, 65.00000000000009, 33631.61534986669], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.78, 0.0, 0.0, 0.375, 0.7605715874341085, 0.8179362287294508, 1.0, 1.0, 1.0000000000000018, 0.16015054928507946], 
reward next is 0.8398, 
noisyNet noise sample is [array([1.8068277], dtype=float32), 1.1242064]. 
=============================================
[2019-04-08 15:55:04,864] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.5698863e-19 2.8098530e-12 2.8008946e-09 6.3289685e-10 1.0785859e-12
 2.4626358e-14 2.0897781e-09 4.6930983e-15 2.7658041e-20 2.2928163e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:04,866] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7153
[2019-04-08 15:55:04,886] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1991564e-19 4.5071599e-10 8.0416051e-08 4.9777604e-08 6.6852890e-12
 6.0892335e-15 2.3989830e-09 2.5573776e-14 1.9600988e-18 2.4723643e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:55:04,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1322
[2019-04-08 15:55:04,917] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.5788511878006, 0.7532675179954197, 0.0, 1.0, 65.0, 47847.84936370866], 
current ob forecast is [], 
actual action is [1.0, 65.0], 
sim time this is 4756200.0000, 
sim time next is 4756800.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 19.0, 26.61012837773119, 0.7527678816143056, 0.0, 1.0, 65.0, 46875.48412264582], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.08333333333333333, 0.7175106981442658, 0.7509226272047685, 0.0, 1.0, 1.0, 0.2232165910602182], 
reward next is 0.7768, 
noisyNet noise sample is [array([1.7253172], dtype=float32), 1.2971357]. 
=============================================
[2019-04-08 15:55:04,944] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 25.0, 17.0, 152.0, 22.5, 28.87949081652006, 1.250514997595966, 1.0, 1.0, 65.0, 0.0], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 4989600.0000, 
sim time next is 4990200.0000, 
raw observation next is [6.0, 24.66666666666667, 0.0, 0.0, 22.5, 28.87876015810799, 1.157601871465699, 1.0, 1.0, 65.0, 23891.82625103963], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.2466666666666667, 0.0, 0.0, 0.375, 0.9065633465089992, 0.8858672904885664, 1.0, 1.0, 1.0, 0.11377060119542681], 
reward next is 0.8862, 
noisyNet noise sample is [array([-0.26669073], dtype=float32), -0.3024157]. 
=============================================
[2019-04-08 15:55:05,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.8920420e-19 6.5963796e-10 1.0345990e-07 6.3858806e-08 1.0363310e-11
 9.6630976e-15 3.1675456e-09 4.1847720e-14 3.6377625e-18 5.1564674e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:55:05,012] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5661
[2019-04-08 15:55:05,034] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [6.0, 24.33333333333334, 0.0, 0.0, 22.5, 28.25384468435176, 1.223196364317082, 1.0, 1.0, 65.0, 42309.13779237283], 
current ob forecast is [], 
actual action is [11.0, 65.0], 
sim time this is 4990800.0000, 
sim time next is 4991400.0000, 
raw observation next is [6.0, 24.0, 0.0, 0.0, 22.5, 27.65965114173641, 1.134524528481981, 1.0, 1.0, 65.0, 24518.5651357569], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.24, 0.0, 0.0, 0.375, 0.8049709284780343, 0.8781748428273269, 1.0, 1.0, 1.0, 0.11675507207503286], 
reward next is 0.8832, 
noisyNet noise sample is [array([-0.26669073], dtype=float32), -0.3024157]. 
=============================================
[2019-04-08 15:55:05,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5912620e-18 1.8132656e-10 1.0112418e-07 1.0583961e-08 3.4498127e-10
 1.3480124e-12 1.4717656e-08 1.9112007e-15 7.0906982e-19 5.8207298e-21
 9.9999988e-01], sum to 1.0000
[2019-04-08 15:55:05,314] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1051
[2019-04-08 15:55:05,332] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.38383352655171, 0.6835397986538373, 0.0, 1.0, 65.0, 50212.00906992453], 
current ob forecast is [], 
actual action is [-1.0, 65.0], 
sim time this is 4765200.0000, 
sim time next is 4765800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 19.0, 26.36387358074099, 0.679868315576102, 0.0, 1.0, 65.0, 50727.65036400808], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.08333333333333333, 0.6969894650617491, 0.7266227718587007, 0.0, 1.0, 1.0, 0.24156023982860989], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.2800919], dtype=float32), 0.8345599]. 
=============================================
[2019-04-08 15:55:07,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4314096e-16 8.9991135e-09 4.4106723e-06 7.6304821e-07 2.7408850e-10
 8.2550107e-13 1.4476313e-08 2.0084368e-13 5.1564322e-17 1.3330128e-19
 9.9999475e-01], sum to 1.0000
[2019-04-08 15:55:07,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2804
[2019-04-08 15:55:07,179] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 19.0, 27.57511331204629, 1.006026383703662, 0.0, 1.0, 65.0, 33386.60723452769], 
current ob forecast is [], 
actual action is [7.0, 65.0], 
sim time this is 4660200.0000, 
sim time next is 4660800.0000, 
raw observation next is [2.0, 57.0, 0.0, 0.0, 19.0, 27.51493965382829, 0.998970977027304, 0.0, 1.0, 65.0, 33008.97609657763], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.57, 0.0, 0.0, 0.08333333333333333, 0.7929116378190241, 0.8329903256757679, 0.0, 1.0, 1.0, 0.15718560045989347], 
reward next is 0.8428, 
noisyNet noise sample is [array([-0.75921816], dtype=float32), -0.38213995]. 
=============================================
[2019-04-08 15:55:07,468] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.2369396e-18 7.1432851e-12 3.5083547e-09 2.1165270e-08 1.2109161e-12
 1.3928271e-14 5.7899030e-10 4.2939136e-16 2.0987830e-19 2.9629072e-21
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:07,469] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3603
[2019-04-08 15:55:07,484] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 19.0, 26.33712626531839, 0.6919515421782064, 0.0, 1.0, 65.0, 47536.71205879437], 
current ob forecast is [], 
actual action is [-0.16666666666666696, 65.0], 
sim time this is 4783800.0000, 
sim time next is 4784400.0000, 
raw observation next is [-5.0, 77.0, 149.0, 420.0, 19.0, 26.35561876128082, 0.698186317409255, 0.0, 1.0, 65.0, 46981.115479952], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.77, 0.49666666666666665, 0.46408839779005523, 0.08333333333333333, 0.6963015634400683, 0.7327287724697517, 0.0, 1.0, 1.0, 0.22371959752358095], 
reward next is 0.7763, 
noisyNet noise sample is [array([0.02991937], dtype=float32), -1.9620451]. 
=============================================
[2019-04-08 15:55:07,786] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4688754e-18 3.7250876e-11 1.2696323e-08 3.4224215e-08 4.9453107e-12
 2.7171649e-13 3.0868781e-09 3.5204968e-15 6.2845072e-18 8.2755104e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:07,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9024
[2019-04-08 15:55:07,809] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [-0.1666666666666667, 78.0, 0.0, 0.0, 22.5, 27.52649715904392, 0.9791906731016912, 1.0, 1.0, 65.0, 28041.23013446524], 
current ob forecast is [], 
actual action is [4.833333333333333, 65.0], 
sim time this is 4731000.0000, 
sim time next is 4731600.0000, 
raw observation next is [-0.3333333333333333, 78.0, 0.0, 0.0, 22.5, 27.67529688562114, 0.9742845263541664, 1.0, 1.0, 65.0, 31786.56729935271], 
processed observation next is [1.0, 0.782608695652174, 0.4533702677747, 0.78, 0.0, 0.0, 0.375, 0.8062747404684284, 0.8247615087847221, 1.0, 1.0, 1.0, 0.15136460618739386], 
reward next is 0.8486, 
noisyNet noise sample is [array([-1.0827727], dtype=float32), 1.771608]. 
=============================================
[2019-04-08 15:55:08,111] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1697131e-19 4.6467819e-11 1.1408636e-09 1.8112249e-08 8.3821457e-12
 3.8165702e-14 4.0221471e-10 6.0867029e-15 2.1592356e-19 7.0946389e-22
 1.0000000e+00], sum to 1.0000
[2019-04-08 15:55:08,114] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9813
[2019-04-08 15:55:08,130] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [0.3333333333333333, 87.0, 0.0, 0.0, 19.0, 27.35754927427301, 0.8866728268317136, 0.0, 1.0, 65.0, 32848.20052528231], 
current ob forecast is [], 
actual action is [5.333333333333333, 65.0], 
sim time this is 4679400.0000, 
sim time next is 4680000.0000, 
raw observation next is [0.0, 92.0, 0.0, 0.0, 19.0, 27.29671440613876, 0.8992977912053428, 0.0, 1.0, 65.0, 33436.0689672141], 
processed observation next is [1.0, 0.17391304347826086, 0.46260387811634357, 0.92, 0.0, 0.0, 0.08333333333333333, 0.7747262005115635, 0.7997659304017809, 0.0, 1.0, 1.0, 0.15921937603435288], 
reward next is 0.8408, 
noisyNet noise sample is [array([3.284895], dtype=float32), 0.6129921]. 
=============================================
[2019-04-08 15:55:08,151] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.41758]
 [81.1986 ]
 [81.00349]
 [80.88343]
 [80.74942]], R is [[81.69254303]
 [81.71920013]
 [81.73938751]
 [81.76543427]
 [81.80127716]].
[2019-04-08 15:55:09,597] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6204951e-17 1.4123138e-09 7.0065022e-07 1.2339050e-06 2.5719310e-10
 9.3748360e-12 7.7213880e-09 5.7358889e-14 6.4170362e-17 2.8422310e-20
 9.9999809e-01], sum to 1.0000
[2019-04-08 15:55:09,597] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6637
[2019-04-08 15:55:09,623] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 10, 
current raw observation is [3.0, 37.0, 160.0, 713.0, 19.0, 27.36549139149377, 0.9609163601564149, 0.0, 1.0, 65.0, 28182.18377214643], 
current ob forecast is [], 
actual action is [8.0, 65.0], 
sim time this is 4803600.0000, 
sim time next is 4804200.0000, 
raw observation next is [3.0, 37.0, 148.0, 742.0, 19.0, 27.38088074194561, 0.9664248789784798, 0.0, 1.0, 65.0, 28110.14791460591], 
processed observation next is [0.0, 0.6086956521739131, 0.5457063711911359, 0.37, 0.49333333333333335, 0.8198895027624309, 0.08333333333333333, 0.7817400618288008, 0.82214162632616, 0.0, 1.0, 1.0, 0.1338578472124091], 
reward next is 0.8661, 
noisyNet noise sample is [array([0.22607048], dtype=float32), -0.41751757]. 
=============================================
[2019-04-08 15:55:09,876] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-08 15:55:09,878] A3C_EVAL-Part4-Light-Pit-Train-v2 INFO:Evaluation job starts!
[2019-04-08 15:55:09,879] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:55:09,879] A3C_EVAL-Part4-Light-Pit-Test-v3 INFO:Evaluation job starts!
[2019-04-08 15:55:09,879] A3C_EVAL-Part4-Light-Pit-Test-v4 INFO:Evaluation job starts!
[2019-04-08 15:55:09,879] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:55:09,881] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-08 15:55:09,883] EPLUS_ENV_Part4-Light-Pit-Train-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Train-v2-res1/Eplus-env-sub_run21
[2019-04-08 15:55:09,902] EPLUS_ENV_Part4-Light-Pit-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v4-res1/Eplus-env-sub_run21
[2019-04-08 15:55:09,914] EPLUS_ENV_Part4-Light-Pit-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/63/Eplus-env-Part4-Light-Pit-Test-v3-res1/Eplus-env-sub_run21
