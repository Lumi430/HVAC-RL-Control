Using TensorFlow backend.
[2019-04-04 04:45:26,727] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part4_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part4-Light-Pit-Train-v1', eval_act_func='part4_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=100000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=5e-05, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=5000000, metric_func='part4_v1', model_dir='None', model_param=[12, 1], model_type='nn', num_threads=16, output='./Part4-Light-Pit-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part4_v1', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=10, test_env=['Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'], test_mode='Multiple', train_act_func='part4_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=10.0, weight_initer='glorot_uniform', window_len=20)
[2019-04-04 04:45:26,727] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-04 04:45:26.760554: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-04 04:45:43,420] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-04 04:45:43,421] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part4-Light-Pit-Train-v1', 'Part4-Light-Pit-Test-v1', 'Part4-Light-Pit-Test-v2'] ...
[2019-04-04 04:45:43,445] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation worker starts!
[2019-04-04 04:45:43,471] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation worker starts!
[2019-04-04 04:45:43,497] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation worker starts!
[2019-04-04 04:45:43,497] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:43,497] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-04 04:45:43,567] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:43,568] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run1
[2019-04-04 04:45:44,498] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:44,500] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-04 04:45:44,578] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:44,579] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run1
[2019-04-04 04:45:45,501] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:45,501] A3C_AGENT_WORKER-Thread-4 INFO:Local worker starts!
[2019-04-04 04:45:45,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:45,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run1
[2019-04-04 04:45:46,502] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:46,503] A3C_AGENT_WORKER-Thread-5 INFO:Local worker starts!
[2019-04-04 04:45:46,594] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:46,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run1
[2019-04-04 04:45:47,504] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:47,505] A3C_AGENT_WORKER-Thread-6 INFO:Local worker starts!
[2019-04-04 04:45:47,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:47,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run1
[2019-04-04 04:45:47,919] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 04:45:47,920] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:45:47,920] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:45:47,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:47,921] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:45:47,921] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:47,921] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:47,924] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run1
[2019-04-04 04:45:47,925] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run1
[2019-04-04 04:45:47,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run1
[2019-04-04 04:45:48,506] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:48,507] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-04 04:45:48,583] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:48,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run1
[2019-04-04 04:45:49,508] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:49,508] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-04 04:45:49,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:49,622] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run1
[2019-04-04 04:45:50,509] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:50,510] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-04 04:45:50,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:50,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run1
[2019-04-04 04:45:51,510] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:51,511] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-04 04:45:51,628] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:51,629] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run1
[2019-04-04 04:45:52,512] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:52,513] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-04 04:45:52,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:52,675] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run1
[2019-04-04 04:45:53,514] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:53,515] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-04 04:45:53,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:53,640] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run1
[2019-04-04 04:45:54,516] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:54,525] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-04 04:45:54,684] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:54,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run1
[2019-04-04 04:45:55,526] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:55,527] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-04 04:45:55,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:55,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run1
[2019-04-04 04:45:56,528] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:56,529] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-04 04:45:56,682] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:56,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run1
[2019-04-04 04:45:57,529] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:57,530] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-04 04:45:57,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:57,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run1
[2019-04-04 04:45:58,530] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-04 04:45:58,531] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-04 04:45:58,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:45:58,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run1
[2019-04-04 04:46:28,403] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-04 04:46:28,404] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.7, 65.5, 0.0, 0.0, 24.0, 24.13140563556911, 0.08906376428006453, 1.0, 1.0, 0.0]
[2019-04-04 04:46:28,404] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:46:28,405] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.2338249  0.06353957 0.06459201 0.12146319 0.060065   0.1044395
 0.3520758 ], sampled 0.2098802570499254
[2019-04-04 04:46:36,048] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-04 04:46:36,048] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.1, 94.5, 0.0, 0.0, 26.0, 25.42974264860531, 0.5766533188629733, 0.0, 1.0, 29452.72577658023]
[2019-04-04 04:46:36,048] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 04:46:36,049] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [0.14342278 0.02986244 0.07949772 0.05345465 0.04228243 0.06147374
 0.59000623], sampled 0.5788303463910254
[2019-04-04 04:46:58,963] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-04 04:46:58,963] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.358335955, 69.11624796, 87.36442475999999, 171.4249874, 26.0, 24.9124050162343, 0.2521555673340516, 0.0, 1.0, 126717.9784317063]
[2019-04-04 04:46:58,963] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:46:58,963] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.1152534  0.037051   0.11024426 0.07117933 0.03012721 0.07480413
 0.56134063], sampled 0.6411320248742004
[2019-04-04 04:47:49,361] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-04 04:47:49,361] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-8.166666666666668, 59.33333333333334, 0.0, 0.0, 26.0, 25.1893158562864, 0.4473056392587373, 0.0, 1.0, 159756.8682541813]
[2019-04-04 04:47:49,361] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:47:49,362] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [0.12794372 0.07214396 0.10447885 0.0887745  0.03610883 0.10648447
 0.46406573], sampled 0.4322013873596088
[2019-04-04 04:48:08,462] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-04 04:48:08,462] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.7502182585, 56.239645265, 0.0, 0.0, 26.0, 25.08900148860172, 0.4426944125428753, 0.0, 1.0, 160709.3438490669]
[2019-04-04 04:48:08,462] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 04:48:08,463] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [0.13116524 0.09215318 0.09217671 0.10796306 0.04856075 0.12592788
 0.40205327], sampled 0.7097593778942979
[2019-04-04 04:48:29,224] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7388.3283 232611061.0634 1525.9597
[2019-04-04 04:49:00,785] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7277.0073 255980705.0932 1363.8053
[2019-04-04 04:49:06,747] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7218.7742 268217418.7691 1112.7417
[2019-04-04 04:49:07,801] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 7277.007256575204, 255980705.0931572, 1363.8052696696202, 7388.328280650327, 232611061.06343174, 1525.9597474256736, 7218.774196337852, 268217418.76905107, 1112.741704851808]
[2019-04-04 04:49:13,915] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.20442371 0.06144296 0.0473672  0.06826118 0.06381892 0.0915104
 0.46317557], sum to 1.0000
[2019-04-04 04:49:13,915] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1805
[2019-04-04 04:49:14,221] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.22511818268503, -0.5709605124818499, 0.0, 1.0, 40308.39667921604], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 22800.0000, 
sim time next is 23400.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 24.0, 21.2560216395749, -0.5648554714690567, 0.0, 1.0, 40289.55994337339], 
processed observation next is [0.0, 0.2608695652173913, 0.6759002770083103, 0.93, 0.0, 0.0, 0.5, 0.2713351366312417, 0.31171484284364775, 0.0, 1.0, 0.1918550473493971], 
reward next is 0.8081, 
noisyNet noise sample is [array([-1.1522439], dtype=float32), -1.5811133]. 
=============================================
[2019-04-04 04:49:16,322] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.1998002  0.06387416 0.04221398 0.07618722 0.05739015 0.0766151
 0.4839192 ], sum to 1.0000
[2019-04-04 04:49:16,322] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0863
[2019-04-04 04:49:16,767] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [7.7, 93.0, 17.5, 0.0, 26.0, 21.51150680894195, -0.4299633566933276, 0.0, 1.0, 132597.927531752], 
current ob forecast is [], 
actual action is [24.0], 
sim time this is 30000.0000, 
sim time next is 30600.0000, 
raw observation next is [7.7, 93.0, 21.0, 0.0, 24.0, 21.87168225924646, -0.3602770565754703, 0.0, 1.0, 100172.3707125177], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07, 0.0, 0.5, 0.32264018827053825, 0.3799076478081766, 0.0, 1.0, 0.4770112891072272], 
reward next is 0.5230, 
noisyNet noise sample is [array([0.1841862], dtype=float32), 1.2942189]. 
=============================================
[2019-04-04 04:49:20,007] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.18195535 0.04411795 0.04103341 0.06924012 0.05156665 0.06404099
 0.5480456 ], sum to 1.0000
[2019-04-04 04:49:20,008] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0211
[2019-04-04 04:49:20,298] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [8.200000000000001, 86.0, 90.0, 0.0, 25.5, 24.2570953661123, 0.08241838659815055, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 47400.0000, 
sim time next is 48000.0000, 
raw observation next is [8.100000000000001, 86.0, 88.5, 0.0, 23.5, 24.19906031002196, 0.07134862003025037, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6869806094182827, 0.86, 0.295, 0.0, 0.4583333333333333, 0.5165883591684967, 0.5237828733434168, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0067782], dtype=float32), 0.3204967]. 
=============================================
[2019-04-04 04:49:20,302] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[0.44150826]
 [0.42364135]
 [0.48750547]
 [0.4342198 ]
 [0.50930667]], R is [[1.44013691]
 [2.42573547]
 [3.40147805]
 [4.36746311]
 [5.32378864]].
[2019-04-04 04:49:29,192] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.21984926 0.12309121 0.01098489 0.10352084 0.09225646 0.05177841
 0.39851892], sum to 1.0000
[2019-04-04 04:49:29,192] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5175
[2019-04-04 04:49:29,240] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [-5.283333333333333, 74.16666666666667, 0.0, 0.0, 23.0, 23.20574923072139, -0.1253343940244523, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.5], 
sim time this is 105000.0000, 
sim time next is 105600.0000, 
raw observation next is [-5.566666666666667, 74.33333333333334, 0.0, 0.0, 23.5, 23.12208529613586, -0.1479902788131558, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.3084025854108957, 0.7433333333333334, 0.0, 0.0, 0.4583333333333333, 0.4268404413446551, 0.45066990706228144, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.73156846], dtype=float32), -0.70301867]. 
=============================================
[2019-04-04 04:49:38,219] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7376183e-01 1.6016604e-01 4.5992588e-04 1.7095041e-01 8.6059682e-02
 1.2876849e-02 9.5725290e-02], sum to 1.0000
[2019-04-04 04:49:38,219] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3273
[2019-04-04 04:49:38,588] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 19.0, 22.37675886970591, -0.3317426577784661, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 168000.0000, 
sim time next is 168600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 19.0, 22.25399961694713, -0.359303051322515, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.2299168975069252, 0.71, 0.0, 0.0, 0.08333333333333333, 0.35449996807892753, 0.3802323162258283, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0902035], dtype=float32), 0.6212698]. 
=============================================
[2019-04-04 04:49:40,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.38710827 0.19383866 0.00080804 0.13630757 0.11375388 0.03315847
 0.13502516], sum to 1.0000
[2019-04-04 04:49:40,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9012
[2019-04-04 04:49:40,638] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 19.0, 20.34878262842878, -0.7462572855686572, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 179400.0000, 
sim time next is 180000.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 21.0, 20.31612470703966, -0.7469408151711372, 0.0, 1.0, 197249.4588132869], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.74, 0.0, 0.0, 0.25, 0.19301039225330494, 0.2510197282762876, 0.0, 1.0, 0.9392831372061281], 
reward next is 0.0607, 
noisyNet noise sample is [array([-0.03762213], dtype=float32), -0.43354118]. 
=============================================
[2019-04-04 04:49:40,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[5.3914294]
 [5.1990447]
 [5.085607 ]
 [5.3399835]
 [5.465087 ]], R is [[5.20362091]
 [6.15158463]
 [7.09006882]
 [7.08479738]
 [8.01394939]].
[2019-04-04 04:49:53,493] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7697: loss -1.6429
[2019-04-04 04:49:53,574] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7697: learning rate 0.0000
[2019-04-04 04:49:53,623] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7716: loss -2.8864
[2019-04-04 04:49:53,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7729: learning rate 0.0000
[2019-04-04 04:49:54,101] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7806: loss -1.0243
[2019-04-04 04:49:54,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7806: learning rate 0.0000
[2019-04-04 04:49:54,495] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7880: loss -1.9800
[2019-04-04 04:49:54,496] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7880: learning rate 0.0000
[2019-04-04 04:49:54,815] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7941: loss -0.6206
[2019-04-04 04:49:54,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7941: learning rate 0.0000
[2019-04-04 04:49:54,837] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 7947: loss 30.7897
[2019-04-04 04:49:54,840] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 7947: learning rate 0.0000
[2019-04-04 04:49:54,906] A3C_AGENT_WORKER-Thread-5 INFO:Local step 500, global step 7960: loss -5.3259
[2019-04-04 04:49:54,975] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 500, global step 7969: learning rate 0.0000
[2019-04-04 04:49:55,115] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8008: loss -0.7036
[2019-04-04 04:49:55,115] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8008: learning rate 0.0000
[2019-04-04 04:49:55,265] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 8038: loss 24.0223
[2019-04-04 04:49:55,266] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 8038: learning rate 0.0000
[2019-04-04 04:49:55,300] A3C_AGENT_WORKER-Thread-4 INFO:Local step 500, global step 8049: loss -1.1646
[2019-04-04 04:49:55,322] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 500, global step 8049: learning rate 0.0000
[2019-04-04 04:49:55,347] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 8056: loss -1.6939
[2019-04-04 04:49:55,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 8056: learning rate 0.0000
[2019-04-04 04:49:55,440] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 8071: loss -1.2711
[2019-04-04 04:49:55,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 8071: learning rate 0.0000
[2019-04-04 04:49:55,531] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8100: loss -1.8541
[2019-04-04 04:49:55,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8100: learning rate 0.0000
[2019-04-04 04:49:56,008] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 8200: loss -1.2962
[2019-04-04 04:49:56,009] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 8200: learning rate 0.0000
[2019-04-04 04:49:56,124] A3C_AGENT_WORKER-Thread-6 INFO:Local step 500, global step 8233: loss -3.2570
[2019-04-04 04:49:56,137] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 500, global step 8233: learning rate 0.0000
[2019-04-04 04:49:56,260] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8261: loss -2.3550
[2019-04-04 04:49:56,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8261: learning rate 0.0000
[2019-04-04 04:50:01,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9544150e-01 3.4954605e-01 5.0664963e-05 1.5582480e-01 4.2621706e-02
 1.0028374e-02 4.6486918e-02], sum to 1.0000
[2019-04-04 04:50:01,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6776
[2019-04-04 04:50:01,543] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-12.8, 75.83333333333334, 0.0, 0.0, 19.0, 19.516575971699, -1.04717829769932, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [-12.8, 77.0, 0.0, 0.0, 19.0, 19.41666550116287, -1.076564313564197, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.1080332409972299, 0.77, 0.0, 0.0, 0.08333333333333333, 0.11805545843023928, 0.14114522881193436, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3346846], dtype=float32), -2.0930102]. 
=============================================
[2019-04-04 04:50:10,306] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2471675e-01 4.7289774e-01 2.4117540e-04 2.0305382e-01 3.3650074e-02
 2.8277870e-02 3.7162576e-02], sum to 1.0000
[2019-04-04 04:50:10,306] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8663
[2019-04-04 04:50:10,562] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [-9.4, 39.33333333333334, 0.0, 0.0, 19.0, 19.74320408526898, -1.046980068157081, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 409800.0000, 
sim time next is 410400.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 19.0, 19.61349565369866, -1.129264129621416, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.08333333333333333, 0.134457971141555, 0.12357862345952797, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8683075], dtype=float32), 0.5094447]. 
=============================================
[2019-04-04 04:50:28,636] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15189: loss 17.7627
[2019-04-04 04:50:28,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15189: learning rate 0.0000
[2019-04-04 04:50:30,164] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15510: loss 16.4902
[2019-04-04 04:50:30,166] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15510: learning rate 0.0000
[2019-04-04 04:50:30,191] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15517: loss 7.2607
[2019-04-04 04:50:30,205] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15517: learning rate 0.0000
[2019-04-04 04:50:31,410] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15818: loss 13.7373
[2019-04-04 04:50:31,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15818: learning rate 0.0000
[2019-04-04 04:50:31,549] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 15840: loss 10.3964
[2019-04-04 04:50:31,573] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 15840: learning rate 0.0000
[2019-04-04 04:50:31,807] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15915: loss 16.7336
[2019-04-04 04:50:31,812] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15915: learning rate 0.0000
[2019-04-04 04:50:32,249] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16006: loss 8.5913
[2019-04-04 04:50:32,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16006: learning rate 0.0000
[2019-04-04 04:50:32,285] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1000, global step 16017: loss 15.2409
[2019-04-04 04:50:32,286] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1000, global step 16017: learning rate 0.0000
[2019-04-04 04:50:32,519] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16079: loss 19.5976
[2019-04-04 04:50:32,523] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16079: learning rate 0.0000
[2019-04-04 04:50:32,627] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 16109: loss 4.1619
[2019-04-04 04:50:32,629] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 16109: learning rate 0.0000
[2019-04-04 04:50:33,018] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16221: loss 14.1780
[2019-04-04 04:50:33,019] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16221: learning rate 0.0000
[2019-04-04 04:50:33,259] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16293: loss 5.8323
[2019-04-04 04:50:33,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16293: learning rate 0.0000
[2019-04-04 04:50:33,349] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1000, global step 16325: loss 12.2914
[2019-04-04 04:50:33,350] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1000, global step 16325: learning rate 0.0000
[2019-04-04 04:50:33,355] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 16327: loss 11.0158
[2019-04-04 04:50:33,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 16327: learning rate 0.0000
[2019-04-04 04:50:33,540] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1000, global step 16373: loss 16.2968
[2019-04-04 04:50:33,541] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1000, global step 16373: learning rate 0.0000
[2019-04-04 04:50:34,036] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 16510: loss 7.2516
[2019-04-04 04:50:34,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 16510: learning rate 0.0000
[2019-04-04 04:50:44,053] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9150037e-01 2.4342379e-01 5.6008580e-09 4.7038570e-02 1.6864443e-02
 9.2358932e-05 1.0805059e-03], sum to 1.0000
[2019-04-04 04:50:44,053] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3623
[2019-04-04 04:50:44,220] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.73947168357015, -1.265140114331395, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 709200.0000, 
sim time next is 709800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 19.0, 18.68880215047653, -1.273597213268875, 0.0, 1.0, 57338.67409200598], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.08333333333333333, 0.05740017920637749, 0.07546759557704164, 0.0, 1.0, 0.2730413052000285], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.5305068], dtype=float32), -0.29400304]. 
=============================================
[2019-04-04 04:50:47,951] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4135253e-01 2.6807740e-01 2.0475173e-09 7.9360493e-02 1.0046127e-02
 6.3245119e-05 1.1002236e-03], sum to 1.0000
[2019-04-04 04:50:47,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8882
[2019-04-04 04:50:48,138] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 19.0, 18.52681819675755, -1.216113824797983, 0.0, 1.0, 75446.08743854522], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 777000.0000, 
sim time next is 777600.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 19.0, 18.51007365432511, -1.213264064756589, 0.0, 1.0, 64002.34015134344], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.08333333333333333, 0.04250613786042597, 0.09557864508113696, 0.0, 1.0, 0.3047730483397306], 
reward next is 0.6952, 
noisyNet noise sample is [array([-0.74202585], dtype=float32), 0.49575344]. 
=============================================
[2019-04-04 04:50:51,617] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3727902e-01 1.2535283e-01 1.3776039e-08 2.9964916e-02 6.6314111e-03
 8.8885448e-05 6.8287761e-04], sum to 1.0000
[2019-04-04 04:50:51,618] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6125
[2019-04-04 04:50:51,772] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.700000000000001, 72.33333333333334, 0.0, 0.0, 19.0, 18.45708860545821, -1.299135863851535, 1.0, 1.0, 18733.61908289589], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 805200.0000, 
sim time next is 805800.0000, 
raw observation next is [-6.7, 73.66666666666666, 10.66666666666666, 0.0, 19.0, 18.40008848943412, -1.284401915712436, 1.0, 1.0, 18730.24254916934], 
processed observation next is [1.0, 0.30434782608695654, 0.2770083102493075, 0.7366666666666666, 0.035555555555555535, 0.0, 0.08333333333333333, 0.0333407074528432, 0.07186602809585467, 1.0, 1.0, 0.08919163118652067], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.60296935], dtype=float32), -0.15479526]. 
=============================================
[2019-04-04 04:50:51,909] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1802509e-01 2.1474367e-01 2.3655074e-08 5.8834869e-02 6.4075501e-03
 6.1993359e-04 1.3688832e-03], sum to 1.0000
[2019-04-04 04:50:51,917] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8507
[2019-04-04 04:50:52,072] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 71.0, 110.0, 0.0, 19.0, 19.61182820749532, -1.126450009260256, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 819000.0000, 
sim time next is 819600.0000, 
raw observation next is [-4.5, 71.0, 108.1666666666667, 0.0, 19.0, 19.60800927193684, -1.132139041176029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3379501385041552, 0.71, 0.3605555555555557, 0.0, 0.08333333333333333, 0.13400077266140334, 0.12262031960799032, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4972447], dtype=float32), 0.7819608]. 
=============================================
[2019-04-04 04:50:53,663] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 22577: loss -2.2811
[2019-04-04 04:50:53,716] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 22577: learning rate 0.0000
[2019-04-04 04:50:53,779] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2261491e-01 6.8869859e-01 1.4146492e-08 7.9766929e-02 7.0755305e-03
 2.7515276e-04 1.5688465e-03], sum to 1.0000
[2019-04-04 04:50:53,779] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8129
[2019-04-04 04:50:53,949] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 19.0, 18.46113588713389, -1.237440724517293, 1.0, 1.0, 169082.6743438277], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 841800.0000, 
sim time next is 842400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 19.0, 18.66713677315138, -1.273955105706137, 1.0, 1.0, 91595.44404303389], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.82, 0.0, 0.0, 0.08333333333333333, 0.05559473109594837, 0.07534829809795436, 1.0, 1.0, 0.43616878115730423], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3171738], dtype=float32), 1.5038084]. 
=============================================
[2019-04-04 04:50:55,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2247607e-01 2.4154794e-01 2.9553913e-09 2.3199551e-02 1.1933822e-02
 4.0543076e-05 8.0215692e-04], sum to 1.0000
[2019-04-04 04:50:55,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2958
[2019-04-04 04:50:55,762] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 19.0, 18.58037276046958, -1.217616964919696, 0.0, 1.0, 67033.49545287454], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 871200.0000, 
sim time next is 871800.0000, 
raw observation next is [-1.7, 79.00000000000001, 0.0, 0.0, 19.0, 18.69066679286731, -1.202796631901655, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.7900000000000001, 0.0, 0.0, 0.08333333333333333, 0.057555566072275965, 0.099067789366115, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4190711], dtype=float32), 0.68045664]. 
=============================================
[2019-04-04 04:50:56,399] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23517: loss -0.5234
[2019-04-04 04:50:56,403] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23517: learning rate 0.0000
[2019-04-04 04:50:56,650] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23634: loss -1.6683
[2019-04-04 04:50:56,653] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23636: learning rate 0.0000
[2019-04-04 04:50:56,944] A3C_AGENT_WORKER-Thread-5 INFO:Local step 1500, global step 23760: loss -1.5145
[2019-04-04 04:50:56,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 1500, global step 23760: learning rate 0.0000
[2019-04-04 04:50:57,040] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23805: loss -2.4679
[2019-04-04 04:50:57,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23805: learning rate 0.0000
[2019-04-04 04:50:57,434] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23997: loss -0.0041
[2019-04-04 04:50:57,435] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23997: learning rate 0.0000
[2019-04-04 04:50:57,511] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24026: loss -1.7967
[2019-04-04 04:50:57,516] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 24030: loss -0.1226
[2019-04-04 04:50:57,517] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24027: learning rate 0.0000
[2019-04-04 04:50:57,517] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 24030: learning rate 0.0000
[2019-04-04 04:50:57,810] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24179: loss 0.5637
[2019-04-04 04:50:57,810] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24179: learning rate 0.0000
[2019-04-04 04:50:58,030] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24303: loss -6.5792
[2019-04-04 04:50:58,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24303: learning rate 0.0000
[2019-04-04 04:50:58,063] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 24320: loss 0.3045
[2019-04-04 04:50:58,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 24321: learning rate 0.0000
[2019-04-04 04:50:58,214] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 24393: loss -2.3009
[2019-04-04 04:50:58,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 24395: learning rate 0.0000
[2019-04-04 04:50:58,276] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24428: loss -3.1379
[2019-04-04 04:50:58,294] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24434: learning rate 0.0000
[2019-04-04 04:50:58,418] A3C_AGENT_WORKER-Thread-6 INFO:Local step 1500, global step 24489: loss 0.0539
[2019-04-04 04:50:58,419] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 1500, global step 24489: learning rate 0.0000
[2019-04-04 04:50:58,682] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 24610: loss -0.4679
[2019-04-04 04:50:58,683] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 24610: learning rate 0.0000
[2019-04-04 04:50:58,885] A3C_AGENT_WORKER-Thread-4 INFO:Local step 1500, global step 24697: loss -4.3728
[2019-04-04 04:50:58,887] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 1500, global step 24698: learning rate 0.0000
[2019-04-04 04:51:03,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0627165e-01 1.9469209e-01 2.2722316e-08 8.7638617e-02 8.0515724e-03
 4.6394570e-04 2.8820618e-03], sum to 1.0000
[2019-04-04 04:51:03,805] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2933
[2019-04-04 04:51:03,822] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [14.4, 81.0, 98.16666666666667, 0.0, 19.0, 20.43135811036434, -0.7552740161116732, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1002000.0000, 
sim time next is 1002600.0000, 
raw observation next is [14.4, 81.0, 94.0, 0.0, 19.0, 19.29570307665911, -0.8254038061629427, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.31333333333333335, 0.0, 0.08333333333333333, 0.10797525638825907, 0.22486539794568575, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.80074674], dtype=float32), 0.93871516]. 
=============================================
[2019-04-04 04:51:04,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4510187e-01 1.3670033e-01 2.5053710e-06 8.8701136e-02 2.3318805e-02
 2.0200715e-03 4.1552437e-03], sum to 1.0000
[2019-04-04 04:51:04,183] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0051
[2019-04-04 04:51:04,204] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 19.0, 22.16979098981354, -0.336061625646134, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1102200.0000, 
sim time next is 1102800.0000, 
raw observation next is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 19.0, 22.12031799551805, -0.3452278636546677, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8984302862419208, 0.5433333333333334, 0.0, 0.0, 0.08333333333333333, 0.3433598329598375, 0.3849240454484441, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.78334177], dtype=float32), -0.6778912]. 
=============================================
[2019-04-04 04:51:08,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.4476072e-01 1.1709224e-01 6.3916893e-08 3.0883888e-02 6.0544461e-03
 2.1887405e-04 9.8982081e-04], sum to 1.0000
[2019-04-04 04:51:08,926] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2039
[2019-04-04 04:51:09,085] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 19.0, 21.89703749272351, -0.3924088684272256, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1102200.0000, 
sim time next is 1102800.0000, 
raw observation next is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 19.0, 21.86967519596591, -0.3990834134310552, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8984302862419208, 0.5433333333333334, 0.0, 0.0, 0.08333333333333333, 0.32247293299715923, 0.3669721955229816, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21912742], dtype=float32), 0.38796553]. 
=============================================
[2019-04-04 04:51:09,419] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 29823: loss 10.2169
[2019-04-04 04:51:09,421] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 29823: learning rate 0.0000
[2019-04-04 04:51:10,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.7844557e-01 1.0122636e-01 1.3243164e-11 1.8034784e-02 2.1257075e-03
 9.5277037e-06 1.5799116e-04], sum to 1.0000
[2019-04-04 04:51:10,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9735
[2019-04-04 04:51:10,200] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [13.0, 63.0, 0.0, 0.0, 19.0, 20.62616305393607, -0.584334855813658, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1114200.0000, 
sim time next is 1114800.0000, 
raw observation next is [12.9, 63.33333333333333, 0.0, 0.0, 19.0, 20.5929961341844, -0.592218293342518, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8199445983379503, 0.6333333333333333, 0.0, 0.0, 0.08333333333333333, 0.21608301118203332, 0.30259390221916066, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.376933], dtype=float32), -0.121122964]. 
=============================================
[2019-04-04 04:51:12,697] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31296: loss 9.1493
[2019-04-04 04:51:12,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31296: learning rate 0.0000
[2019-04-04 04:51:13,185] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31518: loss 9.4773
[2019-04-04 04:51:13,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31519: learning rate 0.0000
[2019-04-04 04:51:13,482] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 31680: loss 9.2560
[2019-04-04 04:51:13,489] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 31680: learning rate 0.0000
[2019-04-04 04:51:13,644] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2000, global step 31756: loss 8.8347
[2019-04-04 04:51:13,658] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2000, global step 31756: learning rate 0.0000
[2019-04-04 04:51:13,782] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7349358e-01 1.6340634e-02 1.9836567e-10 9.1400333e-03 9.7586983e-04
 1.5971053e-06 4.8268739e-05], sum to 1.0000
[2019-04-04 04:51:13,787] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.2540789e-01 5.2293248e-02 8.7051477e-09 1.8215489e-02 3.5441585e-03
 4.9149348e-05 4.9008417e-04], sum to 1.0000
[2019-04-04 04:51:13,787] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5032
[2019-04-04 04:51:13,795] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2930
[2019-04-04 04:51:13,801] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 19.0, 19.6169358692671, -0.8087180302307407, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1172400.0000, 
sim time next is 1173000.0000, 
raw observation next is [18.3, 65.0, 148.6666666666667, 0.0, 19.0, 19.64600440850794, -0.8043536351704711, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.4955555555555557, 0.0, 0.08333333333333333, 0.13716703404232847, 0.23188212160984298, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2106156], dtype=float32), 3.179531]. 
=============================================
[2019-04-04 04:51:13,804] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 19.0, 19.92790944834229, -0.7491743539326889, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1205400.0000, 
sim time next is 1206000.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 19.0, 19.91598547207788, -0.7519220826603271, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 1.0, 0.922437673130194, 0.75, 0.0, 0.0, 0.08333333333333333, 0.15966545600649015, 0.24935930577989096, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00630925], dtype=float32), 0.31402084]. 
=============================================
[2019-04-04 04:51:13,822] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[21.942715]
 [21.85349 ]
 [21.765465]
 [21.69068 ]
 [21.612757]], R is [[22.78341293]
 [23.55557823]
 [24.32002258]
 [25.07682228]
 [25.82605362]].
[2019-04-04 04:51:13,843] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31852: loss 9.2535
[2019-04-04 04:51:13,848] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31852: learning rate 0.0000
[2019-04-04 04:51:13,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[28.540712]
 [28.480446]
 [28.384398]
 [28.282692]
 [28.193233]], R is [[29.3549614 ]
 [30.06141281]
 [30.76079941]
 [31.45319176]
 [32.13866043]].
[2019-04-04 04:51:14,328] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 32103: loss 8.7124
[2019-04-04 04:51:14,329] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32103: loss 8.7549
[2019-04-04 04:51:14,331] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 32104: learning rate 0.0000
[2019-04-04 04:51:14,334] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32104: learning rate 0.0000
[2019-04-04 04:51:14,437] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32165: loss 9.3838
[2019-04-04 04:51:14,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32165: learning rate 0.0000
[2019-04-04 04:51:14,655] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 32252: loss 8.7280
[2019-04-04 04:51:14,658] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 32252: learning rate 0.0000
[2019-04-04 04:51:14,720] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32298: loss 8.5198
[2019-04-04 04:51:14,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32298: learning rate 0.0000
[2019-04-04 04:51:14,921] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2000, global step 32402: loss 11.7123
[2019-04-04 04:51:14,923] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2000, global step 32402: learning rate 0.0000
[2019-04-04 04:51:15,237] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 32543: loss 8.4139
[2019-04-04 04:51:15,241] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 32544: learning rate 0.0000
[2019-04-04 04:51:15,421] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2000, global step 32601: loss 8.5005
[2019-04-04 04:51:15,423] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2000, global step 32601: learning rate 0.0000
[2019-04-04 04:51:15,493] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32640: loss 8.4995
[2019-04-04 04:51:15,494] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32640: learning rate 0.0000
[2019-04-04 04:51:15,702] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 32724: loss 8.2569
[2019-04-04 04:51:15,707] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 32726: learning rate 0.0000
[2019-04-04 04:51:21,943] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9515545e-01 3.8080506e-03 6.2411060e-15 1.0168067e-03 1.8592471e-05
 1.2917380e-08 1.0559060e-06], sum to 1.0000
[2019-04-04 04:51:21,953] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9887
[2019-04-04 04:51:21,963] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.1, 92.0, 9.0, 0.0, 19.0, 19.28401989521412, -0.9186412496694345, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1324800.0000, 
sim time next is 1325400.0000, 
raw observation next is [1.0, 92.0, 12.0, 0.0, 19.0, 19.28410257727689, -0.9237858452368531, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4903047091412743, 0.92, 0.04, 0.0, 0.08333333333333333, 0.10700854810640752, 0.19207138492104894, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4093758], dtype=float32), -1.1345282]. 
=============================================
[2019-04-04 04:51:22,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7494417e-01 2.4407553e-02 1.8085842e-16 6.3499686e-04 1.3131366e-05
 2.7576241e-09 1.5820972e-07], sum to 1.0000
[2019-04-04 04:51:22,951] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7358
[2019-04-04 04:51:23,045] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.25, 95.5, 0.0, 0.0, 19.0, 18.91259632336391, -1.033822809590184, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1377000.0000, 
sim time next is 1377600.0000, 
raw observation next is [0.1666666666666667, 95.33333333333333, 0.0, 0.0, 19.0, 18.76119886073285, -1.056421312909537, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.9533333333333333, 0.0, 0.0, 0.08333333333333333, 0.06343323839440422, 0.14785956236348766, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1826614], dtype=float32), -0.7689512]. 
=============================================
[2019-04-04 04:51:24,024] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.8533010e-01 9.4199684e-03 1.5901156e-13 5.0465087e-03 1.9382371e-04
 1.2102885e-07 9.4928073e-06], sum to 1.0000
[2019-04-04 04:51:24,033] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7446
[2019-04-04 04:51:24,142] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-0.2, 96.66666666666667, 0.0, 0.0, 19.0, 19.05032631255623, -1.020111654668418, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1394400.0000, 
sim time next is 1395000.0000, 
raw observation next is [-0.3, 97.5, 0.0, 0.0, 19.0, 19.05626734529179, -1.045800641880497, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.4542936288088643, 0.975, 0.0, 0.0, 0.08333333333333333, 0.08802227877431583, 0.1513997860398343, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2817058], dtype=float32), 1.0391263]. 
=============================================
[2019-04-04 04:51:24,165] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[39.79621 ]
 [39.63581 ]
 [40.114033]
 [40.59044 ]
 [41.318527]], R is [[40.46329117]
 [41.0586586 ]
 [41.64807129]
 [42.23159027]
 [42.80927658]].
[2019-04-04 04:51:24,507] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.5672119e-01 4.0628243e-02 3.1794612e-14 2.3958527e-03 2.4769895e-04
 8.2405045e-08 6.8472723e-06], sum to 1.0000
[2019-04-04 04:51:24,510] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3458
[2019-04-04 04:51:24,521] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 19.0, 18.72093388620648, -1.093060096040257, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1407000.0000, 
sim time next is 1407600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 19.0, 18.70750578874734, -1.093309564878789, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.08333333333333333, 0.05895881572894505, 0.13556347837373703, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5553614], dtype=float32), 0.06345741]. 
=============================================
[2019-04-04 04:51:26,473] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 38177: loss 6.6464
[2019-04-04 04:51:26,475] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 38178: learning rate 0.0000
[2019-04-04 04:51:28,430] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39404: loss 5.1547
[2019-04-04 04:51:28,430] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39404: learning rate 0.0000
[2019-04-04 04:51:28,716] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39581: loss 6.7364
[2019-04-04 04:51:28,716] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39581: learning rate 0.0000
[2019-04-04 04:51:29,006] A3C_AGENT_WORKER-Thread-5 INFO:Local step 2500, global step 39777: loss -9.1719
[2019-04-04 04:51:29,009] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 2500, global step 39777: learning rate 0.0000
[2019-04-04 04:51:29,184] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39872: loss 6.7639
[2019-04-04 04:51:29,186] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39873: learning rate 0.0000
[2019-04-04 04:51:29,303] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 39953: loss 6.9494
[2019-04-04 04:51:29,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 39953: learning rate 0.0000
[2019-04-04 04:51:29,360] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39979: loss 7.4552
[2019-04-04 04:51:29,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39981: learning rate 0.0000
[2019-04-04 04:51:29,474] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40056: loss 7.2107
[2019-04-04 04:51:29,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40056: learning rate 0.0000
[2019-04-04 04:51:29,590] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40132: loss 6.8233
[2019-04-04 04:51:29,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40132: learning rate 0.0000
[2019-04-04 04:51:29,735] A3C_AGENT_WORKER-Thread-6 INFO:Local step 2500, global step 40223: loss 6.9347
[2019-04-04 04:51:29,736] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 2500, global step 40223: learning rate 0.0000
[2019-04-04 04:51:29,748] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 40228: loss 6.5674
[2019-04-04 04:51:29,749] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 40228: learning rate 0.0000
[2019-04-04 04:51:29,989] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40394: loss 7.3437
[2019-04-04 04:51:29,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40394: learning rate 0.0000
[2019-04-04 04:51:30,055] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 40443: loss 7.6891
[2019-04-04 04:51:30,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 40443: learning rate 0.0000
[2019-04-04 04:51:30,102] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40464: loss 7.4798
[2019-04-04 04:51:30,104] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40464: learning rate 0.0000
[2019-04-04 04:51:30,373] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 40646: loss 7.3706
[2019-04-04 04:51:30,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 40646: learning rate 0.0000
[2019-04-04 04:51:30,521] A3C_AGENT_WORKER-Thread-4 INFO:Local step 2500, global step 40742: loss 7.3700
[2019-04-04 04:51:30,522] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 2500, global step 40743: learning rate 0.0000
[2019-04-04 04:51:30,595] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.57556546e-01 2.77988762e-02 2.04219731e-11 1.37611860e-02
 7.64887023e-04 4.71596422e-06 1.13818205e-04], sum to 1.0000
[2019-04-04 04:51:30,596] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7700
[2019-04-04 04:51:30,617] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [13.8, 49.0, 162.5, 62.0, 19.0, 21.20277563908981, -0.7006151137071983, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1602000.0000, 
sim time next is 1602600.0000, 
raw observation next is [13.8, 49.0, 167.0, 41.33333333333332, 19.0, 21.0796678697386, -0.6463662479372351, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5566666666666666, 0.045672191528545104, 0.08333333333333333, 0.2566389891448833, 0.28454458402092164, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.55919385], dtype=float32), -1.3281014]. 
=============================================
[2019-04-04 04:51:38,058] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.7156852e-01 1.4410089e-02 3.7667872e-12 1.3046543e-02 8.9535344e-04
 1.7153510e-06 7.7788136e-05], sum to 1.0000
[2019-04-04 04:51:38,063] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9446
[2019-04-04 04:51:38,067] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.5, 92.0, 59.5, 0.0, 19.0, 19.96672615950214, -0.9005408433577085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [1.433333333333333, 92.0, 61.66666666666667, 0.0, 19.0, 20.03050791450795, -0.8993789123218958, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.502308402585411, 0.92, 0.20555555555555557, 0.0, 0.08333333333333333, 0.16920899287566252, 0.20020702922603473, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4535875], dtype=float32), -0.7569607]. 
=============================================
[2019-04-04 04:51:41,530] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47132: loss 1.7605
[2019-04-04 04:51:41,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47132: learning rate 0.0000
[2019-04-04 04:51:43,750] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47688: loss 4.4384
[2019-04-04 04:51:43,751] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47688: learning rate 0.0000
[2019-04-04 04:51:44,384] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3000, global step 47846: loss 1.3152
[2019-04-04 04:51:44,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3000, global step 47846: learning rate 0.0000
[2019-04-04 04:51:44,657] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47915: loss 1.2819
[2019-04-04 04:51:44,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47915: learning rate 0.0000
[2019-04-04 04:51:44,717] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47931: loss 1.3423
[2019-04-04 04:51:44,717] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47931: learning rate 0.0000
[2019-04-04 04:51:44,879] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47974: loss 1.3647
[2019-04-04 04:51:44,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47974: learning rate 0.0000
[2019-04-04 04:51:45,139] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48059: loss 2.4210
[2019-04-04 04:51:45,140] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48059: learning rate 0.0000
[2019-04-04 04:51:45,144] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48061: loss 1.5585
[2019-04-04 04:51:45,145] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48061: learning rate 0.0000
[2019-04-04 04:51:45,275] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48109: loss 1.6406
[2019-04-04 04:51:45,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48109: learning rate 0.0000
[2019-04-04 04:51:45,308] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48121: loss 1.2492
[2019-04-04 04:51:45,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48121: learning rate 0.0000
[2019-04-04 04:51:45,477] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3000, global step 48189: loss 1.5636
[2019-04-04 04:51:45,478] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3000, global step 48189: learning rate 0.0000
[2019-04-04 04:51:45,610] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48242: loss 1.2590
[2019-04-04 04:51:45,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48242: learning rate 0.0000
[2019-04-04 04:51:46,093] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 48459: loss 3.7192
[2019-04-04 04:51:46,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 48460: learning rate 0.0000
[2019-04-04 04:51:46,152] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48488: loss 3.8001
[2019-04-04 04:51:46,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48488: learning rate 0.0000
[2019-04-04 04:51:46,231] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48522: loss 1.2924
[2019-04-04 04:51:46,233] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48523: learning rate 0.0000
[2019-04-04 04:51:46,836] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3000, global step 48819: loss 1.2037
[2019-04-04 04:51:46,837] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3000, global step 48819: learning rate 0.0000
[2019-04-04 04:51:51,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.7411752e-01 1.7586825e-02 6.5798854e-14 8.2112113e-03 7.8819809e-05
 1.3808769e-07 5.5196610e-06], sum to 1.0000
[2019-04-04 04:51:51,711] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9875
[2019-04-04 04:51:51,754] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [-4.5, 75.0, 50.5, 0.0, 19.0, 18.85866899851202, -1.179300213759957, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1872000.0000, 
sim time next is 1872600.0000, 
raw observation next is [-4.5, 76.33333333333333, 43.33333333333333, 0.0, 19.0, 18.82194985460088, -1.195108832189167, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.7633333333333333, 0.14444444444444443, 0.0, 0.08333333333333333, 0.06849582121674007, 0.10163038927027768, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37972683], dtype=float32), -0.48988134]. 
=============================================
[2019-04-04 04:51:54,585] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9450040e-01 4.9977605e-03 1.3450477e-16 4.9178710e-04 9.7764741e-06
 9.5276498e-10 3.5113081e-07], sum to 1.0000
[2019-04-04 04:51:54,590] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5097
[2019-04-04 04:51:54,623] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 19.0, 18.61590967382331, -1.23158939517659, 0.0, 1.0, 18732.26233667998], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 1992600.0000, 
sim time next is 1993200.0000, 
raw observation next is [-5.8, 84.33333333333333, 0.0, 0.0, 19.0, 18.57664366002116, -1.235720193627411, 0.0, 1.0, 45109.39597107043], 
processed observation next is [1.0, 0.043478260869565216, 0.30193905817174516, 0.8433333333333333, 0.0, 0.0, 0.08333333333333333, 0.04805363833509654, 0.08809326879086303, 0.0, 1.0, 0.21480664748128775], 
reward next is 0.7852, 
noisyNet noise sample is [array([-0.4387815], dtype=float32), 0.779354]. 
=============================================
[2019-04-04 04:51:59,767] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.6147281e-01 3.3124264e-02 1.6756675e-14 4.3157809e-03 1.0264638e-03
 1.9047646e-07 6.0429935e-05], sum to 1.0000
[2019-04-04 04:51:59,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2467
[2019-04-04 04:51:59,798] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 19.0, 18.56084089971583, -1.25050331060882, 0.0, 1.0, 27291.60955356902], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2009400.0000, 
sim time next is 2010000.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 19.0, 18.61337994836271, -1.261085990496813, 0.0, 1.0, 18735.97594209254], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.08333333333333333, 0.05111499569689245, 0.079638003167729, 0.0, 1.0, 0.08921893305758352], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.8222562], dtype=float32), 0.22242059]. 
=============================================
[2019-04-04 04:51:59,809] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[47.16118 ]
 [46.843086]
 [46.474957]
 [46.083435]
 [45.83311 ]], R is [[47.62608337]
 [48.01986313]
 [48.23153687]
 [48.32370758]
 [48.49769592]].
[2019-04-04 04:52:00,308] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 54369: loss 0.8805
[2019-04-04 04:52:00,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 54369: learning rate 0.0000
[2019-04-04 04:52:01,375] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.95720923e-01 3.46296234e-03 1.66882325e-15 6.90537039e-04
 1.07812484e-04 3.66196389e-08 1.77156944e-05], sum to 1.0000
[2019-04-04 04:52:01,377] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1217
[2019-04-04 04:52:01,389] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-4.5, 89.33333333333334, 0.0, 0.0, 19.0, 18.61190308393486, -1.223365544854953, 0.0, 1.0, 54084.72426178946], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2078400.0000, 
sim time next is 2079000.0000, 
raw observation next is [-4.5, 88.5, 0.0, 0.0, 19.0, 18.55585541197088, -1.217189200836214, 0.0, 1.0, 73353.40101987611], 
processed observation next is [1.0, 0.043478260869565216, 0.3379501385041552, 0.885, 0.0, 0.0, 0.08333333333333333, 0.0463212843309068, 0.09427026638792868, 0.0, 1.0, 0.34930190961845764], 
reward next is 0.6507, 
noisyNet noise sample is [array([-1.1143426], dtype=float32), 1.4437056]. 
=============================================
[2019-04-04 04:52:01,394] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.4902152e-01 3.2435630e-02 3.0550010e-11 1.5335569e-02 2.5476371e-03
 6.2159834e-06 6.5339002e-04], sum to 1.0000
[2019-04-04 04:52:01,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5071
[2019-04-04 04:52:01,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[51.5336  ]
 [51.936764]
 [52.57519 ]
 [53.30179 ]
 [53.844543]], R is [[51.3324585 ]
 [51.56158829]
 [51.79053116]
 [52.18347549]
 [52.66164017]].
[2019-04-04 04:52:01,460] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-4.5, 76.33333333333334, 154.5, 0.0, 19.0, 19.58287083935122, -1.156069841591739, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2031600.0000, 
sim time next is 2032200.0000, 
raw observation next is [-4.5, 77.0, 156.0, 0.0, 19.0, 19.09406394805092, -1.186098177947749, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.77, 0.52, 0.0, 0.08333333333333333, 0.09117199567090999, 0.10463394068408365, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.20260522], dtype=float32), 2.2226515]. 
=============================================
[2019-04-04 04:52:02,559] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55298: loss 0.9257
[2019-04-04 04:52:02,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55298: learning rate 0.0000
[2019-04-04 04:52:03,135] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55577: loss 0.9058
[2019-04-04 04:52:03,141] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55579: learning rate 0.0000
[2019-04-04 04:52:03,249] A3C_AGENT_WORKER-Thread-5 INFO:Local step 3500, global step 55628: loss 0.8644
[2019-04-04 04:52:03,253] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 3500, global step 55630: learning rate 0.0000
[2019-04-04 04:52:03,466] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55729: loss 0.8006
[2019-04-04 04:52:03,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55729: learning rate 0.0000
[2019-04-04 04:52:03,640] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55811: loss 0.8403
[2019-04-04 04:52:03,640] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55811: learning rate 0.0000
[2019-04-04 04:52:03,867] A3C_AGENT_WORKER-Thread-6 INFO:Local step 3500, global step 55937: loss 0.9154
[2019-04-04 04:52:03,869] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 3500, global step 55938: learning rate 0.0000
[2019-04-04 04:52:03,930] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 55972: loss -2.0095
[2019-04-04 04:52:03,931] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 55973: learning rate 0.0000
[2019-04-04 04:52:04,148] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56090: loss 0.9790
[2019-04-04 04:52:04,148] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56090: learning rate 0.0000
[2019-04-04 04:52:04,219] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56124: loss 0.9937
[2019-04-04 04:52:04,220] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56124: learning rate 0.0000
[2019-04-04 04:52:04,325] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56182: loss 0.9884
[2019-04-04 04:52:04,325] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56182: learning rate 0.0000
[2019-04-04 04:52:04,616] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56350: loss 0.9784
[2019-04-04 04:52:04,618] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56351: learning rate 0.0000
[2019-04-04 04:52:04,941] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56544: loss 0.9323
[2019-04-04 04:52:04,943] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56545: learning rate 0.0000
[2019-04-04 04:52:04,972] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 56560: loss 0.8950
[2019-04-04 04:52:04,973] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 56560: learning rate 0.0000
[2019-04-04 04:52:05,339] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56743: loss 0.8165
[2019-04-04 04:52:05,342] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56743: learning rate 0.0000
[2019-04-04 04:52:06,281] A3C_AGENT_WORKER-Thread-4 INFO:Local step 3500, global step 57112: loss 0.8926
[2019-04-04 04:52:06,282] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 3500, global step 57112: learning rate 0.0000
[2019-04-04 04:52:06,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.6283782e-01 2.8428702e-02 5.3081026e-13 7.6446794e-03 4.6233137e-04
 3.4444929e-07 6.2613946e-04], sum to 1.0000
[2019-04-04 04:52:06,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0605
[2019-04-04 04:52:06,321] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 19.0, 18.4994000497262, -1.255263649558673, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2100000.0000, 
sim time next is 2100600.0000, 
raw observation next is [-7.0, 78.5, 0.0, 0.0, 19.0, 18.52172805508638, -1.263658370039121, 1.0, 1.0, 18735.85677769079], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.785, 0.0, 0.0, 0.08333333333333333, 0.0434773379238651, 0.07878054332029298, 1.0, 1.0, 0.08921836560805138], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1490725], dtype=float32), 0.48115638]. 
=============================================
[2019-04-04 04:52:16,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6842644e-01 3.5847429e-02 2.7801249e-08 7.5291932e-02 6.9509032e-03
 7.1677164e-04 1.2766502e-02], sum to 1.0000
[2019-04-04 04:52:16,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7510
[2019-04-04 04:52:16,174] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.0, 44.0, 89.5, 21.0, 19.0, 20.11711634015717, -0.9898323060311532, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2304000.0000, 
sim time next is 2304600.0000, 
raw observation next is [-0.09999999999999999, 44.83333333333334, 73.33333333333333, 14.0, 19.0, 20.184898075277, -0.9847644653602255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4598337950138504, 0.4483333333333334, 0.24444444444444444, 0.015469613259668509, 0.08333333333333333, 0.18207483960641677, 0.17174517821325816, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36189008], dtype=float32), -1.1318318]. 
=============================================
[2019-04-04 04:52:16,878] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.1478685e-01 7.6703668e-02 2.8224898e-07 4.5375381e-02 1.8941123e-02
 2.1562271e-03 4.2036474e-02], sum to 1.0000
[2019-04-04 04:52:16,882] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-04 04:52:16,907] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.2500000000000001, 44.0, 121.0, 60.0, 19.0, 19.82397951034944, -1.056135383072593, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 2298600.0000, 
sim time next is 2299200.0000, 
raw observation next is [0.5333333333333334, 43.66666666666666, 123.8333333333333, 57.0, 21.0, 19.84712802992818, -1.031286087542293, 1.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.6086956521739131, 0.4773776546629733, 0.4366666666666666, 0.4127777777777777, 0.06298342541436464, 0.25, 0.15392733582734822, 0.1562379708192357, 1.0, 1.0, 0.9343709972347434], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8521653], dtype=float32), -0.7555277]. 
=============================================
[2019-04-04 04:52:18,286] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.7130537e-01 2.0634137e-02 3.6642057e-12 5.2967733e-03 8.4975862e-04
 7.7390960e-06 1.9062783e-03], sum to 1.0000
[2019-04-04 04:52:18,287] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4190
[2019-04-04 04:52:18,299] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-3.4, 69.0, 37.0, 0.0, 19.0, 18.70351357449159, -1.239916882465113, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2363400.0000, 
sim time next is 2364000.0000, 
raw observation next is [-3.4, 69.0, 51.0, 59.99999999999999, 19.0, 18.6268175522185, -1.248401772747839, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.17, 0.06629834254143646, 0.08333333333333333, 0.05223479601820843, 0.08386607575072036, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5294883], dtype=float32), 0.9891215]. 
=============================================
[2019-04-04 04:52:18,318] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[38.658268]
 [38.74033 ]
 [38.863106]
 [38.95864 ]
 [38.885735]], R is [[39.1679306 ]
 [39.77625275]
 [40.37849045]
 [40.97470474]
 [41.3285141 ]].
[2019-04-04 04:52:18,541] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 62721: loss 6.0276
[2019-04-04 04:52:18,545] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 62721: learning rate 0.0000
[2019-04-04 04:52:20,305] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63451: loss 6.5429
[2019-04-04 04:52:20,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63451: learning rate 0.0000
[2019-04-04 04:52:20,714] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63606: loss 6.1557
[2019-04-04 04:52:20,714] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63606: learning rate 0.0000
[2019-04-04 04:52:21,174] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4000, global step 63780: loss 6.0406
[2019-04-04 04:52:21,175] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4000, global step 63780: learning rate 0.0000
[2019-04-04 04:52:21,205] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63789: loss 17.9807
[2019-04-04 04:52:21,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63789: learning rate 0.0000
[2019-04-04 04:52:21,244] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63808: loss 20.9074
[2019-04-04 04:52:21,247] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63811: learning rate 0.0000
[2019-04-04 04:52:21,889] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64056: loss 5.7124
[2019-04-04 04:52:21,893] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64056: learning rate 0.0000
[2019-04-04 04:52:21,932] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4000, global step 64075: loss 5.5753
[2019-04-04 04:52:21,933] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64075: loss 8.0470
[2019-04-04 04:52:21,933] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4000, global step 64075: learning rate 0.0000
[2019-04-04 04:52:21,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64075: learning rate 0.0000
[2019-04-04 04:52:22,037] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64112: loss 5.4438
[2019-04-04 04:52:22,039] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64112: learning rate 0.0000
[2019-04-04 04:52:22,392] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64254: loss 5.4234
[2019-04-04 04:52:22,392] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64254: learning rate 0.0000
[2019-04-04 04:52:22,473] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64294: loss 22.4192
[2019-04-04 04:52:22,475] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64295: learning rate 0.0000
[2019-04-04 04:52:22,686] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64393: loss 5.3079
[2019-04-04 04:52:22,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64393: learning rate 0.0000
[2019-04-04 04:52:22,926] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64512: loss 5.1481
[2019-04-04 04:52:22,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64512: learning rate 0.0000
[2019-04-04 04:52:23,691] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64896: loss 4.7129
[2019-04-04 04:52:23,692] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64896: learning rate 0.0000
[2019-04-04 04:52:24,044] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4000, global step 65066: loss 4.8752
[2019-04-04 04:52:24,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4000, global step 65066: learning rate 0.0000
[2019-04-04 04:52:27,849] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9152595e-01 3.4535942e-03 3.0300471e-15 4.5996597e-03 2.0639155e-04
 9.2100137e-08 2.1431036e-04], sum to 1.0000
[2019-04-04 04:52:27,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9886
[2019-04-04 04:52:27,862] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-1.7, 40.0, 0.0, 0.0, 19.0, 18.79203531689184, -1.263489919606042, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2509200.0000, 
sim time next is 2509800.0000, 
raw observation next is [-1.7, 39.66666666666667, 0.0, 0.0, 19.0, 18.70861648339465, -1.279745468249651, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3966666666666667, 0.0, 0.0, 0.08333333333333333, 0.059051373616220815, 0.07341817725011635, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.306103], dtype=float32), -0.36055887]. 
=============================================
[2019-04-04 04:52:29,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.6192145e-01 3.0440265e-02 8.5454456e-12 7.1491771e-03 2.2581335e-04
 8.7805929e-06 2.5458654e-04], sum to 1.0000
[2019-04-04 04:52:29,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6473
[2019-04-04 04:52:29,964] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [0.8666666666666667, 34.0, 0.0, 0.0, 19.0, 20.33426670038378, -0.9588839017926901, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2569800.0000, 
sim time next is 2570400.0000, 
raw observation next is [0.5, 35.0, 0.0, 0.0, 19.0, 20.32324808224013, -0.919130041221727, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.35, 0.0, 0.0, 0.08333333333333333, 0.19360400685334422, 0.19362331959275766, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.82814205], dtype=float32), -1.1553088]. 
=============================================
[2019-04-04 04:52:32,883] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 69871: loss 2.3678
[2019-04-04 04:52:32,886] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 69871: learning rate 0.0000
[2019-04-04 04:52:35,720] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71287: loss 3.0152
[2019-04-04 04:52:35,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71287: learning rate 0.0000
[2019-04-04 04:52:36,161] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71493: loss 2.6348
[2019-04-04 04:52:36,184] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.0653977e-01 9.5632881e-02 1.3984775e-08 3.3859860e-02 9.3363486e-03
 3.7013942e-03 5.0929751e-02], sum to 1.0000
[2019-04-04 04:52:36,191] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9059
[2019-04-04 04:52:36,192] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71507: learning rate 0.0000
[2019-04-04 04:52:36,236] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [0.5, 45.0, 216.0, 130.0, 19.0, 20.38816672117115, -0.8623044749278433, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2644200.0000, 
sim time next is 2644800.0000, 
raw observation next is [0.5, 45.66666666666667, 205.8333333333333, 142.6666666666667, 19.0, 20.55582747223749, -0.8429877877826644, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4764542936288089, 0.4566666666666667, 0.686111111111111, 0.15764272559852677, 0.08333333333333333, 0.2129856226864574, 0.21900407073911188, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8830755], dtype=float32), 0.64712626]. 
=============================================
[2019-04-04 04:52:36,464] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71634: loss 2.5181
[2019-04-04 04:52:36,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71634: learning rate 0.0000
[2019-04-04 04:52:36,497] A3C_AGENT_WORKER-Thread-5 INFO:Local step 4500, global step 71650: loss 2.6192
[2019-04-04 04:52:36,499] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 4500, global step 71651: learning rate 0.0000
[2019-04-04 04:52:36,743] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71756: loss 9.3300
[2019-04-04 04:52:36,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71768: learning rate 0.0000
[2019-04-04 04:52:36,826] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71789: loss 2.5380
[2019-04-04 04:52:36,846] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71789: learning rate 0.0000
[2019-04-04 04:52:37,144] A3C_AGENT_WORKER-Thread-6 INFO:Local step 4500, global step 71922: loss 1.6660
[2019-04-04 04:52:37,145] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 4500, global step 71922: learning rate 0.0000
[2019-04-04 04:52:37,321] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71998: loss 2.3934
[2019-04-04 04:52:37,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71998: learning rate 0.0000
[2019-04-04 04:52:37,365] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72009: loss 3.0385
[2019-04-04 04:52:37,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72009: learning rate 0.0000
[2019-04-04 04:52:37,510] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72066: loss 15.7697
[2019-04-04 04:52:37,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72066: learning rate 0.0000
[2019-04-04 04:52:37,850] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72178: loss 2.4568
[2019-04-04 04:52:37,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72179: learning rate 0.0000
[2019-04-04 04:52:38,349] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72355: loss 2.6208
[2019-04-04 04:52:38,350] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72355: learning rate 0.0000
[2019-04-04 04:52:38,383] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72370: loss 2.8371
[2019-04-04 04:52:38,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72370: learning rate 0.0000
[2019-04-04 04:52:39,161] A3C_AGENT_WORKER-Thread-4 INFO:Local step 4500, global step 72655: loss 2.4296
[2019-04-04 04:52:39,164] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 4500, global step 72655: learning rate 0.0000
[2019-04-04 04:52:39,755] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72874: loss 3.4620
[2019-04-04 04:52:39,766] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72876: learning rate 0.0000
[2019-04-04 04:52:39,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.4579096e-01 1.1567696e-01 1.9580805e-08 3.7535153e-02 8.1654768e-03
 1.3813544e-03 9.1450125e-02], sum to 1.0000
[2019-04-04 04:52:39,881] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6049
[2019-04-04 04:52:39,935] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-5.4, 57.5, 109.0, 788.0, 19.0, 21.14684660747626, -0.7277408995183522, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2727000.0000, 
sim time next is 2727600.0000, 
raw observation next is [-5.199999999999999, 57.0, 107.8333333333333, 778.8333333333334, 19.0, 21.38082122400088, -0.6136889295257608, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.31855955678670367, 0.57, 0.35944444444444434, 0.8605893186003684, 0.08333333333333333, 0.2817351020000733, 0.29543702349141304, 1.0, 1.0, 0.0], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.23124546], dtype=float32), 0.15709767]. 
=============================================
[2019-04-04 04:52:44,647] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8691143e-01 4.7233813e-02 4.5374868e-10 2.7056299e-02 5.6858244e-03
 2.8234327e-04 1.3283026e-01], sum to 1.0000
[2019-04-04 04:52:44,647] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3233
[2019-04-04 04:52:44,694] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [6.2, 27.0, 60.00000000000001, 71.0, 19.0, 21.78563114492897, -0.5663585185837353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2824800.0000, 
sim time next is 2825400.0000, 
raw observation next is [6.1, 27.5, 49.0, 66.0, 19.0, 21.96181127429713, -0.5524748769450709, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6315789473684211, 0.275, 0.16333333333333333, 0.07292817679558011, 0.08333333333333333, 0.3301509395247608, 0.3158417076849764, 1.0, 1.0, 0.0], 
reward next is 0.4753, 
noisyNet noise sample is [array([1.1848153], dtype=float32), 0.6400506]. 
=============================================
[2019-04-04 04:52:45,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9077498e-01 1.9495717e-01 6.4363093e-13 9.3818419e-03 3.7820274e-03
 7.4640440e-05 1.0102942e-01], sum to 1.0000
[2019-04-04 04:52:45,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4794
[2019-04-04 04:52:45,276] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 19.0, 19.09429432078129, -1.058448702917461, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2841600.0000, 
sim time next is 2842200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 19.0, 19.05168206658792, -1.067186313982908, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.08333333333333333, 0.0876401722156599, 0.14427122867236397, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09576455], dtype=float32), -0.83995044]. 
=============================================
[2019-04-04 04:52:48,719] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.0797013e-01 4.4332895e-02 5.2373597e-15 4.3440829e-03 1.5875734e-03
 1.0742709e-06 4.1764252e-02], sum to 1.0000
[2019-04-04 04:52:48,719] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6089
[2019-04-04 04:52:48,741] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [1.5, 96.5, 0.0, 0.0, 19.0, 19.52198052650961, -1.028015811180391, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 2874600.0000, 
sim time next is 2875200.0000, 
raw observation next is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 19.0, 19.55149394331126, -1.026571107502962, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.5087719298245615, 0.9533333333333335, 0.0, 0.0, 0.08333333333333333, 0.12929116194260507, 0.157809630832346, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.6831367], dtype=float32), -0.22648598]. 
=============================================
[2019-04-04 04:52:51,011] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 77540: loss 19.2852
[2019-04-04 04:52:51,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 77540: learning rate 0.0000
[2019-04-04 04:52:51,552] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7863144e-01 6.3062556e-02 1.0704084e-13 3.8291672e-03 4.1442574e-04
 7.6903980e-06 5.5405468e-01], sum to 1.0000
[2019-04-04 04:52:51,559] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8546
[2019-04-04 04:52:51,575] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 87.5, 0.0, 21.0, 22.14183692145913, -0.4843237573039696, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [23.0], 
sim time this is 2905200.0000, 
sim time next is 2905800.0000, 
raw observation next is [2.0, 100.0, 86.66666666666666, 0.0, 23.0, 22.17515986543786, -0.4866441196556752, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.28888888888888886, 0.0, 0.4166666666666667, 0.34792998878648823, 0.33778529344810826, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.34197137], dtype=float32), -0.58704513]. 
=============================================
[2019-04-04 04:52:55,696] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79327: loss 4.3391
[2019-04-04 04:52:55,705] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79327: learning rate 0.0000
[2019-04-04 04:52:55,891] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79364: loss 1.8777
[2019-04-04 04:52:55,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79364: learning rate 0.0000
[2019-04-04 04:52:56,795] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79631: loss 9.4571
[2019-04-04 04:52:56,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79631: learning rate 0.0000
[2019-04-04 04:52:57,097] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79728: loss -0.7187
[2019-04-04 04:52:57,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79731: learning rate 0.0000
[2019-04-04 04:52:57,202] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5000, global step 79740: loss 4.0408
[2019-04-04 04:52:57,204] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5000, global step 79740: learning rate 0.0000
[2019-04-04 04:52:57,834] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79982: loss 23.4026
[2019-04-04 04:52:57,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79983: learning rate 0.0000
[2019-04-04 04:52:57,921] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79994: loss 19.2728
[2019-04-04 04:52:57,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79994: learning rate 0.0000
[2019-04-04 04:52:58,304] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80147: loss 8.1353
[2019-04-04 04:52:58,305] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80147: learning rate 0.0000
[2019-04-04 04:52:58,496] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5000, global step 80218: loss 18.8193
[2019-04-04 04:52:58,498] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5000, global step 80218: learning rate 0.0000
[2019-04-04 04:52:59,203] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 80503: loss 4.3433
[2019-04-04 04:52:59,205] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 80504: learning rate 0.0000
[2019-04-04 04:52:59,692] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80706: loss 4.3745
[2019-04-04 04:52:59,692] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80706: learning rate 0.0000
[2019-04-04 04:52:59,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.6066099e-01 2.0100342e-02 4.7927326e-16 2.5255287e-03 1.5396688e-03
 4.1176449e-07 1.5173041e-02], sum to 1.0000
[2019-04-04 04:52:59,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5335
[2019-04-04 04:52:59,963] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [-5.166666666666667, 72.0, 0.0, 0.0, 19.0, 20.18696283190254, -0.8680957319700093, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3031800.0000, 
sim time next is 3032400.0000, 
raw observation next is [-5.333333333333334, 73.0, 0.0, 0.0, 19.0, 20.18490386867083, -0.8792593523807212, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.31486611265004616, 0.73, 0.0, 0.0, 0.08333333333333333, 0.18207532238923582, 0.20691354920642627, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5573882], dtype=float32), 0.19538105]. 
=============================================
[2019-04-04 04:53:00,395] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80934: loss 0.1720
[2019-04-04 04:53:00,396] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80934: learning rate 0.0000
[2019-04-04 04:53:00,889] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 81077: loss 0.2748
[2019-04-04 04:53:00,901] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 81077: learning rate 0.0000
[2019-04-04 04:53:02,672] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.7521350e-01 9.5092123e-03 1.7962376e-17 1.3841372e-03 8.9835732e-05
 7.1689641e-07 1.1380256e-01], sum to 1.0000
[2019-04-04 04:53:02,675] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8570
[2019-04-04 04:53:02,695] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [6.8, 99.5, 84.0, 679.0, 19.0, 25.42830015080438, 0.3751214852509077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [19.0], 
sim time this is 3166200.0000, 
sim time next is 3166800.0000, 
raw observation next is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 19.0, 25.46611679141636, 0.3872979949232132, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.9933333333333334, 0.26555555555555554, 0.7171270718232045, 0.08333333333333333, 0.6221763992846968, 0.629099331641071, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5890781], dtype=float32), 1.9132147]. 
=============================================
[2019-04-04 04:53:02,745] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 81626: loss 9.8487
[2019-04-04 04:53:02,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 81626: learning rate 0.0000
[2019-04-04 04:53:02,955] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5000, global step 81662: loss 2.3504
[2019-04-04 04:53:02,957] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5000, global step 81662: learning rate 0.0000
[2019-04-04 04:53:12,369] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 85304: loss 2.0643
[2019-04-04 04:53:12,369] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 85304: learning rate 0.0000
[2019-04-04 04:53:14,531] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.1576141e-02 1.4737745e-02 1.4164062e-20 3.4254929e-03 1.0101793e-04
 2.9089790e-08 9.1015959e-01], sum to 1.0000
[2019-04-04 04:53:14,532] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8448
[2019-04-04 04:53:14,586] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 85.0, 370.0, 26.0, 25.29765230003321, 0.5254874249701503, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3227400.0000, 
sim time next is 3228000.0000, 
raw observation next is [-3.0, 92.0, 87.66666666666667, 417.1666666666667, 26.0, 25.99985230836275, 0.5681482449771812, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.2922222222222222, 0.4609576427255985, 0.6666666666666666, 0.6666543590302293, 0.689382748325727, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0934147], dtype=float32), -0.462249]. 
=============================================
[2019-04-04 04:53:14,598] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.38069 ]
 [91.30622 ]
 [91.508804]
 [91.95844 ]
 [93.62339 ]], R is [[89.59195709]
 [89.69603729]
 [88.93074036]
 [88.0860672 ]
 [88.20520782]].
[2019-04-04 04:53:14,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7378676e-03 1.1591678e-02 3.6219709e-20 1.0183734e-04 1.5919536e-06
 2.8705680e-10 9.8456699e-01], sum to 1.0000
[2019-04-04 04:53:14,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1988
[2019-04-04 04:53:15,090] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 87.5, 112.3333333333333, 815.6666666666666, 26.0, 25.55753912600688, 0.5491921225911098, 1.0, 1.0, 109700.8167028079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3244200.0000, 
sim time next is 3244800.0000, 
raw observation next is [-2.666666666666667, 90.0, 111.6666666666667, 813.8333333333334, 26.0, 24.82226570453913, 0.5623589489917559, 1.0, 1.0, 197081.5791494142], 
processed observation next is [1.0, 0.5652173913043478, 0.38873499538319484, 0.9, 0.37222222222222234, 0.8992633517495396, 0.6666666666666666, 0.5685221420449276, 0.687452982997252, 1.0, 1.0, 0.9384837102353057], 
reward next is 0.0615, 
noisyNet noise sample is [array([0.59693277], dtype=float32), 1.8424429]. 
=============================================
[2019-04-04 04:53:16,102] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 86840: loss 0.0399
[2019-04-04 04:53:16,103] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 86840: learning rate 0.0000
[2019-04-04 04:53:16,476] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 86988: loss 3.7335
[2019-04-04 04:53:16,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 86988: learning rate 0.0000
[2019-04-04 04:53:17,437] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87379: loss 0.0021
[2019-04-04 04:53:17,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87379: learning rate 0.0000
[2019-04-04 04:53:17,445] A3C_AGENT_WORKER-Thread-5 INFO:Local step 5500, global step 87385: loss -1.4390
[2019-04-04 04:53:17,447] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 5500, global step 87385: learning rate 0.0000
[2019-04-04 04:53:17,536] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87427: loss -2.2830
[2019-04-04 04:53:17,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87427: learning rate 0.0000
[2019-04-04 04:53:17,847] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87549: loss -0.0932
[2019-04-04 04:53:17,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87549: learning rate 0.0000
[2019-04-04 04:53:18,594] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87805: loss -0.9054
[2019-04-04 04:53:18,595] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87805: learning rate 0.0000
[2019-04-04 04:53:19,033] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87966: loss -1.8325
[2019-04-04 04:53:19,035] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87966: learning rate 0.0000
[2019-04-04 04:53:19,515] A3C_AGENT_WORKER-Thread-6 INFO:Local step 5500, global step 88122: loss -0.0340
[2019-04-04 04:53:19,518] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 5500, global step 88122: learning rate 0.0000
[2019-04-04 04:53:20,194] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 88375: loss 0.0633
[2019-04-04 04:53:20,195] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 88375: learning rate 0.0000
[2019-04-04 04:53:20,493] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88482: loss -0.0018
[2019-04-04 04:53:20,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88482: learning rate 0.0000
[2019-04-04 04:53:20,657] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88542: loss -7.4868
[2019-04-04 04:53:20,659] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88544: learning rate 0.0000
[2019-04-04 04:53:21,894] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88984: loss 0.3296
[2019-04-04 04:53:21,914] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88990: learning rate 0.0000
[2019-04-04 04:53:22,791] A3C_AGENT_WORKER-Thread-4 INFO:Local step 5500, global step 89341: loss -2.7286
[2019-04-04 04:53:22,792] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 5500, global step 89341: learning rate 0.0000
[2019-04-04 04:53:23,313] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 89558: loss 1.4876
[2019-04-04 04:53:23,316] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 89560: learning rate 0.0000
[2019-04-04 04:53:27,336] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9815143e-02 1.5311335e-03 7.7212493e-25 1.2906508e-04 2.3563534e-06
 5.9716532e-10 9.7852236e-01], sum to 1.0000
[2019-04-04 04:53:27,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5386
[2019-04-04 04:53:27,459] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.85954740408745, 0.3100496246974349, 0.0, 1.0, 41404.52404912738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3370800.0000, 
sim time next is 3371400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.84225176568856, 0.3018843656605817, 0.0, 1.0, 41387.54399024726], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5701876471407132, 0.6006281218868605, 0.0, 1.0, 0.19708354281070123], 
reward next is 0.8029, 
noisyNet noise sample is [array([0.02950137], dtype=float32), 1.0336429]. 
=============================================
[2019-04-04 04:53:27,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6556833e-04 6.9832371e-04 6.5408150e-23 6.3375781e-05 8.1252001e-06
 4.0613735e-10 9.9866462e-01], sum to 1.0000
[2019-04-04 04:53:27,572] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1567
[2019-04-04 04:53:27,621] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 52.0, 104.6666666666667, 780.1666666666667, 26.0, 26.65741644949815, 0.6572813728195572, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3421200.0000, 
sim time next is 3421800.0000, 
raw observation next is [3.0, 53.5, 103.0, 775.0, 26.0, 26.64078578319725, 0.5538994268172681, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.535, 0.3433333333333333, 0.856353591160221, 0.6666666666666666, 0.720065481933104, 0.6846331422724227, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35421062], dtype=float32), -1.1383888]. 
=============================================
[2019-04-04 04:53:28,867] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.2919848e-02 1.4811790e-01 9.8154689e-21 2.0053722e-03 6.8959656e-05
 1.9001151e-08 7.8688794e-01], sum to 1.0000
[2019-04-04 04:53:28,867] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4626
[2019-04-04 04:53:28,883] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.63985625102557, 0.2174326367049087, 0.0, 1.0, 52775.17313115055], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3383400.0000, 
sim time next is 3384000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.64935454933928, 0.2123091837385104, 0.0, 1.0, 45900.50720324348], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5541128791116066, 0.5707697279128369, 0.0, 1.0, 0.21857384382496894], 
reward next is 0.7814, 
noisyNet noise sample is [array([0.32111472], dtype=float32), 0.24325201]. 
=============================================
[2019-04-04 04:53:28,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[93.69697 ]
 [93.66261 ]
 [93.411255]
 [93.62317 ]
 [93.535416]], R is [[93.79309845]
 [93.60385895]
 [93.39044952]
 [93.45654297]
 [93.31856537]].
[2019-04-04 04:53:32,467] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 93356: loss 0.0024
[2019-04-04 04:53:32,468] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 93357: learning rate 0.0000
[2019-04-04 04:53:32,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3067884e-02 2.7646155e-03 6.4370139e-24 1.1049653e-03 5.9798126e-06
 7.9526767e-11 9.8305649e-01], sum to 1.0000
[2019-04-04 04:53:32,599] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2196
[2019-04-04 04:53:32,611] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.15833854121133, 0.3588723233916382, 0.0, 1.0, 41605.55681692255], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3480000.0000, 
sim time next is 3480600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.17255611488052, 0.3435777876677256, 0.0, 1.0, 45859.0928675199], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5977130095733768, 0.6145259292225752, 0.0, 1.0, 0.21837663270247573], 
reward next is 0.7816, 
noisyNet noise sample is [array([-1.2265688], dtype=float32), -0.6264588]. 
=============================================
[2019-04-04 04:53:34,968] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0056912e-01 1.2507772e-01 1.6508638e-20 2.9838953e-02 2.3580929e-03
 3.8480067e-08 5.4215610e-01], sum to 1.0000
[2019-04-04 04:53:34,968] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8752
[2019-04-04 04:53:34,991] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.46920102576906, 0.2167292541002543, 0.0, 1.0, 41057.09210029165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3564000.0000, 
sim time next is 3564600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.43910020651474, 0.2077824186727742, 0.0, 1.0, 41061.76342996456], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5365916838762285, 0.569260806224258, 0.0, 1.0, 0.19553220680935504], 
reward next is 0.8045, 
noisyNet noise sample is [array([0.5000107], dtype=float32), -1.4252571]. 
=============================================
[2019-04-04 04:53:36,012] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 94974: loss 0.0009
[2019-04-04 04:53:36,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 94974: learning rate 0.0000
[2019-04-04 04:53:36,467] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3332986e-03 1.9123232e-03 7.5954340e-22 3.3205567e-04 4.9139999e-05
 6.9072348e-10 9.9537319e-01], sum to 1.0000
[2019-04-04 04:53:36,467] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6322
[2019-04-04 04:53:36,503] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 26.0, 24.76702482526217, 0.2749167476520359, 0.0, 1.0, 40911.70274360282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3561600.0000, 
sim time next is 3562200.0000, 
raw observation next is [-5.5, 67.5, 0.0, 0.0, 26.0, 24.71644049571881, 0.2727090928883567, 0.0, 1.0, 40909.81563089965], 
processed observation next is [0.0, 0.21739130434782608, 0.3102493074792244, 0.675, 0.0, 0.0, 0.6666666666666666, 0.5597033746432342, 0.5909030309627855, 0.0, 1.0, 0.1948086458614269], 
reward next is 0.8052, 
noisyNet noise sample is [array([-0.02258085], dtype=float32), -0.6221479]. 
=============================================
[2019-04-04 04:53:36,702] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95263: loss 0.0005
[2019-04-04 04:53:36,707] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95265: learning rate 0.0000
[2019-04-04 04:53:36,785] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 95301: loss -0.7258
[2019-04-04 04:53:36,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 95303: learning rate 0.0000
[2019-04-04 04:53:37,172] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95482: loss 0.0043
[2019-04-04 04:53:37,173] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95483: learning rate 0.0000
[2019-04-04 04:53:37,216] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6000, global step 95499: loss 0.0052
[2019-04-04 04:53:37,217] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6000, global step 95499: learning rate 0.0000
[2019-04-04 04:53:37,571] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95648: loss 0.0115
[2019-04-04 04:53:37,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95648: learning rate 0.0000
[2019-04-04 04:53:38,358] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95993: loss 0.0018
[2019-04-04 04:53:38,359] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95993: learning rate 0.0000
[2019-04-04 04:53:38,466] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96040: loss 0.0022
[2019-04-04 04:53:38,467] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96041: learning rate 0.0000
[2019-04-04 04:53:39,140] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6000, global step 96364: loss 0.0022
[2019-04-04 04:53:39,141] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6000, global step 96364: learning rate 0.0000
[2019-04-04 04:53:39,475] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 96523: loss 0.0009
[2019-04-04 04:53:39,476] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 96523: learning rate 0.0000
[2019-04-04 04:53:39,880] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96730: loss 0.0016
[2019-04-04 04:53:39,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96731: learning rate 0.0000
[2019-04-04 04:53:40,443] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 97006: loss 0.1243
[2019-04-04 04:53:40,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 97006: learning rate 0.0000
[2019-04-04 04:53:41,076] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 97342: loss -0.0005
[2019-04-04 04:53:41,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 97342: learning rate 0.0000
[2019-04-04 04:53:42,028] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6000, global step 97851: loss 0.0185
[2019-04-04 04:53:42,030] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6000, global step 97851: learning rate 0.0000
[2019-04-04 04:53:42,572] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9201370e-03 3.1148017e-04 2.1184039e-24 9.5320196e-04 2.5960058e-05
 1.0757666e-09 9.9678934e-01], sum to 1.0000
[2019-04-04 04:53:42,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4393
[2019-04-04 04:53:42,582] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666666, 48.0, 89.83333333333333, 716.8333333333333, 26.0, 25.47803224641707, 0.4813306621873788, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3684000.0000, 
sim time next is 3684600.0000, 
raw observation next is [5.5, 48.5, 87.0, 705.0, 26.0, 25.48178046515, 0.478485086371815, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6149584487534627, 0.485, 0.29, 0.7790055248618785, 0.6666666666666666, 0.6234817054291666, 0.6594950287906051, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2700177], dtype=float32), -0.10488858]. 
=============================================
[2019-04-04 04:53:42,716] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 98235: loss 0.0268
[2019-04-04 04:53:42,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 98236: learning rate 0.0000
[2019-04-04 04:53:42,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0812892e-04 9.4017574e-05 4.1320352e-26 1.0606296e-04 7.8928508e-07
 5.8393169e-12 9.9949098e-01], sum to 1.0000
[2019-04-04 04:53:42,849] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4091
[2019-04-04 04:53:42,863] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 31.33333333333333, 284.0, 26.0, 25.37235949717926, 0.411330253690058, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3690600.0000, 
sim time next is 3691200.0000, 
raw observation next is [4.0, 59.0, 23.16666666666666, 224.5, 26.0, 25.31550824859692, 0.3966387767185781, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.0772222222222222, 0.24806629834254143, 0.6666666666666666, 0.6096256873830767, 0.6322129255728594, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5611213], dtype=float32), -0.9302825]. 
=============================================
[2019-04-04 04:53:43,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7609885e-02 8.6802337e-03 6.0386325e-21 2.9466050e-03 1.3138862e-04
 1.3649261e-09 9.6063197e-01], sum to 1.0000
[2019-04-04 04:53:43,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4944
[2019-04-04 04:53:43,349] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 26.0, 25.54607823385278, 0.3531970127416984, 0.0, 1.0, 18745.79612687356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3651600.0000, 
sim time next is 3652200.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 26.0, 25.5274295635903, 0.3487771362126762, 0.0, 1.0, 23977.61592734484], 
processed observation next is [0.0, 0.2608695652173913, 0.7257617728531857, 0.26, 0.0, 0.0, 0.6666666666666666, 0.6272857969658583, 0.6162590454042254, 0.0, 1.0, 0.11417912346354685], 
reward next is 0.8858, 
noisyNet noise sample is [array([-0.7813509], dtype=float32), 0.11684833]. 
=============================================
[2019-04-04 04:53:46,087] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 04:53:46,090] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 04:53:46,091] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 04:53:46,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:53:46,091] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:53:46,092] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 04:53:46,092] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:53:46,097] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run2
[2019-04-04 04:53:46,114] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run2
[2019-04-04 04:53:46,115] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run2
[2019-04-04 04:56:07,663] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2440585], dtype=float32), 0.19320026]
[2019-04-04 04:56:07,664] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.7, 58.0, 182.0, 546.0, 26.0, 26.006003193041, 0.5871546101287811, 1.0, 1.0, 0.0]
[2019-04-04 04:56:07,664] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:56:07,665] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.23469028e-04 1.07408414e-04 1.96521243e-22 8.95773701e-05
 1.82009603e-06 1.48598078e-10 9.99677777e-01], sampled 0.13989803124596845
[2019-04-04 04:56:20,830] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2440585], dtype=float32), 0.19320026]
[2019-04-04 04:56:20,830] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [16.18333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 27.45999103958067, 1.120448241702343, 0.0, 0.0, 0.0]
[2019-04-04 04:56:20,830] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 04:56:20,832] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.9444017e-03 4.9648603e-04 4.2631582e-24 4.1003383e-04 1.8498469e-05
 5.3209749e-11 9.9613059e-01], sampled 0.013159141217606418
[2019-04-04 04:56:30,443] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.9382 239832968.8264 1605.1556
[2019-04-04 04:56:57,466] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7864 263384855.0123 1560.5098
[2019-04-04 04:57:05,233] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6931 275794444.9065 1232.7074
[2019-04-04 04:57:06,276] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 100000, evaluation results [100000.0, 7241.786404703458, 263384855.0122734, 1560.5097983047776, 7353.938243683586, 239832968.82644904, 1605.1556059436036, 7182.693119492983, 275794444.9064783, 1232.707411902662]
[2019-04-04 04:57:09,570] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 101165: loss 3.7422
[2019-04-04 04:57:09,570] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 101165: learning rate 0.0000
[2019-04-04 04:57:10,720] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.8715105e-04 2.1570602e-04 2.0918488e-25 3.0241115e-05 1.9980562e-06
 9.2122490e-12 9.9886489e-01], sum to 1.0000
[2019-04-04 04:57:10,720] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5661
[2019-04-04 04:57:10,754] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 76.0, 0.0, 0.0, 26.0, 25.43359695809706, 0.4664566171451239, 0.0, 1.0, 67641.1381494807], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3795000.0000, 
sim time next is 3795600.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.45589375484901, 0.4567837410298795, 0.0, 1.0, 38767.72527339484], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.621324479570751, 0.6522612470099598, 0.0, 1.0, 0.1846082155875945], 
reward next is 0.8154, 
noisyNet noise sample is [array([-0.57686883], dtype=float32), 0.2521596]. 
=============================================
[2019-04-04 04:57:11,044] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9783709e-05 2.4100997e-05 2.8121101e-26 2.3261273e-05 7.6333968e-08
 2.9061655e-12 9.9993277e-01], sum to 1.0000
[2019-04-04 04:57:11,044] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0701
[2019-04-04 04:57:11,059] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.61115237013103, 0.5241508999292096, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3792600.0000, 
sim time next is 3793200.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.55997886932462, 0.5124793632317044, 0.0, 1.0, 38249.30696632455], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.629998239110385, 0.6708264544105681, 0.0, 1.0, 0.18213955698249784], 
reward next is 0.8179, 
noisyNet noise sample is [array([-0.76453716], dtype=float32), -0.14312439]. 
=============================================
[2019-04-04 04:57:15,190] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 102905: loss 3.4604
[2019-04-04 04:57:15,191] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 102905: learning rate 0.0000
[2019-04-04 04:57:15,379] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 102953: loss 3.2881
[2019-04-04 04:57:15,380] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 102953: learning rate 0.0000
[2019-04-04 04:57:16,148] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103191: loss 3.6035
[2019-04-04 04:57:16,151] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103192: learning rate 0.0000
[2019-04-04 04:57:16,816] A3C_AGENT_WORKER-Thread-5 INFO:Local step 6500, global step 103417: loss 3.0856
[2019-04-04 04:57:16,819] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 6500, global step 103417: learning rate 0.0000
[2019-04-04 04:57:17,005] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103486: loss 3.3068
[2019-04-04 04:57:17,005] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103486: learning rate 0.0000
[2019-04-04 04:57:17,193] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103549: loss 3.0334
[2019-04-04 04:57:17,241] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103560: learning rate 0.0000
[2019-04-04 04:57:18,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.7690813e-04 7.1446993e-04 3.1016219e-22 1.6318873e-03 3.6663194e-05
 8.3850749e-10 9.9684000e-01], sum to 1.0000
[2019-04-04 04:57:18,299] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0883
[2019-04-04 04:57:18,315] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.41476594589875, 0.4540590476727366, 0.0, 1.0, 18765.18448315081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3896400.0000, 
sim time next is 3897000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.68967116136168, 0.4405682547172609, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6408059301134733, 0.6468560849057536, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42855245], dtype=float32), -0.18269697]. 
=============================================
[2019-04-04 04:57:18,364] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[91.87013 ]
 [92.71054 ]
 [92.8931  ]
 [93.48489 ]
 [93.843796]], R is [[90.96378326]
 [90.96479034]
 [90.85331726]
 [90.71764374]
 [90.52675629]].
[2019-04-04 04:57:18,419] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103925: loss 3.0083
[2019-04-04 04:57:18,426] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103929: learning rate 0.0000
[2019-04-04 04:57:18,692] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103997: loss 3.0904
[2019-04-04 04:57:18,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103997: learning rate 0.0000
[2019-04-04 04:57:18,822] A3C_AGENT_WORKER-Thread-6 INFO:Local step 6500, global step 104031: loss 3.0527
[2019-04-04 04:57:18,823] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 6500, global step 104031: learning rate 0.0000
[2019-04-04 04:57:19,064] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 104107: loss 2.9858
[2019-04-04 04:57:19,065] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 104108: learning rate 0.0000
[2019-04-04 04:57:20,726] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104567: loss 3.2459
[2019-04-04 04:57:20,726] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104567: learning rate 0.0000
[2019-04-04 04:57:22,324] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104984: loss 3.1480
[2019-04-04 04:57:22,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104988: learning rate 0.0000
[2019-04-04 04:57:22,459] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 105016: loss 3.1786
[2019-04-04 04:57:22,463] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 105016: learning rate 0.0000
[2019-04-04 04:57:23,407] A3C_AGENT_WORKER-Thread-4 INFO:Local step 6500, global step 105277: loss 3.1614
[2019-04-04 04:57:23,408] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 6500, global step 105277: learning rate 0.0000
[2019-04-04 04:57:25,229] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 105789: loss 2.7285
[2019-04-04 04:57:25,264] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 105789: learning rate 0.0000
[2019-04-04 04:57:31,116] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.6623361e-05 1.5963135e-04 5.3411603e-22 3.5191636e-04 2.0660089e-05
 8.5980101e-10 9.9942124e-01], sum to 1.0000
[2019-04-04 04:57:31,116] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4071
[2019-04-04 04:57:31,189] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.45102304076822, 0.4966071764681101, 1.0, 1.0, 41482.80182539627], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4042800.0000, 
sim time next is 4043400.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.55307371921642, 0.4967830062540715, 1.0, 1.0, 31728.32985187731], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6294228099347018, 0.6655943354180238, 1.0, 1.0, 0.1510872850089396], 
reward next is 0.8489, 
noisyNet noise sample is [array([-0.24313553], dtype=float32), 0.41607267]. 
=============================================
[2019-04-04 04:57:35,135] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4012758e-03 2.0205842e-02 8.6271030e-22 3.9138845e-03 6.1454310e-05
 2.0914435e-09 9.6641755e-01], sum to 1.0000
[2019-04-04 04:57:35,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2539
[2019-04-04 04:57:35,153] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 29.33333333333333, 0.0, 0.0, 26.0, 25.49396608066055, 0.5223290891141541, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4049400.0000, 
sim time next is 4050000.0000, 
raw observation next is [-4.0, 29.0, 0.0, 0.0, 26.0, 25.65037549629464, 0.5252475198979519, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.29, 0.0, 0.0, 0.6666666666666666, 0.6375312913578867, 0.6750825066326507, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21527666], dtype=float32), 0.6961161]. 
=============================================
[2019-04-04 04:57:35,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[94.96229]
 [94.63157]
 [94.37513]
 [93.21363]
 [92.05363]], R is [[95.25453186]
 [95.30198669]
 [94.92787933]
 [94.17258453]
 [93.28411865]].
[2019-04-04 04:57:39,043] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 109432: loss 0.1057
[2019-04-04 04:57:39,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 109432: learning rate 0.0000
[2019-04-04 04:57:40,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9412652e-04 3.8572119e-04 1.2278444e-24 3.4098973e-04 1.0821439e-05
 2.2143648e-11 9.9876827e-01], sum to 1.0000
[2019-04-04 04:57:40,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9897
[2019-04-04 04:57:40,911] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 42.16666666666666, 0.0, 0.0, 26.0, 25.44013139696625, 0.4408782832381323, 0.0, 1.0, 41631.80354427907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4146600.0000, 
sim time next is 4147200.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.41265780178786, 0.4359303206084764, 0.0, 1.0, 53803.8958700941], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.6177214834823216, 0.6453101068694921, 0.0, 1.0, 0.25620902795282907], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.6334424], dtype=float32), 0.027453898]. 
=============================================
[2019-04-04 04:57:41,124] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4546694e-05 2.8998300e-04 2.2085117e-23 1.9784544e-05 1.5196391e-06
 3.9942438e-11 9.9964416e-01], sum to 1.0000
[2019-04-04 04:57:41,125] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8551
[2019-04-04 04:57:41,190] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 25.93703885396147, 0.5510387668909478, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4129200.0000, 
sim time next is 4129800.0000, 
raw observation next is [2.666666666666667, 38.0, 0.0, 0.0, 26.0, 25.84257661285391, 0.5411604152563398, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6535480510711592, 0.6803868050854467, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51600355], dtype=float32), -2.1364744]. 
=============================================
[2019-04-04 04:57:43,699] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 110967: loss 0.1249
[2019-04-04 04:57:43,708] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 110967: learning rate 0.0000
[2019-04-04 04:57:44,368] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111190: loss 0.1433
[2019-04-04 04:57:44,369] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111190: learning rate 0.0000
[2019-04-04 04:57:44,688] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111283: loss 0.1590
[2019-04-04 04:57:44,702] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111283: learning rate 0.0000
[2019-04-04 04:57:45,114] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7000, global step 111434: loss 0.1939
[2019-04-04 04:57:45,116] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7000, global step 111435: learning rate 0.0000
[2019-04-04 04:57:45,395] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111532: loss 0.2196
[2019-04-04 04:57:45,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111532: learning rate 0.0000
[2019-04-04 04:57:46,655] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111944: loss 0.2694
[2019-04-04 04:57:46,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111944: learning rate 0.0000
[2019-04-04 04:57:46,764] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7000, global step 111991: loss 0.2689
[2019-04-04 04:57:46,764] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7000, global step 111991: learning rate 0.0000
[2019-04-04 04:57:46,900] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112041: loss 0.2274
[2019-04-04 04:57:46,902] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112042: learning rate 0.0000
[2019-04-04 04:57:46,919] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112048: loss 0.2238
[2019-04-04 04:57:46,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112048: learning rate 0.0000
[2019-04-04 04:57:47,759] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 112373: loss 0.1568
[2019-04-04 04:57:47,769] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 112373: learning rate 0.0000
[2019-04-04 04:57:48,348] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112572: loss 0.2064
[2019-04-04 04:57:48,349] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112572: learning rate 0.0000
[2019-04-04 04:57:50,167] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 113196: loss 0.2498
[2019-04-04 04:57:50,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 113196: learning rate 0.0000
[2019-04-04 04:57:50,172] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7000, global step 113200: loss 0.2833
[2019-04-04 04:57:50,174] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7000, global step 113200: learning rate 0.0000
[2019-04-04 04:57:50,196] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 113204: loss 0.2066
[2019-04-04 04:57:50,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 113204: learning rate 0.0000
[2019-04-04 04:57:51,656] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 113721: loss 0.2537
[2019-04-04 04:57:51,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 113721: learning rate 0.0000
[2019-04-04 04:57:51,897] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0855355e-04 1.7528546e-05 6.8932395e-23 1.0406507e-05 5.8160521e-07
 2.4327039e-11 9.9976295e-01], sum to 1.0000
[2019-04-04 04:57:51,897] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7983
[2019-04-04 04:57:51,947] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 53.33333333333334, 170.0, 118.0, 26.0, 25.68453632552922, 0.4052346177178782, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4267200.0000, 
sim time next is 4267800.0000, 
raw observation next is [3.5, 53.5, 182.0, 131.0, 26.0, 25.70205462222183, 0.4063423304994596, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.5595567867036012, 0.535, 0.6066666666666667, 0.14475138121546963, 0.6666666666666666, 0.6418378851851525, 0.6354474434998199, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.91119236], dtype=float32), -0.9130273]. 
=============================================
[2019-04-04 04:57:55,733] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6079890e-05 1.6456260e-04 9.6369848e-23 5.4425742e-05 1.7826420e-05
 1.7244586e-11 9.9974710e-01], sum to 1.0000
[2019-04-04 04:57:55,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6927
[2019-04-04 04:57:55,858] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.65, 65.5, 92.0, 491.0, 26.0, 25.8041837988561, 0.5065172884229908, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4350600.0000, 
sim time next is 4351200.0000, 
raw observation next is [5.199999999999999, 62.66666666666667, 94.5, 522.0, 26.0, 26.08971534241526, 0.5366021496146225, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6066481994459835, 0.6266666666666667, 0.315, 0.5767955801104973, 0.6666666666666666, 0.6741429452012717, 0.678867383204874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88889617], dtype=float32), -0.2843431]. 
=============================================
[2019-04-04 04:57:56,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2632145e-06 3.3484888e-05 1.4010803e-26 4.7971862e-06 9.1741720e-07
 2.3709661e-12 9.9995744e-01], sum to 1.0000
[2019-04-04 04:57:56,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2817
[2019-04-04 04:57:56,169] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.8, 28.5, 118.0, 853.0, 26.0, 28.29741356113187, 1.03449136201454, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4365000.0000, 
sim time next is 4365600.0000, 
raw observation next is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 26.0, 28.2050275361548, 0.9151174548382611, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8707294552169899, 0.2866666666666666, 0.39166666666666666, 0.9405156537753223, 0.6666666666666666, 0.8504189613462335, 0.8050391516127537, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03765763], dtype=float32), -0.81925654]. 
=============================================
[2019-04-04 04:58:01,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5343230e-06 1.1126325e-04 5.3832619e-23 5.2698501e-06 1.3855536e-06
 6.9207929e-12 9.9987650e-01], sum to 1.0000
[2019-04-04 04:58:01,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7075
[2019-04-04 04:58:01,182] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.85, 69.83333333333334, 0.0, 0.0, 26.0, 25.57200230621802, 0.4002732589317095, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4335000.0000, 
sim time next is 4335600.0000, 
raw observation next is [3.8, 69.66666666666667, 0.0, 0.0, 26.0, 25.63255611113332, 0.3888641544356082, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5678670360110805, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.6360463425944433, 0.6296213848118694, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02291159], dtype=float32), -0.30315316]. 
=============================================
[2019-04-04 04:58:01,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5883152e-04 8.0996579e-05 6.1842656e-24 2.3034205e-04 1.9007550e-05
 2.4808489e-11 9.9951077e-01], sum to 1.0000
[2019-04-04 04:58:01,419] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5700
[2019-04-04 04:58:01,428] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.449999999999999, 65.5, 0.0, 0.0, 26.0, 25.90530451870749, 0.5950194017245994, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4411800.0000, 
sim time next is 4412400.0000, 
raw observation next is [6.333333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.83663114692381, 0.5848025688532509, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6380424746075716, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6530525955769843, 0.6949341896177503, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0587695], dtype=float32), -0.17291285]. 
=============================================
[2019-04-04 04:58:01,579] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 117542: loss 6.5005
[2019-04-04 04:58:01,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 117542: learning rate 0.0000
[2019-04-04 04:58:04,923] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 118817: loss 6.0158
[2019-04-04 04:58:04,943] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 118817: learning rate 0.0000
[2019-04-04 04:58:06,240] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119294: loss 6.0924
[2019-04-04 04:58:06,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119294: learning rate 0.0000
[2019-04-04 04:58:06,682] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119446: loss 5.6418
[2019-04-04 04:58:06,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119447: learning rate 0.0000
[2019-04-04 04:58:06,956] A3C_AGENT_WORKER-Thread-5 INFO:Local step 7500, global step 119537: loss 5.7692
[2019-04-04 04:58:06,977] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 7500, global step 119537: learning rate 0.0000
[2019-04-04 04:58:07,010] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119558: loss 5.3984
[2019-04-04 04:58:07,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119559: learning rate 0.0000
[2019-04-04 04:58:08,053] A3C_AGENT_WORKER-Thread-6 INFO:Local step 7500, global step 119930: loss 5.6641
[2019-04-04 04:58:08,059] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 7500, global step 119930: learning rate 0.0000
[2019-04-04 04:58:08,106] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119944: loss 5.9046
[2019-04-04 04:58:08,106] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119944: learning rate 0.0000
[2019-04-04 04:58:08,557] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120083: loss 5.7887
[2019-04-04 04:58:08,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120083: learning rate 0.0000
[2019-04-04 04:58:09,069] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120258: loss 5.8568
[2019-04-04 04:58:09,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120258: learning rate 0.0000
[2019-04-04 04:58:09,085] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 120264: loss 5.8742
[2019-04-04 04:58:09,086] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 120264: learning rate 0.0000
[2019-04-04 04:58:09,393] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120371: loss 5.5432
[2019-04-04 04:58:09,394] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120371: learning rate 0.0000
[2019-04-04 04:58:11,188] A3C_AGENT_WORKER-Thread-4 INFO:Local step 7500, global step 120931: loss 5.6580
[2019-04-04 04:58:11,195] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 7500, global step 120933: learning rate 0.0000
[2019-04-04 04:58:12,063] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 121244: loss 6.2856
[2019-04-04 04:58:12,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 121244: learning rate 0.0000
[2019-04-04 04:58:12,648] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 121454: loss 6.2994
[2019-04-04 04:58:12,648] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 121454: learning rate 0.0000
[2019-04-04 04:58:13,192] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 121638: loss 6.2083
[2019-04-04 04:58:13,193] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 121638: learning rate 0.0000
[2019-04-04 04:58:18,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1073825e-05 1.6698873e-04 3.3143999e-24 6.2368075e-05 7.4421194e-05
 1.8696274e-11 9.9966526e-01], sum to 1.0000
[2019-04-04 04:58:18,819] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1483
[2019-04-04 04:58:18,834] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 163.3333333333333, 2.0, 26.0, 25.59544975868576, 0.508632903781505, 1.0, 1.0, 12453.60778015369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4720200.0000, 
sim time next is 4720800.0000, 
raw observation next is [1.0, 72.0, 155.1666666666667, 0.9999999999999998, 26.0, 25.89016668892315, 0.5221251413053899, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4903047091412743, 0.72, 0.5172222222222224, 0.0011049723756906074, 0.6666666666666666, 0.6575138907435957, 0.6740417137684633, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9368496], dtype=float32), 0.8381927]. 
=============================================
[2019-04-04 04:58:19,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3002035e-04 1.1069896e-03 6.3942349e-22 1.2245437e-04 2.5779498e-05
 1.2790140e-09 9.9841464e-01], sum to 1.0000
[2019-04-04 04:58:19,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0465
[2019-04-04 04:58:19,245] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.35, 65.33333333333334, 0.0, 0.0, 26.0, 25.47421308794747, 0.4408504044105821, 0.0, 1.0, 18755.68694616504], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4587000.0000, 
sim time next is 4587600.0000, 
raw observation next is [-0.5, 65.66666666666667, 0.0, 0.0, 26.0, 25.45554050593072, 0.4421821336556546, 0.0, 1.0, 31178.29702508684], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6212950421608934, 0.6473940445518849, 0.0, 1.0, 0.1484680810718421], 
reward next is 0.8515, 
noisyNet noise sample is [array([-0.63106644], dtype=float32), -0.5595903]. 
=============================================
[2019-04-04 04:58:25,777] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 125761: loss 0.0863
[2019-04-04 04:58:25,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 125761: learning rate 0.0000
[2019-04-04 04:58:29,735] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 126930: loss 0.1334
[2019-04-04 04:58:29,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 126930: learning rate 0.0000
[2019-04-04 04:58:30,857] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127267: loss 0.1237
[2019-04-04 04:58:30,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127267: learning rate 0.0000
[2019-04-04 04:58:31,385] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127412: loss 0.1194
[2019-04-04 04:58:31,388] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127412: learning rate 0.0000
[2019-04-04 04:58:31,416] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.2564417e-04 1.4924604e-04 4.6801459e-21 7.6096825e-05 1.1623404e-04
 2.1790463e-09 9.9903274e-01], sum to 1.0000
[2019-04-04 04:58:31,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8612
[2019-04-04 04:58:31,455] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 23.97424147427885, 0.1162616818465118, 0.0, 1.0, 41805.56773123524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4776600.0000, 
sim time next is 4777200.0000, 
raw observation next is [-6.2, 93.0, 0.0, 0.0, 26.0, 23.94024976700536, 0.1088006593797577, 0.0, 1.0, 41845.73937108295], 
processed observation next is [0.0, 0.30434782608695654, 0.2908587257617729, 0.93, 0.0, 0.0, 0.6666666666666666, 0.4950208139171132, 0.5362668864599193, 0.0, 1.0, 0.19926542557658547], 
reward next is 0.8007, 
noisyNet noise sample is [array([1.1377032], dtype=float32), 0.12595634]. 
=============================================
[2019-04-04 04:58:32,012] A3C_AGENT_WORKER-Thread-5 INFO:Local step 8000, global step 127561: loss 0.0914
[2019-04-04 04:58:32,025] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 8000, global step 127564: learning rate 0.0000
[2019-04-04 04:58:32,917] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127795: loss 0.0728
[2019-04-04 04:58:32,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127795: learning rate 0.0000
[2019-04-04 04:58:32,993] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127814: loss 0.0973
[2019-04-04 04:58:33,013] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127818: learning rate 0.0000
[2019-04-04 04:58:33,587] A3C_AGENT_WORKER-Thread-6 INFO:Local step 8000, global step 128002: loss 0.0899
[2019-04-04 04:58:33,597] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 8000, global step 128002: learning rate 0.0000
[2019-04-04 04:58:34,086] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128161: loss 0.0925
[2019-04-04 04:58:34,087] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128161: learning rate 0.0000
[2019-04-04 04:58:34,124] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128176: loss 0.0827
[2019-04-04 04:58:34,128] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128179: learning rate 0.0000
[2019-04-04 04:58:34,334] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 128257: loss 0.0948
[2019-04-04 04:58:34,335] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 128257: learning rate 0.0000
[2019-04-04 04:58:34,431] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2516386e-06 8.0345289e-06 4.3605158e-24 2.5612138e-05 1.2748608e-05
 6.9011472e-12 9.9994636e-01], sum to 1.0000
[2019-04-04 04:58:34,431] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3635
[2019-04-04 04:58:34,439] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 67.5, 252.0, 26.0, 25.14570801672014, 0.3398214386887305, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4899600.0000, 
sim time next is 4900200.0000, 
raw observation next is [2.833333333333333, 44.83333333333334, 56.0, 230.3333333333333, 26.0, 25.1062241232098, 0.3313470129984586, 0.0, 1.0, 26589.6479813364], 
processed observation next is [0.0, 0.7391304347826086, 0.541089566020314, 0.4483333333333334, 0.18666666666666668, 0.25451197053406993, 0.6666666666666666, 0.5921853436008165, 0.6104490043328196, 0.0, 1.0, 0.12661737133969714], 
reward next is 0.8734, 
noisyNet noise sample is [array([0.06322387], dtype=float32), -0.08901802]. 
=============================================
[2019-04-04 04:58:34,793] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128408: loss 0.0669
[2019-04-04 04:58:34,794] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128408: learning rate 0.0000
[2019-04-04 04:58:37,559] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 129193: loss 0.1095
[2019-04-04 04:58:37,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 129193: learning rate 0.0000
[2019-04-04 04:58:38,152] A3C_AGENT_WORKER-Thread-4 INFO:Local step 8000, global step 129349: loss 0.1032
[2019-04-04 04:58:38,153] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 8000, global step 129349: learning rate 0.0000
[2019-04-04 04:58:38,407] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 129417: loss 0.0907
[2019-04-04 04:58:38,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 129420: learning rate 0.0000
[2019-04-04 04:58:40,107] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 129891: loss 0.0581
[2019-04-04 04:58:40,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 129891: learning rate 0.0000
[2019-04-04 04:58:52,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:52,941] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:52,946] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run2
[2019-04-04 04:58:56,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3499257e-04 7.2959525e-04 1.8942459e-22 4.1875077e-04 2.8107419e-05
 5.9994738e-11 9.9868852e-01], sum to 1.0000
[2019-04-04 04:58:56,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8196
[2019-04-04 04:58:56,935] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 38.5, 116.0, 809.0, 26.0, 27.25288792791243, 0.785426801332895, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5049000.0000, 
sim time next is 5049600.0000, 
raw observation next is [4.333333333333333, 37.66666666666667, 117.1666666666667, 815.0, 26.0, 27.32142753865695, 0.807962267229903, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.58264081255771, 0.3766666666666667, 0.39055555555555566, 0.9005524861878453, 0.6666666666666666, 0.7767856282214124, 0.7693207557433009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2386856], dtype=float32), 0.10249595]. 
=============================================
[2019-04-04 04:58:58,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:58,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:58,214] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run2
[2019-04-04 04:58:58,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:58:58,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:58:58,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run2
[2019-04-04 04:59:00,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:00,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:00,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run2
[2019-04-04 04:59:00,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:00,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:00,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run2
[2019-04-04 04:59:00,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:00,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:00,539] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run2
[2019-04-04 04:59:01,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:01,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:01,794] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run2
[2019-04-04 04:59:02,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:02,765] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:02,768] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run2
[2019-04-04 04:59:03,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:03,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:03,023] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run2
[2019-04-04 04:59:03,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:03,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:03,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run2
[2019-04-04 04:59:03,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:03,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:03,323] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run2
[2019-04-04 04:59:03,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:03,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:03,366] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run2
[2019-04-04 04:59:05,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:05,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:05,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run2
[2019-04-04 04:59:06,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:06,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:06,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run2
[2019-04-04 04:59:07,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:07,362] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:07,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run2
[2019-04-04 04:59:07,790] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 04:59:07,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 04:59:07,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run2
[2019-04-04 04:59:23,538] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5134245e-04 3.8577311e-03 9.9042173e-24 4.2286236e-04 4.3448672e-04
 7.1277384e-10 9.9453354e-01], sum to 1.0000
[2019-04-04 04:59:23,538] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8939
[2019-04-04 04:59:23,709] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 56.33333333333333, 0.0, 26.0, 23.66221288229841, -0.03179473305107963, 0.0, 1.0, 56979.61085876448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 37200.0000, 
sim time next is 37800.0000, 
raw observation next is [7.7, 93.0, 60.0, 0.0, 26.0, 23.78110646377529, -0.00384618683684867, 0.0, 1.0, 56871.63729303174], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.2, 0.0, 0.6666666666666666, 0.48175887198127426, 0.49871793772105044, 0.0, 1.0, 0.27081732044300827], 
reward next is 0.7292, 
noisyNet noise sample is [array([-1.4995905], dtype=float32), 0.26915625]. 
=============================================
[2019-04-04 04:59:52,331] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2384134e-02 1.1838987e-02 7.5739775e-18 8.4905988e-03 6.8191970e-03
 3.0248805e-07 9.6046674e-01], sum to 1.0000
[2019-04-04 04:59:52,331] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5083
[2019-04-04 04:59:52,388] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.15, 68.5, 0.0, 0.0, 26.0, 22.91105694487763, -0.1888021732669374, 0.0, 1.0, 47778.12525284861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 279000.0000, 
sim time next is 279600.0000, 
raw observation next is [-11.33333333333333, 69.0, 0.0, 0.0, 26.0, 22.90008239251475, -0.2005072357952256, 0.0, 1.0, 47811.22820896592], 
processed observation next is [1.0, 0.21739130434782608, 0.14866112650046176, 0.69, 0.0, 0.0, 0.6666666666666666, 0.40834019937622923, 0.4331642547349248, 0.0, 1.0, 0.2276725152807901], 
reward next is 0.7723, 
noisyNet noise sample is [array([-2.2215762], dtype=float32), 0.23928364]. 
=============================================
[2019-04-04 04:59:53,291] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9343746e-04 5.2762486e-04 1.4692127e-20 2.8884230e-04 1.5495672e-04
 3.5526355e-09 9.9883515e-01], sum to 1.0000
[2019-04-04 04:59:53,292] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2663
[2019-04-04 04:59:53,309] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.12593500528271, 0.06538919864541552, 0.0, 1.0, 45024.04756446498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 263400.0000, 
sim time next is 264000.0000, 
raw observation next is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 24.08282647686204, 0.05340215667761658, 0.0, 1.0, 45189.37288068415], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5069022064051699, 0.5178007188925389, 0.0, 1.0, 0.21518748990801975], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.78140473], dtype=float32), -1.1374947]. 
=============================================
[2019-04-04 04:59:53,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[76.95207]
 [77.87561]
 [78.81447]
 [79.54283]
 [80.50552]], R is [[76.25772858]
 [76.28075409]
 [76.30438995]
 [76.32855988]
 [76.35305786]].
[2019-04-04 04:59:53,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1025075e-03 2.8837197e-03 6.9495761e-18 1.3514241e-03 6.9827435e-04
 1.4539773e-07 9.9396402e-01], sum to 1.0000
[2019-04-04 04:59:53,721] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8009
[2019-04-04 04:59:53,801] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.55, 68.5, 30.0, 386.0, 26.0, 25.33754497422313, 0.2898121107301926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 289800.0000, 
sim time next is 290400.0000, 
raw observation next is [-12.46666666666667, 68.0, 40.83333333333333, 385.5, 26.0, 25.63172786091829, 0.3217546741508705, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.11726685133887339, 0.68, 0.1361111111111111, 0.42596685082872926, 0.6666666666666666, 0.6359773217431908, 0.6072515580502902, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7590499], dtype=float32), 0.5354389]. 
=============================================
[2019-04-04 05:00:02,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8455032e-03 7.1158241e-03 4.9886039e-17 2.9155849e-03 2.3201441e-03
 6.2048116e-08 9.8480290e-01], sum to 1.0000
[2019-04-04 05:00:02,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4524
[2019-04-04 05:00:02,886] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 49.0, 0.0, 0.0, 26.0, 23.18294125164981, -0.1713370464218783, 0.0, 1.0, 45985.38672591637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 442800.0000, 
sim time next is 443400.0000, 
raw observation next is [-10.7, 49.5, 0.0, 0.0, 26.0, 23.13543456758422, -0.1697080484704252, 0.0, 1.0, 46005.62785744619], 
processed observation next is [1.0, 0.13043478260869565, 0.1662049861495845, 0.495, 0.0, 0.0, 0.6666666666666666, 0.4279528806320183, 0.44343065050985825, 0.0, 1.0, 0.21907441836879138], 
reward next is 0.7809, 
noisyNet noise sample is [array([-0.07996485], dtype=float32), 0.93998396]. 
=============================================
[2019-04-04 05:00:08,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3412255e-06 8.1464634e-05 1.5980886e-22 7.3688711e-06 2.3487705e-06
 7.3623867e-12 9.9990547e-01], sum to 1.0000
[2019-04-04 05:00:08,735] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7130
[2019-04-04 05:00:08,796] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.1, 55.5, 58.0, 764.0, 26.0, 25.72599114830975, 0.3223382364971495, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 387000.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 26.0, 25.68199481463215, 0.3393456177013119, 1.0, 1.0, 200021.6284135398], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 0.6666666666666666, 0.6401662345526793, 0.6131152059004373, 1.0, 1.0, 0.95248394482638], 
reward next is 0.0475, 
noisyNet noise sample is [array([-0.5470306], dtype=float32), 1.5867333]. 
=============================================
[2019-04-04 05:00:14,984] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2106020e-04 4.2204509e-04 2.6378221e-19 1.0420399e-03 2.1997064e-05
 2.4760766e-08 9.9789292e-01], sum to 1.0000
[2019-04-04 05:00:14,984] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7127
[2019-04-04 05:00:15,089] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 42.0, 0.0, 0.0, 26.0, 25.12062533839517, 0.29763095006768, 0.0, 1.0, 48753.23681780809], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 417600.0000, 
sim time next is 418200.0000, 
raw observation next is [-10.1, 42.83333333333334, 0.0, 0.0, 26.0, 25.12204049037079, 0.2938453387713486, 0.0, 1.0, 54340.46854754303], 
processed observation next is [1.0, 0.8695652173913043, 0.18282548476454297, 0.42833333333333345, 0.0, 0.0, 0.6666666666666666, 0.5935033741975658, 0.5979484462571162, 0.0, 1.0, 0.2587641359406811], 
reward next is 0.7412, 
noisyNet noise sample is [array([-0.27526176], dtype=float32), 0.11832197]. 
=============================================
[2019-04-04 05:00:20,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7221674e-04 2.3910601e-03 1.5524345e-20 2.0541798e-03 7.9114718e-04
 2.5170539e-09 9.9449146e-01], sum to 1.0000
[2019-04-04 05:00:20,147] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0684
[2019-04-04 05:00:20,172] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.40159852014582, 0.1469939643452979, 0.0, 1.0, 42114.94227240071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 609600.0000, 
sim time next is 610200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.41466369547253, 0.1421162595199023, 0.0, 1.0, 42097.37020564194], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5345553079560442, 0.5473720865066342, 0.0, 1.0, 0.200463667645914], 
reward next is 0.7995, 
noisyNet noise sample is [array([0.37448624], dtype=float32), 0.09317852]. 
=============================================
[2019-04-04 05:00:39,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4237833e-02 1.9374955e-01 1.6850815e-19 2.4519254e-02 5.6041824e-03
 2.9536679e-07 7.3188895e-01], sum to 1.0000
[2019-04-04 05:00:39,612] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9013
[2019-04-04 05:00:39,626] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.52427232998532, 0.1124556242513536, 0.0, 1.0, 57963.92016391012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 689400.0000, 
sim time next is 690000.0000, 
raw observation next is [-3.899999999999999, 71.0, 0.0, 0.0, 26.0, 24.4550535783574, 0.1050865555035857, 0.0, 1.0, 52492.81708906142], 
processed observation next is [0.0, 1.0, 0.35457063711911363, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5379211315297834, 0.5350288518345286, 0.0, 1.0, 0.24996579566219723], 
reward next is 0.7500, 
noisyNet noise sample is [array([1.4335215], dtype=float32), 0.6525986]. 
=============================================
[2019-04-04 05:00:39,631] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.50891 ]
 [83.474304]
 [83.71215 ]
 [83.67824 ]
 [83.629135]], R is [[83.49189758]
 [83.38096619]
 [83.54715729]
 [83.51399994]
 [83.480896  ]].
[2019-04-04 05:00:46,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.9631183e-04 2.6778230e-03 6.5254139e-20 1.5946853e-03 6.2671403e-05
 9.4602006e-09 9.9486840e-01], sum to 1.0000
[2019-04-04 05:00:46,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1253
[2019-04-04 05:00:46,295] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.299999999999999, 71.0, 0.0, 0.0, 26.0, 24.28649695257364, 0.1348456413505731, 0.0, 1.0, 41601.50765918259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 780600.0000, 
sim time next is 781200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.26148588897155, 0.1301595998178478, 0.0, 1.0, 41585.37538404022], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5217904907476291, 0.5433865332726159, 0.0, 1.0, 0.19802559706685818], 
reward next is 0.8020, 
noisyNet noise sample is [array([-1.3717458], dtype=float32), -0.8206079]. 
=============================================
[2019-04-04 05:00:53,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7689812e-04 1.5057993e-04 1.1747195e-22 1.7677137e-04 2.7127296e-06
 2.4081448e-10 9.9939299e-01], sum to 1.0000
[2019-04-04 05:00:53,191] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5171
[2019-04-04 05:00:53,214] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.066666666666667, 95.66666666666667, 102.8333333333333, 0.0, 26.0, 25.30249527139958, 0.2655331178433126, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 908400.0000, 
sim time next is 909000.0000, 
raw observation next is [3.25, 95.0, 104.0, 0.0, 26.0, 25.23011115977518, 0.2598360058232971, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5526315789473685, 0.95, 0.3466666666666667, 0.0, 0.6666666666666666, 0.6025092633145984, 0.5866120019410991, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.78353584], dtype=float32), -0.34630686]. 
=============================================
[2019-04-04 05:00:53,219] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[93.7463  ]
 [93.372925]
 [92.985085]
 [92.55356 ]
 [92.17171 ]], R is [[94.15343475]
 [94.2118988 ]
 [94.26978302]
 [94.3270874 ]
 [94.38381958]].
[2019-04-04 05:01:01,666] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8229892e-06 3.2837728e-05 5.7753651e-24 2.3593509e-05 4.6118890e-05
 3.6331618e-11 9.9989057e-01], sum to 1.0000
[2019-04-04 05:01:01,667] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8152
[2019-04-04 05:01:01,688] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.58333333333333, 80.5, 0.0, 0.0, 26.0, 25.75474401037617, 0.5771682916854123, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1014600.0000, 
sim time next is 1015200.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.96261226962164, 0.582718063583057, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6635510224684701, 0.6942393545276856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2972141], dtype=float32), -2.2484787]. 
=============================================
[2019-04-04 05:01:03,117] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4338070e-04 8.9469535e-04 9.8551494e-24 2.8188515e-04 8.1361459e-06
 1.1933866e-10 9.9847192e-01], sum to 1.0000
[2019-04-04 05:01:03,121] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7565
[2019-04-04 05:01:03,133] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.66666666666667, 78.33333333333334, 109.3333333333333, 77.99999999999999, 26.0, 26.99648418883108, 0.8382516736285656, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1073400.0000, 
sim time next is 1074000.0000, 
raw observation next is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 26.0, 27.07983144310154, 0.8464824510382267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8513388734995383, 0.7666666666666667, 0.37222222222222234, 0.043093922651933694, 0.6666666666666666, 0.7566526202584617, 0.7821608170127422, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4541921], dtype=float32), 0.1792754]. 
=============================================
[2019-04-04 05:01:03,149] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[98.426285]
 [98.01909 ]
 [97.455536]
 [96.75188 ]
 [96.27149 ]], R is [[98.66703796]
 [98.68036652]
 [98.69356537]
 [98.70662689]
 [98.71955872]].
[2019-04-04 05:01:05,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4827379e-04 2.2130349e-04 1.1531406e-25 3.3696128e-06 4.4871581e-06
 5.3425784e-12 9.9962246e-01], sum to 1.0000
[2019-04-04 05:01:05,052] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7870
[2019-04-04 05:01:05,065] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.95352723341412, 0.6230615818295425, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1030800.0000, 
sim time next is 1031400.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.95653391884445, 0.6151629859886196, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6630444932370375, 0.7050543286628731, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5083888], dtype=float32), 1.0615935]. 
=============================================
[2019-04-04 05:01:08,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6869277e-04 7.0066795e-05 3.1826861e-23 1.7076252e-05 9.0218055e-06
 8.6202115e-11 9.9943513e-01], sum to 1.0000
[2019-04-04 05:01:08,444] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5704
[2019-04-04 05:01:08,461] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.65308709918669, 0.6412842504923313, 0.0, 1.0, 23305.29931218241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.65501233008788, 0.6403383440355398, 0.0, 1.0, 22261.04072651945], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6379176941739901, 0.7134461146785133, 0.0, 1.0, 0.10600495584056882], 
reward next is 0.8940, 
noisyNet noise sample is [array([0.84741855], dtype=float32), 1.1609548]. 
=============================================
[2019-04-04 05:01:08,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[88.80303]
 [89.26434]
 [89.9497 ]
 [90.24545]
 [91.00979]], R is [[88.37693024]
 [88.38218689]
 [88.36975861]
 [88.32206726]
 [88.23194122]].
[2019-04-04 05:01:13,566] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0075795e-03 1.6036720e-04 4.8508461e-23 7.4677841e-06 1.9363826e-05
 1.1784959e-10 9.9880517e-01], sum to 1.0000
[2019-04-04 05:01:13,567] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3894
[2019-04-04 05:01:13,581] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.60764175715581, 0.5487738437454738, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315200.0000, 
sim time next is 1315800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55332187212758, 0.5336466504133606, 0.0, 1.0, 18744.22574633867], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.629443489343965, 0.6778822168044535, 0.0, 1.0, 0.08925821783970796], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.2600092], dtype=float32), 0.2552798]. 
=============================================
[2019-04-04 05:01:14,040] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2275887e-04 1.6506775e-03 1.4772588e-19 3.3842454e-05 4.9486829e-05
 1.6640077e-09 9.9784327e-01], sum to 1.0000
[2019-04-04 05:01:14,051] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7834
[2019-04-04 05:01:14,063] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.43067299690024, 0.1400757020412419, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1237200.0000, 
sim time next is 1237800.0000, 
raw observation next is [15.0, 96.0, 9.333333333333332, 0.0, 26.0, 23.45280133392239, 0.1390736327584275, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.031111111111111107, 0.0, 0.6666666666666666, 0.45440011116019924, 0.5463578775861425, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5190905], dtype=float32), -0.4762185]. 
=============================================
[2019-04-04 05:01:14,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2567571e-03 3.1605596e-03 1.2596504e-23 3.7232522e-04 1.4296400e-05
 4.1635534e-10 9.9119604e-01], sum to 1.0000
[2019-04-04 05:01:14,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8796
[2019-04-04 05:01:14,750] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.55, 99.33333333333334, 6.333333333333332, 0.0, 26.0, 24.57875517091828, 0.4246045146157272, 0.0, 1.0, 42177.66828171539], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1271400.0000, 
sim time next is 1272000.0000, 
raw observation next is [10.9, 98.66666666666667, 0.0, 0.0, 26.0, 24.58102339795773, 0.4281103198916327, 0.0, 1.0, 44926.37007524565], 
processed observation next is [0.0, 0.7391304347826086, 0.7645429362880888, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5484186164964774, 0.6427034399638776, 0.0, 1.0, 0.21393509559640786], 
reward next is 0.7861, 
noisyNet noise sample is [array([0.24180995], dtype=float32), -2.039422]. 
=============================================
[2019-04-04 05:01:14,767] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[103.08656]
 [103.15539]
 [103.20984]
 [103.34699]
 [103.42602]], R is [[102.77561951]
 [102.54701996]
 [102.36873627]
 [102.21439362]
 [102.06192017]].
[2019-04-04 05:01:15,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3946625e-06 1.2012846e-06 3.3013577e-25 1.0569985e-06 2.6147816e-08
 2.8993988e-12 9.9999535e-01], sum to 1.0000
[2019-04-04 05:01:15,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3160
[2019-04-04 05:01:15,044] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.5, 26.66666666666666, 0.0, 26.0, 25.71126896327474, 0.5135984201258775, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1354200.0000, 
sim time next is 1354800.0000, 
raw observation next is [0.9000000000000001, 94.0, 22.33333333333333, 0.0, 26.0, 25.70175726754256, 0.5087686929699463, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.48753462603878117, 0.94, 0.07444444444444442, 0.0, 0.6666666666666666, 0.6418131056285468, 0.6695895643233154, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44857818], dtype=float32), -1.4135438]. 
=============================================
[2019-04-04 05:01:32,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0167846e-06 5.1106931e-06 3.8702400e-24 3.0705814e-06 2.2722629e-07
 1.1090104e-11 9.9998856e-01], sum to 1.0000
[2019-04-04 05:01:32,023] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4506
[2019-04-04 05:01:32,032] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.93333333333334, 52.00000000000001, 54.66666666666667, 30.83333333333334, 26.0, 27.45013360680305, 0.8664077936303342, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1614000.0000, 
sim time next is 1614600.0000, 
raw observation next is [12.75, 52.5, 50.0, 37.0, 26.0, 27.4864954882568, 0.8714478174543273, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8157894736842106, 0.525, 0.16666666666666666, 0.04088397790055249, 0.6666666666666666, 0.7905412906880667, 0.790482605818109, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7390931], dtype=float32), 1.3240682]. 
=============================================
[2019-04-04 05:01:33,031] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1309759e-05 3.0422312e-05 1.7184656e-22 8.2903189e-06 3.7574082e-06
 2.4423246e-11 9.9991620e-01], sum to 1.0000
[2019-04-04 05:01:33,035] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0292
[2019-04-04 05:01:33,072] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 26.0, 26.05918246944113, 0.5836543683984624, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1586400.0000, 
sim time next is 1587000.0000, 
raw observation next is [6.333333333333333, 77.0, 62.99999999999999, 68.66666666666666, 26.0, 26.15609803387455, 0.5926585858603107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6380424746075716, 0.77, 0.20999999999999996, 0.07587476979742172, 0.6666666666666666, 0.6796748361562125, 0.6975528619534369, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0149638], dtype=float32), 0.1459845]. 
=============================================
[2019-04-04 05:01:33,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[85.943596]
 [85.54336 ]
 [85.28566 ]
 [85.21704 ]
 [85.2804  ]], R is [[86.42370605]
 [86.55947113]
 [86.69387817]
 [86.82694244]
 [86.95867157]].
[2019-04-04 05:01:40,366] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0481489e-04 3.7584789e-05 2.6603421e-21 1.9237299e-05 8.8863462e-06
 5.4528809e-10 9.9962938e-01], sum to 1.0000
[2019-04-04 05:01:40,367] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8147
[2019-04-04 05:01:40,420] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 85.0, 119.0, 0.0, 26.0, 24.92418882784363, 0.3573575879245591, 0.0, 1.0, 29960.73063675156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1769400.0000, 
sim time next is 1770000.0000, 
raw observation next is [-2.3, 84.33333333333333, 120.1666666666667, 0.0, 26.0, 24.94065832388313, 0.3560027488057861, 0.0, 1.0, 27776.25427176402], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8433333333333333, 0.40055555555555566, 0.0, 0.6666666666666666, 0.5783881936569276, 0.618667582935262, 0.0, 1.0, 0.13226787748459057], 
reward next is 0.8677, 
noisyNet noise sample is [array([-0.34971148], dtype=float32), -1.6709728]. 
=============================================
[2019-04-04 05:01:40,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.75344]
 [82.59649]
 [82.36959]
 [82.10237]
 [81.80828]], R is [[82.91411591]
 [82.94230652]
 [82.88792419]
 [82.71292114]
 [82.54781342]].
[2019-04-04 05:01:48,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0355546e-05 8.2848319e-06 1.0383731e-20 1.5676154e-05 4.9322739e-06
 4.5081530e-10 9.9996078e-01], sum to 1.0000
[2019-04-04 05:01:48,314] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3018
[2019-04-04 05:01:48,374] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.00180907690805, 0.3222291138661868, 0.0, 1.0, 47185.36203292789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [-3.899999999999999, 82.0, 0.0, 0.0, 26.0, 25.00032737956111, 0.321267957555357, 0.0, 1.0, 46620.4363437233], 
processed observation next is [0.0, 0.7391304347826086, 0.35457063711911363, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5833606149634258, 0.607089319185119, 0.0, 1.0, 0.22200207782725379], 
reward next is 0.7780, 
noisyNet noise sample is [array([0.41097113], dtype=float32), 1.7619311]. 
=============================================
[2019-04-04 05:01:50,000] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.3496692e-05 1.9239426e-04 2.0936017e-20 1.3032163e-05 4.1140393e-06
 2.7918665e-09 9.9970692e-01], sum to 1.0000
[2019-04-04 05:01:50,001] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3292
[2019-04-04 05:01:50,027] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.0066146914753, 0.3077737004073763, 0.0, 1.0, 46047.35026902585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1803000.0000, 
sim time next is 1803600.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 25.03815412444045, 0.3103467653539184, 0.0, 1.0, 46024.75265205438], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5865128437033708, 0.6034489217846395, 0.0, 1.0, 0.2191654888193066], 
reward next is 0.7808, 
noisyNet noise sample is [array([0.00175591], dtype=float32), 0.21867396]. 
=============================================
[2019-04-04 05:01:59,728] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2686552e-03 2.7430695e-04 2.8384436e-18 1.9956274e-04 2.8155084e-05
 1.6422128e-08 9.9722928e-01], sum to 1.0000
[2019-04-04 05:01:59,729] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0879
[2019-04-04 05:01:59,764] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.55, 76.5, 0.0, 0.0, 26.0, 24.05538630329088, 0.02238527170339323, 0.0, 1.0, 45193.16785421497], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1906200.0000, 
sim time next is 1906800.0000, 
raw observation next is [-7.633333333333333, 77.0, 0.0, 0.0, 26.0, 24.04180782829318, 0.02210978835539038, 0.0, 1.0, 45144.30654514456], 
processed observation next is [1.0, 0.043478260869565216, 0.2511542012927055, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5034839856910983, 0.5073699294517968, 0.0, 1.0, 0.2149728883102122], 
reward next is 0.7850, 
noisyNet noise sample is [array([0.87470466], dtype=float32), -0.46972597]. 
=============================================
[2019-04-04 05:02:03,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3452931e-03 2.7308686e-04 2.2340197e-19 1.7744563e-04 1.5708702e-04
 9.1725783e-09 9.9804711e-01], sum to 1.0000
[2019-04-04 05:02:03,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3390
[2019-04-04 05:02:03,741] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11542940925206, 0.1041186419336874, 0.0, 1.0, 43822.83837988741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2092200.0000, 
sim time next is 2092800.0000, 
raw observation next is [-6.366666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 24.17870658322742, 0.1069507942771894, 0.0, 1.0, 43565.18449575889], 
processed observation next is [1.0, 0.21739130434782608, 0.28624192059095105, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5148922152689517, 0.5356502647590632, 0.0, 1.0, 0.20745325950361376], 
reward next is 0.7925, 
noisyNet noise sample is [array([-1.275434], dtype=float32), -1.3260106]. 
=============================================
[2019-04-04 05:02:04,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2649968e-05 2.8116401e-05 2.2605764e-20 1.3612954e-05 9.7327802e-07
 7.5354006e-10 9.9994469e-01], sum to 1.0000
[2019-04-04 05:02:04,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9598
[2019-04-04 05:02:05,014] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.816666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 25.33151537176091, 0.3483253744151336, 1.0, 1.0, 24664.77125767474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1965000.0000, 
sim time next is 1965600.0000, 
raw observation next is [-5.0, 79.0, 0.0, 0.0, 26.0, 25.37742217450906, 0.3459150308115611, 1.0, 1.0, 33762.3215259913], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6147851812090884, 0.6153050102705203, 1.0, 1.0, 0.1607729596475776], 
reward next is 0.8392, 
noisyNet noise sample is [array([-0.9721328], dtype=float32), 1.4023291]. 
=============================================
[2019-04-04 05:02:09,940] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9872546e-05 1.0366282e-04 3.4155070e-22 5.2692385e-06 1.3844498e-06
 1.0763621e-10 9.9985969e-01], sum to 1.0000
[2019-04-04 05:02:09,940] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3187
[2019-04-04 05:02:10,008] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.416666666666667, 81.66666666666667, 132.0, 0.0, 26.0, 25.57080447045203, 0.3198956967967936, 1.0, 1.0, 28319.73974966056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2027400.0000, 
sim time next is 2028000.0000, 
raw observation next is [-5.233333333333333, 80.33333333333334, 139.5, 0.0, 26.0, 25.57396770544938, 0.327451068039565, 1.0, 1.0, 25690.79445088901], 
processed observation next is [1.0, 0.4782608695652174, 0.31763619575253926, 0.8033333333333335, 0.465, 0.0, 0.6666666666666666, 0.6311639754541151, 0.6091503560131883, 1.0, 1.0, 0.12233711643280482], 
reward next is 0.8777, 
noisyNet noise sample is [array([0.2772428], dtype=float32), -0.6571459]. 
=============================================
[2019-04-04 05:02:10,118] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[82.69445 ]
 [82.43772 ]
 [82.12473 ]
 [81.92045 ]
 [81.686516]], R is [[82.98722076]
 [83.02249146]
 [83.03527832]
 [83.04837799]
 [83.06209564]].
[2019-04-04 05:02:33,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5953244e-06 2.6310581e-05 7.3611570e-21 3.5787932e-06 1.9106173e-07
 7.2872003e-10 9.9996233e-01], sum to 1.0000
[2019-04-04 05:02:33,914] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9693
[2019-04-04 05:02:33,934] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.100000000000001, 91.0, 0.0, 0.0, 26.0, 23.59987321537937, -0.04775079182763018, 0.0, 1.0, 43156.89249576013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2269200.0000, 
sim time next is 2269800.0000, 
raw observation next is [-9.2, 91.0, 0.0, 0.0, 26.0, 23.52957004906589, -0.05217350003904785, 0.0, 1.0, 43142.86111665622], 
processed observation next is [1.0, 0.2608695652173913, 0.20775623268698065, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4607975040888241, 0.4826088333203174, 0.0, 1.0, 0.20544219579360104], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.27062133], dtype=float32), -0.95285887]. 
=============================================
[2019-04-04 05:02:35,702] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.08981785e-05 9.73142414e-06 2.76898258e-20 4.40224630e-06
 1.35592586e-06 1.91400917e-09 9.99973655e-01], sum to 1.0000
[2019-04-04 05:02:35,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6594
[2019-04-04 05:02:35,871] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 25.33333333333334, 0.0, 26.0, 24.04263141453865, 0.06973388220480635, 0.0, 1.0, 41993.93585048907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2362200.0000, 
sim time next is 2362800.0000, 
raw observation next is [-3.4, 69.0, 31.16666666666667, 0.0, 26.0, 24.07001252742561, 0.1323920318618422, 0.0, 1.0, 202421.3901192262], 
processed observation next is [0.0, 0.34782608695652173, 0.368421052631579, 0.69, 0.1038888888888889, 0.0, 0.6666666666666666, 0.5058343772854675, 0.5441306772872807, 0.0, 1.0, 0.9639113815201248], 
reward next is 0.0361, 
noisyNet noise sample is [array([-0.17093593], dtype=float32), 1.1275408]. 
=============================================
[2019-04-04 05:02:39,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1166586e-04 2.2241651e-05 5.9179619e-21 5.4286388e-06 1.8676791e-06
 1.2201582e-09 9.9975878e-01], sum to 1.0000
[2019-04-04 05:02:39,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2815
[2019-04-04 05:02:40,012] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333333, 64.0, 0.0, 0.0, 26.0, 24.51127673636048, 0.1833344775194038, 0.0, 1.0, 39969.17238761049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2346000.0000, 
sim time next is 2346600.0000, 
raw observation next is [-2.716666666666667, 64.5, 0.0, 0.0, 26.0, 24.47784817511209, 0.176318351459878, 0.0, 1.0, 40090.96849457977], 
processed observation next is [0.0, 0.13043478260869565, 0.3873499538319483, 0.645, 0.0, 0.0, 0.6666666666666666, 0.5398206812593408, 0.5587727838199593, 0.0, 1.0, 0.1909093737837132], 
reward next is 0.8091, 
noisyNet noise sample is [array([-0.02713321], dtype=float32), 1.5422642]. 
=============================================
[2019-04-04 05:02:41,958] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 05:02:41,960] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:02:41,961] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:02:41,961] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:02:41,961] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:02:41,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:02:41,962] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:02:41,965] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run3
[2019-04-04 05:02:41,983] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run3
[2019-04-04 05:02:42,000] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run3
[2019-04-04 05:02:58,655] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.22821805], dtype=float32), 0.17234372]
[2019-04-04 05:02:58,655] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-6.666666666666666, 81.0, 0.0, 0.0, 26.0, 24.12791160865809, 0.09339923204688873, 0.0, 1.0, 62363.19242945872]
[2019-04-04 05:02:58,655] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:02:58,656] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.4701646e-05 4.3853288e-06 1.0729444e-21 1.4338453e-06 5.1546613e-07
 2.1521571e-10 9.9997902e-01], sampled 0.004001964041630668
[2019-04-04 05:04:59,594] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.22821805], dtype=float32), 0.17234372]
[2019-04-04 05:04:59,594] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.550464877333333, 98.94115464333333, 0.0, 0.0, 26.0, 25.94764526954634, 0.6857472026406789, 0.0, 1.0, 0.0]
[2019-04-04 05:04:59,594] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:04:59,595] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.6322192e-06 1.3523515e-06 7.9176381e-25 1.8174697e-07 1.1538810e-07
 1.0682171e-11 9.9999475e-01], sampled 0.16098082444441342
[2019-04-04 05:05:00,744] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.22821805], dtype=float32), 0.17234372]
[2019-04-04 05:05:00,744] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-11.0, 76.0, 0.0, 0.0, 26.0, 23.85902360878533, 0.08821534145762344, 0.0, 1.0, 44023.85331476562]
[2019-04-04 05:05:00,744] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:05:00,745] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.4872930e-05 1.4630970e-05 2.2771266e-20 4.6641667e-06 1.6320150e-06
 1.1107422e-09 9.9993420e-01], sampled 0.20928771303858118
[2019-04-04 05:05:28,246] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 05:06:02,964] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 05:06:08,335] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 05:06:09,368] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 200000, evaluation results [200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 05:06:24,459] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1233949e-06 1.0513648e-05 1.3794833e-20 1.9451300e-06 5.1592741e-07
 2.0025779e-09 9.9997997e-01], sum to 1.0000
[2019-04-04 05:06:24,481] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0432
[2019-04-04 05:06:24,589] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.35, 29.5, 0.0, 0.0, 26.0, 24.94148182648747, 0.2494708936849692, 0.0, 1.0, 106680.8001492349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2489400.0000, 
sim time next is 2490000.0000, 
raw observation next is [-0.4666666666666666, 29.33333333333334, 0.0, 0.0, 26.0, 25.01837962860699, 0.2651183123420979, 0.0, 1.0, 62781.7979794069], 
processed observation next is [0.0, 0.8260869565217391, 0.44967682363804257, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5848649690505825, 0.5883727707806993, 0.0, 1.0, 0.2989609427590805], 
reward next is 0.7010, 
noisyNet noise sample is [array([1.0517554], dtype=float32), 0.87661076]. 
=============================================
[2019-04-04 05:06:24,620] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.721794]
 [76.31553 ]
 [75.84983 ]
 [75.898605]
 [75.932144]], R is [[76.80632019]
 [76.53025818]
 [75.93994904]
 [76.01152802]
 [76.05261993]].
[2019-04-04 05:06:27,072] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8903014e-07 2.1219252e-07 7.3693067e-24 5.4233749e-07 1.6293393e-08
 4.2749918e-11 9.9999845e-01], sum to 1.0000
[2019-04-04 05:06:27,077] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7402
[2019-04-04 05:06:27,085] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.533333333333333, 26.66666666666667, 167.0, 413.0, 26.0, 25.78521624935557, 0.3636259215378381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2555400.0000, 
sim time next is 2556000.0000, 
raw observation next is [3.8, 26.0, 165.0, 378.5, 26.0, 25.72370399413612, 0.3663128248596985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.26, 0.55, 0.41823204419889504, 0.6666666666666666, 0.6436419995113433, 0.6221042749532328, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8423721], dtype=float32), 1.6330882]. 
=============================================
[2019-04-04 05:06:27,210] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.639  ]
 [90.64809]
 [90.50354]
 [90.24162]
 [90.07296]], R is [[90.7312851 ]
 [90.82397461]
 [90.91573334]
 [91.00657654]
 [91.09651184]].
[2019-04-04 05:06:36,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9084932e-06 5.2786650e-06 2.4651002e-22 8.1024552e-07 3.2196132e-07
 1.8262619e-11 9.9998760e-01], sum to 1.0000
[2019-04-04 05:06:36,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0102
[2019-04-04 05:06:36,726] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 83.0, 0.0, 0.0, 26.0, 24.30502293409619, 0.09987116872557454, 0.0, 1.0, 42997.1252773469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2610000.0000, 
sim time next is 2610600.0000, 
raw observation next is [-6.283333333333333, 82.16666666666667, 0.0, 0.0, 26.0, 24.22274180366114, 0.08480649499071385, 0.0, 1.0, 43124.78924888631], 
processed observation next is [1.0, 0.21739130434782608, 0.288550323176362, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5185618169717617, 0.5282688316635713, 0.0, 1.0, 0.205356139280411], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.20624295], dtype=float32), 0.4435571]. 
=============================================
[2019-04-04 05:06:39,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.35372726e-07 4.83985332e-07 7.70008392e-24 1.12890959e-07
 1.14763745e-08 6.10827153e-11 9.99998927e-01], sum to 1.0000
[2019-04-04 05:06:39,881] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3224
[2019-04-04 05:06:39,955] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 50.5, 245.0, 147.0, 26.0, 25.71924457897833, 0.2802656685387987, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2637000.0000, 
sim time next is 2637600.0000, 
raw observation next is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 26.0, 25.08760561163622, 0.3138641330985527, 1.0, 1.0, 132687.4563362171], 
processed observation next is [1.0, 0.5217391304347826, 0.43028624192059095, 0.4933333333333334, 0.7716666666666666, 0.17421731123388587, 0.6666666666666666, 0.5906338009696851, 0.6046213776995176, 1.0, 1.0, 0.6318450301724623], 
reward next is 0.3682, 
noisyNet noise sample is [array([0.23472708], dtype=float32), 1.0289869]. 
=============================================
[2019-04-04 05:06:43,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2306773e-06 1.5771518e-07 4.0731346e-24 6.8656263e-08 1.7675174e-09
 1.5170551e-12 9.9999857e-01], sum to 1.0000
[2019-04-04 05:06:43,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0775
[2019-04-04 05:06:43,794] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 58.5, 110.3333333333333, 791.6666666666666, 26.0, 25.80135972176183, 0.5242294679421503, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2725800.0000, 
sim time next is 2726400.0000, 
raw observation next is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 26.0, 25.99444401155127, 0.5529689754961352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.30747922437673136, 0.58, 0.3655555555555557, 0.872744014732965, 0.6666666666666666, 0.6662036676292725, 0.6843229918320451, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21729049], dtype=float32), 0.16060443]. 
=============================================
[2019-04-04 05:07:03,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6533071e-07 2.3193238e-06 7.0491026e-23 2.4889346e-07 3.9811368e-08
 1.8227466e-11 9.9999690e-01], sum to 1.0000
[2019-04-04 05:07:03,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7084
[2019-04-04 05:07:03,135] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 94.16666666666666, 84.0, 0.0, 26.0, 25.46505560905006, 0.3154315100070117, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2891400.0000, 
sim time next is 2892000.0000, 
raw observation next is [1.0, 95.33333333333334, 85.5, 0.0, 26.0, 25.43076659423538, 0.3104214135495404, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.9533333333333335, 0.285, 0.0, 0.6666666666666666, 0.619230549519615, 0.6034738045165134, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.3742539], dtype=float32), -0.40591568]. 
=============================================
[2019-04-04 05:07:03,254] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[91.42489 ]
 [91.39226 ]
 [91.37442 ]
 [91.45126 ]
 [91.463684]], R is [[91.50302887]
 [91.49904633]
 [91.49510193]
 [91.49120331]
 [91.48734283]].
[2019-04-04 05:07:11,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0248009e-04 9.4484967e-06 1.5368861e-21 8.0596528e-06 3.5630430e-06
 3.7194006e-10 9.9977642e-01], sum to 1.0000
[2019-04-04 05:07:11,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0407
[2019-04-04 05:07:12,003] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 73.0, 95.66666666666666, 62.83333333333333, 26.0, 24.61257074690423, 0.3135246881524914, 0.0, 1.0, 87760.46136687917], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2968800.0000, 
sim time next is 2969400.0000, 
raw observation next is [-4.0, 72.0, 107.3333333333333, 76.66666666666666, 26.0, 25.00984414543303, 0.3507439627775344, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.72, 0.3577777777777777, 0.08471454880294658, 0.6666666666666666, 0.5841536787860857, 0.6169146542591781, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5919449], dtype=float32), 0.7213087]. 
=============================================
[2019-04-04 05:07:21,914] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1229231e-05 2.2271346e-05 9.8031960e-20 1.7876380e-06 3.9274123e-06
 1.7410632e-09 9.9990082e-01], sum to 1.0000
[2019-04-04 05:07:21,915] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1255
[2019-04-04 05:07:21,997] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93510819493864, 0.2774300809456069, 0.0, 1.0, 38149.91466377437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.9423152980596, 0.2721059941181795, 0.0, 1.0, 38074.72333805376], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5785262748383001, 0.5907019980393932, 0.0, 1.0, 0.18130820637168457], 
reward next is 0.8187, 
noisyNet noise sample is [array([-1.7649337], dtype=float32), 0.99250615]. 
=============================================
[2019-04-04 05:07:31,875] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7893564e-05 5.0177044e-07 3.4231444e-24 5.4022655e-07 1.5267899e-07
 5.6959413e-11 9.9998093e-01], sum to 1.0000
[2019-04-04 05:07:31,895] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6602
[2019-04-04 05:07:31,917] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.45014364067578, 0.3202273735626772, 0.0, 1.0, 28877.29932024463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3129600.0000, 
sim time next is 3130200.0000, 
raw observation next is [3.5, 100.0, 0.0, 0.0, 26.0, 25.48332789697912, 0.3154276637445058, 0.0, 1.0, 22904.604147745], 
processed observation next is [1.0, 0.21739130434782608, 0.5595567867036012, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6236106580815933, 0.6051425545815019, 0.0, 1.0, 0.10906954356069049], 
reward next is 0.8909, 
noisyNet noise sample is [array([-0.91994333], dtype=float32), -1.580143]. 
=============================================
[2019-04-04 05:07:33,723] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.8454881e-06 4.0434171e-07 1.4741445e-24 2.4308127e-07 7.7744289e-08
 1.3737850e-11 9.9999034e-01], sum to 1.0000
[2019-04-04 05:07:33,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4078
[2019-04-04 05:07:33,741] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.21682247101361, 0.2868590744258635, 0.0, 1.0, 54075.71516608466], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3125400.0000, 
sim time next is 3126000.0000, 
raw observation next is [2.733333333333333, 100.0, 0.0, 0.0, 26.0, 25.1949620458576, 0.2920807639083881, 0.0, 1.0, 53980.96416678719], 
processed observation next is [1.0, 0.17391304347826086, 0.538319482917821, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5995801704881334, 0.5973602546361293, 0.0, 1.0, 0.25705221031803427], 
reward next is 0.7429, 
noisyNet noise sample is [array([-0.88430566], dtype=float32), -0.708182]. 
=============================================
[2019-04-04 05:07:33,758] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[90.92554]
 [90.83527]
 [90.69231]
 [90.63078]
 [90.50287]], R is [[90.89963531]
 [90.73313904]
 [90.56723785]
 [90.40007019]
 [90.22541046]].
[2019-04-04 05:07:45,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6016409e-07 1.1823943e-07 8.7093538e-23 7.3278130e-09 7.6967268e-09
 6.3598536e-12 9.9999964e-01], sum to 1.0000
[2019-04-04 05:07:45,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8157
[2019-04-04 05:07:45,868] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 84.0, 0.0, 0.0, 26.0, 25.13921259133572, 0.4209409477379227, 0.0, 1.0, 43415.81356779931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3283200.0000, 
sim time next is 3283800.0000, 
raw observation next is [-7.0, 81.66666666666667, 0.0, 0.0, 26.0, 25.11813410324554, 0.413784293845313, 0.0, 1.0, 43492.17694177867], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.5931778419371284, 0.6379280979484377, 0.0, 1.0, 0.20710560448466034], 
reward next is 0.7929, 
noisyNet noise sample is [array([-1.8402591], dtype=float32), 0.33587793]. 
=============================================
[2019-04-04 05:07:48,772] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4620083e-06 4.0716381e-07 1.4632163e-21 4.4062094e-08 5.0037290e-08
 3.6020385e-11 9.9999809e-01], sum to 1.0000
[2019-04-04 05:07:48,787] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5893
[2019-04-04 05:07:48,800] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.81269261861162, 0.294314282065314, 0.0, 1.0, 41164.86070969047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3378000.0000, 
sim time next is 3378600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82563467720822, 0.2876497300754918, 0.0, 1.0, 41138.59593599567], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5688028897673517, 0.5958832433584973, 0.0, 1.0, 0.19589807588569366], 
reward next is 0.8041, 
noisyNet noise sample is [array([2.3659928], dtype=float32), 0.4110097]. 
=============================================
[2019-04-04 05:07:52,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9757987e-07 1.5890346e-07 1.8986947e-21 1.2529415e-07 1.6794493e-08
 1.4838525e-10 9.9999928e-01], sum to 1.0000
[2019-04-04 05:07:52,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5237
[2019-04-04 05:07:52,874] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 26.0, 25.57698684848944, 0.3974036479016587, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3397800.0000, 
sim time next is 3398400.0000, 
raw observation next is [-2.0, 60.0, 44.5, 264.5, 26.0, 25.68058259835522, 0.3822761417654371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.6, 0.14833333333333334, 0.29226519337016577, 0.6666666666666666, 0.6400485498629349, 0.6274253805884791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20010228], dtype=float32), 0.6437136]. 
=============================================
[2019-04-04 05:07:56,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.00009219e-07 2.27938045e-07 1.11922586e-20 8.70148185e-08
 3.13628483e-08 5.06593422e-12 9.99999404e-01], sum to 1.0000
[2019-04-04 05:07:56,139] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9364
[2019-04-04 05:07:56,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 26.0, 26.38673282698884, 0.6766108378315531, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3341400.0000, 
sim time next is 3342000.0000, 
raw observation next is [-2.0, 47.33333333333334, 64.5, 529.8333333333333, 26.0, 26.5696177371272, 0.6797178708207267, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.47333333333333344, 0.215, 0.5854511970534069, 0.6666666666666666, 0.7141348114272666, 0.7265726236069089, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3180584], dtype=float32), 0.01050169]. 
=============================================
[2019-04-04 05:07:56,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[83.85334 ]
 [84.13638 ]
 [84.392494]
 [84.57091 ]
 [84.80995 ]], R is [[83.68572998]
 [83.84887695]
 [84.01039124]
 [83.87243652]
 [84.03371429]].
[2019-04-04 05:08:01,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2360064e-07 3.5940626e-08 2.8678241e-24 1.1340431e-08 5.5905516e-09
 6.0494205e-12 9.9999988e-01], sum to 1.0000
[2019-04-04 05:08:01,556] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8399
[2019-04-04 05:08:01,562] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 60.33333333333334, 107.3333333333333, 753.3333333333334, 26.0, 26.26353079561235, 0.5763754904386424, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3493200.0000, 
sim time next is 3493800.0000, 
raw observation next is [0.5, 60.5, 109.0, 770.0, 26.0, 26.21617283851798, 0.5810239389136196, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.605, 0.36333333333333334, 0.850828729281768, 0.6666666666666666, 0.6846810698764983, 0.6936746463045399, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26200917], dtype=float32), 0.8487305]. 
=============================================
[2019-04-04 05:08:11,355] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1422036e-07 4.5020595e-08 7.7081757e-23 8.5766828e-08 3.0845126e-08
 1.2814153e-10 9.9999964e-01], sum to 1.0000
[2019-04-04 05:08:11,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8618
[2019-04-04 05:08:11,405] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.36491458525624, 0.5900672882549447, 0.0, 1.0, 62126.06465094622], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.66612647064687, 0.6105201893875377, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6388438725539057, 0.7035067297958459, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1958009], dtype=float32), 1.1425006]. 
=============================================
[2019-04-04 05:08:13,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2234753e-08 6.9322543e-09 4.3595211e-23 4.9021374e-09 1.0428637e-09
 1.3001820e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:08:13,514] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1711
[2019-04-04 05:08:13,564] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 54.16666666666667, 116.6666666666667, 820.6666666666667, 26.0, 25.17565913374882, 0.4458010073725249, 0.0, 1.0, 18712.09048306755], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3586200.0000, 
sim time next is 3586800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 117.3333333333333, 821.8333333333334, 26.0, 25.17569618697387, 0.4479376202945838, 0.0, 1.0, 18711.51231074541], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.391111111111111, 0.9081031307550645, 0.6666666666666666, 0.5979746822478225, 0.6493125400981946, 0.0, 1.0, 0.08910243957497814], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.36594516], dtype=float32), -0.037923966]. 
=============================================
[2019-04-04 05:08:18,257] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2366662e-08 1.0950991e-08 2.7714490e-24 1.3833577e-09 2.7223721e-10
 2.8492107e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:08:18,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1467
[2019-04-04 05:08:18,293] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 48.5, 110.6666666666667, 810.0, 26.0, 25.99413043388459, 0.6238931334831198, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3851400.0000, 
sim time next is 3852000.0000, 
raw observation next is [2.0, 48.0, 109.5, 803.0, 26.0, 26.27028317615746, 0.6689612544704744, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.365, 0.887292817679558, 0.6666666666666666, 0.6891902646797883, 0.7229870848234915, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4749405], dtype=float32), 1.9451427]. 
=============================================
[2019-04-04 05:08:18,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.868065]
 [94.05772 ]
 [94.146904]
 [94.26097 ]
 [94.33689 ]], R is [[93.81234741]
 [93.8742218 ]
 [93.83630371]
 [93.89794159]
 [93.95896149]].
[2019-04-04 05:08:18,664] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3579950e-07 4.7624393e-08 3.5454681e-23 1.7381074e-08 2.3568938e-09
 2.0840387e-12 9.9999976e-01], sum to 1.0000
[2019-04-04 05:08:18,666] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1964
[2019-04-04 05:08:18,702] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.12319690560131, 0.281269074160826, 0.0, 1.0, 41589.68246436252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3735600.0000, 
sim time next is 3736200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.0843992526332, 0.2819215337812886, 0.0, 1.0, 41667.67515265119], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5903666043861001, 0.5939738445937629, 0.0, 1.0, 0.19841750072691042], 
reward next is 0.8016, 
noisyNet noise sample is [array([-0.25173378], dtype=float32), 0.72696537]. 
=============================================
[2019-04-04 05:08:25,035] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.13288376e-07 6.65531132e-08 5.93122300e-23 5.92771734e-08
 1.07750004e-07 3.96589948e-11 9.99999285e-01], sum to 1.0000
[2019-04-04 05:08:25,037] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6446
[2019-04-04 05:08:25,047] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 63.0, 0.0, 0.0, 26.0, 25.41481779075384, 0.4591035773117453, 0.0, 1.0, 29315.97125408922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3700800.0000, 
sim time next is 3701400.0000, 
raw observation next is [2.833333333333333, 62.83333333333333, 0.0, 0.0, 26.0, 25.60301711264962, 0.4708548094738496, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.541089566020314, 0.6283333333333333, 0.0, 0.0, 0.6666666666666666, 0.6335847593874684, 0.6569516031579499, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.323118], dtype=float32), -0.6523633]. 
=============================================
[2019-04-04 05:08:35,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6412814e-08 4.8914359e-08 1.5943294e-23 3.7777546e-08 8.7439558e-09
 1.7393519e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:08:35,735] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5800
[2019-04-04 05:08:35,743] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.69698274479822, 0.4253645583616018, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3897600.0000, 
sim time next is 3898200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.5531965235323, 0.3987837398776019, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6294330436276917, 0.632927913292534, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42474476], dtype=float32), -0.040210847]. 
=============================================
[2019-04-04 05:08:43,690] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8588084e-07 4.8867446e-07 1.1653069e-21 6.8650934e-08 1.5613425e-08
 1.7052541e-10 9.9999917e-01], sum to 1.0000
[2019-04-04 05:08:43,691] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8345
[2019-04-04 05:08:43,707] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.33333333333333, 65.0, 0.0, 0.0, 26.0, 24.06657132442518, 0.09579529606709908, 0.0, 1.0, 43711.43619697225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3990000.0000, 
sim time next is 3990600.0000, 
raw observation next is [-12.5, 66.0, 0.0, 0.0, 26.0, 24.04040549088656, 0.08143282893276703, 0.0, 1.0, 43726.02318306875], 
processed observation next is [1.0, 0.17391304347826086, 0.11634349030470914, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5033671242405466, 0.5271442763109223, 0.0, 1.0, 0.20821915801461308], 
reward next is 0.7918, 
noisyNet noise sample is [array([-0.02910861], dtype=float32), 1.6626123]. 
=============================================
[2019-04-04 05:08:44,232] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1472163e-08 5.5312306e-09 4.9273727e-24 1.4355646e-09 2.4969471e-09
 9.7695437e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:08:44,234] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6889
[2019-04-04 05:08:44,299] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 36.0, 105.6666666666667, 694.0, 26.0, 26.29243521487123, 0.5244457082026198, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4095600.0000, 
sim time next is 4096200.0000, 
raw observation next is [-2.166666666666667, 35.5, 107.3333333333333, 709.0, 26.0, 26.33947618374446, 0.5404797497801375, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4025854108956602, 0.355, 0.3577777777777777, 0.7834254143646409, 0.6666666666666666, 0.6949563486453716, 0.6801599165933792, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6279185], dtype=float32), 0.3373996]. 
=============================================
[2019-04-04 05:08:44,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.80217633e-08 5.67645912e-08 1.63860228e-21 1.11811346e-07
 3.64818931e-08 4.64258146e-11 9.99999762e-01], sum to 1.0000
[2019-04-04 05:08:44,758] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6701
[2019-04-04 05:08:44,773] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 34.66666666666667, 0.0, 0.0, 26.0, 24.75015297734081, 0.2177895745996149, 0.0, 1.0, 40289.95047856955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4079400.0000, 
sim time next is 4080000.0000, 
raw observation next is [-4.0, 35.33333333333334, 0.0, 0.0, 26.0, 24.79415329747249, 0.2101460284846881, 0.0, 1.0, 40247.45885684295], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.35333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5661794414560409, 0.5700486761615627, 0.0, 1.0, 0.19165456598496644], 
reward next is 0.8083, 
noisyNet noise sample is [array([0.44186684], dtype=float32), 0.605603]. 
=============================================
[2019-04-04 05:08:44,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.01186 ]
 [78.08333 ]
 [78.150856]
 [78.26432 ]
 [78.31597 ]], R is [[77.99016571]
 [78.01840973]
 [78.04611206]
 [78.07326508]
 [78.10004425]].
[2019-04-04 05:08:45,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7709034e-07 2.8189649e-07 6.9025597e-21 5.2584717e-08 2.7246635e-08
 9.1310494e-11 9.9999917e-01], sum to 1.0000
[2019-04-04 05:08:45,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5824
[2019-04-04 05:08:45,489] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.46799044674703, 0.4702265552872409, 1.0, 1.0, 26890.27330100852], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4045200.0000, 
sim time next is 4045800.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.42079131931351, 0.4607695053154486, 0.0, 1.0, 26865.88745267154], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.6666666666666666, 0.618399276609459, 0.6535898351051496, 0.0, 1.0, 0.127932797393674], 
reward next is 0.8721, 
noisyNet noise sample is [array([0.06488366], dtype=float32), 0.8307434]. 
=============================================
[2019-04-04 05:08:53,075] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.75135455e-07 6.94515947e-08 8.69782809e-24 1.50329214e-07
 6.76567646e-09 1.16951735e-11 9.99999642e-01], sum to 1.0000
[2019-04-04 05:08:53,076] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7370
[2019-04-04 05:08:53,095] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 55.0, 26.5, 26.0, 25.31670178537833, 0.3283670587630224, 0.0, 1.0, 39003.05835704508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4262400.0000, 
sim time next is 4263000.0000, 
raw observation next is [3.0, 49.66666666666667, 73.33333333333334, 35.33333333333334, 26.0, 25.31951492376782, 0.3337420807833309, 0.0, 1.0, 38912.19128795216], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.4966666666666667, 0.24444444444444446, 0.03904235727440148, 0.6666666666666666, 0.6099595769806516, 0.6112473602611103, 0.0, 1.0, 0.1852961489902484], 
reward next is 0.8147, 
noisyNet noise sample is [array([-1.2585425], dtype=float32), 0.7898671]. 
=============================================
[2019-04-04 05:08:53,106] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.62704 ]
 [86.29068 ]
 [86.111496]
 [85.98461 ]
 [85.952446]], R is [[86.99867249]
 [86.94295502]
 [86.88745117]
 [86.83227539]
 [86.77751923]].
[2019-04-04 05:08:53,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5865015e-06 2.0876316e-06 3.1759578e-22 1.1500001e-07 8.8471829e-08
 6.5338202e-10 9.9999607e-01], sum to 1.0000
[2019-04-04 05:08:53,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0382
[2019-04-04 05:08:53,846] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.23192153357978, 0.3738040185860109, 0.0, 1.0, 39321.86997100872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4156800.0000, 
sim time next is 4157400.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.20439905194671, 0.367129850601606, 0.0, 1.0, 39370.20023193712], 
processed observation next is [0.0, 0.08695652173913043, 0.3841181902123731, 0.4933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6003665876622257, 0.622376616867202, 0.0, 1.0, 0.1874771439616053], 
reward next is 0.8125, 
noisyNet noise sample is [array([1.8538066], dtype=float32), -0.21015318]. 
=============================================
[2019-04-04 05:08:55,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9364803e-08 6.9372703e-08 1.4201046e-23 8.5247853e-09 8.0176479e-09
 2.1794443e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:08:55,810] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9637
[2019-04-04 05:08:55,834] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 35.0, 115.6666666666667, 790.0, 26.0, 25.27537238546806, 0.4083799340773173, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4186200.0000, 
sim time next is 4186800.0000, 
raw observation next is [-1.0, 35.0, 116.5, 798.0, 26.0, 25.22851211802133, 0.4062008048612595, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4349030470914128, 0.35, 0.3883333333333333, 0.881767955801105, 0.6666666666666666, 0.602376009835111, 0.6354002682870865, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0281458], dtype=float32), 1.2068079]. 
=============================================
[2019-04-04 05:08:58,330] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6655655e-07 2.8998153e-08 6.2199681e-25 9.7479447e-09 8.2282661e-09
 5.1839050e-12 9.9999964e-01], sum to 1.0000
[2019-04-04 05:08:58,337] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9498
[2019-04-04 05:08:58,349] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.416666666666667, 75.16666666666667, 0.0, 0.0, 26.0, 25.59458779498613, 0.4209795849309758, 0.0, 1.0, 18738.88679311129], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4317000.0000, 
sim time next is 4317600.0000, 
raw observation next is [4.433333333333334, 75.33333333333334, 0.0, 0.0, 26.0, 25.60146502809562, 0.4125843836153072, 0.0, 1.0, 18736.84735143989], 
processed observation next is [0.0, 1.0, 0.5854108956602032, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6334554190079684, 0.637528127871769, 0.0, 1.0, 0.08922308262590424], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.8741817], dtype=float32), 0.7512876]. 
=============================================
[2019-04-04 05:08:58,895] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5571126e-08 3.3084726e-09 9.6381359e-23 5.8092757e-09 1.0812694e-09
 3.1393564e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:08:58,929] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0159
[2019-04-04 05:08:58,937] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.8, 40.66666666666667, 20.0, 135.8333333333333, 26.0, 25.09882159339946, 0.3323898592240054, 0.0, 1.0, 54992.69195852872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4210800.0000, 
sim time next is 4211400.0000, 
raw observation next is [1.75, 40.83333333333334, 16.0, 108.6666666666667, 26.0, 25.03960619655434, 0.3311471928059138, 0.0, 1.0, 66626.10720142954], 
processed observation next is [0.0, 0.7391304347826086, 0.5110803324099724, 0.40833333333333344, 0.05333333333333334, 0.12007366482504608, 0.6666666666666666, 0.5866338497128618, 0.6103823976019712, 0.0, 1.0, 0.31726717714966446], 
reward next is 0.6827, 
noisyNet noise sample is [array([0.90545267], dtype=float32), -2.3875606]. 
=============================================
[2019-04-04 05:08:58,942] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.07713657e-10 7.18094140e-10 1.26464195e-26 2.08909556e-10
 1.18525828e-10 4.61227566e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 05:08:58,942] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0571
[2019-04-04 05:08:59,000] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 196.0, 6.0, 26.0, 25.37471708281522, 0.4716528484452598, 1.0, 1.0, 117226.580553847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4455000.0000, 
sim time next is 4455600.0000, 
raw observation next is [0.0, 92.0, 177.5, 5.0, 26.0, 24.64733831566519, 0.4482719494407192, 1.0, 1.0, 196265.7523955822], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.5916666666666667, 0.0055248618784530384, 0.6666666666666666, 0.5539448596387659, 0.6494239831469064, 1.0, 1.0, 0.9345988209313438], 
reward next is 0.0654, 
noisyNet noise sample is [array([0.02521113], dtype=float32), 0.85324067]. 
=============================================
[2019-04-04 05:09:05,002] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6086306e-08 1.1847462e-08 5.1687287e-24 3.5164220e-08 4.8227040e-09
 2.4911538e-12 9.9999988e-01], sum to 1.0000
[2019-04-04 05:09:05,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9813
[2019-04-04 05:09:05,015] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.9, 70.0, 0.0, 0.0, 26.0, 25.46670229495601, 0.39517577106875, 0.0, 1.0, 75409.34404577997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4334400.0000, 
sim time next is 4335000.0000, 
raw observation next is [3.85, 69.83333333333334, 0.0, 0.0, 26.0, 25.57309112533397, 0.4004990287490158, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.569252077562327, 0.6983333333333335, 0.0, 0.0, 0.6666666666666666, 0.6310909271111642, 0.633499676249672, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28373224], dtype=float32), -2.0115957]. 
=============================================
[2019-04-04 05:09:05,033] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[90.95661]
 [90.88952]
 [91.06803]
 [91.27297]
 [91.45082]], R is [[90.89794159]
 [90.62986755]
 [90.59676361]
 [90.6907959 ]
 [90.78388977]].
[2019-04-04 05:09:06,352] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4435849e-09 1.8713580e-09 3.7247676e-26 1.2331726e-09 1.3169681e-10
 5.5382260e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:06,352] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7203
[2019-04-04 05:09:06,361] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.2, 84.66666666666667, 157.5, 64.5, 26.0, 26.1465472377193, 0.5962611975204929, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4440000.0000, 
sim time next is 4440600.0000, 
raw observation next is [1.15, 85.0, 165.0, 31.0, 26.0, 26.22322105984174, 0.609207043837186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49445983379501385, 0.85, 0.55, 0.03425414364640884, 0.6666666666666666, 0.6852684216534785, 0.7030690146123953, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4530904], dtype=float32), -1.3926041]. 
=============================================
[2019-04-04 05:09:07,715] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3685761e-09 9.2560564e-09 4.7128649e-24 4.2708135e-09 1.0225953e-09
 7.6793519e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:07,716] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5914
[2019-04-04 05:09:07,724] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.5, 54.0, 0.0, 0.0, 26.0, 27.25896958420482, 0.9146809292397232, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4390200.0000, 
sim time next is 4390800.0000, 
raw observation next is [11.33333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 27.21251495215427, 0.901935444194066, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7765466297322253, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.7677095793461891, 0.8006451480646887, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9460378], dtype=float32), -1.8233342]. 
=============================================
[2019-04-04 05:09:10,397] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6659909e-08 2.3495343e-08 9.6829418e-24 1.3480639e-08 6.2150987e-09
 3.9853953e-12 9.9999988e-01], sum to 1.0000
[2019-04-04 05:09:10,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4730
[2019-04-04 05:09:10,410] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.37650464361551, 0.4848888851227837, 0.0, 1.0, 41330.95748128178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4490400.0000, 
sim time next is 4491000.0000, 
raw observation next is [-0.4, 72.5, 0.0, 0.0, 26.0, 25.38298865725593, 0.4812991378037139, 0.0, 1.0, 38861.59939188824], 
processed observation next is [1.0, 1.0, 0.45152354570637127, 0.725, 0.0, 0.0, 0.6666666666666666, 0.6152490547713274, 0.6604330459345713, 0.0, 1.0, 0.1850552351994678], 
reward next is 0.8149, 
noisyNet noise sample is [array([-0.9540615], dtype=float32), 0.31065786]. 
=============================================
[2019-04-04 05:09:10,430] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.86792]
 [82.72053]
 [82.81857]
 [82.70681]
 [82.48813]], R is [[82.79213715]
 [82.76740265]
 [82.72634125]
 [82.67372894]
 [82.58735657]].
[2019-04-04 05:09:12,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6211048e-08 3.3880163e-09 4.2619437e-24 2.6358835e-09 3.2523622e-09
 2.0187259e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:12,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7486
[2019-04-04 05:09:12,479] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 73.0, 55.5, 33.0, 26.0, 25.48395545531906, 0.4158325261043272, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4521600.0000, 
sim time next is 4522200.0000, 
raw observation next is [-0.6666666666666667, 72.83333333333334, 74.00000000000001, 44.00000000000001, 26.0, 25.43635470320596, 0.4016113098614502, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.44413665743305636, 0.7283333333333334, 0.2466666666666667, 0.04861878453038675, 0.6666666666666666, 0.6196962252671634, 0.6338704366204834, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0892742], dtype=float32), -0.6935612]. 
=============================================
[2019-04-04 05:09:16,877] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6911793e-09 6.2413619e-10 1.1977644e-25 8.2457191e-10 1.6358481e-10
 6.5497320e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:16,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8954
[2019-04-04 05:09:16,919] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 106.3333333333333, 0.0, 26.0, 26.20419663498959, 0.5485198732406681, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4699200.0000, 
sim time next is 4699800.0000, 
raw observation next is [0.0, 92.0, 115.0, 0.0, 26.0, 26.29209212418917, 0.5613067677616389, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.38333333333333336, 0.0, 0.6666666666666666, 0.6910076770157643, 0.6871022559205463, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.668464], dtype=float32), 1.054927]. 
=============================================
[2019-04-04 05:09:17,260] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3773980e-08 6.6717467e-09 2.6925234e-22 6.2486309e-09 2.0703854e-09
 3.1348448e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:17,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3652
[2019-04-04 05:09:17,272] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 26.3467061363079, 0.6799022000908447, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4647000.0000, 
sim time next is 4647600.0000, 
raw observation next is [3.0, 53.0, 0.0, 0.0, 26.0, 26.25580065605363, 0.65665730482391, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.53, 0.0, 0.0, 0.6666666666666666, 0.6879833880044691, 0.7188857682746367, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1418262], dtype=float32), -1.1871241]. 
=============================================
[2019-04-04 05:09:19,880] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0835595e-08 1.1753220e-09 1.0625944e-25 4.1226551e-09 8.8530833e-10
 3.4271112e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:19,881] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0982
[2019-04-04 05:09:19,948] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 58.66666666666667, 156.5, 678.5, 26.0, 25.47514025014097, 0.4567302973978353, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4789200.0000, 
sim time next is 4789800.0000, 
raw observation next is [-2.5, 55.5, 153.0, 730.0, 26.0, 25.42339251457328, 0.4520133844847844, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.39335180055401664, 0.555, 0.51, 0.8066298342541437, 0.6666666666666666, 0.6186160428811066, 0.6506711281615948, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3718698], dtype=float32), 0.42879966]. 
=============================================
[2019-04-04 05:09:22,867] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.1388204e-09 1.7150361e-08 3.1198522e-25 4.3603090e-09 9.0153229e-10
 3.6672284e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:22,868] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8009
[2019-04-04 05:09:22,888] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 82.66666666666667, 0.0, 0.0, 26.0, 25.09892499616224, 0.5020584749739483, 0.0, 1.0, 73961.06653468902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4740000.0000, 
sim time next is 4740600.0000, 
raw observation next is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 26.0, 25.27120863189263, 0.5238336377287739, 0.0, 1.0, 52555.47415258313], 
processed observation next is [1.0, 0.8695652173913043, 0.41181902123730385, 0.8383333333333334, 0.0, 0.0, 0.6666666666666666, 0.6059340526577192, 0.674611212576258, 0.0, 1.0, 0.2502641626313482], 
reward next is 0.7497, 
noisyNet noise sample is [array([-1.5202549], dtype=float32), 0.16484469]. 
=============================================
[2019-04-04 05:09:29,379] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.56422275e-07 1.15522006e-07 7.90449280e-22 1.72031605e-07
 7.28549949e-08 2.46691084e-10 9.99998689e-01], sum to 1.0000
[2019-04-04 05:09:29,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6635
[2019-04-04 05:09:29,434] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 46.66666666666667, 30.99999999999999, 186.6666666666666, 26.0, 25.28512126239918, 0.2732474448171678, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4953000.0000, 
sim time next is 4953600.0000, 
raw observation next is [-2.0, 46.0, 46.5, 280.0, 26.0, 25.2275740182149, 0.2751191824562587, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.46, 0.155, 0.30939226519337015, 0.6666666666666666, 0.6022978348512416, 0.5917063941520863, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0422941], dtype=float32), -1.2618555]. 
=============================================
[2019-04-04 05:09:37,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:37,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:37,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run3
[2019-04-04 05:09:37,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:37,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:37,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run3
[2019-04-04 05:09:40,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:40,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:40,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run3
[2019-04-04 05:09:42,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:42,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:42,485] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run3
[2019-04-04 05:09:43,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:43,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:43,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run3
[2019-04-04 05:09:43,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:43,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:43,932] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run3
[2019-04-04 05:09:44,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:44,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:44,130] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run3
[2019-04-04 05:09:44,151] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:44,151] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:44,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run3
[2019-04-04 05:09:44,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:44,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:44,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run3
[2019-04-04 05:09:44,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:44,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:44,763] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run3
[2019-04-04 05:09:44,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:44,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:44,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run3
[2019-04-04 05:09:45,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:45,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:45,759] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run3
[2019-04-04 05:09:46,146] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:46,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:46,150] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run3
[2019-04-04 05:09:47,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:47,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:47,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run3
[2019-04-04 05:09:47,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:47,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:47,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run3
[2019-04-04 05:09:48,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:09:48,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:09:48,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run3
[2019-04-04 05:09:59,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.5238429e-09 1.1059576e-08 4.4944920e-24 4.9883466e-09 3.0628908e-09
 4.7128642e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:09:59,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3002
[2019-04-04 05:09:59,385] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 63.16666666666667, 42.33333333333334, 2.999999999999999, 26.0, 25.31498845580347, 0.2533325595360833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 119400.0000, 
sim time next is 120000.0000, 
raw observation next is [-7.8, 65.33333333333334, 43.66666666666666, 1.5, 26.0, 25.31741570690654, 0.2552981293621413, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24653739612188366, 0.6533333333333334, 0.14555555555555552, 0.0016574585635359116, 0.6666666666666666, 0.6097846422422117, 0.5850993764540471, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0440257], dtype=float32), 0.5452878]. 
=============================================
[2019-04-04 05:09:59,402] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.48352]
 [84.56266]
 [84.7739 ]
 [84.9637 ]
 [84.94137]], R is [[84.49195862]
 [84.64704132]
 [84.80057526]
 [84.95256805]
 [85.1030426 ]].
[2019-04-04 05:10:14,574] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0405454e-08 9.9493143e-09 3.5393071e-23 6.5365415e-09 1.7500793e-09
 4.4653391e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:10:14,574] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2848
[2019-04-04 05:10:14,648] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.95, 63.5, 149.0, 0.0, 26.0, 24.38140046515079, 0.2867363518438434, 1.0, 1.0, 200978.3746996869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 221400.0000, 
sim time next is 222000.0000, 
raw observation next is [-3.766666666666667, 63.0, 143.6666666666667, 0.0, 26.0, 25.30132369799749, 0.3540707847525419, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.358264081255771, 0.63, 0.47888888888888903, 0.0, 0.6666666666666666, 0.6084436414997908, 0.618023594917514, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09475794], dtype=float32), -0.45689625]. 
=============================================
[2019-04-04 05:10:14,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.0876  ]
 [81.202805]
 [80.5733  ]
 [80.77031 ]
 [80.87349 ]], R is [[82.21963501]
 [81.44039917]
 [80.6758194 ]
 [80.77991486]
 [80.80033112]].
[2019-04-04 05:10:28,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8631674e-08 1.6461065e-08 2.4251829e-21 1.3288284e-08 8.8681125e-09
 2.3861576e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:10:28,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4635
[2019-04-04 05:10:28,558] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 75.83333333333334, 0.0, 0.0, 26.0, 24.87957393386549, 0.2348964661590384, 0.0, 1.0, 44520.35259172615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 334200.0000, 
sim time next is 334800.0000, 
raw observation next is [-12.8, 77.0, 0.0, 0.0, 26.0, 24.73933886952356, 0.2088593246967304, 0.0, 1.0, 46654.66775230417], 
processed observation next is [1.0, 0.9130434782608695, 0.1080332409972299, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5616115724602967, 0.5696197748989101, 0.0, 1.0, 0.2221650845347818], 
reward next is 0.7778, 
noisyNet noise sample is [array([0.35051027], dtype=float32), 0.24855824]. 
=============================================
[2019-04-04 05:10:28,799] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.0502602e-08 4.4775518e-08 4.0031814e-21 9.7131485e-09 1.9887027e-08
 4.7015648e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:10:28,805] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6536
[2019-04-04 05:10:28,867] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.72635098491163, -0.009128637589630348, 0.0, 1.0, 47184.94092371457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.65919300768117, -0.01447291577866559, 0.0, 1.0, 47224.31904413381], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.4715994173067643, 0.4951756947404448, 0.0, 1.0, 0.2248777097339705], 
reward next is 0.7751, 
noisyNet noise sample is [array([-0.32446977], dtype=float32), -1.8351305]. 
=============================================
[2019-04-04 05:10:29,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5499286e-07 1.2413609e-07 5.3009679e-20 4.0155875e-08 5.1788248e-08
 1.2595723e-10 9.9999952e-01], sum to 1.0000
[2019-04-04 05:10:29,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0577
[2019-04-04 05:10:29,434] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.15075530735789, -0.3704022797251074, 0.0, 1.0, 49233.1877315402], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 362400.0000, 
sim time next is 363000.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.11768814488953, -0.3597372947397015, 0.0, 1.0, 49481.32121765371], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.34314067874079424, 0.3800875684200995, 0.0, 1.0, 0.23562533913168435], 
reward next is 0.7644, 
noisyNet noise sample is [array([-0.91586834], dtype=float32), -0.08233394]. 
=============================================
[2019-04-04 05:10:29,445] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.629654]
 [68.75215 ]
 [68.86802 ]
 [68.9861  ]
 [69.10476 ]], R is [[68.54983521]
 [68.62989044]
 [68.70863342]
 [68.78632355]
 [68.86316681]].
[2019-04-04 05:10:32,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.6503595e-07 2.8169850e-07 2.2723963e-20 1.1803695e-07 7.4137908e-08
 2.8095357e-10 9.9999857e-01], sum to 1.0000
[2019-04-04 05:10:32,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1007
[2019-04-04 05:10:32,072] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.31837056696897, -0.3441635343371313, 0.0, 1.0, 49411.31393660411], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 360600.0000, 
sim time next is 361200.0000, 
raw observation next is [-15.6, 73.0, 0.0, 0.0, 26.0, 22.22853864503994, -0.3597676573508699, 0.0, 1.0, 49396.53194184722], 
processed observation next is [1.0, 0.17391304347826086, 0.030470914127423816, 0.73, 0.0, 0.0, 0.6666666666666666, 0.352378220419995, 0.38007744754971, 0.0, 1.0, 0.23522158067546295], 
reward next is 0.7648, 
noisyNet noise sample is [array([-1.1269888], dtype=float32), -1.0267112]. 
=============================================
[2019-04-04 05:10:32,493] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8834978e-08 1.6691202e-07 1.0468374e-20 6.4409384e-08 2.1977268e-08
 1.7737158e-10 9.9999976e-01], sum to 1.0000
[2019-04-04 05:10:32,503] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4006
[2019-04-04 05:10:32,568] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.283333333333333, 27.5, 125.6666666666667, 0.0, 26.0, 25.01174556734457, 0.1487594256048127, 1.0, 1.0, 90513.23132639893], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 478200.0000, 
sim time next is 478800.0000, 
raw observation next is [-1.2, 28.0, 124.0, 0.0, 26.0, 24.98723743956625, 0.1569609369856965, 1.0, 1.0, 73047.47583006487], 
processed observation next is [1.0, 0.5652173913043478, 0.42936288088642666, 0.28, 0.41333333333333333, 0.0, 0.6666666666666666, 0.5822697866305209, 0.5523203123285655, 1.0, 1.0, 0.3478451230003089], 
reward next is 0.6522, 
noisyNet noise sample is [array([-0.4856311], dtype=float32), 0.5957204]. 
=============================================
[2019-04-04 05:10:52,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8390290e-07 2.0625501e-08 5.3908924e-23 1.1532140e-08 1.4333706e-08
 9.1347607e-12 9.9999976e-01], sum to 1.0000
[2019-04-04 05:10:52,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-04 05:10:52,433] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.283333333333334, 64.33333333333334, 92.66666666666667, 25.33333333333334, 26.0, 24.88751849114681, 0.2052409245765505, 0.0, 1.0, 35950.46793970876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 26.0, 24.86615190673492, 0.2046242402807458, 0.0, 1.0, 52295.36203770612], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.6366666666666667, 0.3027777777777778, 0.03499079189686925, 0.6666666666666666, 0.5721793255612434, 0.5682080800935819, 0.0, 1.0, 0.2490255335128863], 
reward next is 0.7510, 
noisyNet noise sample is [array([0.79730755], dtype=float32), -0.2835193]. 
=============================================
[2019-04-04 05:11:10,757] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4673840e-09 6.3176675e-10 5.0873874e-26 6.6791885e-11 1.1379466e-10
 7.8503556e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:11:10,757] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6058
[2019-04-04 05:11:10,780] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 84.0, 72.16666666666667, 0.0, 26.0, 25.5139135395823, 0.3029987277742555, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 901200.0000, 
sim time next is 901800.0000, 
raw observation next is [1.1, 84.0, 77.0, 0.0, 26.0, 25.53266843565831, 0.2942946787044691, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.25666666666666665, 0.0, 0.6666666666666666, 0.6277223696381924, 0.598098226234823, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21224017], dtype=float32), -1.6252401]. 
=============================================
[2019-04-04 05:11:11,090] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8630532e-09 2.8343052e-09 2.9615820e-23 4.0964050e-09 2.3568361e-09
 4.0245602e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:11:11,091] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9140
[2019-04-04 05:11:11,154] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 75.0, 21.33333333333334, 0.0, 26.0, 25.41330475271119, 0.2662927514863024, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 807000.0000, 
sim time next is 807600.0000, 
raw observation next is [-6.533333333333334, 75.0, 26.66666666666667, 0.0, 26.0, 25.41764477140353, 0.2933917509010219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.28162511542012925, 0.75, 0.0888888888888889, 0.0, 0.6666666666666666, 0.6181370642836276, 0.5977972503003407, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.486944], dtype=float32), 0.051367406]. 
=============================================
[2019-04-04 05:11:15,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3833555e-08 1.6877193e-08 8.2569248e-25 1.6219737e-09 5.9210375e-10
 2.1173707e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:11:15,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5419
[2019-04-04 05:11:15,780] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 79.66666666666667, 0.0, 0.0, 26.0, 24.790060548924, 0.248015909947488, 0.0, 1.0, 41164.91678702298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 859800.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 0.0, 0.0, 26.0, 24.79141964176652, 0.241999522460024, 0.0, 1.0, 41034.75110482295], 
processed observation next is [1.0, 1.0, 0.38504155124653744, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5659516368138767, 0.5806665074866747, 0.0, 1.0, 0.1954035766896331], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.43076304], dtype=float32), 1.2783129]. 
=============================================
[2019-04-04 05:11:17,820] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5785165e-08 2.6692701e-09 8.1508659e-25 3.7005534e-09 8.2297600e-09
 1.5976383e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:11:17,831] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1818
[2019-04-04 05:11:17,919] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.99289755714832, 0.2482116820843802, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.13529510005398, 0.2494024886136979, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5946079250044983, 0.5831341628712327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44867808], dtype=float32), -1.8273083]. 
=============================================
[2019-04-04 05:11:19,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6066278e-09 2.2205173e-09 4.9569879e-25 6.6775868e-10 9.8434361e-10
 5.0228403e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:11:19,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3149
[2019-04-04 05:11:19,066] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 83.33333333333334, 0.0, 0.0, 26.0, 25.38380331026597, 0.2952114469861034, 1.0, 1.0, 18709.9008475651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 841200.0000, 
sim time next is 841800.0000, 
raw observation next is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.1653425307126, 0.2882110956705842, 1.0, 1.0, 55203.95948523111], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5971118775593833, 0.5960703652235281, 1.0, 1.0, 0.2628759975487196], 
reward next is 0.7371, 
noisyNet noise sample is [array([1.9969033], dtype=float32), -1.33814]. 
=============================================
[2019-04-04 05:11:20,509] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.2395340e-09 1.9245403e-09 1.2920586e-23 1.6960442e-09 6.1969887e-09
 5.1925751e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:11:20,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7055
[2019-04-04 05:11:20,570] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1833333333333333, 73.33333333333333, 19.33333333333334, 0.0, 26.0, 24.92862839406163, 0.2199285592904818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 893400.0000, 
sim time next is 894000.0000, 
raw observation next is [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 26.0, 24.90941937703329, 0.2508761708785909, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4727608494921515, 0.7466666666666667, 0.08055555555555557, 0.0, 0.6666666666666666, 0.5757849480861076, 0.5836253902928636, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3491359], dtype=float32), -1.4676509]. 
=============================================
[2019-04-04 05:11:20,581] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.79795 ]
 [86.49668 ]
 [86.22818 ]
 [86.01184 ]
 [85.756035]], R is [[87.18719482]
 [87.31532288]
 [87.44216919]
 [87.56774902]
 [87.69207001]].
[2019-04-04 05:11:26,929] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.1847772e-10 2.6101868e-10 3.9461932e-26 1.3529647e-10 2.0333972e-10
 2.2279145e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:11:26,932] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3086
[2019-04-04 05:11:26,940] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.9, 86.0, 120.0, 0.0, 26.0, 26.67963870611066, 0.6810713779217078, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 991800.0000, 
sim time next is 992400.0000, 
raw observation next is [12.0, 86.0, 121.3333333333333, 0.0, 26.0, 26.67331479946529, 0.6842747717141441, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7950138504155125, 0.86, 0.40444444444444433, 0.0, 0.6666666666666666, 0.7227762332887743, 0.7280915905713813, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71169865], dtype=float32), -0.20549324]. 
=============================================
[2019-04-04 05:11:26,955] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 05:11:26,956] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:11:26,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:11:26,959] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:11:26,959] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:11:26,960] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:11:26,961] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:11:26,962] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run4
[2019-04-04 05:11:26,980] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run4
[2019-04-04 05:11:26,998] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run4
[2019-04-04 05:11:43,158] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23081353], dtype=float32), 0.16111995]
[2019-04-04 05:11:43,158] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-6.1, 81.0, 0.0, 0.0, 26.0, 25.00322050170512, 0.3117213115834161, 1.0, 1.0, 150884.1943472258]
[2019-04-04 05:11:43,158] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:11:43,159] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.1079508e-09 8.7359053e-09 7.7932698e-23 3.7222645e-09 1.9820181e-09
 8.4525945e-12 1.0000000e+00], sampled 0.11341990562952431
[2019-04-04 05:11:57,552] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23081353], dtype=float32), 0.16111995]
[2019-04-04 05:11:57,553] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.866666666666667, 77.0, 173.6666666666667, 420.6666666666667, 26.0, 24.83180849085385, 0.2045024194011261, 1.0, 1.0, 80257.02923206164]
[2019-04-04 05:11:57,553] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:11:57,553] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.8994998e-09 2.3349249e-09 3.9282342e-24 1.1080622e-09 5.7050120e-10
 1.5476101e-12 1.0000000e+00], sampled 0.8009032061403434
[2019-04-04 05:12:19,350] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23081353], dtype=float32), 0.16111995]
[2019-04-04 05:12:19,350] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.45, 55.0, 0.0, 0.0, 26.0, 24.96585430268768, 0.2904382390870479, 0.0, 1.0, 27051.04248208749]
[2019-04-04 05:12:19,350] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:12:19,351] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.1147853e-07 4.0873687e-08 3.1777531e-21 2.5439483e-08 2.2356300e-08
 5.9091759e-11 9.9999988e-01], sampled 0.3321368047060258
[2019-04-04 05:13:15,468] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.23081353], dtype=float32), 0.16111995]
[2019-04-04 05:13:15,469] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.032712614666667, 99.45049041333334, 0.0, 0.0, 26.0, 24.27981504205226, 0.1117264422786024, 0.0, 1.0, 57451.94000965426]
[2019-04-04 05:13:15,469] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:13:15,469] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.2737923e-08 8.2868272e-09 3.6370164e-24 4.2377395e-09 4.0891215e-09
 2.9087345e-12 1.0000000e+00], sampled 0.724427323517701
[2019-04-04 05:13:42,798] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 05:13:43,256] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.23081353], dtype=float32), 0.16111995]
[2019-04-04 05:13:43,256] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.732528631, 47.97026672, 139.3485888, 803.5595573, 26.0, 26.27164975122683, 0.5959360619537061, 1.0, 1.0, 0.0]
[2019-04-04 05:13:43,256] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:13:43,257] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.4838315e-09 2.3577891e-09 3.7600923e-23 1.5414408e-09 1.0429054e-09
 2.0761914e-12 1.0000000e+00], sampled 0.7825522196479018
[2019-04-04 05:13:44,554] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.23081353], dtype=float32), 0.16111995]
[2019-04-04 05:13:44,554] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 30.0, 0.0, 0.0, 26.0, 25.30295362725307, 0.4869661163550605, 0.0, 1.0, 169364.3613647529]
[2019-04-04 05:13:44,554] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:13:44,555] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.26152671e-07 1.62967766e-07 1.24776795e-20 5.34221059e-08
 5.13315577e-08 2.06591827e-10 9.99999642e-01], sampled 0.870631742610577
[2019-04-04 05:14:02,806] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5698 263430344.9894 1551.9755
[2019-04-04 05:14:05,455] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 05:14:06,478] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 300000, evaluation results [300000.0, 7241.569785764876, 263430344.989374, 1551.9755349129598, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 05:14:07,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5618568e-09 1.4676331e-09 2.8485546e-27 2.2553809e-10 2.6549551e-10
 4.7602374e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:07,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7479
[2019-04-04 05:14:07,117] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.93643346312462, 0.6450209666233139, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1048800.0000, 
sim time next is 1049400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 26.02342135515887, 0.6374973190849568, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6686184462632392, 0.7124991063616523, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31835437], dtype=float32), 0.09484564]. 
=============================================
[2019-04-04 05:14:07,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8249136e-10 3.2279368e-10 5.8121898e-28 3.7840810e-11 1.4129704e-10
 3.5694649e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:07,304] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1698
[2019-04-04 05:14:07,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.0, 77.66666666666667, 0.0, 0.0, 26.0, 25.83368400141918, 0.5897466130841934, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1053600.0000, 
sim time next is 1054200.0000, 
raw observation next is [13.9, 77.83333333333333, 0.0, 0.0, 26.0, 25.76176386288858, 0.5770837115549844, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.847645429362881, 0.7783333333333333, 0.0, 0.0, 0.6666666666666666, 0.6468136552407149, 0.6923612371849948, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6333282], dtype=float32), 1.7590837]. 
=============================================
[2019-04-04 05:14:12,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3881817e-09 1.3087578e-09 5.7346698e-27 1.6644289e-10 1.1973418e-10
 2.0121581e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:12,743] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1480
[2019-04-04 05:14:12,756] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.36666666666667, 78.33333333333334, 0.0, 0.0, 26.0, 25.63120361410396, 0.630170805180906, 0.0, 1.0, 31093.252120047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1131600.0000, 
sim time next is 1132200.0000, 
raw observation next is [10.55, 78.0, 0.0, 0.0, 26.0, 25.63013045765854, 0.6302560692067598, 0.0, 1.0, 27857.27763606772], 
processed observation next is [0.0, 0.08695652173913043, 0.754847645429363, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6358442048048785, 0.7100853564022532, 0.0, 1.0, 0.1326537030288939], 
reward next is 0.8673, 
noisyNet noise sample is [array([1.8987067], dtype=float32), -0.765467]. 
=============================================
[2019-04-04 05:14:13,592] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.944550e-09 9.110308e-10 2.865741e-28 6.965691e-10 7.169592e-11
 9.312109e-15 1.000000e+00], sum to 1.0000
[2019-04-04 05:14:13,597] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1607
[2019-04-04 05:14:13,607] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.78333333333333, 82.5, 0.0, 0.0, 26.0, 25.62688635293034, 0.6083839280570256, 0.0, 1.0, 37901.8225247876], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1145400.0000, 
sim time next is 1146000.0000, 
raw observation next is [11.96666666666667, 82.0, 0.0, 0.0, 26.0, 25.6230904345656, 0.6105089025052972, 0.0, 1.0, 32283.6665585505], 
processed observation next is [0.0, 0.2608695652173913, 0.7940904893813484, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6352575362138001, 0.7035029675017658, 0.0, 1.0, 0.15373174551690713], 
reward next is 0.8463, 
noisyNet noise sample is [array([0.63946104], dtype=float32), -0.72074026]. 
=============================================
[2019-04-04 05:14:13,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[95.32665]
 [95.16064]
 [94.97538]
 [94.90058]
 [94.85113]], R is [[95.35906982]
 [95.22499084]
 [95.09086609]
 [94.99136353]
 [94.95230103]].
[2019-04-04 05:14:15,347] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5135460e-09 1.0027114e-09 4.7176293e-25 1.9718777e-10 5.6999544e-10
 3.3035789e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:15,347] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1773
[2019-04-04 05:14:15,384] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 100.0, 86.5, 0.0, 26.0, 24.70938401756385, 0.4496120801522759, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1249200.0000, 
sim time next is 1249800.0000, 
raw observation next is [14.4, 99.33333333333334, 89.33333333333334, 0.0, 26.0, 24.89117898571879, 0.4656925621488011, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9933333333333334, 0.2977777777777778, 0.0, 0.6666666666666666, 0.5742649154765657, 0.6552308540496004, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13821554], dtype=float32), 0.056456834]. 
=============================================
[2019-04-04 05:14:17,531] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9966595e-10 9.2687446e-12 4.5970290e-29 7.7724015e-11 3.2506595e-11
 8.9420201e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:17,535] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6745
[2019-04-04 05:14:17,551] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.53018349530058, 0.595128399878519, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1285200.0000, 
sim time next is 1285800.0000, 
raw observation next is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.50893181133063, 0.5888945490678317, 0.0, 1.0, 25460.42596165899], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6257443176108858, 0.6962981830226106, 0.0, 1.0, 0.12124012362694758], 
reward next is 0.8788, 
noisyNet noise sample is [array([1.1391507], dtype=float32), -0.24630572]. 
=============================================
[2019-04-04 05:14:21,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1412533e-10 4.0800199e-10 1.2111716e-25 1.8502148e-10 3.2476247e-10
 7.5541255e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:21,342] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9168
[2019-04-04 05:14:21,369] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 100.1666666666667, 0.0, 26.0, 25.69319787048423, 0.5343136338309148, 1.0, 1.0, 26896.59772116946], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1345200.0000, 
sim time next is 1345800.0000, 
raw observation next is [1.1, 92.0, 94.33333333333333, 0.0, 26.0, 25.73526231480488, 0.5418785984617771, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.3144444444444444, 0.0, 0.6666666666666666, 0.6446051929004067, 0.680626199487259, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8386605], dtype=float32), -0.34277657]. 
=============================================
[2019-04-04 05:14:23,544] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0927912e-09 2.0568689e-09 4.5393408e-25 3.4143266e-10 7.2378281e-10
 1.2017626e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:23,546] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8377
[2019-04-04 05:14:23,582] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 9.0, 0.0, 26.0, 25.72756441364302, 0.5471051786577963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1324800.0000, 
sim time next is 1325400.0000, 
raw observation next is [1.0, 92.0, 12.0, 0.0, 26.0, 25.62581038026366, 0.5300071250553089, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4903047091412743, 0.92, 0.04, 0.0, 0.6666666666666666, 0.6354841983553049, 0.6766690416851029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38300383], dtype=float32), -0.14148112]. 
=============================================
[2019-04-04 05:14:27,117] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7584509e-09 5.6479971e-10 1.5889303e-25 1.6138538e-10 3.5892911e-10
 1.0940483e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:27,120] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3253
[2019-04-04 05:14:27,139] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.29222782269044, 0.4808230706705741, 0.0, 1.0, 39517.56514039978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1392600.0000, 
sim time next is 1393200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.34629919438041, 0.4877591195937814, 0.0, 1.0, 39476.92854078091], 
processed observation next is [1.0, 0.13043478260869565, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6121915995317009, 0.6625863731979271, 0.0, 1.0, 0.18798537400371862], 
reward next is 0.8120, 
noisyNet noise sample is [array([1.1198725], dtype=float32), -0.063167825]. 
=============================================
[2019-04-04 05:14:28,761] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5470552e-09 3.6113774e-09 5.4359271e-25 1.6068209e-09 1.2422898e-09
 5.1323415e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:28,761] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2482
[2019-04-04 05:14:28,791] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 84.83333333333334, 0.0, 0.0, 26.0, 25.7016986789647, 0.5338645116602434, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1565400.0000, 
sim time next is 1566000.0000, 
raw observation next is [4.4, 86.0, 0.0, 0.0, 26.0, 25.60156706412787, 0.5257040620735105, 0.0, 1.0, 18736.71446041186], 
processed observation next is [1.0, 0.13043478260869565, 0.5844875346260389, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6334639220106558, 0.6752346873578369, 0.0, 1.0, 0.08922244981148504], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.43791038], dtype=float32), -0.28419518]. 
=============================================
[2019-04-04 05:14:28,816] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.71945]
 [86.72764]
 [86.88659]
 [87.00927]
 [87.05073]], R is [[86.68110657]
 [86.81429291]
 [86.94615173]
 [87.03205109]
 [87.05845642]].
[2019-04-04 05:14:33,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9815639e-10 8.4771923e-10 3.5792329e-26 3.9982978e-10 8.4366049e-11
 7.5196246e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:33,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4362
[2019-04-04 05:14:33,355] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 100.0, 27.66666666666666, 0.0, 26.0, 25.76735501755446, 0.4889379364010651, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1500600.0000, 
sim time next is 1501200.0000, 
raw observation next is [1.6, 100.0, 32.5, 0.0, 26.0, 25.82561485644807, 0.4933226006851905, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5069252077562327, 1.0, 0.10833333333333334, 0.0, 0.6666666666666666, 0.6521345713706724, 0.6644408668950635, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48875698], dtype=float32), -0.32733735]. 
=============================================
[2019-04-04 05:14:49,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7482991e-08 5.0084719e-09 2.2258964e-22 5.6144569e-09 6.1572134e-09
 1.2183674e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:49,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8346
[2019-04-04 05:14:49,113] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.483333333333333, 86.16666666666667, 40.66666666666666, 0.0, 26.0, 24.97855466178236, 0.3304793867083899, 0.0, 1.0, 64707.09898018058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1786200.0000, 
sim time next is 1786800.0000, 
raw observation next is [-3.566666666666667, 85.33333333333334, 34.33333333333334, 0.0, 26.0, 24.97640432872877, 0.3331087779962963, 0.0, 1.0, 56937.79337887621], 
processed observation next is [0.0, 0.6956521739130435, 0.3638042474607572, 0.8533333333333334, 0.11444444444444447, 0.0, 0.6666666666666666, 0.5813670273940641, 0.6110362593320987, 0.0, 1.0, 0.27113234942322006], 
reward next is 0.7289, 
noisyNet noise sample is [array([1.4789402], dtype=float32), 0.9550981]. 
=============================================
[2019-04-04 05:14:51,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8146281e-07 7.2540452e-08 4.7962456e-21 1.5957591e-08 3.6600099e-08
 4.7151252e-11 9.9999976e-01], sum to 1.0000
[2019-04-04 05:14:51,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4734
[2019-04-04 05:14:51,535] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 23.98693362464066, 0.06872632644386133, 0.0, 1.0, 46740.53216738699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1825200.0000, 
sim time next is 1825800.0000, 
raw observation next is [-6.2, 86.33333333333333, 0.0, 0.0, 26.0, 23.95084767252502, 0.06139918729695543, 0.0, 1.0, 46791.32986269236], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.4959039727104182, 0.5204663957656518, 0.0, 1.0, 0.22281585648901123], 
reward next is 0.7772, 
noisyNet noise sample is [array([-0.00725098], dtype=float32), 0.5976656]. 
=============================================
[2019-04-04 05:14:51,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0725489e-07 4.4239354e-08 1.0687594e-21 2.4601958e-09 2.6176377e-08
 1.2554250e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:14:51,623] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3797
[2019-04-04 05:14:51,650] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 78.33333333333334, 0.0, 0.0, 26.0, 24.40964691438182, 0.1645686077038295, 0.0, 1.0, 45781.04206101874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1816800.0000, 
sim time next is 1817400.0000, 
raw observation next is [-5.5, 78.16666666666666, 0.0, 0.0, 26.0, 24.37724949317457, 0.1595942682828584, 0.0, 1.0, 45854.25793643053], 
processed observation next is [0.0, 0.0, 0.3102493074792244, 0.7816666666666666, 0.0, 0.0, 0.6666666666666666, 0.5314374577645475, 0.5531980894276195, 0.0, 1.0, 0.21835360922109776], 
reward next is 0.7816, 
noisyNet noise sample is [array([1.7734976], dtype=float32), -1.4466991]. 
=============================================
[2019-04-04 05:14:54,637] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1488919e-09 6.3064021e-09 1.0660684e-20 6.4591319e-09 1.7172120e-09
 1.4902289e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:14:54,638] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-04 05:14:54,695] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 62.0, 116.1666666666667, 0.0, 26.0, 25.76057828209153, 0.3445323607463697, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1952400.0000, 
sim time next is 1953000.0000, 
raw observation next is [-3.1, 62.0, 112.0, 0.0, 26.0, 25.76909154140076, 0.342071035507735, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37673130193905824, 0.62, 0.37333333333333335, 0.0, 0.6666666666666666, 0.6474242951167298, 0.6140236785025783, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3838222], dtype=float32), 0.84514225]. 
=============================================
[2019-04-04 05:14:54,707] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.265686]
 [70.45418 ]
 [70.64065 ]
 [70.86716 ]
 [71.11371 ]], R is [[70.43530273]
 [70.7309494 ]
 [71.02364349]
 [71.3134079 ]
 [71.49073029]].
[2019-04-04 05:14:54,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1038716e-07 2.3455179e-08 2.0428710e-21 2.1371383e-08 8.6397787e-09
 2.7910363e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:14:54,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0389
[2019-04-04 05:14:54,969] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 81.0, 0.0, 0.0, 26.0, 23.78586040340056, 0.01795251803094739, 0.0, 1.0, 46956.49055342565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1830600.0000, 
sim time next is 1831200.0000, 
raw observation next is [-6.199999999999999, 80.33333333333334, 0.0, 0.0, 26.0, 23.74841624873595, 0.0105619601704026, 0.0, 1.0, 46988.89903281634], 
processed observation next is [0.0, 0.17391304347826086, 0.2908587257617729, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.4790346873946625, 0.5035206533901342, 0.0, 1.0, 0.2237566620610302], 
reward next is 0.7762, 
noisyNet noise sample is [array([0.32209444], dtype=float32), -0.78824717]. 
=============================================
[2019-04-04 05:14:58,321] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.1036054e-08 2.0069704e-08 4.4150195e-22 8.1809448e-09 1.7492690e-08
 1.1230373e-12 9.9999988e-01], sum to 1.0000
[2019-04-04 05:14:58,326] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7142
[2019-04-04 05:14:58,395] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.966666666666667, 78.0, 143.3333333333333, 86.83333333333334, 26.0, 25.15029260823104, 0.2491430190760422, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1849200.0000, 
sim time next is 1849800.0000, 
raw observation next is [-5.783333333333333, 78.0, 138.6666666666667, 79.66666666666667, 26.0, 25.09314421200662, 0.2335841098157055, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3024007386888274, 0.78, 0.46222222222222237, 0.08802946593001842, 0.6666666666666666, 0.5910953510005518, 0.5778613699385685, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5376344], dtype=float32), -0.71368563]. 
=============================================
[2019-04-04 05:15:00,080] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5668000e-08 1.0699290e-08 8.4085247e-22 9.1230756e-09 1.4564709e-08
 4.2005965e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:15:00,080] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3941
[2019-04-04 05:15:00,098] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.2, 86.5, 0.0, 0.0, 26.0, 23.19586501877458, -0.1795779032600242, 0.0, 1.0, 44627.96567899168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1924200.0000, 
sim time next is 1924800.0000, 
raw observation next is [-9.3, 88.0, 0.0, 0.0, 26.0, 23.14543316773755, -0.1807285873991039, 0.0, 1.0, 44583.86250996306], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.88, 0.0, 0.0, 0.6666666666666666, 0.42878609731146256, 0.43975713753363205, 0.0, 1.0, 0.21230410719030027], 
reward next is 0.7877, 
noisyNet noise sample is [array([-1.2876863], dtype=float32), -0.7606469]. 
=============================================
[2019-04-04 05:15:02,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7545522e-08 2.6255922e-09 1.6128592e-22 4.0886614e-09 2.8685194e-09
 6.3172878e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:15:02,336] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8583
[2019-04-04 05:15:02,388] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.01824461411896, 0.2478341719048429, 0.0, 1.0, 38676.10327370564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1881000.0000, 
sim time next is 1881600.0000, 
raw observation next is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.0339312976027, 0.2448642207214138, 0.0, 1.0, 30835.65940577471], 
processed observation next is [0.0, 0.782608695652174, 0.3333333333333333, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5861609414668916, 0.5816214069071379, 0.0, 1.0, 0.14683647336083194], 
reward next is 0.8532, 
noisyNet noise sample is [array([0.5552577], dtype=float32), 1.14816]. 
=============================================
[2019-04-04 05:15:24,159] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0649663e-10 1.4636160e-09 1.0155150e-23 4.4359127e-10 1.5820717e-10
 1.5739076e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:15:24,160] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0624
[2019-04-04 05:15:24,245] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 68.0, 144.0, 0.0, 26.0, 25.169562067043, 0.2245557250711841, 1.0, 1.0, 54609.20892406352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2208600.0000, 
sim time next is 2209200.0000, 
raw observation next is [-3.733333333333333, 69.0, 140.0, 0.0, 26.0, 25.00787926244521, 0.339117377856549, 1.0, 1.0, 136431.5469567876], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.69, 0.4666666666666667, 0.0, 0.6666666666666666, 0.5839899385371009, 0.613039125952183, 1.0, 1.0, 0.6496740331275601], 
reward next is 0.3503, 
noisyNet noise sample is [array([-0.40633613], dtype=float32), -0.6722005]. 
=============================================
[2019-04-04 05:15:28,755] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.9548173e-08 8.9108589e-08 2.7254253e-22 5.1091442e-09 5.1336331e-09
 2.5626645e-11 9.9999976e-01], sum to 1.0000
[2019-04-04 05:15:28,755] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5284
[2019-04-04 05:15:28,781] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 78.33333333333334, 0.0, 0.0, 26.0, 23.83139489353874, 0.01226114520727534, 0.0, 1.0, 41896.16952555708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2181000.0000, 
sim time next is 2181600.0000, 
raw observation next is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.77279941848266, 0.007508377405894916, 0.0, 1.0, 41901.31589316241], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.79, 0.0, 0.0, 0.6666666666666666, 0.4810666182068883, 0.5025027924686316, 0.0, 1.0, 0.1995300756817258], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.6388966], dtype=float32), 0.27978036]. 
=============================================
[2019-04-04 05:15:55,373] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7778971e-09 1.1559802e-08 2.0377545e-24 1.2129430e-09 1.3463393e-09
 5.7413896e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:15:55,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2830
[2019-04-04 05:15:55,396] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.41089788569676, 0.4135741988862, 0.0, 1.0, 43958.37408230102], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2583600.0000, 
sim time next is 2584200.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.3910486563515, 0.4117030541116891, 0.0, 1.0, 52161.68657750445], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6159207213626251, 0.6372343513705631, 0.0, 1.0, 0.24838898370240214], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.45370993], dtype=float32), -0.74272954]. 
=============================================
[2019-04-04 05:16:05,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6526077e-09 1.9932664e-09 2.6262483e-24 1.3734434e-09 5.9151906e-10
 1.5149226e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:05,460] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8481
[2019-04-04 05:16:05,511] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 64.0, 90.0, 172.5, 26.0, 25.642504071544, 0.3743482541127569, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2794800.0000, 
sim time next is 2795400.0000, 
raw observation next is [-6.0, 64.0, 108.0, 207.0, 26.0, 25.79005677590941, 0.3892165487803085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.36, 0.2287292817679558, 0.6666666666666666, 0.6491713979924508, 0.6297388495934362, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0250924], dtype=float32), -1.448805]. 
=============================================
[2019-04-04 05:16:07,838] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5303013e-08 2.3268255e-08 5.5532864e-22 1.2709138e-08 9.1471373e-09
 2.2604743e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:16:07,838] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2759
[2019-04-04 05:16:07,855] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.08757945947819, -0.1099431370389052, 0.0, 1.0, 43425.96799931899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2699400.0000, 
sim time next is 2700000.0000, 
raw observation next is [-16.0, 83.0, 0.0, 0.0, 26.0, 23.08813308392492, -0.1239352117427715, 0.0, 1.0, 43343.73676134676], 
processed observation next is [1.0, 0.2608695652173913, 0.01939058171745151, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4240110903270766, 0.45868826275240954, 0.0, 1.0, 0.20639874648260362], 
reward next is 0.7936, 
noisyNet noise sample is [array([1.59254], dtype=float32), -0.36988536]. 
=============================================
[2019-04-04 05:16:07,870] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.93081 ]
 [76.22635 ]
 [76.47415 ]
 [76.7088  ]
 [76.936874]], R is [[75.71067047]
 [75.7467804 ]
 [75.78221893]
 [75.81689453]
 [75.85066223]].
[2019-04-04 05:16:13,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1311204e-08 3.7021856e-08 1.2250109e-22 6.4390808e-09 8.7134389e-09
 3.3007288e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:16:13,069] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0279
[2019-04-04 05:16:13,091] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.63972806748424, 0.1953581263165057, 0.0, 1.0, 41150.38282318524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2775000.0000, 
sim time next is 2775600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.58893506862823, 0.1876427689905971, 0.0, 1.0, 41051.58476002089], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.549077922385686, 0.5625475896635324, 0.0, 1.0, 0.19548373695248042], 
reward next is 0.8045, 
noisyNet noise sample is [array([-1.2669567], dtype=float32), -0.9682206]. 
=============================================
[2019-04-04 05:16:26,045] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5494959e-08 6.5203043e-09 2.0469644e-23 7.8525915e-09 2.5287812e-09
 1.4142855e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:26,045] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6291
[2019-04-04 05:16:26,108] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 158.0, 114.0, 26.0, 25.17885219138453, 0.3210316998184884, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2972400.0000, 
sim time next is 2973000.0000, 
raw observation next is [-4.0, 71.0, 162.0, 96.00000000000001, 26.0, 25.08441746640755, 0.2988760204703728, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3518005540166205, 0.71, 0.54, 0.10607734806629836, 0.6666666666666666, 0.5903681222006293, 0.5996253401567909, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6902726], dtype=float32), -0.72690344]. 
=============================================
[2019-04-04 05:16:26,116] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[78.80336 ]
 [79.143684]
 [79.27022 ]
 [79.307106]
 [79.33325 ]], R is [[78.5538559 ]
 [78.76831818]
 [78.9806366 ]
 [79.19083405]
 [79.39892578]].
[2019-04-04 05:16:28,174] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8662225e-09 4.9538431e-09 8.0985533e-24 2.1451991e-09 9.6000563e-10
 3.2780408e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:28,175] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9098
[2019-04-04 05:16:28,206] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.01018516558955, 0.3650983842257174, 0.0, 1.0, 43370.6767026108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2937000.0000, 
sim time next is 2937600.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.98606751460233, 0.355965333439929, 0.0, 1.0, 43347.34238026133], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5821722928835275, 0.618655111146643, 0.0, 1.0, 0.20641591609648255], 
reward next is 0.7936, 
noisyNet noise sample is [array([-2.0308247], dtype=float32), 1.4622748]. 
=============================================
[2019-04-04 05:16:29,662] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.5704252e-09 2.4103211e-09 4.2739797e-22 9.4724628e-10 5.5121681e-09
 6.0436777e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:29,662] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9919
[2019-04-04 05:16:29,694] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 60.83333333333333, 107.0, 783.6666666666666, 26.0, 25.12197136666815, 0.4065574867164665, 0.0, 1.0, 18714.83354681669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2987400.0000, 
sim time next is 2988000.0000, 
raw observation next is [-2.0, 60.0, 105.5, 775.5, 26.0, 25.11560919456324, 0.4066400174440891, 0.0, 1.0, 18713.92244468681], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.3516666666666667, 0.8569060773480663, 0.6666666666666666, 0.5929674328802701, 0.6355466724813631, 0.0, 1.0, 0.08911391640327053], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.74735993], dtype=float32), 1.1079289]. 
=============================================
[2019-04-04 05:16:29,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.859146]
 [75.25785 ]
 [75.6443  ]
 [76.13105 ]
 [76.552025]], R is [[74.68378448]
 [74.84783173]
 [75.01023102]
 [75.26013184]
 [75.41837311]].
[2019-04-04 05:16:30,738] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3726846e-08 1.5523849e-08 1.3571396e-20 6.2924523e-09 5.4684164e-09
 1.1545044e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:30,738] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4948
[2019-04-04 05:16:30,788] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 31.0, 286.5, 26.0, 25.09527684634763, 0.3689477632659163, 0.0, 1.0, 20023.77097813785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2998800.0000, 
sim time next is 2999400.0000, 
raw observation next is [-1.166666666666667, 55.83333333333334, 22.66666666666666, 224.0, 26.0, 25.09202974464196, 0.3585795498795973, 0.0, 1.0, 27198.70935165413], 
processed observation next is [0.0, 0.7391304347826086, 0.43028624192059095, 0.5583333333333335, 0.07555555555555554, 0.24751381215469614, 0.6666666666666666, 0.5910024787201632, 0.6195265166265324, 0.0, 1.0, 0.1295176635793054], 
reward next is 0.8705, 
noisyNet noise sample is [array([1.2954051], dtype=float32), 0.23484239]. 
=============================================
[2019-04-04 05:16:31,172] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1971380e-11 1.1604143e-11 5.0771071e-28 4.3359613e-12 6.3981086e-12
 1.0359910e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:31,175] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1312
[2019-04-04 05:16:31,190] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 110.0, 765.6666666666666, 26.0, 27.01216268747133, 0.730001359279821, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3149400.0000, 
sim time next is 3150000.0000, 
raw observation next is [7.0, 100.0, 111.0, 775.5, 26.0, 27.03888673038923, 0.7431670971545227, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6565096952908588, 1.0, 0.37, 0.8569060773480663, 0.6666666666666666, 0.7532405608657692, 0.7477223657181743, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83495885], dtype=float32), -0.17318963]. 
=============================================
[2019-04-04 05:16:31,198] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[95.02001 ]
 [95.06533 ]
 [95.062416]
 [95.25245 ]
 [95.488815]], R is [[95.07529449]
 [95.12454224]
 [95.17329407]
 [95.22156525]
 [95.26934814]].
[2019-04-04 05:16:38,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4173993e-11 1.1793995e-10 3.1301009e-27 2.9407227e-11 3.6337745e-11
 1.0108623e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:38,889] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7466
[2019-04-04 05:16:38,895] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 26.71841129377468, 0.8654679145053209, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3174600.0000, 
sim time next is 3175200.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 26.92412069699157, 0.8689484737004403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7436767247492974, 0.78964949123348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6120092], dtype=float32), -1.3409176]. 
=============================================
[2019-04-04 05:16:39,962] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.2018739e-11 2.7493346e-10 3.4064135e-27 2.0379389e-11 3.5207660e-11
 8.7983223e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:39,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8668
[2019-04-04 05:16:39,979] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 26.92400336311429, 0.8689273635721199, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3175200.0000, 
sim time next is 3175800.0000, 
raw observation next is [5.666666666666667, 100.0, 0.0, 0.0, 26.0, 26.96046485110598, 0.859504927588156, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6195752539242845, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7467054042588316, 0.7865016425293853, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5633929], dtype=float32), 0.0034751287]. 
=============================================
[2019-04-04 05:16:43,155] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.5672567e-10 4.7956245e-10 6.2152270e-25 8.5876729e-11 2.7210587e-10
 6.9692380e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:43,155] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3177
[2019-04-04 05:16:43,168] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 95.16666666666667, 104.3333333333333, 783.3333333333334, 26.0, 26.75671888981066, 0.8124937177063464, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3247800.0000, 
sim time next is 3248400.0000, 
raw observation next is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 26.0, 26.82653675238075, 0.8238859196633371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37026777469990774, 0.9033333333333334, 0.3422222222222223, 0.8576427255985268, 0.6666666666666666, 0.7355447293650625, 0.7746286398877791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2358706], dtype=float32), -0.10917494]. 
=============================================
[2019-04-04 05:16:56,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4300796e-09 2.7657299e-09 7.7669371e-25 3.0498370e-10 4.9966287e-10
 9.7037417e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:16:56,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6117
[2019-04-04 05:16:56,684] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.66666666666667, 114.0, 797.3333333333333, 26.0, 26.54357436729003, 0.6408200684953538, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3410400.0000, 
sim time next is 3411000.0000, 
raw observation next is [3.0, 47.0, 115.0, 804.0, 26.0, 26.61117980821327, 0.41938281812987, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.47, 0.38333333333333336, 0.8883977900552487, 0.6666666666666666, 0.7175983173511057, 0.6397942727099567, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19552225], dtype=float32), 0.04081039]. 
=============================================
[2019-04-04 05:16:56,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.8841  ]
 [87.80274 ]
 [87.7134  ]
 [87.587364]
 [87.46733 ]], R is [[88.0552063 ]
 [88.1746521 ]
 [88.29290771]
 [88.40998077]
 [88.52587891]].
[2019-04-04 05:17:02,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0831704e-09 1.0251380e-09 2.5161744e-25 2.6209326e-10 2.6500691e-09
 4.6271429e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:02,776] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7002
[2019-04-04 05:17:02,841] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 59.66666666666667, 301.6666666666667, 26.0, 25.27225413540064, 0.3790444400715428, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3485400.0000, 
sim time next is 3486000.0000, 
raw observation next is [-1.0, 71.0, 73.83333333333334, 350.3333333333333, 26.0, 25.23722162024007, 0.3929606159878977, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.24611111111111114, 0.3871086556169429, 0.6666666666666666, 0.6031018016866726, 0.6309868719959659, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62638795], dtype=float32), -0.078411065]. 
=============================================
[2019-04-04 05:17:02,844] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[86.755936]
 [85.82069 ]
 [84.89837 ]
 [84.00512 ]
 [83.90793 ]], R is [[87.74829865]
 [87.87081909]
 [87.99211121]
 [88.11219025]
 [88.23107147]].
[2019-04-04 05:17:03,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6974030e-09 3.0896230e-09 4.6213574e-24 5.1235954e-10 7.7760571e-09
 3.7384009e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:03,729] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3312
[2019-04-04 05:17:03,752] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.166666666666667, 44.5, 116.6666666666667, 824.6666666666667, 26.0, 25.44425272967495, 0.4570698894530852, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3672600.0000, 
sim time next is 3673200.0000, 
raw observation next is [4.333333333333334, 44.0, 116.8333333333333, 826.8333333333334, 26.0, 25.38792996996418, 0.4576403123397372, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.58264081255771, 0.44, 0.3894444444444443, 0.9136279926335176, 0.6666666666666666, 0.6156608308303483, 0.6525467707799124, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12793574], dtype=float32), 0.29948375]. 
=============================================
[2019-04-04 05:17:15,275] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8587762e-08 6.5958069e-08 5.6822280e-21 4.9585523e-08 3.3070162e-08
 2.1207391e-10 9.9999976e-01], sum to 1.0000
[2019-04-04 05:17:15,299] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1235
[2019-04-04 05:17:15,372] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.11709928576162, 0.3475577445033385, 0.0, 1.0, 18708.46501446351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3610800.0000, 
sim time next is 3611400.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.1118071805491, 0.3391589196740195, 0.0, 1.0, 18708.16581931047], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5926505983790916, 0.6130529732246731, 0.0, 1.0, 0.08908650390147843], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.19570036], dtype=float32), -0.98491484]. 
=============================================
[2019-04-04 05:17:16,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8550578e-08 3.8426640e-09 1.7651119e-23 4.6266933e-09 3.1744107e-09
 2.2990913e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:16,089] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5955
[2019-04-04 05:17:16,105] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 26.0, 25.48443224913774, 0.3593061697434218, 0.0, 1.0, 38166.27468244714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3649800.0000, 
sim time next is 3650400.0000, 
raw observation next is [10.0, 25.0, 0.0, 0.0, 26.0, 25.50100340713882, 0.3653192786514679, 0.0, 1.0, 25159.64060117041], 
processed observation next is [0.0, 0.2608695652173913, 0.739612188365651, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6250836172615685, 0.6217730928838227, 0.0, 1.0, 0.11980781238652576], 
reward next is 0.8802, 
noisyNet noise sample is [array([-1.6533989], dtype=float32), 0.38813895]. 
=============================================
[2019-04-04 05:17:20,562] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6820664e-08 5.2961163e-10 1.0831147e-24 1.7215245e-09 3.7479450e-10
 7.0746181e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:20,565] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7272
[2019-04-04 05:17:20,579] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.58347793544825, 0.3874625923145098, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3718800.0000, 
sim time next is 3719400.0000, 
raw observation next is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.51621355081851, 0.3761387452740439, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6263511292348758, 0.6253795817580147, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0950615], dtype=float32), -0.39581162]. 
=============================================
[2019-04-04 05:17:22,300] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4365709e-09 1.2168412e-08 3.9660274e-24 2.0271365e-09 2.2889861e-09
 2.6869245e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:22,300] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6831
[2019-04-04 05:17:22,362] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 67.0, 0.0, 0.0, 26.0, 25.09883112431356, 0.276440910914435, 0.0, 1.0, 41865.75529131875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3737400.0000, 
sim time next is 3738000.0000, 
raw observation next is [-3.333333333333333, 69.0, 0.0, 0.0, 26.0, 25.0967240590406, 0.284856366480653, 0.0, 1.0, 41965.4500325559], 
processed observation next is [1.0, 0.2608695652173913, 0.37026777469990774, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5913936715867166, 0.5949521221602176, 0.0, 1.0, 0.19983547634550428], 
reward next is 0.8002, 
noisyNet noise sample is [array([-0.11589377], dtype=float32), -0.84728175]. 
=============================================
[2019-04-04 05:17:22,366] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.05659 ]
 [83.07223 ]
 [83.097984]
 [83.16307 ]
 [83.18307 ]], R is [[83.05239105]
 [83.02250671]
 [82.99343109]
 [82.96508026]
 [82.93738556]].
[2019-04-04 05:17:32,629] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8701410e-10 2.1542115e-09 7.7712676e-24 5.2757504e-10 5.5225413e-10
 6.3228296e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:32,629] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6697
[2019-04-04 05:17:32,694] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333333, 40.33333333333334, 114.1666666666667, 818.0, 26.0, 25.73448006969409, 0.6530336294882839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3937200.0000, 
sim time next is 3937800.0000, 
raw observation next is [-5.166666666666667, 39.16666666666666, 112.3333333333333, 812.0, 26.0, 26.29149582318975, 0.6973035477805071, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.31948291782086796, 0.39166666666666655, 0.37444444444444436, 0.8972375690607735, 0.6666666666666666, 0.6909579852658124, 0.7324345159268356, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39064214], dtype=float32), 1.2640831]. 
=============================================
[2019-04-04 05:17:53,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1772103e-08 9.8131324e-08 5.1285327e-21 1.5956040e-08 4.1088800e-08
 2.0692115e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 05:17:53,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9247
[2019-04-04 05:17:53,383] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.46098558328369, 0.4853110379540038, 0.0, 1.0, 64010.81647600716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053600.0000, 
sim time next is 4054200.0000, 
raw observation next is [-5.166666666666667, 32.0, 0.0, 0.0, 26.0, 25.45984417683852, 0.4394693407686881, 0.0, 1.0, 48412.21203549852], 
processed observation next is [1.0, 0.9565217391304348, 0.31948291782086796, 0.32, 0.0, 0.0, 0.6666666666666666, 0.62165368140321, 0.6464897802562294, 0.0, 1.0, 0.23053434302618345], 
reward next is 0.7695, 
noisyNet noise sample is [array([2.4081025], dtype=float32), 0.43095696]. 
=============================================
[2019-04-04 05:17:54,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3540017e-09 3.1792520e-09 2.5779796e-23 1.5698115e-09 3.9675103e-09
 2.3490816e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:54,264] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2507
[2019-04-04 05:17:54,286] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.38834813791655, 0.3396107078751349, 0.0, 1.0, 48577.99536044719], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4242600.0000, 
sim time next is 4243200.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.3995358621915, 0.3404908855843103, 0.0, 1.0, 37898.37310290455], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6166279885159582, 0.6134969618614368, 0.0, 1.0, 0.18046844334716453], 
reward next is 0.8195, 
noisyNet noise sample is [array([1.7747034], dtype=float32), -1.4433616]. 
=============================================
[2019-04-04 05:17:55,380] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9502259e-11 1.5459123e-10 2.7954562e-24 4.9651609e-11 4.0641268e-11
 3.9285483e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:17:55,380] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6826
[2019-04-04 05:17:55,470] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 24.0, 105.6666666666667, 800.0, 26.0, 27.13516678840183, 0.6565421692298835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4026000.0000, 
sim time next is 4026600.0000, 
raw observation next is [-2.5, 23.0, 104.0, 794.0, 26.0, 26.47675918500377, 0.6862569173780121, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.39335180055401664, 0.23, 0.3466666666666667, 0.8773480662983425, 0.6666666666666666, 0.706396598750314, 0.7287523057926707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29135764], dtype=float32), 0.27908358]. 
=============================================
[2019-04-04 05:18:02,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9116808e-09 1.8920243e-08 6.9744684e-24 8.7088203e-09 5.3940794e-09
 3.4223509e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:18:02,579] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6332
[2019-04-04 05:18:02,592] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.16666666666666, 0.0, 0.0, 26.0, 25.3387520300827, 0.5620989993752556, 0.0, 1.0, 109522.4412661126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.59287763587752, 0.586623659049826, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6327398029897934, 0.6955412196832754, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.190727], dtype=float32), -0.14642756]. 
=============================================
[2019-04-04 05:18:03,181] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6478960e-09 8.4469178e-09 2.5164169e-23 4.1891193e-09 2.0665585e-09
 1.2472739e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:18:03,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5200
[2019-04-04 05:18:03,217] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 40.5, 0.0, 0.0, 26.0, 25.37168205379473, 0.4364729069500992, 0.0, 1.0, 46980.92900460962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4149000.0000, 
sim time next is 4149600.0000, 
raw observation next is [-1.0, 40.0, 0.0, 0.0, 26.0, 25.41578845488229, 0.4378990917684999, 0.0, 1.0, 18764.56899385913], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6179823712401907, 0.6459663639228334, 0.0, 1.0, 0.08935509044694824], 
reward next is 0.9106, 
noisyNet noise sample is [array([-0.04500958], dtype=float32), 0.6017308]. 
=============================================
[2019-04-04 05:18:15,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.50613871e-10 1.07744376e-10 1.60553176e-27 5.20623371e-11
 8.58646071e-11 9.49384915e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 05:18:15,031] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3639
[2019-04-04 05:18:15,048] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.966666666666667, 56.66666666666667, 100.6666666666667, 622.0, 26.0, 25.49662960177029, 0.4701381495293341, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4290600.0000, 
sim time next is 4291200.0000, 
raw observation next is [7.0, 56.0, 93.0, 605.5, 26.0, 25.53519082418621, 0.4719556492601192, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6565096952908588, 0.56, 0.31, 0.669060773480663, 0.6666666666666666, 0.6279325686821841, 0.657318549753373, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1944975], dtype=float32), 0.6534319]. 
=============================================
[2019-04-04 05:18:21,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.73469400e-10 3.63514704e-11 5.53523828e-25 2.89344174e-11
 1.07545216e-10 3.64758035e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 05:18:21,460] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3605
[2019-04-04 05:18:21,479] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.866666666666667, 57.33333333333334, 77.66666666666666, 572.5, 26.0, 25.55833148278995, 0.4692033675749285, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4292400.0000, 
sim time next is 4293000.0000, 
raw observation next is [6.8, 58.0, 70.0, 556.0, 26.0, 25.56690684244002, 0.4667148585366225, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6509695290858727, 0.58, 0.23333333333333334, 0.6143646408839779, 0.6666666666666666, 0.630575570203335, 0.6555716195122075, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0291401], dtype=float32), 0.40026948]. 
=============================================
[2019-04-04 05:18:21,484] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[91.16638]
 [91.43661]
 [91.67913]
 [91.89696]
 [92.15552]], R is [[90.97548676]
 [91.06573486]
 [91.15507507]
 [91.24352264]
 [91.33108521]].
[2019-04-04 05:18:23,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0084755e-08 3.2772707e-09 8.9602776e-25 1.0402791e-09 1.0043809e-08
 1.4673314e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:18:23,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8915
[2019-04-04 05:18:23,060] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.2, 75.0, 0.0, 0.0, 26.0, 25.50501343794821, 0.4047296936564169, 0.0, 1.0, 37270.29515524016], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4323600.0000, 
sim time next is 4324200.0000, 
raw observation next is [4.25, 74.5, 0.0, 0.0, 26.0, 25.53265702057303, 0.4095757830319493, 0.0, 1.0, 20471.09125790681], 
processed observation next is [1.0, 0.043478260869565216, 0.5803324099722993, 0.745, 0.0, 0.0, 0.6666666666666666, 0.6277214183810859, 0.6365252610106498, 0.0, 1.0, 0.09748138694241339], 
reward next is 0.9025, 
noisyNet noise sample is [array([-0.9810586], dtype=float32), -0.046312064]. 
=============================================
[2019-04-04 05:18:24,141] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8112842e-09 2.2279785e-10 2.3662442e-25 2.1242172e-10 1.2684329e-10
 9.7015950e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:18:24,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5936
[2019-04-04 05:18:24,171] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.4, 68.0, 0.0, 0.0, 26.0, 25.74300923690413, 0.5450196643019296, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4426200.0000, 
sim time next is 4426800.0000, 
raw observation next is [3.266666666666667, 68.0, 0.0, 0.0, 26.0, 25.69314505212219, 0.5290747662552454, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5530932594644506, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6410954210101826, 0.6763582554184152, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0065725], dtype=float32), -0.7413917]. 
=============================================
[2019-04-04 05:18:26,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5420149e-08 1.7268102e-08 1.2072836e-24 2.7769942e-09 1.2727351e-09
 3.6410903e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:18:26,222] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1912
[2019-04-04 05:18:26,237] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.533333333333334, 68.0, 0.0, 0.0, 26.0, 25.68699971651514, 0.5595283610099938, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4425600.0000, 
sim time next is 4426200.0000, 
raw observation next is [3.4, 68.0, 0.0, 0.0, 26.0, 25.74300923690413, 0.5450196643019296, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.556786703601108, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6452507697420108, 0.6816732214339766, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5199181], dtype=float32), -1.5997266]. 
=============================================
[2019-04-04 05:18:29,213] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.94389418e-09 5.19835064e-09 1.05592014e-23 1.34034106e-09
 1.04882802e-09 2.16850496e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 05:18:29,214] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6740
[2019-04-04 05:18:29,232] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 73.0, 0.0, 0.0, 26.0, 25.28925414934671, 0.4422032187008544, 0.0, 1.0, 42637.09595340549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4502400.0000, 
sim time next is 4503000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.34747231279461, 0.4472415114791808, 0.0, 1.0, 42507.43949570083], 
processed observation next is [1.0, 0.08695652173913043, 0.4367497691597415, 0.73, 0.0, 0.0, 0.6666666666666666, 0.612289359399551, 0.6490805038263936, 0.0, 1.0, 0.20241637855095634], 
reward next is 0.7976, 
noisyNet noise sample is [array([-0.39638743], dtype=float32), 0.33181012]. 
=============================================
[2019-04-04 05:18:29,265] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.57471 ]
 [80.7026  ]
 [80.872475]
 [81.07878 ]
 [80.88374 ]], R is [[80.60865784]
 [80.59954071]
 [80.58979797]
 [80.57834625]
 [80.5613327 ]].
[2019-04-04 05:18:32,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0842142e-08 1.9302872e-08 2.7763721e-22 6.4850183e-09 6.9104060e-09
 2.7769809e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:18:32,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0595
[2019-04-04 05:18:32,982] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.68615312376028, 0.5658845560805047, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4569600.0000, 
sim time next is 4570200.0000, 
raw observation next is [1.5, 59.0, 0.0, 0.0, 26.0, 25.72907871715509, 0.5621206292246536, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5041551246537397, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6440898930962575, 0.6873735430748845, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1407874], dtype=float32), 0.71542275]. 
=============================================
[2019-04-04 05:18:42,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3973286e-08 1.6134699e-09 2.4678015e-23 2.6707316e-09 1.9319695e-09
 3.2508750e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:18:42,385] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3758
[2019-04-04 05:18:42,416] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.16242954277987, 0.3272571140799677, 0.0, 1.0, 39221.23212940553], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4844400.0000, 
sim time next is 4845000.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.18092007526204, 0.3244788323647743, 0.0, 1.0, 39182.72824522843], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5984100062718367, 0.6081596107882581, 0.0, 1.0, 0.18658442021537347], 
reward next is 0.8134, 
noisyNet noise sample is [array([-1.1249605], dtype=float32), -1.1690208]. 
=============================================
[2019-04-04 05:18:42,425] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.023964]
 [83.10191 ]
 [82.85358 ]
 [82.64016 ]
 [82.48045 ]], R is [[83.26765442]
 [83.24821472]
 [83.22879791]
 [83.20935059]
 [83.18964386]].
[2019-04-04 05:18:45,879] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 05:18:45,880] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:18:45,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:18:45,881] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:18:45,882] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:18:45,883] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run5
[2019-04-04 05:18:45,899] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:18:45,901] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:18:45,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run5
[2019-04-04 05:18:45,923] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run5
[2019-04-04 05:18:54,729] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.24477015], dtype=float32), 0.14463556]
[2019-04-04 05:18:54,729] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.158417198333334, 86.48641630499999, 0.0, 0.0, 26.0, 24.58720087559784, 0.2090307613677954, 0.0, 1.0, 29287.47347957384]
[2019-04-04 05:18:54,730] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:18:54,731] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7258289e-09 7.9328411e-10 1.1152983e-25 4.4045917e-10 5.0709303e-10
 2.4957182e-13 1.0000000e+00], sampled 0.4787157972647307
[2019-04-04 05:20:25,717] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.24477015], dtype=float32), 0.14463556]
[2019-04-04 05:20:25,717] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.060638134666666, 71.69660895333334, 143.1476964666667, 0.0, 26.0, 25.69793591798502, 0.404155242151883, 1.0, 1.0, 35572.89990040069]
[2019-04-04 05:20:25,717] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:20:25,718] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3379865e-09 1.9541166e-09 7.8364963e-23 1.6779929e-09 4.9657128e-10
 3.4267452e-12 1.0000000e+00], sampled 0.6266740866081336
[2019-04-04 05:21:20,871] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 05:21:44,068] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 05:21:46,122] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 05:21:47,147] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 400000, evaluation results [400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 05:21:47,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3073371e-09 6.5190235e-09 4.1832609e-23 1.7653771e-09 1.7425517e-09
 1.4870792e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:21:47,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1221
[2019-04-04 05:21:47,408] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.95279102876458, 0.3594645911885798, 0.0, 1.0, 41141.27824587585], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4755600.0000, 
sim time next is 4756200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.92045630052909, 0.3589987058520723, 0.0, 1.0, 41050.6348964639], 
processed observation next is [0.0, 0.043478260869565216, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5767046917107574, 0.6196662352840241, 0.0, 1.0, 0.1954792137926852], 
reward next is 0.8045, 
noisyNet noise sample is [array([-2.4326038], dtype=float32), -2.1043568]. 
=============================================
[2019-04-04 05:21:47,583] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8304513e-11 7.4328516e-10 5.7380423e-26 5.2757704e-10 2.2894990e-11
 1.4435655e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:21:47,583] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2846
[2019-04-04 05:21:47,597] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.666666666666668, 25.33333333333333, 88.66666666666667, 751.8333333333333, 26.0, 27.24090550746304, 0.8552414004408053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4981200.0000, 
sim time next is 4981800.0000, 
raw observation next is [8.833333333333332, 25.16666666666667, 85.33333333333334, 729.6666666666667, 26.0, 27.49972098155332, 0.8823961404993219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7072945521698984, 0.2516666666666667, 0.2844444444444445, 0.8062615101289136, 0.6666666666666666, 0.7916434151294434, 0.7941320468331073, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24531378], dtype=float32), -1.0363134]. 
=============================================
[2019-04-04 05:21:52,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5602921e-09 2.2870354e-09 1.3552537e-24 1.3068795e-09 3.7910686e-10
 2.6953064e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:21:52,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7975
[2019-04-04 05:21:52,467] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 54.66666666666667, 297.1666666666666, 188.0, 26.0, 25.07791305109466, 0.3215223932088137, 0.0, 1.0, 18713.40456387374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4876800.0000, 
sim time next is 4877400.0000, 
raw observation next is [-0.6666666666666666, 53.33333333333334, 294.3333333333334, 212.0, 26.0, 25.02306728452872, 0.3223155529797259, 0.0, 1.0, 33800.55849842299], 
processed observation next is [0.0, 0.43478260869565216, 0.44413665743305636, 0.5333333333333334, 0.9811111111111114, 0.23425414364640884, 0.6666666666666666, 0.5852556070440601, 0.6074385176599086, 0.0, 1.0, 0.16095504046868092], 
reward next is 0.8390, 
noisyNet noise sample is [array([1.588577], dtype=float32), -0.04377997]. 
=============================================
[2019-04-04 05:21:54,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:21:54,299] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:21:54,311] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run4
[2019-04-04 05:21:54,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0122738e-08 1.1470653e-08 3.1739355e-22 1.2521390e-08 5.2552349e-09
 5.0433799e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:21:54,895] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-04 05:21:54,909] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.01348269307109, 0.2339237049121905, 0.0, 1.0, 38658.70368384671], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4946400.0000, 
sim time next is 4947000.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03172519049469, 0.2369079584023087, 0.0, 1.0, 38643.64514240235], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5859770992078909, 0.5789693194674362, 0.0, 1.0, 0.18401735782096357], 
reward next is 0.8160, 
noisyNet noise sample is [array([0.548025], dtype=float32), -0.87972414]. 
=============================================
[2019-04-04 05:21:54,922] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.01868]
 [77.14888]
 [77.31994]
 [77.44968]
 [77.57689]], R is [[76.93954468]
 [76.9860611 ]
 [77.03216553]
 [77.07802582]
 [77.12364197]].
[2019-04-04 05:21:55,522] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.01709055e-08 3.95245658e-09 2.37079997e-23 1.01324735e-08
 4.78119588e-09 1.75908645e-11 9.99999881e-01], sum to 1.0000
[2019-04-04 05:21:55,522] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3809
[2019-04-04 05:21:55,532] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.35105733881791, 0.3477685240163191, 0.0, 1.0, 45565.18796800071], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4932000.0000, 
sim time next is 4932600.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.54949225135038, 0.3474200590953562, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6291243542791983, 0.6158066863651187, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3741463], dtype=float32), -0.15117455]. 
=============================================
[2019-04-04 05:21:56,730] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:21:56,730] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:21:56,734] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run4
[2019-04-04 05:21:57,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3676674e-07 1.2741152e-08 9.1793180e-23 2.5894813e-08 8.2185432e-09
 1.1497773e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:21:57,463] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0853
[2019-04-04 05:21:57,476] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 42.0, 0.0, 0.0, 26.0, 25.49359538450468, 0.3695706808898806, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4927200.0000, 
sim time next is 4927800.0000, 
raw observation next is [0.1666666666666666, 42.5, 0.0, 0.0, 26.0, 25.614750809472, 0.3715638484591106, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.4672206832871654, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6345625674559999, 0.6238546161530368, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.62360406], dtype=float32), -3.3096986]. 
=============================================
[2019-04-04 05:21:58,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:21:58,743] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:21:58,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run4
[2019-04-04 05:21:59,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:21:59,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:21:59,072] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run4
[2019-04-04 05:22:04,263] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:04,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:04,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run4
[2019-04-04 05:22:04,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:04,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:04,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run4
[2019-04-04 05:22:05,218] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:05,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:05,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run4
[2019-04-04 05:22:05,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:05,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:05,507] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run4
[2019-04-04 05:22:05,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:05,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:05,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run4
[2019-04-04 05:22:07,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:07,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:07,046] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run4
[2019-04-04 05:22:07,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:07,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:07,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run4
[2019-04-04 05:22:07,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:07,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:07,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run4
[2019-04-04 05:22:07,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:07,837] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:07,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run4
[2019-04-04 05:22:07,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:07,901] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:07,904] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run4
[2019-04-04 05:22:08,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:08,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:08,247] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run4
[2019-04-04 05:22:09,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:22:09,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:22:09,771] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run4
[2019-04-04 05:22:12,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7427574e-09 1.3966615e-09 3.6337260e-26 5.2426175e-10 4.2869544e-10
 5.7011850e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:22:12,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3910
[2019-04-04 05:22:12,805] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.1, 87.5, 0.0, 0.0, 26.0, 24.60387685203898, 0.2060029493106725, 0.0, 1.0, 38881.28373722572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 66600.0000, 
sim time next is 67200.0000, 
raw observation next is [4.0, 87.0, 0.0, 0.0, 26.0, 24.60695077406228, 0.207017534964379, 0.0, 1.0, 40628.10301614374], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5505792311718567, 0.5690058449881263, 0.0, 1.0, 0.1934671572197321], 
reward next is 0.8065, 
noisyNet noise sample is [array([-0.4481669], dtype=float32), 0.637418]. 
=============================================
[2019-04-04 05:22:33,105] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.28860815e-08 1.84863644e-08 2.44899053e-22 6.90198876e-09
 3.35706285e-09 7.70742758e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 05:22:33,119] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9852
[2019-04-04 05:22:33,206] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 17.0, 157.0, 26.0, 24.40427705977291, 0.1259352286072122, 1.0, 1.0, 90169.12504313787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 201600.0000, 
sim time next is 202200.0000, 
raw observation next is [-8.816666666666666, 78.0, 22.66666666666667, 203.3333333333333, 26.0, 24.75431909910569, 0.154670295104421, 1.0, 1.0, 50896.79900477489], 
processed observation next is [1.0, 0.34782608695652173, 0.21837488457987075, 0.78, 0.07555555555555557, 0.22467771639042353, 0.6666666666666666, 0.5628599249254741, 0.551556765034807, 1.0, 1.0, 0.2423657095465471], 
reward next is 0.7576, 
noisyNet noise sample is [array([0.07488849], dtype=float32), 0.19470567]. 
=============================================
[2019-04-04 05:22:40,341] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.6282567e-08 5.5309794e-08 2.5744452e-20 2.0784301e-08 3.6174388e-08
 4.4565790e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 05:22:40,341] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4016
[2019-04-04 05:22:40,357] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.66666666666667, 69.0, 0.0, 0.0, 26.0, 23.25581194638117, -0.1130215196485651, 0.0, 1.0, 47800.39444627356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 350400.0000, 
sim time next is 351000.0000, 
raw observation next is [-14.75, 69.0, 0.0, 0.0, 26.0, 23.29819291892085, -0.1212209936813704, 0.0, 1.0, 47895.43855705271], 
processed observation next is [1.0, 0.043478260869565216, 0.05401662049861495, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4415160765767376, 0.45959300210620985, 0.0, 1.0, 0.22807351693834624], 
reward next is 0.7719, 
noisyNet noise sample is [array([0.5110928], dtype=float32), 1.4509494]. 
=============================================
[2019-04-04 05:22:40,362] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[67.39297]
 [67.38595]
 [67.42933]
 [67.45302]
 [67.2662 ]], R is [[67.5579071 ]
 [67.65470886]
 [67.75089264]
 [67.84635925]
 [67.94108582]].
[2019-04-04 05:22:58,202] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3811950e-07 3.2257242e-07 2.8173115e-19 7.0130461e-08 8.0331539e-08
 4.6677900e-10 9.9999940e-01], sum to 1.0000
[2019-04-04 05:22:58,204] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9778
[2019-04-04 05:22:58,220] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.7, 50.0, 0.0, 0.0, 26.0, 23.19006142319212, -0.1619924066852084, 0.0, 1.0, 45942.88737060973], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 442200.0000, 
sim time next is 442800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 23.18807021745657, -0.1704510504962111, 0.0, 1.0, 45968.28828525393], 
processed observation next is [1.0, 0.13043478260869565, 0.1689750692520776, 0.49, 0.0, 0.0, 0.6666666666666666, 0.4323391847880475, 0.44318298316792964, 0.0, 1.0, 0.21889661088216159], 
reward next is 0.7811, 
noisyNet noise sample is [array([0.03543103], dtype=float32), -0.27344763]. 
=============================================
[2019-04-04 05:23:04,532] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0442176e-09 8.2018309e-10 1.1515718e-24 6.2721867e-10 4.9257469e-09
 2.5230152e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:04,532] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4425
[2019-04-04 05:23:04,587] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 67.0, 129.1666666666667, 42.5, 26.0, 25.12817801536438, 0.2261483259524955, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 639600.0000, 
sim time next is 640200.0000, 
raw observation next is [-3.9, 66.0, 123.3333333333333, 34.00000000000001, 26.0, 25.05340007092958, 0.2119236805358627, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.66, 0.411111111111111, 0.03756906077348067, 0.6666666666666666, 0.5877833392441317, 0.5706412268452875, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1250857], dtype=float32), 0.28956634]. 
=============================================
[2019-04-04 05:23:05,058] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.9396720e-09 1.4962742e-09 8.2455031e-24 1.2828837e-09 3.5732323e-10
 1.6319292e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:05,058] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1966
[2019-04-04 05:23:05,141] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 83.0, 70.5, 59.0, 26.0, 24.97676462117228, 0.3140757898991607, 0.0, 1.0, 39039.82426490853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 576000.0000, 
sim time next is 576600.0000, 
raw observation next is [-1.283333333333333, 83.66666666666667, 60.66666666666666, 54.33333333333333, 26.0, 24.98605177884274, 0.3125763669180246, 0.0, 1.0, 34125.30292545212], 
processed observation next is [0.0, 0.6956521739130435, 0.4270544783010157, 0.8366666666666667, 0.2022222222222222, 0.060036832412523014, 0.6666666666666666, 0.5821709815702283, 0.6041921223060082, 0.0, 1.0, 0.16250144250215293], 
reward next is 0.8375, 
noisyNet noise sample is [array([0.08092164], dtype=float32), -1.1104113]. 
=============================================
[2019-04-04 05:23:09,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2370225e-08 9.4262198e-09 3.2546722e-23 2.0957736e-09 6.9661027e-10
 5.9700092e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:09,005] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5081
[2019-04-04 05:23:09,029] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 26.0, 25.02531936966371, 0.2196593467886331, 0.0, 1.0, 42740.31806773723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678600.0000, 
sim time next is 679200.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.99765895773892, 0.2123492821102044, 0.0, 1.0, 42483.78807146917], 
processed observation next is [0.0, 0.8695652173913043, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5831382464782434, 0.5707830940367348, 0.0, 1.0, 0.20230375272128176], 
reward next is 0.7977, 
noisyNet noise sample is [array([-1.5425379], dtype=float32), -0.53000647]. 
=============================================
[2019-04-04 05:23:35,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.7944967e-11 1.0529024e-10 3.4400144e-26 1.7454704e-11 3.4735277e-11
 4.6084328e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:35,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6059
[2019-04-04 05:23:35,782] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 85.0, 0.0, 0.0, 26.0, 25.16248379120529, 0.2941370464990105, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 847200.0000, 
sim time next is 847800.0000, 
raw observation next is [-3.65, 84.5, 0.0, 0.0, 26.0, 25.04763481759518, 0.2681150388315321, 1.0, 1.0, 24720.62871611482], 
processed observation next is [1.0, 0.8260869565217391, 0.3614958448753463, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5873029014662651, 0.5893716796105107, 1.0, 1.0, 0.11771727960054677], 
reward next is 0.8823, 
noisyNet noise sample is [array([-0.5685007], dtype=float32), -1.2924328]. 
=============================================
[2019-04-04 05:23:47,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3187516e-10 9.0674496e-11 5.2263187e-27 1.0085495e-10 2.8525031e-11
 5.0139936e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:47,085] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4658
[2019-04-04 05:23:47,097] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 26.0, 25.4070438556405, 0.5491325982062846, 0.0, 1.0, 55386.65418307683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [2.1, 92.0, 0.0, 0.0, 26.0, 25.41299594323612, 0.5527884657345088, 0.0, 1.0, 43429.76518141748], 
processed observation next is [1.0, 0.17391304347826086, 0.5207756232686982, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6177496619363433, 0.6842628219115029, 0.0, 1.0, 0.20680840562579753], 
reward next is 0.7932, 
noisyNet noise sample is [array([-0.5793365], dtype=float32), 0.70974374]. 
=============================================
[2019-04-04 05:23:47,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[91.79797 ]
 [91.797295]
 [91.91333 ]
 [91.98004 ]
 [91.90661 ]], R is [[91.61795044]
 [91.43802643]
 [91.32256317]
 [91.21292877]
 [90.95339966]].
[2019-04-04 05:23:48,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9518005e-10 1.3414760e-10 1.3737347e-27 2.6219363e-11 1.1151729e-10
 1.2625528e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:48,565] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1537
[2019-04-04 05:23:48,582] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.73072201518756, 0.619269484806908, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027200.0000, 
sim time next is 1027800.0000, 
raw observation next is [14.4, 76.0, 0.0, 0.0, 26.0, 25.81706221988414, 0.6229881632690072, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6514218516570116, 0.707662721089669, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.59612495], dtype=float32), -2.0403605]. 
=============================================
[2019-04-04 05:23:50,200] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1695575e-08 6.1352678e-09 5.7122829e-22 7.3741266e-09 5.7512297e-09
 8.6612653e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:50,210] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3698
[2019-04-04 05:23:50,219] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.63333333333333, 52.0, 0.0, 0.0, 26.0, 25.91107889114355, 0.7509257086989855, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1100400.0000, 
sim time next is 1101000.0000, 
raw observation next is [16.36666666666667, 52.5, 0.0, 0.0, 26.0, 26.20048742633513, 0.7719069770843706, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9159741458910436, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6833739521945942, 0.7573023256947903, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1632044], dtype=float32), -0.33536184]. 
=============================================
[2019-04-04 05:23:50,234] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.070625]
 [79.28475 ]
 [79.37791 ]
 [79.42736 ]
 [79.56784 ]], R is [[79.1322403 ]
 [79.34091949]
 [79.54750824]
 [79.75203705]
 [79.95452118]].
[2019-04-04 05:23:52,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.65214820e-09 7.15650705e-10 1.28921096e-26 7.87836185e-10
 4.08005102e-10 4.24395111e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 05:23:52,443] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3867
[2019-04-04 05:23:52,450] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.56666666666667, 66.33333333333334, 122.1666666666667, 0.0, 26.0, 25.3867660823113, 0.5298656293937104, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1160400.0000, 
sim time next is 1161000.0000, 
raw observation next is [17.75, 66.0, 130.0, 0.0, 26.0, 25.32669258698045, 0.5227851711235746, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9542936288088645, 0.66, 0.43333333333333335, 0.0, 0.6666666666666666, 0.6105577155817041, 0.6742617237078582, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4244807], dtype=float32), 0.49735162]. 
=============================================
[2019-04-04 05:23:52,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[89.980125]
 [90.970245]
 [91.97579 ]
 [92.76431 ]
 [93.68898 ]], R is [[89.10177612]
 [89.21076202]
 [89.31865692]
 [89.42546844]
 [89.53121185]].
[2019-04-04 05:23:55,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1258296e-08 3.4116139e-09 9.3535471e-23 3.3228029e-09 2.3642783e-09
 2.0441472e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:55,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9719
[2019-04-04 05:23:55,164] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.18333333333333, 77.5, 0.0, 0.0, 26.0, 24.21966704437249, 0.2954126973515848, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1209000.0000, 
sim time next is 1209600.0000, 
raw observation next is [16.1, 78.0, 0.0, 0.0, 26.0, 24.19162849880782, 0.2903189847889261, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5159690415673183, 0.596772994929642, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48294917], dtype=float32), 1.5795305]. 
=============================================
[2019-04-04 05:23:55,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3529655e-08 3.1335463e-09 1.8112078e-23 4.0435020e-09 1.4322040e-08
 1.0327023e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:55,438] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5276
[2019-04-04 05:23:55,444] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.58347076702589, 0.1641174065662103, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1231200.0000, 
sim time next is 1231800.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.56011825058016, 0.1603319134027564, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.4633431875483467, 0.5534439711342521, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90670174], dtype=float32), 0.85678786]. 
=============================================
[2019-04-04 05:23:57,092] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2145467e-10 4.5357057e-10 4.7609364e-29 7.2711157e-11 5.9801553e-11
 1.5087027e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:57,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5868
[2019-04-04 05:23:57,120] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 92.0, 0.0, 26.0, 25.63836966361508, 0.4707148673578399, 1.0, 1.0, 18680.56354532288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1425000.0000, 
sim time next is 1425600.0000, 
raw observation next is [0.0, 95.0, 93.0, 0.0, 26.0, 25.66879744707587, 0.476390560878333, 1.0, 1.0, 18681.24447575885], 
processed observation next is [1.0, 0.5217391304347826, 0.46260387811634357, 0.95, 0.31, 0.0, 0.6666666666666666, 0.6390664539229892, 0.658796853626111, 1.0, 1.0, 0.08895830702742309], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.04981911], dtype=float32), 1.1535738]. 
=============================================
[2019-04-04 05:23:59,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4742760e-10 9.9793521e-11 7.6034616e-28 9.7420232e-12 5.1534270e-11
 1.2230728e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:23:59,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6867
[2019-04-04 05:23:59,736] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 96.0, 0.0, 0.0, 26.0, 25.41546516462842, 0.5825242408017938, 0.0, 1.0, 47938.43207216716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1296000.0000, 
sim time next is 1296600.0000, 
raw observation next is [4.300000000000001, 95.5, 0.0, 0.0, 26.0, 25.4108805338883, 0.5828741717905211, 0.0, 1.0, 44326.36894658029], 
processed observation next is [1.0, 0.0, 0.5817174515235458, 0.955, 0.0, 0.0, 0.6666666666666666, 0.617573377824025, 0.6942913905968404, 0.0, 1.0, 0.21107794736466803], 
reward next is 0.7889, 
noisyNet noise sample is [array([-1.9824086], dtype=float32), 0.24193627]. 
=============================================
[2019-04-04 05:24:00,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5839399e-10 7.1012037e-11 3.2836085e-28 8.3377111e-11 2.4275104e-10
 3.0931471e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:00,800] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6293
[2019-04-04 05:24:00,824] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 100.0, 9.5, 0.0, 26.0, 24.59283117520657, 0.4239649499376067, 0.0, 1.0, 26902.94546961281], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1270800.0000, 
sim time next is 1271400.0000, 
raw observation next is [11.55, 99.33333333333334, 6.333333333333332, 0.0, 26.0, 24.58320774206833, 0.4243766439037286, 0.0, 1.0, 41897.84233436369], 
processed observation next is [0.0, 0.7391304347826086, 0.7825484764542937, 0.9933333333333334, 0.02111111111111111, 0.0, 0.6666666666666666, 0.5486006451723607, 0.6414588813012428, 0.0, 1.0, 0.19951353492554139], 
reward next is 0.8005, 
noisyNet noise sample is [array([1.0359404], dtype=float32), -0.5147989]. 
=============================================
[2019-04-04 05:24:05,079] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5443752e-11 3.4546643e-11 3.0824698e-25 7.9265906e-11 1.5111040e-10
 1.6261173e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:05,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0009
[2019-04-04 05:24:05,088] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8, 94.5, 18.0, 0.0, 26.0, 25.69498484848552, 0.4854196680296946, 1.0, 1.0, 52296.45787519138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1355400.0000, 
sim time next is 1356000.0000, 
raw observation next is [0.7000000000000001, 95.0, 15.0, 0.0, 26.0, 25.55343850316529, 0.4887714564547792, 1.0, 1.0, 36300.10573948561], 
processed observation next is [1.0, 0.6956521739130435, 0.4819944598337951, 0.95, 0.05, 0.0, 0.6666666666666666, 0.6294532085971074, 0.6629238188182597, 1.0, 1.0, 0.1728576463785029], 
reward next is 0.8271, 
noisyNet noise sample is [array([-1.224496], dtype=float32), -0.44790575]. 
=============================================
[2019-04-04 05:24:05,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.49033 ]
 [85.272514]
 [85.36314 ]
 [85.45538 ]
 [85.52953 ]], R is [[85.58592987]
 [85.48104095]
 [85.62622833]
 [85.76996613]
 [85.91226959]].
[2019-04-04 05:24:08,145] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3059684e-09 1.7567983e-09 2.3226301e-24 2.4788474e-10 8.5980356e-10
 6.3513306e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:08,146] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9487
[2019-04-04 05:24:08,157] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.30102945979066, 0.4856379357994918, 0.0, 1.0, 54290.12579355369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1387200.0000, 
sim time next is 1387800.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.29993092815446, 0.4821363699696331, 0.0, 1.0, 44791.5232731612], 
processed observation next is [1.0, 0.043478260869565216, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.608327577346205, 0.660712123323211, 0.0, 1.0, 0.21329296796743427], 
reward next is 0.7867, 
noisyNet noise sample is [array([-0.03246645], dtype=float32), -0.5295135]. 
=============================================
[2019-04-04 05:24:14,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3708702e-10 3.5127387e-10 1.7323967e-25 3.6847869e-11 5.1150979e-11
 6.8688504e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:14,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3098
[2019-04-04 05:24:14,668] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 133.8333333333333, 0.0, 26.0, 27.43684764887038, 0.7538987936961151, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1608000.0000, 
sim time next is 1608600.0000, 
raw observation next is [13.8, 49.0, 122.6666666666667, 0.0, 26.0, 26.88958143679467, 0.7844970403192129, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.844875346260388, 0.49, 0.408888888888889, 0.0, 0.6666666666666666, 0.7407984530662226, 0.7614990134397376, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.43792307], dtype=float32), 2.5890641]. 
=============================================
[2019-04-04 05:24:14,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0239405e-10 1.7213628e-10 2.1808760e-25 1.6137953e-10 3.9891795e-10
 2.1453790e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:14,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4879
[2019-04-04 05:24:14,812] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 92.0, 0.0, 0.0, 26.0, 25.34732627484668, 0.4679777456000285, 0.0, 1.0, 58844.19736402338], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1476000.0000, 
sim time next is 1476600.0000, 
raw observation next is [2.2, 92.33333333333334, 0.0, 0.0, 26.0, 25.36106206691964, 0.4809078983546853, 0.0, 1.0, 45407.00271073073], 
processed observation next is [1.0, 0.08695652173913043, 0.5235457063711911, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.61342183890997, 0.6603026327848951, 0.0, 1.0, 0.2162238224320511], 
reward next is 0.7838, 
noisyNet noise sample is [array([-0.09054103], dtype=float32), 2.9799688]. 
=============================================
[2019-04-04 05:24:20,013] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0042525e-10 4.1677664e-10 8.2490539e-25 4.5078741e-10 2.4461530e-10
 1.3282295e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:20,014] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6482
[2019-04-04 05:24:20,022] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 86.66666666666667, 87.0, 0.0, 26.0, 25.87070826073181, 0.525600289906801, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1683600.0000, 
sim time next is 1684200.0000, 
raw observation next is [1.1, 85.33333333333333, 91.0, 0.0, 26.0, 25.86052618545463, 0.5240802004277877, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.8533333333333333, 0.30333333333333334, 0.0, 0.6666666666666666, 0.6550438487878859, 0.6746934001425959, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60874444], dtype=float32), 1.2762978]. 
=============================================
[2019-04-04 05:24:21,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.7692050e-09 5.4123412e-09 8.6014845e-24 2.1160820e-09 8.7098462e-10
 4.4053520e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:21,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0973
[2019-04-04 05:24:21,563] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.12298599387675, 0.3341018301897815, 0.0, 1.0, 49784.77058361328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1799400.0000, 
sim time next is 1800000.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.12513724839566, 0.330110763564654, 0.0, 1.0, 47430.06844452791], 
processed observation next is [0.0, 0.8695652173913043, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.593761437366305, 0.6100369211882181, 0.0, 1.0, 0.22585746878346624], 
reward next is 0.7741, 
noisyNet noise sample is [array([-1.7144036], dtype=float32), -1.44191]. 
=============================================
[2019-04-04 05:24:21,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.770935]
 [78.76521 ]
 [78.69641 ]
 [78.49743 ]
 [78.14891 ]], R is [[78.77667999]
 [78.75184631]
 [78.69026184]
 [78.5265274 ]
 [78.16423035]].
[2019-04-04 05:24:37,372] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1114155e-08 1.4586227e-08 9.4721110e-23 6.1055929e-09 2.5089297e-09
 1.3629930e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:24:37,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6831
[2019-04-04 05:24:37,391] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 82.0, 0.0, 0.0, 26.0, 24.71751718405249, 0.2319262873615746, 0.0, 1.0, 45585.15786827578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1810800.0000, 
sim time next is 1811400.0000, 
raw observation next is [-5.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.68498847170027, 0.2254813499281989, 0.0, 1.0, 45534.38447946731], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.557082372641689, 0.5751604499760663, 0.0, 1.0, 0.21683040228317768], 
reward next is 0.7832, 
noisyNet noise sample is [array([0.02072293], dtype=float32), 0.36799783]. 
=============================================
[2019-04-04 05:25:07,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4569113e-10 1.8229768e-09 1.2630859e-24 3.3064271e-10 2.7205657e-10
 2.7947536e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:25:07,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5071
[2019-04-04 05:25:07,747] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 67.66666666666667, 269.3333333333334, 106.5, 26.0, 25.8821960738556, 0.4410250169004434, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2115600.0000, 
sim time next is 2116200.0000, 
raw observation next is [-6.800000000000001, 65.83333333333333, 245.6666666666667, 112.0, 26.0, 25.92188121155137, 0.4399438704559503, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2742382271468144, 0.6583333333333333, 0.818888888888889, 0.12375690607734807, 0.6666666666666666, 0.6601567676292808, 0.6466479568186502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4496456], dtype=float32), -1.5263276]. 
=============================================
[2019-04-04 05:25:35,618] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5049386e-08 1.1573613e-08 2.6377658e-23 2.4696132e-09 4.1149248e-09
 1.0571942e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:25:35,641] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6165
[2019-04-04 05:25:35,698] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 63.0, 0.0, 0.0, 26.0, 24.53399347836289, 0.1920901211838017, 0.0, 1.0, 39755.88521956332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2344800.0000, 
sim time next is 2345400.0000, 
raw observation next is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.52150073818959, 0.1860453205060347, 0.0, 1.0, 39862.47935494516], 
processed observation next is [0.0, 0.13043478260869565, 0.3919667590027701, 0.635, 0.0, 0.0, 0.6666666666666666, 0.5434583948491326, 0.5620151068353448, 0.0, 1.0, 0.1898213302616436], 
reward next is 0.8102, 
noisyNet noise sample is [array([1.2568235], dtype=float32), -0.8529853]. 
=============================================
[2019-04-04 05:25:41,779] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0780328e-08 5.4358051e-09 1.8688014e-23 4.7350390e-09 1.3500602e-09
 7.2024561e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:25:41,779] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5130
[2019-04-04 05:25:41,871] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.5, 0.0, 0.0, 26.0, 24.81828029479663, 0.2649517954661158, 0.0, 1.0, 38511.03404110295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2335800.0000, 
sim time next is 2336400.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.79001587154515, 0.2688315479867441, 0.0, 1.0, 38522.95655818343], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5658346559620959, 0.5896105159955813, 0.0, 1.0, 0.18344265027706397], 
reward next is 0.8166, 
noisyNet noise sample is [array([0.25348955], dtype=float32), 2.2619734]. 
=============================================
[2019-04-04 05:25:50,537] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.01419334e-08 1.48504435e-08 7.32892726e-22 3.41128192e-09
 7.76211273e-09 5.34215033e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 05:25:50,538] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9016
[2019-04-04 05:25:50,623] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.95, 39.5, 76.0, 777.0, 26.0, 25.00895035058559, 0.2284379566815816, 0.0, 1.0, 18744.69522215685], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2457000.0000, 
sim time next is 2457600.0000, 
raw observation next is [-3.4, 38.33333333333334, 77.66666666666667, 785.6666666666666, 26.0, 24.95901718089326, 0.2305152568623789, 0.0, 1.0, 41246.86450795596], 
processed observation next is [0.0, 0.43478260869565216, 0.368421052631579, 0.3833333333333334, 0.2588888888888889, 0.8681399631675875, 0.6666666666666666, 0.5799180984077715, 0.5768384189541264, 0.0, 1.0, 0.19641364051407598], 
reward next is 0.8036, 
noisyNet noise sample is [array([3.1537673], dtype=float32), 1.2621372]. 
=============================================
[2019-04-04 05:26:01,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8187205e-10 7.4241524e-10 4.8347579e-25 2.7945324e-10 8.1009248e-11
 1.9066007e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:01,881] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4970
[2019-04-04 05:26:01,944] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 62.0, 0.0, 0.0, 26.0, 25.00335446788086, 0.3526062572748268, 1.0, 1.0, 87131.51347100359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2662800.0000, 
sim time next is 2663400.0000, 
raw observation next is [-1.2, 62.5, 0.0, 0.0, 26.0, 24.9714935240008, 0.3584046472876619, 0.0, 1.0, 81202.30901040214], 
processed observation next is [1.0, 0.8260869565217391, 0.42936288088642666, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5809577936667333, 0.619468215762554, 0.0, 1.0, 0.3866776619542959], 
reward next is 0.6133, 
noisyNet noise sample is [array([-1.2585799], dtype=float32), -0.96656597]. 
=============================================
[2019-04-04 05:26:20,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3437250e-09 2.1403888e-09 3.4856283e-25 2.2823932e-10 4.1289169e-10
 4.8218326e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:20,098] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1560
[2019-04-04 05:26:20,124] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 70.0, 0.0, 0.0, 26.0, 24.86467627462983, 0.2975792525388537, 0.0, 1.0, 44448.41055514399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2676000.0000, 
sim time next is 2676600.0000, 
raw observation next is [-6.0, 70.5, 0.0, 0.0, 26.0, 24.80762542643659, 0.2935986286873746, 0.0, 1.0, 44428.25718777307], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5673021188697159, 0.5978662095624582, 0.0, 1.0, 0.21156312946558606], 
reward next is 0.7884, 
noisyNet noise sample is [array([-0.14702341], dtype=float32), 0.38443267]. 
=============================================
[2019-04-04 05:26:21,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5082605e-08 1.8234992e-08 7.8502254e-24 1.6863220e-09 4.4902935e-09
 2.1455653e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:21,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3997
[2019-04-04 05:26:21,958] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.333333333333334, 70.16666666666667, 0.0, 0.0, 26.0, 24.53405557434862, 0.2117973060740423, 0.0, 1.0, 44390.65573560485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2682600.0000, 
sim time next is 2683200.0000, 
raw observation next is [-9.666666666666668, 71.33333333333334, 0.0, 0.0, 26.0, 24.4648809721929, 0.193055691025741, 0.0, 1.0, 44405.43176912737], 
processed observation next is [1.0, 0.043478260869565216, 0.19482917820867957, 0.7133333333333334, 0.0, 0.0, 0.6666666666666666, 0.5387400810160751, 0.5643518970085803, 0.0, 1.0, 0.2114544369958446], 
reward next is 0.7885, 
noisyNet noise sample is [array([1.5882547], dtype=float32), -0.7026916]. 
=============================================
[2019-04-04 05:26:25,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5970119e-10 1.2795089e-09 9.5554690e-24 2.3810803e-10 1.8589025e-10
 6.5748405e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:25,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6278
[2019-04-04 05:26:25,385] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 63.16666666666667, 112.6666666666667, 793.0, 26.0, 25.9332442247323, 0.4757844382074192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2722200.0000, 
sim time next is 2722800.0000, 
raw observation next is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 26.0, 25.9437117050253, 0.4666115158403307, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2594644506001847, 0.6233333333333334, 0.376111111111111, 0.8795580110497238, 0.6666666666666666, 0.6619759754187751, 0.655537171946777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3148928], dtype=float32), 1.9313899]. 
=============================================
[2019-04-04 05:26:25,680] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7733450e-09 3.4570258e-10 2.1199319e-24 2.7320057e-10 4.1791495e-10
 3.7317968e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:25,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4771
[2019-04-04 05:26:25,698] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.333333333333334, 70.0, 0.0, 0.0, 26.0, 24.5669902791336, 0.2350758367448013, 0.0, 1.0, 44417.12844140778], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2680800.0000, 
sim time next is 2681400.0000, 
raw observation next is [-8.666666666666666, 69.5, 0.0, 0.0, 26.0, 24.56304797787883, 0.2274264213557241, 0.0, 1.0, 44412.28746141483], 
processed observation next is [1.0, 0.0, 0.22253000923361038, 0.695, 0.0, 0.0, 0.6666666666666666, 0.546920664823236, 0.5758088071185746, 0.0, 1.0, 0.21148708314959444], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.17398709], dtype=float32), -0.22584096]. 
=============================================
[2019-04-04 05:26:37,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5639993e-10 2.3346933e-09 2.6307317e-23 2.2176392e-09 5.4984095e-10
 7.8053444e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:37,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5390
[2019-04-04 05:26:37,591] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.666666666666667, 31.16666666666666, 0.0, 0.0, 26.0, 25.4656963487742, 0.2278788532146097, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2830200.0000, 
sim time next is 2830800.0000, 
raw observation next is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 26.0, 24.62582880961871, 0.1976389521448454, 1.0, 1.0, 196217.9094192962], 
processed observation next is [1.0, 0.782608695652174, 0.58264081255771, 0.3233333333333334, 0.0, 0.0, 0.6666666666666666, 0.5521524008015591, 0.5658796507149485, 1.0, 1.0, 0.9343709972347438], 
reward next is 0.0656, 
noisyNet noise sample is [array([-1.2345351], dtype=float32), 0.97920334]. 
=============================================
[2019-04-04 05:26:39,982] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.3876573e-10 4.3303705e-10 5.1499538e-24 3.6121817e-10 2.3311547e-10
 4.0297044e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:39,982] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4647
[2019-04-04 05:26:40,002] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 70.33333333333333, 0.0, 0.0, 26.0, 25.19707866327409, 0.3251921354236524, 0.0, 1.0, 46603.1377242784], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2850600.0000, 
sim time next is 2851200.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 25.1391783993023, 0.3113677699249824, 0.0, 1.0, 52206.07118347048], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5949315332751915, 0.6037892566416608, 0.0, 1.0, 0.24860033896890704], 
reward next is 0.7514, 
noisyNet noise sample is [array([0.48656812], dtype=float32), -1.0584058]. 
=============================================
[2019-04-04 05:26:40,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6652103e-09 1.0480387e-08 2.7617711e-24 9.3680708e-10 1.3623238e-09
 1.3080745e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:40,877] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8444
[2019-04-04 05:26:40,927] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.38448137357232, 0.4321434539896289, 0.0, 1.0, 42575.65767306398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2841600.0000, 
sim time next is 2842200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.43974332867863, 0.4360251709378224, 0.0, 1.0, 18761.70884112401], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6199786107232192, 0.6453417236459408, 0.0, 1.0, 0.0893414706720191], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.40789506], dtype=float32), 1.6395568]. 
=============================================
[2019-04-04 05:26:43,875] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4802976e-10 1.2390242e-09 9.1492402e-26 1.2719821e-10 1.9646322e-10
 1.3244879e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:43,875] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0482
[2019-04-04 05:26:43,902] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.85382388648476, 0.239914428866904, 0.0, 1.0, 55773.86891216321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2868600.0000, 
sim time next is 2869200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.8096240416827, 0.2404126181495265, 0.0, 1.0, 55777.89868559438], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.567468670140225, 0.5801375393831755, 0.0, 1.0, 0.26560904135997326], 
reward next is 0.7344, 
noisyNet noise sample is [array([1.1369759], dtype=float32), -0.21269049]. 
=============================================
[2019-04-04 05:26:52,792] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9954452e-08 2.8416030e-08 2.8687574e-22 7.7515638e-09 5.9413350e-09
 8.4755180e-12 9.9999988e-01], sum to 1.0000
[2019-04-04 05:26:52,794] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6757
[2019-04-04 05:26:52,821] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.11406771180776, 0.07546812518156033, 0.0, 1.0, 39962.04879645998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3037800.0000, 
sim time next is 3038400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.08025170683843, 0.06840716411479587, 0.0, 1.0, 40045.30071062675], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5066876422365357, 0.5228023880382653, 0.0, 1.0, 0.19069190814584167], 
reward next is 0.8093, 
noisyNet noise sample is [array([0.09503853], dtype=float32), 0.8134765]. 
=============================================
[2019-04-04 05:26:57,317] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.8020882e-11 1.0878221e-10 2.8649774e-27 2.6868643e-11 4.1527649e-12
 2.4255513e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:57,317] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9052
[2019-04-04 05:26:57,332] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.05805854504052, 0.7035865266191498, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.93342431216127, 0.6830509041992059, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5503231763619576, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6611186926801059, 0.7276836347330686, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14561898], dtype=float32), -0.6348061]. 
=============================================
[2019-04-04 05:26:59,197] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.0066825e-09 6.7414487e-09 4.7542722e-23 1.2417426e-09 4.9154392e-10
 2.1160909e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:59,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9940
[2019-04-04 05:26:59,233] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.42866328382127, 0.5417853259457495, 0.0, 1.0, 83301.59557899495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3276000.0000, 
sim time next is 3276600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.41351859206494, 0.4990034255490309, 0.0, 1.0, 67274.73179093328], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6177932160054116, 0.6663344751830103, 0.0, 1.0, 0.32035586567111085], 
reward next is 0.6796, 
noisyNet noise sample is [array([-1.0023077], dtype=float32), -0.65539783]. 
=============================================
[2019-04-04 05:26:59,914] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4677205e-12 6.5352313e-13 4.4501722e-30 8.8464023e-13 2.1852459e-12
 1.6525551e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 05:26:59,915] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7149
[2019-04-04 05:26:59,937] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 112.3333333333333, 811.6666666666666, 26.0, 27.31598668936194, 0.856802131636443, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3157800.0000, 
sim time next is 3158400.0000, 
raw observation next is [7.0, 100.0, 112.1666666666667, 808.8333333333334, 26.0, 27.33096829769233, 0.869076725621039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.373888888888889, 0.8937384898710866, 0.6666666666666666, 0.7775806914743608, 0.7896922418736797, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.65846074], dtype=float32), -1.2550817]. 
=============================================
[2019-04-04 05:27:01,141] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0435720e-09 4.8931414e-09 1.0128229e-22 8.5490592e-10 5.5519550e-10
 1.1197713e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:01,161] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3944
[2019-04-04 05:27:01,198] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 80.33333333333334, 0.0, 0.0, 26.0, 25.00562467940301, 0.2831062008675246, 0.0, 1.0, 53396.36910306997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3088200.0000, 
sim time next is 3088800.0000, 
raw observation next is [-0.6, 82.0, 0.0, 0.0, 26.0, 24.99176819956359, 0.2838153664125689, 0.0, 1.0, 50345.09180038461], 
processed observation next is [0.0, 0.782608695652174, 0.44598337950138506, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5826473499636325, 0.5946051221375229, 0.0, 1.0, 0.23973853238278386], 
reward next is 0.7603, 
noisyNet noise sample is [array([0.26385877], dtype=float32), 0.32357898]. 
=============================================
[2019-04-04 05:27:11,498] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4164980e-09 3.4407988e-09 2.3201884e-22 9.2660141e-10 3.5048300e-09
 1.8206190e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:11,498] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9295
[2019-04-04 05:27:11,528] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.3598477633458, 0.462259774709489, 1.0, 1.0, 25288.52036596868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3354000.0000, 
sim time next is 3354600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.27338732833458, 0.4484047795710936, 1.0, 1.0, 25226.23884930118], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6061156106945482, 0.6494682598570312, 1.0, 1.0, 0.12012494690143419], 
reward next is 0.8799, 
noisyNet noise sample is [array([0.3133548], dtype=float32), 0.5289976]. 
=============================================
[2019-04-04 05:27:11,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1340066e-09 3.1995335e-09 4.8252558e-23 2.8765021e-09 5.2195723e-09
 2.9368480e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:11,759] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3420
[2019-04-04 05:27:11,779] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.78840036169179, 0.5639609908622033, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3535200.0000, 
sim time next is 3535800.0000, 
raw observation next is [-1.0, 76.0, 0.0, 0.0, 26.0, 25.70721875866396, 0.5073779360134476, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6422682298886633, 0.6691259786711492, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37923962], dtype=float32), -0.682536]. 
=============================================
[2019-04-04 05:27:12,067] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3622120e-09 3.6655277e-09 8.2676491e-23 1.8461372e-09 1.2912917e-09
 1.7524992e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:12,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7515
[2019-04-04 05:27:12,099] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 26.0, 26.36828984600321, 0.6730847606401636, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.51813685043662, 0.6924886092527304, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3748845798707295, 0.5, 0.3322222222222222, 0.8022099447513812, 0.6666666666666666, 0.7098447375363849, 0.7308295364175769, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.48184872], dtype=float32), -0.75676376]. 
=============================================
[2019-04-04 05:27:12,168] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7047458e-09 4.9389834e-09 7.8387949e-24 2.5033371e-09 1.0996551e-09
 3.8181758e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:12,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9527
[2019-04-04 05:27:12,197] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25181239167098, 0.3792896494471843, 0.0, 1.0, 41806.94853437308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23692757623222, 0.3775296839661653, 0.0, 1.0, 41756.37014034262], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6030772980193516, 0.6258432279887217, 0.0, 1.0, 0.19883985781115532], 
reward next is 0.8012, 
noisyNet noise sample is [array([1.9509345], dtype=float32), 0.553869]. 
=============================================
[2019-04-04 05:27:14,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.5648009e-10 5.4921101e-10 1.9247368e-22 2.4927727e-10 1.9869946e-10
 1.1554577e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:14,298] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6926
[2019-04-04 05:27:14,321] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 73.5, 587.5, 26.0, 26.04204494118304, 0.6483160012345127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3340800.0000, 
sim time next is 3341400.0000, 
raw observation next is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 26.0, 26.39040021824707, 0.6772872881692384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46666666666666673, 0.23, 0.6173112338858197, 0.6666666666666666, 0.6992000181872559, 0.7257624293897461, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.56242293], dtype=float32), -0.7348294]. 
=============================================
[2019-04-04 05:27:15,506] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.4111074e-10 4.1998236e-09 9.4874486e-22 5.0638675e-09 3.5032659e-09
 1.2063714e-10 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:15,507] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8088
[2019-04-04 05:27:15,556] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 54.16666666666667, 0.0, 0.0, 26.0, 25.42584424957465, 0.5135989006805963, 1.0, 1.0, 74180.34452213389], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3347400.0000, 
sim time next is 3348000.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45967893653649, 0.5421454251278937, 1.0, 1.0, 46869.58616582953], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.621639911378041, 0.6807151417092978, 1.0, 1.0, 0.22318850555156922], 
reward next is 0.7768, 
noisyNet noise sample is [array([-1.3456898], dtype=float32), -0.66989636]. 
=============================================
[2019-04-04 05:27:15,566] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.708954]
 [75.91421 ]
 [75.7791  ]
 [76.2358  ]
 [76.74171 ]], R is [[75.67875671]
 [75.56872559]
 [75.22566986]
 [75.47341156]
 [75.71868134]].
[2019-04-04 05:27:16,786] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4929124e-10 5.4529364e-10 7.5340930e-25 2.1988238e-10 1.5186800e-10
 2.9023780e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:16,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0931
[2019-04-04 05:27:16,806] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 28.33333333333333, 251.6666666666666, 26.0, 26.38382372072548, 0.596409898305238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3431400.0000, 
sim time next is 3432000.0000, 
raw observation next is [2.0, 67.0, 20.16666666666666, 186.3333333333333, 26.0, 26.10969899539645, 0.5236053801492554, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.0672222222222222, 0.2058931860036832, 0.6666666666666666, 0.6758082496163708, 0.6745351267164185, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06700192], dtype=float32), 1.0711654]. 
=============================================
[2019-04-04 05:27:16,811] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.94169]
 [84.4686 ]
 [84.94451]
 [85.32823]
 [85.59846]], R is [[83.56842041]
 [83.73273468]
 [83.89540863]
 [84.05645752]
 [84.21589661]].
[2019-04-04 05:27:17,642] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3276225e-10 9.1376473e-10 4.4493688e-25 3.9899178e-10 4.2180692e-10
 3.8013012e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:27:17,646] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6539
[2019-04-04 05:27:17,686] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.52495739683878, 0.4597080939088098, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3461400.0000, 
sim time next is 3462000.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.54597201229974, 0.4528317847374296, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6288310010249782, 0.6509439282458098, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41236454], dtype=float32), 1.0486628]. 
=============================================
[2019-04-04 05:27:17,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.651276]
 [84.82118 ]
 [84.83888 ]
 [84.93682 ]
 [85.01986 ]], R is [[84.38220978]
 [84.53839111]
 [84.44591522]
 [84.36364746]
 [84.29246521]].
[2019-04-04 05:27:19,047] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 05:27:19,053] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:27:19,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:27:19,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run6
[2019-04-04 05:27:19,078] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:27:19,081] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:27:19,081] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:27:19,081] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:27:19,085] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run6
[2019-04-04 05:27:19,096] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run6
[2019-04-04 05:29:18,436] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26595384], dtype=float32), 0.13140936]
[2019-04-04 05:29:18,436] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.6836124255000001, 82.768774995, 106.4875605, 817.1032415, 26.0, 25.56734710916803, 0.5368658808249983, 1.0, 1.0, 55930.07957032616]
[2019-04-04 05:29:18,436] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:29:18,437] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.2307736e-11 7.8061418e-11 4.5706672e-26 3.2828101e-11 2.8408945e-11
 4.0970584e-14 1.0000000e+00], sampled 0.49498658362213244
[2019-04-04 05:29:32,861] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 05:29:38,201] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.26595384], dtype=float32), 0.13140936]
[2019-04-04 05:29:38,202] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.812538622666667, 22.59841774666667, 61.6745332, 805.9780439, 26.0, 26.22050892681454, 0.5688914384582333, 1.0, 1.0, 0.0]
[2019-04-04 05:29:38,202] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:29:38,203] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.06119036e-10 7.02879088e-10 1.23637144e-23 3.80510068e-10
 2.20067825e-10 8.53697213e-13 1.00000000e+00], sampled 0.6460328353934617
[2019-04-04 05:29:56,530] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 05:30:02,459] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 05:30:03,490] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 500000, evaluation results [500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 05:30:07,021] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.4721599e-09 1.8778270e-08 5.0898245e-22 7.9443980e-09 7.2877566e-09
 1.2712199e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:30:07,025] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6401
[2019-04-04 05:30:07,049] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 70.0, 3.0, 121.0, 26.0, 24.28495810116007, 0.190334928596643, 0.0, 1.0, 41542.06047585433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3569400.0000, 
sim time next is 3570000.0000, 
raw observation next is [-6.666666666666666, 70.0, 17.16666666666666, 171.6666666666667, 26.0, 24.24928226857997, 0.1925944391811048, 0.0, 1.0, 41553.42177211324], 
processed observation next is [0.0, 0.30434782608695654, 0.2779316712834719, 0.7, 0.0572222222222222, 0.18968692449355437, 0.6666666666666666, 0.5207735223816643, 0.5641981463937016, 0.0, 1.0, 0.19787343701006305], 
reward next is 0.8021, 
noisyNet noise sample is [array([1.3883706], dtype=float32), 0.7971687]. 
=============================================
[2019-04-04 05:30:07,062] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[76.265816]
 [76.0546  ]
 [76.22779 ]
 [76.38982 ]
 [76.53106 ]], R is [[76.65265656]
 [76.68830872]
 [76.72398376]
 [76.75987244]
 [76.79595947]].
[2019-04-04 05:30:20,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0169979e-10 1.4334871e-10 3.0999826e-24 4.7259507e-10 7.5449459e-11
 2.0235976e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:30:20,471] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1206
[2019-04-04 05:30:20,498] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.166666666666667, 49.5, 79.33333333333334, 644.0, 26.0, 25.50079354955123, 0.481116519598205, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3685800.0000, 
sim time next is 3686400.0000, 
raw observation next is [5.0, 50.0, 75.5, 613.5, 26.0, 25.49174683246943, 0.4776011811299606, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.6011080332409973, 0.5, 0.25166666666666665, 0.6779005524861879, 0.6666666666666666, 0.6243122360391192, 0.6592003937099868, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4370101], dtype=float32), 0.15716787]. 
=============================================
[2019-04-04 05:30:48,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3382274e-09 1.9538484e-09 2.4597081e-24 6.6463157e-10 2.3819764e-09
 2.1613076e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:30:48,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9139
[2019-04-04 05:30:48,187] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.33153292344273, 0.3229241469388159, 0.0, 1.0, 44598.87331556507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4251000.0000, 
sim time next is 4251600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.34046830198195, 0.3288955655182493, 0.0, 1.0, 41405.95315915846], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6117056918318292, 0.6096318551727498, 0.0, 1.0, 0.1971712055198022], 
reward next is 0.8028, 
noisyNet noise sample is [array([0.8639352], dtype=float32), -0.23121016]. 
=============================================
[2019-04-04 05:30:55,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5201067e-11 3.7676390e-11 5.7885639e-26 1.0579183e-11 9.6319419e-12
 1.1326088e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:30:55,937] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2324
[2019-04-04 05:30:55,966] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 30.66666666666667, 112.6666666666667, 818.0, 26.0, 26.19442147105134, 0.6437240877474631, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4110600.0000, 
sim time next is 4111200.0000, 
raw observation next is [3.0, 31.0, 111.0, 812.0, 26.0, 26.5646663513324, 0.6830233503075709, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.31, 0.37, 0.8972375690607735, 0.6666666666666666, 0.7137221959443666, 0.7276744501025236, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55005956], dtype=float32), -0.17368805]. 
=============================================
[2019-04-04 05:30:59,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2550889e-09 1.2591915e-09 9.0544279e-24 4.5418522e-10 2.1998431e-10
 1.8874503e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:30:59,902] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2453
[2019-04-04 05:30:59,911] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 39.5, 53.33333333333334, 421.0, 26.0, 25.49846554219806, 0.4198315953103999, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4207800.0000, 
sim time next is 4208400.0000, 
raw observation next is [2.0, 40.0, 46.0, 356.5, 26.0, 25.42625274265549, 0.4037471421699977, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.518005540166205, 0.4, 0.15333333333333332, 0.3939226519337017, 0.6666666666666666, 0.6188543952212907, 0.6345823807233325, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8877091], dtype=float32), -1.2180452]. 
=============================================
[2019-04-04 05:31:01,326] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.85301077e-11 4.41603004e-11 4.39955777e-27 4.32140781e-11
 1.48404813e-11 1.19269185e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 05:31:01,326] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4889
[2019-04-04 05:31:01,343] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 120.5, 834.5, 26.0, 25.20466068344134, 0.4046200821511599, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4276800.0000, 
sim time next is 4277400.0000, 
raw observation next is [7.0, 52.0, 120.3333333333333, 838.6666666666667, 26.0, 25.2243136395231, 0.4083584039364109, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.401111111111111, 0.9267034990791898, 0.6666666666666666, 0.602026136626925, 0.6361194679788036, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47201553], dtype=float32), -0.12134144]. 
=============================================
[2019-04-04 05:31:08,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3865556e-08 8.4183363e-09 1.2480139e-23 3.5921690e-09 1.0618761e-08
 2.5867721e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:31:08,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8156
[2019-04-04 05:31:08,271] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 36.66666666666666, 17.66666666666666, 26.0, 25.32360431052815, 0.3212496792100126, 0.0, 1.0, 39076.68303164816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4261800.0000, 
sim time next is 4262400.0000, 
raw observation next is [3.0, 49.0, 55.0, 26.5, 26.0, 25.32209189538507, 0.3258617240862324, 0.0, 1.0, 39004.32224053835], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.49, 0.18333333333333332, 0.029281767955801105, 0.6666666666666666, 0.6101743246154226, 0.6086205746954109, 0.0, 1.0, 0.18573486781208737], 
reward next is 0.8143, 
noisyNet noise sample is [array([0.8505496], dtype=float32), -0.53949374]. 
=============================================
[2019-04-04 05:31:13,165] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7675967e-10 1.4387634e-09 7.0764446e-23 3.6454945e-10 5.9511318e-10
 8.4760318e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:13,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1014
[2019-04-04 05:31:13,189] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.32077125428365, 0.4651063584534199, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4476000.0000, 
sim time next is 4476600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.18363646504341, 0.4430645664953452, 1.0, 1.0, 23885.29897833113], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5986363720869509, 0.6476881888317817, 1.0, 1.0, 0.11373951894443396], 
reward next is 0.8863, 
noisyNet noise sample is [array([0.17078972], dtype=float32), -0.39750308]. 
=============================================
[2019-04-04 05:31:15,704] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.0453473e-09 1.3389901e-08 1.0367859e-22 3.2246095e-09 2.9447782e-09
 6.6680728e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:15,704] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1924
[2019-04-04 05:31:15,717] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.35, 65.33333333333334, 0.0, 0.0, 26.0, 25.47292750336202, 0.4393524933925459, 0.0, 1.0, 18755.97308541553], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4587000.0000, 
sim time next is 4587600.0000, 
raw observation next is [-0.5, 65.66666666666667, 0.0, 0.0, 26.0, 25.4551296848968, 0.4407039966538104, 0.0, 1.0, 30509.17402297095], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6212608070747333, 0.6469013322179368, 0.0, 1.0, 0.14528178106176642], 
reward next is 0.8547, 
noisyNet noise sample is [array([1.1034015], dtype=float32), 1.7221714]. 
=============================================
[2019-04-04 05:31:16,899] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2393239e-09 3.9727501e-09 1.2758280e-22 1.1361450e-09 1.3835635e-09
 4.2830435e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:16,900] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1893
[2019-04-04 05:31:16,930] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.60474089722592, 0.5650338282331148, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4483800.0000, 
sim time next is 4484400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.58643883072681, 0.5545549333474481, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6322032358939008, 0.6848516444491494, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3457395], dtype=float32), -0.98073435]. 
=============================================
[2019-04-04 05:31:17,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2895639e-09 4.7136001e-10 1.7810805e-24 9.9696351e-10 3.0951894e-10
 2.9805870e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:17,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0101
[2019-04-04 05:31:17,313] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2666666666666667, 72.33333333333334, 113.0, 55.0, 26.0, 25.84199619743781, 0.4857563334423485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4524000.0000, 
sim time next is 4524600.0000, 
raw observation next is [-0.1333333333333333, 72.16666666666666, 115.0, 44.00000000000001, 26.0, 25.925798897685, 0.5087866390391557, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4589104339796861, 0.7216666666666666, 0.38333333333333336, 0.04861878453038675, 0.6666666666666666, 0.6604832414737499, 0.6695955463463852, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37319124], dtype=float32), 0.94315493]. 
=============================================
[2019-04-04 05:31:20,664] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1556065e-10 1.9888488e-10 4.1252882e-25 7.1530233e-11 1.0627687e-10
 3.7804915e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:20,664] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3824
[2019-04-04 05:31:20,682] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.833333333333333, 49.0, 126.3333333333333, 843.6666666666667, 26.0, 26.30837149178197, 0.6885656873060323, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4625400.0000, 
sim time next is 4626000.0000, 
raw observation next is [4.0, 49.0, 129.5, 836.0, 26.0, 26.47772273239582, 0.7272027421557768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5734072022160666, 0.49, 0.43166666666666664, 0.9237569060773481, 0.6666666666666666, 0.7064768943663182, 0.7424009140519257, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6597264], dtype=float32), 0.6983711]. 
=============================================
[2019-04-04 05:31:20,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.96171 ]
 [87.848755]
 [87.7894  ]
 [87.805   ]
 [87.81543 ]], R is [[88.23093414]
 [88.34862518]
 [88.4651413 ]
 [88.58049011]
 [88.69468689]].
[2019-04-04 05:31:22,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5812360e-10 8.2096252e-10 8.2712682e-25 1.3640677e-10 1.3418343e-10
 1.6561896e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:22,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5894
[2019-04-04 05:31:22,425] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.0, 264.0, 113.0, 26.0, 26.24099005927828, 0.6068687332363047, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4545000.0000, 
sim time next is 4545600.0000, 
raw observation next is [3.0, 46.33333333333334, 245.5, 96.16666666666667, 26.0, 26.32254778277995, 0.6177189472219645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.46333333333333343, 0.8183333333333334, 0.10626151012891345, 0.6666666666666666, 0.6935456485649958, 0.7059063157406548, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.63699], dtype=float32), 0.19118792]. 
=============================================
[2019-04-04 05:31:27,551] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0541770e-09 9.8673658e-10 1.0142374e-23 2.1493602e-09 3.3476108e-10
 3.6490524e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:27,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3063
[2019-04-04 05:31:27,570] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.50977626255818, 0.5029520814666312, 0.0, 1.0, 61976.57489161978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4666200.0000, 
sim time next is 4666800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.47649264658408, 0.5056794874646059, 0.0, 1.0, 63151.90519822166], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6230410538820067, 0.6685598291548686, 0.0, 1.0, 0.3007233580867698], 
reward next is 0.6993, 
noisyNet noise sample is [array([-0.7082155], dtype=float32), -0.14689699]. 
=============================================
[2019-04-04 05:31:31,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6485777e-10 6.5901423e-10 1.3638265e-24 1.2061022e-10 7.1253815e-11
 1.3028133e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:31,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9137
[2019-04-04 05:31:31,868] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.29541875261745, 0.4572771534422471, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4735800.0000, 
sim time next is 4736400.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.2274755260652, 0.4408655647366916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6022896271720999, 0.6469551882455639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7009388], dtype=float32), 0.35237685]. 
=============================================
[2019-04-04 05:31:38,140] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3960401e-08 6.7252437e-08 1.4811955e-21 3.4386154e-08 1.5252624e-08
 6.2373114e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:31:38,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4178
[2019-04-04 05:31:38,161] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.02455404340505, 0.2279256812161097, 0.0, 1.0, 38654.98369697621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4948200.0000, 
sim time next is 4948800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03037806962126, 0.2197346755060496, 0.0, 1.0, 38671.46466260744], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.585864839135105, 0.5732448918353499, 0.0, 1.0, 0.1841498317267021], 
reward next is 0.8159, 
noisyNet noise sample is [array([-1.1421481], dtype=float32), 1.1524386]. 
=============================================
[2019-04-04 05:31:40,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.8915649e-09 9.8122488e-09 2.2107480e-23 2.6045983e-09 1.1675755e-09
 7.7336982e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:40,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0146
[2019-04-04 05:31:40,091] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 50.0, 0.0, 0.0, 26.0, 25.23796087682423, 0.2845205912529093, 0.0, 1.0, 38825.56857207812], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4937400.0000, 
sim time next is 4938000.0000, 
raw observation next is [-1.666666666666667, 50.0, 0.0, 0.0, 26.0, 25.21315946380812, 0.2880748799539728, 0.0, 1.0, 38510.72981795999], 
processed observation next is [1.0, 0.13043478260869565, 0.4164358264081256, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6010966219840098, 0.5960249599846575, 0.0, 1.0, 0.1833844277045714], 
reward next is 0.8166, 
noisyNet noise sample is [array([-0.31988165], dtype=float32), -1.0086792]. 
=============================================
[2019-04-04 05:31:40,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.709015]
 [78.73048 ]
 [78.73417 ]
 [78.53322 ]
 [78.569115]], R is [[78.70622253]
 [78.73427582]
 [78.75647736]
 [78.75998688]
 [78.71198273]].
[2019-04-04 05:31:43,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0509202e-11 5.3656812e-11 3.3385052e-25 9.3890132e-11 1.5075278e-11
 1.6077527e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:43,432] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6858
[2019-04-04 05:31:43,467] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 25.16666666666667, 22.66666666666667, 202.6666666666667, 26.0, 27.23548611409671, 0.8395088197655118, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4989000.0000, 
sim time next is 4989600.0000, 
raw observation next is [6.0, 25.0, 17.0, 152.0, 26.0, 27.21147079733535, 0.6694008929908072, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 0.25, 0.056666666666666664, 0.16795580110497238, 0.6666666666666666, 0.7676225664446124, 0.7231336309969357, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10852919], dtype=float32), -0.22788385]. 
=============================================
[2019-04-04 05:31:45,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:45,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:45,159] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run5
[2019-04-04 05:31:48,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:48,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:48,595] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run5
[2019-04-04 05:31:50,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:50,826] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:50,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run5
[2019-04-04 05:31:53,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:53,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:53,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run5
[2019-04-04 05:31:53,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3653343e-12 5.6851308e-11 1.5211659e-27 1.6369022e-11 4.3669690e-12
 1.0398043e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:31:53,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3912
[2019-04-04 05:31:53,895] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 18.33333333333334, 79.33333333333333, 611.6666666666666, 26.0, 28.90812493778622, 1.186229048156571, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5070000.0000, 
sim time next is 5070600.0000, 
raw observation next is [12.0, 18.0, 76.0, 585.0, 26.0, 29.01404943062593, 1.195390355775805, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.18, 0.25333333333333335, 0.6464088397790055, 0.6666666666666666, 0.917837452552161, 0.8984634519252683, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.54293114], dtype=float32), -0.3386468]. 
=============================================
[2019-04-04 05:31:56,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:56,603] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:56,605] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run5
[2019-04-04 05:31:57,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:57,055] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:57,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run5
[2019-04-04 05:31:57,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:57,843] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:57,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run5
[2019-04-04 05:31:58,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:31:58,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:31:58,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run5
[2019-04-04 05:32:00,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:00,272] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:00,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run5
[2019-04-04 05:32:00,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:00,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:00,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run5
[2019-04-04 05:32:00,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:00,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:00,655] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run5
[2019-04-04 05:32:00,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:00,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:00,715] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run5
[2019-04-04 05:32:00,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:00,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:00,899] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run5
[2019-04-04 05:32:03,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:03,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:03,663] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run5
[2019-04-04 05:32:04,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:04,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:04,563] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run5
[2019-04-04 05:32:05,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:32:05,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:32:05,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run5
[2019-04-04 05:32:46,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2136141e-08 1.0230819e-07 2.1648352e-19 1.8630070e-08 8.3146944e-08
 4.5995543e-10 9.9999976e-01], sum to 1.0000
[2019-04-04 05:32:46,147] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7352
[2019-04-04 05:32:46,165] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.98770827583859, -0.4362840859319763, 0.0, 1.0, 48660.85589311972], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366600.0000, 
sim time next is 367200.0000, 
raw observation next is [-16.2, 78.0, 0.0, 0.0, 26.0, 21.88175869250681, -0.4563420629864208, 0.0, 1.0, 48767.91066054386], 
processed observation next is [1.0, 0.2608695652173913, 0.013850415512465375, 0.78, 0.0, 0.0, 0.6666666666666666, 0.32347989104223424, 0.3478859790045264, 0.0, 1.0, 0.23222814600258979], 
reward next is 0.7678, 
noisyNet noise sample is [array([1.0714619], dtype=float32), 0.3493204]. 
=============================================
[2019-04-04 05:32:52,054] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9478287e-10 9.4268804e-10 9.0505811e-27 1.7500984e-10 1.7437243e-10
 2.4225576e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:32:52,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5669
[2019-04-04 05:32:52,069] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.6, 91.66666666666666, 0.0, 0.0, 26.0, 24.84917100177972, 0.2361269433624806, 0.0, 1.0, 39656.3739808264], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 520800.0000, 
sim time next is 521400.0000, 
raw observation next is [4.8, 90.33333333333334, 0.0, 0.0, 26.0, 24.84579573376676, 0.2360770225604949, 0.0, 1.0, 39638.24496215793], 
processed observation next is [0.0, 0.0, 0.5955678670360112, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.5704829778138967, 0.5786923408534983, 0.0, 1.0, 0.18875354743884729], 
reward next is 0.8112, 
noisyNet noise sample is [array([0.15682042], dtype=float32), 0.3505834]. 
=============================================
[2019-04-04 05:32:57,424] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2254461e-09 2.9228984e-09 4.9353325e-23 8.4976709e-10 4.6735654e-10
 6.9187967e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:32:57,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5970
[2019-04-04 05:32:57,478] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 40.0, 16.66666666666667, 0.0, 26.0, 24.8683784027478, 0.2325869169961444, 1.0, 1.0, 58600.04206612171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 492000.0000, 
sim time next is 492600.0000, 
raw observation next is [1.1, 41.5, 13.33333333333334, 0.0, 26.0, 25.32245586147739, 0.2759084837573411, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.415, 0.04444444444444447, 0.0, 0.6666666666666666, 0.6102046551231158, 0.5919694945857804, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3482407], dtype=float32), -0.4103798]. 
=============================================
[2019-04-04 05:33:07,891] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.1002936e-10 6.8667294e-10 2.6084647e-25 2.4349020e-10 4.9606769e-10
 8.5659398e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:33:07,891] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0291
[2019-04-04 05:33:07,935] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.15, 83.5, 0.0, 0.0, 26.0, 24.63665028590652, 0.1964241464142303, 0.0, 1.0, 40302.14530255074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 534600.0000, 
sim time next is 535200.0000, 
raw observation next is [1.966666666666667, 84.0, 0.0, 0.0, 26.0, 24.61195633232596, 0.1915809887686618, 0.0, 1.0, 40381.73522624399], 
processed observation next is [0.0, 0.17391304347826086, 0.5170821791320407, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5509963610271633, 0.5638603295895539, 0.0, 1.0, 0.19229397726782851], 
reward next is 0.8077, 
noisyNet noise sample is [array([-0.26060486], dtype=float32), -1.7495203]. 
=============================================
[2019-04-04 05:33:09,046] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.8745920e-10 6.3556514e-11 2.9078319e-26 5.2216481e-11 2.2057209e-10
 7.9010649e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:33:09,047] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0908
[2019-04-04 05:33:09,110] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.0885339191361, 0.2880852036446447, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 551400.0000, 
sim time next is 552000.0000, 
raw observation next is [-0.2, 89.66666666666667, 125.6666666666667, 103.1666666666667, 26.0, 25.03746571155731, 0.2794296629935397, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4570637119113574, 0.8966666666666667, 0.418888888888889, 0.11399631675874773, 0.6666666666666666, 0.5864554759631092, 0.5931432209978466, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.212509], dtype=float32), -1.6377159]. 
=============================================
[2019-04-04 05:33:09,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[88.96232 ]
 [88.69026 ]
 [88.45631 ]
 [88.26793 ]
 [88.148544]], R is [[89.2769165 ]
 [89.38414764]
 [89.49031067]
 [89.59540558]
 [89.69945526]].
[2019-04-04 05:33:15,109] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7612233e-09 5.2939573e-09 2.5949302e-23 5.2663901e-10 1.8386102e-09
 6.1905173e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:33:15,109] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6013
[2019-04-04 05:33:15,142] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 70.0, 0.0, 0.0, 26.0, 24.71147059158906, 0.1608794861159111, 0.0, 1.0, 41781.43693051115], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 685800.0000, 
sim time next is 686400.0000, 
raw observation next is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.71867761717925, 0.1558110137431093, 0.0, 1.0, 41699.50157116403], 
processed observation next is [0.0, 0.9565217391304348, 0.35918744228993543, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.5598898014316042, 0.5519370045810365, 0.0, 1.0, 0.1985690551007811], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.39002398], dtype=float32), -0.44152433]. 
=============================================
[2019-04-04 05:33:17,916] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4187809e-09 3.7845900e-09 1.5620087e-22 4.2077053e-09 1.0932808e-09
 1.0404725e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:33:17,917] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5234
[2019-04-04 05:33:17,946] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 26.0, 24.68520167067255, 0.1480111871376663, 0.0, 1.0, 41630.89268653015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 687000.0000, 
sim time next is 687600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.65054447446813, 0.1402517569249488, 0.0, 1.0, 41573.78653294971], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5542120395390109, 0.5467505856416496, 0.0, 1.0, 0.19797041206166527], 
reward next is 0.8020, 
noisyNet noise sample is [array([2.3025393], dtype=float32), 0.13296025]. 
=============================================
[2019-04-04 05:33:27,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7063371e-11 3.4754297e-11 8.0651876e-26 6.6250366e-11 1.2676655e-11
 4.4614015e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:33:27,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0433
[2019-04-04 05:33:27,294] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 26.0, 25.8369665905087, 0.4438218008294426, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 742800.0000, 
sim time next is 743400.0000, 
raw observation next is [0.25, 46.0, 80.0, 714.0, 26.0, 25.89073588865145, 0.4519577484267781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46952908587257625, 0.46, 0.26666666666666666, 0.7889502762430939, 0.6666666666666666, 0.6575613240542874, 0.650652582808926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1900084], dtype=float32), 0.6452833]. 
=============================================
[2019-04-04 05:33:40,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6633409e-11 1.0638517e-10 2.7335106e-26 1.9288453e-11 1.5348937e-11
 3.8502181e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:33:40,495] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5449
[2019-04-04 05:33:40,537] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 26.0, 25.9545977390524, 0.4248328077476158, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 834600.0000, 
sim time next is 835200.0000, 
raw observation next is [-3.9, 82.0, 39.0, 0.0, 26.0, 26.01863073175444, 0.4054039283291568, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.13, 0.0, 0.6666666666666666, 0.6682192276462032, 0.6351346427763856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18372491], dtype=float32), 1.0543725]. 
=============================================
[2019-04-04 05:33:56,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.6668907e-11 1.6564607e-10 4.0718142e-27 3.5343235e-11 2.0601150e-10
 3.6594809e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:33:56,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4405
[2019-04-04 05:33:56,250] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.633333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.60140708788246, 0.5646036889605133, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1300800.0000, 
sim time next is 1301400.0000, 
raw observation next is [3.55, 92.5, 0.0, 0.0, 26.0, 25.49840340180864, 0.5544002896982843, 0.0, 1.0, 64969.91906742582], 
processed observation next is [1.0, 0.043478260869565216, 0.5609418282548477, 0.925, 0.0, 0.0, 0.6666666666666666, 0.6248669501507201, 0.6848000965660948, 0.0, 1.0, 0.309380566987742], 
reward next is 0.6906, 
noisyNet noise sample is [array([-1.6106644], dtype=float32), 0.45955163]. 
=============================================
[2019-04-04 05:34:07,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.3602757e-11 4.3153559e-10 2.0064967e-26 3.7017001e-11 7.6798283e-11
 1.1625947e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:07,162] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8582
[2019-04-04 05:34:07,193] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.68333333333333, 76.0, 0.0, 0.0, 26.0, 25.64900218269987, 0.6424397943303458, 0.0, 1.0, 34483.30313114921], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1126200.0000, 
sim time next is 1126800.0000, 
raw observation next is [10.5, 77.0, 0.0, 0.0, 26.0, 25.65036193176282, 0.6422468373032667, 0.0, 1.0, 27061.03723463996], 
processed observation next is [0.0, 0.043478260869565216, 0.7534626038781165, 0.77, 0.0, 0.0, 0.6666666666666666, 0.637530160980235, 0.7140822791010889, 0.0, 1.0, 0.12886208206971408], 
reward next is 0.8711, 
noisyNet noise sample is [array([0.37626666], dtype=float32), 0.4364776]. 
=============================================
[2019-04-04 05:34:07,704] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.36475145e-11 1.62265583e-11 1.48164258e-28 8.86037164e-12
 1.06350224e-11 2.04589645e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 05:34:07,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7861
[2019-04-04 05:34:07,718] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.41666666666667, 72.0, 0.0, 0.0, 26.0, 25.73457136783875, 0.6408098685650018, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1123800.0000, 
sim time next is 1124400.0000, 
raw observation next is [11.23333333333333, 73.0, 0.0, 0.0, 26.0, 25.69369235832968, 0.6387205114542581, 0.0, 1.0, 59485.98662951997], 
processed observation next is [0.0, 0.0, 0.7737765466297323, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6411410298608068, 0.7129068371514194, 0.0, 1.0, 0.2832666029977141], 
reward next is 0.7167, 
noisyNet noise sample is [array([-0.8466777], dtype=float32), 1.2677411]. 
=============================================
[2019-04-04 05:34:18,345] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.6675631e-11 8.3163330e-11 4.2854950e-27 6.3103425e-11 5.6904141e-11
 1.4376597e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:18,346] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2471
[2019-04-04 05:34:18,424] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8, 92.0, 18.0, 0.0, 26.0, 25.72109666919557, 0.5770021191048572, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 26.0, 25.95731973512394, 0.5913353779505082, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 0.6666666666666666, 0.6631099779269949, 0.6971117926501694, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06719375], dtype=float32), 0.116554074]. 
=============================================
[2019-04-04 05:34:24,191] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4742177e-09 3.8700256e-09 2.4887215e-24 5.8219735e-10 5.5247988e-09
 8.6896386e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:24,191] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5418
[2019-04-04 05:34:24,210] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.23694287956236, 0.4682504873827349, 0.0, 1.0, 39921.77526286623], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1389600.0000, 
sim time next is 1390200.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.2116630812203, 0.4686646501551406, 0.0, 1.0, 39781.14245797413], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6009719234350251, 0.6562215500517136, 0.0, 1.0, 0.18943401170463872], 
reward next is 0.8106, 
noisyNet noise sample is [array([-0.19923633], dtype=float32), 1.1036175]. 
=============================================
[2019-04-04 05:34:29,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5909599e-09 7.8548645e-10 1.3995601e-24 2.9417677e-10 4.2505843e-10
 1.0594929e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:29,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4839
[2019-04-04 05:34:29,830] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 95.83333333333334, 0.0, 0.0, 26.0, 25.40215472004422, 0.4926089124732131, 0.0, 1.0, 23140.66342715878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1393800.0000, 
sim time next is 1394400.0000, 
raw observation next is [-0.2, 96.66666666666667, 0.0, 0.0, 26.0, 25.45505319079764, 0.4912311244493791, 0.0, 1.0, 18762.37902655805], 
processed observation next is [1.0, 0.13043478260869565, 0.4570637119113574, 0.9666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6212544325664698, 0.6637437081497931, 0.0, 1.0, 0.08934466203122882], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.4131011], dtype=float32), -1.2177714]. 
=============================================
[2019-04-04 05:34:31,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3906732e-10 1.7237028e-09 2.6971361e-24 6.3306915e-11 1.3561735e-10
 4.6076180e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:31,213] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4585
[2019-04-04 05:34:31,237] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 25.52563006730549, 0.5417824215380899, 0.0, 1.0, 40198.02822588547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1461000.0000, 
sim time next is 1461600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.52021423161152, 0.5393788645385545, 0.0, 1.0, 39372.69452971256], 
processed observation next is [1.0, 0.9565217391304348, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6266845193009599, 0.6797929548461848, 0.0, 1.0, 0.18748902157005978], 
reward next is 0.8125, 
noisyNet noise sample is [array([-0.5702632], dtype=float32), 1.0841095]. 
=============================================
[2019-04-04 05:34:37,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9829096e-09 9.2788366e-10 1.2191172e-24 2.8905914e-10 6.4337136e-10
 6.1868345e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:37,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2749
[2019-04-04 05:34:37,870] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 100.6666666666667, 0.0, 26.0, 24.95478167219747, 0.3333353274222173, 0.0, 1.0, 33970.08874347828], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1766400.0000, 
sim time next is 1767000.0000, 
raw observation next is [-2.3, 87.0, 104.3333333333333, 0.0, 26.0, 24.92229441409252, 0.3316072295228289, 0.0, 1.0, 56758.80847902502], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3477777777777777, 0.0, 0.6666666666666666, 0.5768578678410433, 0.6105357431742763, 0.0, 1.0, 0.27028004037630965], 
reward next is 0.7297, 
noisyNet noise sample is [array([-2.0128524], dtype=float32), 0.9559628]. 
=============================================
[2019-04-04 05:34:37,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.23399 ]
 [81.22676 ]
 [81.239555]
 [81.26169 ]
 [81.17403 ]], R is [[81.22350311]
 [81.24950409]
 [81.34777832]
 [81.4450531 ]
 [81.42062378]].
[2019-04-04 05:34:43,958] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.9162259e-10 1.4138555e-09 1.1277356e-24 1.4906276e-10 1.2007380e-10
 1.8843594e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:43,959] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7446
[2019-04-04 05:34:43,977] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 26.0, 25.97078913577122, 0.5606615026904317, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1698000.0000, 
sim time next is 1698600.0000, 
raw observation next is [1.516666666666667, 82.16666666666666, 45.33333333333334, 0.0, 26.0, 26.01127117210967, 0.5624175417662769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5046168051708219, 0.8216666666666665, 0.15111111111111114, 0.0, 0.6666666666666666, 0.6676059310091391, 0.6874725139220924, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4162015], dtype=float32), -0.39129674]. 
=============================================
[2019-04-04 05:34:49,007] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.2271197e-11 5.1776866e-10 4.7879900e-26 2.1093181e-11 3.0391450e-11
 5.0383051e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:49,011] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1517
[2019-04-04 05:34:49,018] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.65166767937008, 0.591841409681305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.57808109625306, 0.5760091901266039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6315067580210885, 0.6920030633755346, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0556152], dtype=float32), 2.2601676]. 
=============================================
[2019-04-04 05:34:49,271] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7783311e-10 3.7778591e-10 2.6313212e-25 2.8375803e-11 1.4973212e-10
 1.2853205e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:34:49,271] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0517
[2019-04-04 05:34:49,325] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.35190556326903, 0.4716885264051141, 0.0, 1.0, 43196.18831257158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1728600.0000, 
sim time next is 1729200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.35123259415212, 0.4693487239639071, 0.0, 1.0, 43132.54395480958], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6126027161793433, 0.6564495746546357, 0.0, 1.0, 0.2053930664514742], 
reward next is 0.7946, 
noisyNet noise sample is [array([-1.519422], dtype=float32), 0.5906972]. 
=============================================
[2019-04-04 05:35:00,687] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9227985e-08 7.6482030e-09 8.8346090e-22 1.0838277e-09 4.0853796e-09
 2.4883534e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:35:00,687] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4841
[2019-04-04 05:35:00,716] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.75033799253831, 0.2388588845515238, 0.0, 1.0, 45643.30429965832], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1810200.0000, 
sim time next is 1810800.0000, 
raw observation next is [-5.0, 82.0, 0.0, 0.0, 26.0, 24.71751718588985, 0.2319262878770381, 0.0, 1.0, 45585.15786711282], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5597930988241542, 0.5773087626256793, 0.0, 1.0, 0.21707218031958486], 
reward next is 0.7829, 
noisyNet noise sample is [array([-1.1675565], dtype=float32), -0.09153136]. 
=============================================
[2019-04-04 05:35:01,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.48450905e-08 2.45974690e-08 1.08537099e-20 8.65665406e-09
 3.94550481e-09 4.93678431e-11 1.00000000e+00], sum to 1.0000
[2019-04-04 05:35:01,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4301
[2019-04-04 05:35:01,351] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.81440838939429, -0.04667588964282277, 0.0, 1.0, 44942.88705322437], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1914600.0000, 
sim time next is 1915200.0000, 
raw observation next is [-8.4, 78.0, 0.0, 0.0, 26.0, 23.80419309399711, -0.06023500572585672, 0.0, 1.0, 45039.67449760708], 
processed observation next is [1.0, 0.17391304347826086, 0.2299168975069252, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4836827578330925, 0.47992166475804776, 0.0, 1.0, 0.2144746404647956], 
reward next is 0.7855, 
noisyNet noise sample is [array([-1.5896116], dtype=float32), 1.086942]. 
=============================================
[2019-04-04 05:35:06,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4866087e-08 7.1812734e-09 4.8577817e-22 2.7866245e-09 3.2971836e-09
 2.3879969e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:35:06,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9476
[2019-04-04 05:35:06,335] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 86.33333333333333, 0.0, 0.0, 26.0, 23.95084767252502, 0.06139918729695543, 0.0, 1.0, 46791.32986269236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1825800.0000, 
sim time next is 1826400.0000, 
raw observation next is [-6.2, 85.66666666666667, 0.0, 0.0, 26.0, 23.91674208160165, 0.0651271927060629, 0.0, 1.0, 46825.69917471828], 
processed observation next is [0.0, 0.13043478260869565, 0.2908587257617729, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.49306184013347093, 0.5217090642353542, 0.0, 1.0, 0.22297951987961084], 
reward next is 0.7770, 
noisyNet noise sample is [array([-0.12200791], dtype=float32), 1.4190619]. 
=============================================
[2019-04-04 05:35:09,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3298468e-10 1.0696184e-09 7.8907314e-23 2.2954512e-10 2.5913813e-10
 1.0212182e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:35:09,433] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5692
[2019-04-04 05:35:09,484] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 201.5, 123.0, 26.0, 25.71021300561715, 0.3177933869733382, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1940400.0000, 
sim time next is 1941000.0000, 
raw observation next is [-5.5, 73.33333333333333, 211.6666666666667, 85.33333333333331, 26.0, 25.71001834781836, 0.3167115168002254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3102493074792244, 0.7333333333333333, 0.7055555555555557, 0.09429097605893184, 0.6666666666666666, 0.6425015289848632, 0.6055705056000752, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6745687], dtype=float32), 0.931026]. 
=============================================
[2019-04-04 05:35:09,488] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.41193 ]
 [77.80053 ]
 [78.155136]
 [78.438095]
 [78.861565]], R is [[77.25537872]
 [77.48282623]
 [77.70800018]
 [77.93092346]
 [78.15161133]].
[2019-04-04 05:35:11,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2250198e-09 5.4836470e-08 7.4482638e-21 7.4706756e-09 1.6917511e-08
 2.1381065e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 05:35:11,769] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9895
[2019-04-04 05:35:11,796] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.54403372631601, -0.1124635611246551, 0.0, 1.0, 45072.5090611969], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1920600.0000, 
sim time next is 1921200.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.51514084538955, -0.1256645797769173, 0.0, 1.0, 44997.64793343841], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.6666666666666666, 0.4595950704491291, 0.4581118067410275, 0.0, 1.0, 0.21427451396875435], 
reward next is 0.7857, 
noisyNet noise sample is [array([-0.03874826], dtype=float32), 0.30156916]. 
=============================================
[2019-04-04 05:35:14,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5385359e-09 6.2113430e-09 4.5299867e-23 4.3928039e-10 2.4375892e-09
 5.1787676e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:35:14,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6725
[2019-04-04 05:35:15,004] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.21575946943488, 0.08299146802713411, 0.0, 1.0, 41126.1696474472], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2005200.0000, 
sim time next is 2005800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.23833787499748, 0.08055929256018196, 0.0, 1.0, 41094.26167813214], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5198614895831234, 0.5268530975200606, 0.0, 1.0, 0.19568696037205782], 
reward next is 0.8043, 
noisyNet noise sample is [array([0.0117329], dtype=float32), 0.9525057]. 
=============================================
[2019-04-04 05:35:25,043] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6533532e-10 1.5731116e-09 1.7668761e-24 8.5018698e-10 3.5325190e-10
 7.9220782e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:35:25,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7064
[2019-04-04 05:35:25,107] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.766666666666666, 88.33333333333334, 54.33333333333333, 19.83333333333333, 26.0, 25.52222153921747, 0.3077901539414912, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2277600.0000, 
sim time next is 2278200.0000, 
raw observation next is [-8.583333333333334, 87.66666666666666, 63.66666666666666, 23.66666666666666, 26.0, 25.62898479729675, 0.307890572322278, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.22483841181902123, 0.8766666666666666, 0.2122222222222222, 0.026151012891344378, 0.6666666666666666, 0.6357487331080623, 0.6026301907740926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6278259], dtype=float32), -0.017695166]. 
=============================================
[2019-04-04 05:35:25,479] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 05:35:25,480] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:35:25,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:35:25,482] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run7
[2019-04-04 05:35:25,504] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:35:25,505] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:35:25,508] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:35:25,509] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:35:25,511] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run7
[2019-04-04 05:35:25,525] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run7
[2019-04-04 05:36:57,311] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.27738976], dtype=float32), 0.11467268]
[2019-04-04 05:36:57,311] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.983333333333333, 80.66666666666667, 0.0, 0.0, 26.0, 24.77275577659363, 0.3601224469555701, 0.0, 1.0, 45672.4428270063]
[2019-04-04 05:36:57,311] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:36:57,312] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.1147342e-09 3.0149623e-09 6.4697725e-24 7.1335848e-10 1.2689126e-09
 2.1752366e-12 1.0000000e+00], sampled 0.3427881271180333
[2019-04-04 05:37:40,540] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 05:38:01,870] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.27738976], dtype=float32), 0.11467268]
[2019-04-04 05:38:01,870] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.209184436333333, 83.82591679999999, 23.82219521666666, 5.662215903333332, 26.0, 25.22873644147261, 0.3420751038098548, 0.0, 1.0, 0.0]
[2019-04-04 05:38:01,870] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:38:01,872] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6513987e-09 1.7199098e-09 4.2464130e-24 5.5391247e-10 9.3590879e-10
 1.4657705e-12 1.0000000e+00], sampled 0.6720472471040235
[2019-04-04 05:38:10,425] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 05:38:12,557] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.27738976], dtype=float32), 0.11467268]
[2019-04-04 05:38:12,557] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.750682553000001, 46.7959987, 0.0, 0.0, 26.0, 24.48819700708434, 0.2165689678337445, 1.0, 1.0, 202492.0880658789]
[2019-04-04 05:38:12,557] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:38:12,558] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3603718e-08 1.9945947e-08 4.3021147e-22 6.1262404e-09 4.8968296e-09
 2.3372127e-11 1.0000000e+00], sampled 0.9229237273879598
[2019-04-04 05:38:15,999] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 05:38:17,047] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 600000, evaluation results [600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 05:38:19,362] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.0139106e-09 5.0585487e-09 5.5144211e-23 1.1532511e-09 2.0448017e-09
 2.6899498e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:19,362] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1115
[2019-04-04 05:38:19,420] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 86.33333333333333, 0.0, 0.0, 26.0, 24.11543424188593, 0.104119958614293, 0.0, 1.0, 43822.83469406805], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2092200.0000, 
sim time next is 2092800.0000, 
raw observation next is [-6.366666666666667, 85.66666666666667, 0.0, 0.0, 26.0, 24.17871140946733, 0.106952114422017, 0.0, 1.0, 43565.18079876307], 
processed observation next is [1.0, 0.21739130434782608, 0.28624192059095105, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5148926174556108, 0.535650704807339, 0.0, 1.0, 0.20745324189887174], 
reward next is 0.7925, 
noisyNet noise sample is [array([-2.436058], dtype=float32), -0.2961242]. 
=============================================
[2019-04-04 05:38:24,341] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.71522774e-10 3.96282873e-09 1.30595315e-23 1.53298196e-09
 1.98372666e-10 1.63466844e-11 1.00000000e+00], sum to 1.0000
[2019-04-04 05:38:24,341] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5538
[2019-04-04 05:38:24,379] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.283333333333333, 69.66666666666667, 172.3333333333333, 70.0, 26.0, 25.74319499846047, 0.3495057331877049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2285400.0000, 
sim time next is 2286000.0000, 
raw observation next is [-5.0, 68.0, 169.5, 80.0, 26.0, 25.74628130103325, 0.3495152671845472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.32409972299168976, 0.68, 0.565, 0.08839779005524862, 0.6666666666666666, 0.6455234417527708, 0.6165050890615157, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7504365], dtype=float32), -0.8592698]. 
=============================================
[2019-04-04 05:38:24,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.87509]
 [82.95028]
 [82.86164]
 [82.73026]
 [82.62247]], R is [[82.88362122]
 [83.05478668]
 [83.22424316]
 [83.39199829]
 [83.55808258]].
[2019-04-04 05:38:28,229] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6014268e-09 7.1056552e-09 2.1426404e-23 5.6903655e-09 3.0525271e-09
 1.7615912e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:28,230] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6686
[2019-04-04 05:38:28,251] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.14886341213963, 0.08516802762487825, 0.0, 1.0, 42074.23045600582], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175600.0000, 
sim time next is 2176200.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.12438756386508, 0.07403210653297965, 0.0, 1.0, 42051.50385924979], 
processed observation next is [1.0, 0.17391304347826086, 0.28393351800554023, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5103656303220901, 0.5246773688443266, 0.0, 1.0, 0.20024525647261804], 
reward next is 0.7998, 
noisyNet noise sample is [array([0.8377322], dtype=float32), 2.4121993]. 
=============================================
[2019-04-04 05:38:34,855] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1770864e-10 8.5519947e-10 6.5152477e-23 1.8096899e-10 8.1305213e-11
 1.6209320e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:34,855] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4686
[2019-04-04 05:38:34,922] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.2500000000000001, 44.0, 121.0, 60.0, 26.0, 26.29149078334521, 0.4924973194799805, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2298600.0000, 
sim time next is 2299200.0000, 
raw observation next is [0.5333333333333334, 43.66666666666666, 123.8333333333333, 57.0, 26.0, 26.29197305884876, 0.4982064880142248, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4773776546629733, 0.4366666666666666, 0.4127777777777777, 0.06298342541436464, 0.6666666666666666, 0.6909977549040635, 0.6660688293380749, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85935014], dtype=float32), -0.23442103]. 
=============================================
[2019-04-04 05:38:40,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6172542e-08 1.3104645e-08 1.6188259e-22 2.1006319e-09 2.8636595e-09
 1.7563030e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:40,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3708
[2019-04-04 05:38:40,772] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.20917106169112, 0.1183870251932469, 0.0, 1.0, 41035.51773328586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2352000.0000, 
sim time next is 2352600.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.19452350110534, 0.1134680641518058, 0.0, 1.0, 41092.6364387465], 
processed observation next is [0.0, 0.21739130434782608, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5162102917587784, 0.537822688050602, 0.0, 1.0, 0.1956792211368881], 
reward next is 0.8043, 
noisyNet noise sample is [array([-1.1738546], dtype=float32), 2.419668]. 
=============================================
[2019-04-04 05:38:44,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6021452e-09 9.8619513e-09 5.2330322e-23 1.3557631e-09 1.2130471e-09
 1.8805688e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:44,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0559
[2019-04-04 05:38:45,023] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.79001587163654, 0.268831548007806, 0.0, 1.0, 38522.95655818642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2336400.0000, 
sim time next is 2337000.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.8335804942405, 0.2654516646893542, 0.0, 1.0, 38530.42486251384], 
processed observation next is [0.0, 0.043478260869565216, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5694650411867084, 0.5884838882297848, 0.0, 1.0, 0.1834782136310183], 
reward next is 0.8165, 
noisyNet noise sample is [array([-0.514935], dtype=float32), 1.1564833]. 
=============================================
[2019-04-04 05:38:45,039] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.7532  ]
 [77.98119 ]
 [77.82835 ]
 [77.959236]
 [78.28208 ]], R is [[77.8117218 ]
 [77.85016632]
 [77.88827515]
 [77.92619324]
 [77.96405029]].
[2019-04-04 05:38:50,488] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5594033e-10 1.3558690e-09 4.2423492e-24 1.1727395e-10 5.3294041e-10
 2.2682995e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:50,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3948
[2019-04-04 05:38:50,539] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 62.5, 0.0, 0.0, 26.0, 24.97149352399255, 0.3584046472843103, 0.0, 1.0, 81202.30901069965], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2663400.0000, 
sim time next is 2664000.0000, 
raw observation next is [-1.2, 63.0, 0.0, 0.0, 26.0, 24.95541146597098, 0.3727447394239703, 0.0, 1.0, 64368.91402298488], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5796176221642483, 0.6242482464746567, 0.0, 1.0, 0.3065186382046899], 
reward next is 0.6935, 
noisyNet noise sample is [array([1.0749396], dtype=float32), 0.014438742]. 
=============================================
[2019-04-04 05:38:50,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.00824]
 [82.87164]
 [81.96918]
 [83.30305]
 [85.0345 ]], R is [[81.23695374]
 [81.03790283]
 [80.81261444]
 [80.88220978]
 [81.07338715]].
[2019-04-04 05:38:55,688] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4986369e-08 1.9973205e-08 5.0883515e-21 1.6876839e-08 1.0264748e-08
 1.5817307e-10 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:55,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2895
[2019-04-04 05:38:55,706] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 43.83333333333334, 0.0, 0.0, 26.0, 24.55372687203524, 0.1416368753548356, 0.0, 1.0, 43138.21355830986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2419800.0000, 
sim time next is 2420400.0000, 
raw observation next is [-5.8, 44.66666666666667, 0.0, 0.0, 26.0, 24.52508120054438, 0.1422611338007609, 0.0, 1.0, 43146.58261177186], 
processed observation next is [0.0, 0.0, 0.30193905817174516, 0.4466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5437567667120318, 0.547420377933587, 0.0, 1.0, 0.20545991719891363], 
reward next is 0.7945, 
noisyNet noise sample is [array([0.4948554], dtype=float32), -1.019899]. 
=============================================
[2019-04-04 05:38:56,396] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8030093e-11 1.5772780e-10 5.6017678e-25 1.1680248e-10 5.9748166e-12
 7.1772330e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:56,396] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5094
[2019-04-04 05:38:56,447] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 50.0, 63.66666666666666, 117.0, 26.0, 24.91240645806792, 0.379276257020172, 1.0, 1.0, 194744.333434249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2652000.0000, 
sim time next is 2652600.0000, 
raw observation next is [0.5, 50.0, 52.33333333333334, 110.0, 26.0, 25.39702870010244, 0.4366919619190482, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4764542936288089, 0.5, 0.17444444444444449, 0.12154696132596685, 0.6666666666666666, 0.6164190583418699, 0.6455639873063493, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5075752], dtype=float32), -0.69380426]. 
=============================================
[2019-04-04 05:38:57,985] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.32201758e-10 9.38448874e-10 5.85762749e-25 3.53512053e-10
 1.07828045e-10 2.85658921e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 05:38:57,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9602
[2019-04-04 05:38:58,019] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 64.66666666666667, 0.0, 0.0, 26.0, 25.24265086803084, 0.4437879486164287, 0.0, 1.0, 52633.80782400165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2667000.0000, 
sim time next is 2667600.0000, 
raw observation next is [-1.2, 65.0, 0.0, 0.0, 26.0, 25.36209804119012, 0.4587974201818167, 0.0, 1.0, 46979.20167103971], 
processed observation next is [1.0, 0.9130434782608695, 0.42936288088642666, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6135081700991766, 0.6529324733939389, 0.0, 1.0, 0.22371048414780814], 
reward next is 0.7763, 
noisyNet noise sample is [array([-0.94595283], dtype=float32), 0.34030217]. 
=============================================
[2019-04-04 05:38:58,592] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.5338297e-11 9.4710717e-10 5.4914501e-25 5.6465643e-10 2.7072419e-10
 1.3597931e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:58,594] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3416
[2019-04-04 05:38:58,710] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.05546926202134, 0.4104981286060509, 1.0, 1.0, 87070.66076756982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11391007458462, 0.4171017289266114, 0.0, 1.0, 37711.26777526681], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5928258395487184, 0.6390339096422039, 0.0, 1.0, 0.1795774655965086], 
reward next is 0.8204, 
noisyNet noise sample is [array([0.68938136], dtype=float32), 0.05002183]. 
=============================================
[2019-04-04 05:38:59,923] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5577830e-09 9.2273771e-09 8.2089718e-24 8.9238461e-10 2.2031068e-09
 7.1351388e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:38:59,924] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3462
[2019-04-04 05:38:59,950] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 45.66666666666667, 0.0, 0.0, 26.0, 24.98410758034917, 0.1888183636333582, 0.0, 1.0, 38511.08999087518], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2517600.0000, 
sim time next is 2518200.0000, 
raw observation next is [-1.7, 46.5, 0.0, 0.0, 26.0, 25.01493642069213, 0.1894419233600829, 0.0, 1.0, 38456.60243714591], 
processed observation next is [1.0, 0.13043478260869565, 0.4155124653739613, 0.465, 0.0, 0.0, 0.6666666666666666, 0.5845780350576776, 0.5631473077866943, 0.0, 1.0, 0.1831266782721234], 
reward next is 0.8169, 
noisyNet noise sample is [array([0.9182301], dtype=float32), 0.39734825]. 
=============================================
[2019-04-04 05:39:13,159] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.7585426e-11 3.5320810e-10 3.7044514e-24 5.3153208e-11 5.3017431e-11
 2.2584866e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:39:13,214] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6086
[2019-04-04 05:39:13,257] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.166666666666666, 64.0, 112.3333333333333, 787.0, 26.0, 25.98686026843815, 0.4714517048107085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2721000.0000, 
sim time next is 2721600.0000, 
raw observation next is [-8.0, 64.0, 112.5, 790.0, 26.0, 25.94380731560665, 0.4710848062256616, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.24099722991689754, 0.64, 0.375, 0.8729281767955801, 0.6666666666666666, 0.661983942967221, 0.6570282687418872, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.15249608], dtype=float32), 0.51560646]. 
=============================================
[2019-04-04 05:39:19,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.8900559e-09 7.0179738e-08 3.5151294e-22 5.3306870e-09 1.9282595e-08
 2.0478046e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:39:19,439] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5997
[2019-04-04 05:39:19,465] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.47707399810576, 0.1530432241703344, 0.0, 1.0, 40769.88268615626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2780400.0000, 
sim time next is 2781000.0000, 
raw observation next is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.47691519440482, 0.1493897302538978, 0.0, 1.0, 40795.89654772561], 
processed observation next is [1.0, 0.17391304347826086, 0.28254847645429365, 0.615, 0.0, 0.0, 0.6666666666666666, 0.5397429328670684, 0.5497965767512992, 0.0, 1.0, 0.19426617403678864], 
reward next is 0.8057, 
noisyNet noise sample is [array([-1.4530108], dtype=float32), -2.3955793]. 
=============================================
[2019-04-04 05:39:19,484] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.667534]
 [75.72188 ]
 [75.784935]
 [75.83514 ]
 [75.89817 ]], R is [[75.66909027]
 [75.71825409]
 [75.76695251]
 [75.81522369]
 [75.86303711]].
[2019-04-04 05:39:19,916] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4813245e-10 1.0561191e-09 5.5400786e-24 7.2361855e-10 1.7713361e-10
 9.2034410e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:39:19,917] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7606
[2019-04-04 05:39:19,953] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 28.0, 38.0, 61.0, 26.0, 25.63049867203118, 0.3917184562656946, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2826000.0000, 
sim time next is 2826600.0000, 
raw observation next is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 26.0, 25.75972350954894, 0.3971785715792159, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6241920590951062, 0.2833333333333334, 0.08999999999999997, 0.061878453038674036, 0.6666666666666666, 0.6466436257957451, 0.6323928571930719, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46696505], dtype=float32), -0.28528464]. 
=============================================
[2019-04-04 05:39:22,466] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9868444e-11 8.1953566e-10 2.5139870e-25 7.6639521e-11 5.1380473e-11
 4.1690561e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:39:22,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4426
[2019-04-04 05:39:22,540] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 0.0, 0.0, 26.0, 24.55325106825936, 0.3453595654161898, 1.0, 1.0, 87638.92892547887], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2833200.0000, 
sim time next is 2833800.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 0.0, 0.0, 26.0, 24.96556458546091, 0.3923579624536894, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.3816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5804637154550759, 0.6307859874845632, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54918677], dtype=float32), -0.07858348]. 
=============================================
[2019-04-04 05:39:22,961] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.416939e-10 9.758045e-10 3.406157e-25 8.699963e-11 3.062835e-11
 6.754666e-13 1.000000e+00], sum to 1.0000
[2019-04-04 05:39:22,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0192
[2019-04-04 05:39:23,008] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 56.5, 159.3333333333333, 324.6666666666666, 26.0, 25.92652492156721, 0.4110572763108416, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2800200.0000, 
sim time next is 2800800.0000, 
raw observation next is [-3.0, 55.0, 163.0, 370.5, 26.0, 25.93313409137191, 0.420393932301562, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.55, 0.5433333333333333, 0.4093922651933702, 0.6666666666666666, 0.661094507614326, 0.6401313107671873, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44834882], dtype=float32), -0.29829818]. 
=============================================
[2019-04-04 05:39:33,953] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5757198e-10 3.1819578e-09 8.6296831e-24 3.3137570e-10 3.7125983e-10
 2.4907977e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:39:33,953] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2708
[2019-04-04 05:39:33,973] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.79839551445584, 0.3073573801149441, 0.0, 1.0, 43185.62860831132], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2944200.0000, 
sim time next is 2944800.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.77293832344245, 0.2987411124022186, 0.0, 1.0, 43143.86363792477], 
processed observation next is [0.0, 0.08695652173913043, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5644115269535375, 0.5995803708007396, 0.0, 1.0, 0.20544696970440365], 
reward next is 0.7946, 
noisyNet noise sample is [array([-0.6272692], dtype=float32), 0.6772365]. 
=============================================
[2019-04-04 05:39:40,821] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8624284e-08 2.5618041e-08 2.1921825e-21 2.7618818e-09 4.3396331e-09
 2.1754657e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:39:40,822] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5297
[2019-04-04 05:39:40,864] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.00593145204431, 0.3208935042504026, 0.0, 1.0, 48857.25962106968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3004200.0000, 
sim time next is 3004800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.00394410345423, 0.3206570214202051, 0.0, 1.0, 40892.15462071516], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5836620086211859, 0.6068856738067351, 0.0, 1.0, 0.19472454581292933], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.5488024], dtype=float32), -1.471935]. 
=============================================
[2019-04-04 05:40:12,054] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1220831e-10 4.3416015e-10 1.6974973e-24 1.3008133e-10 1.8424040e-10
 2.9745612e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:12,055] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9196
[2019-04-04 05:40:12,060] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 51.5, 0.0, 0.0, 26.0, 25.91242723504627, 0.588421972439448, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3520200.0000, 
sim time next is 3520800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 26.04723469976449, 0.5910601754436543, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6706028916470409, 0.6970200584812182, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01868321], dtype=float32), 1.160383]. 
=============================================
[2019-04-04 05:40:23,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2664637e-10 2.6872604e-10 9.0586387e-25 1.3698612e-10 3.5038639e-11
 1.1369078e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:23,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8352
[2019-04-04 05:40:23,865] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.59095658542489, 0.5225379247932846, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3870600.0000, 
sim time next is 3871200.0000, 
raw observation next is [1.0, 51.00000000000001, 0.0, 0.0, 26.0, 25.54391985612515, 0.504533567308191, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.5100000000000001, 0.0, 0.0, 0.6666666666666666, 0.6286599880104292, 0.668177855769397, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.2431948], dtype=float32), -0.5145385]. 
=============================================
[2019-04-04 05:40:34,536] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.5399063e-10 6.7130661e-09 5.2330697e-24 2.9656602e-10 9.4257124e-10
 6.1300296e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:34,537] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0905
[2019-04-04 05:40:34,550] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 73.0, 0.0, 0.0, 26.0, 25.3215678305565, 0.4440821227897273, 0.0, 1.0, 71124.6288799931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3796800.0000, 
sim time next is 3797400.0000, 
raw observation next is [-3.0, 72.0, 0.0, 0.0, 26.0, 25.29923721814371, 0.4427279110458824, 0.0, 1.0, 55561.40620712566], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6082697681786424, 0.6475759703486275, 0.0, 1.0, 0.2645781247958365], 
reward next is 0.7354, 
noisyNet noise sample is [array([-1.0018587], dtype=float32), 1.7994441]. 
=============================================
[2019-04-04 05:40:34,561] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.9976582e-11 1.6925242e-10 1.6775800e-24 6.1137789e-11 4.9677661e-11
 6.6366400e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:34,569] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1249
[2019-04-04 05:40:34,589] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 70.0, 0.0, 0.0, 26.0, 25.5722858202066, 0.4930419994036752, 1.0, 1.0, 74765.33457734213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3784200.0000, 
sim time next is 3784800.0000, 
raw observation next is [-2.0, 69.0, 0.0, 0.0, 26.0, 25.43814747587798, 0.4772593069546264, 1.0, 1.0, 51257.83019201336], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6198456229898316, 0.6590864356515421, 1.0, 1.0, 0.2440849056762541], 
reward next is 0.7559, 
noisyNet noise sample is [array([-1.3392545], dtype=float32), -0.39011362]. 
=============================================
[2019-04-04 05:40:36,777] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.6634100e-09 1.5817405e-09 1.3879384e-23 4.8534587e-10 3.8343667e-10
 1.3133696e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:36,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0461
[2019-04-04 05:40:36,831] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.86919532449953, 0.2823700551186903, 0.0, 1.0, 43978.25098933714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3818400.0000, 
sim time next is 3819000.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.82938412158337, 0.2788454864524402, 0.0, 1.0, 43989.55834713976], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5691153434652808, 0.5929484954841467, 0.0, 1.0, 0.2094740873673322], 
reward next is 0.7905, 
noisyNet noise sample is [array([0.0358395], dtype=float32), 0.97577095]. 
=============================================
[2019-04-04 05:40:36,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.11857 ]
 [79.22786 ]
 [79.340576]
 [79.45325 ]
 [79.571884]], R is [[78.99055481]
 [78.99123383]
 [78.99209595]
 [78.99321747]
 [78.99462128]].
[2019-04-04 05:40:39,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2107519e-11 3.7964142e-11 1.5729909e-25 1.4782144e-11 4.6853264e-11
 7.9747718e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:39,236] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5133
[2019-04-04 05:40:39,315] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.666666666666667, 35.66666666666667, 93.16666666666666, 480.0, 26.0, 27.35460685273068, 0.571860824764417, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4119600.0000, 
sim time next is 4120200.0000, 
raw observation next is [3.5, 36.0, 93.0, 437.0, 26.0, 27.30280359010295, 0.7847012632251958, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5595567867036012, 0.36, 0.31, 0.48287292817679556, 0.6666666666666666, 0.7752336325085792, 0.761567087741732, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.52360266], dtype=float32), -0.40234885]. 
=============================================
[2019-04-04 05:40:44,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9751958e-09 1.4639406e-08 2.9684817e-23 7.2031364e-10 4.6061333e-09
 2.6851314e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:44,096] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0565
[2019-04-04 05:40:44,110] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.46565745605039, 0.5660325337742216, 0.0, 1.0, 18759.25425434499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3877200.0000, 
sim time next is 3877800.0000, 
raw observation next is [-1.0, 59.16666666666666, 0.0, 0.0, 26.0, 25.63943777345906, 0.5748000150630587, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.6366198144549218, 0.6916000050210195, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13394019], dtype=float32), -1.0948131]. 
=============================================
[2019-04-04 05:40:47,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7030567e-08 9.8320134e-09 1.9987860e-21 5.3099356e-09 1.3963825e-08
 1.0840173e-10 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:47,423] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8170
[2019-04-04 05:40:47,438] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.16666666666667, 58.83333333333334, 0.0, 0.0, 26.0, 24.72624676304253, 0.2764948966777076, 0.0, 1.0, 43842.50480785495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3978600.0000, 
sim time next is 3979200.0000, 
raw observation next is [-11.33333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.73370907817024, 0.2671236238937156, 0.0, 1.0, 43785.71735076367], 
processed observation next is [1.0, 0.043478260869565216, 0.14866112650046176, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5611424231808533, 0.5890412079645718, 0.0, 1.0, 0.20850341595601748], 
reward next is 0.7915, 
noisyNet noise sample is [array([-1.373634], dtype=float32), 1.5428776]. 
=============================================
[2019-04-04 05:40:51,004] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7896745e-09 6.7781687e-09 1.0136268e-22 2.5550588e-09 2.5265670e-10
 2.1563551e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:51,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9072
[2019-04-04 05:40:51,051] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 30.16666666666667, 0.0, 0.0, 26.0, 25.54215652697, 0.5030828750381496, 1.0, 1.0, 100072.7915087351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4042200.0000, 
sim time next is 4042800.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.55957584510033, 0.5227613974085432, 1.0, 1.0, 58395.98903145958], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6299646537583609, 0.6742537991361811, 1.0, 1.0, 0.2780761382450456], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.9174727], dtype=float32), 1.1743824]. 
=============================================
[2019-04-04 05:40:53,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2400970e-10 1.8737902e-09 1.8083694e-24 5.8494522e-11 5.4352428e-10
 1.1359244e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:53,498] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4442
[2019-04-04 05:40:53,536] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 39.5, 0.0, 0.0, 26.0, 25.32801196048116, 0.4949055331062746, 0.0, 1.0, 198133.9827187007], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4134600.0000, 
sim time next is 4135200.0000, 
raw observation next is [1.0, 38.33333333333334, 0.0, 0.0, 26.0, 25.35058506905516, 0.5406294629698394, 0.0, 1.0, 181710.7269039515], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.3833333333333334, 0.0, 0.0, 0.6666666666666666, 0.6125487557545967, 0.6802098209899464, 0.0, 1.0, 0.8652891757331024], 
reward next is 0.1347, 
noisyNet noise sample is [array([-0.5536825], dtype=float32), 0.8609053]. 
=============================================
[2019-04-04 05:40:55,442] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9366101e-10 2.2264717e-09 2.3447713e-23 4.9979915e-10 4.1941440e-11
 6.5567191e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:40:55,442] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8223
[2019-04-04 05:40:55,489] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 30.16666666666667, 0.0, 0.0, 26.0, 25.54215652697, 0.5030828750381496, 1.0, 1.0, 100072.7915087351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4042200.0000, 
sim time next is 4042800.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.55957584510033, 0.5227613974085432, 1.0, 1.0, 58395.98903145958], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6299646537583609, 0.6742537991361811, 1.0, 1.0, 0.2780761382450456], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.13246514], dtype=float32), 1.8287311]. 
=============================================
[2019-04-04 05:41:04,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5634976e-09 4.0543657e-10 4.4311411e-26 3.2066724e-11 2.9729322e-10
 4.7886613e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:04,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6672
[2019-04-04 05:41:04,527] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.199999999999999, 73.0, 0.0, 0.0, 26.0, 25.80819900145363, 0.4680271936802159, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4308000.0000, 
sim time next is 4308600.0000, 
raw observation next is [5.15, 73.0, 0.0, 0.0, 26.0, 25.81289900825547, 0.4599606856444589, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.605263157894737, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6510749173546225, 0.653320228548153, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6238751], dtype=float32), -0.7641078]. 
=============================================
[2019-04-04 05:41:05,132] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3095187e-10 2.0504519e-10 7.9549436e-26 2.4795933e-11 1.4337796e-10
 3.4380020e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:05,133] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1144
[2019-04-04 05:41:05,154] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.4, 62.0, 24.0, 228.0, 26.0, 25.48143270033765, 0.4085985728612342, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4296600.0000, 
sim time next is 4297200.0000, 
raw observation next is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 26.0, 25.43817077973623, 0.394429176408117, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6380424746075716, 0.6266666666666667, 0.06666666666666667, 0.20994475138121546, 0.6666666666666666, 0.6198475649780191, 0.631476392136039, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2226076], dtype=float32), 1.4020041]. 
=============================================
[2019-04-04 05:41:06,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6445397e-09 1.5220708e-09 4.1855571e-24 5.0961402e-10 5.7827737e-10
 1.8364904e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:06,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7004
[2019-04-04 05:41:06,619] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 69.0, 0.0, 0.0, 26.0, 24.96590766671659, 0.2903946596023546, 0.0, 1.0, 56602.74389250556], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4302000.0000, 
sim time next is 4302600.0000, 
raw observation next is [5.9, 69.66666666666667, 0.0, 0.0, 26.0, 24.92371655845263, 0.288846891237495, 0.0, 1.0, 56695.86860771634], 
processed observation next is [0.0, 0.8260869565217391, 0.626038781163435, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.5769763798710524, 0.596282297079165, 0.0, 1.0, 0.2699803267034111], 
reward next is 0.7300, 
noisyNet noise sample is [array([0.01365468], dtype=float32), 1.435611]. 
=============================================
[2019-04-04 05:41:07,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6645892e-10 1.3979780e-09 1.1905442e-25 2.8754749e-10 7.5718515e-10
 1.8371665e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:07,647] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1020
[2019-04-04 05:41:07,656] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.1, 66.0, 0.0, 0.0, 26.0, 25.74056321939412, 0.5578633293864073, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4413600.0000, 
sim time next is 4414200.0000, 
raw observation next is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 26.0, 25.66689542217449, 0.5615948010454662, 0.0, 1.0, 148432.7184830936], 
processed observation next is [1.0, 0.08695652173913043, 0.6265004616805172, 0.6616666666666667, 0.0, 0.0, 0.6666666666666666, 0.6389079518478743, 0.6871982670151554, 0.0, 1.0, 0.7068224689671124], 
reward next is 0.2932, 
noisyNet noise sample is [array([1.5047361], dtype=float32), 1.2483749]. 
=============================================
[2019-04-04 05:41:08,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9807636e-09 9.4854435e-10 4.9041737e-25 3.4970754e-10 1.4790169e-09
 1.1807172e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:08,041] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6725
[2019-04-04 05:41:08,053] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 25.41277053232727, 0.3583224584454687, 0.0, 1.0, 85519.45668198958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4344000.0000, 
sim time next is 4344600.0000, 
raw observation next is [2.966666666666666, 74.33333333333333, 0.0, 0.0, 26.0, 25.40052083562912, 0.365163275971975, 0.0, 1.0, 67023.3162507906], 
processed observation next is [1.0, 0.2608695652173913, 0.5447830101569714, 0.7433333333333333, 0.0, 0.0, 0.6666666666666666, 0.61671006963576, 0.6217210919906583, 0.0, 1.0, 0.31915864881328854], 
reward next is 0.6808, 
noisyNet noise sample is [array([1.3649967], dtype=float32), 1.3656547]. 
=============================================
[2019-04-04 05:41:10,551] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2751157e-10 1.0020613e-09 2.3808670e-25 2.5351884e-10 2.1839940e-10
 9.5088644e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:10,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9780
[2019-04-04 05:41:10,568] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 74.0, 20.83333333333334, 45.83333333333334, 26.0, 25.69367769758369, 0.5001989546350954, 1.0, 1.0, 30599.13808561741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4470000.0000, 
sim time next is 4470600.0000, 
raw observation next is [0.0, 73.0, 16.66666666666667, 36.66666666666667, 26.0, 25.36239030764239, 0.5179123757346854, 1.0, 1.0, 25712.83292266226], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.73, 0.05555555555555557, 0.04051565377532229, 0.6666666666666666, 0.6135325256368658, 0.6726374585782285, 1.0, 1.0, 0.12244206153648694], 
reward next is 0.8776, 
noisyNet noise sample is [array([0.5084919], dtype=float32), -0.027997414]. 
=============================================
[2019-04-04 05:41:11,048] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.9022753e-10 3.7728187e-10 1.9117443e-26 9.0843798e-11 1.0193369e-10
 2.9896113e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:11,048] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7823
[2019-04-04 05:41:11,058] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 20.0, 38.66666666666666, 26.0, 25.58536502160785, 0.501066288608615, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4434000.0000, 
sim time next is 4434600.0000, 
raw observation next is [2.0, 80.0, 39.99999999999999, 77.33333333333331, 26.0, 25.59993170287482, 0.4964355528073414, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.1333333333333333, 0.08545119705340698, 0.6666666666666666, 0.6333276419062349, 0.6654785176024471, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06260703], dtype=float32), -1.0709546]. 
=============================================
[2019-04-04 05:41:24,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7153995e-12 6.5688522e-12 6.2321244e-27 1.7722298e-12 3.6883218e-12
 4.0966109e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:24,437] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3643
[2019-04-04 05:41:24,443] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.666666666666667, 45.33333333333333, 182.0, 132.0, 26.0, 27.11030049170092, 0.8561833077421892, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4635600.0000, 
sim time next is 4636200.0000, 
raw observation next is [5.833333333333333, 44.16666666666667, 169.0, 135.0, 26.0, 27.39690669572712, 0.8781571497375736, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6241920590951062, 0.4416666666666667, 0.5633333333333334, 0.14917127071823205, 0.6666666666666666, 0.7830755579772601, 0.7927190499125246, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7352337], dtype=float32), 0.03623416]. 
=============================================
[2019-04-04 05:41:25,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7986379e-09 2.3333056e-08 5.5283644e-23 2.5641933e-09 3.1910987e-09
 5.8767917e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:25,754] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7093
[2019-04-04 05:41:25,800] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 80.33333333333334, 0.0, 0.0, 26.0, 24.97570029923953, 0.4457960372242545, 0.0, 1.0, 199457.506581233], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4738800.0000, 
sim time next is 4739400.0000, 
raw observation next is [-1.5, 81.5, 0.0, 0.0, 26.0, 24.99953747844299, 0.4788736098190463, 0.0, 1.0, 131838.9833938584], 
processed observation next is [1.0, 0.8695652173913043, 0.4210526315789474, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5832947898702491, 0.6596245366063488, 0.0, 1.0, 0.6278046828278971], 
reward next is 0.3722, 
noisyNet noise sample is [array([2.090787], dtype=float32), 1.9848146]. 
=============================================
[2019-04-04 05:41:28,565] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5443113e-09 9.0676835e-09 5.0456459e-24 7.7352252e-10 7.4761319e-10
 2.7161921e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:28,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4141
[2019-04-04 05:41:28,588] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 29.0, 0.0, 0.0, 26.0, 25.63063561196137, 0.5378137373259393, 0.0, 1.0, 196651.9742631053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5000400.0000, 
sim time next is 5001000.0000, 
raw observation next is [3.833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.60229522267966, 0.5601024377364064, 0.0, 1.0, 128920.7768984751], 
processed observation next is [1.0, 0.9130434782608695, 0.5687903970452447, 0.3033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6335246018899717, 0.6867008125788021, 0.0, 1.0, 0.6139084614213101], 
reward next is 0.3861, 
noisyNet noise sample is [array([0.27291822], dtype=float32), -0.08693471]. 
=============================================
[2019-04-04 05:41:28,627] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.22417]
 [79.02341]
 [78.18636]
 [78.31163]
 [78.71529]], R is [[80.58164978]
 [79.83939362]
 [79.10662842]
 [79.31555939]
 [79.52240753]].
[2019-04-04 05:41:31,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7014243e-09 3.7244039e-08 9.9798433e-23 2.8802902e-09 5.3383085e-09
 3.1066072e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:31,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-04 05:41:31,708] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.73913244433583, 0.2883634992373491, 0.0, 1.0, 40548.27411111189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4762800.0000, 
sim time next is 4763400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.67583451659301, 0.2766512762858221, 0.0, 1.0, 40588.72060253459], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5563195430494176, 0.592217092095274, 0.0, 1.0, 0.19327962191683137], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.23290952], dtype=float32), 2.078466]. 
=============================================
[2019-04-04 05:41:34,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:34,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:34,445] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run6
[2019-04-04 05:41:34,717] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.46123224e-11 9.19030685e-11 1.53259676e-23 2.74089813e-11
 1.08948746e-10 1.86596872e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 05:41:34,729] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2472
[2019-04-04 05:41:34,740] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.5, 82.0, 549.0, 26.0, 25.20350919887836, 0.4296344591591866, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4811400.0000, 
sim time next is 4812000.0000, 
raw observation next is [3.0, 35.0, 73.83333333333334, 488.3333333333333, 26.0, 25.21070579468701, 0.4205323611842324, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.35, 0.24611111111111114, 0.5395948434622467, 0.6666666666666666, 0.6008921495572507, 0.6401774537280774, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85056883], dtype=float32), -0.42969996]. 
=============================================
[2019-04-04 05:41:34,744] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.20049 ]
 [79.39347 ]
 [79.60987 ]
 [79.867386]
 [80.128975]], R is [[79.0552597 ]
 [79.26470947]
 [79.47206116]
 [79.67733765]
 [79.88056183]].
[2019-04-04 05:41:35,814] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8419587e-09 1.4170952e-09 1.7929631e-23 1.6297419e-09 9.5705299e-10
 2.7213622e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:35,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0993
[2019-04-04 05:41:35,831] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 60.0, 0.0, 0.0, 26.0, 25.10254561305516, 0.30168953352435, 0.0, 1.0, 39140.09427459569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4847400.0000, 
sim time next is 4848000.0000, 
raw observation next is [-2.666666666666667, 60.0, 0.0, 0.0, 26.0, 25.0746897737458, 0.2952570315860539, 0.0, 1.0, 39146.06270761979], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5895574811454832, 0.5984190105286846, 0.0, 1.0, 0.1864098224172371], 
reward next is 0.8136, 
noisyNet noise sample is [array([-0.04873698], dtype=float32), 0.21030407]. 
=============================================
[2019-04-04 05:41:35,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[79.34545 ]
 [79.415924]
 [79.531   ]
 [79.45669 ]
 [79.57722 ]], R is [[79.25952911]
 [79.28055573]
 [79.30136108]
 [79.32193756]
 [79.34225464]].
[2019-04-04 05:41:36,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:36,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:36,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run6
[2019-04-04 05:41:36,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5961990e-09 6.7872246e-09 1.8841135e-22 1.2117498e-09 6.0589342e-09
 4.3086238e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:36,836] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3809
[2019-04-04 05:41:36,864] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.54949225135038, 0.3474200590953562, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4932600.0000, 
sim time next is 4933200.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.5796644050667, 0.3328904893521553, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6316387004222251, 0.6109634964507185, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.89075446], dtype=float32), -0.41301343]. 
=============================================
[2019-04-04 05:41:39,037] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4390213e-09 3.8008885e-09 1.6267621e-22 1.7326554e-09 1.1292380e-09
 1.3666523e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:39,038] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2270
[2019-04-04 05:41:39,063] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 38.0, 0.0, 0.0, 26.0, 25.56361670766733, 0.474106263523172, 0.0, 1.0, 24070.94910037158], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5010000.0000, 
sim time next is 5010600.0000, 
raw observation next is [2.166666666666667, 39.0, 0.0, 0.0, 26.0, 25.53889872632751, 0.4672849876478825, 0.0, 1.0, 40705.9396728452], 
processed observation next is [1.0, 1.0, 0.5226223453370269, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6282415605272925, 0.6557616625492941, 0.0, 1.0, 0.1938378079659295], 
reward next is 0.8062, 
noisyNet noise sample is [array([0.11385554], dtype=float32), -1.6380272]. 
=============================================
[2019-04-04 05:41:39,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1624340e-09 6.8898092e-09 9.8008628e-23 9.6801731e-09 2.3420526e-09
 8.3628070e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:39,943] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6087
[2019-04-04 05:41:39,959] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.30702256213702, 0.298429718990426, 0.0, 1.0, 75984.14160278325], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4935000.0000, 
sim time next is 4935600.0000, 
raw observation next is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.29345449288046, 0.2949968820420242, 0.0, 1.0, 54685.57491511018], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.5, 0.0, 0.0, 0.6666666666666666, 0.607787874406705, 0.598332294014008, 0.0, 1.0, 0.26040749959576276], 
reward next is 0.7396, 
noisyNet noise sample is [array([-0.10086769], dtype=float32), -0.9404827]. 
=============================================
[2019-04-04 05:41:41,340] A3C_AGENT_WORKER-Thread-20 INFO:Local step 42500, global step 676411: loss 79.3428
[2019-04-04 05:41:41,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 42500, global step 676411: learning rate 0.0000
[2019-04-04 05:41:43,062] A3C_AGENT_WORKER-Thread-18 INFO:Local step 42500, global step 677152: loss 81.0701
[2019-04-04 05:41:43,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 42500, global step 677153: learning rate 0.0000
[2019-04-04 05:41:44,283] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7793657e-09 1.7743363e-09 2.5862535e-23 3.0516409e-10 6.9209288e-10
 8.0581678e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:44,283] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8847
[2019-04-04 05:41:44,299] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 36.0, 0.0, 0.0, 26.0, 25.4515951732586, 0.3667000070558522, 0.0, 1.0, 24401.86546759314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4917600.0000, 
sim time next is 4918200.0000, 
raw observation next is [0.8333333333333334, 36.5, 0.0, 0.0, 26.0, 25.43935808997933, 0.3625531793944313, 0.0, 1.0, 37280.64286759164], 
processed observation next is [0.0, 0.9565217391304348, 0.4856879039704525, 0.365, 0.0, 0.0, 0.6666666666666666, 0.6199465074982774, 0.6208510597981438, 0.0, 1.0, 0.17752687079805543], 
reward next is 0.8225, 
noisyNet noise sample is [array([0.4046096], dtype=float32), 0.34554264]. 
=============================================
[2019-04-04 05:41:44,492] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8284490e-11 5.4023317e-11 2.7729373e-26 3.4504611e-12 2.7897578e-12
 1.3125718e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:44,493] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0546
[2019-04-04 05:41:44,500] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.333333333333333, 24.66666666666667, 122.8333333333333, 861.6666666666667, 26.0, 27.12856165655557, 0.7266486556808721, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4969200.0000, 
sim time next is 4969800.0000, 
raw observation next is [6.5, 24.5, 123.0, 865.0, 26.0, 27.08718274018135, 0.7269359863683335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6426592797783934, 0.245, 0.41, 0.9558011049723757, 0.6666666666666666, 0.7572652283484459, 0.7423119954561112, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58768773], dtype=float32), 0.7465104]. 
=============================================
[2019-04-04 05:41:44,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:44,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:44,682] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run6
[2019-04-04 05:41:44,846] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.14727563e-10 1.97638864e-10 1.36349800e-25 1.00929036e-10
 1.12354848e-10 1.16905658e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 05:41:44,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7309
[2019-04-04 05:41:44,892] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.45, 86.0, 79.0, 0.0, 26.0, 24.45891312537651, 0.1662790722608276, 0.0, 1.0, 29345.69595567067], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 52200.0000, 
sim time next is 52800.0000, 
raw observation next is [7.366666666666667, 86.0, 74.16666666666667, 0.0, 26.0, 24.50632140861189, 0.1667588362405483, 0.0, 1.0, 18738.43220984866], 
processed observation next is [0.0, 0.6086956521739131, 0.6666666666666667, 0.86, 0.24722222222222223, 0.0, 0.6666666666666666, 0.5421934507176575, 0.5555862787468494, 0.0, 1.0, 0.0892306295707079], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.4988446], dtype=float32), 0.5941699]. 
=============================================
[2019-04-04 05:41:45,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:45,556] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:45,563] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run6
[2019-04-04 05:41:46,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:46,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:46,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run6
[2019-04-04 05:41:46,958] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7341762e-09 4.6415516e-09 5.5384908e-25 1.1955350e-09 1.8310299e-09
 7.6353756e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:46,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0281
[2019-04-04 05:41:46,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:46,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:46,990] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run6
[2019-04-04 05:41:47,019] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.05, 90.5, 0.0, 0.0, 26.0, 24.60460091242745, 0.2095965362051254, 0.0, 1.0, 40533.52185906207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 77400.0000, 
sim time next is 78000.0000, 
raw observation next is [0.8666666666666667, 92.33333333333334, 0.0, 0.0, 26.0, 24.58182071120883, 0.2054789175634937, 0.0, 1.0, 40472.2897358344], 
processed observation next is [0.0, 0.9130434782608695, 0.4866112650046169, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5484850592674025, 0.5684929725211646, 0.0, 1.0, 0.19272518921825904], 
reward next is 0.8073, 
noisyNet noise sample is [array([-0.33063415], dtype=float32), -0.6777215]. 
=============================================
[2019-04-04 05:41:47,075] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.76774 ]
 [87.91688 ]
 [88.092   ]
 [88.290215]
 [88.50731 ]], R is [[87.55867767]
 [87.49007416]
 [87.42189026]
 [87.35414886]
 [87.28694153]].
[2019-04-04 05:41:49,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:49,164] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:49,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run6
[2019-04-04 05:41:49,735] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5948027e-10 1.0943328e-08 1.7933591e-24 3.0578226e-10 6.0760358e-10
 2.8177369e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:41:49,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1587
[2019-04-04 05:41:49,741] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.9, 19.0, 0.0, 0.0, 26.0, 26.99979394497655, 0.8284123256354455, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5088000.0000, 
sim time next is 5088600.0000, 
raw observation next is [8.85, 19.0, 0.0, 0.0, 26.0, 26.93989040781812, 0.8161162524804394, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7077562326869806, 0.19, 0.0, 0.0, 0.6666666666666666, 0.7449908673181765, 0.7720387508268131, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.90656954], dtype=float32), -1.2219824]. 
=============================================
[2019-04-04 05:41:49,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:49,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:49,980] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run6
[2019-04-04 05:41:50,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:50,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:50,519] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run6
[2019-04-04 05:41:51,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:51,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:51,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run6
[2019-04-04 05:41:51,344] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:51,344] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:51,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run6
[2019-04-04 05:41:51,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:51,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:51,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run6
[2019-04-04 05:41:51,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:51,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:51,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run6
[2019-04-04 05:41:52,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:52,041] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:52,062] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run6
[2019-04-04 05:41:52,136] A3C_AGENT_WORKER-Thread-19 INFO:Local step 42500, global step 680062: loss 82.7538
[2019-04-04 05:41:52,137] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 42500, global step 680062: learning rate 0.0000
[2019-04-04 05:41:52,222] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:52,222] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:52,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run6
[2019-04-04 05:41:52,703] A3C_AGENT_WORKER-Thread-5 INFO:Local step 42500, global step 680118: loss 84.0424
[2019-04-04 05:41:52,704] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 42500, global step 680118: learning rate 0.0000
[2019-04-04 05:41:54,376] A3C_AGENT_WORKER-Thread-2 INFO:Local step 42500, global step 680297: loss 86.3294
[2019-04-04 05:41:54,379] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 42500, global step 680297: learning rate 0.0000
[2019-04-04 05:41:54,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:41:54,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:41:54,725] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run6
[2019-04-04 05:41:55,003] A3C_AGENT_WORKER-Thread-16 INFO:Local step 42500, global step 680357: loss 84.7842
[2019-04-04 05:41:55,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 42500, global step 680357: learning rate 0.0000
[2019-04-04 05:41:58,627] A3C_AGENT_WORKER-Thread-12 INFO:Local step 42500, global step 680684: loss 85.8164
[2019-04-04 05:41:58,630] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 42500, global step 680684: learning rate 0.0000
[2019-04-04 05:41:59,361] A3C_AGENT_WORKER-Thread-10 INFO:Local step 42500, global step 680778: loss 84.5226
[2019-04-04 05:41:59,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 42500, global step 680778: learning rate 0.0000
[2019-04-04 05:42:00,098] A3C_AGENT_WORKER-Thread-13 INFO:Local step 42500, global step 680900: loss 83.8273
[2019-04-04 05:42:00,098] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 42500, global step 680900: learning rate 0.0000
[2019-04-04 05:42:00,892] A3C_AGENT_WORKER-Thread-11 INFO:Local step 42500, global step 681051: loss 83.3971
[2019-04-04 05:42:00,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 42500, global step 681051: learning rate 0.0000
[2019-04-04 05:42:01,019] A3C_AGENT_WORKER-Thread-17 INFO:Local step 42500, global step 681081: loss 83.0728
[2019-04-04 05:42:01,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 42500, global step 681081: learning rate 0.0000
[2019-04-04 05:42:01,364] A3C_AGENT_WORKER-Thread-3 INFO:Local step 42500, global step 681185: loss 83.6543
[2019-04-04 05:42:01,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 42500, global step 681190: learning rate 0.0000
[2019-04-04 05:42:01,613] A3C_AGENT_WORKER-Thread-4 INFO:Local step 42500, global step 681266: loss 83.4205
[2019-04-04 05:42:01,614] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 42500, global step 681268: learning rate 0.0000
[2019-04-04 05:42:01,739] A3C_AGENT_WORKER-Thread-6 INFO:Local step 42500, global step 681311: loss 84.6470
[2019-04-04 05:42:01,744] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 42500, global step 681314: learning rate 0.0000
[2019-04-04 05:42:01,766] A3C_AGENT_WORKER-Thread-14 INFO:Local step 42500, global step 681319: loss 83.7410
[2019-04-04 05:42:01,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 42500, global step 681319: learning rate 0.0000
[2019-04-04 05:42:03,628] A3C_AGENT_WORKER-Thread-15 INFO:Local step 42500, global step 681801: loss 84.9096
[2019-04-04 05:42:03,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 42500, global step 681801: learning rate 0.0000
[2019-04-04 05:42:11,137] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43000, global step 683887: loss 0.0603
[2019-04-04 05:42:11,137] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43000, global step 683887: learning rate 0.0000
[2019-04-04 05:42:13,056] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43000, global step 684373: loss 0.0589
[2019-04-04 05:42:13,057] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43000, global step 684373: learning rate 0.0000
[2019-04-04 05:42:22,441] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43000, global step 686771: loss 0.0691
[2019-04-04 05:42:22,445] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43000, global step 686771: learning rate 0.0000
[2019-04-04 05:42:23,105] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43000, global step 686912: loss 0.0594
[2019-04-04 05:42:23,109] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43000, global step 686913: learning rate 0.0000
[2019-04-04 05:42:25,262] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43000, global step 687415: loss 0.0599
[2019-04-04 05:42:25,264] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43000, global step 687415: learning rate 0.0000
[2019-04-04 05:42:25,781] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43000, global step 687574: loss 0.0708
[2019-04-04 05:42:25,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43000, global step 687574: learning rate 0.0000
[2019-04-04 05:42:28,386] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43000, global step 688418: loss 0.0372
[2019-04-04 05:42:28,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43000, global step 688418: learning rate 0.0000
[2019-04-04 05:42:29,666] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43000, global step 688716: loss 0.0659
[2019-04-04 05:42:29,666] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43000, global step 688716: learning rate 0.0000
[2019-04-04 05:42:30,320] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43000, global step 688877: loss 0.0677
[2019-04-04 05:42:30,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43000, global step 688877: learning rate 0.0000
[2019-04-04 05:42:31,050] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43000, global step 689031: loss 0.0760
[2019-04-04 05:42:31,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43000, global step 689031: learning rate 0.0000
[2019-04-04 05:42:31,381] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43000, global step 689093: loss 0.0550
[2019-04-04 05:42:31,382] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43000, global step 689093: learning rate 0.0000
[2019-04-04 05:42:31,394] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43000, global step 689094: loss 0.0498
[2019-04-04 05:42:31,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43000, global step 689094: learning rate 0.0000
[2019-04-04 05:42:31,870] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43000, global step 689192: loss 0.0485
[2019-04-04 05:42:31,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43000, global step 689192: learning rate 0.0000
[2019-04-04 05:42:32,092] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43000, global step 689238: loss 0.0518
[2019-04-04 05:42:32,094] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43000, global step 689238: learning rate 0.0000
[2019-04-04 05:42:32,213] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43000, global step 689265: loss 0.0479
[2019-04-04 05:42:32,215] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43000, global step 689265: learning rate 0.0000
[2019-04-04 05:42:34,383] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43000, global step 689810: loss 0.0457
[2019-04-04 05:42:34,390] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43000, global step 689810: learning rate 0.0000
[2019-04-04 05:42:40,999] A3C_AGENT_WORKER-Thread-20 INFO:Local step 43500, global step 691610: loss 0.0198
[2019-04-04 05:42:40,999] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 43500, global step 691610: learning rate 0.0000
[2019-04-04 05:42:41,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2059882e-08 8.8218407e-08 2.3898083e-20 1.2627805e-08 2.6017033e-09
 2.7334285e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 05:42:41,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0624
[2019-04-04 05:42:41,363] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.41781588566442, 0.3660192702500362, 1.0, 1.0, 23842.12272650197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 410400.0000, 
sim time next is 411000.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.56189653091911, 0.3671376187320992, 1.0, 1.0, 34523.80684485524], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6301580442432592, 0.6223792062440331, 1.0, 1.0, 0.16439908021359637], 
reward next is 0.8356, 
noisyNet noise sample is [array([0.59674764], dtype=float32), 0.123623796]. 
=============================================
[2019-04-04 05:42:41,374] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.25237 ]
 [72.11395 ]
 [71.96095 ]
 [71.182556]
 [70.72188 ]], R is [[72.5079422 ]
 [72.66932678]
 [72.62149048]
 [71.94412994]
 [71.34442902]].
[2019-04-04 05:42:42,161] A3C_AGENT_WORKER-Thread-18 INFO:Local step 43500, global step 691929: loss 0.0249
[2019-04-04 05:42:42,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 43500, global step 691929: learning rate 0.0000
[2019-04-04 05:42:51,284] A3C_AGENT_WORKER-Thread-5 INFO:Local step 43500, global step 694654: loss 0.2106
[2019-04-04 05:42:51,287] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 43500, global step 694654: learning rate 0.0000
[2019-04-04 05:42:51,449] A3C_AGENT_WORKER-Thread-19 INFO:Local step 43500, global step 694720: loss 0.1950
[2019-04-04 05:42:51,449] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 43500, global step 694720: learning rate 0.0000
[2019-04-04 05:42:52,883] A3C_AGENT_WORKER-Thread-2 INFO:Local step 43500, global step 695296: loss 0.1294
[2019-04-04 05:42:52,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 43500, global step 695297: learning rate 0.0000
[2019-04-04 05:42:53,368] A3C_AGENT_WORKER-Thread-16 INFO:Local step 43500, global step 695469: loss 0.1149
[2019-04-04 05:42:53,368] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 43500, global step 695469: learning rate 0.0000
[2019-04-04 05:42:54,018] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1420596e-10 1.3629163e-09 3.2483077e-24 2.3911065e-10 8.3133035e-11
 5.1677434e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:42:54,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2371
[2019-04-04 05:42:54,070] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.22742281717996, 0.3958715926577653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750600.0000, 
sim time next is 751200.0000, 
raw observation next is [-2.066666666666666, 51.0, 56.66666666666667, 2.833333333333333, 26.0, 25.67409943608536, 0.4271760999592983, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40535549399815335, 0.51, 0.1888888888888889, 0.0031307550644567215, 0.6666666666666666, 0.6395082863404467, 0.6423920333197661, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2135618], dtype=float32), -0.07710243]. 
=============================================
[2019-04-04 05:42:56,864] A3C_AGENT_WORKER-Thread-12 INFO:Local step 43500, global step 696448: loss 0.0601
[2019-04-04 05:42:56,864] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 43500, global step 696448: learning rate 0.0000
[2019-04-04 05:42:57,746] A3C_AGENT_WORKER-Thread-10 INFO:Local step 43500, global step 696763: loss 0.0403
[2019-04-04 05:42:57,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 43500, global step 696763: learning rate 0.0000
[2019-04-04 05:42:58,034] A3C_AGENT_WORKER-Thread-13 INFO:Local step 43500, global step 696872: loss 0.0337
[2019-04-04 05:42:58,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 43500, global step 696872: learning rate 0.0000
[2019-04-04 05:42:58,225] A3C_AGENT_WORKER-Thread-3 INFO:Local step 43500, global step 696948: loss 0.0293
[2019-04-04 05:42:58,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 43500, global step 696948: learning rate 0.0000
[2019-04-04 05:42:58,628] A3C_AGENT_WORKER-Thread-14 INFO:Local step 43500, global step 697091: loss 0.0419
[2019-04-04 05:42:58,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 43500, global step 697091: learning rate 0.0000
[2019-04-04 05:42:59,246] A3C_AGENT_WORKER-Thread-11 INFO:Local step 43500, global step 697335: loss 0.0408
[2019-04-04 05:42:59,246] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 43500, global step 697335: learning rate 0.0000
[2019-04-04 05:42:59,815] A3C_AGENT_WORKER-Thread-17 INFO:Local step 43500, global step 697540: loss 0.0327
[2019-04-04 05:42:59,816] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 43500, global step 697540: learning rate 0.0000
[2019-04-04 05:42:59,941] A3C_AGENT_WORKER-Thread-4 INFO:Local step 43500, global step 697587: loss 0.0415
[2019-04-04 05:42:59,942] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 43500, global step 697587: learning rate 0.0000
[2019-04-04 05:42:59,995] A3C_AGENT_WORKER-Thread-6 INFO:Local step 43500, global step 697608: loss 0.0343
[2019-04-04 05:42:59,995] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 43500, global step 697608: learning rate 0.0000
[2019-04-04 05:43:00,691] A3C_AGENT_WORKER-Thread-15 INFO:Local step 43500, global step 697855: loss 0.0299
[2019-04-04 05:43:00,691] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 43500, global step 697855: learning rate 0.0000
[2019-04-04 05:43:05,372] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44000, global step 699352: loss 0.0853
[2019-04-04 05:43:05,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44000, global step 699352: learning rate 0.0000
[2019-04-04 05:43:06,988] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 05:43:06,989] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:43:06,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:06,990] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:43:06,992] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:06,993] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:43:06,997] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:43:06,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run8
[2019-04-04 05:43:07,020] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run8
[2019-04-04 05:43:07,035] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run8
[2019-04-04 05:43:15,064] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2842274], dtype=float32), 0.09657689]
[2019-04-04 05:43:15,064] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-8.2, 61.0, 139.0, 504.6666666666667, 26.0, 25.75765824159769, 0.3921245316077059, 1.0, 1.0, 0.0]
[2019-04-04 05:43:15,064] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:43:15,064] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.3158155e-10 2.1197771e-10 2.9835548e-25 7.3504293e-11 5.9597730e-11
 1.8580169e-13 1.0000000e+00], sampled 0.7040565077574519
[2019-04-04 05:44:39,145] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2842274], dtype=float32), 0.09657689]
[2019-04-04 05:44:39,145] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.15, 64.5, 0.0, 0.0, 26.0, 25.18323028699766, 0.385111697466983, 1.0, 1.0, 47704.46265875052]
[2019-04-04 05:44:39,145] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:44:39,146] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.2884568e-10 3.7767570e-10 2.4771660e-24 1.3050355e-10 1.0855836e-10
 4.8489728e-13 1.0000000e+00], sampled 0.4335821533278391
[2019-04-04 05:44:47,680] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2842274], dtype=float32), 0.09657689]
[2019-04-04 05:44:47,680] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.3, 77.0, 0.0, 0.0, 26.0, 24.49942329133836, 0.1333582527072135, 0.0, 1.0, 39407.55334041474]
[2019-04-04 05:44:47,680] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:44:47,681] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.1032706e-09 3.3156871e-09 9.1130931e-24 1.0161190e-09 1.4490552e-09
 2.4634040e-12 1.0000000e+00], sampled 0.9595354501173329
[2019-04-04 05:44:47,851] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2842274], dtype=float32), 0.09657689]
[2019-04-04 05:44:47,851] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.800000000000001, 78.16666666666666, 0.0, 0.0, 26.0, 24.27622080016734, 0.1490202953310122, 0.0, 1.0, 42612.04239390764]
[2019-04-04 05:44:47,851] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:44:47,852] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.7055986e-09 5.1568341e-09 3.1312177e-23 1.1102221e-09 2.6947760e-09
 4.7841080e-12 1.0000000e+00], sampled 0.8403780870830845
[2019-04-04 05:45:02,455] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2842274], dtype=float32), 0.09657689]
[2019-04-04 05:45:02,456] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.45, 33.5, 86.0, 829.0, 26.0, 24.95442697859027, 0.2527996265931221, 0.0, 1.0, 24207.15691577416]
[2019-04-04 05:45:02,456] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:45:02,456] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.8855580e-09 3.2307410e-09 1.0977786e-22 9.8115516e-10 1.5234243e-09
 3.5171263e-12 1.0000000e+00], sampled 0.02458078898449989
[2019-04-04 05:46:01,868] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2842274], dtype=float32), 0.09657689]
[2019-04-04 05:46:01,868] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [9.8, 44.0, 243.5, 358.5, 26.0, 26.16632099358287, 0.8383961526679472, 0.0, 1.0, 0.0]
[2019-04-04 05:46:01,868] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:46:01,869] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.4198979e-11 3.0649389e-11 1.7551519e-26 1.4071310e-11 1.8159613e-11
 1.9431215e-14 1.0000000e+00], sampled 0.13261361072305144
[2019-04-04 05:46:10,344] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 05:46:41,902] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.5698 263430344.9894 1551.9755
[2019-04-04 05:46:46,234] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 05:46:47,286] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 700000, evaluation results [700000.0, 7241.569785764876, 263430344.989374, 1551.9755349129598, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 05:46:48,819] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44000, global step 700282: loss 0.0520
[2019-04-04 05:46:48,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44000, global step 700282: learning rate 0.0000
[2019-04-04 05:46:55,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2071343e-09 4.4316018e-09 5.0073953e-23 1.4762915e-09 1.2451721e-09
 1.9445778e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:46:55,039] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8880
[2019-04-04 05:46:55,069] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.799999999999999, 73.0, 0.0, 0.0, 26.0, 23.91880156051562, 0.04860337961359807, 0.0, 1.0, 41435.05405352618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 787200.0000, 
sim time next is 787800.0000, 
raw observation next is [-7.8, 73.5, 0.0, 0.0, 26.0, 23.8994677868201, 0.04185673211411756, 0.0, 1.0, 41388.60063178797], 
processed observation next is [1.0, 0.08695652173913043, 0.24653739612188366, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4916223155683417, 0.5139522440380392, 0.0, 1.0, 0.19708857443708558], 
reward next is 0.8029, 
noisyNet noise sample is [array([0.60254925], dtype=float32), 0.017188396]. 
=============================================
[2019-04-04 05:46:59,823] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44000, global step 702687: loss 0.0752
[2019-04-04 05:46:59,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44000, global step 702687: learning rate 0.0000
[2019-04-04 05:47:00,406] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44000, global step 702800: loss 0.0693
[2019-04-04 05:47:00,421] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44000, global step 702800: learning rate 0.0000
[2019-04-04 05:47:01,954] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44000, global step 703160: loss 0.0621
[2019-04-04 05:47:01,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44000, global step 703160: learning rate 0.0000
[2019-04-04 05:47:02,578] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44000, global step 703297: loss 0.0603
[2019-04-04 05:47:02,592] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44000, global step 703297: learning rate 0.0000
[2019-04-04 05:47:04,642] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7926409e-09 2.6822842e-09 2.9035909e-23 8.7955043e-10 1.6117413e-09
 2.1839303e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:04,642] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9908
[2019-04-04 05:47:04,686] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.90777749188882, 0.2753570964384197, 0.0, 1.0, 42406.96612846266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 854400.0000, 
sim time next is 855000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.87163829038169, 0.2683030035072023, 0.0, 1.0, 42276.77392790313], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5726365241984741, 0.5894343345024008, 0.0, 1.0, 0.201317971085253], 
reward next is 0.7987, 
noisyNet noise sample is [array([0.10850786], dtype=float32), -0.064721145]. 
=============================================
[2019-04-04 05:47:04,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.254524]
 [81.15392 ]
 [81.16621 ]
 [81.30838 ]
 [81.71978 ]], R is [[81.365448  ]
 [81.34986115]
 [81.33329773]
 [81.3139801 ]
 [81.2857666 ]].
[2019-04-04 05:47:05,937] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0673839e-11 1.0573823e-10 2.2346719e-26 5.8651937e-11 7.4625937e-11
 4.4170003e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:05,937] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3745
[2019-04-04 05:47:06,025] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 99.33333333333334, 0.0, 0.0, 26.0, 25.05745261694072, 0.3817583280821175, 0.0, 1.0, 41148.35507629056], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 940200.0000, 
sim time next is 940800.0000, 
raw observation next is [5.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.09173189731098, 0.3861298812918736, 0.0, 1.0, 40655.88209889648], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5909776581092482, 0.6287099604306245, 0.0, 1.0, 0.1935994385661737], 
reward next is 0.8064, 
noisyNet noise sample is [array([0.58200574], dtype=float32), 1.2550254]. 
=============================================
[2019-04-04 05:47:06,782] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44000, global step 704292: loss 0.0736
[2019-04-04 05:47:06,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44000, global step 704292: learning rate 0.0000
[2019-04-04 05:47:09,202] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44000, global step 704927: loss 0.0734
[2019-04-04 05:47:09,212] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44000, global step 704927: loss 0.0780
[2019-04-04 05:47:09,213] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44000, global step 704927: learning rate 0.0000
[2019-04-04 05:47:09,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44000, global step 704927: learning rate 0.0000
[2019-04-04 05:47:10,598] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44000, global step 705302: loss 0.0749
[2019-04-04 05:47:10,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44000, global step 705302: learning rate 0.0000
[2019-04-04 05:47:10,706] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44000, global step 705336: loss 0.0718
[2019-04-04 05:47:10,708] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44000, global step 705336: learning rate 0.0000
[2019-04-04 05:47:11,625] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44000, global step 705592: loss 0.0741
[2019-04-04 05:47:11,625] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44000, global step 705592: learning rate 0.0000
[2019-04-04 05:47:12,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1393154e-09 3.3727163e-09 9.4070377e-24 1.3448378e-09 5.6110283e-10
 2.5323732e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:12,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0366
[2019-04-04 05:47:12,466] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 72.0, 0.0, 0.0, 26.0, 24.67845239734818, 0.1749363586713776, 0.0, 1.0, 38904.08533425637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 883800.0000, 
sim time next is 884400.0000, 
raw observation next is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.62256499194848, 0.1641023118524476, 0.0, 1.0, 38889.69918574175], 
processed observation next is [1.0, 0.21739130434782608, 0.4570637119113574, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5518804159957066, 0.5547007706174826, 0.0, 1.0, 0.1851890437416274], 
reward next is 0.8148, 
noisyNet noise sample is [array([-1.3585035], dtype=float32), 0.07136896]. 
=============================================
[2019-04-04 05:47:12,563] A3C_AGENT_WORKER-Thread-20 INFO:Local step 44500, global step 705834: loss 1.3222
[2019-04-04 05:47:12,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 44500, global step 705836: learning rate 0.0000
[2019-04-04 05:47:12,810] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44000, global step 705924: loss 0.0714
[2019-04-04 05:47:12,812] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44000, global step 705924: learning rate 0.0000
[2019-04-04 05:47:12,941] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44000, global step 705975: loss 0.0825
[2019-04-04 05:47:12,944] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44000, global step 705975: learning rate 0.0000
[2019-04-04 05:47:13,160] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44000, global step 706035: loss 0.0735
[2019-04-04 05:47:13,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44000, global step 706035: learning rate 0.0000
[2019-04-04 05:47:15,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.2074456e-13 5.4358980e-13 2.6467055e-31 2.3287603e-13 2.1035556e-12
 2.6245064e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:15,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8945
[2019-04-04 05:47:15,162] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 107.0, 117.0, 26.0, 26.95312563801329, 0.8352418899979117, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1072800.0000, 
sim time next is 1073400.0000, 
raw observation next is [13.66666666666667, 78.33333333333334, 109.3333333333333, 77.99999999999999, 26.0, 27.04837380246572, 0.8494201794219874, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8411819021237306, 0.7833333333333334, 0.36444444444444435, 0.08618784530386739, 0.6666666666666666, 0.7540311502054765, 0.7831400598073291, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.111107], dtype=float32), 0.14310807]. 
=============================================
[2019-04-04 05:47:16,003] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44000, global step 706735: loss 0.0814
[2019-04-04 05:47:16,003] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44000, global step 706735: learning rate 0.0000
[2019-04-04 05:47:16,739] A3C_AGENT_WORKER-Thread-18 INFO:Local step 44500, global step 706957: loss 1.3830
[2019-04-04 05:47:16,769] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 44500, global step 706957: learning rate 0.0000
[2019-04-04 05:47:19,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4365282e-10 2.1498553e-10 1.6380814e-24 3.3488608e-11 1.5294385e-10
 5.1272441e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:19,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4120
[2019-04-04 05:47:19,978] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.98333333333333, 49.83333333333334, 23.66666666666667, 0.9999999999999998, 26.0, 27.94564167701661, 1.026271019481197, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1097400.0000, 
sim time next is 1098000.0000, 
raw observation next is [17.7, 50.0, 18.0, 1.5, 26.0, 27.96416548877713, 1.027869892466817, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9529085872576178, 0.5, 0.06, 0.0016574585635359116, 0.6666666666666666, 0.8303471240647609, 0.8426232974889389, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1273514], dtype=float32), -1.2669016]. 
=============================================
[2019-04-04 05:47:20,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[86.87864 ]
 [86.86223 ]
 [86.84255 ]
 [86.85124 ]
 [86.896515]], R is [[87.04043579]
 [87.17002869]
 [87.29833221]
 [87.42534637]
 [87.55109406]].
[2019-04-04 05:47:25,813] A3C_AGENT_WORKER-Thread-19 INFO:Local step 44500, global step 709901: loss 1.4032
[2019-04-04 05:47:25,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 44500, global step 709901: learning rate 0.0000
[2019-04-04 05:47:26,638] A3C_AGENT_WORKER-Thread-5 INFO:Local step 44500, global step 710150: loss 1.3721
[2019-04-04 05:47:26,638] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 44500, global step 710150: learning rate 0.0000
[2019-04-04 05:47:27,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5653648e-11 2.8634874e-11 1.5379971e-30 2.1494638e-12 6.0559092e-12
 4.6886628e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:27,936] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0248
[2019-04-04 05:47:27,957] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.68333333333333, 69.16666666666667, 207.3333333333333, 143.3333333333333, 26.0, 27.28235791757907, 0.9343740406609488, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077000.0000, 
sim time next is 1077600.0000, 
raw observation next is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.3641311709984, 0.9629047030957963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9021237303785783, 0.6833333333333335, 0.7688888888888891, 0.19797421731123394, 0.6666666666666666, 0.7803442642498668, 0.8209682343652654, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1786174], dtype=float32), 0.1823398]. 
=============================================
[2019-04-04 05:47:28,374] A3C_AGENT_WORKER-Thread-2 INFO:Local step 44500, global step 710892: loss 1.1701
[2019-04-04 05:47:28,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 44500, global step 710892: learning rate 0.0000
[2019-04-04 05:47:28,499] A3C_AGENT_WORKER-Thread-16 INFO:Local step 44500, global step 710951: loss 1.1362
[2019-04-04 05:47:28,500] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 44500, global step 710951: learning rate 0.0000
[2019-04-04 05:47:31,403] A3C_AGENT_WORKER-Thread-12 INFO:Local step 44500, global step 712088: loss 0.9877
[2019-04-04 05:47:31,403] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 44500, global step 712088: learning rate 0.0000
[2019-04-04 05:47:32,711] A3C_AGENT_WORKER-Thread-14 INFO:Local step 44500, global step 712622: loss 1.0593
[2019-04-04 05:47:32,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 44500, global step 712622: learning rate 0.0000
[2019-04-04 05:47:32,896] A3C_AGENT_WORKER-Thread-10 INFO:Local step 44500, global step 712698: loss 1.0210
[2019-04-04 05:47:32,897] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 44500, global step 712698: learning rate 0.0000
[2019-04-04 05:47:34,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5920942e-09 2.1528397e-09 3.2796734e-23 1.1540080e-09 4.0112549e-09
 4.1255515e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:34,341] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0652
[2019-04-04 05:47:34,354] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 79.0, 0.0, 0.0, 26.0, 24.11484937877451, 0.2809740988652162, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1211400.0000, 
sim time next is 1212000.0000, 
raw observation next is [16.1, 79.33333333333333, 0.0, 0.0, 26.0, 24.12406314642709, 0.2835889398422699, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.5103385955355909, 0.59452964661409, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60164356], dtype=float32), -0.11961692]. 
=============================================
[2019-04-04 05:47:34,411] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.32366 ]
 [80.313774]
 [80.13519 ]
 [80.110886]
 [80.00823 ]], R is [[80.62025452]
 [80.8140564 ]
 [81.00592041]
 [81.19586182]
 [81.3839035 ]].
[2019-04-04 05:47:34,413] A3C_AGENT_WORKER-Thread-3 INFO:Local step 44500, global step 713278: loss 1.0415
[2019-04-04 05:47:34,435] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 44500, global step 713278: learning rate 0.0000
[2019-04-04 05:47:34,724] A3C_AGENT_WORKER-Thread-13 INFO:Local step 44500, global step 713411: loss 0.9499
[2019-04-04 05:47:34,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 44500, global step 713413: learning rate 0.0000
[2019-04-04 05:47:35,118] A3C_AGENT_WORKER-Thread-11 INFO:Local step 44500, global step 713547: loss 0.8872
[2019-04-04 05:47:35,118] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 44500, global step 713547: learning rate 0.0000
[2019-04-04 05:47:35,915] A3C_AGENT_WORKER-Thread-4 INFO:Local step 44500, global step 713856: loss 0.8338
[2019-04-04 05:47:35,916] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 44500, global step 713856: learning rate 0.0000
[2019-04-04 05:47:35,963] A3C_AGENT_WORKER-Thread-6 INFO:Local step 44500, global step 713871: loss 0.8231
[2019-04-04 05:47:35,970] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 44500, global step 713871: learning rate 0.0000
[2019-04-04 05:47:36,311] A3C_AGENT_WORKER-Thread-17 INFO:Local step 44500, global step 714005: loss 0.7794
[2019-04-04 05:47:36,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 44500, global step 714005: learning rate 0.0000
[2019-04-04 05:47:38,657] A3C_AGENT_WORKER-Thread-15 INFO:Local step 44500, global step 714714: loss 0.7368
[2019-04-04 05:47:38,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 44500, global step 714714: learning rate 0.0000
[2019-04-04 05:47:39,710] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45000, global step 715045: loss 1.9921
[2019-04-04 05:47:39,762] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45000, global step 715050: learning rate 0.0000
[2019-04-04 05:47:43,468] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45000, global step 716196: loss 2.3554
[2019-04-04 05:47:43,468] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45000, global step 716196: learning rate 0.0000
[2019-04-04 05:47:46,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5329972e-10 8.7756641e-10 1.9544932e-24 5.7030509e-11 8.6851575e-11
 5.5026399e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:46,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0449
[2019-04-04 05:47:46,880] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 41.33333333333334, 0.0, 26.0, 25.91991581566293, 0.4057190215303292, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1438800.0000, 
sim time next is 1439400.0000, 
raw observation next is [1.1, 92.0, 36.66666666666667, 0.0, 26.0, 25.39024000116536, 0.4281415693438564, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.12222222222222223, 0.0, 0.6666666666666666, 0.6158533334304467, 0.6427138564479521, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55068475], dtype=float32), -0.7371322]. 
=============================================
[2019-04-04 05:47:50,793] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45000, global step 718824: loss 2.3096
[2019-04-04 05:47:50,794] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45000, global step 718824: learning rate 0.0000
[2019-04-04 05:47:50,832] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45000, global step 718837: loss 2.3003
[2019-04-04 05:47:50,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45000, global step 718837: learning rate 0.0000
[2019-04-04 05:47:51,932] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45000, global step 719267: loss 2.2548
[2019-04-04 05:47:51,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45000, global step 719267: learning rate 0.0000
[2019-04-04 05:47:52,072] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45000, global step 719331: loss 2.2186
[2019-04-04 05:47:52,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45000, global step 719331: learning rate 0.0000
[2019-04-04 05:47:54,470] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45000, global step 720323: loss 2.1297
[2019-04-04 05:47:54,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45000, global step 720323: learning rate 0.0000
[2019-04-04 05:47:55,283] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45000, global step 720670: loss 2.0264
[2019-04-04 05:47:55,286] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45000, global step 720671: learning rate 0.0000
[2019-04-04 05:47:55,720] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45000, global step 720882: loss 2.0105
[2019-04-04 05:47:55,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45000, global step 720882: learning rate 0.0000
[2019-04-04 05:47:56,559] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45000, global step 721287: loss 1.8806
[2019-04-04 05:47:56,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45000, global step 721287: learning rate 0.0000
[2019-04-04 05:47:56,859] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45000, global step 721466: loss 1.8701
[2019-04-04 05:47:56,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45000, global step 721467: learning rate 0.0000
[2019-04-04 05:47:57,294] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45000, global step 721702: loss 1.8495
[2019-04-04 05:47:57,294] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45000, global step 721702: learning rate 0.0000
[2019-04-04 05:47:58,160] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45000, global step 722157: loss 1.8480
[2019-04-04 05:47:58,163] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45000, global step 722158: learning rate 0.0000
[2019-04-04 05:47:58,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8458482e-09 3.7282399e-09 1.5597461e-22 2.7154503e-09 2.3119737e-09
 4.5987898e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:58,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8655
[2019-04-04 05:47:58,333] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 26.0, 24.96741012278507, 0.3565215301540889, 0.0, 1.0, 33893.49551372251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [-2.716666666666667, 83.0, 123.6666666666667, 0.0, 26.0, 25.00347778164445, 0.3543477498859848, 0.0, 1.0, 19595.45333871308], 
processed observation next is [0.0, 0.5217391304347826, 0.3873499538319483, 0.83, 0.4122222222222223, 0.0, 0.6666666666666666, 0.5836231484703708, 0.6181159166286616, 0.0, 1.0, 0.09331168256530038], 
reward next is 0.9067, 
noisyNet noise sample is [array([0.61715776], dtype=float32), -0.8092029]. 
=============================================
[2019-04-04 05:47:58,445] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45000, global step 722303: loss 1.8505
[2019-04-04 05:47:58,447] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45000, global step 722305: learning rate 0.0000
[2019-04-04 05:47:58,764] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45000, global step 722449: loss 1.7962
[2019-04-04 05:47:58,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45000, global step 722450: learning rate 0.0000
[2019-04-04 05:47:59,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4453021e-10 1.0784743e-09 2.3264861e-25 1.5809073e-10 5.7003458e-10
 7.0426830e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:59,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2309
[2019-04-04 05:47:59,098] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.533333333333333, 85.33333333333334, 0.0, 0.0, 26.0, 25.48338889030008, 0.5143980192260135, 0.0, 1.0, 72496.82910546628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1568400.0000, 
sim time next is 1569000.0000, 
raw observation next is [4.566666666666666, 85.16666666666667, 0.0, 0.0, 26.0, 25.50506875166619, 0.5237443617344261, 0.0, 1.0, 39447.51668192286], 
processed observation next is [1.0, 0.13043478260869565, 0.5891043397968606, 0.8516666666666667, 0.0, 0.0, 0.6666666666666666, 0.6254223959721825, 0.6745814539114754, 0.0, 1.0, 0.18784531753296602], 
reward next is 0.8122, 
noisyNet noise sample is [array([0.02745075], dtype=float32), 0.62241614]. 
=============================================
[2019-04-04 05:47:59,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.84983 ]
 [85.66608 ]
 [85.58872 ]
 [85.630714]
 [85.666374]], R is [[85.83337402]
 [85.62982178]
 [85.53700256]
 [85.57476044]
 [85.60424042]].
[2019-04-04 05:47:59,409] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45000, global step 722779: loss 1.7790
[2019-04-04 05:47:59,412] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45000, global step 722779: learning rate 0.0000
[2019-04-04 05:47:59,547] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0390347e-11 5.2462260e-11 9.3811906e-26 9.3157478e-12 3.6783176e-12
 2.2628480e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:47:59,547] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6558
[2019-04-04 05:47:59,566] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.7, 60.5, 0.0, 0.0, 26.0, 26.3052705064862, 0.6687042857081772, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1535400.0000, 
sim time next is 1536000.0000, 
raw observation next is [9.600000000000001, 60.66666666666667, 0.0, 0.0, 26.0, 26.1921495738383, 0.6670877595694823, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7285318559556788, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6826791311531917, 0.7223625865231608, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.55073565], dtype=float32), -0.39969853]. 
=============================================
[2019-04-04 05:47:59,599] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.80033]
 [87.42971]
 [87.10255]
 [86.78331]
 [86.45754]], R is [[88.29273987]
 [88.40981293]
 [88.52571869]
 [88.64046478]
 [88.75405884]].
[2019-04-04 05:48:01,353] A3C_AGENT_WORKER-Thread-20 INFO:Local step 45500, global step 723713: loss 0.1031
[2019-04-04 05:48:01,355] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 45500, global step 723713: learning rate 0.0000
[2019-04-04 05:48:03,375] A3C_AGENT_WORKER-Thread-18 INFO:Local step 45500, global step 724691: loss 0.1020
[2019-04-04 05:48:03,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 45500, global step 724692: learning rate 0.0000
[2019-04-04 05:48:09,311] A3C_AGENT_WORKER-Thread-5 INFO:Local step 45500, global step 726856: loss 0.0629
[2019-04-04 05:48:09,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 45500, global step 726856: learning rate 0.0000
[2019-04-04 05:48:09,754] A3C_AGENT_WORKER-Thread-19 INFO:Local step 45500, global step 726974: loss 0.0648
[2019-04-04 05:48:09,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 45500, global step 726974: learning rate 0.0000
[2019-04-04 05:48:10,821] A3C_AGENT_WORKER-Thread-2 INFO:Local step 45500, global step 727340: loss 0.0638
[2019-04-04 05:48:10,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 45500, global step 727341: learning rate 0.0000
[2019-04-04 05:48:11,127] A3C_AGENT_WORKER-Thread-16 INFO:Local step 45500, global step 727439: loss 0.0541
[2019-04-04 05:48:11,128] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 45500, global step 727441: learning rate 0.0000
[2019-04-04 05:48:11,347] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.8529407e-10 3.5572401e-09 3.2376068e-24 1.9913464e-10 8.6259433e-10
 2.2473401e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:48:11,347] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1430
[2019-04-04 05:48:11,371] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 91.00000000000001, 0.0, 0.0, 26.0, 25.28754318655522, 0.4401968867700554, 0.0, 1.0, 42919.50722499871], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1735800.0000, 
sim time next is 1736400.0000, 
raw observation next is [0.1333333333333334, 91.0, 0.0, 0.0, 26.0, 25.27320064638216, 0.438236565733765, 0.0, 1.0, 42932.04539880941], 
processed observation next is [0.0, 0.08695652173913043, 0.46629732225300097, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6061000538651801, 0.6460788552445883, 0.0, 1.0, 0.20443831142290195], 
reward next is 0.7956, 
noisyNet noise sample is [array([-2.1019905], dtype=float32), -0.8335341]. 
=============================================
[2019-04-04 05:48:13,160] A3C_AGENT_WORKER-Thread-12 INFO:Local step 45500, global step 728067: loss 0.0410
[2019-04-04 05:48:13,161] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 45500, global step 728067: learning rate 0.0000
[2019-04-04 05:48:14,443] A3C_AGENT_WORKER-Thread-14 INFO:Local step 45500, global step 728370: loss 0.0407
[2019-04-04 05:48:14,443] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 45500, global step 728370: learning rate 0.0000
[2019-04-04 05:48:14,717] A3C_AGENT_WORKER-Thread-10 INFO:Local step 45500, global step 728438: loss 0.0358
[2019-04-04 05:48:14,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 45500, global step 728438: learning rate 0.0000
[2019-04-04 05:48:16,074] A3C_AGENT_WORKER-Thread-3 INFO:Local step 45500, global step 728785: loss 0.0345
[2019-04-04 05:48:16,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 45500, global step 728785: learning rate 0.0000
[2019-04-04 05:48:17,147] A3C_AGENT_WORKER-Thread-13 INFO:Local step 45500, global step 729060: loss 0.0339
[2019-04-04 05:48:17,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 45500, global step 729060: learning rate 0.0000
[2019-04-04 05:48:17,778] A3C_AGENT_WORKER-Thread-11 INFO:Local step 45500, global step 729246: loss 0.0431
[2019-04-04 05:48:17,781] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 45500, global step 729246: learning rate 0.0000
[2019-04-04 05:48:18,291] A3C_AGENT_WORKER-Thread-4 INFO:Local step 45500, global step 729406: loss 0.0436
[2019-04-04 05:48:18,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 45500, global step 729406: learning rate 0.0000
[2019-04-04 05:48:18,813] A3C_AGENT_WORKER-Thread-6 INFO:Local step 45500, global step 729579: loss 0.0379
[2019-04-04 05:48:18,814] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 45500, global step 729579: learning rate 0.0000
[2019-04-04 05:48:19,086] A3C_AGENT_WORKER-Thread-17 INFO:Local step 45500, global step 729688: loss 0.0336
[2019-04-04 05:48:19,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 45500, global step 729688: learning rate 0.0000
[2019-04-04 05:48:19,931] A3C_AGENT_WORKER-Thread-15 INFO:Local step 45500, global step 729998: loss 0.0448
[2019-04-04 05:48:19,934] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 45500, global step 729999: learning rate 0.0000
[2019-04-04 05:48:27,421] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46000, global step 732054: loss 0.7219
[2019-04-04 05:48:27,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46000, global step 732054: learning rate 0.0000
[2019-04-04 05:48:31,114] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46000, global step 733047: loss 0.8313
[2019-04-04 05:48:31,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46000, global step 733047: learning rate 0.0000
[2019-04-04 05:48:38,281] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46000, global step 735069: loss 0.7770
[2019-04-04 05:48:38,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46000, global step 735072: learning rate 0.0000
[2019-04-04 05:48:38,321] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46000, global step 735076: loss 0.7513
[2019-04-04 05:48:38,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46000, global step 735076: learning rate 0.0000
[2019-04-04 05:48:38,891] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46000, global step 735202: loss 0.8046
[2019-04-04 05:48:38,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46000, global step 735202: learning rate 0.0000
[2019-04-04 05:48:39,715] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46000, global step 735405: loss 0.8014
[2019-04-04 05:48:39,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46000, global step 735405: learning rate 0.0000
[2019-04-04 05:48:41,396] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46000, global step 735836: loss 0.6741
[2019-04-04 05:48:41,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46000, global step 735836: learning rate 0.0000
[2019-04-04 05:48:43,111] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46000, global step 736273: loss 0.6867
[2019-04-04 05:48:43,112] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46000, global step 736273: learning rate 0.0000
[2019-04-04 05:48:43,455] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46000, global step 736386: loss 0.6305
[2019-04-04 05:48:43,456] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46000, global step 736386: learning rate 0.0000
[2019-04-04 05:48:44,082] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46000, global step 736596: loss 0.6081
[2019-04-04 05:48:44,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46000, global step 736596: learning rate 0.0000
[2019-04-04 05:48:45,144] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46000, global step 736998: loss 0.5318
[2019-04-04 05:48:45,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46000, global step 736998: learning rate 0.0000
[2019-04-04 05:48:46,327] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46000, global step 737312: loss 0.5170
[2019-04-04 05:48:46,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46000, global step 737312: learning rate 0.0000
[2019-04-04 05:48:46,582] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46000, global step 737367: loss 0.5496
[2019-04-04 05:48:46,583] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46000, global step 737367: learning rate 0.0000
[2019-04-04 05:48:46,891] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46000, global step 737441: loss 0.5313
[2019-04-04 05:48:46,891] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46000, global step 737441: learning rate 0.0000
[2019-04-04 05:48:47,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7396735e-10 3.1277483e-09 4.0851399e-23 1.4491733e-10 6.1068361e-10
 1.1846422e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:48:47,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6123
[2019-04-04 05:48:47,567] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.21737653277829, 0.3976263854053873, 0.0, 1.0, 42206.82806102424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152800.0000, 
sim time next is 2153400.0000, 
raw observation next is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 26.0, 25.19147858450103, 0.3490897065955352, 0.0, 1.0, 42519.48520479897], 
processed observation next is [1.0, 0.9565217391304348, 0.2742382271468144, 0.8283333333333335, 0.0, 0.0, 0.6666666666666666, 0.5992898820417526, 0.616363235531845, 0.0, 1.0, 0.20247373907047128], 
reward next is 0.7975, 
noisyNet noise sample is [array([0.35433418], dtype=float32), -0.2669608]. 
=============================================
[2019-04-04 05:48:47,626] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46000, global step 737622: loss 0.5642
[2019-04-04 05:48:47,626] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46000, global step 737622: learning rate 0.0000
[2019-04-04 05:48:48,155] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46000, global step 737745: loss 0.5034
[2019-04-04 05:48:48,164] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46000, global step 737747: learning rate 0.0000
[2019-04-04 05:48:55,847] A3C_AGENT_WORKER-Thread-20 INFO:Local step 46500, global step 739972: loss 0.0493
[2019-04-04 05:48:55,848] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 46500, global step 739972: learning rate 0.0000
[2019-04-04 05:48:57,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6237824e-07 2.1217291e-07 1.0560588e-19 4.1449685e-08 6.1418717e-08
 5.0295568e-10 9.9999952e-01], sum to 1.0000
[2019-04-04 05:48:57,577] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8615
[2019-04-04 05:48:57,592] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 60.16666666666666, 0.0, 0.0, 26.0, 23.18829477971211, -0.1628461744581783, 0.0, 1.0, 44068.93863902552], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2441400.0000, 
sim time next is 2442000.0000, 
raw observation next is [-9.100000000000001, 60.33333333333334, 0.0, 0.0, 26.0, 23.15244956829401, -0.172656522560055, 0.0, 1.0, 44042.25480662292], 
processed observation next is [0.0, 0.2608695652173913, 0.21052631578947364, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.4293707973578342, 0.44244782581331504, 0.0, 1.0, 0.20972502288868058], 
reward next is 0.7903, 
noisyNet noise sample is [array([-0.82098824], dtype=float32), 1.9331212]. 
=============================================
[2019-04-04 05:48:57,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[66.17053]
 [66.29672]
 [66.41269]
 [66.53734]
 [66.64122]], R is [[66.17282867]
 [66.30124664]
 [66.42814636]
 [66.5535202 ]
 [66.67743683]].
[2019-04-04 05:48:57,822] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0712025e-10 8.6717866e-10 1.3670567e-23 2.1710399e-10 1.6386119e-10
 1.8638177e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:48:57,822] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2195
[2019-04-04 05:48:57,907] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 70.5, 13.66666666666666, 0.0, 26.0, 25.68560474353259, 0.3129328095002157, 1.0, 1.0, 36275.1717695824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2221800.0000, 
sim time next is 2222400.0000, 
raw observation next is [-4.5, 70.0, 8.333333333333332, 0.0, 26.0, 25.34253479388782, 0.3222834657894443, 1.0, 1.0, 33453.02674430147], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.7, 0.027777777777777773, 0.0, 0.6666666666666666, 0.6118778994906515, 0.6074278219298147, 1.0, 1.0, 0.15930012735381652], 
reward next is 0.8407, 
noisyNet noise sample is [array([-1.8821918], dtype=float32), 1.2994152]. 
=============================================
[2019-04-04 05:48:59,218] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1346320e-09 2.3723835e-08 3.4072780e-23 6.4049893e-10 8.8738605e-10
 1.2972200e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:48:59,218] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8802
[2019-04-04 05:48:59,266] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 70.0, 0.0, 0.0, 26.0, 25.15850841825549, 0.3917526195542656, 0.0, 1.0, 142518.2339733135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2233200.0000, 
sim time next is 2233800.0000, 
raw observation next is [-5.0, 69.5, 0.0, 0.0, 26.0, 25.13777204102743, 0.4022660292589341, 0.0, 1.0, 91065.4550027276], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5948143367522857, 0.6340886764196447, 0.0, 1.0, 0.43364502382251235], 
reward next is 0.5664, 
noisyNet noise sample is [array([1.0972798], dtype=float32), -0.9303702]. 
=============================================
[2019-04-04 05:48:59,281] A3C_AGENT_WORKER-Thread-18 INFO:Local step 46500, global step 741007: loss 0.0509
[2019-04-04 05:48:59,283] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 46500, global step 741008: learning rate 0.0000
[2019-04-04 05:49:04,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.1865747e-09 2.7840743e-09 1.0569368e-21 3.5756387e-10 1.7295352e-09
 4.4181590e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:49:04,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3191
[2019-04-04 05:49:04,282] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 46.0, 79.0, 58.0, 26.0, 24.95386236342366, 0.2846462888370581, 0.0, 1.0, 49397.7183590956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2392200.0000, 
sim time next is 2392800.0000, 
raw observation next is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 26.0, 24.94797735316108, 0.2844648081343451, 0.0, 1.0, 45660.27674844666], 
processed observation next is [0.0, 0.6956521739130435, 0.45152354570637127, 0.4566666666666667, 0.2227777777777778, 0.056353591160221, 0.6666666666666666, 0.5789981127634233, 0.5948216027114483, 0.0, 1.0, 0.21742988927831744], 
reward next is 0.7826, 
noisyNet noise sample is [array([0.11475149], dtype=float32), 1.2193353]. 
=============================================
[2019-04-04 05:49:06,137] A3C_AGENT_WORKER-Thread-2 INFO:Local step 46500, global step 743166: loss 0.0877
[2019-04-04 05:49:06,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 46500, global step 743166: learning rate 0.0000
[2019-04-04 05:49:06,310] A3C_AGENT_WORKER-Thread-5 INFO:Local step 46500, global step 743223: loss 0.0797
[2019-04-04 05:49:06,319] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 46500, global step 743229: learning rate 0.0000
[2019-04-04 05:49:06,956] A3C_AGENT_WORKER-Thread-16 INFO:Local step 46500, global step 743473: loss 0.0700
[2019-04-04 05:49:06,958] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 46500, global step 743473: learning rate 0.0000
[2019-04-04 05:49:07,566] A3C_AGENT_WORKER-Thread-19 INFO:Local step 46500, global step 743729: loss 0.0678
[2019-04-04 05:49:07,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 46500, global step 743730: learning rate 0.0000
[2019-04-04 05:49:07,819] A3C_AGENT_WORKER-Thread-12 INFO:Local step 46500, global step 743837: loss 0.0593
[2019-04-04 05:49:07,820] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 46500, global step 743837: learning rate 0.0000
[2019-04-04 05:49:07,837] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7651201e-10 2.6049758e-09 7.7735213e-24 3.9965137e-10 2.3706984e-10
 1.0234841e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:49:07,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0283
[2019-04-04 05:49:07,887] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 36.0, 0.0, 0.0, 26.0, 24.97583633636908, 0.3397244153446112, 1.0, 1.0, 81781.82283234474], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2574000.0000, 
sim time next is 2574600.0000, 
raw observation next is [-0.7833333333333333, 37.33333333333334, 0.0, 0.0, 26.0, 25.0443546075729, 0.3514488808603051, 1.0, 1.0, 20890.11230781976], 
processed observation next is [1.0, 0.8260869565217391, 0.44090489381348114, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5870295506310749, 0.617149626953435, 1.0, 1.0, 0.09947672527533219], 
reward next is 0.9005, 
noisyNet noise sample is [array([-1.4432929], dtype=float32), 0.38966027]. 
=============================================
[2019-04-04 05:49:09,679] A3C_AGENT_WORKER-Thread-10 INFO:Local step 46500, global step 744434: loss 0.0824
[2019-04-04 05:49:09,682] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 46500, global step 744434: learning rate 0.0000
[2019-04-04 05:49:10,592] A3C_AGENT_WORKER-Thread-14 INFO:Local step 46500, global step 744689: loss 0.0718
[2019-04-04 05:49:10,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 46500, global step 744691: learning rate 0.0000
[2019-04-04 05:49:11,060] A3C_AGENT_WORKER-Thread-3 INFO:Local step 46500, global step 744838: loss 0.0785
[2019-04-04 05:49:11,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 46500, global step 744838: learning rate 0.0000
[2019-04-04 05:49:12,072] A3C_AGENT_WORKER-Thread-13 INFO:Local step 46500, global step 745141: loss 0.0833
[2019-04-04 05:49:12,072] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 46500, global step 745141: learning rate 0.0000
[2019-04-04 05:49:12,901] A3C_AGENT_WORKER-Thread-4 INFO:Local step 46500, global step 745442: loss 0.0928
[2019-04-04 05:49:12,902] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 46500, global step 745442: learning rate 0.0000
[2019-04-04 05:49:13,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7276548e-11 2.0092514e-10 5.1221493e-25 1.4688946e-11 5.0258835e-11
 3.8912439e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:49:13,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7941
[2019-04-04 05:49:13,106] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4166666666666667, 46.33333333333334, 191.0, 189.6666666666667, 26.0, 25.46031412401701, 0.4455401773113885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2639400.0000, 
sim time next is 2640000.0000, 
raw observation next is [-0.2333333333333334, 45.66666666666667, 177.5, 200.3333333333333, 26.0, 25.84649216947739, 0.4813745730640873, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.456140350877193, 0.4566666666666667, 0.5916666666666667, 0.2213627992633517, 0.6666666666666666, 0.6538743474564491, 0.6604581910213624, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.2949748], dtype=float32), 0.9940813]. 
=============================================
[2019-04-04 05:49:13,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.323296]
 [86.86067 ]
 [87.03102 ]
 [86.62766 ]
 [86.45319 ]], R is [[85.84654236]
 [85.98807526]
 [85.81587982]
 [85.01599884]
 [84.53399658]].
[2019-04-04 05:49:13,287] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6191185e-08 2.4629488e-08 1.1005087e-21 7.4898807e-09 5.3444618e-09
 2.3419161e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:49:13,293] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2135
[2019-04-04 05:49:13,337] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.05, 43.5, 0.0, 0.0, 26.0, 24.96943627435964, 0.2607026122171117, 0.0, 1.0, 36860.13989581777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2399400.0000, 
sim time next is 2400000.0000, 
raw observation next is [-2.166666666666667, 43.33333333333333, 0.0, 0.0, 26.0, 24.96329138821897, 0.2590867862486727, 0.0, 1.0, 43947.04924618492], 
processed observation next is [0.0, 0.782608695652174, 0.4025854108956602, 0.4333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5802742823515809, 0.5863622620828909, 0.0, 1.0, 0.20927166307707104], 
reward next is 0.7907, 
noisyNet noise sample is [array([-0.8611178], dtype=float32), 0.10998546]. 
=============================================
[2019-04-04 05:49:13,340] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[73.70585]
 [73.52087]
 [73.38598]
 [73.16818]
 [72.98485]], R is [[73.98454285]
 [74.06917572]
 [74.17027283]
 [74.22746277]
 [74.26209259]].
[2019-04-04 05:49:13,533] A3C_AGENT_WORKER-Thread-11 INFO:Local step 46500, global step 745669: loss 0.0995
[2019-04-04 05:49:13,535] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 46500, global step 745669: learning rate 0.0000
[2019-04-04 05:49:13,598] A3C_AGENT_WORKER-Thread-17 INFO:Local step 46500, global step 745691: loss 0.0997
[2019-04-04 05:49:13,601] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 46500, global step 745691: learning rate 0.0000
[2019-04-04 05:49:13,814] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.5082893e-09 5.9983938e-09 2.6547767e-22 1.2691475e-09 4.0013723e-10
 1.2025182e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:49:13,814] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4881
[2019-04-04 05:49:13,855] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 41.0, 0.0, 0.0, 26.0, 24.97674472847585, 0.1852679035902022, 0.0, 1.0, 38705.64923803845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2514600.0000, 
sim time next is 2515200.0000, 
raw observation next is [-1.7, 42.0, 0.0, 0.0, 26.0, 24.9602912527448, 0.1852125825410776, 0.0, 1.0, 38660.13633837409], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5800242710620666, 0.5617375275136925, 0.0, 1.0, 0.1840958873255909], 
reward next is 0.8159, 
noisyNet noise sample is [array([-0.21543545], dtype=float32), -0.14444129]. 
=============================================
[2019-04-04 05:49:14,156] A3C_AGENT_WORKER-Thread-6 INFO:Local step 46500, global step 745929: loss 0.1070
[2019-04-04 05:49:14,157] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 46500, global step 745929: learning rate 0.0000
[2019-04-04 05:49:14,271] A3C_AGENT_WORKER-Thread-15 INFO:Local step 46500, global step 745975: loss 0.1041
[2019-04-04 05:49:14,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 46500, global step 745975: learning rate 0.0000
[2019-04-04 05:49:18,510] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47000, global step 747515: loss 0.6676
[2019-04-04 05:49:18,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47000, global step 747515: learning rate 0.0000
[2019-04-04 05:49:19,110] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1710927e-09 2.5242224e-09 4.6911972e-24 3.1427685e-10 5.4537580e-10
 3.3991703e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:49:19,110] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1733
[2019-04-04 05:49:19,125] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 26.0, 24.90176689911098, 0.2551997886150426, 0.0, 1.0, 41645.68953062339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.97989800304001, 0.2524244878694317, 0.0, 1.0, 41599.97741537738], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5816581669200008, 0.5841414959564772, 0.0, 1.0, 0.1980951305494161], 
reward next is 0.8019, 
noisyNet noise sample is [array([0.83963656], dtype=float32), -0.8855122]. 
=============================================
[2019-04-04 05:49:21,200] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47000, global step 748627: loss 0.6843
[2019-04-04 05:49:21,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47000, global step 748627: learning rate 0.0000
[2019-04-04 05:49:22,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.81351503e-10 4.97255570e-10 1.55162038e-24 1.95564814e-10
 1.10000495e-10 8.14438442e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 05:49:22,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2565
[2019-04-04 05:49:22,237] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.78555306892745, 0.1295510213742476, 0.0, 1.0, 38543.3750348419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2527200.0000, 
sim time next is 2527800.0000, 
raw observation next is [-2.383333333333333, 56.5, 0.0, 0.0, 26.0, 24.73906990825132, 0.1256096568207206, 0.0, 1.0, 38627.51128689079], 
processed observation next is [1.0, 0.2608695652173913, 0.3965835641735919, 0.565, 0.0, 0.0, 0.6666666666666666, 0.5615891590209433, 0.5418698856069069, 0.0, 1.0, 0.18394052993757518], 
reward next is 0.8161, 
noisyNet noise sample is [array([-0.17009641], dtype=float32), 0.072783396]. 
=============================================
[2019-04-04 05:49:27,593] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47000, global step 750955: loss 0.5886
[2019-04-04 05:49:27,593] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47000, global step 750955: learning rate 0.0000
[2019-04-04 05:49:28,435] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47000, global step 751202: loss 0.5851
[2019-04-04 05:49:28,435] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47000, global step 751202: learning rate 0.0000
[2019-04-04 05:49:29,233] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47000, global step 751430: loss 0.6295
[2019-04-04 05:49:29,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47000, global step 751431: learning rate 0.0000
[2019-04-04 05:49:29,456] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47000, global step 751502: loss 0.6229
[2019-04-04 05:49:29,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47000, global step 751502: learning rate 0.0000
[2019-04-04 05:49:30,000] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47000, global step 751672: loss 0.6235
[2019-04-04 05:49:30,001] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47000, global step 751672: learning rate 0.0000
[2019-04-04 05:49:31,646] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47000, global step 752212: loss 0.6669
[2019-04-04 05:49:31,648] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47000, global step 752212: learning rate 0.0000
[2019-04-04 05:49:32,266] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47000, global step 752424: loss 0.6793
[2019-04-04 05:49:32,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47000, global step 752424: learning rate 0.0000
[2019-04-04 05:49:33,236] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47000, global step 752747: loss 0.7262
[2019-04-04 05:49:33,237] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47000, global step 752747: learning rate 0.0000
[2019-04-04 05:49:33,714] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47000, global step 752936: loss 0.7492
[2019-04-04 05:49:33,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47000, global step 752936: learning rate 0.0000
[2019-04-04 05:49:34,663] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47000, global step 753289: loss 0.8637
[2019-04-04 05:49:34,664] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47000, global step 753289: learning rate 0.0000
[2019-04-04 05:49:35,353] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47000, global step 753490: loss 0.8414
[2019-04-04 05:49:35,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47000, global step 753490: learning rate 0.0000
[2019-04-04 05:49:36,033] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47000, global step 753712: loss 0.8997
[2019-04-04 05:49:36,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47000, global step 753712: learning rate 0.0000
[2019-04-04 05:49:36,080] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47000, global step 753723: loss 0.9135
[2019-04-04 05:49:36,083] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47000, global step 753723: learning rate 0.0000
[2019-04-04 05:49:36,803] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47000, global step 753936: loss 0.9506
[2019-04-04 05:49:36,803] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47000, global step 753936: learning rate 0.0000
[2019-04-04 05:49:38,318] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.99694900e-10 1.92039218e-10 1.66849012e-26 1.08620286e-10
 2.09524481e-10 9.85706979e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 05:49:38,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1707
[2019-04-04 05:49:38,357] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 87.33333333333334, 0.0, 0.0, 26.0, 25.16909837404203, 0.3853225876765645, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2920800.0000, 
sim time next is 2921400.0000, 
raw observation next is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.04050753596379, 0.3775653204153253, 1.0, 1.0, 63032.74195368057], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5867089613303159, 0.6258551068051085, 1.0, 1.0, 0.3001559140651456], 
reward next is 0.6998, 
noisyNet noise sample is [array([0.799123], dtype=float32), -0.43897063]. 
=============================================
[2019-04-04 05:49:39,648] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0453270e-09 1.0228748e-08 1.7692647e-23 7.1075223e-10 1.1955373e-09
 2.4548536e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:49:39,648] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7425
[2019-04-04 05:49:39,709] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11392675087005, 0.4170734548615414, 0.0, 1.0, 37701.46491945855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2750400.0000, 
sim time next is 2751000.0000, 
raw observation next is [-5.166666666666667, 59.83333333333334, 0.0, 0.0, 26.0, 25.17947994827071, 0.4160602786366573, 0.0, 1.0, 18728.44359550271], 
processed observation next is [1.0, 0.8695652173913043, 0.31948291782086796, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.5982899956892259, 0.6386867595455524, 0.0, 1.0, 0.0891830647404891], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.5385871], dtype=float32), -0.119319096]. 
=============================================
[2019-04-04 05:49:39,718] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.71581]
 [82.34905]
 [81.2772 ]
 [83.24902]
 [82.57783]], R is [[79.55567932]
 [79.58059692]
 [79.37023926]
 [79.19941711]
 [79.00647736]].
[2019-04-04 05:49:41,900] A3C_AGENT_WORKER-Thread-20 INFO:Local step 47500, global step 755751: loss 0.0321
[2019-04-04 05:49:41,900] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 47500, global step 755751: learning rate 0.0000
[2019-04-04 05:49:45,798] A3C_AGENT_WORKER-Thread-18 INFO:Local step 47500, global step 757095: loss 0.0357
[2019-04-04 05:49:45,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 47500, global step 757095: learning rate 0.0000
[2019-04-04 05:49:51,576] A3C_AGENT_WORKER-Thread-2 INFO:Local step 47500, global step 759127: loss 0.0621
[2019-04-04 05:49:51,577] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 47500, global step 759127: learning rate 0.0000
[2019-04-04 05:49:51,821] A3C_AGENT_WORKER-Thread-5 INFO:Local step 47500, global step 759213: loss 0.0530
[2019-04-04 05:49:51,821] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 47500, global step 759213: learning rate 0.0000
[2019-04-04 05:49:52,924] A3C_AGENT_WORKER-Thread-16 INFO:Local step 47500, global step 759612: loss 0.0531
[2019-04-04 05:49:52,924] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 47500, global step 759612: learning rate 0.0000
[2019-04-04 05:49:53,073] A3C_AGENT_WORKER-Thread-19 INFO:Local step 47500, global step 759670: loss 0.0538
[2019-04-04 05:49:53,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 47500, global step 759670: learning rate 0.0000
[2019-04-04 05:49:53,615] A3C_AGENT_WORKER-Thread-12 INFO:Local step 47500, global step 759912: loss 0.0564
[2019-04-04 05:49:53,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 47500, global step 759912: learning rate 0.0000
[2019-04-04 05:49:54,941] A3C_AGENT_WORKER-Thread-10 INFO:Local step 47500, global step 760411: loss 0.0523
[2019-04-04 05:49:54,957] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 47500, global step 760416: learning rate 0.0000
[2019-04-04 05:49:56,022] A3C_AGENT_WORKER-Thread-14 INFO:Local step 47500, global step 760730: loss 0.0474
[2019-04-04 05:49:56,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 47500, global step 760731: learning rate 0.0000
[2019-04-04 05:49:56,499] A3C_AGENT_WORKER-Thread-3 INFO:Local step 47500, global step 760868: loss 0.0347
[2019-04-04 05:49:56,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 47500, global step 760868: learning rate 0.0000
[2019-04-04 05:49:57,345] A3C_AGENT_WORKER-Thread-13 INFO:Local step 47500, global step 761164: loss 0.0284
[2019-04-04 05:49:57,348] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 47500, global step 761165: learning rate 0.0000
[2019-04-04 05:49:58,638] A3C_AGENT_WORKER-Thread-4 INFO:Local step 47500, global step 761654: loss 0.0152
[2019-04-04 05:49:58,645] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 47500, global step 761657: learning rate 0.0000
[2019-04-04 05:49:58,934] A3C_AGENT_WORKER-Thread-15 INFO:Local step 47500, global step 761763: loss 0.0137
[2019-04-04 05:49:58,941] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 47500, global step 761763: learning rate 0.0000
[2019-04-04 05:49:59,153] A3C_AGENT_WORKER-Thread-17 INFO:Local step 47500, global step 761847: loss 0.0130
[2019-04-04 05:49:59,156] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 47500, global step 761847: learning rate 0.0000
[2019-04-04 05:49:59,624] A3C_AGENT_WORKER-Thread-6 INFO:Local step 47500, global step 762039: loss 0.0091
[2019-04-04 05:49:59,624] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 47500, global step 762039: learning rate 0.0000
[2019-04-04 05:49:59,869] A3C_AGENT_WORKER-Thread-11 INFO:Local step 47500, global step 762129: loss 0.0083
[2019-04-04 05:49:59,871] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 47500, global step 762130: learning rate 0.0000
[2019-04-04 05:50:01,342] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48000, global step 762772: loss 55.2883
[2019-04-04 05:50:01,343] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48000, global step 762772: learning rate 0.0000
[2019-04-04 05:50:02,042] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.3102810e-11 3.5050488e-10 3.8556494e-25 6.8470951e-11 8.6018699e-11
 1.9256682e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:02,043] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0430
[2019-04-04 05:50:02,094] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.25051926610011, 0.6073758929516184, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3264600.0000, 
sim time next is 3265200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.65605556096883, 0.6190375233076216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6380046300807359, 0.7063458411025406, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.539984], dtype=float32), 0.6929721]. 
=============================================
[2019-04-04 05:50:02,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8613060e-13 3.7487044e-13 1.2851525e-30 9.6313440e-14 1.4538119e-12
 3.0698340e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:02,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-04 05:50:02,326] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 26.0, 27.16585768630426, 0.8023743668491043, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3153000.0000, 
sim time next is 3153600.0000, 
raw observation next is [8.0, 93.0, 113.5, 814.0, 26.0, 27.22060185370187, 0.8141174447292379, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6842105263157896, 0.93, 0.37833333333333335, 0.8994475138121547, 0.6666666666666666, 0.7683834878084891, 0.7713724815764126, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1384327], dtype=float32), -1.4553748]. 
=============================================
[2019-04-04 05:50:05,222] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48000, global step 764422: loss 55.4101
[2019-04-04 05:50:05,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48000, global step 764422: learning rate 0.0000
[2019-04-04 05:50:06,889] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2456542e-09 7.6062001e-10 9.0213209e-26 6.3246693e-11 2.1341791e-10
 2.8321365e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:06,890] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5638
[2019-04-04 05:50:06,914] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 25.35531556543287, 0.315116941076453, 0.0, 1.0, 57222.26729956547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3133800.0000, 
sim time next is 3134400.0000, 
raw observation next is [5.333333333333334, 100.0, 0.0, 0.0, 26.0, 25.3679518457508, 0.32232797994256, 0.0, 1.0, 53446.50724335496], 
processed observation next is [1.0, 0.2608695652173913, 0.6103416435826409, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6139959871458999, 0.6074426599808533, 0.0, 1.0, 0.25450717734930933], 
reward next is 0.7455, 
noisyNet noise sample is [array([0.00337723], dtype=float32), 0.45446488]. 
=============================================
[2019-04-04 05:50:07,796] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6313855e-11 1.9550580e-11 2.9583112e-28 1.8336038e-11 4.4355974e-11
 5.3958125e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:07,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7566
[2019-04-04 05:50:07,873] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 42.0, 237.0, 26.0, 25.37600581538433, 0.3723072969966096, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3139200.0000, 
sim time next is 3139800.0000, 
raw observation next is [6.166666666666666, 100.0, 55.66666666666668, 288.6666666666667, 26.0, 25.60880831148994, 0.3830750740090569, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6334256694367498, 1.0, 0.18555555555555558, 0.31896869244935544, 0.6666666666666666, 0.6340673592908285, 0.6276916913363523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4307404], dtype=float32), 0.6454786]. 
=============================================
[2019-04-04 05:50:07,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6355289e-11 2.8159131e-10 4.4241001e-25 7.9197898e-11 5.4907353e-11
 3.4403456e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:07,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2419
[2019-04-04 05:50:08,018] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 76.0, 42.33333333333334, 375.3333333333334, 26.0, 26.46779450256879, 0.5676619844707281, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3257400.0000, 
sim time next is 3258000.0000, 
raw observation next is [-4.0, 77.0, 34.0, 307.5, 26.0, 26.52242206879365, 0.7174982546911223, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.77, 0.11333333333333333, 0.3397790055248619, 0.6666666666666666, 0.7102018390661374, 0.7391660848970408, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8588799], dtype=float32), 1.0449611]. 
=============================================
[2019-04-04 05:50:08,026] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.94236]
 [86.71853]
 [87.28031]
 [87.70639]
 [88.11375]], R is [[85.40432739]
 [85.55028534]
 [85.69478607]
 [85.83783722]
 [85.97946167]].
[2019-04-04 05:50:09,865] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48000, global step 766695: loss 55.1779
[2019-04-04 05:50:09,868] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48000, global step 766696: learning rate 0.0000
[2019-04-04 05:50:10,057] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48000, global step 766782: loss 54.9489
[2019-04-04 05:50:10,058] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48000, global step 766782: learning rate 0.0000
[2019-04-04 05:50:11,119] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48000, global step 767254: loss 54.9384
[2019-04-04 05:50:11,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48000, global step 767254: learning rate 0.0000
[2019-04-04 05:50:11,155] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.2712074e-11 4.7622628e-10 1.7221667e-22 1.2403714e-10 1.2808683e-10
 3.7792084e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:11,194] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0387
[2019-04-04 05:50:11,210] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 54.0, 116.0, 805.5, 26.0, 25.94001885601104, 0.5578672511731724, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3330000.0000, 
sim time next is 3330600.0000, 
raw observation next is [-4.833333333333334, 53.33333333333333, 115.3333333333333, 803.6666666666666, 26.0, 25.950977804727, 0.5577301322681268, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.32871652816251157, 0.5333333333333333, 0.3844444444444443, 0.8880294659300184, 0.6666666666666666, 0.66258148372725, 0.6859100440893756, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8271761], dtype=float32), -0.4478451]. 
=============================================
[2019-04-04 05:50:11,487] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48000, global step 767408: loss 55.5148
[2019-04-04 05:50:11,488] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48000, global step 767408: learning rate 0.0000
[2019-04-04 05:50:12,528] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48000, global step 767798: loss 56.0045
[2019-04-04 05:50:12,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48000, global step 767798: learning rate 0.0000
[2019-04-04 05:50:13,810] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48000, global step 768385: loss 56.4115
[2019-04-04 05:50:13,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48000, global step 768386: learning rate 0.0000
[2019-04-04 05:50:14,401] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48000, global step 768619: loss 56.3886
[2019-04-04 05:50:14,432] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48000, global step 768619: learning rate 0.0000
[2019-04-04 05:50:15,042] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48000, global step 768885: loss 56.5799
[2019-04-04 05:50:15,043] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48000, global step 768885: learning rate 0.0000
[2019-04-04 05:50:15,521] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48000, global step 769073: loss 56.6634
[2019-04-04 05:50:15,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48000, global step 769073: learning rate 0.0000
[2019-04-04 05:50:16,673] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48000, global step 769578: loss 57.0606
[2019-04-04 05:50:16,677] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48000, global step 769581: learning rate 0.0000
[2019-04-04 05:50:17,111] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48000, global step 769740: loss 57.1433
[2019-04-04 05:50:17,111] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48000, global step 769740: learning rate 0.0000
[2019-04-04 05:50:17,201] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48000, global step 769771: loss 57.7910
[2019-04-04 05:50:17,202] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48000, global step 769771: learning rate 0.0000
[2019-04-04 05:50:17,549] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48000, global step 769882: loss 58.0701
[2019-04-04 05:50:17,551] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48000, global step 769882: learning rate 0.0000
[2019-04-04 05:50:17,877] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48000, global step 769998: loss 57.9472
[2019-04-04 05:50:17,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48000, global step 769999: learning rate 0.0000
[2019-04-04 05:50:18,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7873568e-08 2.4150392e-08 1.8935463e-21 2.8506024e-09 6.6713648e-09
 4.7768161e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:18,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5120
[2019-04-04 05:50:18,558] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.51209547210761, 0.4974171650849033, 0.0, 1.0, 41947.0725467114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361200.0000, 
sim time next is 3361800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45429055912672, 0.4932092327923643, 0.0, 1.0, 71281.13748638306], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6211908799272265, 0.6644030775974548, 0.0, 1.0, 0.3394339880303955], 
reward next is 0.6606, 
noisyNet noise sample is [array([-0.1444255], dtype=float32), -0.03786655]. 
=============================================
[2019-04-04 05:50:19,867] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4895045e-11 2.6921032e-10 5.9760568e-24 2.5481259e-11 1.0024943e-11
 3.6140782e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:19,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3432
[2019-04-04 05:50:19,914] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 73.5, 587.5, 26.0, 26.04204494118304, 0.6483160012345127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3340800.0000, 
sim time next is 3341400.0000, 
raw observation next is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 26.0, 26.39040021824707, 0.6772872881692384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.46666666666666673, 0.23, 0.6173112338858197, 0.6666666666666666, 0.6992000181872559, 0.7257624293897461, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47717863], dtype=float32), -0.64694864]. 
=============================================
[2019-04-04 05:50:20,014] A3C_AGENT_WORKER-Thread-20 INFO:Local step 48500, global step 770947: loss 1.3056
[2019-04-04 05:50:20,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 48500, global step 770947: learning rate 0.0000
[2019-04-04 05:50:20,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3071228e-12 1.1462112e-10 4.2940229e-26 5.2380444e-12 6.8721990e-12
 1.7521164e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:20,417] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4967
[2019-04-04 05:50:20,446] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 10.0, 100.8333333333333, 26.0, 25.73569984122184, 0.52995645168803, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3433200.0000, 
sim time next is 3433800.0000, 
raw observation next is [2.0, 67.0, 0.0, 0.0, 26.0, 25.71794982184244, 0.3730156912699203, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6431624851535366, 0.6243385637566401, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18085803], dtype=float32), 0.7762362]. 
=============================================
[2019-04-04 05:50:23,620] A3C_AGENT_WORKER-Thread-18 INFO:Local step 48500, global step 772530: loss 1.1541
[2019-04-04 05:50:23,620] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 48500, global step 772530: learning rate 0.0000
[2019-04-04 05:50:26,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1657530e-09 1.3631192e-09 5.7478630e-24 3.2688535e-10 3.2554326e-10
 2.4563275e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:26,612] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1700
[2019-04-04 05:50:26,638] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.66927088300581, 0.5360338608149399, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3448800.0000, 
sim time next is 3449400.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.62603199400968, 0.4647106498214083, 0.0, 1.0, 26646.07216546434], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6355026661674733, 0.6549035499404695, 0.0, 1.0, 0.12688605793078256], 
reward next is 0.8731, 
noisyNet noise sample is [array([0.23351437], dtype=float32), -0.28513202]. 
=============================================
[2019-04-04 05:50:28,237] A3C_AGENT_WORKER-Thread-2 INFO:Local step 48500, global step 774696: loss 1.1762
[2019-04-04 05:50:28,240] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 48500, global step 774696: learning rate 0.0000
[2019-04-04 05:50:28,372] A3C_AGENT_WORKER-Thread-5 INFO:Local step 48500, global step 774751: loss 1.1866
[2019-04-04 05:50:28,373] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 48500, global step 774752: learning rate 0.0000
[2019-04-04 05:50:29,867] A3C_AGENT_WORKER-Thread-19 INFO:Local step 48500, global step 775481: loss 1.0923
[2019-04-04 05:50:29,868] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 48500, global step 775481: learning rate 0.0000
[2019-04-04 05:50:30,317] A3C_AGENT_WORKER-Thread-16 INFO:Local step 48500, global step 775693: loss 1.1008
[2019-04-04 05:50:30,318] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 48500, global step 775694: learning rate 0.0000
[2019-04-04 05:50:31,287] A3C_AGENT_WORKER-Thread-12 INFO:Local step 48500, global step 776160: loss 1.0441
[2019-04-04 05:50:31,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 48500, global step 776160: learning rate 0.0000
[2019-04-04 05:50:32,651] A3C_AGENT_WORKER-Thread-10 INFO:Local step 48500, global step 776833: loss 0.9875
[2019-04-04 05:50:32,653] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 48500, global step 776833: learning rate 0.0000
[2019-04-04 05:50:33,177] A3C_AGENT_WORKER-Thread-14 INFO:Local step 48500, global step 777079: loss 0.9552
[2019-04-04 05:50:33,178] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 48500, global step 777079: learning rate 0.0000
[2019-04-04 05:50:33,508] A3C_AGENT_WORKER-Thread-3 INFO:Local step 48500, global step 777229: loss 0.9937
[2019-04-04 05:50:33,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 48500, global step 777229: learning rate 0.0000
[2019-04-04 05:50:33,903] A3C_AGENT_WORKER-Thread-13 INFO:Local step 48500, global step 777425: loss 1.0024
[2019-04-04 05:50:33,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 48500, global step 777425: learning rate 0.0000
[2019-04-04 05:50:34,293] A3C_AGENT_WORKER-Thread-4 INFO:Local step 48500, global step 777614: loss 0.9901
[2019-04-04 05:50:34,293] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 48500, global step 777614: learning rate 0.0000
[2019-04-04 05:50:34,978] A3C_AGENT_WORKER-Thread-17 INFO:Local step 48500, global step 777942: loss 0.9825
[2019-04-04 05:50:34,979] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 48500, global step 777942: learning rate 0.0000
[2019-04-04 05:50:35,403] A3C_AGENT_WORKER-Thread-15 INFO:Local step 48500, global step 778144: loss 0.9697
[2019-04-04 05:50:35,403] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 48500, global step 778144: learning rate 0.0000
[2019-04-04 05:50:35,606] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49000, global step 778263: loss 1.7540
[2019-04-04 05:50:35,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49000, global step 778263: learning rate 0.0000
[2019-04-04 05:50:35,932] A3C_AGENT_WORKER-Thread-6 INFO:Local step 48500, global step 778439: loss 0.8877
[2019-04-04 05:50:35,936] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 48500, global step 778440: learning rate 0.0000
[2019-04-04 05:50:36,691] A3C_AGENT_WORKER-Thread-11 INFO:Local step 48500, global step 778852: loss 0.8915
[2019-04-04 05:50:36,692] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 48500, global step 778852: learning rate 0.0000
[2019-04-04 05:50:38,940] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49000, global step 780085: loss 1.6116
[2019-04-04 05:50:38,943] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49000, global step 780085: learning rate 0.0000
[2019-04-04 05:50:43,712] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49000, global step 782366: loss 1.4493
[2019-04-04 05:50:43,712] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49000, global step 782366: learning rate 0.0000
[2019-04-04 05:50:43,761] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49000, global step 782389: loss 1.4607
[2019-04-04 05:50:43,772] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49000, global step 782393: learning rate 0.0000
[2019-04-04 05:50:45,429] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49000, global step 783125: loss 1.4646
[2019-04-04 05:50:45,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49000, global step 783125: learning rate 0.0000
[2019-04-04 05:50:45,966] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49000, global step 783364: loss 1.5302
[2019-04-04 05:50:45,968] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49000, global step 783364: learning rate 0.0000
[2019-04-04 05:50:46,628] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49000, global step 783661: loss 1.4786
[2019-04-04 05:50:46,629] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49000, global step 783661: learning rate 0.0000
[2019-04-04 05:50:48,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0668981e-11 5.8570693e-10 3.5607292e-24 2.7349553e-11 4.6258327e-11
 2.1766008e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:48,165] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9545
[2019-04-04 05:50:48,197] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 38.0, 105.0, 788.0, 26.0, 26.9787364305754, 0.7710180941056747, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3940200.0000, 
sim time next is 3940800.0000, 
raw observation next is [-4.333333333333334, 38.0, 102.1666666666667, 777.3333333333334, 26.0, 26.98063411735293, 0.7798111095581369, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3425669436749769, 0.38, 0.34055555555555567, 0.8589318600368324, 0.6666666666666666, 0.7483861764460776, 0.7599370365193789, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19287412], dtype=float32), -0.23224856]. 
=============================================
[2019-04-04 05:50:48,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5602162e-11 9.3365468e-11 5.8815602e-26 2.2743222e-11 2.1019403e-11
 2.1262599e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:48,317] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5555
[2019-04-04 05:50:48,332] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.63457215665083, 0.6937527007454812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111800.0000, 
sim time next is 4112400.0000, 
raw observation next is [3.333333333333333, 32.33333333333334, 107.6666666666667, 800.0, 26.0, 26.86202403625707, 0.7233813160644081, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5549399815327793, 0.3233333333333334, 0.358888888888889, 0.8839779005524862, 0.6666666666666666, 0.7385020030214223, 0.7411271053548028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3161411], dtype=float32), -0.5718301]. 
=============================================
[2019-04-04 05:50:48,638] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49000, global step 784545: loss 1.5370
[2019-04-04 05:50:48,641] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49000, global step 784546: learning rate 0.0000
[2019-04-04 05:50:48,657] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49000, global step 784549: loss 1.5291
[2019-04-04 05:50:48,658] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49000, global step 784550: learning rate 0.0000
[2019-04-04 05:50:48,958] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49000, global step 784684: loss 1.5441
[2019-04-04 05:50:48,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49000, global step 784684: learning rate 0.0000
[2019-04-04 05:50:49,761] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49000, global step 785040: loss 1.4364
[2019-04-04 05:50:49,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49000, global step 785040: learning rate 0.0000
[2019-04-04 05:50:50,254] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49000, global step 785246: loss 1.4567
[2019-04-04 05:50:50,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49000, global step 785246: learning rate 0.0000
[2019-04-04 05:50:51,027] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49000, global step 785550: loss 1.5556
[2019-04-04 05:50:51,030] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49000, global step 785550: learning rate 0.0000
[2019-04-04 05:50:51,067] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49000, global step 785571: loss 1.5449
[2019-04-04 05:50:51,068] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49000, global step 785571: learning rate 0.0000
[2019-04-04 05:50:51,308] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49000, global step 785673: loss 1.5850
[2019-04-04 05:50:51,309] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49000, global step 785673: learning rate 0.0000
[2019-04-04 05:50:51,452] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.2919936e-11 5.9958172e-10 1.3333781e-23 9.5780855e-11 1.6568011e-11
 8.2186368e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:51,452] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4434
[2019-04-04 05:50:51,469] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 37.0, 118.5, 828.5, 26.0, 26.44405242298257, 0.5947328745432109, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4017600.0000, 
sim time next is 4018200.0000, 
raw observation next is [-5.666666666666667, 35.66666666666667, 118.3333333333333, 832.6666666666667, 26.0, 26.47164408829987, 0.6002544033777878, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.30563250230840255, 0.3566666666666667, 0.3944444444444443, 0.9200736648250462, 0.6666666666666666, 0.7059703406916557, 0.7000848011259292, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.003714], dtype=float32), 0.3426628]. 
=============================================
[2019-04-04 05:50:52,183] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49000, global step 785987: loss 1.5374
[2019-04-04 05:50:52,183] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49000, global step 785987: learning rate 0.0000
[2019-04-04 05:50:53,282] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0971843e-10 3.5024356e-10 1.7584183e-23 1.4220820e-10 5.7813702e-11
 2.0972528e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:50:53,282] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2405
[2019-04-04 05:50:53,290] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 32.66666666666666, 134.0, 817.3333333333334, 26.0, 25.12552069425379, 0.397363016391192, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4192800.0000, 
sim time next is 4193400.0000, 
raw observation next is [1.833333333333333, 33.33333333333334, 150.0, 787.6666666666667, 26.0, 25.11710093255336, 0.397348690780887, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.5133887349953832, 0.3333333333333334, 0.5, 0.8703499079189688, 0.6666666666666666, 0.5930917443794467, 0.632449563593629, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08208174], dtype=float32), -0.6569979]. 
=============================================
[2019-04-04 05:50:54,096] A3C_AGENT_WORKER-Thread-20 INFO:Local step 49500, global step 786749: loss 1.1169
[2019-04-04 05:50:54,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 49500, global step 786749: learning rate 0.0000
[2019-04-04 05:50:57,527] A3C_AGENT_WORKER-Thread-18 INFO:Local step 49500, global step 788192: loss 1.4485
[2019-04-04 05:50:57,529] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 49500, global step 788192: learning rate 0.0000
[2019-04-04 05:51:01,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.7578120e-10 9.2011909e-09 1.6647789e-24 3.8847039e-10 4.6566240e-10
 2.5388198e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:51:01,048] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7654
[2019-04-04 05:51:01,061] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.16666666666666, 0.0, 0.0, 26.0, 25.47298311021645, 0.5703616190214101, 0.0, 1.0, 44474.78444768094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.71966180432339, 0.5897693970202159, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6433051503602826, 0.6965897990067386, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9586577], dtype=float32), -0.056661423]. 
=============================================
[2019-04-04 05:51:02,311] A3C_AGENT_WORKER-Thread-2 INFO:Local step 49500, global step 790449: loss 2.0272
[2019-04-04 05:51:02,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 49500, global step 790449: learning rate 0.0000
[2019-04-04 05:51:02,595] A3C_AGENT_WORKER-Thread-5 INFO:Local step 49500, global step 790595: loss 2.0221
[2019-04-04 05:51:02,597] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 49500, global step 790595: learning rate 0.0000
[2019-04-04 05:51:02,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4855051e-09 3.6449330e-09 1.3344721e-23 7.9938850e-10 4.0465342e-09
 3.8601026e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:51:02,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8238
[2019-04-04 05:51:02,831] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 36.33333333333333, 15.33333333333333, 78.16666666666664, 26.0, 25.26867050908424, 0.3119569093327456, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4088400.0000, 
sim time next is 4089000.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 30.66666666666666, 156.3333333333333, 26.0, 25.35803340562426, 0.3253649882580697, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3471837488457987, 0.35166666666666674, 0.1022222222222222, 0.17274401473296497, 0.6666666666666666, 0.6131694504686882, 0.6084549960860232, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8913326], dtype=float32), -0.35031506]. 
=============================================
[2019-04-04 05:51:02,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.93031 ]
 [79.40605 ]
 [76.88741 ]
 [76.14315 ]
 [76.207726]], R is [[84.57360077]
 [84.72786713]
 [84.88059235]
 [84.06765747]
 [84.03712463]].
[2019-04-04 05:51:04,136] A3C_AGENT_WORKER-Thread-19 INFO:Local step 49500, global step 791341: loss 2.2212
[2019-04-04 05:51:04,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 49500, global step 791341: learning rate 0.0000
[2019-04-04 05:51:05,010] A3C_AGENT_WORKER-Thread-12 INFO:Local step 49500, global step 791830: loss 2.2120
[2019-04-04 05:51:05,012] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 49500, global step 791830: learning rate 0.0000
[2019-04-04 05:51:05,200] A3C_AGENT_WORKER-Thread-16 INFO:Local step 49500, global step 791916: loss 2.2288
[2019-04-04 05:51:05,202] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 49500, global step 791916: learning rate 0.0000
[2019-04-04 05:51:06,755] A3C_AGENT_WORKER-Thread-10 INFO:Local step 49500, global step 792780: loss 2.2391
[2019-04-04 05:51:06,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 49500, global step 792780: learning rate 0.0000
[2019-04-04 05:51:06,776] A3C_AGENT_WORKER-Thread-3 INFO:Local step 49500, global step 792790: loss 2.2835
[2019-04-04 05:51:06,778] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 49500, global step 792791: learning rate 0.0000
[2019-04-04 05:51:06,934] A3C_AGENT_WORKER-Thread-14 INFO:Local step 49500, global step 792872: loss 2.2016
[2019-04-04 05:51:06,936] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 49500, global step 792873: learning rate 0.0000
[2019-04-04 05:51:07,970] A3C_AGENT_WORKER-Thread-4 INFO:Local step 49500, global step 793398: loss 2.1905
[2019-04-04 05:51:07,972] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 49500, global step 793400: learning rate 0.0000
[2019-04-04 05:51:08,836] A3C_AGENT_WORKER-Thread-13 INFO:Local step 49500, global step 793856: loss 2.2899
[2019-04-04 05:51:08,838] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 49500, global step 793857: learning rate 0.0000
[2019-04-04 05:51:08,933] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4192732e-12 8.9740359e-12 1.7890592e-26 2.6659842e-12 1.8041119e-12
 7.1095380e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:51:08,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6103
[2019-04-04 05:51:08,968] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 52.66666666666666, 0.0, 26.0, 26.28517520741781, 0.6023588582923595, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4465200.0000, 
sim time next is 4465800.0000, 
raw observation next is [0.0, 78.0, 49.0, 0.0, 26.0, 26.26838611189486, 0.4917911544453467, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.16333333333333333, 0.0, 0.6666666666666666, 0.6890321759912382, 0.6639303848151156, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1610339], dtype=float32), -0.08221435]. 
=============================================
[2019-04-04 05:51:09,309] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50000, global step 794118: loss 0.1912
[2019-04-04 05:51:09,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50000, global step 794118: learning rate 0.0000
[2019-04-04 05:51:09,311] A3C_AGENT_WORKER-Thread-15 INFO:Local step 49500, global step 794118: loss 2.2360
[2019-04-04 05:51:09,315] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 49500, global step 794118: learning rate 0.0000
[2019-04-04 05:51:09,444] A3C_AGENT_WORKER-Thread-6 INFO:Local step 49500, global step 794185: loss 2.2207
[2019-04-04 05:51:09,446] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 49500, global step 794185: learning rate 0.0000
[2019-04-04 05:51:09,499] A3C_AGENT_WORKER-Thread-17 INFO:Local step 49500, global step 794212: loss 2.1429
[2019-04-04 05:51:09,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 49500, global step 794212: learning rate 0.0000
[2019-04-04 05:51:10,308] A3C_AGENT_WORKER-Thread-11 INFO:Local step 49500, global step 794638: loss 2.2327
[2019-04-04 05:51:10,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 49500, global step 794638: learning rate 0.0000
[2019-04-04 05:51:11,119] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.4130846e-09 1.7836063e-09 1.8428558e-23 2.9912315e-09 7.1835304e-10
 1.3450077e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:51:11,119] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2301
[2019-04-04 05:51:11,131] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 25.44761343293107, 0.3679362901582939, 0.0, 1.0, 43844.53831833181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4227600.0000, 
sim time next is 4228200.0000, 
raw observation next is [1.0, 47.0, 0.0, 0.0, 26.0, 25.425181894845, 0.3692935434976026, 0.0, 1.0, 51830.97270445188], 
processed observation next is [0.0, 0.9565217391304348, 0.4903047091412743, 0.47, 0.0, 0.0, 0.6666666666666666, 0.61876515790375, 0.6230978478325342, 0.0, 1.0, 0.24681415573548515], 
reward next is 0.7532, 
noisyNet noise sample is [array([0.31215194], dtype=float32), -0.49011922]. 
=============================================
[2019-04-04 05:51:11,449] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50000, global step 795306: loss 0.1871
[2019-04-04 05:51:11,450] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50000, global step 795306: learning rate 0.0000
[2019-04-04 05:51:14,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1946585e-11 2.8795513e-11 9.1225400e-27 1.2692647e-11 8.2114957e-12
 3.4487861e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:51:14,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9481
[2019-04-04 05:51:14,827] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.15, 34.5, 81.66666666666667, 0.0, 26.0, 28.80457531143676, 1.144815157405964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4377000.0000, 
sim time next is 4377600.0000, 
raw observation next is [13.0, 35.0, 71.0, 0.0, 26.0, 28.5015611102056, 0.9442803419856131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.35, 0.23666666666666666, 0.0, 0.6666666666666666, 0.8751300925171334, 0.8147601139952044, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8182719], dtype=float32), -0.4965859]. 
=============================================
[2019-04-04 05:51:16,658] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50000, global step 798201: loss 0.0538
[2019-04-04 05:51:16,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50000, global step 798201: learning rate 0.0000
[2019-04-04 05:51:17,247] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50000, global step 798488: loss 0.0423
[2019-04-04 05:51:17,248] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50000, global step 798488: learning rate 0.0000
[2019-04-04 05:51:18,196] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9011694e-11 1.4436667e-10 1.3442646e-26 2.5766729e-11 9.4909407e-11
 1.7365087e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:51:18,199] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7816
[2019-04-04 05:51:18,215] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.26666666666667, 46.0, 0.0, 0.0, 26.0, 27.9145058207699, 1.037801039967756, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4386000.0000, 
sim time next is 4386600.0000, 
raw observation next is [12.2, 47.0, 0.0, 0.0, 26.0, 27.94701596778084, 1.026292969253263, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8005540166204987, 0.47, 0.0, 0.0, 0.6666666666666666, 0.8289179973150699, 0.8420976564177544, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06072915], dtype=float32), 0.98348016]. 
=============================================
[2019-04-04 05:51:18,232] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50000, global step 799034: loss 0.0450
[2019-04-04 05:51:18,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50000, global step 799034: learning rate 0.0000
[2019-04-04 05:51:19,379] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50000, global step 799684: loss 0.0402
[2019-04-04 05:51:19,379] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50000, global step 799684: learning rate 0.0000
[2019-04-04 05:51:19,535] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50000, global step 799767: loss 0.0392
[2019-04-04 05:51:19,536] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50000, global step 799767: learning rate 0.0000
[2019-04-04 05:51:20,009] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 05:51:20,010] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 05:51:20,010] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:51:20,010] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 05:51:20,010] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 05:51:20,011] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:51:20,011] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:51:20,014] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run9
[2019-04-04 05:51:20,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run9
[2019-04-04 05:51:20,048] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run9
[2019-04-04 05:51:52,050] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.27953452], dtype=float32), 0.09106537]
[2019-04-04 05:51:52,051] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.841808458166667, 87.09854978333334, 0.0, 0.0, 26.0, 24.79702428370646, 0.1996233538956461, 0.0, 1.0, 39402.54813595449]
[2019-04-04 05:51:52,051] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:51:52,052] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.8386241e-09 1.2195311e-09 8.4570533e-25 2.9484523e-10 7.3686535e-10
 7.0630497e-13 1.0000000e+00], sampled 0.8563911157129886
[2019-04-04 05:51:52,498] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.27953452], dtype=float32), 0.09106537]
[2019-04-04 05:51:52,498] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.4629791434999999, 85.488187615, 0.0, 0.0, 26.0, 24.33908553730931, 0.1160142519882145, 0.0, 1.0, 40602.83539914664]
[2019-04-04 05:51:52,498] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:51:52,499] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.5335317e-09 2.4768778e-09 9.0723721e-24 6.3493899e-10 1.5726705e-09
 2.2089689e-12 1.0000000e+00], sampled 0.3171888085032881
[2019-04-04 05:52:19,545] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.27953452], dtype=float32), 0.09106537]
[2019-04-04 05:52:19,545] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.6868012855, 89.7252833, 0.0, 0.0, 26.0, 25.48396650591727, 0.6195793963076414, 0.0, 1.0, 70637.26076075534]
[2019-04-04 05:52:19,545] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 05:52:19,546] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.5441745e-10 3.6287826e-10 1.1170867e-25 7.8513862e-11 2.2504135e-10
 1.6799845e-13 1.0000000e+00], sampled 0.7937183656197716
[2019-04-04 05:52:44,471] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.27953452], dtype=float32), 0.09106537]
[2019-04-04 05:52:44,471] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.2, 81.0, 155.5, 178.5, 26.0, 25.97713144699702, 0.5910313433425521, 1.0, 1.0, 0.0]
[2019-04-04 05:52:44,471] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 05:52:44,472] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2418839e-11 5.8951836e-11 1.2525198e-25 2.2766095e-11 1.7863622e-11
 6.3438695e-14 1.0000000e+00], sampled 0.612269440327216
[2019-04-04 05:54:15,714] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.27953452], dtype=float32), 0.09106537]
[2019-04-04 05:54:15,714] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862239525782, 0.4026604670861376, 0.0, 1.0, 44162.28474514392]
[2019-04-04 05:54:15,714] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 05:54:15,715] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.1132624e-09 6.5433277e-09 1.2402456e-22 1.6403012e-09 4.1309436e-09
 9.7458890e-12 1.0000000e+00], sampled 0.533030463970886
[2019-04-04 05:54:26,085] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7956 239862923.3492 1605.0219
[2019-04-04 05:54:55,127] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 05:55:00,363] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 05:55:01,402] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 800000, evaluation results [800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.795603099225, 239862923.34916055, 1605.0219348007852, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 05:55:03,474] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50000, global step 800668: loss 0.0420
[2019-04-04 05:55:03,476] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50000, global step 800668: learning rate 0.0000
[2019-04-04 05:55:03,743] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50000, global step 800733: loss 0.0388
[2019-04-04 05:55:03,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50000, global step 800733: learning rate 0.0000
[2019-04-04 05:55:03,976] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50000, global step 800802: loss 0.0350
[2019-04-04 05:55:03,978] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50000, global step 800802: learning rate 0.0000
[2019-04-04 05:55:04,216] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1912416e-10 1.6442417e-09 3.0534822e-24 1.5661561e-10 1.5660225e-09
 1.4336660e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:04,216] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2725
[2019-04-04 05:55:04,265] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.00732214993347, 0.4629934539619163, 1.0, 1.0, 45851.15365175533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4478400.0000, 
sim time next is 4479000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.04367462954371, 0.4639409576811991, 0.0, 1.0, 18709.41619102495], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.586972885795309, 0.654646985893733, 0.0, 1.0, 0.08909245805249975], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.8967141], dtype=float32), -0.5090408]. 
=============================================
[2019-04-04 05:55:04,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.23233 ]
 [81.35198 ]
 [83.066734]
 [82.019394]
 [81.01691 ]], R is [[80.10005188]
 [80.08071136]
 [79.75814819]
 [79.51083374]
 [79.60198975]].
[2019-04-04 05:55:05,504] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50000, global step 801225: loss 0.0334
[2019-04-04 05:55:05,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50000, global step 801225: learning rate 0.0000
[2019-04-04 05:55:06,827] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50000, global step 801667: loss 0.0390
[2019-04-04 05:55:06,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50000, global step 801667: learning rate 0.0000
[2019-04-04 05:55:08,069] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50000, global step 802016: loss 0.0394
[2019-04-04 05:55:08,070] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50000, global step 802016: learning rate 0.0000
[2019-04-04 05:55:08,101] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50000, global step 802021: loss 0.0441
[2019-04-04 05:55:08,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50000, global step 802021: learning rate 0.0000
[2019-04-04 05:55:08,122] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50000, global step 802028: loss 0.0428
[2019-04-04 05:55:08,144] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50000, global step 802028: learning rate 0.0000
[2019-04-04 05:55:08,837] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50000, global step 802220: loss 0.0409
[2019-04-04 05:55:08,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50000, global step 802220: learning rate 0.0000
[2019-04-04 05:55:09,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5936347e-10 3.0996913e-10 1.5822044e-25 1.3897013e-10 3.1775713e-10
 7.9261744e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:09,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1738
[2019-04-04 05:55:09,726] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 143.5, 340.0, 26.0, 26.09284538978513, 0.5234936235737083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4611600.0000, 
sim time next is 4612200.0000, 
raw observation next is [-1.666666666666667, 69.16666666666667, 150.3333333333333, 396.3333333333334, 26.0, 26.10163241899538, 0.5266342955707605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4164358264081256, 0.6916666666666668, 0.501111111111111, 0.437937384898711, 0.6666666666666666, 0.6751360349162816, 0.6755447651902534, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00338309], dtype=float32), 1.1441361]. 
=============================================
[2019-04-04 05:55:10,739] A3C_AGENT_WORKER-Thread-20 INFO:Local step 50500, global step 802783: loss 0.0619
[2019-04-04 05:55:10,740] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 50500, global step 802783: learning rate 0.0000
[2019-04-04 05:55:14,618] A3C_AGENT_WORKER-Thread-18 INFO:Local step 50500, global step 804085: loss 0.0761
[2019-04-04 05:55:14,619] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 50500, global step 804085: learning rate 0.0000
[2019-04-04 05:55:15,050] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5355915e-12 1.8060442e-11 2.1369385e-27 3.9893735e-12 6.0382623e-12
 3.5712743e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:15,050] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1079
[2019-04-04 05:55:15,101] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 61.83333333333333, 152.3333333333333, 595.0, 26.0, 26.36441169091865, 0.6084342226024259, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4614600.0000, 
sim time next is 4615200.0000, 
raw observation next is [0.0, 60.0, 146.5, 638.0, 26.0, 26.44661745943677, 0.6264626186598731, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.48833333333333334, 0.7049723756906078, 0.6666666666666666, 0.7038847882863974, 0.7088208728866244, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3992054], dtype=float32), 0.7718484]. 
=============================================
[2019-04-04 05:55:21,928] A3C_AGENT_WORKER-Thread-2 INFO:Local step 50500, global step 806461: loss 0.0803
[2019-04-04 05:55:21,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 50500, global step 806462: learning rate 0.0000
[2019-04-04 05:55:22,135] A3C_AGENT_WORKER-Thread-5 INFO:Local step 50500, global step 806532: loss 0.0745
[2019-04-04 05:55:22,136] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 50500, global step 806532: learning rate 0.0000
[2019-04-04 05:55:24,924] A3C_AGENT_WORKER-Thread-19 INFO:Local step 50500, global step 807303: loss 0.0750
[2019-04-04 05:55:24,927] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 50500, global step 807303: learning rate 0.0000
[2019-04-04 05:55:26,130] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0350456e-11 4.3921672e-10 6.7511253e-24 6.3093190e-11 9.7967551e-11
 8.1229489e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:26,130] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0525
[2019-04-04 05:55:26,169] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 82.66666666666667, 0.0, 0.0, 26.0, 25.09898827048732, 0.5020641238172933, 0.0, 1.0, 73966.59840148574], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4740000.0000, 
sim time next is 4740600.0000, 
raw observation next is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 26.0, 25.27128243403869, 0.523841366176364, 0.0, 1.0, 52557.40115099897], 
processed observation next is [1.0, 0.8695652173913043, 0.41181902123730385, 0.8383333333333334, 0.0, 0.0, 0.6666666666666666, 0.6059402028365574, 0.6746137887254546, 0.0, 1.0, 0.2502733388142808], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.5417208], dtype=float32), 0.8393138]. 
=============================================
[2019-04-04 05:55:27,515] A3C_AGENT_WORKER-Thread-12 INFO:Local step 50500, global step 807912: loss 0.0411
[2019-04-04 05:55:27,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 50500, global step 807912: learning rate 0.0000
[2019-04-04 05:55:27,882] A3C_AGENT_WORKER-Thread-16 INFO:Local step 50500, global step 807981: loss 0.0391
[2019-04-04 05:55:27,882] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 50500, global step 807981: learning rate 0.0000
[2019-04-04 05:55:30,471] A3C_AGENT_WORKER-Thread-14 INFO:Local step 50500, global step 808659: loss 0.0410
[2019-04-04 05:55:30,488] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 50500, global step 808659: learning rate 0.0000
[2019-04-04 05:55:30,998] A3C_AGENT_WORKER-Thread-3 INFO:Local step 50500, global step 808789: loss 0.0434
[2019-04-04 05:55:31,060] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 50500, global step 808789: learning rate 0.0000
[2019-04-04 05:55:31,468] A3C_AGENT_WORKER-Thread-10 INFO:Local step 50500, global step 808887: loss 0.0389
[2019-04-04 05:55:31,472] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 50500, global step 808887: learning rate 0.0000
[2019-04-04 05:55:32,511] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0937339e-08 3.5194065e-09 2.9413834e-23 8.7804358e-10 7.5926518e-09
 8.6107952e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:32,511] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8198
[2019-04-04 05:55:32,577] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.30102702225793, 0.1962472821899956, 0.0, 1.0, 41313.08952729549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4771200.0000, 
sim time next is 4771800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.31275900916256, 0.188153224704694, 0.0, 1.0, 41334.46556223487], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5260632507635465, 0.5627177415682313, 0.0, 1.0, 0.19683078839159462], 
reward next is 0.8032, 
noisyNet noise sample is [array([0.3223968], dtype=float32), -0.27311242]. 
=============================================
[2019-04-04 05:55:33,745] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0145407e-10 1.4250858e-09 3.7469337e-23 2.4258812e-10 7.5858009e-10
 1.0174367e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:33,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7816
[2019-04-04 05:55:33,850] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 199.5, 398.0, 26.0, 25.07151881637233, 0.3709688122588122, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4892400.0000, 
sim time next is 4893000.0000, 
raw observation next is [3.0, 45.0, 187.3333333333333, 406.0, 26.0, 25.08049739751291, 0.3743944478452382, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.6244444444444442, 0.4486187845303867, 0.6666666666666666, 0.5900414497927425, 0.624798149281746, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2761188], dtype=float32), -0.040915646]. 
=============================================
[2019-04-04 05:55:33,887] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.01747 ]
 [80.199394]
 [80.376915]
 [80.43865 ]
 [80.49234 ]], R is [[80.01977539]
 [80.2195816 ]
 [80.41738892]
 [80.5242157 ]
 [80.62997437]].
[2019-04-04 05:55:33,950] A3C_AGENT_WORKER-Thread-13 INFO:Local step 50500, global step 809445: loss 0.0353
[2019-04-04 05:55:33,970] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 50500, global step 809448: learning rate 0.0000
[2019-04-04 05:55:34,120] A3C_AGENT_WORKER-Thread-4 INFO:Local step 50500, global step 809475: loss 0.0369
[2019-04-04 05:55:34,209] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 50500, global step 809477: learning rate 0.0000
[2019-04-04 05:55:36,386] A3C_AGENT_WORKER-Thread-17 INFO:Local step 50500, global step 809962: loss 0.0274
[2019-04-04 05:55:36,387] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 50500, global step 809962: learning rate 0.0000
[2019-04-04 05:55:36,477] A3C_AGENT_WORKER-Thread-15 INFO:Local step 50500, global step 809971: loss 0.0333
[2019-04-04 05:55:36,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 50500, global step 809971: learning rate 0.0000
[2019-04-04 05:55:36,682] A3C_AGENT_WORKER-Thread-6 INFO:Local step 50500, global step 810021: loss 0.0217
[2019-04-04 05:55:36,696] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 50500, global step 810022: learning rate 0.0000
[2019-04-04 05:55:38,206] A3C_AGENT_WORKER-Thread-11 INFO:Local step 50500, global step 810354: loss 0.0274
[2019-04-04 05:55:38,207] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 50500, global step 810354: learning rate 0.0000
[2019-04-04 05:55:39,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:55:39,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:55:39,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run7
[2019-04-04 05:55:39,416] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3907560e-09 2.4876203e-09 2.3745747e-23 7.4987250e-10 7.7350926e-10
 2.3734831e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:39,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7676
[2019-04-04 05:55:39,473] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.38710764350263, 0.383683233206179, 0.0, 1.0, 50627.1191802789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4837800.0000, 
sim time next is 4838400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.441979581261, 0.3800203191248455, 0.0, 1.0, 18761.16361679431], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6201649651050832, 0.6266734397082818, 0.0, 1.0, 0.08933887436568719], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.03824285], dtype=float32), -0.5709366]. 
=============================================
[2019-04-04 05:55:43,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:55:43,786] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:55:43,790] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run7
[2019-04-04 05:55:53,844] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.6545917e-09 1.5465716e-08 1.4604971e-22 1.3920551e-09 1.4885856e-09
 1.5165605e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:55:53,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6471
[2019-04-04 05:55:53,863] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.20723075970928, 0.2692747091700548, 0.0, 1.0, 38417.01644168738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4942800.0000, 
sim time next is 4943400.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.18779624505439, 0.2629710601562481, 0.0, 1.0, 38472.25367918319], 
processed observation next is [1.0, 0.21739130434782608, 0.4025854108956602, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.5989830204211991, 0.5876570200520826, 0.0, 1.0, 0.18320120799611042], 
reward next is 0.8168, 
noisyNet noise sample is [array([1.5473784], dtype=float32), -0.3959703]. 
=============================================
[2019-04-04 05:55:54,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:55:54,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:55:54,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run7
[2019-04-04 05:55:56,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:55:56,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:55:56,220] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run7
[2019-04-04 05:55:59,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:55:59,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:55:59,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run7
[2019-04-04 05:56:01,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:01,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:01,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run7
[2019-04-04 05:56:01,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:01,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:01,983] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run7
[2019-04-04 05:56:07,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:07,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:07,468] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run7
[2019-04-04 05:56:07,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:07,596] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:07,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run7
[2019-04-04 05:56:07,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8661012e-12 5.8965866e-12 7.5474823e-29 1.6817230e-12 2.6097163e-12
 5.8775754e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 05:56:07,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3548
[2019-04-04 05:56:07,693] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 18.33333333333334, 79.33333333333333, 611.6666666666666, 26.0, 28.90820763771088, 1.186252323885489, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5070000.0000, 
sim time next is 5070600.0000, 
raw observation next is [12.0, 18.0, 76.0, 585.0, 26.0, 29.01413291508042, 1.19541396272036, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.18, 0.25333333333333335, 0.6464088397790055, 0.6666666666666666, 0.917844409590035, 0.8984713209067866, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5748629], dtype=float32), 0.5947652]. 
=============================================
[2019-04-04 05:56:07,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:07,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:07,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run7
[2019-04-04 05:56:10,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:10,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:10,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run7
[2019-04-04 05:56:10,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:10,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:10,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run7
[2019-04-04 05:56:11,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:11,109] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:11,113] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run7
[2019-04-04 05:56:11,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:11,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:11,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run7
[2019-04-04 05:56:11,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:11,534] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:11,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run7
[2019-04-04 05:56:12,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 05:56:12,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 05:56:12,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run7
[2019-04-04 05:56:32,123] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5582384e-11 2.0915515e-10 1.4201250e-25 3.2112382e-11 1.6253570e-11
 2.9483859e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:56:32,145] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4694
[2019-04-04 05:56:32,228] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.799999999999999, 82.0, 189.0, 32.16666666666666, 26.0, 25.3966182823171, 0.2883243135245966, 1.0, 1.0, 42240.28206117477], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 124800.0000, 
sim time next is 125400.0000, 
raw observation next is [-7.8, 84.0, 188.0, 28.33333333333334, 26.0, 25.31844125748749, 0.2955977974339073, 1.0, 1.0, 41962.92698271617], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.84, 0.6266666666666667, 0.03130755064456722, 0.6666666666666666, 0.6098701047906241, 0.5985325991446357, 1.0, 1.0, 0.19982346182245794], 
reward next is 0.8002, 
noisyNet noise sample is [array([-1.0007795], dtype=float32), 0.027478626]. 
=============================================
[2019-04-04 05:56:34,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8088399e-09 1.3079051e-08 2.9228974e-21 1.9230506e-09 1.0764753e-08
 9.4148673e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:56:34,170] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7834
[2019-04-04 05:56:34,229] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.36110842084121, -0.1025044071657327, 0.0, 1.0, 44088.86109029114], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 184200.0000, 
sim time next is 184800.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 23.31624409276012, -0.1102817337009118, 0.0, 1.0, 44126.68170926346], 
processed observation next is [1.0, 0.13043478260869565, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.44302034106334326, 0.46323942209969604, 0.0, 1.0, 0.21012705575839744], 
reward next is 0.7899, 
noisyNet noise sample is [array([-0.22367823], dtype=float32), 0.7343419]. 
=============================================
[2019-04-04 05:56:35,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.4022004e-10 1.3182679e-09 2.4026753e-23 1.2551123e-10 1.2564846e-10
 5.9651832e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:56:35,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4069
[2019-04-04 05:56:35,370] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.15229530791034, 0.2600162836125674, 1.0, 1.0, 20643.80146537471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 238200.0000, 
sim time next is 238800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.15583172211258, 0.2234791573598836, 1.0, 1.0, 26506.78941323305], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5963193101760483, 0.5744930524532945, 1.0, 1.0, 0.1262228067296812], 
reward next is 0.8738, 
noisyNet noise sample is [array([-1.0219105], dtype=float32), -0.8594149]. 
=============================================
[2019-04-04 05:56:38,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8386238e-09 8.3154372e-09 3.0897987e-21 3.5112735e-09 4.8678777e-09
 6.0583386e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:56:38,639] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6428
[2019-04-04 05:56:38,678] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.71135720381717, -0.2543395165221862, 0.0, 1.0, 44953.60286300204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.65566635602637, -0.2599609174470396, 0.0, 1.0, 44957.70212229715], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.38797219633553076, 0.4133463608509868, 0.0, 1.0, 0.21408429582046262], 
reward next is 0.7859, 
noisyNet noise sample is [array([-0.27885637], dtype=float32), -0.28981775]. 
=============================================
[2019-04-04 05:56:42,701] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.7409925e-10 8.9406322e-09 7.0410125e-22 1.2335136e-09 2.4356466e-09
 8.1608132e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:56:42,701] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8645
[2019-04-04 05:56:42,749] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 73.33333333333333, 0.0, 0.0, 26.0, 24.54567363774285, 0.1784882008017659, 0.0, 1.0, 44155.79998082417], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 251400.0000, 
sim time next is 252000.0000, 
raw observation next is [-3.9, 75.0, 0.0, 0.0, 26.0, 24.51369520840438, 0.1730210927603322, 0.0, 1.0, 44197.73220390179], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5428079340336982, 0.5576736975867774, 0.0, 1.0, 0.21046539144715137], 
reward next is 0.7895, 
noisyNet noise sample is [array([1.0213691], dtype=float32), 0.66819537]. 
=============================================
[2019-04-04 05:56:42,775] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.99786]
 [75.87672]
 [75.67657]
 [75.43562]
 [75.21492]], R is [[76.1697464 ]
 [76.19778442]
 [76.22476196]
 [76.24970245]
 [76.27428436]].
[2019-04-04 05:56:56,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6656620e-11 5.3935417e-10 5.6731453e-24 1.2327581e-10 3.7956124e-10
 3.4462762e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:56:56,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9465
[2019-04-04 05:56:56,803] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.88333333333333, 51.0, 57.0, 899.0, 26.0, 25.76993970437299, 0.2908747732070613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 391800.0000, 
sim time next is 392400.0000, 
raw observation next is [-11.7, 51.0, 56.5, 896.0, 26.0, 25.86460845083914, 0.4067784625772552, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.13850415512465375, 0.51, 0.18833333333333332, 0.9900552486187846, 0.6666666666666666, 0.6553840375699282, 0.6355928208590851, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4715806], dtype=float32), -0.88249844]. 
=============================================
[2019-04-04 05:56:57,839] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.9329630e-08 2.2343810e-08 3.1176111e-20 9.2283443e-09 1.1894467e-08
 8.5513902e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 05:56:57,839] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9696
[2019-04-04 05:56:57,853] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.66666666666667, 69.0, 0.0, 0.0, 26.0, 23.25581194638117, -0.1130215196485651, 0.0, 1.0, 47800.39444627356], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 350400.0000, 
sim time next is 351000.0000, 
raw observation next is [-14.75, 69.0, 0.0, 0.0, 26.0, 23.29819291892085, -0.1212209936813704, 0.0, 1.0, 47895.43855705271], 
processed observation next is [1.0, 0.043478260869565216, 0.05401662049861495, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4415160765767376, 0.45959300210620985, 0.0, 1.0, 0.22807351693834624], 
reward next is 0.7719, 
noisyNet noise sample is [array([-0.32684475], dtype=float32), 0.4563187]. 
=============================================
[2019-04-04 05:56:57,879] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[66.77012 ]
 [66.87347 ]
 [66.978905]
 [67.15332 ]
 [67.23574 ]], R is [[66.81232452]
 [66.9165802 ]
 [67.0201416 ]
 [67.12290955]
 [67.22486877]].
[2019-04-04 05:57:05,468] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6474507e-08 6.1327547e-08 1.7729020e-20 7.0834352e-09 1.3880522e-08
 9.1301328e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 05:57:05,468] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4183
[2019-04-04 05:57:05,481] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.42712149837105, -0.1163161861369142, 0.0, 1.0, 45688.52046904894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 438600.0000, 
sim time next is 439200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.34885006994265, -0.1232588294714476, 0.0, 1.0, 45747.35838805421], 
processed observation next is [1.0, 0.08695652173913043, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.44573750582855415, 0.4589137235095175, 0.0, 1.0, 0.21784456375263908], 
reward next is 0.7822, 
noisyNet noise sample is [array([-0.98274064], dtype=float32), -0.8441225]. 
=============================================
[2019-04-04 05:57:06,021] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2007386e-09 4.7756615e-10 3.0919163e-24 7.4481463e-11 1.5122746e-10
 7.7810707e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:06,021] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1827
[2019-04-04 05:57:06,073] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 85.0, 41.0, 45.0, 26.0, 24.97017561923451, 0.3040882156890815, 0.0, 1.0, 45546.85599563065], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 577800.0000, 
sim time next is 578400.0000, 
raw observation next is [-1.533333333333333, 85.66666666666667, 34.16666666666667, 37.5, 26.0, 24.95605252552194, 0.3027443074381306, 0.0, 1.0, 50127.51130480439], 
processed observation next is [0.0, 0.6956521739130435, 0.42012927054478305, 0.8566666666666667, 0.1138888888888889, 0.04143646408839779, 0.6666666666666666, 0.579671043793495, 0.6009147691460436, 0.0, 1.0, 0.2387024347847828], 
reward next is 0.7613, 
noisyNet noise sample is [array([-0.8055765], dtype=float32), 0.70878637]. 
=============================================
[2019-04-04 05:57:15,011] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.1624996e-10 1.8209950e-10 6.2845535e-26 8.5800436e-11 3.5347633e-10
 3.3812181e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:15,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5570
[2019-04-04 05:57:15,084] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 91.0, 89.0, 103.5, 26.0, 25.12168733719194, 0.295139307621884, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.08878445193561, 0.2881411100600778, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4598337950138504, 0.9033333333333334, 0.3577777777777777, 0.11418047882136276, 0.6666666666666666, 0.5907320376613008, 0.5960470366866927, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.350798], dtype=float32), -0.04281414]. 
=============================================
[2019-04-04 05:57:15,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6717921e-09 9.7076069e-10 5.6987832e-24 1.3024916e-10 1.2023238e-10
 7.5652570e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:15,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1309
[2019-04-04 05:57:15,810] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 83.0, 0.0, 0.0, 26.0, 24.85455687433885, 0.2462084852582791, 0.0, 1.0, 42900.86715484204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 598800.0000, 
sim time next is 599400.0000, 
raw observation next is [-3.1, 83.0, 0.0, 0.0, 26.0, 24.8582481376911, 0.2414491535464535, 0.0, 1.0, 42839.56651582883], 
processed observation next is [0.0, 0.9565217391304348, 0.37673130193905824, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5715206781409249, 0.5804830511821512, 0.0, 1.0, 0.2039979357896611], 
reward next is 0.7960, 
noisyNet noise sample is [array([-1.1985756], dtype=float32), 1.851412]. 
=============================================
[2019-04-04 05:57:19,529] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0341198e-09 4.2648143e-09 2.6627066e-23 1.4681538e-09 2.9335159e-09
 1.8410998e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:19,548] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4135
[2019-04-04 05:57:19,564] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.82335430505099, 0.1772772369142697, 0.0, 1.0, 41969.42287775488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 684000.0000, 
sim time next is 684600.0000, 
raw observation next is [-3.483333333333333, 69.33333333333333, 0.0, 0.0, 26.0, 24.79255871472069, 0.1695037793280072, 0.0, 1.0, 41903.01177795805], 
processed observation next is [0.0, 0.9565217391304348, 0.3661126500461681, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.5660465595600576, 0.5565012597760024, 0.0, 1.0, 0.19953815132360975], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.23153286], dtype=float32), 0.9025134]. 
=============================================
[2019-04-04 05:57:20,636] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2707219e-09 3.5308232e-09 6.7001720e-24 6.0507083e-10 8.3138313e-10
 3.8901794e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:20,639] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8075
[2019-04-04 05:57:20,652] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 75.0, 0.0, 0.0, 26.0, 24.29300790817129, 0.06332658127738273, 0.0, 1.0, 41551.88538604445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 703200.0000, 
sim time next is 703800.0000, 
raw observation next is [-3.1, 75.0, 0.0, 0.0, 26.0, 24.36372942839149, 0.05828471405456568, 0.0, 1.0, 41572.65702182856], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5303107856992909, 0.5194282380181886, 0.0, 1.0, 0.19796503343727886], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.62405974], dtype=float32), -0.8787688]. 
=============================================
[2019-04-04 05:57:20,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7242749e-09 2.2121556e-09 3.2358907e-24 2.4019445e-10 2.3836386e-10
 5.6708728e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:20,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2725
[2019-04-04 05:57:20,879] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.24417802198495, 0.01725057346043914, 0.0, 1.0, 41981.65673939204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 715800.0000, 
sim time next is 716400.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.17120464091381, 0.005628991457353807, 0.0, 1.0, 42035.76293784881], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5142670534094842, 0.5018763304857846, 0.0, 1.0, 0.20017029970404196], 
reward next is 0.7998, 
noisyNet noise sample is [array([-0.87358046], dtype=float32), -0.94414276]. 
=============================================
[2019-04-04 05:57:38,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4248914e-11 1.8638577e-11 6.5779637e-28 7.4860560e-12 4.9920701e-12
 6.8905012e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:38,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8162
[2019-04-04 05:57:38,128] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 25.76935418584988, 0.3958771348322467, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922800.0000, 
sim time next is 923400.0000, 
raw observation next is [4.7, 92.5, 18.0, 0.0, 26.0, 25.72368528647226, 0.2909613676929741, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.592797783933518, 0.925, 0.06, 0.0, 0.6666666666666666, 0.6436404405393551, 0.5969871225643247, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1850259], dtype=float32), 0.52714306]. 
=============================================
[2019-04-04 05:57:38,136] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1702748e-10 2.1825063e-11 4.2445098e-28 1.1857347e-11 8.3256770e-12
 6.9288096e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:38,138] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3973
[2019-04-04 05:57:38,150] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.45994589277844, 0.467837792626999, 0.0, 1.0, 40248.97528848458], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 971400.0000, 
sim time next is 972000.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.57205301863976, 0.4794603013826313, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.63100441821998, 0.6598201004608771, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05891207], dtype=float32), 1.180304]. 
=============================================
[2019-04-04 05:57:38,171] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[94.5731  ]
 [94.44633 ]
 [94.232956]
 [94.2181  ]
 [94.28124 ]], R is [[94.52371979]
 [94.38682556]
 [94.18112946]
 [94.15003204]
 [94.20853424]].
[2019-04-04 05:57:38,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5483654e-10 2.9578928e-10 3.4903917e-25 4.7010278e-11 6.7878259e-11
 3.5149249e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:38,320] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7751
[2019-04-04 05:57:38,324] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.55, 55.0, 0.0, 0.0, 26.0, 26.47225836474515, 0.7600301564570654, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1103400.0000, 
sim time next is 1104000.0000, 
raw observation next is [15.36666666666667, 55.66666666666667, 0.0, 0.0, 26.0, 26.38882239146096, 0.7524960471296692, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8882733148661128, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.6990685326217466, 0.7508320157098898, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06601091], dtype=float32), -0.53620756]. 
=============================================
[2019-04-04 05:57:38,329] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[90.89169 ]
 [90.87047 ]
 [90.8898  ]
 [90.848434]
 [90.82418 ]], R is [[91.00292206]
 [91.09289551]
 [91.18196869]
 [91.27014923]
 [91.35745239]].
[2019-04-04 05:57:41,308] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.8493770e-12 2.4983498e-11 1.4677476e-28 3.2509061e-12 7.2190700e-12
 3.6389065e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:41,308] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7579
[2019-04-04 05:57:41,329] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.66666666666667, 0.0, 0.0, 26.0, 25.86425972913853, 0.6231090142585715, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1028400.0000, 
sim time next is 1029000.0000, 
raw observation next is [14.4, 75.33333333333333, 0.0, 0.0, 26.0, 25.889127767212, 0.6213847806557112, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7533333333333333, 0.0, 0.0, 0.6666666666666666, 0.6574273139343333, 0.7071282602185703, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49631178], dtype=float32), -0.2304427]. 
=============================================
[2019-04-04 05:57:41,363] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[97.8935  ]
 [97.8148  ]
 [97.89192 ]
 [98.1852  ]
 [98.404625]], R is [[97.81652832]
 [97.83836365]
 [97.85997772]
 [97.88137817]
 [97.902565  ]].
[2019-04-04 05:57:43,492] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.1940421e-10 4.2142573e-10 5.4832864e-25 1.0931926e-10 2.1734346e-10
 4.3356885e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:43,537] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8262
[2019-04-04 05:57:43,548] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.78651452917165, 0.2196883615292922, 0.0, 1.0, 39436.89078638557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 868800.0000, 
sim time next is 869400.0000, 
raw observation next is [-2.0, 79.5, 0.0, 0.0, 26.0, 24.75598200917459, 0.2241161367041825, 0.0, 1.0, 39419.24702649705], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.795, 0.0, 0.0, 0.6666666666666666, 0.5629985007645493, 0.5747053789013942, 0.0, 1.0, 0.1877107001261764], 
reward next is 0.8123, 
noisyNet noise sample is [array([0.07076894], dtype=float32), 0.72512615]. 
=============================================
[2019-04-04 05:57:45,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.7686477e-12 1.5431866e-11 6.3175529e-27 2.6743926e-12 1.4380424e-12
 2.2135554e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:45,670] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0664
[2019-04-04 05:57:45,707] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 83.33333333333333, 57.66666666666666, 0.0, 26.0, 25.43387517819533, 0.2876799390094029, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 899400.0000, 
sim time next is 900000.0000, 
raw observation next is [1.1, 84.0, 62.5, 0.0, 26.0, 25.4405895176471, 0.2949414888262283, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.20833333333333334, 0.0, 0.6666666666666666, 0.6200491264705917, 0.5983138296087428, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28673556], dtype=float32), 0.5338741]. 
=============================================
[2019-04-04 05:57:45,715] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.65073 ]
 [90.571526]
 [90.51652 ]
 [90.53133 ]
 [90.55332 ]], R is [[90.8011322 ]
 [90.89311981]
 [90.98419189]
 [91.07434845]
 [91.16360474]].
[2019-04-04 05:57:48,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2291681e-11 1.5008000e-11 2.3488028e-28 1.9778497e-12 7.1321352e-12
 1.4012768e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:57:48,781] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1928
[2019-04-04 05:57:48,797] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 92.0, 110.3333333333333, 0.0, 26.0, 26.10695419515282, 0.591398615179604, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1335000.0000, 
sim time next is 1335600.0000, 
raw observation next is [1.1, 92.0, 114.5, 0.0, 26.0, 26.11485694853901, 0.5918578759193486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.38166666666666665, 0.0, 0.6666666666666666, 0.6762380790449175, 0.6972859586397829, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9935443], dtype=float32), -0.98445857]. 
=============================================
[2019-04-04 05:58:02,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9659066e-10 8.3055418e-10 3.0231724e-25 4.6067965e-11 1.6399844e-10
 5.1807434e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:02,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3720
[2019-04-04 05:58:02,018] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.95, 82.33333333333334, 0.0, 0.0, 26.0, 25.63560341146033, 0.5172790518018534, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1576200.0000, 
sim time next is 1576800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.65225850832185, 0.5179638931962525, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6376882090268209, 0.6726546310654175, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2152507], dtype=float32), 0.21825571]. 
=============================================
[2019-04-04 05:58:02,431] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5096809e-10 1.1144228e-09 3.7763753e-25 2.9453200e-11 1.2292994e-10
 2.4094198e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:02,440] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0325
[2019-04-04 05:58:02,493] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 91.0, 0.0, 0.0, 26.0, 24.93535655499545, 0.4584292571313853, 0.0, 1.0, 199564.5892163587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [1.35, 90.5, 0.0, 0.0, 26.0, 24.98537416015688, 0.4961658579151891, 0.0, 1.0, 128543.7826799539], 
processed observation next is [1.0, 0.8695652173913043, 0.5000000000000001, 0.905, 0.0, 0.0, 0.6666666666666666, 0.5821145133464066, 0.665388619305063, 0.0, 1.0, 0.6121132508569234], 
reward next is 0.3879, 
noisyNet noise sample is [array([-0.7044196], dtype=float32), 0.11716839]. 
=============================================
[2019-04-04 05:58:03,134] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0413976e-11 5.2109428e-11 1.2556533e-25 8.9061701e-12 1.4932021e-11
 8.1789908e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:03,134] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6850
[2019-04-04 05:58:03,171] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 75.0, 0.0, 26.0, 25.03959587145579, 0.461845958642073, 1.0, 1.0, 19055.40427914089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1434000.0000, 
sim time next is 1434600.0000, 
raw observation next is [1.1, 92.0, 72.0, 0.0, 26.0, 25.53236005572786, 0.4925055930051008, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.24, 0.0, 0.6666666666666666, 0.627696671310655, 0.6641685310017003, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5080327], dtype=float32), 0.36344448]. 
=============================================
[2019-04-04 05:58:08,292] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2869017e-11 1.8683434e-10 2.0843819e-26 4.0138812e-12 4.2503469e-11
 7.0246792e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:08,293] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0811
[2019-04-04 05:58:08,320] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.116666666666667, 96.66666666666666, 78.0, 235.9999999999999, 26.0, 26.04645600683811, 0.5472631076609404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1507800.0000, 
sim time next is 1508400.0000, 
raw observation next is [3.3, 96.0, 80.5, 354.0, 26.0, 26.06658065194552, 0.5663758513927363, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.554016620498615, 0.96, 0.2683333333333333, 0.3911602209944751, 0.6666666666666666, 0.6722150543287935, 0.6887919504642454, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.18266174], dtype=float32), 0.2727061]. 
=============================================
[2019-04-04 05:58:16,131] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8693342e-11 2.2038874e-10 1.5774193e-25 2.4538576e-11 1.7337909e-11
 7.2579272e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:16,134] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4546
[2019-04-04 05:58:16,156] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 94.66666666666667, 0.0, 0.0, 26.0, 25.28462049136245, 0.4705715862695556, 0.0, 1.0, 45265.27136242059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1480800.0000, 
sim time next is 1481400.0000, 
raw observation next is [2.2, 95.0, 0.0, 0.0, 26.0, 25.4014726788148, 0.4704632215980583, 0.0, 1.0, 23311.00720089785], 
processed observation next is [1.0, 0.13043478260869565, 0.5235457063711911, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6167893899012334, 0.6568210738660194, 0.0, 1.0, 0.11100479619475166], 
reward next is 0.8890, 
noisyNet noise sample is [array([-0.62675303], dtype=float32), 0.17761569]. 
=============================================
[2019-04-04 05:58:16,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9239620e-11 2.5402840e-11 1.6026261e-26 4.3434275e-12 6.1507479e-12
 2.9196745e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:16,566] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0680
[2019-04-04 05:58:16,588] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.9, 51.0, 77.0, 478.0, 26.0, 26.64565625983978, 0.7562554683153219, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1524600.0000, 
sim time next is 1525200.0000, 
raw observation next is [12.0, 50.66666666666666, 78.66666666666667, 403.0000000000001, 26.0, 26.82864210286532, 0.7679281934506514, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7950138504155125, 0.5066666666666666, 0.26222222222222225, 0.44530386740331507, 0.6666666666666666, 0.7357201752387766, 0.7559760644835505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.0192092], dtype=float32), 0.7021484]. 
=============================================
[2019-04-04 05:58:33,248] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7760581e-08 3.6043598e-08 7.2357576e-21 4.7580886e-09 1.8585958e-08
 2.8685887e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 05:58:33,250] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9513
[2019-04-04 05:58:33,298] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.1, 85.0, 0.0, 0.0, 26.0, 24.10114040851075, 0.09202715828417778, 0.0, 1.0, 46559.27911294236], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1823400.0000, 
sim time next is 1824000.0000, 
raw observation next is [-6.133333333333333, 85.66666666666667, 0.0, 0.0, 26.0, 24.06332765667968, 0.0840318290133849, 0.0, 1.0, 46624.14415145315], 
processed observation next is [0.0, 0.08695652173913043, 0.2927054478301016, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5052773047233066, 0.5280106096711283, 0.0, 1.0, 0.2220197340545388], 
reward next is 0.7780, 
noisyNet noise sample is [array([1.1744324], dtype=float32), 0.4646636]. 
=============================================
[2019-04-04 05:58:33,315] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.8938  ]
 [70.96077 ]
 [71.06705 ]
 [71.068245]
 [71.22097 ]], R is [[70.8707428 ]
 [70.94032288]
 [71.0094986 ]
 [71.0782547 ]
 [71.14662933]].
[2019-04-04 05:58:38,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5373812e-09 5.4219402e-09 2.3409918e-22 1.4195949e-09 4.0352042e-09
 2.6609090e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:38,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1785
[2019-04-04 05:58:38,727] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 82.16666666666667, 0.0, 0.0, 26.0, 24.05790031469976, 0.0612982478511782, 0.0, 1.0, 43596.21460823753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2095800.0000, 
sim time next is 2096400.0000, 
raw observation next is [-6.700000000000001, 81.33333333333334, 0.0, 0.0, 26.0, 23.98472202579568, 0.04789040548172604, 0.0, 1.0, 43623.51100897638], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.4987268354829733, 0.5159634684939086, 0.0, 1.0, 0.2077310048046494], 
reward next is 0.7923, 
noisyNet noise sample is [array([1.2243339], dtype=float32), 0.35804474]. 
=============================================
[2019-04-04 05:58:48,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4797044e-09 2.6550273e-09 8.4736611e-23 3.6086764e-10 1.0831332e-09
 5.0077100e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:58:48,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1087
[2019-04-04 05:58:48,576] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 26.0, 24.40772634736112, 0.165703474443369, 0.0, 1.0, 42524.34712749637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [-7.100000000000001, 78.66666666666667, 0.0, 0.0, 26.0, 24.33692482185795, 0.1591664737399606, 0.0, 1.0, 42566.79063875372], 
processed observation next is [1.0, 0.043478260869565216, 0.26592797783933514, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5280770684881624, 0.5530554912466535, 0.0, 1.0, 0.2026990030416844], 
reward next is 0.7973, 
noisyNet noise sample is [array([0.10425027], dtype=float32), 0.46575105]. 
=============================================
[2019-04-04 05:59:03,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0431660e-09 1.5797175e-09 9.0410709e-24 3.5891612e-10 1.3828590e-09
 1.5124058e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:03,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9755
[2019-04-04 05:59:03,358] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 86.0, 0.0, 0.0, 26.0, 24.62384574435463, 0.2016989458205753, 0.0, 1.0, 42796.58379403744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2082600.0000, 
sim time next is 2083200.0000, 
raw observation next is [-4.833333333333333, 86.0, 0.0, 0.0, 26.0, 24.59123995785789, 0.1883832160428728, 0.0, 1.0, 42891.04508016923], 
processed observation next is [1.0, 0.08695652173913043, 0.3287165281625116, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5492699964881576, 0.5627944053476243, 0.0, 1.0, 0.20424307181032966], 
reward next is 0.7958, 
noisyNet noise sample is [array([0.52315164], dtype=float32), 2.2077947]. 
=============================================
[2019-04-04 05:59:04,245] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1020468e-09 7.1703243e-09 1.1065574e-22 1.1321581e-09 2.9643710e-10
 1.0452363e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:04,245] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0027
[2019-04-04 05:59:04,279] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.716666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.24505347742162, 0.103055326493283, 0.0, 1.0, 43951.38940364017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2256600.0000, 
sim time next is 2257200.0000, 
raw observation next is [-7.8, 86.0, 0.0, 0.0, 26.0, 24.12912042207584, 0.09778009698763086, 0.0, 1.0, 43749.01016476208], 
processed observation next is [1.0, 0.13043478260869565, 0.24653739612188366, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5107600351729866, 0.5325933656625436, 0.0, 1.0, 0.20832861983220038], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.74884814], dtype=float32), -1.042782]. 
=============================================
[2019-04-04 05:59:07,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.6124702e-11 5.1697400e-11 1.0574266e-25 7.7401913e-12 2.2812866e-11
 3.7274077e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:07,242] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7165
[2019-04-04 05:59:07,333] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.983333333333333, 79.5, 126.6666666666667, 42.66666666666667, 26.0, 25.6486401670879, 0.3179417178435578, 1.0, 1.0, 18732.41801364534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2281800.0000, 
sim time next is 2282400.0000, 
raw observation next is [-6.7, 78.0, 139.5, 44.5, 26.0, 25.62989757317027, 0.325842559357997, 1.0, 1.0, 18730.00607466905], 
processed observation next is [1.0, 0.43478260869565216, 0.2770083102493075, 0.78, 0.465, 0.049171270718232046, 0.6666666666666666, 0.635824797764189, 0.6086141864526656, 1.0, 1.0, 0.08919050511747167], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.25116932], dtype=float32), -1.1058941]. 
=============================================
[2019-04-04 05:59:17,278] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5059596e-09 4.2459192e-09 3.9319989e-23 1.5869290e-09 2.3385796e-09
 1.9030810e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:17,278] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1986
[2019-04-04 05:59:17,320] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.8, 50.33333333333334, 0.0, 0.0, 26.0, 24.89582860530006, 0.1571427908531271, 0.0, 1.0, 38390.8314039379], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2520600.0000, 
sim time next is 2521200.0000, 
raw observation next is [-1.9, 51.66666666666667, 0.0, 0.0, 26.0, 24.84455193196185, 0.1527488107157383, 0.0, 1.0, 38414.92157537369], 
processed observation next is [1.0, 0.17391304347826086, 0.4099722991689751, 0.5166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5703793276634874, 0.5509162702385794, 0.0, 1.0, 0.18292819797796997], 
reward next is 0.8171, 
noisyNet noise sample is [array([-1.008321], dtype=float32), -2.2863111]. 
=============================================
[2019-04-04 05:59:17,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4277830e-09 9.1965937e-09 2.2223075e-22 7.1054618e-10 1.5897313e-09
 3.2204311e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:17,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5642
[2019-04-04 05:59:17,662] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69355605268231, -0.008804859052648268, 0.0, 1.0, 43320.51582285787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265600.0000, 
sim time next is 2266200.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.68407621132116, -0.01663613792270174, 0.0, 1.0, 43271.28142145483], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.47367301761009656, 0.49445462069243273, 0.0, 1.0, 0.2060537210545468], 
reward next is 0.7939, 
noisyNet noise sample is [array([0.99044156], dtype=float32), -0.69789696]. 
=============================================
[2019-04-04 05:59:30,242] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8393524e-09 2.4838038e-09 9.0885925e-22 1.8931967e-09 7.8643092e-10
 2.0436102e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:30,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4860
[2019-04-04 05:59:30,289] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.0, 82.5, 199.5, 26.0, 25.00893832467692, 0.2986082422657942, 0.0, 1.0, 19376.18839819319], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2390400.0000, 
sim time next is 2391000.0000, 
raw observation next is [-0.09999999999999999, 46.66666666666667, 81.33333333333333, 152.3333333333333, 26.0, 24.98955924278372, 0.2918677835191434, 0.0, 1.0, 36778.08348889476], 
processed observation next is [0.0, 0.6956521739130435, 0.4598337950138504, 0.46666666666666673, 0.2711111111111111, 0.16832412523020251, 0.6666666666666666, 0.5824632702319766, 0.5972892611730478, 0.0, 1.0, 0.17513373089949885], 
reward next is 0.8249, 
noisyNet noise sample is [array([-0.13859138], dtype=float32), -0.7739513]. 
=============================================
[2019-04-04 05:59:30,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.17945 ]
 [74.415886]
 [74.58279 ]
 [74.620285]
 [74.380775]], R is [[74.01841736]
 [74.18596649]
 [74.3549881 ]
 [74.52231598]
 [74.68795776]].
[2019-04-04 05:59:37,948] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0220856e-10 4.0187214e-09 1.5636058e-23 4.5628737e-10 3.1521832e-09
 1.7316195e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:37,951] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9877
[2019-04-04 05:59:37,971] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.81424931799754, 0.1449979067014878, 0.0, 1.0, 38374.38448292117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2525400.0000, 
sim time next is 2526000.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.81979943397394, 0.1435869601864395, 0.0, 1.0, 38408.46280566837], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5683166194978284, 0.5478623200621465, 0.0, 1.0, 0.18289744193175414], 
reward next is 0.8171, 
noisyNet noise sample is [array([1.2662702], dtype=float32), -0.9466187]. 
=============================================
[2019-04-04 05:59:38,038] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.244934]
 [80.17556 ]
 [80.10182 ]
 [80.030754]
 [79.950676]], R is [[80.31600189]
 [80.33010864]
 [80.3441391 ]
 [80.35801697]
 [80.37171936]].
[2019-04-04 05:59:40,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4513206e-09 4.0249346e-09 5.7421898e-23 6.5820149e-10 4.8598781e-10
 1.9053339e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:40,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6503
[2019-04-04 05:59:40,764] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.31666666666667, 77.16666666666667, 0.0, 0.0, 26.0, 24.13780042210816, 0.1402121825712496, 0.0, 1.0, 44457.12905850862], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2686200.0000, 
sim time next is 2686800.0000, 
raw observation next is [-11.63333333333333, 78.33333333333334, 0.0, 0.0, 26.0, 24.13569538344599, 0.1316754899040631, 0.0, 1.0, 44429.11687741301], 
processed observation next is [1.0, 0.08695652173913043, 0.14035087719298256, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5113079486204993, 0.543891829968021, 0.0, 1.0, 0.21156722322577626], 
reward next is 0.7884, 
noisyNet noise sample is [array([-1.0344068], dtype=float32), -0.99122804]. 
=============================================
[2019-04-04 05:59:41,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3519880e-11 8.8631484e-11 5.8520887e-25 1.3097254e-11 9.5145888e-12
 4.9391242e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:41,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5919
[2019-04-04 05:59:41,259] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 50.66666666666667, 20.0, 186.6666666666666, 26.0, 25.72521747142292, 0.4622105285282825, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2740200.0000, 
sim time next is 2740800.0000, 
raw observation next is [-3.333333333333333, 51.33333333333334, 11.5, 119.8333333333333, 26.0, 25.86881986861039, 0.2906147474600818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.37026777469990774, 0.5133333333333334, 0.03833333333333333, 0.1324125230202578, 0.6666666666666666, 0.6557349890508659, 0.5968715824866939, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.77415836], dtype=float32), 1.383161]. 
=============================================
[2019-04-04 05:59:45,343] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.43741463e-08 1.28940965e-08 2.22130735e-22 1.13978704e-09
 8.18000956e-09 4.71183335e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 05:59:45,344] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4707
[2019-04-04 05:59:45,372] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 72.5, 0.0, 0.0, 26.0, 24.37633053274087, 0.1749680135208345, 0.0, 1.0, 44440.28774423861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2683800.0000, 
sim time next is 2684400.0000, 
raw observation next is [-10.33333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 24.28302280272342, 0.15935302092632, 0.0, 1.0, 44468.22147829094], 
processed observation next is [1.0, 0.043478260869565216, 0.17636195752539252, 0.7366666666666667, 0.0, 0.0, 0.6666666666666666, 0.523585233560285, 0.5531176736421067, 0.0, 1.0, 0.21175343561090926], 
reward next is 0.7882, 
noisyNet noise sample is [array([1.9831396], dtype=float32), -0.8084965]. 
=============================================
[2019-04-04 05:59:50,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2314521e-09 3.6930420e-10 4.8748950e-23 3.5516518e-10 1.2714728e-10
 8.4253877e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 05:59:50,383] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6570
[2019-04-04 05:59:50,450] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 206.0, 555.3333333333334, 26.0, 25.03694188342874, 0.383526950967225, 0.0, 1.0, 18911.59584853701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2981400.0000, 
sim time next is 2982000.0000, 
raw observation next is [-3.0, 65.0, 193.5, 623.1666666666667, 26.0, 25.0476130719808, 0.386681984929447, 0.0, 1.0, 20944.1395616923], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.645, 0.6885819521178638, 0.6666666666666666, 0.5873010893317332, 0.6288939949764824, 0.0, 1.0, 0.09973399791282048], 
reward next is 0.9003, 
noisyNet noise sample is [array([0.52682763], dtype=float32), -1.0104952]. 
=============================================
[2019-04-04 05:59:50,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.84581 ]
 [80.081955]
 [80.193306]
 [80.069496]
 [79.69643 ]], R is [[79.68650055]
 [79.79957581]
 [79.90514374]
 [79.9569397 ]
 [79.94528961]].
[2019-04-04 06:00:01,286] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7377397e-12 3.6554752e-12 8.0896444e-28 2.8214154e-12 1.4699897e-12
 6.2213749e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:00:01,286] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8655
[2019-04-04 06:00:01,344] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 96.5, 71.0, 54.0, 26.0, 25.15613702378743, 0.4209336089274429, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2910600.0000, 
sim time next is 2911200.0000, 
raw observation next is [2.0, 95.33333333333334, 60.16666666666667, 51.83333333333333, 26.0, 25.58478147182302, 0.4462653556776039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.9533333333333335, 0.20055555555555557, 0.057274401473296495, 0.6666666666666666, 0.6320651226519184, 0.6487551185592013, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.79091394], dtype=float32), 0.115737416]. 
=============================================
[2019-04-04 06:00:03,808] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.44830071e-13 7.88531024e-13 2.05592533e-30 1.32972695e-14
 1.16676214e-14 1.81179461e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 06:00:03,815] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4104
[2019-04-04 06:00:03,831] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 110.1666666666667, 798.8333333333334, 26.0, 27.46548719954303, 0.8999518871777435, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3159600.0000, 
sim time next is 3160200.0000, 
raw observation next is [7.0, 100.0, 108.3333333333333, 791.6666666666666, 26.0, 27.53715764515556, 0.9156425129716977, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6565096952908588, 1.0, 0.361111111111111, 0.8747697974217311, 0.6666666666666666, 0.7947631370962966, 0.8052141709905659, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8517607], dtype=float32), -0.030550845]. 
=============================================
[2019-04-04 06:00:16,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3500801e-09 2.8815966e-10 8.7567870e-25 1.2592798e-10 2.1207720e-10
 5.7880063e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:00:16,745] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3594
[2019-04-04 06:00:16,775] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 100.0, 0.0, 0.0, 26.0, 25.30312104079673, 0.3219739130187723, 0.0, 1.0, 39579.75028266211], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3111600.0000, 
sim time next is 3112200.0000, 
raw observation next is [0.5, 100.0, 0.0, 0.0, 26.0, 25.30587643803458, 0.3208571295493419, 0.0, 1.0, 39470.09687545754], 
processed observation next is [1.0, 0.0, 0.4764542936288089, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6088230365028817, 0.6069523765164473, 0.0, 1.0, 0.18795284226408351], 
reward next is 0.8120, 
noisyNet noise sample is [array([1.7097768], dtype=float32), -0.39703202]. 
=============================================
[2019-04-04 06:00:17,471] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1325199e-09 3.7449386e-09 1.9785226e-22 2.5787145e-10 9.4771790e-10
 2.0728991e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:00:17,472] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0161
[2019-04-04 06:00:17,543] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 217.0, 154.0, 26.0, 24.97850470787592, 0.3390372724516872, 0.0, 1.0, 18743.28119188454], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2977200.0000, 
sim time next is 2977800.0000, 
raw observation next is [-3.0, 65.0, 230.0, 197.3333333333333, 26.0, 24.9963368883564, 0.3424755225193072, 0.0, 1.0, 18739.89428127718], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.7666666666666667, 0.21804788213627987, 0.6666666666666666, 0.5830280740296999, 0.6141585075064357, 0.0, 1.0, 0.08923759181560562], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.08591612], dtype=float32), -0.23637864]. 
=============================================
[2019-04-04 06:00:24,983] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 06:00:25,001] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:00:25,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:00:25,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run10
[2019-04-04 06:00:25,046] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:00:25,046] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:00:25,048] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run10
[2019-04-04 06:00:25,077] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:00:25,097] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:00:25,099] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run10
[2019-04-04 06:01:54,021] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.270269], dtype=float32), 0.07225653]
[2019-04-04 06:01:54,021] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.0, 83.0, 0.0, 0.0, 26.0, 24.18292979248429, 0.1113310569274339, 0.0, 1.0, 46376.36725905495]
[2019-04-04 06:01:54,021] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:01:54,022] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.8028552e-09 5.9487402e-09 2.0852928e-22 1.1518859e-09 3.5897039e-09
 1.2513706e-11 1.0000000e+00], sampled 0.046035578963707136
[2019-04-04 06:03:20,318] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.270269], dtype=float32), 0.07225653]
[2019-04-04 06:03:20,318] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.917682012666666, 55.13820728, 114.72902317, 643.9835228333334, 26.0, 26.35762016727479, 0.6232151289733738, 1.0, 1.0, 0.0]
[2019-04-04 06:03:20,318] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:03:20,319] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.20516183e-11 4.60721808e-11 1.68014726e-25 1.27556264e-11
 1.38353305e-11 5.58441029e-14 1.00000000e+00], sampled 0.3151245897879399
[2019-04-04 06:03:28,032] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 06:03:47,681] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.270269], dtype=float32), 0.07225653]
[2019-04-04 06:03:47,682] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.7278736205, 88.67341252, 0.0, 0.0, 26.0, 25.33918818191456, 0.4687897072317355, 0.0, 1.0, 55150.70369401991]
[2019-04-04 06:03:47,682] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:03:47,683] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.9152475e-10 1.0666826e-09 3.4612235e-24 1.9988276e-10 3.9864415e-10
 1.0555274e-12 1.0000000e+00], sampled 0.04140785459418017
[2019-04-04 06:04:04,483] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 06:04:09,645] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:04:10,710] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 900000, evaluation results [900000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:04:34,620] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7300427e-09 1.3221591e-08 2.0596321e-21 2.4584934e-09 2.6101383e-09
 5.8929576e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:04:34,635] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8825
[2019-04-04 06:04:34,668] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666666, 75.0, 0.0, 0.0, 26.0, 24.81792197866354, 0.2609575492160428, 0.0, 1.0, 41678.80680455471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3385200.0000, 
sim time next is 3385800.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.83594583023137, 0.2562209181586658, 0.0, 1.0, 41826.74402995993], 
processed observation next is [1.0, 0.17391304347826086, 0.3102493074792244, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5696621525192809, 0.5854069727195553, 0.0, 1.0, 0.19917497157123779], 
reward next is 0.8008, 
noisyNet noise sample is [array([-0.49270737], dtype=float32), 0.6059342]. 
=============================================
[2019-04-04 06:04:39,426] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1005286e-08 5.3916128e-08 5.2170343e-20 5.6944578e-09 3.1679217e-08
 2.9376968e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 06:04:39,427] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5947
[2019-04-04 06:04:39,549] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 76.0, 0.0, 0.0, 26.0, 23.85902360878533, 0.08821534145762344, 0.0, 1.0, 44023.85331476562], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3308400.0000, 
sim time next is 3309000.0000, 
raw observation next is [-11.0, 77.33333333333334, 0.0, 0.0, 26.0, 23.86391429302424, 0.07456711444557412, 0.0, 1.0, 44038.68699561705], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.4886595244186867, 0.5248557048151914, 0.0, 1.0, 0.20970803331246213], 
reward next is 0.7903, 
noisyNet noise sample is [array([0.1691212], dtype=float32), -0.6669643]. 
=============================================
[2019-04-04 06:04:39,680] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[67.40058 ]
 [67.602554]
 [67.82994 ]
 [68.05859 ]
 [68.293175]], R is [[67.33094025]
 [67.44799042]
 [67.56394958]
 [67.67892456]
 [67.79306793]].
[2019-04-04 06:04:40,828] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.7247021e-09 4.4503641e-09 3.7805010e-23 9.6766239e-10 2.3552451e-09
 1.2123727e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:04:40,828] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2047
[2019-04-04 06:04:40,894] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 62.5, 2.0, 107.0, 26.0, 24.92115233006141, 0.3300567186641471, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3396600.0000, 
sim time next is 3397200.0000, 
raw observation next is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 26.0, 25.41409869212297, 0.3732327566930405, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.6166666666666667, 0.05388888888888887, 0.17624309392265194, 0.6666666666666666, 0.6178415576769142, 0.6244109188976802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.45759302], dtype=float32), 0.559227]. 
=============================================
[2019-04-04 06:04:49,095] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8146525e-09 5.8589267e-09 1.1629271e-22 1.1171789e-09 7.1840783e-10
 3.4124090e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:04:49,095] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4625
[2019-04-04 06:04:49,170] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.83333333333333, 0.0, 0.0, 26.0, 24.72174474093424, 0.2177088673958831, 0.0, 1.0, 42843.44827267014], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3391800.0000, 
sim time next is 3392400.0000, 
raw observation next is [-3.0, 61.66666666666667, 0.0, 0.0, 26.0, 24.68208156871632, 0.2165805974389627, 0.0, 1.0, 42859.40090874704], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5568401307263601, 0.5721935324796542, 0.0, 1.0, 0.20409238527974782], 
reward next is 0.7959, 
noisyNet noise sample is [array([-0.48220038], dtype=float32), 1.5197114]. 
=============================================
[2019-04-04 06:04:53,450] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.5604973e-10 8.3107871e-10 1.7500401e-23 1.9665142e-10 4.0230677e-10
 2.4558042e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:04:53,452] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1134
[2019-04-04 06:04:53,525] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 0.0, 0.0, 26.0, 25.03939174915879, 0.2811602099170605, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3741600.0000, 
sim time next is 3742200.0000, 
raw observation next is [-4.0, 74.0, 5.0, 136.0, 26.0, 25.2190214043551, 0.2887672596452079, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.74, 0.016666666666666666, 0.15027624309392265, 0.6666666666666666, 0.6015851170295917, 0.5962557532150693, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4720014], dtype=float32), 1.2181396]. 
=============================================
[2019-04-04 06:04:57,127] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2582920e-09 2.9486056e-09 6.2347047e-23 7.1640732e-10 3.0114451e-09
 7.9195444e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:04:57,127] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4678
[2019-04-04 06:04:57,158] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.25412190983038, 0.3444556607470212, 0.0, 1.0, 41436.28880755237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3726600.0000, 
sim time next is 3727200.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.35418578795074, 0.3425122550951423, 0.0, 1.0, 41225.90976129709], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6128488156625617, 0.6141707516983808, 0.0, 1.0, 0.1963138560061766], 
reward next is 0.8037, 
noisyNet noise sample is [array([2.8451993], dtype=float32), -0.28829575]. 
=============================================
[2019-04-04 06:05:02,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2255377e-11 1.4336483e-10 1.4356534e-26 3.3006788e-11 2.5492405e-10
 6.3723165e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:02,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3665
[2019-04-04 06:05:02,611] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.166666666666667, 44.5, 116.6666666666667, 824.6666666666667, 26.0, 25.44377919642034, 0.457000917492458, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3672600.0000, 
sim time next is 3673200.0000, 
raw observation next is [4.333333333333334, 44.0, 116.8333333333333, 826.8333333333334, 26.0, 25.38756660775856, 0.4575829391170367, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.58264081255771, 0.44, 0.3894444444444443, 0.9136279926335176, 0.6666666666666666, 0.6156305506465468, 0.6525276463723456, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7199192], dtype=float32), -1.367752]. 
=============================================
[2019-04-04 06:05:10,502] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1813354e-11 6.0373095e-11 1.7791714e-25 3.9325633e-11 2.3095649e-11
 1.4992678e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:10,503] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0584
[2019-04-04 06:05:10,537] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 46.83333333333333, 0.0, 0.0, 26.0, 26.25022805035329, 0.6228737241720051, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3865800.0000, 
sim time next is 3866400.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 26.17085349897577, 0.5038277593416433, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6809044582479808, 0.6679425864472144, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8454119], dtype=float32), 2.1869285]. 
=============================================
[2019-04-04 06:05:12,624] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1425842e-09 4.2880770e-09 1.6977395e-21 1.1583766e-09 4.7820081e-09
 2.5529712e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:12,624] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4752
[2019-04-04 06:05:12,649] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.166666666666666, 53.83333333333334, 0.0, 0.0, 26.0, 25.09607645810601, 0.3739386775285752, 0.0, 1.0, 44233.20318780119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3971400.0000, 
sim time next is 3972000.0000, 
raw observation next is [-9.333333333333334, 54.66666666666667, 0.0, 0.0, 26.0, 25.04759307674608, 0.3623951249292167, 0.0, 1.0, 44096.89882123662], 
processed observation next is [1.0, 1.0, 0.20406278855032317, 0.5466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5872994230621732, 0.6207983749764056, 0.0, 1.0, 0.20998523248207912], 
reward next is 0.7900, 
noisyNet noise sample is [array([-0.07412713], dtype=float32), -0.3145898]. 
=============================================
[2019-04-04 06:05:12,663] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[71.454254]
 [71.64703 ]
 [71.71525 ]
 [71.79101 ]
 [71.80637 ]], R is [[71.35121155]
 [71.42707062]
 [71.49933624]
 [71.56137085]
 [71.5967865 ]].
[2019-04-04 06:05:16,468] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3857528e-09 5.5628968e-09 1.7662108e-22 1.3030563e-09 1.1636093e-09
 1.9006942e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:16,469] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6914
[2019-04-04 06:05:16,496] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.88489138401096, 0.2680382634821208, 0.0, 1.0, 42333.27091130289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3912000.0000, 
sim time next is 3912600.0000, 
raw observation next is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.80060352091012, 0.2530881031385069, 0.0, 1.0, 42487.29870437142], 
processed observation next is [1.0, 0.2608695652173913, 0.27331486611265005, 0.6316666666666666, 0.0, 0.0, 0.6666666666666666, 0.5667169600758433, 0.5843627010461689, 0.0, 1.0, 0.20232047002081627], 
reward next is 0.7977, 
noisyNet noise sample is [array([0.9850878], dtype=float32), 0.29846194]. 
=============================================
[2019-04-04 06:05:19,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.27661287e-10 3.00000053e-10 1.89173738e-25 1.13421085e-10
 6.76919562e-11 8.17033683e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:05:19,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7842
[2019-04-04 06:05:19,883] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 37.0, 102.0, 644.0, 26.0, 26.10470191000693, 0.4925011417782312, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4094400.0000, 
sim time next is 4095000.0000, 
raw observation next is [-2.5, 36.5, 104.0, 679.0, 26.0, 26.20421056989942, 0.5158958057679439, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39335180055401664, 0.365, 0.3466666666666667, 0.7502762430939226, 0.6666666666666666, 0.683684214158285, 0.6719652685893146, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6525202], dtype=float32), -0.394855]. 
=============================================
[2019-04-04 06:05:19,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[88.60568 ]
 [88.55758 ]
 [88.586655]
 [88.445145]
 [88.17835 ]], R is [[88.76655579]
 [88.87889099]
 [88.99010468]
 [89.10020447]
 [89.20920563]].
[2019-04-04 06:05:22,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6241048e-11 1.0172936e-10 5.4104395e-25 2.0647794e-11 2.9904298e-11
 5.5876311e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:22,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2687
[2019-04-04 06:05:22,978] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.166666666666666, 40.66666666666667, 114.3333333333333, 792.6666666666667, 26.0, 26.56654887755948, 0.5868880215437026, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4013400.0000, 
sim time next is 4014000.0000, 
raw observation next is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.56591163649881, 0.5814053621988381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.24099722991689754, 0.4, 0.385, 0.8823204419889503, 0.6666666666666666, 0.7138259697082342, 0.6938017873996127, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23366807], dtype=float32), 0.07794356]. 
=============================================
[2019-04-04 06:05:23,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.78547 ]
 [83.72118 ]
 [83.567444]
 [83.48348 ]
 [83.411095]], R is [[84.00049591]
 [84.16049194]
 [84.3188858 ]
 [84.47570038]
 [84.6309433 ]].
[2019-04-04 06:05:23,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6974839e-10 1.9496166e-08 4.1212835e-22 1.0794313e-09 2.1808646e-09
 5.1845295e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:23,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4754
[2019-04-04 06:05:23,326] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 30.0, 0.0, 0.0, 26.0, 25.30282787078413, 0.4869596878968483, 0.0, 1.0, 169364.2736412829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048200.0000, 
sim time next is 4048800.0000, 
raw observation next is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.34791845533588, 0.5092822256503513, 0.0, 1.0, 88507.20079971795], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.29666666666666663, 0.0, 0.0, 0.6666666666666666, 0.6123265379446566, 0.6697607418834505, 0.0, 1.0, 0.42146286095103785], 
reward next is 0.5785, 
noisyNet noise sample is [array([-0.5720822], dtype=float32), 1.4705737]. 
=============================================
[2019-04-04 06:05:24,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.2541425e-09 9.2389474e-09 3.7637126e-21 1.8186009e-09 2.2876070e-09
 7.0894263e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:24,752] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4924
[2019-04-04 06:05:24,767] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.833333333333334, 57.16666666666667, 0.0, 0.0, 26.0, 24.89068182176062, 0.3253091323861728, 0.0, 1.0, 44079.92618742499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3973800.0000, 
sim time next is 3974400.0000, 
raw observation next is [-10.0, 58.0, 0.0, 0.0, 26.0, 24.84146399843219, 0.3149658550872248, 0.0, 1.0, 44087.72703398751], 
processed observation next is [1.0, 0.0, 0.18559556786703602, 0.58, 0.0, 0.0, 0.6666666666666666, 0.5701219998693491, 0.6049886183624082, 0.0, 1.0, 0.20994155730470243], 
reward next is 0.7901, 
noisyNet noise sample is [array([-0.15583389], dtype=float32), 1.8095096]. 
=============================================
[2019-04-04 06:05:27,086] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.06519399e-10 1.39343315e-10 6.69920087e-24 1.02005265e-10
 2.20780866e-10 6.33010286e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:05:27,086] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2707
[2019-04-04 06:05:27,096] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 39.33333333333333, 144.6666666666667, 540.0, 26.0, 25.35755113229998, 0.439389323687092, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4203600.0000, 
sim time next is 4204200.0000, 
raw observation next is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 26.0, 25.36100364117926, 0.4369635989185024, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.541089566020314, 0.3816666666666667, 0.43111111111111095, 0.5988950276243094, 0.6666666666666666, 0.6134169700982716, 0.6456545329728341, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.783069], dtype=float32), 0.38402936]. 
=============================================
[2019-04-04 06:05:30,335] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5994282e-08 6.8433805e-09 1.7949829e-22 4.6567861e-09 7.7812947e-09
 1.1813283e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:30,340] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2122
[2019-04-04 06:05:30,367] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 49.0, 0.0, 0.0, 26.0, 24.5491681082431, 0.2015422879455181, 0.0, 1.0, 40197.65148682257], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4172400.0000, 
sim time next is 4173000.0000, 
raw observation next is [-5.0, 49.83333333333334, 0.0, 0.0, 26.0, 24.5081599524001, 0.2147816153822746, 0.0, 1.0, 40599.80212903266], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.4983333333333334, 0.0, 0.0, 0.6666666666666666, 0.5423466627000083, 0.5715938717940915, 0.0, 1.0, 0.19333239109063172], 
reward next is 0.8067, 
noisyNet noise sample is [array([0.5993888], dtype=float32), 1.2015285]. 
=============================================
[2019-04-04 06:05:30,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.95471]
 [77.16474]
 [77.37085]
 [77.55238]
 [77.72659]], R is [[76.79959106]
 [76.84017944]
 [76.88090515]
 [76.92175293]
 [76.96268463]].
[2019-04-04 06:05:32,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5821255e-12 3.8184746e-12 1.4284022e-29 5.3109985e-13 3.2294469e-13
 1.8336701e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:32,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7476
[2019-04-04 06:05:32,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.4, 44.0, 0.0, 0.0, 26.0, 27.72252575428364, 1.01161323777679, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [12.33333333333333, 45.0, 0.0, 0.0, 26.0, 27.78138014171072, 1.036306388478157, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8042474607571561, 0.45, 0.0, 0.0, 0.6666666666666666, 0.8151150118092266, 0.8454354628260523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0347316], dtype=float32), -0.48202685]. 
=============================================
[2019-04-04 06:05:32,559] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1647083e-11 6.5745458e-11 3.5191444e-28 9.2554775e-12 3.9168058e-11
 4.9948649e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:32,561] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0382
[2019-04-04 06:05:32,573] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.6, 58.5, 0.0, 0.0, 26.0, 27.02016634304054, 0.876022059706817, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4393800.0000, 
sim time next is 4394400.0000, 
raw observation next is [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.96362796085408, 0.8685628850196615, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7525392428439522, 0.5866666666666666, 0.0, 0.0, 0.6666666666666666, 0.74696899673784, 0.7895209616732205, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.17011929], dtype=float32), 0.1075789]. 
=============================================
[2019-04-04 06:05:33,654] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4736900e-08 4.9125175e-09 8.4041549e-24 8.7144658e-10 2.1037194e-09
 2.0648266e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:33,655] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2743
[2019-04-04 06:05:33,670] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40118299639241, 0.3453786345200212, 0.0, 1.0, 46606.89555498009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4241400.0000, 
sim time next is 4242000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40681508406181, 0.3453960955716912, 0.0, 1.0, 40107.09326043897], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6172345903384843, 0.6151320318572304, 0.0, 1.0, 0.1909861583830427], 
reward next is 0.8090, 
noisyNet noise sample is [array([-0.13588488], dtype=float32), -0.117170356]. 
=============================================
[2019-04-04 06:05:33,683] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[85.135414]
 [84.93285 ]
 [84.92624 ]
 [84.81951 ]
 [84.8127  ]], R is [[85.16989136]
 [85.09625244]
 [85.09745026]
 [85.10890198]
 [85.11148071]].
[2019-04-04 06:05:34,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6980165e-11 2.3435247e-11 6.6634251e-29 2.6772777e-11 4.6019464e-12
 3.2929755e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:34,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7099
[2019-04-04 06:05:34,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.416666666666667, 83.33333333333333, 135.0, 165.0, 26.0, 26.03915394114394, 0.5998196624173474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4438200.0000, 
sim time next is 4438800.0000, 
raw observation next is [1.3, 84.0, 142.5, 131.5, 26.0, 26.17496112394989, 0.5985749268683356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49861495844875353, 0.84, 0.475, 0.1453038674033149, 0.6666666666666666, 0.6812467603291573, 0.6995249756227785, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2298877], dtype=float32), 0.367619]. 
=============================================
[2019-04-04 06:05:34,529] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9947034e-12 3.0827606e-12 2.3810267e-28 4.8667998e-13 1.3762498e-12
 2.6199150e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:34,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2020
[2019-04-04 06:05:34,540] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 85.33333333333333, 179.3333333333333, 50.16666666666666, 26.0, 26.3100074008005, 0.6246015988340452, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4441200.0000, 
sim time next is 4441800.0000, 
raw observation next is [1.05, 85.66666666666667, 193.6666666666667, 69.33333333333333, 26.0, 26.36185477865678, 0.6261638385267259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49168975069252085, 0.8566666666666667, 0.6455555555555557, 0.07661141804788213, 0.6666666666666666, 0.6968212315547317, 0.7087212795089086, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24761097], dtype=float32), -0.28696862]. 
=============================================
[2019-04-04 06:05:40,109] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3239455e-11 7.4750303e-11 1.0742696e-28 6.4321244e-12 5.6548039e-12
 7.1363190e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:40,112] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9935
[2019-04-04 06:05:40,148] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.199999999999999, 62.66666666666667, 94.5, 522.0, 26.0, 26.08987397541419, 0.5366654807330432, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4351200.0000, 
sim time next is 4351800.0000, 
raw observation next is [5.75, 59.83333333333333, 97.0, 553.0, 26.0, 26.27725665242484, 0.5619693987101277, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6218836565096953, 0.5983333333333333, 0.3233333333333333, 0.611049723756906, 0.6666666666666666, 0.6897713877020699, 0.6873231329033759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2810674], dtype=float32), 1.5127892]. 
=============================================
[2019-04-04 06:05:41,646] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9592531e-10 4.0330113e-09 3.0740050e-24 1.1310460e-10 4.6478568e-10
 2.4267396e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:41,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-04 06:05:41,669] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.44565018086555, 0.4462786935532688, 0.0, 1.0, 123417.1092031513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4573800.0000, 
sim time next is 4574400.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.30612154697157, 0.4491169069777614, 0.0, 1.0, 118808.5333836131], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.608843462247631, 0.6497056356592538, 0.0, 1.0, 0.5657549208743481], 
reward next is 0.4342, 
noisyNet noise sample is [array([1.796259], dtype=float32), -0.106633745]. 
=============================================
[2019-04-04 06:05:42,442] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0205781e-09 5.6964111e-10 9.2744483e-25 6.0652067e-11 2.6536184e-10
 8.1252381e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:42,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3692
[2019-04-04 06:05:42,469] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.85, 69.83333333333334, 0.0, 0.0, 26.0, 25.57422453219453, 0.3990515380819877, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4335000.0000, 
sim time next is 4335600.0000, 
raw observation next is [3.8, 69.66666666666667, 0.0, 0.0, 26.0, 25.63364673631331, 0.3875449804300069, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.5678670360110805, 0.6966666666666668, 0.0, 0.0, 0.6666666666666666, 0.6361372280261092, 0.6291816601433357, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.61088264], dtype=float32), -1.083889]. 
=============================================
[2019-04-04 06:05:48,529] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2591547e-13 3.5684201e-12 4.1212354e-30 4.8248937e-13 2.3245842e-13
 5.2484046e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:48,530] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9183
[2019-04-04 06:05:48,554] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 26.0, 28.2056858643945, 0.9152935272437358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4365600.0000, 
sim time next is 4366200.0000, 
raw observation next is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 27.65435093386403, 0.9486084692514133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8688827331486613, 0.2883333333333334, 0.39, 0.9384898710865562, 0.6666666666666666, 0.8045292444886692, 0.8162028230838044, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41738173], dtype=float32), 0.35458216]. 
=============================================
[2019-04-04 06:05:51,988] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.2597424e-10 7.2458944e-10 2.6007176e-23 2.6588975e-10 4.9022286e-10
 1.1149718e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:51,993] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2475
[2019-04-04 06:05:52,006] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.75, 70.0, 0.0, 0.0, 26.0, 25.27002240209794, 0.4009456609731659, 0.0, 1.0, 40973.94079491738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4595400.0000, 
sim time next is 4596000.0000, 
raw observation next is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.29662380247669, 0.3988181624302387, 0.0, 1.0, 37662.69067943077], 
processed observation next is [1.0, 0.17391304347826086, 0.41181902123730385, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.6080519835397243, 0.6329393874767463, 0.0, 1.0, 0.1793461460925275], 
reward next is 0.8207, 
noisyNet noise sample is [array([-0.5461386], dtype=float32), -0.55623776]. 
=============================================
[2019-04-04 06:05:52,009] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.52212 ]
 [77.56753 ]
 [77.57836 ]
 [77.50172 ]
 [77.504585]], R is [[77.49429321]
 [77.52423859]
 [77.51049805]
 [77.41674805]
 [77.39698792]].
[2019-04-04 06:05:57,564] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7817163e-10 4.8942123e-10 7.4405006e-24 2.0568601e-10 1.3522165e-10
 8.0569996e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:05:57,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8671
[2019-04-04 06:05:57,578] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666666, 97.33333333333333, 0.0, 0.0, 26.0, 25.595002253001, 0.4537615224568336, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4682400.0000, 
sim time next is 4683000.0000, 
raw observation next is [-0.8333333333333334, 98.66666666666667, 0.0, 0.0, 26.0, 25.58885513276893, 0.448723859508454, 0.0, 1.0, 18736.672800439], 
processed observation next is [1.0, 0.17391304347826086, 0.43951985226223456, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6324045943974109, 0.6495746198361513, 0.0, 1.0, 0.0892222514306619], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.50803965], dtype=float32), 0.83555484]. 
=============================================
[2019-04-04 06:05:57,588] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[82.45229]
 [82.47662]
 [82.4886 ]
 [82.47418]
 [82.43034]], R is [[82.6116333 ]
 [82.78551483]
 [82.95765686]
 [83.12808228]
 [83.29679871]].
[2019-04-04 06:06:00,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.10521052e-12 4.26492383e-11 9.67255944e-26 5.37495872e-12
 1.36601711e-11 1.20020435e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:06:00,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4698
[2019-04-04 06:06:00,744] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 78.0, 0.0, 0.0, 26.0, 25.46870806872292, 0.4486782821404582, 1.0, 1.0, 25060.77965604894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4731600.0000, 
sim time next is 4732200.0000, 
raw observation next is [-0.5, 78.0, 0.0, 0.0, 26.0, 25.29691304876611, 0.4383573089699069, 1.0, 1.0, 25752.05802064527], 
processed observation next is [1.0, 0.782608695652174, 0.44875346260387816, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6080760873971759, 0.646119102989969, 1.0, 1.0, 0.12262884771735842], 
reward next is 0.8774, 
noisyNet noise sample is [array([1.0320109], dtype=float32), -1.1848383]. 
=============================================
[2019-04-04 06:06:02,115] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.33288611e-09 1.07589635e-08 2.46786867e-22 1.14473264e-09
 3.90089960e-09 7.43300040e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 06:06:02,115] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2341
[2019-04-04 06:06:02,128] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 50.0, 0.0, 0.0, 26.0, 25.26628704323304, 0.292921750733034, 0.0, 1.0, 39996.01001760258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4936800.0000, 
sim time next is 4937400.0000, 
raw observation next is [-1.5, 50.0, 0.0, 0.0, 26.0, 25.23796087682423, 0.2845205912529093, 0.0, 1.0, 38825.56857207812], 
processed observation next is [1.0, 0.13043478260869565, 0.4210526315789474, 0.5, 0.0, 0.0, 0.6666666666666666, 0.603163406402019, 0.5948401970843031, 0.0, 1.0, 0.18488365986703867], 
reward next is 0.8151, 
noisyNet noise sample is [array([1.031739], dtype=float32), -0.70981514]. 
=============================================
[2019-04-04 06:06:09,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7766871e-09 3.8938452e-09 8.0436612e-23 1.7649429e-09 3.2061007e-09
 4.4224420e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:06:09,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4302
[2019-04-04 06:06:09,827] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.30064443847949, 0.1961621623825639, 0.0, 1.0, 41313.24162684047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4771200.0000, 
sim time next is 4771800.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.31237520401486, 0.1880675601744506, 0.0, 1.0, 41334.62872933998], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5260312670012383, 0.5626891867248168, 0.0, 1.0, 0.19683156537780944], 
reward next is 0.8032, 
noisyNet noise sample is [array([0.24984705], dtype=float32), 0.70643896]. 
=============================================
[2019-04-04 06:06:11,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:11,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:11,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run8
[2019-04-04 06:06:11,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4599380e-09 1.7934617e-09 2.2821791e-22 2.2166748e-09 3.2629857e-10
 2.5027138e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:06:11,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2006
[2019-04-04 06:06:11,774] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 41.5, 0.0, 0.0, 26.0, 25.01396681165377, 0.3457997860858857, 0.0, 1.0, 18996.50026208359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4818600.0000, 
sim time next is 4819200.0000, 
raw observation next is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.02042626856862, 0.339442212343933, 0.0, 1.0, 19654.07437361286], 
processed observation next is [0.0, 0.782608695652174, 0.4995383194829178, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5850355223807183, 0.6131474041146444, 0.0, 1.0, 0.09359083035053743], 
reward next is 0.9064, 
noisyNet noise sample is [array([0.2627567], dtype=float32), 0.36453933]. 
=============================================
[2019-04-04 06:06:13,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:13,687] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:13,691] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run8
[2019-04-04 06:06:13,928] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7739269e-09 1.4802333e-09 1.1892336e-23 6.4343519e-10 2.4955554e-10
 1.0998590e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:06:13,929] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4147
[2019-04-04 06:06:13,985] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 46.66666666666667, 30.99999999999999, 186.6666666666666, 26.0, 25.28505340103747, 0.2732379465515654, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4953000.0000, 
sim time next is 4953600.0000, 
raw observation next is [-2.0, 46.0, 46.5, 280.0, 26.0, 25.22750673147654, 0.27510961673412, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.46, 0.155, 0.30939226519337015, 0.6666666666666666, 0.6022922276230451, 0.59170320557804, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4921002], dtype=float32), 1.0649267]. 
=============================================
[2019-04-04 06:06:15,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:15,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:15,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run8
[2019-04-04 06:06:17,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:17,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:17,023] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run8
[2019-04-04 06:06:22,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:22,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:22,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run8
[2019-04-04 06:06:23,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:23,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:23,059] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run8
[2019-04-04 06:06:23,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:23,119] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:23,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run8
[2019-04-04 06:06:25,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:25,213] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:25,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run8
[2019-04-04 06:06:27,217] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5378972e-09 2.9556271e-09 3.9922874e-24 3.8254036e-10 3.1210662e-10
 1.8208949e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:06:27,217] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3566
[2019-04-04 06:06:27,289] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 65.0, 65.33333333333334, 173.0, 26.0, 25.584944168691, 0.4155815591987598, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5038800.0000, 
sim time next is 5039400.0000, 
raw observation next is [-2.166666666666667, 65.0, 71.66666666666666, 245.0, 26.0, 25.58886088914432, 0.4229868723969737, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.65, 0.23888888888888885, 0.27071823204419887, 0.6666666666666666, 0.6324050740953601, 0.6409956241323246, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.058441], dtype=float32), -0.662456]. 
=============================================
[2019-04-04 06:06:28,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:28,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:28,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run8
[2019-04-04 06:06:28,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:28,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:28,421] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run8
[2019-04-04 06:06:30,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:30,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:30,168] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run8
[2019-04-04 06:06:30,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:30,231] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:30,268] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run8
[2019-04-04 06:06:30,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:30,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:30,928] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run8
[2019-04-04 06:06:32,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:32,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:32,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run8
[2019-04-04 06:06:32,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:32,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:32,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run8
[2019-04-04 06:06:32,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:06:32,905] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:06:32,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run8
[2019-04-04 06:06:40,387] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.0167670e-02 4.6494059e-02 5.0578982e-04 4.6787743e-02 3.2648146e-02
 2.7166443e-02 7.8623015e-01], sum to 1.0000
[2019-04-04 06:06:40,387] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4939
[2019-04-04 06:06:40,399] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.2, 95.16666666666667, 0.0, 0.0, 19.0, 18.7323782676697, -1.110112876664128, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [21.0], 
sim time this is 600.0000, 
sim time next is 1200.0000, 
raw observation next is [2.4, 95.33333333333334, 0.0, 0.0, 21.0, 18.69959041274062, -1.098141037812602, 0.0, 1.0, 198128.1454825078], 
processed observation next is [0.0, 0.0, 0.5290858725761773, 0.9533333333333335, 0.0, 0.0, 0.25, 0.058299201061718264, 0.13395298739579933, 0.0, 1.0, 0.9434673594405133], 
reward next is 0.0565, 
noisyNet noise sample is [array([0.46276984], dtype=float32), 0.8404432]. 
=============================================
[2019-04-04 06:06:41,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.28772529e-09 1.25488615e-08 1.58243700e-21 9.68533920e-10
 3.55175533e-09 3.40337585e-11 1.00000000e+00], sum to 1.0000
[2019-04-04 06:06:41,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7090
[2019-04-04 06:06:41,038] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.85809589508709, -0.2262692373377495, 0.0, 1.0, 44902.66608725009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 193200.0000, 
sim time next is 193800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.7914684333241, -0.2373244590424635, 0.0, 1.0, 44943.71573729312], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.39928903611034156, 0.42089184698584553, 0.0, 1.0, 0.2140176939871101], 
reward next is 0.7860, 
noisyNet noise sample is [array([0.8717028], dtype=float32), -0.4253149]. 
=============================================
[2019-04-04 06:06:49,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.1151299e-09 8.1750029e-09 1.4114441e-20 2.2443791e-09 1.0564495e-09
 3.2623553e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:06:49,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9110
[2019-04-04 06:06:49,911] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.71178837462841, -0.2542553896173845, 0.0, 1.0, 44953.25353150383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.65610158040396, -0.2598757296748834, 0.0, 1.0, 44957.35091117842], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.38800846503366326, 0.4133747567750388, 0.0, 1.0, 0.21408262338656392], 
reward next is 0.7859, 
noisyNet noise sample is [array([0.03759551], dtype=float32), 1.8595204]. 
=============================================
[2019-04-04 06:06:59,157] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3317070e-10 4.3198276e-10 4.2450041e-24 6.1653745e-11 6.2049185e-11
 4.0052526e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:06:59,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7477
[2019-04-04 06:06:59,211] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.583333333333333, 62.5, 138.3333333333333, 0.0, 26.0, 25.84403032438265, 0.402392962623317, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 222600.0000, 
sim time next is 223200.0000, 
raw observation next is [-3.4, 62.0, 133.0, 0.0, 26.0, 26.04183948325052, 0.4183720964447887, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.368421052631579, 0.62, 0.44333333333333336, 0.0, 0.6666666666666666, 0.6701532902708767, 0.6394573654815963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47891203], dtype=float32), 0.3852754]. 
=============================================
[2019-04-04 06:07:02,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.2330841e-11 7.1105830e-11 1.3383408e-24 1.3005174e-11 2.4271019e-11
 6.5923294e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:02,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1414
[2019-04-04 06:07:02,142] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.05, 46.5, 83.0, 758.0, 26.0, 25.40248485823452, 0.3540170195324577, 1.0, 1.0, 142753.8981698468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 304200.0000, 
sim time next is 304800.0000, 
raw observation next is [-9.866666666666667, 45.66666666666667, 85.0, 736.8333333333334, 26.0, 24.60864735638268, 0.3296979623693908, 1.0, 1.0, 200611.3222299295], 
processed observation next is [1.0, 0.5217391304347826, 0.18928901200369344, 0.4566666666666667, 0.2833333333333333, 0.8141804788213628, 0.6666666666666666, 0.5507206130318899, 0.6098993207897969, 1.0, 1.0, 0.9552920106187119], 
reward next is 0.0447, 
noisyNet noise sample is [array([0.04585572], dtype=float32), 0.48550457]. 
=============================================
[2019-04-04 06:07:06,331] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.3114400e-10 1.5789162e-09 2.9396561e-23 3.8530762e-10 5.8741750e-10
 1.1922609e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:06,332] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9696
[2019-04-04 06:07:06,355] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 77.33333333333334, 0.0, 0.0, 26.0, 24.46117997356117, 0.1593161444588466, 0.0, 1.0, 44179.42109850749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [-3.9, 78.5, 0.0, 0.0, 26.0, 24.42796967631594, 0.1522721240304382, 0.0, 1.0, 44179.02728437835], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.785, 0.0, 0.0, 0.6666666666666666, 0.535664139692995, 0.5507573746768127, 0.0, 1.0, 0.21037632040180168], 
reward next is 0.7896, 
noisyNet noise sample is [array([-0.62748545], dtype=float32), 1.4469484]. 
=============================================
[2019-04-04 06:07:07,880] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.5424883e-08 2.5781317e-08 2.0305816e-19 4.1498094e-09 9.1355954e-09
 5.5180631e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 06:07:07,880] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5363
[2019-04-04 06:07:07,926] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.2, 67.5, 0.0, 0.0, 26.0, 22.49436981691802, -0.271494243364201, 0.0, 1.0, 47923.38660380243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 283800.0000, 
sim time next is 284400.0000, 
raw observation next is [-12.3, 67.0, 0.0, 0.0, 26.0, 22.52900624515986, -0.2781489002776362, 0.0, 1.0, 47957.80718720323], 
processed observation next is [1.0, 0.30434782608695654, 0.12188365650969527, 0.67, 0.0, 0.0, 0.6666666666666666, 0.37741718709665495, 0.40728369990745455, 0.0, 1.0, 0.22837051041525347], 
reward next is 0.7716, 
noisyNet noise sample is [array([2.709191], dtype=float32), -0.68980473]. 
=============================================
[2019-04-04 06:07:08,706] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6375468e-10 3.3099984e-10 3.1684766e-24 1.3316333e-10 2.6591004e-10
 3.8730824e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:08,707] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0478
[2019-04-04 06:07:08,801] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 26.0, 25.82324356890561, 0.3602448133428093, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 297600.0000, 
sim time next is 298200.0000, 
raw observation next is [-10.78333333333333, 60.5, 93.33333333333334, 560.6666666666666, 26.0, 25.80085491900044, 0.3581640626709315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.1638965835641737, 0.605, 0.3111111111111111, 0.6195211786372007, 0.6666666666666666, 0.6500712432500366, 0.6193880208903105, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11481161], dtype=float32), 2.1672325]. 
=============================================
[2019-04-04 06:07:19,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4139880e-11 3.1224417e-10 4.9865389e-24 4.8011588e-11 2.8020994e-11
 1.0446478e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:19,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2663
[2019-04-04 06:07:19,407] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999998, 36.66666666666667, 81.33333333333334, 0.0, 26.0, 25.38789014139089, 0.2643387051754841, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 485400.0000, 
sim time next is 486000.0000, 
raw observation next is [0.0, 37.0, 75.0, 0.0, 26.0, 25.59505862549405, 0.2857127061756037, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.37, 0.25, 0.0, 0.6666666666666666, 0.6329215521245043, 0.5952375687252013, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4805108], dtype=float32), 0.09131417]. 
=============================================
[2019-04-04 06:07:19,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.59349 ]
 [82.700226]
 [82.685974]
 [81.820984]
 [82.00862 ]], R is [[82.60900116]
 [82.78291321]
 [82.84147644]
 [82.0683136 ]
 [82.24763489]].
[2019-04-04 06:07:20,298] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1434991e-10 6.6405126e-10 2.2065605e-23 8.1880877e-11 1.4117259e-10
 2.2187008e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:20,298] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0697
[2019-04-04 06:07:20,383] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 37.0, 21.0, 403.0, 26.0, 25.90203791551682, 0.4764561615414877, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 405000.0000, 
sim time next is 405600.0000, 
raw observation next is [-8.9, 36.66666666666667, 17.5, 338.6666666666667, 26.0, 26.15051961086881, 0.3373514625962482, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.21606648199445982, 0.3666666666666667, 0.058333333333333334, 0.37421731123388585, 0.6666666666666666, 0.6792099675724007, 0.6124504875320828, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.82992256], dtype=float32), -1.5022427]. 
=============================================
[2019-04-04 06:07:23,515] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3966782e-09 8.7647406e-10 8.3397756e-23 2.7823477e-10 4.3208825e-10
 7.4965826e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:23,515] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0931
[2019-04-04 06:07:23,574] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.283333333333334, 64.33333333333334, 92.66666666666667, 25.33333333333334, 26.0, 24.88751950897929, 0.2052406953107334, 0.0, 1.0, 35950.45452208197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 645000.0000, 
sim time next is 645600.0000, 
raw observation next is [-3.166666666666667, 63.66666666666667, 90.83333333333334, 31.66666666666667, 26.0, 24.86615127796273, 0.2046239469436819, 0.0, 1.0, 52296.32522668125], 
processed observation next is [0.0, 0.4782608695652174, 0.3748845798707295, 0.6366666666666667, 0.3027777777777778, 0.03499079189686925, 0.6666666666666666, 0.5721792731635608, 0.5682079823145606, 0.0, 1.0, 0.24903012012705356], 
reward next is 0.7510, 
noisyNet noise sample is [array([-1.0913763], dtype=float32), 0.6925992]. 
=============================================
[2019-04-04 06:07:26,807] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0249830e-10 1.6392276e-10 4.2020012e-25 4.6816283e-11 8.1268006e-11
 5.0245723e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:26,807] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0882
[2019-04-04 06:07:26,848] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.8, 25.5, 124.3333333333333, 0.0, 26.0, 25.11429365125633, 0.1581294708256856, 1.0, 1.0, 36016.12796888007], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 474600.0000, 
sim time next is 475200.0000, 
raw observation next is [-1.7, 25.0, 125.5, 0.0, 26.0, 25.14686875702069, 0.1672050348183133, 1.0, 1.0, 18707.6652130389], 
processed observation next is [1.0, 0.5217391304347826, 0.4155124653739613, 0.25, 0.41833333333333333, 0.0, 0.6666666666666666, 0.5955723964183909, 0.5557350116061045, 1.0, 1.0, 0.08908412006209], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.71825397], dtype=float32), 0.4497477]. 
=============================================
[2019-04-04 06:07:33,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8516657e-10 6.3647129e-11 5.0427354e-26 1.9768437e-11 2.9027338e-11
 5.4185043e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:33,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9909
[2019-04-04 06:07:33,366] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 12.0, 37.99999999999999, 26.0, 24.31888398139618, 0.1465189206325483, 0.0, 1.0, 41086.67711052085], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 546600.0000, 
sim time next is 547200.0000, 
raw observation next is [0.5, 92.0, 17.5, 54.5, 26.0, 24.34865135696345, 0.1477418423502798, 0.0, 1.0, 41025.70747964992], 
processed observation next is [0.0, 0.34782608695652173, 0.4764542936288089, 0.92, 0.058333333333333334, 0.06022099447513812, 0.6666666666666666, 0.5290542797469543, 0.5492472807834267, 0.0, 1.0, 0.19536051180785677], 
reward next is 0.8046, 
noisyNet noise sample is [array([-0.28558877], dtype=float32), 0.23399135]. 
=============================================
[2019-04-04 06:07:43,195] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8912145e-10 2.2115976e-10 3.7802542e-24 5.3307227e-11 2.2349203e-10
 1.0794785e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:43,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5090
[2019-04-04 06:07:43,242] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.77075185366729, 0.2270033620490607, 0.0, 1.0, 39503.94929971015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 867600.0000, 
sim time next is 868200.0000, 
raw observation next is [-2.2, 79.83333333333334, 0.0, 0.0, 26.0, 24.79317189304761, 0.225851600670369, 0.0, 1.0, 39465.43501141184], 
processed observation next is [1.0, 0.043478260869565216, 0.4016620498614959, 0.7983333333333335, 0.0, 0.0, 0.6666666666666666, 0.5660976577539675, 0.575283866890123, 0.0, 1.0, 0.18793064291148495], 
reward next is 0.8121, 
noisyNet noise sample is [array([-0.4793805], dtype=float32), -0.1795053]. 
=============================================
[2019-04-04 06:07:48,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4453143e-10 3.6678266e-10 4.0083674e-26 3.5070800e-11 2.2311370e-11
 5.4745183e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:48,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7268
[2019-04-04 06:07:48,698] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384755964628, 0.3308074057168242, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808800.0000, 
sim time next is 809400.0000, 
raw observation next is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 26.0, 25.871694127301, 0.3152053168449394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.288550323176362, 0.75, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6559745106084168, 0.6050684389483131, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.87458897], dtype=float32), -0.014238578]. 
=============================================
[2019-04-04 06:07:49,290] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3605445e-11 1.3106425e-11 5.0104969e-28 7.5958120e-12 9.2569606e-12
 1.9867139e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:49,291] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0777
[2019-04-04 06:07:49,315] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 84.0, 80.33333333333334, 0.0, 26.0, 25.46660593687812, 0.2913349055176993, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 902400.0000, 
sim time next is 903000.0000, 
raw observation next is [1.1, 84.0, 83.66666666666666, 0.0, 26.0, 25.452744797513, 0.2912276533609816, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.27888888888888885, 0.0, 0.6666666666666666, 0.6210620664594165, 0.5970758844536606, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47249717], dtype=float32), 0.15342371]. 
=============================================
[2019-04-04 06:07:49,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[94.62848]
 [94.51715]
 [94.48304]
 [94.43381]
 [94.33595]], R is [[94.78819275]
 [94.84030914]
 [94.89190674]
 [94.94298553]
 [94.99355316]].
[2019-04-04 06:07:49,676] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4376428e-11 4.5249603e-12 3.5188490e-28 1.8523846e-12 8.4991980e-13
 2.7519731e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:49,678] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8599
[2019-04-04 06:07:49,691] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.616666666666666, 93.66666666666666, 101.3333333333333, 0.0, 26.0, 25.16360031258764, 0.2544374686793878, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 910200.0000, 
sim time next is 910800.0000, 
raw observation next is [3.8, 93.0, 100.0, 0.0, 26.0, 25.16036132297651, 0.252295848575135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.3333333333333333, 0.0, 0.6666666666666666, 0.5966967769147091, 0.5840986161917117, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.26728788], dtype=float32), -1.5768685]. 
=============================================
[2019-04-04 06:07:59,240] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3729142e-10 4.8508739e-11 4.3815176e-28 7.9941661e-12 2.0943169e-11
 2.7136299e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:07:59,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7192
[2019-04-04 06:07:59,254] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.15, 81.5, 0.0, 0.0, 26.0, 25.62829111518462, 0.6127366325927864, 0.0, 1.0, 25008.2417502817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1146600.0000, 
sim time next is 1147200.0000, 
raw observation next is [12.33333333333333, 81.0, 0.0, 0.0, 26.0, 25.64418175696454, 0.6138254118103535, 0.0, 1.0, 18727.70131426611], 
processed observation next is [0.0, 0.2608695652173913, 0.8042474607571561, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6370151464137116, 0.7046084706034512, 0.0, 1.0, 0.08917953006793385], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.3429886], dtype=float32), 0.30080107]. 
=============================================
[2019-04-04 06:08:01,193] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.4083661e-10 1.7735301e-10 4.6799878e-25 3.4662544e-11 1.0596031e-10
 3.4140022e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:01,194] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6103
[2019-04-04 06:08:01,215] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 24.7303943294307, 0.2302659608049119, 0.0, 1.0, 40793.0507959004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 861600.0000, 
sim time next is 862200.0000, 
raw observation next is [-2.55, 79.5, 0.0, 0.0, 26.0, 24.69922431603894, 0.2250509832482663, 0.0, 1.0, 40658.45283621625], 
processed observation next is [1.0, 1.0, 0.3919667590027701, 0.795, 0.0, 0.0, 0.6666666666666666, 0.558268693003245, 0.5750169944160888, 0.0, 1.0, 0.19361168017245833], 
reward next is 0.8064, 
noisyNet noise sample is [array([-0.3948692], dtype=float32), 0.65344083]. 
=============================================
[2019-04-04 06:08:01,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0575225e-08 2.6783720e-10 1.2425648e-24 1.9886970e-10 3.7575149e-10
 3.6836711e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:01,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2193
[2019-04-04 06:08:01,578] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 78.33333333333333, 0.0, 0.0, 26.0, 24.16406590526762, 0.285260985663863, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1210200.0000, 
sim time next is 1210800.0000, 
raw observation next is [16.1, 78.66666666666667, 0.0, 0.0, 26.0, 24.13840717412264, 0.2807690298581762, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5115339311768867, 0.5935896766193921, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2327971], dtype=float32), -1.6736803]. 
=============================================
[2019-04-04 06:08:05,779] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4440846e-11 1.4296036e-11 1.8894863e-26 1.1854521e-11 7.8963164e-12
 6.7780703e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:05,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3447
[2019-04-04 06:08:05,787] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 63.5, 0.0, 26.0, 27.53928084007664, 0.9739404609176266, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1094400.0000, 
sim time next is 1095000.0000, 
raw observation next is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 26.0, 27.72136589603385, 0.9971247718708675, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9921514312096033, 0.4916666666666667, 0.18, 0.0, 0.6666666666666666, 0.8101138246694873, 0.8323749239569559, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7027298], dtype=float32), -1.9616325]. 
=============================================
[2019-04-04 06:08:05,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[93.06311 ]
 [93.02726 ]
 [92.998405]
 [92.9623  ]
 [92.910484]], R is [[93.16864777]
 [93.23696136]
 [93.30459595]
 [93.37155151]
 [93.43783569]].
[2019-04-04 06:08:07,308] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.34950609e-11 7.16338655e-11 1.18315866e-27 4.88665185e-12
 4.50410499e-12 1.33296465e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:08:07,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1046
[2019-04-04 06:08:07,391] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 9.0, 0.0, 26.0, 25.05375484092359, 0.4296938118634375, 1.0, 1.0, 127877.2384800802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1357200.0000, 
sim time next is 1357800.0000, 
raw observation next is [0.5, 96.0, 5.999999999999998, 0.0, 26.0, 24.38308974745081, 0.4368673853398315, 1.0, 1.0, 197326.727735802], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.019999999999999993, 0.0, 0.6666666666666666, 0.5319241456209008, 0.6456224617799439, 1.0, 1.0, 0.9396510844562], 
reward next is 0.0603, 
noisyNet noise sample is [array([1.0802921], dtype=float32), -0.16712922]. 
=============================================
[2019-04-04 06:08:11,459] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0843660e-11 4.7007405e-11 3.3967056e-28 5.4656449e-12 6.3593826e-12
 3.3998245e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:11,459] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7069
[2019-04-04 06:08:11,476] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 26.02342135515887, 0.6374973190849568, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1049400.0000, 
sim time next is 1050000.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 25.99488205852138, 0.6321848095991237, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6662401715434484, 0.7107282698663746, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03201453], dtype=float32), 0.6017304]. 
=============================================
[2019-04-04 06:08:11,490] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[97.0627 ]
 [97.06223]
 [97.07828]
 [97.10344]
 [96.90218]], R is [[97.01674652]
 [97.04657745]
 [97.07611084]
 [97.10535431]
 [96.86771393]].
[2019-04-04 06:08:12,317] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.40714591e-11 3.47999518e-10 4.01456284e-27 5.79640546e-12
 1.85370747e-11 1.13231134e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:08:12,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7419
[2019-04-04 06:08:12,331] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.53333333333333, 64.66666666666667, 0.0, 0.0, 26.0, 25.75688242783894, 0.6538773807850798, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1117200.0000, 
sim time next is 1117800.0000, 
raw observation next is [12.45, 65.0, 0.0, 0.0, 26.0, 25.7085731366133, 0.6426826546697703, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8074792243767314, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6423810947177749, 0.71422755155659, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42945147], dtype=float32), 0.86207944]. 
=============================================
[2019-04-04 06:08:22,394] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0083629e-10 1.1306944e-10 4.3669570e-25 1.2554994e-11 5.6768631e-11
 3.7830307e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:22,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7515
[2019-04-04 06:08:22,414] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 74.0, 0.0, 0.0, 26.0, 25.57808109625306, 0.5760091901266039, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1627200.0000, 
sim time next is 1627800.0000, 
raw observation next is [7.616666666666667, 74.33333333333334, 0.0, 0.0, 26.0, 25.48761857925103, 0.5596153273749805, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6735918744228995, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6239682149375859, 0.6865384424583268, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9449087], dtype=float32), -1.5831976]. 
=============================================
[2019-04-04 06:08:23,732] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6815868e-10 3.5398842e-10 7.3932438e-26 3.1594755e-11 2.5438490e-10
 6.5483548e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:23,733] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1825
[2019-04-04 06:08:23,744] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.800000000000001, 83.33333333333334, 0.0, 0.0, 26.0, 25.59104668342856, 0.5290139194196284, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1574400.0000, 
sim time next is 1575000.0000, 
raw observation next is [4.85, 83.0, 0.0, 0.0, 26.0, 25.63605819268797, 0.5183103265520653, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.5969529085872576, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6363381827239974, 0.6727701088506884, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6063633], dtype=float32), -0.2591419]. 
=============================================
[2019-04-04 06:08:23,754] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.23294 ]
 [84.391914]
 [84.58219 ]
 [84.50611 ]
 [84.32901 ]], R is [[84.25760651]
 [84.41503143]
 [84.5708847 ]
 [84.43312836]
 [84.17444611]].
[2019-04-04 06:08:26,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.8410093e-11 6.2804317e-11 3.7604599e-26 1.5878934e-11 3.7935259e-11
 4.2400653e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:26,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2389
[2019-04-04 06:08:26,355] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 92.0, 71.66666666666667, 0.0, 26.0, 25.90049092390933, 0.5361049088299774, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1680600.0000, 
sim time next is 1681200.0000, 
raw observation next is [1.1, 92.0, 74.5, 0.0, 26.0, 25.89499361503237, 0.5372907601289569, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.92, 0.24833333333333332, 0.0, 0.6666666666666666, 0.6579161345860308, 0.6790969200429856, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5878752], dtype=float32), -0.91900575]. 
=============================================
[2019-04-04 06:08:27,946] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3841330e-11 5.1098421e-11 1.8426772e-26 1.5801648e-11 1.6253081e-10
 6.3898946e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:27,950] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1277
[2019-04-04 06:08:27,966] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.1, 96.16666666666666, 0.0, 0.0, 26.0, 25.74938300047975, 0.5711720844856935, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1649400.0000, 
sim time next is 1650000.0000, 
raw observation next is [7.0, 96.33333333333333, 0.0, 0.0, 26.0, 25.69118402075772, 0.5731964540011075, 0.0, 1.0, 34472.01686468608], 
processed observation next is [1.0, 0.08695652173913043, 0.6565096952908588, 0.9633333333333333, 0.0, 0.0, 0.6666666666666666, 0.64093200172981, 0.6910654846670359, 0.0, 1.0, 0.1641524612604099], 
reward next is 0.8358, 
noisyNet noise sample is [array([-0.14605707], dtype=float32), 0.22724314]. 
=============================================
[2019-04-04 06:08:27,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[87.37676 ]
 [87.340996]
 [87.404   ]
 [87.386955]
 [87.44149 ]], R is [[87.41976166]
 [87.54556274]
 [87.67010498]
 [87.79340363]
 [87.91547394]].
[2019-04-04 06:08:36,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7834584e-09 1.0385106e-09 9.7830453e-23 2.5020899e-10 5.2291654e-10
 2.1581264e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:08:36,687] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5261
[2019-04-04 06:08:36,743] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 82.5, 0.0, 26.0, 24.98251051574804, 0.3462424584031483, 0.0, 1.0, 33546.62614776976], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1782000.0000, 
sim time next is 1782600.0000, 
raw observation next is [-2.9, 87.0, 77.0, 0.0, 26.0, 24.995696214495, 0.3438633586954738, 0.0, 1.0, 33127.64831680715], 
processed observation next is [0.0, 0.6521739130434783, 0.38227146814404434, 0.87, 0.25666666666666665, 0.0, 0.6666666666666666, 0.5829746845412499, 0.614621119565158, 0.0, 1.0, 0.15775070627051024], 
reward next is 0.8422, 
noisyNet noise sample is [array([0.41154122], dtype=float32), 0.7115506]. 
=============================================
[2019-04-04 06:08:48,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.04110824e-10 2.54983368e-10 1.24776111e-24 1.31616376e-11
 6.70645553e-11 3.89790739e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:08:48,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7625
[2019-04-04 06:08:48,254] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 94.0, 0.0, 0.0, 26.0, 25.36174824269355, 0.4809118262337628, 0.0, 1.0, 49512.97691768205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1725600.0000, 
sim time next is 1726200.0000, 
raw observation next is [0.25, 93.5, 0.0, 0.0, 26.0, 25.36254282977009, 0.4795915284125747, 0.0, 1.0, 45727.17319783988], 
processed observation next is [1.0, 1.0, 0.46952908587257625, 0.935, 0.0, 0.0, 0.6666666666666666, 0.6135452358141741, 0.6598638428041915, 0.0, 1.0, 0.21774844379923752], 
reward next is 0.7823, 
noisyNet noise sample is [array([-0.48265558], dtype=float32), 1.2910054]. 
=============================================
[2019-04-04 06:08:52,079] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 06:08:52,089] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:08:52,089] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:08:52,091] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:08:52,091] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:08:52,097] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:08:52,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run11
[2019-04-04 06:08:52,153] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:08:52,169] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run11
[2019-04-04 06:08:52,239] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run11
[2019-04-04 06:11:53,011] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.25769597], dtype=float32), 0.060616177]
[2019-04-04 06:11:53,012] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.75, 86.0, 102.0, 77.0, 26.0, 26.26557505413637, 0.8366400145392553, 0.0, 1.0, 0.0]
[2019-04-04 06:11:53,012] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:11:53,013] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.3325596e-11 1.8111221e-11 5.2727542e-28 5.3609318e-12 1.3161889e-11
 6.6317492e-15 1.0000000e+00], sampled 0.4833957931130526
[2019-04-04 06:12:00,441] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 06:12:10,343] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.25769597], dtype=float32), 0.060616177]
[2019-04-04 06:12:10,343] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [10.46666666666667, 58.66666666666666, 0.0, 0.0, 26.0, 26.96362796085408, 0.8685628850196615, 0.0, 1.0, 0.0]
[2019-04-04 06:12:10,343] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:12:10,344] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.6721102e-10 1.8271461e-10 1.9190666e-25 3.6614711e-11 6.5425221e-11
 2.1124926e-13 1.0000000e+00], sampled 0.3628284659220725
[2019-04-04 06:12:30,561] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 06:12:35,604] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:12:36,634] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1000000, evaluation results [1000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:12:42,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.8971225e-09 2.5795917e-09 5.4813499e-22 8.4848922e-10 9.1289376e-10
 2.9403062e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:12:42,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7378
[2019-04-04 06:12:43,013] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.45577426001391, 0.1849205434207194, 0.0, 1.0, 42651.76270229545], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1987200.0000, 
sim time next is 1987800.0000, 
raw observation next is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.43500750565472, 0.1779633875393098, 0.0, 1.0, 42539.22174022312], 
processed observation next is [1.0, 0.0, 0.30470914127423826, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5362506254712267, 0.55932112917977, 0.0, 1.0, 0.20256772257249103], 
reward next is 0.7974, 
noisyNet noise sample is [array([-0.27110067], dtype=float32), -0.40154207]. 
=============================================
[2019-04-04 06:12:48,655] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6831197e-08 1.5512157e-08 3.7902955e-21 4.4748352e-09 1.4517865e-08
 3.3468811e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:12:48,655] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6262
[2019-04-04 06:12:48,728] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 26.0, 24.63826087932295, 0.2261688009811902, 0.0, 1.0, 84617.47164899629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1846200.0000, 
sim time next is 1846800.0000, 
raw observation next is [-6.7, 78.0, 87.5, 47.0, 26.0, 24.97961537003874, 0.2558487375974559, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2770083102493075, 0.78, 0.2916666666666667, 0.051933701657458566, 0.6666666666666666, 0.581634614169895, 0.5852829125324853, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38944107], dtype=float32), 0.17330249]. 
=============================================
[2019-04-04 06:12:59,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2337682e-11 7.6088663e-11 4.7808406e-24 3.9199522e-11 9.3507459e-12
 8.3389228e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:12:59,774] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7814
[2019-04-04 06:12:59,832] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 62.0, 52.0, 0.0, 26.0, 25.55532408247431, 0.3160022003522833, 1.0, 1.0, 26354.08982037136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1958400.0000, 
sim time next is 1959000.0000, 
raw observation next is [-2.983333333333333, 64.16666666666667, 44.66666666666666, 0.0, 26.0, 25.53946175851087, 0.3109184634532439, 1.0, 1.0, 30873.87871335168], 
processed observation next is [1.0, 0.6956521739130435, 0.37996306555863346, 0.6416666666666667, 0.14888888888888885, 0.0, 0.6666666666666666, 0.628288479875906, 0.6036394878177479, 1.0, 1.0, 0.14701847006357943], 
reward next is 0.8530, 
noisyNet noise sample is [array([0.30727896], dtype=float32), 0.9051567]. 
=============================================
[2019-04-04 06:12:59,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.9629  ]
 [79.73083 ]
 [79.46548 ]
 [79.220695]
 [78.90452 ]], R is [[80.20146942]
 [80.2739563 ]
 [80.34247589]
 [80.39923096]
 [80.42242432]].
[2019-04-04 06:13:10,950] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.7768456e-10 1.0730902e-09 2.6504142e-23 1.2379937e-10 4.7298016e-10
 1.4077074e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:13:10,950] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1151
[2019-04-04 06:13:10,974] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 77.0, 0.0, 0.0, 26.0, 24.563204035315, 0.221238836495219, 0.0, 1.0, 44138.9422369574], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2245200.0000, 
sim time next is 2245800.0000, 
raw observation next is [-6.616666666666667, 77.5, 0.0, 0.0, 26.0, 24.51216292196222, 0.2103711150619994, 0.0, 1.0, 44182.03470760248], 
processed observation next is [1.0, 1.0, 0.2793167128347184, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5426802434968515, 0.5701237050206664, 0.0, 1.0, 0.2103906414647737], 
reward next is 0.7896, 
noisyNet noise sample is [array([-1.0298522], dtype=float32), 2.0905914]. 
=============================================
[2019-04-04 06:13:29,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6584160e-11 2.7548566e-10 1.1529476e-24 4.9854974e-11 6.2192793e-11
 8.4022578e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:13:29,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7988
[2019-04-04 06:13:29,239] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.866666666666666, 70.66666666666666, 0.0, 0.0, 26.0, 24.99861267575968, 0.361365802206095, 1.0, 1.0, 135714.531848945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2230800.0000, 
sim time next is 2231400.0000, 
raw observation next is [-4.933333333333334, 70.83333333333334, 0.0, 0.0, 26.0, 25.00219678937218, 0.3785890143680111, 0.0, 1.0, 84415.33398752917], 
processed observation next is [1.0, 0.8260869565217391, 0.3259464450600185, 0.7083333333333335, 0.0, 0.0, 0.6666666666666666, 0.5835163991143485, 0.6261963381226704, 0.0, 1.0, 0.4019777808929961], 
reward next is 0.5980, 
noisyNet noise sample is [array([0.7003383], dtype=float32), 1.0345445]. 
=============================================
[2019-04-04 06:13:31,989] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3323388e-09 1.4205727e-09 8.9360891e-22 8.5023727e-10 4.4152487e-10
 4.2070410e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:13:31,989] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7370
[2019-04-04 06:13:32,035] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.00000000000001, 110.6666666666667, 227.3333333333334, 26.0, 24.94912489195336, 0.3080602168246775, 0.0, 1.0, 28169.63016020905], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2387400.0000, 
sim time next is 2388000.0000, 
raw observation next is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 26.0, 24.96511345834705, 0.3130466557968632, 0.0, 1.0, 20412.17705888427], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.3277777777777778, 0.31399631675874773, 0.6666666666666666, 0.5804261215289209, 0.604348885265621, 0.0, 1.0, 0.09720084313754414], 
reward next is 0.9028, 
noisyNet noise sample is [array([-1.6421171], dtype=float32), -0.08908327]. 
=============================================
[2019-04-04 06:13:32,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[73.260376]
 [72.828064]
 [72.37891 ]
 [71.932434]
 [71.60478 ]], R is [[73.74983215]
 [73.87819672]
 [73.95684052]
 [74.010849  ]
 [74.08728027]].
[2019-04-04 06:13:46,259] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3440035e-11 4.4408373e-11 6.6152165e-25 2.2537200e-11 1.0448197e-11
 1.8325006e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:13:46,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3172
[2019-04-04 06:13:46,301] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.5, 74.0, 110.3333333333333, 653.3333333333333, 26.0, 26.07265279908997, 0.4636508372798537, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2715000.0000, 
sim time next is 2715600.0000, 
raw observation next is [-11.0, 72.0, 113.6666666666667, 663.6666666666667, 26.0, 26.10218855146347, 0.4645319525927477, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.15789473684210528, 0.72, 0.378888888888889, 0.7333333333333334, 0.6666666666666666, 0.6751823792886226, 0.6548439841975826, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8125892], dtype=float32), -0.9460524]. 
=============================================
[2019-04-04 06:13:47,888] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.7756415e-09 2.9993039e-09 1.2794736e-22 7.0400080e-10 2.4718516e-09
 4.0725986e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:13:47,888] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1200
[2019-04-04 06:13:47,920] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.97989800314202, 0.252424487895651, 0.0, 1.0, 41599.9774153428], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2599200.0000, 
sim time next is 2599800.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.96495603887083, 0.2425397326109028, 0.0, 1.0, 41596.16613187322], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.580413003239236, 0.5808465775369677, 0.0, 1.0, 0.19807698158034867], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.07979149], dtype=float32), 0.9305]. 
=============================================
[2019-04-04 06:13:51,705] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3463186e-09 1.7366091e-09 7.3449853e-24 1.1648212e-10 1.2662652e-09
 1.9092655e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:13:51,705] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3957
[2019-04-04 06:13:51,763] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.88257074650033, 0.2227773525614618, 0.0, 1.0, 41674.07414499095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601000.0000, 
sim time next is 2601600.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.81874491471099, 0.2215162594780667, 0.0, 1.0, 41722.92493415414], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5682287428925825, 0.5738387531593555, 0.0, 1.0, 0.19868059492454354], 
reward next is 0.8013, 
noisyNet noise sample is [array([1.5595933], dtype=float32), -0.124302454]. 
=============================================
[2019-04-04 06:13:55,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2478269e-10 6.3498867e-10 3.4099305e-23 2.3889365e-10 8.3344875e-10
 3.0692137e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:13:55,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3568
[2019-04-04 06:13:55,983] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.88257074650033, 0.2227773525614618, 0.0, 1.0, 41674.07414499095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2601000.0000, 
sim time next is 2601600.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.81874491471099, 0.2215162594780667, 0.0, 1.0, 41722.92493415414], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5682287428925825, 0.5738387531593555, 0.0, 1.0, 0.19868059492454354], 
reward next is 0.8013, 
noisyNet noise sample is [array([-0.5259254], dtype=float32), -0.915978]. 
=============================================
[2019-04-04 06:14:13,507] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2064042e-09 1.4375071e-09 3.7604797e-23 3.6372641e-10 1.0113528e-09
 4.0032921e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:13,510] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3943
[2019-04-04 06:14:13,532] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 50.0, 0.0, 0.0, 26.0, 25.44397574300375, 0.3653150224051646, 0.0, 1.0, 60109.35093758661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2845200.0000, 
sim time next is 2845800.0000, 
raw observation next is [2.0, 53.0, 0.0, 0.0, 26.0, 25.33248884828018, 0.3597224417864493, 0.0, 1.0, 87889.3319272514], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.53, 0.0, 0.0, 0.6666666666666666, 0.6110407373566819, 0.6199074805954831, 0.0, 1.0, 0.4185206282250067], 
reward next is 0.5815, 
noisyNet noise sample is [array([0.23422256], dtype=float32), -0.7410564]. 
=============================================
[2019-04-04 06:14:24,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6846038e-12 7.5393717e-13 4.4599124e-29 1.5267372e-13 1.5375968e-13
 5.8876261e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:24,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8638
[2019-04-04 06:14:24,153] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.666666666666666, 95.33333333333334, 113.8333333333333, 808.0, 26.0, 27.1543198533792, 0.7877491987856852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3152400.0000, 
sim time next is 3153000.0000, 
raw observation next is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 26.0, 27.1658024320778, 0.8023468797008455, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6795937211449677, 0.9416666666666665, 0.378888888888889, 0.8961325966850828, 0.6666666666666666, 0.7638168693398167, 0.7674489599002818, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86693066], dtype=float32), -1.2607496]. 
=============================================
[2019-04-04 06:14:24,178] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[101.56247]
 [101.50661]
 [101.42236]
 [101.35726]
 [101.29473]], R is [[101.6084137 ]
 [101.59233093]
 [101.57640839]
 [101.56064606]
 [101.54504395]].
[2019-04-04 06:14:26,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5551612e-09 6.7838946e-10 4.5637595e-22 1.0770457e-09 2.8815528e-10
 2.2021497e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:26,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6192
[2019-04-04 06:14:26,137] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 26.0, 25.0505408531936, 0.3961856875255278, 0.0, 1.0, 30595.81056794603], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2983200.0000, 
sim time next is 2983800.0000, 
raw observation next is [-3.0, 65.0, 157.3333333333333, 727.3333333333334, 26.0, 25.06000844448869, 0.3996871104254309, 0.0, 1.0, 27291.73332690532], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.5244444444444443, 0.8036832412523021, 0.6666666666666666, 0.5883340370407243, 0.633229036808477, 0.0, 1.0, 0.12996063489002532], 
reward next is 0.8700, 
noisyNet noise sample is [array([-2.1526623], dtype=float32), 0.9476607]. 
=============================================
[2019-04-04 06:14:33,689] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.6291908e-10 1.2812837e-10 2.0419876e-24 4.4519641e-11 1.8001917e-10
 2.6673458e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:33,690] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2661
[2019-04-04 06:14:33,713] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.30526344330958, 0.3231263991870789, 0.0, 1.0, 40246.05916105882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3109800.0000, 
sim time next is 3110400.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.29962031211526, 0.3234255689822552, 0.0, 1.0, 39888.30023972627], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6083016926762715, 0.607808522994085, 0.0, 1.0, 0.18994428685583936], 
reward next is 0.8101, 
noisyNet noise sample is [array([0.11440777], dtype=float32), 0.10143283]. 
=============================================
[2019-04-04 06:14:34,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.89069635e-11 3.92503217e-11 2.46203381e-26 1.06749158e-11
 1.38651556e-11 5.55506840e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:14:34,013] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8950
[2019-04-04 06:14:34,073] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 80.5, 86.0, 396.0, 26.0, 25.72915608320453, 0.4591446878367839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3313800.0000, 
sim time next is 3314400.0000, 
raw observation next is [-9.666666666666668, 79.33333333333333, 89.0, 432.5, 26.0, 25.8559589188165, 0.4721827779149445, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.19482917820867957, 0.7933333333333333, 0.2966666666666667, 0.47790055248618785, 0.6666666666666666, 0.6546632432347085, 0.6573942593049815, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32778966], dtype=float32), -0.19462697]. 
=============================================
[2019-04-04 06:14:41,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2259287e-10 3.6582828e-10 8.0786476e-25 1.2940501e-10 5.6722741e-11
 9.9389572e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:41,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3435
[2019-04-04 06:14:41,592] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 75.0, 0.0, 0.0, 26.0, 25.0942852049638, 0.4514655133905939, 1.0, 1.0, 92187.44333351948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3436800.0000, 
sim time next is 3437400.0000, 
raw observation next is [1.166666666666667, 77.0, 0.0, 0.0, 26.0, 25.17683935427649, 0.4678899958224105, 1.0, 1.0, 18682.28021202432], 
processed observation next is [1.0, 0.782608695652174, 0.49492151431209613, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5980699461897073, 0.6559633319408035, 1.0, 1.0, 0.08896323910487772], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.6991629], dtype=float32), 0.66735256]. 
=============================================
[2019-04-04 06:14:44,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7650202e-09 2.8124953e-09 2.3321778e-21 3.6581085e-10 1.1038117e-09
 1.6211400e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:44,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8004
[2019-04-04 06:14:44,983] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.266666666666667, 76.66666666666667, 0.0, 0.0, 26.0, 24.65952507643939, 0.260494182923626, 0.0, 1.0, 43749.08681259878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3298800.0000, 
sim time next is 3299400.0000, 
raw observation next is [-9.45, 76.5, 0.0, 0.0, 26.0, 24.63948184146096, 0.2440155656966474, 0.0, 1.0, 43757.01175742861], 
processed observation next is [1.0, 0.17391304347826086, 0.20083102493074795, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5532901534550799, 0.5813385218988825, 0.0, 1.0, 0.20836672265442197], 
reward next is 0.7916, 
noisyNet noise sample is [array([0.27118224], dtype=float32), 1.7374257]. 
=============================================
[2019-04-04 06:14:47,229] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6728638e-09 4.4569962e-10 3.5553949e-24 2.0603350e-10 3.8068212e-10
 2.2963673e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:47,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3631
[2019-04-04 06:14:47,258] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 71.16666666666667, 0.0, 0.0, 26.0, 25.38546513035485, 0.4384441653618968, 0.0, 1.0, 36078.03345453968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3467400.0000, 
sim time next is 3468000.0000, 
raw observation next is [1.0, 70.33333333333334, 0.0, 0.0, 26.0, 25.45071589423288, 0.433445522178746, 0.0, 1.0, 18762.80190415776], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.6208929911860732, 0.6444818407262486, 0.0, 1.0, 0.08934667573408457], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.12660138], dtype=float32), 1.3050205]. 
=============================================
[2019-04-04 06:14:47,275] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.87722]
 [80.9415 ]
 [80.97586]
 [80.9073 ]
 [80.92291]], R is [[80.74641418]
 [80.76715088]
 [80.73182678]
 [80.66075134]
 [80.63833618]].
[2019-04-04 06:14:52,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9863080e-10 8.9720775e-10 2.3555911e-24 2.9188912e-10 3.6976727e-10
 6.7682849e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:52,959] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3218
[2019-04-04 06:14:52,993] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.26774394784553, 0.3860617360472606, 0.0, 1.0, 41906.98576001008], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3476400.0000, 
sim time next is 3477000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.24873969859949, 0.388650353272429, 0.0, 1.0, 41890.30296305628], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6040616415499575, 0.6295501177574764, 0.0, 1.0, 0.19947763315741088], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.6190883], dtype=float32), 0.76632106]. 
=============================================
[2019-04-04 06:14:53,015] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.42096 ]
 [82.499306]
 [82.59546 ]
 [82.71408 ]
 [82.84131 ]], R is [[82.32271576]
 [82.29993439]
 [82.27729034]
 [82.25487518]
 [82.23275757]].
[2019-04-04 06:14:53,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7622417e-09 1.0666744e-09 5.8818357e-24 1.6016644e-10 4.0364645e-10
 3.2608901e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:53,270] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5840
[2019-04-04 06:14:53,304] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 69.5, 0.0, 0.0, 26.0, 25.44518112023595, 0.4398324453751692, 0.0, 1.0, 18760.04013735847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3468600.0000, 
sim time next is 3469200.0000, 
raw observation next is [1.0, 68.66666666666667, 0.0, 0.0, 26.0, 25.52394474885376, 0.4346235011846069, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6269953957378135, 0.6448745003948689, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7952178], dtype=float32), 1.1052128]. 
=============================================
[2019-04-04 06:14:58,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.9260505e-11 2.2177088e-11 3.8202885e-27 1.2627378e-11 4.2455504e-11
 3.1221018e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:58,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2500
[2019-04-04 06:14:58,758] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 60.66666666666667, 110.0, 776.6666666666667, 26.0, 26.22540323224104, 0.5938540509898603, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3494400.0000, 
sim time next is 3495000.0000, 
raw observation next is [0.8333333333333334, 60.83333333333334, 111.0, 783.3333333333333, 26.0, 26.27458422646531, 0.5802662089988812, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4856879039704525, 0.6083333333333334, 0.37, 0.865561694290976, 0.6666666666666666, 0.6895486855387759, 0.6934220696662937, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.863571], dtype=float32), -0.77478725]. 
=============================================
[2019-04-04 06:14:58,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[92.84695 ]
 [93.00097 ]
 [93.17095 ]
 [93.35748 ]
 [93.562706]], R is [[92.87921906]
 [92.95042419]
 [93.0209198 ]
 [93.0907135 ]
 [93.1598053 ]].
[2019-04-04 06:14:59,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7177648e-10 6.2468489e-11 3.8814747e-25 2.9847600e-11 1.2207174e-10
 8.5115718e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:14:59,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3040
[2019-04-04 06:14:59,769] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333333, 49.0, 83.16666666666666, 674.5, 26.0, 25.51167794990573, 0.484662549734836, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3685200.0000, 
sim time next is 3685800.0000, 
raw observation next is [5.166666666666667, 49.5, 79.33333333333334, 644.0, 26.0, 25.50283690478212, 0.4811639163295369, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6057248384118191, 0.495, 0.2644444444444445, 0.7116022099447514, 0.6666666666666666, 0.6252364087318435, 0.6603879721098457, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09483574], dtype=float32), -2.9462023]. 
=============================================
[2019-04-04 06:15:00,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0945416e-10 9.4418517e-10 2.8572761e-24 6.8389634e-11 3.6014361e-10
 2.4896578e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:15:00,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1168
[2019-04-04 06:15:00,750] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.20243645694078, 0.5529562488359516, 0.0, 1.0, 99478.85531621941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3530400.0000, 
sim time next is 3531000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.36805573151265, 0.5902443239171239, 0.0, 1.0, 62022.36599482335], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.614004644292721, 0.6967481079723746, 0.0, 1.0, 0.2953445999753493], 
reward next is 0.7047, 
noisyNet noise sample is [array([0.97458494], dtype=float32), 0.95903593]. 
=============================================
[2019-04-04 06:15:00,759] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.31334 ]
 [81.54211 ]
 [80.51868 ]
 [79.482544]
 [79.344505]], R is [[82.61070251]
 [82.3108902 ]
 [81.64893341]
 [80.8865509 ]
 [80.77227783]].
[2019-04-04 06:15:03,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8650243e-09 2.1441766e-09 1.1055448e-22 1.3901766e-09 4.6947574e-10
 9.3284963e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:15:03,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1606
[2019-04-04 06:15:03,283] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 44.66666666666667, 0.0, 0.0, 26.0, 25.44716478977913, 0.3926915560734632, 0.0, 1.0, 57170.96803896352], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3619200.0000, 
sim time next is 3619800.0000, 
raw observation next is [-1.5, 46.0, 0.0, 0.0, 26.0, 25.41822427316493, 0.3913601890567307, 0.0, 1.0, 59799.85087491576], 
processed observation next is [0.0, 0.9130434782608695, 0.4210526315789474, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6181853560970776, 0.6304533963522435, 0.0, 1.0, 0.28476119464245603], 
reward next is 0.7152, 
noisyNet noise sample is [array([-0.7228325], dtype=float32), -0.64095354]. 
=============================================
[2019-04-04 06:15:18,025] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1438016e-10 5.9996147e-10 1.2261745e-23 2.6762376e-10 2.2926247e-10
 1.1563324e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:15:18,025] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2585
[2019-04-04 06:15:18,041] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 68.0, 0.0, 0.0, 26.0, 25.36344336069509, 0.3985560841286345, 0.0, 1.0, 44929.17412744511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3900600.0000, 
sim time next is 3901200.0000, 
raw observation next is [-2.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.39165496539485, 0.3927933686152232, 0.0, 1.0, 30903.77033816755], 
processed observation next is [1.0, 0.13043478260869565, 0.38873499538319484, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6159712471162374, 0.630931122871741, 0.0, 1.0, 0.14716081113413118], 
reward next is 0.8528, 
noisyNet noise sample is [array([1.1812699], dtype=float32), 3.175402]. 
=============================================
[2019-04-04 06:15:23,278] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5784676e-09 2.3520443e-09 5.7072736e-22 7.8339030e-10 2.2408717e-09
 6.7134271e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:15:23,280] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3403
[2019-04-04 06:15:23,338] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.53200520662247, 0.4896538226577514, 0.0, 1.0, 26822.18592002568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4045800.0000, 
sim time next is 4046400.0000, 
raw observation next is [-4.0, 31.0, 0.0, 0.0, 26.0, 25.49097035136522, 0.4878509789893246, 0.0, 1.0, 26820.91652165841], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.31, 0.0, 0.0, 0.6666666666666666, 0.624247529280435, 0.6626169929964415, 0.0, 1.0, 0.1277186501031353], 
reward next is 0.8723, 
noisyNet noise sample is [array([2.9715798], dtype=float32), 2.1437445]. 
=============================================
[2019-04-04 06:15:29,448] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1584225e-09 1.4517145e-08 1.3450938e-21 1.8904542e-09 4.0513530e-09
 3.1225651e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:15:29,449] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9283
[2019-04-04 06:15:29,465] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 30.66666666666667, 0.0, 0.0, 26.0, 25.4833671865053, 0.4828031997091805, 0.0, 1.0, 74903.6534743932], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053000.0000, 
sim time next is 4053600.0000, 
raw observation next is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.46098558328369, 0.4853110379540038, 0.0, 1.0, 64010.81647600716], 
processed observation next is [1.0, 0.9565217391304348, 0.32409972299168976, 0.31, 0.0, 0.0, 0.6666666666666666, 0.6217487986069742, 0.6617703459846679, 0.0, 1.0, 0.3048134117905103], 
reward next is 0.6952, 
noisyNet noise sample is [array([-0.8696672], dtype=float32), 1.2380463]. 
=============================================
[2019-04-04 06:15:35,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.03996506e-10 3.33155253e-10 4.41774236e-25 1.48185041e-11
 3.43657151e-11 2.02080513e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:15:35,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5417
[2019-04-04 06:15:35,235] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.32077125428365, 0.4651063584534199, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4476000.0000, 
sim time next is 4476600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.18363646504341, 0.4430645664953452, 1.0, 1.0, 23885.29897833113], 
processed observation next is [1.0, 0.8260869565217391, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5986363720869509, 0.6476881888317817, 1.0, 1.0, 0.11373951894443396], 
reward next is 0.8863, 
noisyNet noise sample is [array([-0.7467729], dtype=float32), -0.4532502]. 
=============================================
[2019-04-04 06:15:35,957] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.26382937e-10 2.64368638e-10 3.86097555e-26 5.81195161e-11
 4.10437212e-10 1.00641304e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:15:35,957] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4721
[2019-04-04 06:15:35,978] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.483333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 25.47330029517646, 0.4013512215522402, 0.0, 1.0, 66210.74880853322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4319400.0000, 
sim time next is 4320000.0000, 
raw observation next is [4.5, 76.0, 0.0, 0.0, 26.0, 25.45209239294702, 0.4106686142318267, 0.0, 1.0, 59693.63946094541], 
processed observation next is [1.0, 0.0, 0.5872576177285319, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6210076994122516, 0.6368895380772756, 0.0, 1.0, 0.28425542600450193], 
reward next is 0.7157, 
noisyNet noise sample is [array([-0.09323642], dtype=float32), -0.051633388]. 
=============================================
[2019-04-04 06:15:36,013] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[88.59685]
 [88.45762]
 [88.35419]
 [88.37494]
 [88.49722]], R is [[88.75172424]
 [88.54891205]
 [88.38468933]
 [88.3370285 ]
 [88.36443329]].
[2019-04-04 06:15:36,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1842664e-11 5.5974499e-11 5.9851046e-27 7.5862268e-12 6.3011380e-12
 6.9313377e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:15:36,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8702
[2019-04-04 06:15:36,726] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 26.0, 25.72005139444486, 0.5493199266150012, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4414200.0000, 
sim time next is 4414800.0000, 
raw observation next is [5.733333333333334, 66.33333333333334, 0.0, 0.0, 26.0, 25.67476880745327, 0.54849118361564, 0.0, 1.0, 137587.3042231172], 
processed observation next is [1.0, 0.08695652173913043, 0.6214219759926132, 0.6633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6395640672877724, 0.6828303945385467, 0.0, 1.0, 0.6551776391577009], 
reward next is 0.3448, 
noisyNet noise sample is [array([0.50676894], dtype=float32), 1.4542332]. 
=============================================
[2019-04-04 06:15:43,982] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5473779e-11 6.2908380e-11 3.1394312e-25 1.0084681e-11 4.6747909e-12
 1.5230173e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:15:43,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8255
[2019-04-04 06:15:44,008] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 52.66666666666666, 0.0, 26.0, 26.28517520741781, 0.6023588582923595, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4465200.0000, 
sim time next is 4465800.0000, 
raw observation next is [0.0, 78.0, 49.0, 0.0, 26.0, 26.26838611189486, 0.4917911544453467, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.46260387811634357, 0.78, 0.16333333333333333, 0.0, 0.6666666666666666, 0.6890321759912382, 0.6639303848151156, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32114318], dtype=float32), 1.5707175]. 
=============================================
[2019-04-04 06:15:51,173] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.29902988e-11 4.99494890e-11 1.00499314e-25 2.05236418e-11
 1.53912699e-11 3.19106619e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:15:51,175] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5382
[2019-04-04 06:15:51,211] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 80.33333333333333, 0.0, 26.0, 26.17418300560234, 0.5224846795107138, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4697400.0000, 
sim time next is 4698000.0000, 
raw observation next is [0.0, 92.0, 89.0, 0.0, 26.0, 26.14700200139416, 0.5245164217494978, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.2966666666666667, 0.0, 0.6666666666666666, 0.6789168334495134, 0.6748388072498326, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2059878], dtype=float32), -0.45751122]. 
=============================================
[2019-04-04 06:15:51,221] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[92.01542 ]
 [91.28805 ]
 [90.50389 ]
 [89.96872 ]
 [89.074394]], R is [[92.53222656]
 [92.60690308]
 [92.68083191]
 [92.75402069]
 [92.82648468]].
[2019-04-04 06:16:06,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8226360e-09 5.1010418e-10 2.5435473e-22 7.1263773e-10 4.3262263e-10
 3.4634637e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:06,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5831
[2019-04-04 06:16:06,105] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 41.5, 0.0, 0.0, 26.0, 25.01396681165377, 0.3457997860858857, 0.0, 1.0, 18996.50026208359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4818600.0000, 
sim time next is 4819200.0000, 
raw observation next is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.02042626856862, 0.339442212343933, 0.0, 1.0, 19654.07437361286], 
processed observation next is [0.0, 0.782608695652174, 0.4995383194829178, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5850355223807183, 0.6131474041146444, 0.0, 1.0, 0.09359083035053743], 
reward next is 0.9064, 
noisyNet noise sample is [array([-0.11050157], dtype=float32), -0.36088055]. 
=============================================
[2019-04-04 06:16:11,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:11,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:11,206] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run9
[2019-04-04 06:16:13,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:13,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:13,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run9
[2019-04-04 06:16:14,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:14,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:14,940] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run9
[2019-04-04 06:16:15,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5779720e-10 3.7684224e-11 1.3667959e-23 2.8821228e-11 6.4496089e-11
 1.1193586e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:15,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2781
[2019-04-04 06:16:15,030] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 44.16666666666667, 260.0, 383.3333333333333, 26.0, 25.1043111096241, 0.3695480848467281, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4888200.0000, 
sim time next is 4888800.0000, 
raw observation next is [2.0, 44.0, 254.0, 381.0, 26.0, 25.09817154645964, 0.3674311653006325, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.44, 0.8466666666666667, 0.42099447513812155, 0.6666666666666666, 0.5915142955383033, 0.6224770551002109, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31025922], dtype=float32), -0.4030678]. 
=============================================
[2019-04-04 06:16:15,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:15,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:15,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run9
[2019-04-04 06:16:19,128] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:19,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:19,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run9
[2019-04-04 06:16:20,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:20,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:20,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run9
[2019-04-04 06:16:22,862] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:22,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:22,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run9
[2019-04-04 06:16:26,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:26,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:26,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run9
[2019-04-04 06:16:26,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:26,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:26,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run9
[2019-04-04 06:16:27,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:27,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:27,088] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run9
[2019-04-04 06:16:27,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:27,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:27,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run9
[2019-04-04 06:16:27,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:27,829] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:27,833] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4877127e-11 1.9999592e-11 1.1072893e-27 8.4939635e-13 3.4107693e-12
 3.1880365e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:27,833] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8323
[2019-04-04 06:16:27,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run9
[2019-04-04 06:16:27,877] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.333333333333333, 37.66666666666667, 117.1666666666667, 815.0, 26.0, 27.32139880478702, 0.8079522116383151, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5049600.0000, 
sim time next is 5050200.0000, 
raw observation next is [4.666666666666667, 36.83333333333333, 118.3333333333333, 821.0, 26.0, 27.40081201702216, 0.5883367339711695, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5918744228993538, 0.3683333333333333, 0.3944444444444443, 0.907182320441989, 0.6666666666666666, 0.7834010014185134, 0.6961122446570566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6158626], dtype=float32), 0.06285429]. 
=============================================
[2019-04-04 06:16:28,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:28,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:28,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run9
[2019-04-04 06:16:29,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:29,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:29,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run9
[2019-04-04 06:16:31,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3397328e-09 9.1885938e-10 1.0346513e-24 3.2497932e-10 2.0443247e-10
 2.0983971e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:31,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6934
[2019-04-04 06:16:31,191] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 21.0, 0.0, 26.0, 21.9191293035185, -0.3492143463773412, 0.0, 1.0, 100090.5945139615], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 30600.0000, 
sim time next is 31200.0000, 
raw observation next is [7.699999999999999, 93.0, 23.83333333333333, 0.0, 26.0, 22.31207022914705, -0.294830626905519, 0.0, 1.0, 74397.33781352831], 
processed observation next is [0.0, 0.34782608695652173, 0.6759002770083103, 0.93, 0.07944444444444443, 0.0, 0.6666666666666666, 0.35933918576225415, 0.401723124364827, 0.0, 1.0, 0.3542730372072777], 
reward next is 0.6457, 
noisyNet noise sample is [array([1.3104964], dtype=float32), -1.6397953]. 
=============================================
[2019-04-04 06:16:31,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:31,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:31,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run9
[2019-04-04 06:16:31,503] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:16:31,503] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:16:31,533] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run9
[2019-04-04 06:16:38,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4703481e-09 9.4499775e-10 4.7555164e-26 9.0999701e-11 5.5579808e-11
 2.4512063e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:38,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3584
[2019-04-04 06:16:38,542] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 56.33333333333333, 0.0, 26.0, 23.65213702935432, -0.03418555664812662, 0.0, 1.0, 56992.18281250409], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 37200.0000, 
sim time next is 37800.0000, 
raw observation next is [7.7, 93.0, 60.0, 0.0, 26.0, 23.77112192162978, -0.006217185508436834, 0.0, 1.0, 56883.28061114538], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.2, 0.0, 0.6666666666666666, 0.4809268268024818, 0.49792760483052106, 0.0, 1.0, 0.270872764814978], 
reward next is 0.7291, 
noisyNet noise sample is [array([-0.3637776], dtype=float32), -1.6330795]. 
=============================================
[2019-04-04 06:16:40,701] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8979682e-10 3.3640540e-10 6.2246702e-25 6.0050839e-11 4.4481593e-12
 1.3172624e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:40,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6431
[2019-04-04 06:16:40,758] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 75.0, 101.5, 0.0, 26.0, 25.3365173608052, 0.2127622456632948, 1.0, 1.0, 18734.92729576059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 208800.0000, 
sim time next is 209400.0000, 
raw observation next is [-7.116666666666666, 74.5, 109.0, 0.0, 26.0, 25.33514379932834, 0.2169339185402063, 1.0, 1.0, 19547.34997078387], 
processed observation next is [1.0, 0.43478260869565216, 0.265466297322253, 0.745, 0.36333333333333334, 0.0, 0.6666666666666666, 0.6112619832773616, 0.5723113061800688, 1.0, 1.0, 0.09308261890849462], 
reward next is 0.9069, 
noisyNet noise sample is [array([0.4194228], dtype=float32), -0.8351884]. 
=============================================
[2019-04-04 06:16:47,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6302704e-09 7.1909776e-09 1.6626618e-21 3.9433812e-10 9.7105146e-10
 1.6571795e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:47,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8114
[2019-04-04 06:16:47,891] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.00626890781786, 0.03472972263974324, 0.0, 1.0, 45501.41067808626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 266400.0000, 
sim time next is 267000.0000, 
raw observation next is [-7.566666666666666, 70.33333333333334, 0.0, 0.0, 26.0, 23.95393227425037, 0.02301151696347806, 0.0, 1.0, 45548.58033049418], 
processed observation next is [1.0, 0.08695652173913043, 0.2530009233610342, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.4961610228541975, 0.5076705056544927, 0.0, 1.0, 0.2168980015737818], 
reward next is 0.7831, 
noisyNet noise sample is [array([0.41467953], dtype=float32), -0.6746572]. 
=============================================
[2019-04-04 06:16:47,895] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.235954]
 [71.44792 ]
 [71.59411 ]
 [71.82868 ]
 [72.00724 ]], R is [[71.14165497]
 [71.21356201]
 [71.28492737]
 [71.35582733]
 [71.42642975]].
[2019-04-04 06:16:56,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0191997e-08 1.2732141e-08 1.9240990e-21 5.2537619e-09 3.1404450e-09
 3.0377814e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:16:56,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3823
[2019-04-04 06:16:56,984] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.7, 81.0, 12.5, 263.5, 26.0, 23.34937087830939, -0.1171762221104732, 0.0, 1.0, 96285.22724192735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 374400.0000, 
sim time next is 375000.0000, 
raw observation next is [-16.51666666666667, 82.5, 16.66666666666667, 334.6666666666667, 26.0, 23.65095690174391, -0.07171929955998688, 1.0, 1.0, 89010.66764446924], 
processed observation next is [1.0, 0.34782608695652173, 0.0050784856879038795, 0.825, 0.05555555555555557, 0.3697974217311234, 0.6666666666666666, 0.4709130751453259, 0.4760935668133377, 1.0, 1.0, 0.4238603221165202], 
reward next is 0.5761, 
noisyNet noise sample is [array([0.1256623], dtype=float32), -0.9680322]. 
=============================================
[2019-04-04 06:16:57,052] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[73.1596  ]
 [74.61166 ]
 [71.60301 ]
 [68.567764]
 [64.89566 ]], R is [[75.22891235]
 [75.01811981]
 [74.71038055]
 [74.15711212]
 [73.44716644]].
[2019-04-04 06:16:58,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.7323272e-08 5.5788480e-08 7.4453422e-20 1.2302481e-08 6.8004153e-09
 4.0532519e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 06:16:58,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7691
[2019-04-04 06:16:58,694] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.34876555651363, -0.1232821143728805, 0.0, 1.0, 45747.42414785441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 439200.0000, 
sim time next is 439800.0000, 
raw observation next is [-11.1, 54.0, 0.0, 0.0, 26.0, 23.3322577143985, -0.1344919714723817, 0.0, 1.0, 45794.14543435257], 
processed observation next is [1.0, 0.08695652173913043, 0.1551246537396122, 0.54, 0.0, 0.0, 0.6666666666666666, 0.44435480953320844, 0.4551693428425394, 0.0, 1.0, 0.2180673592112027], 
reward next is 0.7819, 
noisyNet noise sample is [array([-1.5362289], dtype=float32), 0.07001099]. 
=============================================
[2019-04-04 06:17:07,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3033540e-10 1.5604312e-10 3.6800566e-24 2.3919082e-11 3.4151196e-11
 9.5402385e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:17:07,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4042
[2019-04-04 06:17:07,102] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.05, 78.0, 39.0, 738.0, 26.0, 25.38449185070468, 0.2460616842839414, 1.0, 1.0, 53266.67675694892], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 379800.0000, 
sim time next is 380400.0000, 
raw observation next is [-14.86666666666667, 74.0, 44.33333333333333, 736.5, 26.0, 25.47722726430327, 0.2783712597828721, 1.0, 1.0, 56243.41935077996], 
processed observation next is [1.0, 0.391304347826087, 0.05078485687903958, 0.74, 0.14777777777777776, 0.8138121546961326, 0.6666666666666666, 0.6231022720252725, 0.592790419927624, 1.0, 1.0, 0.2678258064322855], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.52085614], dtype=float32), 1.8002491]. 
=============================================
[2019-04-04 06:17:19,885] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 06:17:19,934] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:17:19,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:19,936] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:17:19,936] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:19,936] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:17:19,937] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:17:19,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run12
[2019-04-04 06:17:19,964] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run12
[2019-04-04 06:17:19,989] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run12
[2019-04-04 06:17:39,157] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2504083], dtype=float32), 0.036933914]
[2019-04-04 06:17:39,157] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.813716667333334, 72.02191273000001, 103.523217975, 0.0, 26.0, 25.10735833502542, 0.2012896885523248, 1.0, 1.0, 73026.49678675557]
[2019-04-04 06:17:39,157] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:17:39,159] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.9397260e-10 5.3010984e-10 8.7274404e-25 8.0199347e-11 3.7072186e-11
 2.8895836e-13 1.0000000e+00], sampled 0.07776340188534292
[2019-04-04 06:17:43,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2504083], dtype=float32), 0.036933914]
[2019-04-04 06:17:43,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-14.02368471666667, 74.27567063000001, 0.0, 0.0, 26.0, 21.88096079133118, -0.3420999321299035, 1.0, 1.0, 202261.6178443114]
[2019-04-04 06:17:43,113] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:17:43,115] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.9760023e-08 2.0511775e-08 4.0230373e-21 4.3067900e-09 2.5377878e-09
 2.6672825e-11 1.0000000e+00], sampled 0.1716000230791226
[2019-04-04 06:19:19,823] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2504083], dtype=float32), 0.036933914]
[2019-04-04 06:19:19,823] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.203175124333333, 74.57262239666666, 0.0, 0.0, 26.0, 25.06405422372708, 0.3293626987861422, 1.0, 1.0, 75073.40595768091]
[2019-04-04 06:19:19,823] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:19:19,824] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.3007857e-10 2.7360836e-10 4.0582185e-25 4.4620720e-11 3.1195768e-11
 1.4822164e-13 1.0000000e+00], sampled 0.8361585587756284
[2019-04-04 06:20:33,197] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2504083], dtype=float32), 0.036933914]
[2019-04-04 06:20:33,197] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [4.866666666666667, 82.33333333333334, 0.0, 0.0, 26.0, 26.21530079471417, 0.6451511357059624, 1.0, 1.0, 0.0]
[2019-04-04 06:20:33,197] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:20:33,198] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.5690735e-11 1.5098933e-11 2.7269999e-27 2.4457214e-12 1.8220076e-12
 4.9313818e-15 1.0000000e+00], sampled 0.9438523917018099
[2019-04-04 06:20:35,680] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 06:21:10,931] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 06:21:15,539] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 06:21:16,574] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1100000, evaluation results [1100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 06:21:43,875] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6228449e-09 1.6553718e-09 1.5061851e-23 4.9585958e-10 2.7168323e-10
 1.7988582e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:21:43,875] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3250
[2019-04-04 06:21:43,897] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 66.0, 0.0, 0.0, 26.0, 23.7367275379913, -0.01136690145368181, 0.0, 1.0, 44066.9610697463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 625200.0000, 
sim time next is 625800.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.71236043277385, -0.01237325820813897, 0.0, 1.0, 43985.3607398653], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.655, 0.0, 0.0, 0.6666666666666666, 0.4760300360644874, 0.495875580597287, 0.0, 1.0, 0.2094540987612633], 
reward next is 0.7905, 
noisyNet noise sample is [array([-0.5846107], dtype=float32), -0.9260892]. 
=============================================
[2019-04-04 06:21:46,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2010330e-11 3.6237732e-11 4.5357703e-27 1.7714229e-11 7.0229413e-12
 4.9829678e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:21:46,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3330
[2019-04-04 06:21:46,963] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.15, 67.0, 139.0, 68.0, 26.0, 25.87350514096686, 0.3330410024407361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 729000.0000, 
sim time next is 729600.0000, 
raw observation next is [-0.9666666666666668, 66.66666666666667, 129.8333333333333, 186.5, 26.0, 25.86998799532421, 0.3441106034221828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43582640812557716, 0.6666666666666667, 0.4327777777777776, 0.20607734806629835, 0.6666666666666666, 0.655832332943684, 0.6147035344740609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5482008], dtype=float32), -0.9065365]. 
=============================================
[2019-04-04 06:21:48,178] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7370357e-09 1.5325347e-09 4.3253805e-23 5.1986848e-10 2.7812760e-10
 2.5535739e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:21:48,179] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0063
[2019-04-04 06:21:48,239] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 60.0, 131.5, 74.5, 26.0, 24.91553889866832, 0.2350302764795781, 0.0, 1.0, 36966.42612162078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 655200.0000, 
sim time next is 655800.0000, 
raw observation next is [-1.1, 59.0, 114.6666666666667, 68.33333333333333, 26.0, 24.91819659166958, 0.2310973842868585, 0.0, 1.0, 34490.3687326334], 
processed observation next is [0.0, 0.6086956521739131, 0.4321329639889197, 0.59, 0.38222222222222235, 0.07550644567219153, 0.6666666666666666, 0.5765163826391317, 0.5770324614289528, 0.0, 1.0, 0.16423985110777808], 
reward next is 0.8358, 
noisyNet noise sample is [array([-0.81917745], dtype=float32), 0.4327957]. 
=============================================
[2019-04-04 06:21:51,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.01167671e-10 4.55438742e-10 1.80880489e-23 1.06025605e-10
 4.81993161e-11 3.38092157e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:21:51,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2129
[2019-04-04 06:21:51,294] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.00000000000001, 97.83333333333333, 62.16666666666667, 26.0, 24.90291613763326, 0.2274835242069253, 0.0, 1.0, 41806.6153819488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 656400.0000, 
sim time next is 657000.0000, 
raw observation next is [-0.8999999999999999, 57.0, 81.0, 56.0, 26.0, 24.88794536962607, 0.2247427680445503, 0.0, 1.0, 45935.82944490803], 
processed observation next is [0.0, 0.6086956521739131, 0.43767313019390586, 0.57, 0.27, 0.061878453038674036, 0.6666666666666666, 0.5739954474688392, 0.57491425601485, 0.0, 1.0, 0.2187420449757525], 
reward next is 0.7813, 
noisyNet noise sample is [array([-0.17666642], dtype=float32), -1.0387899]. 
=============================================
[2019-04-04 06:21:51,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.423485]
 [79.10811 ]
 [79.81409 ]
 [80.39664 ]
 [80.90137 ]], R is [[77.74880981]
 [77.77223969]
 [77.83052063]
 [77.87631226]
 [77.97220612]].
[2019-04-04 06:21:57,180] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0826376e-10 2.7527453e-10 8.1121851e-25 1.3092215e-10 5.5045243e-11
 4.3457598e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:21:57,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4781
[2019-04-04 06:21:57,304] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.18729244031784, 0.03180908812882842, 0.0, 1.0, 41683.46460290565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 711600.0000, 
sim time next is 712200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.24286685675699, 0.03671712284704738, 0.0, 1.0, 41705.4822498903], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5202389047297492, 0.5122390409490157, 0.0, 1.0, 0.19859753452328716], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.5973351], dtype=float32), -0.09019895]. 
=============================================
[2019-04-04 06:21:58,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2430075e-11 1.2280848e-11 6.3884598e-28 5.9107745e-12 6.3025892e-13
 3.0362243e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:21:58,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1918
[2019-04-04 06:21:58,679] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 52.99999999999999, 16.33333333333333, 26.0, 25.70856098890648, 0.3040692575263489, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 723000.0000, 
sim time next is 723600.0000, 
raw observation next is [-2.3, 76.0, 65.0, 24.5, 26.0, 25.78850462644036, 0.3082370364966315, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3988919667590028, 0.76, 0.21666666666666667, 0.02707182320441989, 0.6666666666666666, 0.6490420522033634, 0.6027456788322105, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2301672], dtype=float32), -1.6845338]. 
=============================================
[2019-04-04 06:22:00,078] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.13914431e-12 1.29739075e-11 6.00241008e-28 2.36737496e-12
 1.12275044e-12 6.38689047e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 06:22:00,078] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8443
[2019-04-04 06:22:00,109] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.15, 67.0, 139.0, 68.0, 26.0, 25.87350514096686, 0.3330410024407361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 729000.0000, 
sim time next is 729600.0000, 
raw observation next is [-0.9666666666666668, 66.66666666666667, 129.8333333333333, 186.5, 26.0, 25.86998799532421, 0.3441106034221828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43582640812557716, 0.6666666666666667, 0.4327777777777776, 0.20607734806629835, 0.6666666666666666, 0.655832332943684, 0.6147035344740609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0642287], dtype=float32), -2.8083892]. 
=============================================
[2019-04-04 06:22:01,914] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9145021e-09 3.4013805e-09 4.9678209e-23 4.6966026e-10 1.5149487e-09
 9.9612619e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:01,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2044
[2019-04-04 06:22:01,929] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.36226083923728, 0.1583777844700988, 0.0, 1.0, 41881.0435522901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 777600.0000, 
sim time next is 778200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.32542986994673, 0.1499976436240303, 0.0, 1.0, 41810.42354629549], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5271191558288942, 0.5499992145413434, 0.0, 1.0, 0.19909725498235947], 
reward next is 0.8009, 
noisyNet noise sample is [array([1.616662], dtype=float32), 0.5609667]. 
=============================================
[2019-04-04 06:22:12,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3907574e-11 2.8982747e-11 1.8154771e-29 5.8117157e-12 1.6371669e-12
 2.9035032e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:12,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4739
[2019-04-04 06:22:12,770] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 24.89638187908671, 0.4063829618091391, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1023600.0000, 
sim time next is 1024200.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.84753042757691, 0.4273175761513228, 0.0, 1.0, 196650.5451585569], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5706275356314091, 0.642439192050441, 0.0, 1.0, 0.9364311674216995], 
reward next is 0.0636, 
noisyNet noise sample is [array([-1.3103269], dtype=float32), 0.46472114]. 
=============================================
[2019-04-04 06:22:22,761] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5284450e-10 8.1425949e-11 3.6252646e-27 8.3825932e-12 3.0986137e-11
 2.9283095e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:22,762] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5315
[2019-04-04 06:22:22,770] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.26666666666667, 100.0, 24.33333333333333, 0.0, 26.0, 24.60753234425115, 0.4210896593107105, 0.0, 1.0, 24169.20280818559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1268400.0000, 
sim time next is 1269000.0000, 
raw observation next is [13.0, 100.0, 19.0, 0.0, 26.0, 24.60079649918382, 0.4214553394132321, 0.0, 1.0, 26533.60516128196], 
processed observation next is [0.0, 0.6956521739130435, 0.8227146814404434, 1.0, 0.06333333333333334, 0.0, 0.6666666666666666, 0.5500663749319848, 0.640485113137744, 0.0, 1.0, 0.12635050076800933], 
reward next is 0.8736, 
noisyNet noise sample is [array([-0.12350408], dtype=float32), 0.26799977]. 
=============================================
[2019-04-04 06:22:22,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.544235]
 [93.54051 ]
 [93.54878 ]
 [93.53967 ]
 [93.53653 ]], R is [[93.49291229]
 [93.44289398]
 [93.41472626]
 [93.39163208]
 [93.36876678]].
[2019-04-04 06:22:22,885] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6575452e-09 7.2750034e-10 2.5502777e-24 2.5591057e-10 3.0972622e-10
 9.1509719e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:22,885] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7715
[2019-04-04 06:22:22,892] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 78.0, 0.0, 0.0, 26.0, 24.19162849880782, 0.2903189847889261, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1209600.0000, 
sim time next is 1210200.0000, 
raw observation next is [16.1, 78.33333333333333, 0.0, 0.0, 26.0, 24.16406590526762, 0.285260985663863, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.7833333333333333, 0.0, 0.0, 0.6666666666666666, 0.5136721587723017, 0.5950869952212877, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7599016], dtype=float32), 1.4368631]. 
=============================================
[2019-04-04 06:22:27,747] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4036657e-10 6.2586436e-11 3.5583999e-28 1.0388742e-11 6.2410576e-12
 1.3496415e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:27,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1149
[2019-04-04 06:22:27,761] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.11666666666667, 80.5, 0.0, 0.0, 26.0, 25.62216666231978, 0.5970340078864028, 0.0, 1.0, 24752.07907255277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1062600.0000, 
sim time next is 1063200.0000, 
raw observation next is [12.93333333333334, 81.0, 0.0, 0.0, 26.0, 25.68458653687527, 0.622005800978403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.8208679593721148, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6403822114062724, 0.7073352669928009, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5060663], dtype=float32), 2.8561602]. 
=============================================
[2019-04-04 06:22:30,636] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0535188e-12 1.1982939e-11 6.1556889e-27 7.3198245e-13 2.3569087e-12
 1.4099517e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:30,640] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8955
[2019-04-04 06:22:30,670] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 26.0, 26.01352269324047, 0.5393020086416019, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.05053112176828, 0.5166125816028565, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1822222222222222, 0.0, 0.6666666666666666, 0.67087759348069, 0.6722041938676188, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5699819], dtype=float32), -0.7695885]. 
=============================================
[2019-04-04 06:22:30,685] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.56039 ]
 [86.69341 ]
 [86.86753 ]
 [87.071175]
 [87.30895 ]], R is [[86.54747772]
 [86.68200684]
 [86.81518555]
 [86.94703674]
 [87.07756805]].
[2019-04-04 06:22:31,343] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2452491e-08 1.4254202e-09 1.3494250e-23 4.7250587e-10 6.1293581e-10
 4.6457760e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:31,345] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2749
[2019-04-04 06:22:31,354] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 82.5, 0.0, 0.0, 26.0, 24.0049628987579, 0.2516670621751306, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1216200.0000, 
sim time next is 1216800.0000, 
raw observation next is [16.1, 83.0, 0.0, 0.0, 26.0, 23.98415991919266, 0.2472898991815484, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.9085872576177286, 0.83, 0.0, 0.0, 0.6666666666666666, 0.498679993266055, 0.5824299663938495, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5615917], dtype=float32), -0.56640893]. 
=============================================
[2019-04-04 06:22:33,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6108023e-11 1.2359805e-11 5.3022662e-27 3.9921135e-12 6.9999258e-12
 9.5010644e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:33,564] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3925
[2019-04-04 06:22:33,594] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.33894602099891, 0.5109045262903821, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1361400.0000, 
sim time next is 1362000.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.38783436414933, 0.4861293515624119, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6156528636791109, 0.6620431171874707, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34208897], dtype=float32), 1.060512]. 
=============================================
[2019-04-04 06:22:33,603] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.13418]
 [91.23538]
 [91.29329]
 [91.32593]
 [91.39221]], R is [[91.13371277]
 [91.22237396]
 [91.31015015]
 [91.39704895]
 [91.483078  ]].
[2019-04-04 06:22:35,581] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.16002104e-10 7.50470242e-11 1.41375152e-26 1.63939556e-11
 1.77649492e-11 2.96878407e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:22:35,582] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1839
[2019-04-04 06:22:35,618] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.9, 98.66666666666667, 0.0, 0.0, 26.0, 24.58417631255258, 0.4273595120439284, 0.0, 1.0, 45616.46136125735], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1272000.0000, 
sim time next is 1272600.0000, 
raw observation next is [10.25, 98.0, 0.0, 0.0, 26.0, 24.59008741360616, 0.4330186878471358, 0.0, 1.0, 45080.17197789974], 
processed observation next is [0.0, 0.7391304347826086, 0.7465373961218837, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5491739511338466, 0.6443395626157119, 0.0, 1.0, 0.2146674856090464], 
reward next is 0.7853, 
noisyNet noise sample is [array([1.1866056], dtype=float32), 1.2806323]. 
=============================================
[2019-04-04 06:22:39,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.02468194e-10 5.12213709e-11 2.33864872e-26 6.31446891e-12
 5.98886175e-12 3.49575478e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:22:39,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9572
[2019-04-04 06:22:39,871] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.7, 100.0, 37.33333333333334, 0.0, 26.0, 25.85638804365466, 0.4925092270645715, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1501800.0000, 
sim time next is 1502400.0000, 
raw observation next is [1.8, 100.0, 42.16666666666667, 0.0, 26.0, 25.86135983179641, 0.5054018843739424, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5124653739612189, 1.0, 0.14055555555555557, 0.0, 0.6666666666666666, 0.6551133193163675, 0.6684672947913142, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4507885], dtype=float32), 0.44765127]. 
=============================================
[2019-04-04 06:22:43,575] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7544724e-11 3.2278430e-11 7.2747564e-28 2.8361558e-12 2.1365310e-12
 4.2333966e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:43,581] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8011
[2019-04-04 06:22:43,614] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.383333333333333, 92.0, 37.66666666666667, 0.0, 26.0, 25.76732370583536, 0.5333769442031125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1673400.0000, 
sim time next is 1674000.0000, 
raw observation next is [2.2, 92.0, 41.5, 0.0, 26.0, 25.73568996198895, 0.5358315876153741, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5235457063711911, 0.92, 0.13833333333333334, 0.0, 0.6666666666666666, 0.6446408301657458, 0.6786105292051247, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4740744], dtype=float32), -0.24221992]. 
=============================================
[2019-04-04 06:22:43,628] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[92.88222 ]
 [93.31602 ]
 [93.78867 ]
 [94.242386]
 [94.591545]], R is [[92.52304077]
 [92.59780884]
 [92.67182922]
 [92.74510956]
 [92.81765747]].
[2019-04-04 06:22:51,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0362043e-09 2.8160434e-09 7.5287602e-22 6.6136518e-10 2.0542068e-09
 3.4437743e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:51,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6185
[2019-04-04 06:22:51,306] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.66666666666667, 105.6666666666667, 0.0, 26.0, 25.0435470227132, 0.3414249999664768, 0.0, 1.0, 27091.36919827135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1779000.0000, 
sim time next is 1779600.0000, 
raw observation next is [-2.8, 84.33333333333334, 102.3333333333333, 0.0, 26.0, 25.00035839273205, 0.3359900543139973, 0.0, 1.0, 58093.72349867324], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8433333333333334, 0.341111111111111, 0.0, 0.6666666666666666, 0.5833631993943375, 0.6119966847713324, 0.0, 1.0, 0.27663677856511065], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.20746584], dtype=float32), 0.22213718]. 
=============================================
[2019-04-04 06:22:57,481] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.7938016e-12 2.6047347e-11 1.5280003e-26 2.5887801e-12 9.4516398e-13
 2.7198806e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:22:57,482] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6154
[2019-04-04 06:22:57,524] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 86.0, 107.0, 0.0, 26.0, 25.72850842898086, 0.4836082315428227, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1686600.0000, 
sim time next is 1687200.0000, 
raw observation next is [1.1, 86.66666666666667, 105.8333333333333, 0.0, 26.0, 25.5962663802712, 0.3685088842205732, 1.0, 1.0, 94340.80365552373], 
processed observation next is [1.0, 0.5217391304347826, 0.49307479224376743, 0.8666666666666667, 0.3527777777777777, 0.0, 0.6666666666666666, 0.6330221983559333, 0.6228362947401911, 1.0, 1.0, 0.44924192216916065], 
reward next is 0.5508, 
noisyNet noise sample is [array([-1.3920962], dtype=float32), -0.71593034]. 
=============================================
[2019-04-04 06:23:02,678] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9149153e-09 5.7194072e-09 6.2951479e-22 8.5689433e-10 1.8847082e-09
 1.7122880e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:02,678] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1474
[2019-04-04 06:23:02,694] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 80.5, 0.0, 0.0, 26.0, 24.62126590546995, 0.2120908213666982, 0.0, 1.0, 45483.34585050339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1812600.0000, 
sim time next is 1813200.0000, 
raw observation next is [-5.0, 80.0, 0.0, 0.0, 26.0, 24.59074110280127, 0.2054997158288671, 0.0, 1.0, 45486.24304835903], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5492284252334393, 0.568499905276289, 0.0, 1.0, 0.21660115737313823], 
reward next is 0.7834, 
noisyNet noise sample is [array([-0.04388265], dtype=float32), 1.5604787]. 
=============================================
[2019-04-04 06:23:05,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7535699e-09 1.2207204e-09 3.3132265e-21 7.3545281e-10 5.7638444e-10
 1.9830021e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:05,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8606
[2019-04-04 06:23:05,266] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333333, 85.0, 0.0, 0.0, 26.0, 25.03765559589858, 0.248229133885771, 0.0, 1.0, 38938.43119323318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1878000.0000, 
sim time next is 1878600.0000, 
raw observation next is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.02548023568153, 0.2454515311377328, 0.0, 1.0, 50271.40686608154], 
processed observation next is [0.0, 0.7391304347826086, 0.32640812557710064, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5854566863067943, 0.581817177045911, 0.0, 1.0, 0.23938765174324542], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.4642994], dtype=float32), -0.3557672]. 
=============================================
[2019-04-04 06:23:06,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5979176e-09 5.7355973e-09 2.7411194e-21 9.8165120e-10 1.4022455e-09
 1.1860446e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:06,893] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2592
[2019-04-04 06:23:07,008] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 74.33333333333333, 162.6666666666667, 71.0, 26.0, 24.97006703325474, 0.2491906919688205, 0.0, 1.0, 40513.2663979592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1854600.0000, 
sim time next is 1855200.0000, 
raw observation next is [-5.4, 73.66666666666667, 173.3333333333333, 76.0, 26.0, 24.95655110288769, 0.2530117806990611, 0.0, 1.0, 51316.92368706244], 
processed observation next is [0.0, 0.4782608695652174, 0.31301939058171746, 0.7366666666666667, 0.5777777777777776, 0.08397790055248619, 0.6666666666666666, 0.5797125919073075, 0.5843372602330204, 0.0, 1.0, 0.2443663032717259], 
reward next is 0.7556, 
noisyNet noise sample is [array([-0.53811413], dtype=float32), -0.7954178]. 
=============================================
[2019-04-04 06:23:13,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7130728e-10 4.2169751e-10 1.0414032e-23 4.9790831e-11 2.9481976e-11
 2.7795694e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:13,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6370
[2019-04-04 06:23:13,833] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 79.0, 126.0, 0.0, 26.0, 26.29660625165349, 0.4902889877750398, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2037600.0000, 
sim time next is 2038200.0000, 
raw observation next is [-4.0, 80.16666666666667, 118.6666666666667, 0.0, 26.0, 26.35890798908308, 0.4908797680525374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.8016666666666667, 0.39555555555555566, 0.0, 0.6666666666666666, 0.6965756657569232, 0.6636265893508458, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3697564], dtype=float32), 0.9390992]. 
=============================================
[2019-04-04 06:23:15,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7585667e-11 2.0465213e-10 2.2588606e-24 1.8726522e-11 1.2375869e-11
 9.4488409e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:15,576] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0844
[2019-04-04 06:23:15,622] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.866666666666666, 85.0, 62.33333333333333, 0.0, 26.0, 25.54746188489154, 0.2797162900974745, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2020800.0000, 
sim time next is 2021400.0000, 
raw observation next is [-5.8, 84.5, 69.0, 0.0, 26.0, 25.50788034531727, 0.284191166954882, 1.0, 1.0, 18724.46385866081], 
processed observation next is [1.0, 0.391304347826087, 0.30193905817174516, 0.845, 0.23, 0.0, 0.6666666666666666, 0.625656695443106, 0.5947303889849607, 1.0, 1.0, 0.08916411361267051], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.7379894], dtype=float32), 1.3281674]. 
=============================================
[2019-04-04 06:23:35,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5303497e-09 6.1591279e-09 2.7921643e-23 2.7625863e-10 9.6564168e-10
 7.9638618e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:35,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4015
[2019-04-04 06:23:35,641] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 83.0, 0.0, 0.0, 26.0, 25.2418347339725, 0.4065072865468594, 0.0, 1.0, 42514.28222096964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2151600.0000, 
sim time next is 2152200.0000, 
raw observation next is [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.22880137705562, 0.402811064269501, 0.0, 1.0, 42264.40818521218], 
processed observation next is [1.0, 0.9130434782608695, 0.2820867959372115, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6024001147546351, 0.6342703547565004, 0.0, 1.0, 0.20125908659624847], 
reward next is 0.7987, 
noisyNet noise sample is [array([1.1494905], dtype=float32), -0.7876275]. 
=============================================
[2019-04-04 06:23:35,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0525213e-09 6.4892749e-09 2.9583092e-22 1.7874956e-09 1.6735724e-09
 1.5679957e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:35,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5357
[2019-04-04 06:23:35,981] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 24.30471993289186, 0.1492211344562627, 0.0, 1.0, 42594.85071370676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2166000.0000, 
sim time next is 2166600.0000, 
raw observation next is [-6.800000000000001, 78.16666666666666, 0.0, 0.0, 26.0, 24.27622062715998, 0.1490202452244833, 0.0, 1.0, 42612.04250523754], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.7816666666666666, 0.0, 0.0, 0.6666666666666666, 0.5230183855966649, 0.5496734150748278, 0.0, 1.0, 0.2029144881201788], 
reward next is 0.7971, 
noisyNet noise sample is [array([0.68106544], dtype=float32), -0.31095532]. 
=============================================
[2019-04-04 06:23:36,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8641091e-09 5.0461657e-09 7.1527379e-22 1.4138581e-09 3.6887777e-09
 2.5576014e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:36,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7597
[2019-04-04 06:23:36,294] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49793332647902, 0.1320579434254893, 0.0, 1.0, 42595.02810495335], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2172000.0000, 
sim time next is 2172600.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.37407286567034, 0.1177945325656538, 0.0, 1.0, 42391.93842068542], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5311727388058616, 0.5392648441885513, 0.0, 1.0, 0.20186637343183536], 
reward next is 0.7981, 
noisyNet noise sample is [array([0.48501664], dtype=float32), -1.0286193]. 
=============================================
[2019-04-04 06:23:39,560] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8383456e-11 7.4271406e-10 1.5198226e-24 1.5046359e-10 9.2486171e-11
 3.5127267e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:39,560] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7247
[2019-04-04 06:23:39,611] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.5, 0.0, 0.0, 26.0, 25.89063890896941, 0.513746972458774, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2139000.0000, 
sim time next is 2139600.0000, 
raw observation next is [-5.0, 72.0, 0.0, 0.0, 26.0, 26.08795524657295, 0.4952905593743821, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.32409972299168976, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6739962705477458, 0.665096853124794, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00850962], dtype=float32), -0.52704084]. 
=============================================
[2019-04-04 06:23:43,940] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0143389e-10 2.6250851e-10 4.4723246e-25 6.0320034e-11 2.7388130e-11
 2.1902844e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:43,943] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7657
[2019-04-04 06:23:43,971] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 53.33333333333334, 0.0, 0.0, 26.0, 25.36623556699928, 0.3645280505873267, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2313600.0000, 
sim time next is 2314200.0000, 
raw observation next is [-1.2, 53.66666666666666, 0.0, 0.0, 26.0, 25.28632601457305, 0.3478914433953901, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.5366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6071938345477541, 0.61596381446513, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7416898], dtype=float32), 0.6504392]. 
=============================================
[2019-04-04 06:23:56,171] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6017414e-09 3.3794585e-09 2.6425020e-21 6.5494049e-10 4.3663573e-10
 7.1002791e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:23:56,171] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6752
[2019-04-04 06:23:56,250] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 52.0, 175.5, 0.0, 26.0, 24.94660947976571, 0.3057619255377428, 0.0, 1.0, 19947.55140163242], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2383200.0000, 
sim time next is 2383800.0000, 
raw observation next is [0.0, 51.16666666666667, 170.3333333333333, 0.0, 26.0, 24.98782714792967, 0.3024683316372985, 0.0, 1.0, 18720.655907369], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.5116666666666667, 0.5677777777777776, 0.0, 0.6666666666666666, 0.5823189289941393, 0.6008227772124328, 0.0, 1.0, 0.08914598051128095], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.13786505], dtype=float32), -0.36226946]. 
=============================================
[2019-04-04 06:24:00,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.03950037e-09 6.26234842e-10 3.93240219e-24 8.76265116e-10
 1.00157924e-10 1.92524230e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 06:24:00,568] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0011
[2019-04-04 06:24:00,633] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 68.33333333333333, 93.0, 240.0, 26.0, 25.33185232970915, 0.3223504988455673, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2365800.0000, 
sim time next is 2366400.0000, 
raw observation next is [-3.2, 67.66666666666667, 107.0, 300.0, 26.0, 25.30114495614872, 0.3199781133364896, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37396121883656513, 0.6766666666666667, 0.3566666666666667, 0.3314917127071823, 0.6666666666666666, 0.6084287463457265, 0.6066593711121632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9908134], dtype=float32), -0.8390706]. 
=============================================
[2019-04-04 06:24:00,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5974304e-09 1.0627591e-09 3.2049817e-24 2.7038566e-10 2.5144184e-10
 6.5927727e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:00,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3895
[2019-04-04 06:24:00,973] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 67.0, 121.0, 360.0, 26.0, 25.26066344433154, 0.3198570493925914, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2367000.0000, 
sim time next is 2367600.0000, 
raw observation next is [-3.0, 66.33333333333333, 124.0, 375.0, 26.0, 25.21610060737213, 0.3127275280617152, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.6633333333333333, 0.41333333333333333, 0.4143646408839779, 0.6666666666666666, 0.6013417172810108, 0.6042425093539051, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8195119], dtype=float32), 0.4557509]. 
=============================================
[2019-04-04 06:24:03,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.7951208e-09 4.7295243e-09 7.2763051e-22 1.0559438e-09 3.0776759e-10
 1.7269384e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:03,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7006
[2019-04-04 06:24:03,202] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 26.0, 24.95061022506106, 0.2839013695780121, 0.0, 1.0, 39577.80530987131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2393400.0000, 
sim time next is 2394000.0000, 
raw observation next is [-0.6, 45.0, 42.5, 37.0, 26.0, 24.95703713100634, 0.2818529347973943, 0.0, 1.0, 34895.39414714964], 
processed observation next is [0.0, 0.7391304347826086, 0.44598337950138506, 0.45, 0.14166666666666666, 0.04088397790055249, 0.6666666666666666, 0.5797530942505285, 0.5939509782657981, 0.0, 1.0, 0.1661685435578554], 
reward next is 0.8338, 
noisyNet noise sample is [array([0.0167699], dtype=float32), -1.4019644]. 
=============================================
[2019-04-04 06:24:03,213] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.06511]
 [73.20248]
 [73.2885 ]
 [73.42915]
 [73.57544]], R is [[73.01675415]
 [73.09812164]
 [73.14971161]
 [73.1829834 ]
 [73.2250824 ]].
[2019-04-04 06:24:12,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1226959e-10 3.0534747e-10 3.7513223e-24 4.2272082e-11 1.5521139e-10
 1.0492047e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:12,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1297
[2019-04-04 06:24:12,265] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666667, 71.5, 0.0, 0.0, 26.0, 24.76123577649392, 0.2715041754710985, 0.0, 1.0, 44401.0738526901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2677800.0000, 
sim time next is 2678400.0000, 
raw observation next is [-7.0, 72.0, 0.0, 0.0, 26.0, 24.70056441055226, 0.2625007838339724, 0.0, 1.0, 44416.86833493737], 
processed observation next is [1.0, 0.0, 0.2686980609418283, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5583803675460217, 0.5875002612779908, 0.0, 1.0, 0.2115088968330351], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.09680644], dtype=float32), 0.045363195]. 
=============================================
[2019-04-04 06:24:13,198] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2943678e-11 2.3787732e-11 1.5841483e-26 7.4492652e-12 1.7110269e-12
 4.3579452e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:13,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7080
[2019-04-04 06:24:13,242] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 56.5, 0.0, 0.0, 26.0, 25.47670833166533, 0.4140213742233625, 1.0, 1.0, 52069.29269378883], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2745000.0000, 
sim time next is 2745600.0000, 
raw observation next is [-4.666666666666666, 57.33333333333333, 0.0, 0.0, 26.0, 25.25577807241628, 0.4008411493524076, 1.0, 1.0, 52969.37532562642], 
processed observation next is [1.0, 0.782608695652174, 0.33333333333333337, 0.5733333333333333, 0.0, 0.0, 0.6666666666666666, 0.6046481727013567, 0.6336137164508026, 1.0, 1.0, 0.252235120598221], 
reward next is 0.7478, 
noisyNet noise sample is [array([-0.06562483], dtype=float32), 1.5676857]. 
=============================================
[2019-04-04 06:24:14,481] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5509473e-10 2.1695123e-10 2.0942181e-25 7.2334354e-11 2.5974368e-11
 3.2568607e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:14,482] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0957
[2019-04-04 06:24:14,575] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.33333333333333, 88.33333333333334, 82.83333333333334, 377.0, 26.0, 25.91862088338338, 0.4092565918935074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2709600.0000, 
sim time next is 2710200.0000, 
raw observation next is [-14.16666666666667, 89.66666666666667, 85.66666666666667, 424.0, 26.0, 26.02374888291741, 0.4276603084897286, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.07017543859649114, 0.8966666666666667, 0.28555555555555556, 0.4685082872928177, 0.6666666666666666, 0.6686457402431175, 0.6425534361632429, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.67537504], dtype=float32), 0.33886296]. 
=============================================
[2019-04-04 06:24:18,148] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.2456472e-12 1.0008263e-11 5.4624482e-26 1.1210215e-12 2.3591485e-12
 1.8066583e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:18,150] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0883
[2019-04-04 06:24:18,212] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 58.5, 110.3333333333333, 791.6666666666666, 26.0, 25.80143069086825, 0.5242403938711852, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2725800.0000, 
sim time next is 2726400.0000, 
raw observation next is [-5.6, 58.0, 109.6666666666667, 789.8333333333334, 26.0, 25.99450057383382, 0.5529674977247893, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.30747922437673136, 0.58, 0.3655555555555557, 0.872744014732965, 0.6666666666666666, 0.6662083811528184, 0.6843224992415964, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6172426], dtype=float32), -2.9026039]. 
=============================================
[2019-04-04 06:24:25,750] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0315287e-10 3.7276196e-10 2.0172211e-23 7.6491147e-11 7.3504716e-11
 9.5688316e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:25,750] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0435
[2019-04-04 06:24:25,778] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.333333333333334, 62.33333333333334, 112.8333333333333, 796.0, 26.0, 25.94372402164388, 0.4666143291333089, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2722800.0000, 
sim time next is 2723400.0000, 
raw observation next is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.86484689482273, 0.3619529563967844, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2686980609418283, 0.615, 0.37666666666666665, 0.8828729281767956, 0.6666666666666666, 0.6554039079018942, 0.6206509854655948, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.564075], dtype=float32), 0.8096497]. 
=============================================
[2019-04-04 06:24:42,310] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9961783e-10 5.7560556e-10 7.8621873e-23 2.4835942e-10 1.9382061e-10
 1.0754593e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:42,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5404
[2019-04-04 06:24:42,335] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 104.0, 790.5, 26.0, 25.11654289567031, 0.3627777553716112, 0.0, 1.0, 18708.99029009449], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3074400.0000, 
sim time next is 3075000.0000, 
raw observation next is [-0.8333333333333334, 41.5, 102.3333333333333, 785.3333333333334, 26.0, 25.1196217096183, 0.3637640042112804, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.43951985226223456, 0.415, 0.341111111111111, 0.8677716390423573, 0.6666666666666666, 0.5933018091348584, 0.6212546680704268, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2378964], dtype=float32), 1.4679404]. 
=============================================
[2019-04-04 06:24:42,343] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.58666 ]
 [76.6257  ]
 [76.647156]
 [76.64438 ]
 [76.64542 ]], R is [[76.67672729]
 [76.82086945]
 [76.96356201]
 [77.10482025]
 [77.24465942]].
[2019-04-04 06:24:49,403] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.3423658e-09 1.8606425e-08 1.2062173e-21 2.3744420e-09 3.5196550e-09
 4.8564007e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:49,403] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4368
[2019-04-04 06:24:49,429] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.99791531570199, 0.2839178794289527, 0.0, 1.0, 38297.09375885328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3020400.0000, 
sim time next is 3021000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.96200863393279, 0.2769924552415536, 0.0, 1.0, 38224.2023017942], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5801673861610658, 0.5923308184138513, 0.0, 1.0, 0.18202001096092474], 
reward next is 0.8180, 
noisyNet noise sample is [array([-0.19672693], dtype=float32), 0.2522975]. 
=============================================
[2019-04-04 06:24:49,446] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[72.514305]
 [72.621956]
 [72.73578 ]
 [72.84501 ]
 [72.93632 ]], R is [[72.49669647]
 [72.5893631 ]
 [72.68074799]
 [72.77082825]
 [72.85961914]].
[2019-04-04 06:24:51,462] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8407111e-10 2.7171587e-10 1.9412066e-25 5.0999104e-11 1.0282628e-10
 1.6114276e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:51,462] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8019
[2019-04-04 06:24:51,486] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 100.0, 0.0, 0.0, 26.0, 25.73638474409691, 0.5956536323677283, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3201600.0000, 
sim time next is 3202200.0000, 
raw observation next is [0.5, 100.0, 0.0, 0.0, 26.0, 25.72348866011853, 0.5765989133904355, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4764542936288089, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6436240550098775, 0.6921996377968118, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.07054199], dtype=float32), 1.2568077]. 
=============================================
[2019-04-04 06:24:54,078] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8366075e-09 5.0217586e-09 8.4541592e-23 2.7805386e-10 3.0245392e-10
 7.1867946e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:24:54,084] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9000
[2019-04-04 06:24:54,114] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 79.33333333333333, 0.0, 0.0, 26.0, 25.51548771032388, 0.5943987216042588, 0.0, 1.0, 18749.84388307668], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3271800.0000, 
sim time next is 3272400.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 26.0, 25.66616883915915, 0.5950578020380705, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.32409972299168976, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6388474032632624, 0.6983526006793568, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51620454], dtype=float32), 0.46217164]. 
=============================================
[2019-04-04 06:25:02,125] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5555148e-08 7.4869098e-09 1.9778434e-22 8.0167273e-10 2.1732078e-09
 4.3781949e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:02,126] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5633
[2019-04-04 06:25:02,137] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 25.1750656163288, 0.3996792064539759, 0.0, 1.0, 42964.40326943688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3366600.0000, 
sim time next is 3367200.0000, 
raw observation next is [-5.333333333333334, 73.0, 0.0, 0.0, 26.0, 25.13921906281267, 0.3912398715698881, 0.0, 1.0, 42057.87736541917], 
processed observation next is [1.0, 1.0, 0.31486611265004616, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5949349219010559, 0.6304132905232961, 0.0, 1.0, 0.20027560650199605], 
reward next is 0.7997, 
noisyNet noise sample is [array([-0.1803041], dtype=float32), 0.059779976]. 
=============================================
[2019-04-04 06:25:05,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7720141e-10 2.4705271e-09 7.7472666e-23 8.6616658e-11 2.9874497e-10
 1.3636216e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:05,702] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1385
[2019-04-04 06:25:05,734] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.68692308643618, 0.5576067917790853, 0.0, 1.0, 43770.18539608092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3268800.0000, 
sim time next is 3269400.0000, 
raw observation next is [-4.166666666666667, 72.66666666666667, 0.0, 0.0, 26.0, 25.58792585365006, 0.5457565122142713, 0.0, 1.0, 42687.73738691747], 
processed observation next is [1.0, 0.8695652173913043, 0.3471837488457987, 0.7266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6323271544708383, 0.6819188374047571, 0.0, 1.0, 0.20327493993770224], 
reward next is 0.7967, 
noisyNet noise sample is [array([-0.7941221], dtype=float32), -0.8948985]. 
=============================================
[2019-04-04 06:25:07,061] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6327244e-09 6.6706649e-09 5.8403894e-22 4.7650345e-10 2.5534024e-09
 6.9291903e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:07,075] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6935
[2019-04-04 06:25:07,092] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.82582731358528, 0.2876868560023934, 0.0, 1.0, 41138.95715518847], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3378600.0000, 
sim time next is 3379200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.79770385879045, 0.2880369297648992, 0.0, 1.0, 41126.51101256716], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5664753215658708, 0.5960123099216331, 0.0, 1.0, 0.19584052863127221], 
reward next is 0.8042, 
noisyNet noise sample is [array([0.7476972], dtype=float32), 1.2342477]. 
=============================================
[2019-04-04 06:25:08,309] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4048361e-10 3.1905736e-10 3.1680156e-25 6.5633075e-11 1.6180262e-11
 3.0950591e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:08,311] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2129
[2019-04-04 06:25:08,374] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 26.0, 25.57698328643263, 0.3974027186347329, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3397800.0000, 
sim time next is 3398400.0000, 
raw observation next is [-2.0, 60.0, 44.5, 264.5, 26.0, 25.68057900825761, 0.3822752058633924, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.40720221606648205, 0.6, 0.14833333333333334, 0.29226519337016577, 0.6666666666666666, 0.6400482506881341, 0.6274250686211308, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3255769], dtype=float32), -1.1165979]. 
=============================================
[2019-04-04 06:25:21,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1153245e-09 1.4783683e-09 4.8146627e-24 1.6709688e-10 2.3802493e-10
 3.1209483e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:21,180] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7704
[2019-04-04 06:25:21,240] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 74.0, 5.0, 136.0, 26.0, 25.21880631218215, 0.2887305227992982, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3742200.0000, 
sim time next is 3742800.0000, 
raw observation next is [-4.0, 73.0, 19.0, 184.8333333333333, 26.0, 25.20947614672972, 0.3100435570615359, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3518005540166205, 0.73, 0.06333333333333334, 0.20423572744014729, 0.6666666666666666, 0.6007896788941434, 0.6033478523538452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.57642925], dtype=float32), -0.79775304]. 
=============================================
[2019-04-04 06:25:23,853] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4687419e-09 1.2940704e-09 3.0622192e-24 1.7388521e-10 8.0839765e-11
 2.4008597e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:23,857] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3420
[2019-04-04 06:25:23,923] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 67.5, 100.0, 676.0, 26.0, 25.89943495648097, 0.5270079587883992, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3576600.0000, 
sim time next is 3577200.0000, 
raw observation next is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 26.0, 25.86305451765483, 0.5216572257141798, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3148661126500462, 0.6666666666666665, 0.3394444444444443, 0.7620626151012891, 0.6666666666666666, 0.6552545431379025, 0.6738857419047267, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50087255], dtype=float32), 0.510906]. 
=============================================
[2019-04-04 06:25:25,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7906225e-09 1.2127979e-08 9.8514985e-24 7.0768691e-10 1.0995356e-09
 7.4591461e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:25,927] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6599
[2019-04-04 06:25:25,941] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.40338792463152, 0.4508933793342496, 0.0, 1.0, 66441.48253802297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3796200.0000, 
sim time next is 3796800.0000, 
raw observation next is [-3.0, 73.0, 0.0, 0.0, 26.0, 25.32150939735202, 0.444090124883286, 0.0, 1.0, 71103.17986913382], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6101257831126684, 0.648030041627762, 0.0, 1.0, 0.3385865708053991], 
reward next is 0.6614, 
noisyNet noise sample is [array([0.89935565], dtype=float32), -0.40468165]. 
=============================================
[2019-04-04 06:25:32,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6143536e-09 4.5875721e-09 5.7336531e-23 4.3398959e-10 2.9618671e-10
 3.8749685e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:32,030] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2980
[2019-04-04 06:25:32,042] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666666, 62.0, 0.0, 0.0, 26.0, 25.00477269481232, 0.2996106019262002, 0.0, 1.0, 41580.87458622423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3909000.0000, 
sim time next is 3909600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.92137362713659, 0.2939757552946549, 0.0, 1.0, 41752.84906455474], 
processed observation next is [1.0, 0.2608695652173913, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5767811355947158, 0.5979919184315516, 0.0, 1.0, 0.198823090783594], 
reward next is 0.8012, 
noisyNet noise sample is [array([0.15584444], dtype=float32), 1.927552]. 
=============================================
[2019-04-04 06:25:39,047] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8768263e-11 1.6784593e-10 2.5013313e-25 1.8016678e-11 1.2256979e-11
 1.2019225e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:39,049] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8759
[2019-04-04 06:25:39,055] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 26.0, 26.78212967799563, 0.7386493810719074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3858600.0000, 
sim time next is 3859200.0000, 
raw observation next is [3.0, 45.0, 75.5, 634.0, 26.0, 26.88208176940148, 0.7516313569092299, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.25166666666666665, 0.7005524861878453, 0.6666666666666666, 0.7401734807834567, 0.75054378563641, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37297225], dtype=float32), 0.14722103]. 
=============================================
[2019-04-04 06:25:50,639] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6032514e-09 2.6717455e-09 1.1713005e-24 7.9974077e-10 4.6915083e-10
 1.0061141e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:50,640] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3360
[2019-04-04 06:25:50,694] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 50.33333333333334, 91.66666666666667, 44.16666666666667, 26.0, 25.32687674313894, 0.3694749794009052, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4263600.0000, 
sim time next is 4264200.0000, 
raw observation next is [3.0, 51.0, 110.0, 53.0, 26.0, 25.53733436798247, 0.3962038242235126, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.51, 0.36666666666666664, 0.05856353591160221, 0.6666666666666666, 0.6281111973318726, 0.6320679414078375, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6393591], dtype=float32), -1.1779233]. 
=============================================
[2019-04-04 06:25:52,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5424936e-09 1.1798388e-09 8.4100510e-25 5.3925309e-11 8.8080911e-11
 4.9060400e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:52,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0132
[2019-04-04 06:25:52,151] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.3899692348828, 0.3275504737406975, 0.0, 1.0, 32805.49659303593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4252800.0000, 
sim time next is 4253400.0000, 
raw observation next is [3.0, 47.0, 0.0, 0.0, 26.0, 25.38630074438007, 0.325038240861362, 0.0, 1.0, 38083.08412038348], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6155250620316725, 0.6083460802871207, 0.0, 1.0, 0.1813480196208737], 
reward next is 0.8187, 
noisyNet noise sample is [array([-1.2796085], dtype=float32), -0.100931995]. 
=============================================
[2019-04-04 06:25:53,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0171815e-10 8.8781266e-10 1.7642275e-26 3.5035697e-11 1.7354717e-11
 7.1559021e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:25:53,757] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9827
[2019-04-04 06:25:53,773] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.983333333333333, 70.83333333333334, 0.0, 0.0, 26.0, 25.43612521935347, 0.4129654165843479, 0.0, 1.0, 41348.69401252695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4331400.0000, 
sim time next is 4332000.0000, 
raw observation next is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.62523078717932, 0.4147022354109358, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5724838411819021, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6354358989316099, 0.6382340784703119, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8635922], dtype=float32), 0.08108847]. 
=============================================
[2019-04-04 06:25:53,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[90.48289 ]
 [90.435974]
 [90.51524 ]
 [90.25798 ]
 [89.95603 ]], R is [[90.32229614]
 [90.2221756 ]
 [90.0383606 ]
 [89.83885193]
 [89.4690094 ]].
[2019-04-04 06:25:55,225] A3C_AGENT_WORKER-Thread-5 INFO:Evaluating...
[2019-04-04 06:25:55,226] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:25:55,226] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:25:55,226] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:25:55,227] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:25:55,227] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:25:55,228] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:25:55,231] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run13
[2019-04-04 06:25:55,262] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run13
[2019-04-04 06:25:55,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run13
[2019-04-04 06:26:14,460] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23936757], dtype=float32), 0.022514755]
[2019-04-04 06:26:14,461] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-4.6, 51.0, 0.0, 0.0, 26.0, 25.05874896579874, 0.3668583492173954, 0.0, 1.0, 45703.88185227352]
[2019-04-04 06:26:14,461] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:26:14,462] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.9151815e-09 5.6712737e-09 1.6879295e-22 7.3585549e-10 9.0513674e-10
 4.8125709e-12 1.0000000e+00], sampled 0.6548997584934284
[2019-04-04 06:27:33,085] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.23936757], dtype=float32), 0.022514755]
[2019-04-04 06:27:33,086] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.722035249, 75.46859553666667, 0.0, 0.0, 26.0, 23.89134344361877, 0.0300446327721921, 0.0, 1.0, 43523.78091688783]
[2019-04-04 06:27:33,086] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 06:27:33,086] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [5.7182836e-09 3.8133416e-09 7.8251449e-23 6.4071520e-10 8.0174151e-10
 2.9401436e-12 1.0000000e+00], sampled 0.31229280353322797
[2019-04-04 06:28:31,696] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.23936757], dtype=float32), 0.022514755]
[2019-04-04 06:28:31,696] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.333333333333333, 73.0, 63.33333333333334, 540.1666666666667, 26.0, 27.08709816110531, 0.5242184521403552, 1.0, 1.0, 0.0]
[2019-04-04 06:28:31,697] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:28:31,698] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.5126377e-12 7.5645679e-12 2.0247261e-27 1.0973132e-12 8.9272546e-13
 2.0556194e-15 1.0000000e+00], sampled 0.1742288765605592
[2019-04-04 06:29:01,838] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 06:29:34,415] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 06:29:38,631] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:29:39,665] A3C_AGENT_WORKER-Thread-5 INFO:Global step: 1200000, evaluation results [1200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:29:40,484] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.2311977e-10 2.1518902e-10 4.0619335e-26 1.5046839e-11 2.1889681e-11
 3.8831827e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:40,484] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2356
[2019-04-04 06:29:40,517] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.983333333333333, 70.83333333333334, 0.0, 0.0, 26.0, 25.43687487351304, 0.4116026210383315, 0.0, 1.0, 41410.12116764113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4331400.0000, 
sim time next is 4332000.0000, 
raw observation next is [3.966666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.62632950158801, 0.4134185227469283, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.5724838411819021, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6355274584656675, 0.6378061742489761, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19810094], dtype=float32), 1.0747942]. 
=============================================
[2019-04-04 06:29:40,536] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[89.33847 ]
 [89.28464 ]
 [89.35733 ]
 [89.09415 ]
 [88.789406]], R is [[89.19159698]
 [89.10249329]
 [88.92964935]
 [88.74119568]
 [88.38249969]].
[2019-04-04 06:29:43,638] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3487905e-10 2.0287988e-10 2.0377234e-24 1.3943184e-10 1.8434506e-11
 8.1483933e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:43,638] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5278
[2019-04-04 06:29:43,652] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 38.16666666666667, 129.3333333333333, 542.0, 26.0, 25.36474982058953, 0.4447169376135038, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4204200.0000, 
sim time next is 4204800.0000, 
raw observation next is [3.0, 37.0, 114.0, 544.0, 26.0, 25.36068804337992, 0.4481167751502684, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.38, 0.6011049723756906, 0.6666666666666666, 0.6133906702816599, 0.6493722583834228, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52824825], dtype=float32), -0.2302326]. 
=============================================
[2019-04-04 06:29:45,729] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.9622788e-10 2.4915128e-10 3.1884460e-24 4.4648472e-11 5.1656891e-11
 1.3915141e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:45,733] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0340
[2019-04-04 06:29:45,746] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.4124066814156, 0.3596221266739391, 0.0, 1.0, 41853.93093582476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.41862333014118, 0.3586608920030726, 0.0, 1.0, 37142.36623889351], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6182186108450983, 0.6195536306676909, 0.0, 1.0, 0.17686841066139766], 
reward next is 0.8231, 
noisyNet noise sample is [array([-0.17260659], dtype=float32), -0.79336196]. 
=============================================
[2019-04-04 06:29:47,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0589797e-10 2.4823296e-10 3.0460598e-24 5.5655338e-11 1.7190192e-11
 8.9076390e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:47,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5686
[2019-04-04 06:29:47,322] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 31.66666666666667, 118.8333333333333, 826.1666666666666, 26.0, 25.07100153552656, 0.3800239784868635, 0.0, 1.0, 19313.8941631898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4189200.0000, 
sim time next is 4189800.0000, 
raw observation next is [0.6666666666666667, 30.83333333333333, 118.6666666666667, 830.3333333333334, 26.0, 25.0468526958025, 0.3918159788861394, 0.0, 1.0, 22314.90300088832], 
processed observation next is [0.0, 0.4782608695652174, 0.4810710987996307, 0.3083333333333333, 0.39555555555555566, 0.9174953959484347, 0.6666666666666666, 0.5872377246502083, 0.6306053262953798, 0.0, 1.0, 0.10626144286137296], 
reward next is 0.8937, 
noisyNet noise sample is [array([-1.0086253], dtype=float32), -0.18113284]. 
=============================================
[2019-04-04 06:29:47,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9645578e-10 3.5958350e-10 2.1646815e-24 4.1159569e-11 5.4537555e-11
 1.3896515e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:47,598] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4114
[2019-04-04 06:29:47,638] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 34.16666666666667, 117.3333333333333, 806.0, 26.0, 25.11545053542289, 0.3892565273496273, 0.0, 1.0, 18705.58955302931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4187400.0000, 
sim time next is 4188000.0000, 
raw observation next is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 26.0, 25.13207899321517, 0.3865265967903277, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4533702677747, 0.3333333333333334, 0.393888888888889, 0.8994475138121547, 0.6666666666666666, 0.5943399161012642, 0.6288421989301093, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.70639884], dtype=float32), -0.19277789]. 
=============================================
[2019-04-04 06:29:47,644] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.37882 ]
 [84.2963  ]
 [84.3032  ]
 [84.394844]
 [84.40046 ]], R is [[84.49223328]
 [84.55823517]
 [84.62358093]
 [84.77734375]
 [84.92957306]].
[2019-04-04 06:29:50,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7612840e-10 3.7386277e-10 1.0788997e-25 1.9580957e-11 3.7391035e-11
 7.0746919e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:50,622] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4697
[2019-04-04 06:29:50,648] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666667, 52.5, 120.6666666666667, 830.3333333333334, 26.0, 25.20786660979692, 0.3984007233520767, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4276200.0000, 
sim time next is 4276800.0000, 
raw observation next is [7.0, 52.0, 120.5, 834.5, 26.0, 25.21134794371913, 0.4030611469022774, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.40166666666666667, 0.9220994475138121, 0.6666666666666666, 0.6009456619765942, 0.6343537156340925, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42601487], dtype=float32), -0.17989317]. 
=============================================
[2019-04-04 06:29:50,791] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.2405423e-11 5.7504120e-11 4.1669137e-26 3.4231392e-12 2.9117930e-12
 6.9314269e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:50,792] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9681
[2019-04-04 06:29:50,806] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 131.3333333333333, 825.3333333333334, 26.0, 25.27423932470244, 0.4259804472766375, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4279200.0000, 
sim time next is 4279800.0000, 
raw observation next is [7.0, 52.0, 142.6666666666667, 803.6666666666667, 26.0, 25.33237788586542, 0.4309792352880772, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.47555555555555573, 0.8880294659300185, 0.6666666666666666, 0.6110314904887849, 0.6436597450960257, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40620604], dtype=float32), -0.8269931]. 
=============================================
[2019-04-04 06:29:58,472] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0196602e-09 9.9023767e-10 1.7222175e-24 4.5304725e-11 9.1709397e-11
 1.5462374e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:29:58,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5546
[2019-04-04 06:29:58,484] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.4, 70.33333333333333, 0.0, 0.0, 26.0, 25.48835197151182, 0.373227685589935, 0.0, 1.0, 18753.66731281949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4340400.0000, 
sim time next is 4341000.0000, 
raw observation next is [3.35, 70.66666666666667, 0.0, 0.0, 26.0, 25.44948725428051, 0.3794028449939719, 0.0, 1.0, 42448.54698686583], 
processed observation next is [1.0, 0.21739130434782608, 0.5554016620498616, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6207906045233758, 0.6264676149979906, 0.0, 1.0, 0.20213593803269445], 
reward next is 0.7979, 
noisyNet noise sample is [array([-0.3759158], dtype=float32), 0.8374271]. 
=============================================
[2019-04-04 06:29:58,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.736824]
 [82.89149 ]
 [83.04205 ]
 [83.0766  ]
 [83.00682 ]], R is [[82.65926361]
 [82.74337006]
 [82.82662201]
 [82.8078537 ]
 [82.66490936]].
[2019-04-04 06:30:13,741] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.3777701e-09 4.5981607e-10 2.9484354e-25 2.6849906e-10 5.4246269e-10
 2.6457910e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:13,742] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2426
[2019-04-04 06:30:13,806] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 75.0, 157.3333333333333, 420.3333333333333, 26.0, 25.65451900138266, 0.4657198512994356, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4785000.0000, 
sim time next is 4785600.0000, 
raw observation next is [-4.333333333333334, 73.0, 165.6666666666667, 420.6666666666667, 26.0, 25.63910324625488, 0.4642906195263557, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3425669436749769, 0.73, 0.5522222222222224, 0.46482504604051567, 0.6666666666666666, 0.6365919371879066, 0.6547635398421185, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4868966], dtype=float32), 1.2065275]. 
=============================================
[2019-04-04 06:30:22,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2746137e-08 1.4656616e-08 5.3334590e-22 1.4525807e-09 1.6758402e-09
 3.1684010e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:22,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3397
[2019-04-04 06:30:22,210] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 26.0, 24.41969828623034, 0.1400115038898663, 0.0, 1.0, 39593.21697286191], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4865400.0000, 
sim time next is 4866000.0000, 
raw observation next is [-4.0, 69.0, 23.5, 52.16666666666666, 26.0, 24.39214888430804, 0.1438168307388422, 0.0, 1.0, 39564.93494466007], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.69, 0.07833333333333334, 0.05764272559852669, 0.6666666666666666, 0.5326790736923366, 0.547938943579614, 0.0, 1.0, 0.1884044521174289], 
reward next is 0.8116, 
noisyNet noise sample is [array([1.46634], dtype=float32), -0.9552077]. 
=============================================
[2019-04-04 06:30:22,246] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.42728]
 [75.48702]
 [75.55443]
 [75.64865]
 [75.75366]], R is [[76.06961823]
 [76.12038422]
 [76.17073059]
 [76.22068024]
 [76.27027893]].
[2019-04-04 06:30:24,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1663646e-09 1.9926734e-09 6.9385381e-24 1.0868163e-10 1.5492715e-10
 1.3528619e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:24,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8246
[2019-04-04 06:30:24,188] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 67.0, 0.0, 0.0, 26.0, 25.68174780301176, 0.476614721421624, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4677000.0000, 
sim time next is 4677600.0000, 
raw observation next is [1.333333333333333, 72.0, 0.0, 0.0, 26.0, 25.6546704228174, 0.4591525256196293, 0.0, 1.0, 18728.50836786075], 
processed observation next is [1.0, 0.13043478260869565, 0.4995383194829178, 0.72, 0.0, 0.0, 0.6666666666666666, 0.63788920190145, 0.6530508418732098, 0.0, 1.0, 0.0891833731802893], 
reward next is 0.9108, 
noisyNet noise sample is [array([-2.808172], dtype=float32), 1.2660264]. 
=============================================
[2019-04-04 06:30:34,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:34,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:34,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run10
[2019-04-04 06:30:35,656] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5760353e-12 2.2736140e-12 4.1891909e-28 7.6106146e-13 1.1074625e-12
 4.2508875e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:35,657] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1400
[2019-04-04 06:30:35,723] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 17.33333333333334, 62.66666666666667, 487.3333333333334, 26.0, 29.14112502602905, 1.034447299582888, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5071800.0000, 
sim time next is 5072400.0000, 
raw observation next is [12.0, 17.0, 56.0, 438.5, 26.0, 29.16168662187965, 1.20226337002405, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7950138504155125, 0.17, 0.18666666666666668, 0.4845303867403315, 0.6666666666666666, 0.9301405518233041, 0.9007544566746833, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.001353], dtype=float32), 0.91800356]. 
=============================================
[2019-04-04 06:30:36,069] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8699133e-10 1.2412756e-10 4.4699557e-24 2.4673130e-11 1.9768588e-11
 7.1301011e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:36,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6788
[2019-04-04 06:30:36,104] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 114.0, 732.0, 26.0, 25.17971161781472, 0.4452597211595135, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4806600.0000, 
sim time next is 4807200.0000, 
raw observation next is [3.0, 37.0, 105.5, 729.5, 26.0, 25.18554610781176, 0.4440344088789673, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3516666666666667, 0.8060773480662984, 0.6666666666666666, 0.5987955089843133, 0.6480114696263225, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18860988], dtype=float32), -0.43103084]. 
=============================================
[2019-04-04 06:30:36,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:36,697] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:36,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run10
[2019-04-04 06:30:37,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:37,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:37,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run10
[2019-04-04 06:30:39,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:39,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:39,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run10
[2019-04-04 06:30:39,888] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:39,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:39,892] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run10
[2019-04-04 06:30:40,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8111009e-09 1.6816037e-09 1.1962040e-23 2.1896790e-10 5.1114918e-10
 1.2744336e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:40,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6861
[2019-04-04 06:30:40,183] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 45.33333333333334, 0.0, 0.0, 26.0, 25.51487473170037, 0.3204088605241389, 0.0, 1.0, 28164.46364442998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4929600.0000, 
sim time next is 4930200.0000, 
raw observation next is [-0.5, 46.5, 0.0, 0.0, 26.0, 25.42825800872156, 0.3137669073105281, 0.0, 1.0, 77373.81732395255], 
processed observation next is [1.0, 0.043478260869565216, 0.44875346260387816, 0.465, 0.0, 0.0, 0.6666666666666666, 0.6190215007267966, 0.6045889691035093, 0.0, 1.0, 0.3684467491616788], 
reward next is 0.6316, 
noisyNet noise sample is [array([-1.0253187], dtype=float32), -0.58965594]. 
=============================================
[2019-04-04 06:30:42,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:42,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:42,336] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run10
[2019-04-04 06:30:44,283] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:44,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:44,306] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run10
[2019-04-04 06:30:44,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3131586e-09 9.1448482e-10 3.0414286e-23 2.0193903e-10 3.8623399e-10
 4.8884421e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:44,521] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9152
[2019-04-04 06:30:44,551] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.07846349058602, 0.5994400677087498, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4996200.0000, 
sim time next is 4996800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.04917917699689, 0.5871874516033936, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.6707649314164076, 0.6957291505344645, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.00399433], dtype=float32), -0.534335]. 
=============================================
[2019-04-04 06:30:48,885] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0089688e-09 1.5435295e-09 3.4545359e-24 2.3764934e-10 2.1275137e-10
 2.0219734e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:48,885] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5095
[2019-04-04 06:30:48,915] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.3, 25.0, 0.0, 0.0, 26.0, 26.01527520949966, 0.5727187899282417, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [8.25, 27.5, 0.0, 0.0, 26.0, 25.93525263584377, 0.5558571718959865, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6911357340720222, 0.275, 0.0, 0.0, 0.6666666666666666, 0.6612710529869809, 0.6852857239653288, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7871946], dtype=float32), 0.18515778]. 
=============================================
[2019-04-04 06:30:50,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:50,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:50,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run10
[2019-04-04 06:30:50,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:50,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:50,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run10
[2019-04-04 06:30:51,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:51,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:51,302] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run10
[2019-04-04 06:30:51,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:51,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:51,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run10
[2019-04-04 06:30:51,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:51,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:51,721] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run10
[2019-04-04 06:30:51,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:51,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:51,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run10
[2019-04-04 06:30:52,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:52,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:52,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run10
[2019-04-04 06:30:52,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:52,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:52,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run10
[2019-04-04 06:30:54,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:30:54,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:30:54,327] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run10
[2019-04-04 06:30:56,247] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5213521e-10 2.5967892e-10 7.1679412e-25 2.5688167e-11 4.4338887e-11
 3.3622530e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:30:56,247] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9280
[2019-04-04 06:30:56,339] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.366666666666667, 86.0, 74.16666666666667, 0.0, 26.0, 24.5055980053187, 0.1661517108199529, 0.0, 1.0, 18738.55844890749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 52800.0000, 
sim time next is 53400.0000, 
raw observation next is [7.283333333333333, 86.0, 69.33333333333334, 0.0, 26.0, 24.49727512317199, 0.1616553036275158, 0.0, 1.0, 24508.61877228752], 
processed observation next is [0.0, 0.6086956521739131, 0.6643582640812559, 0.86, 0.23111111111111116, 0.0, 0.6666666666666666, 0.5414395935976657, 0.5538851012091719, 0.0, 1.0, 0.11670770843946437], 
reward next is 0.8833, 
noisyNet noise sample is [array([-0.8641131], dtype=float32), -0.77216065]. 
=============================================
[2019-04-04 06:31:34,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8999119e-08 7.3600781e-08 4.4293793e-20 8.1947507e-09 2.1431594e-08
 4.9313064e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 06:31:34,783] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4373
[2019-04-04 06:31:34,819] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.9, 68.0, 0.0, 0.0, 26.0, 23.72635098491163, -0.009128637589630348, 0.0, 1.0, 47184.94092371457], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [-13.9, 67.33333333333334, 0.0, 0.0, 26.0, 23.65919300768117, -0.01447291577866559, 0.0, 1.0, 47224.31904413381], 
processed observation next is [1.0, 1.0, 0.07756232686980608, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.4715994173067643, 0.4951756947404448, 0.0, 1.0, 0.2248777097339705], 
reward next is 0.7751, 
noisyNet noise sample is [array([0.45725426], dtype=float32), -0.37037307]. 
=============================================
[2019-04-04 06:31:37,814] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2726806e-09 1.3115322e-08 4.9956095e-21 2.3390303e-09 1.7767984e-09
 8.6689007e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:31:37,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4043
[2019-04-04 06:31:37,849] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.73333333333333, 74.0, 0.0, 0.0, 26.0, 24.05201680568464, 0.06376763898901965, 0.0, 1.0, 47063.09633987259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 340800.0000, 
sim time next is 341400.0000, 
raw observation next is [-13.81666666666667, 72.0, 0.0, 0.0, 26.0, 23.98762742905242, 0.04767337519418476, 0.0, 1.0, 47075.25516060694], 
processed observation next is [1.0, 0.9565217391304348, 0.0798707294552169, 0.72, 0.0, 0.0, 0.6666666666666666, 0.498968952421035, 0.5158911250647282, 0.0, 1.0, 0.2241678817171759], 
reward next is 0.7758, 
noisyNet noise sample is [array([0.39272547], dtype=float32), 0.26535687]. 
=============================================
[2019-04-04 06:31:43,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7842933e-08 3.6974505e-08 2.5394970e-20 6.9188855e-09 7.3899797e-09
 4.1227136e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 06:31:43,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9622
[2019-04-04 06:31:43,786] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.33333333333333, 52.33333333333334, 0.0, 0.0, 26.0, 24.15025130919734, 0.06860687359578016, 0.0, 1.0, 44741.88744325441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 427200.0000, 
sim time next is 427800.0000, 
raw observation next is [-11.51666666666667, 53.16666666666666, 0.0, 0.0, 26.0, 24.09293546757745, 0.05549122326144542, 0.0, 1.0, 44747.95342157176], 
processed observation next is [1.0, 0.9565217391304348, 0.14358264081255764, 0.5316666666666666, 0.0, 0.0, 0.6666666666666666, 0.5077446222981209, 0.5184970744204819, 0.0, 1.0, 0.21308549248367503], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.00181467], dtype=float32), -1.7693334]. 
=============================================
[2019-04-04 06:32:00,472] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.2490433e-10 3.6386380e-10 1.8819772e-25 1.3184308e-10 6.2326505e-11
 3.3419834e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:00,477] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5740
[2019-04-04 06:32:00,524] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.45, 55.5, 0.0, 0.0, 26.0, 25.14303555736009, 0.3185369476502521, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 761400.0000, 
sim time next is 762000.0000, 
raw observation next is [-4.633333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 25.06787094881918, 0.2990072652951073, 1.0, 1.0, 45504.73717107778], 
processed observation next is [1.0, 0.8260869565217391, 0.3342566943674977, 0.5633333333333332, 0.0, 0.0, 0.6666666666666666, 0.5889892457349317, 0.5996690884317024, 1.0, 1.0, 0.21668922462417992], 
reward next is 0.7833, 
noisyNet noise sample is [array([-1.924171], dtype=float32), 0.4103271]. 
=============================================
[2019-04-04 06:32:00,535] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.73807 ]
 [88.067665]
 [88.413956]
 [88.07036 ]
 [87.67146 ]], R is [[87.48926544]
 [87.61437225]
 [87.73822784]
 [87.38607025]
 [86.9793396 ]].
[2019-04-04 06:32:07,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7806807e-11 1.2332071e-10 1.4415755e-25 2.1002733e-11 2.1784014e-11
 5.8954110e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:07,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0630
[2019-04-04 06:32:07,205] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.100000000000001, 58.5, 0.0, 0.0, 26.0, 24.9514026928253, 0.3223675884903132, 1.0, 1.0, 63927.30345191906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 763800.0000, 
sim time next is 764400.0000, 
raw observation next is [-5.2, 59.0, 0.0, 0.0, 26.0, 25.01070949865711, 0.3235629941380637, 0.0, 1.0, 77195.01407838715], 
processed observation next is [1.0, 0.8695652173913043, 0.31855955678670367, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5842257915547592, 0.6078543313793546, 0.0, 1.0, 0.3675953051351769], 
reward next is 0.6324, 
noisyNet noise sample is [array([1.1218101], dtype=float32), -0.23669405]. 
=============================================
[2019-04-04 06:32:22,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8294206e-09 5.4581811e-10 2.9675826e-26 6.4190014e-11 1.2727078e-10
 7.2179337e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:22,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1481
[2019-04-04 06:32:22,547] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.233333333333333, 84.33333333333334, 0.0, 0.0, 26.0, 25.36186257300121, 0.4492530389472041, 0.0, 1.0, 42729.41757613902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 956400.0000, 
sim time next is 957000.0000, 
raw observation next is [6.416666666666666, 83.16666666666667, 0.0, 0.0, 26.0, 25.45991158789886, 0.4564618938495801, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6403508771929826, 0.8316666666666667, 0.0, 0.0, 0.6666666666666666, 0.6216592989915716, 0.6521539646165267, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6584727], dtype=float32), -0.023122825]. 
=============================================
[2019-04-04 06:32:22,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[91.85084 ]
 [91.80482 ]
 [91.800316]
 [91.75083 ]
 [91.67294 ]], R is [[91.80210114]
 [91.68060303]
 [91.62796783]
 [91.53088379]
 [91.43462372]].
[2019-04-04 06:32:26,891] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.7008245e-11 4.2928765e-11 8.2179200e-27 5.3324996e-12 9.0459211e-12
 7.4318264e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:26,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3213
[2019-04-04 06:32:26,919] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23977803303761, 0.4092096379802662, 0.0, 1.0, 38549.2671142858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 948600.0000, 
sim time next is 949200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.24021052798343, 0.4145045783637066, 0.0, 1.0, 38454.95948723696], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6033508773319524, 0.6381681927879023, 0.0, 1.0, 0.1831188547011284], 
reward next is 0.8169, 
noisyNet noise sample is [array([0.06181881], dtype=float32), -0.55764145]. 
=============================================
[2019-04-04 06:32:28,395] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.04667525e-10 1.70519043e-10 2.64270714e-26 4.31649785e-11
 4.54028204e-11 1.71850517e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:32:28,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5305
[2019-04-04 06:32:28,426] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.35, 92.0, 0.0, 0.0, 26.0, 25.30625581143205, 0.5209274681551932, 0.0, 1.0, 56181.0947152072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1319400.0000, 
sim time next is 1320000.0000, 
raw observation next is [1.266666666666667, 92.0, 0.0, 0.0, 26.0, 25.3541770853876, 0.5299962511917439, 0.0, 1.0, 45991.24918938802], 
processed observation next is [1.0, 0.2608695652173913, 0.4976915974145891, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6128480904489667, 0.6766654170639147, 0.0, 1.0, 0.21900594852089533], 
reward next is 0.7810, 
noisyNet noise sample is [array([0.29636705], dtype=float32), -0.06326133]. 
=============================================
[2019-04-04 06:32:28,442] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[90.75867 ]
 [90.722305]
 [90.551605]
 [90.34841 ]
 [90.35966 ]], R is [[90.59359741]
 [90.42012787]
 [90.1448822 ]
 [89.85458374]
 [89.76197052]].
[2019-04-04 06:32:34,023] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1255130e-11 4.4720408e-11 4.4067612e-28 6.5006620e-12 5.4805518e-12
 3.5384577e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:34,025] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9760
[2019-04-04 06:32:34,036] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 24.84753070932601, 0.4273176523760929, 0.0, 1.0, 196650.5451134171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1024200.0000, 
sim time next is 1024800.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.83251530042941, 0.4730710096951243, 0.0, 1.0, 198382.711794217], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5693762750357841, 0.6576903365650414, 0.0, 1.0, 0.9446795799724619], 
reward next is 0.0553, 
noisyNet noise sample is [array([0.5076122], dtype=float32), -0.33019474]. 
=============================================
[2019-04-04 06:32:34,113] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6611709e-11 5.7438460e-11 7.5332480e-28 7.9347808e-13 4.8894768e-12
 7.3706164e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:34,114] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0873
[2019-04-04 06:32:34,124] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.18236570297579, 0.5717128499023015, 0.0, 1.0, 86481.55071634872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1026000.0000, 
sim time next is 1026600.0000, 
raw observation next is [14.4, 76.66666666666667, 0.0, 0.0, 26.0, 25.50938651676638, 0.6034302538747894, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.7666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6257822097305317, 0.7011434179582632, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03199914], dtype=float32), -0.29392108]. 
=============================================
[2019-04-04 06:32:42,595] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2588740e-10 1.8590052e-10 1.3975694e-25 1.7630434e-11 4.8071332e-11
 6.0102890e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:42,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1928
[2019-04-04 06:32:42,609] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.60778177257971, 0.5477148217391307, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315200.0000, 
sim time next is 1315800.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55343473028335, 0.5325912291564134, 0.0, 1.0, 18744.20024836863], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6294528941902792, 0.6775304097188045, 0.0, 1.0, 0.089258096420803], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.545833], dtype=float32), -0.2525146]. 
=============================================
[2019-04-04 06:32:43,642] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.8981275e-12 1.4631176e-11 7.2178169e-27 3.7469689e-12 1.3106518e-12
 2.3939205e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:43,642] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3514
[2019-04-04 06:32:43,664] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.7, 51.66666666666667, 76.33333333333333, 539.6666666666666, 26.0, 25.99701545753517, 0.6898716494369279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1523400.0000, 
sim time next is 1524000.0000, 
raw observation next is [11.8, 51.33333333333334, 76.66666666666667, 508.8333333333334, 26.0, 26.4355068260707, 0.7178440016166582, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7894736842105264, 0.5133333333333334, 0.2555555555555556, 0.5622467771639044, 0.6666666666666666, 0.7029589021725583, 0.7392813338722194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6091958], dtype=float32), 0.20397957]. 
=============================================
[2019-04-04 06:32:43,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.00316 ]
 [93.15051 ]
 [93.240326]
 [93.271545]
 [93.29001 ]], R is [[92.99581909]
 [93.06586456]
 [93.13520813]
 [93.20385742]
 [93.27182007]].
[2019-04-04 06:32:46,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2105932e-10 4.1617054e-10 1.7114440e-24 6.2081978e-11 2.6470102e-11
 4.0050924e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:46,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8212
[2019-04-04 06:32:46,914] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.68652467809071, 0.5629039060301487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1558800.0000, 
sim time next is 1559400.0000, 
raw observation next is [5.0, 81.50000000000001, 0.0, 0.0, 26.0, 25.65077802836193, 0.5595889547030564, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.6375648356968275, 0.6865296515676854, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4240284], dtype=float32), 0.05248682]. 
=============================================
[2019-04-04 06:32:46,994] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.8436900e-10 8.5647439e-10 2.1179849e-23 1.7717280e-10 3.2680184e-10
 2.3101232e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:46,997] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4337
[2019-04-04 06:32:47,009] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 0.0, 0.0, 26.0, 25.52184740276703, 0.5772929618785919, 0.0, 1.0, 18750.14888232714], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1374000.0000, 
sim time next is 1374600.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.48591048160988, 0.5700123329346336, 0.0, 1.0, 36108.98609967782], 
processed observation next is [1.0, 0.9130434782608695, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6238258734674901, 0.6900041109782112, 0.0, 1.0, 0.17194755285560867], 
reward next is 0.8281, 
noisyNet noise sample is [array([-0.8426521], dtype=float32), 2.2627811]. 
=============================================
[2019-04-04 06:32:59,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2782584e-09 2.0338831e-09 4.9478709e-22 4.8669002e-10 4.4867723e-10
 2.1250458e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:32:59,950] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5404
[2019-04-04 06:33:00,019] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 26.0, 24.96741012278507, 0.3565215301540889, 0.0, 1.0, 33893.49551372251], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1773600.0000, 
sim time next is 1774200.0000, 
raw observation next is [-2.716666666666667, 83.0, 123.6666666666667, 0.0, 26.0, 25.00347778164445, 0.3543477498859848, 0.0, 1.0, 19595.45333871308], 
processed observation next is [0.0, 0.5217391304347826, 0.3873499538319483, 0.83, 0.4122222222222223, 0.0, 0.6666666666666666, 0.5836231484703708, 0.6181159166286616, 0.0, 1.0, 0.09331168256530038], 
reward next is 0.9067, 
noisyNet noise sample is [array([1.7514583], dtype=float32), 0.40947783]. 
=============================================
[2019-04-04 06:33:03,266] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2750274e-09 1.9221997e-09 1.3221793e-23 3.0436481e-10 1.0610153e-09
 3.9932768e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:33:03,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2033
[2019-04-04 06:33:03,289] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.94253116860676, 0.3663299308977526, 0.0, 1.0, 43610.34328726076], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [-0.8999999999999999, 85.0, 0.0, 0.0, 26.0, 24.98323077608004, 0.3633682238982506, 0.0, 1.0, 43639.16557852679], 
processed observation next is [0.0, 0.21739130434782608, 0.43767313019390586, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5819358980066699, 0.6211227412994168, 0.0, 1.0, 0.20780555037393708], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.33260265], dtype=float32), 0.8726987]. 
=============================================
[2019-04-04 06:33:10,713] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2076337e-08 3.6729478e-08 7.8920675e-20 8.1709803e-09 6.3681673e-09
 2.6275393e-10 9.9999988e-01], sum to 1.0000
[2019-04-04 06:33:10,713] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6206
[2019-04-04 06:33:10,738] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 88.0, 0.0, 0.0, 26.0, 23.14543316615907, -0.1807285878027474, 0.0, 1.0, 44583.86251116874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1924800.0000, 
sim time next is 1925400.0000, 
raw observation next is [-9.4, 89.5, 0.0, 0.0, 26.0, 23.15657408791583, -0.1868858606891419, 0.0, 1.0, 44533.11061667466], 
processed observation next is [1.0, 0.2608695652173913, 0.20221606648199447, 0.895, 0.0, 0.0, 0.6666666666666666, 0.42971450732631905, 0.43770471310361936, 0.0, 1.0, 0.21206243150797457], 
reward next is 0.7879, 
noisyNet noise sample is [array([-0.83766305], dtype=float32), 0.21774451]. 
=============================================
[2019-04-04 06:33:18,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8666249e-08 5.9001732e-09 1.7897533e-21 1.4657952e-09 4.3868784e-09
 3.3974219e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:33:18,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6422
[2019-04-04 06:33:18,186] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.00880322946809, 0.3151774601432937, 0.0, 1.0, 48147.30965864663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1796400.0000, 
sim time next is 1797000.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01150164596977, 0.3143140389017838, 0.0, 1.0, 45585.51167848399], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5842918038308141, 0.6047713463005946, 0.0, 1.0, 0.21707386513563806], 
reward next is 0.7829, 
noisyNet noise sample is [array([-0.02214978], dtype=float32), -0.81514704]. 
=============================================
[2019-04-04 06:33:18,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.06685]
 [72.99738]
 [72.9102 ]
 [72.78609]
 [72.64378]], R is [[73.16621399]
 [73.20528412]
 [73.24220276]
 [73.27613831]
 [73.30834961]].
[2019-04-04 06:33:22,761] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1579089e-08 9.8088249e-09 1.8953240e-21 4.2751740e-09 7.0696009e-09
 2.0494694e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:33:22,772] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7171
[2019-04-04 06:33:22,842] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.233333333333333, 85.0, 0.0, 0.0, 26.0, 25.01575723553228, 0.2458148949299596, 0.0, 1.0, 54364.29354395329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1885200.0000, 
sim time next is 1885800.0000, 
raw observation next is [-5.416666666666667, 85.5, 0.0, 0.0, 26.0, 25.03946494283571, 0.2446656188896683, 0.0, 1.0, 47309.83112551711], 
processed observation next is [0.0, 0.8260869565217391, 0.3125577100646353, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5866220785696425, 0.5815552062965561, 0.0, 1.0, 0.22528491012151003], 
reward next is 0.7747, 
noisyNet noise sample is [array([1.2093244], dtype=float32), 0.43075562]. 
=============================================
[2019-04-04 06:33:40,265] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8760203e-11 7.6837994e-11 3.9090982e-25 1.2894987e-11 7.2249659e-12
 7.9453174e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:33:40,265] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3461
[2019-04-04 06:33:40,309] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.1, 79.0, 140.6666666666667, 0.0, 26.0, 25.81266566350234, 0.452621526392419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2036400.0000, 
sim time next is 2037000.0000, 
raw observation next is [-4.0, 79.0, 133.3333333333333, 0.0, 26.0, 26.13050848547026, 0.4760679570549104, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3518005540166205, 0.79, 0.4444444444444443, 0.0, 0.6666666666666666, 0.6775423737891882, 0.6586893190183035, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15047269], dtype=float32), -1.0456156]. 
=============================================
[2019-04-04 06:33:40,316] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.40052 ]
 [85.644165]
 [85.88507 ]
 [85.15151 ]
 [84.47171 ]], R is [[85.25411224]
 [85.40157318]
 [85.54756165]
 [84.73826599]
 [83.94771576]].
[2019-04-04 06:33:49,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.6452250e-09 2.8258953e-09 1.5146960e-22 2.3714852e-10 3.0650632e-10
 2.3159027e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:33:49,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6347
[2019-04-04 06:33:49,426] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 77.0, 0.0, 0.0, 26.0, 23.8669954849398, 0.03272881408701542, 0.0, 1.0, 41939.8081202485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2179800.0000, 
sim time next is 2180400.0000, 
raw observation next is [-6.199999999999999, 77.66666666666667, 0.0, 0.0, 26.0, 23.87992426297167, 0.02470334461376994, 0.0, 1.0, 41905.96022901432], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.48999368858097253, 0.50823444820459, 0.0, 1.0, 0.19955219156673487], 
reward next is 0.8004, 
noisyNet noise sample is [array([-0.3828162], dtype=float32), -0.5367907]. 
=============================================
[2019-04-04 06:33:53,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7214815e-10 1.1923265e-09 1.5326084e-23 2.2675663e-10 1.7461806e-10
 2.1288958e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:33:53,334] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4744
[2019-04-04 06:33:53,405] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 70.0, 0.0, 0.0, 26.0, 25.15850841825549, 0.3917526195542656, 0.0, 1.0, 142518.2339733135], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2233200.0000, 
sim time next is 2233800.0000, 
raw observation next is [-5.0, 69.5, 0.0, 0.0, 26.0, 25.13777204102743, 0.4022660292589341, 0.0, 1.0, 91065.4550027276], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.695, 0.0, 0.0, 0.6666666666666666, 0.5948143367522857, 0.6340886764196447, 0.0, 1.0, 0.43364502382251235], 
reward next is 0.5664, 
noisyNet noise sample is [array([1.4020286], dtype=float32), -0.57308584]. 
=============================================
[2019-04-04 06:33:54,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5723898e-09 1.4155994e-08 8.9292744e-22 2.2032622e-09 3.0947542e-09
 1.0960984e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:33:54,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-04 06:33:54,803] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.9, 86.16666666666667, 0.0, 0.0, 26.0, 24.1267127936026, 0.1004539897640537, 0.0, 1.0, 43737.09222615077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2257800.0000, 
sim time next is 2258400.0000, 
raw observation next is [-8.0, 86.33333333333334, 0.0, 0.0, 26.0, 24.15635758961614, 0.09977588330870964, 0.0, 1.0, 43682.06076511289], 
processed observation next is [1.0, 0.13043478260869565, 0.24099722991689754, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.5130297991346783, 0.5332586277695699, 0.0, 1.0, 0.20800981316720424], 
reward next is 0.7920, 
noisyNet noise sample is [array([-1.1491611], dtype=float32), 0.2373397]. 
=============================================
[2019-04-04 06:34:16,532] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8585594e-11 1.9074620e-10 3.8374250e-25 3.7414294e-11 1.1886603e-11
 1.6108160e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:34:16,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0528
[2019-04-04 06:34:16,562] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.9, 29.0, 59.5, 135.8333333333333, 26.0, 25.50388861233261, 0.3797646713975489, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2565600.0000, 
sim time next is 2566200.0000, 
raw observation next is [2.8, 29.0, 49.00000000000001, 109.6666666666667, 26.0, 25.66113691921421, 0.3881982481309039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5401662049861496, 0.29, 0.16333333333333336, 0.12117863720073668, 0.6666666666666666, 0.6384280766011843, 0.6293994160436346, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0068116], dtype=float32), -0.6979517]. 
=============================================
[2019-04-04 06:34:18,858] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1143269e-09 1.5405912e-09 5.8649675e-23 2.6036681e-10 4.6355683e-10
 2.0519326e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:34:18,863] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4469
[2019-04-04 06:34:18,946] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.76130649335748, 0.1413536115204148, 0.0, 1.0, 38370.37640492142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2523600.0000, 
sim time next is 2524200.0000, 
raw observation next is [-2.3, 57.0, 0.0, 0.0, 26.0, 24.77963383054747, 0.1428822119237866, 0.0, 1.0, 38362.84756241553], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5649694858789559, 0.5476274039745955, 0.0, 1.0, 0.18268022648769303], 
reward next is 0.8173, 
noisyNet noise sample is [array([0.52525204], dtype=float32), -1.3459088]. 
=============================================
[2019-04-04 06:34:24,926] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0116566e-11 5.7177218e-11 1.4026701e-25 9.5112321e-12 1.4460877e-12
 1.6730065e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:34:24,926] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4373
[2019-04-04 06:34:24,968] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.110223024625157e-16, 48.0, 133.1666666666667, 720.5, 26.0, 26.06124552352647, 0.4605177145916466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2805600.0000, 
sim time next is 2806200.0000, 
raw observation next is [0.5, 47.0, 125.0, 763.0, 26.0, 26.02839973547854, 0.4537229095861594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4764542936288089, 0.47, 0.4166666666666667, 0.8430939226519337, 0.6666666666666666, 0.6690333112898784, 0.6512409698620532, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5041166], dtype=float32), 1.4362451]. 
=============================================
[2019-04-04 06:34:25,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1239976e-08 1.4938710e-08 9.6561042e-21 2.2537610e-09 4.3199533e-09
 3.0523844e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:34:25,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7559
[2019-04-04 06:34:25,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 88.33333333333334, 0.0, 0.0, 26.0, 23.42010681961304, -0.02414641470665616, 0.0, 1.0, 44413.4547743009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2694000.0000, 
sim time next is 2694600.0000, 
raw observation next is [-15.0, 87.0, 0.0, 0.0, 26.0, 23.40160163831709, -0.03562714537994365, 0.0, 1.0, 44318.26056193966], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.87, 0.0, 0.0, 0.6666666666666666, 0.45013346985975744, 0.4881242848733521, 0.0, 1.0, 0.2110393360092365], 
reward next is 0.7890, 
noisyNet noise sample is [array([0.0774353], dtype=float32), 0.3982904]. 
=============================================
[2019-04-04 06:34:39,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5878586e-09 1.7570093e-09 1.6082145e-22 6.8712885e-10 5.2800286e-10
 3.0703088e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:34:39,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8631
[2019-04-04 06:34:39,987] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 71.0, 0.0, 0.0, 26.0, 24.60527887881784, 0.2497021991480936, 0.0, 1.0, 44430.08555026496], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2679600.0000, 
sim time next is 2680200.0000, 
raw observation next is [-8.0, 70.5, 0.0, 0.0, 26.0, 24.61944682449102, 0.2402973981713166, 0.0, 1.0, 44418.53293036347], 
processed observation next is [1.0, 0.0, 0.24099722991689754, 0.705, 0.0, 0.0, 0.6666666666666666, 0.5516205687075851, 0.5800991327237722, 0.0, 1.0, 0.2115168234779213], 
reward next is 0.7885, 
noisyNet noise sample is [array([-1.1930281], dtype=float32), -0.97225904]. 
=============================================
[2019-04-04 06:34:50,006] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 06:34:50,007] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:34:50,008] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:34:50,012] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:34:50,012] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:34:50,012] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:34:50,014] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:34:50,018] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run14
[2019-04-04 06:34:50,057] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run14
[2019-04-04 06:34:50,089] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run14
[2019-04-04 06:37:55,435] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 06:38:30,844] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 06:38:34,904] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 06:38:35,950] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1300000, evaluation results [1300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 06:38:39,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1729609e-10 1.8609396e-09 1.2160365e-26 8.3136373e-11 1.3890010e-11
 9.5377504e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:38:39,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6028
[2019-04-04 06:38:39,871] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 35.83333333333334, 0.0, 0.0, 26.0, 24.14566163862146, 0.2838368669286251, 1.0, 1.0, 199712.2689202693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2832600.0000, 
sim time next is 2833200.0000, 
raw observation next is [3.0, 37.0, 0.0, 0.0, 26.0, 24.55325106825936, 0.3453595654161898, 1.0, 1.0, 87638.92892547887], 
processed observation next is [1.0, 0.8260869565217391, 0.5457063711911359, 0.37, 0.0, 0.0, 0.6666666666666666, 0.54610425568828, 0.6151198551387299, 1.0, 1.0, 0.4173282329784708], 
reward next is 0.5827, 
noisyNet noise sample is [array([-0.70281583], dtype=float32), 0.31706798]. 
=============================================
[2019-04-04 06:38:52,584] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2922742e-10 1.0519269e-10 2.8584834e-26 5.0725409e-12 3.8599152e-11
 3.4390185e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:38:52,591] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3134
[2019-04-04 06:38:52,638] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 98.83333333333334, 0.0, 0.0, 26.0, 24.89310544857761, 0.2448947600178673, 0.0, 1.0, 55708.69950202957], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2872200.0000, 
sim time next is 2872800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 24.87903418373469, 0.2363941372649189, 0.0, 1.0, 55413.54071604238], 
processed observation next is [1.0, 0.2608695652173913, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5732528486445574, 0.578798045754973, 0.0, 1.0, 0.2638740034097256], 
reward next is 0.7361, 
noisyNet noise sample is [array([1.8114457], dtype=float32), 0.17443924]. 
=============================================
[2019-04-04 06:39:11,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7659083e-10 5.2956012e-10 5.2971013e-23 8.0019810e-11 8.8783592e-11
 6.0598316e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:11,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1888
[2019-04-04 06:39:11,510] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.96594331600538, 0.6042375385983327, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3260400.0000, 
sim time next is 3261000.0000, 
raw observation next is [-4.0, 67.0, 0.0, 0.0, 26.0, 25.76624255682317, 0.6012769703844792, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3518005540166205, 0.67, 0.0, 0.0, 0.6666666666666666, 0.6471868797352641, 0.7004256567948265, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2636453], dtype=float32), -1.3064907]. 
=============================================
[2019-04-04 06:39:11,530] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.41035]
 [80.53196]
 [81.46491]
 [82.42763]
 [83.26248]], R is [[79.15182495]
 [79.36030579]
 [79.5667038 ]
 [79.77103424]
 [79.97332764]].
[2019-04-04 06:39:18,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6096002e-11 1.5465709e-09 2.5510269e-24 7.0965595e-11 4.3674630e-11
 1.8529878e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:18,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9729
[2019-04-04 06:39:18,739] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.31237911510316, 0.4626773028093623, 1.0, 1.0, 196883.5567517166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3263400.0000, 
sim time next is 3264000.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.83275340069412, 0.5446232597151197, 1.0, 1.0, 198613.6809809116], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.56939611672451, 0.6815410865717065, 1.0, 1.0, 0.9457794332424362], 
reward next is 0.0542, 
noisyNet noise sample is [array([-1.1049998], dtype=float32), -0.50201035]. 
=============================================
[2019-04-04 06:39:18,749] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.04786]
 [82.87095]
 [81.65973]
 [81.65235]
 [81.69202]], R is [[84.41980743]
 [83.63806915]
 [82.8673172 ]
 [83.03864288]
 [83.20825958]].
[2019-04-04 06:39:19,018] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6229617e-11 3.3620898e-11 1.4106009e-25 8.5972392e-12 6.4836357e-12
 7.8783754e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:19,019] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9625
[2019-04-04 06:39:19,033] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 54.0, 118.0, 811.0, 26.0, 26.24688528366656, 0.5773641152130289, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3328200.0000, 
sim time next is 3328800.0000, 
raw observation next is [-5.333333333333333, 54.0, 117.3333333333333, 809.1666666666667, 26.0, 26.11310742709342, 0.5635354962624805, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.3148661126500462, 0.54, 0.391111111111111, 0.8941068139963169, 0.6666666666666666, 0.6760922855911184, 0.6878451654208267, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.07097355], dtype=float32), -0.19775464]. 
=============================================
[2019-04-04 06:39:21,930] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8951113e-11 3.0304797e-11 5.1097544e-26 4.2445687e-12 2.6011389e-12
 1.0805486e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:21,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1432
[2019-04-04 06:39:21,952] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.66666666666667, 114.0, 797.3333333333333, 26.0, 26.54381868157125, 0.640894424210953, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3410400.0000, 
sim time next is 3411000.0000, 
raw observation next is [3.0, 47.0, 115.0, 804.0, 26.0, 26.61142676651667, 0.4194531454810574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.47, 0.38333333333333336, 0.8883977900552487, 0.6666666666666666, 0.7176188972097224, 0.6398177151603525, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21802996], dtype=float32), -1.2502885]. 
=============================================
[2019-04-04 06:39:21,955] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[90.03349 ]
 [89.970924]
 [89.90286 ]
 [89.80603 ]
 [89.69038 ]], R is [[90.12644958]
 [90.22518921]
 [90.32293701]
 [90.41970825]
 [90.51551056]].
[2019-04-04 06:39:32,143] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.56604362e-11 1.56070795e-10 2.67004903e-24 2.89199064e-11
 1.43417275e-11 1.94179606e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:39:32,146] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0652
[2019-04-04 06:39:32,180] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 51.33333333333333, 112.6666666666667, 792.0, 26.0, 26.08313680386637, 0.5901574658026384, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3332400.0000, 
sim time next is 3333000.0000, 
raw observation next is [-4.166666666666667, 50.66666666666667, 111.3333333333333, 784.0, 26.0, 26.15852050110319, 0.4926808057770867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3471837488457987, 0.5066666666666667, 0.371111111111111, 0.8662983425414365, 0.6666666666666666, 0.6798767084252658, 0.664226935259029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2879276], dtype=float32), -0.29315698]. 
=============================================
[2019-04-04 06:39:32,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[85.16256 ]
 [85.184814]
 [85.1881  ]
 [85.22198 ]
 [85.26473 ]], R is [[85.240448  ]
 [85.38804626]
 [85.53416443]
 [85.67882538]
 [85.82203674]].
[2019-04-04 06:39:36,396] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5637703e-11 2.1388358e-11 1.9283045e-26 4.3983328e-12 7.9174601e-12
 3.1729664e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:36,397] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6131
[2019-04-04 06:39:36,424] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 59.0, 115.0, 810.0, 26.0, 26.31439222117447, 0.6162053536616964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3497400.0000, 
sim time next is 3498000.0000, 
raw observation next is [1.666666666666667, 58.33333333333334, 115.1666666666667, 812.1666666666666, 26.0, 26.31595385589979, 0.6223694682622131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5087719298245615, 0.5833333333333335, 0.383888888888889, 0.8974217311233885, 0.6666666666666666, 0.6929961546583158, 0.7074564894207377, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43171722], dtype=float32), -0.65439355]. 
=============================================
[2019-04-04 06:39:36,434] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[93.02474 ]
 [92.970985]
 [92.955635]
 [92.975075]
 [93.170166]], R is [[93.12475586]
 [93.19351196]
 [93.26158142]
 [93.32896423]
 [93.39567566]].
[2019-04-04 06:39:40,016] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5091591e-12 7.3455764e-12 1.4904535e-26 6.4682417e-13 8.2240313e-13
 3.8430864e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:40,019] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7319
[2019-04-04 06:39:40,027] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 97.33333333333334, 749.6666666666667, 26.0, 26.74344988119462, 0.7223304576243632, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3509400.0000, 
sim time next is 3510000.0000, 
raw observation next is [3.0, 49.0, 95.0, 734.0, 26.0, 26.75388603798483, 0.7315900900094028, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.31666666666666665, 0.8110497237569061, 0.6666666666666666, 0.7294905031654025, 0.7438633633364676, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4692956], dtype=float32), -0.14233755]. 
=============================================
[2019-04-04 06:39:40,039] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[90.93109 ]
 [90.986946]
 [91.01356 ]
 [91.01858 ]
 [91.07233 ]], R is [[90.94213867]
 [91.03271484]
 [91.12239075]
 [91.21116638]
 [91.29905701]].
[2019-04-04 06:39:47,669] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0473158e-09 7.2772793e-10 1.7883028e-25 5.7811633e-10 8.0560246e-11
 1.7779211e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:47,669] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0496
[2019-04-04 06:39:47,682] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47446365088431, 0.3782090915202834, 0.0, 1.0, 36729.34325316867], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3640200.0000, 
sim time next is 3640800.0000, 
raw observation next is [8.133333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.58807004854777, 0.3783744400713804, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.687903970452447, 0.2766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6323391707123142, 0.6261248133571268, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00453163], dtype=float32), 2.0784283]. 
=============================================
[2019-04-04 06:39:48,467] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9306730e-09 1.0887568e-09 2.7018306e-25 4.7324183e-10 4.3892698e-10
 6.7685560e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:48,478] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2892
[2019-04-04 06:39:48,491] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50924389374303, 0.3692698015499326, 0.0, 1.0, 24862.89220955169], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3636000.0000, 
sim time next is 3636600.0000, 
raw observation next is [8.866666666666667, 25.33333333333334, 0.0, 0.0, 26.0, 25.51344138848265, 0.3670664117034373, 0.0, 1.0, 25407.70748844287], 
processed observation next is [0.0, 0.08695652173913043, 0.7082179132040629, 0.2533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6261201157068875, 0.6223554705678124, 0.0, 1.0, 0.12098908327829938], 
reward next is 0.8790, 
noisyNet noise sample is [array([-1.0038105], dtype=float32), 0.11523074]. 
=============================================
[2019-04-04 06:39:49,555] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9224971e-09 6.8222290e-09 3.5512509e-22 9.6821262e-10 2.1028930e-09
 1.3542077e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:39:49,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8575
[2019-04-04 06:39:49,664] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.166666666666667, 63.0, 0.0, 0.0, 26.0, 24.64138884229653, 0.2364954928161705, 0.0, 1.0, 42752.44652320597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3913800.0000, 
sim time next is 3914400.0000, 
raw observation next is [-7.333333333333334, 62.0, 5.0, 135.8333333333333, 26.0, 24.63974944397295, 0.3510473397003071, 1.0, 1.0, 202408.5437745771], 
processed observation next is [1.0, 0.30434782608695654, 0.2594644506001847, 0.62, 0.016666666666666666, 0.1500920810313075, 0.6666666666666666, 0.5533124536644124, 0.6170157799001024, 1.0, 1.0, 0.9638502084503671], 
reward next is 0.0361, 
noisyNet noise sample is [array([0.51446587], dtype=float32), 1.7237623]. 
=============================================
[2019-04-04 06:40:13,106] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8232455e-10 1.0208862e-09 1.0318331e-24 2.7811220e-10 9.7234644e-11
 8.6771009e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:13,106] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2260
[2019-04-04 06:40:13,125] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 46.33333333333334, 0.0, 0.0, 26.0, 25.3899692348828, 0.3275504737406975, 0.0, 1.0, 32805.49659303593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4252800.0000, 
sim time next is 4253400.0000, 
raw observation next is [3.0, 47.0, 0.0, 0.0, 26.0, 25.38630074438007, 0.325038240861362, 0.0, 1.0, 38083.08412038348], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6155250620316725, 0.6083460802871207, 0.0, 1.0, 0.1813480196208737], 
reward next is 0.8187, 
noisyNet noise sample is [array([0.32658526], dtype=float32), -1.7052188]. 
=============================================
[2019-04-04 06:40:14,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5872946e-10 1.8538383e-11 1.0895373e-25 5.3292969e-12 3.9274417e-12
 5.8720208e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:14,138] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3699
[2019-04-04 06:40:14,145] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.4, 62.0, 24.0, 228.0, 26.0, 25.48143270033765, 0.4085985728612342, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4296600.0000, 
sim time next is 4297200.0000, 
raw observation next is [6.333333333333333, 62.66666666666667, 20.0, 190.0, 26.0, 25.43817077973623, 0.394429176408117, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.6380424746075716, 0.6266666666666667, 0.06666666666666667, 0.20994475138121546, 0.6666666666666666, 0.6198475649780191, 0.631476392136039, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6680543], dtype=float32), 1.7714301]. 
=============================================
[2019-04-04 06:40:17,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9196373e-10 4.3744930e-10 2.1485968e-25 7.5192734e-11 1.5245622e-11
 5.6310012e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:17,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3021
[2019-04-04 06:40:17,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.033333333333333, 73.66666666666667, 0.0, 0.0, 26.0, 25.41277053232727, 0.3583224584454687, 0.0, 1.0, 85519.45668198958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4344000.0000, 
sim time next is 4344600.0000, 
raw observation next is [2.966666666666666, 74.33333333333333, 0.0, 0.0, 26.0, 25.40052083562912, 0.365163275971975, 0.0, 1.0, 67023.3162507906], 
processed observation next is [1.0, 0.2608695652173913, 0.5447830101569714, 0.7433333333333333, 0.0, 0.0, 0.6666666666666666, 0.61671006963576, 0.6217210919906583, 0.0, 1.0, 0.31915864881328854], 
reward next is 0.6808, 
noisyNet noise sample is [array([-0.45809084], dtype=float32), -1.1791893]. 
=============================================
[2019-04-04 06:40:21,009] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.7414537e-10 1.4627194e-10 1.5091411e-23 7.4805648e-11 6.1277303e-11
 1.7194601e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:21,009] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9783
[2019-04-04 06:40:21,017] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.95, 40.16666666666667, 38.66666666666666, 292.0, 26.0, 25.37686266243526, 0.3884382849650432, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4209000.0000, 
sim time next is 4209600.0000, 
raw observation next is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 26.0, 25.29239307723198, 0.3646874301721719, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.515235457063712, 0.40333333333333343, 0.10444444444444442, 0.2513812154696133, 0.6666666666666666, 0.6076994231026651, 0.6215624767240573, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68172234], dtype=float32), 1.0377076]. 
=============================================
[2019-04-04 06:40:31,039] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3915082e-09 1.6426901e-09 4.2838691e-23 2.3478883e-10 8.9195412e-10
 3.6197852e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:31,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5431
[2019-04-04 06:40:31,057] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 25.29566456900705, 0.3971516571523019, 0.0, 1.0, 37655.96869173537], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4596000.0000, 
sim time next is 4596600.0000, 
raw observation next is [-1.916666666666667, 70.66666666666667, 0.0, 0.0, 26.0, 25.28316720921138, 0.3959487888137015, 0.0, 1.0, 36614.00606938427], 
processed observation next is [1.0, 0.17391304347826086, 0.4095106186518929, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.6069306007676151, 0.6319829296045671, 0.0, 1.0, 0.1743524098542108], 
reward next is 0.8256, 
noisyNet noise sample is [array([-0.27541545], dtype=float32), -0.032563925]. 
=============================================
[2019-04-04 06:40:31,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.9282266e-09 2.1433793e-09 1.4062242e-22 4.8772153e-10 2.9482907e-09
 8.3341606e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:31,963] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4404
[2019-04-04 06:40:32,015] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.50678810841445, 0.5399563255586105, 0.0, 1.0, 57598.37979708827], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4485600.0000, 
sim time next is 4486200.0000, 
raw observation next is [-0.05, 72.0, 0.0, 0.0, 26.0, 25.46504218643415, 0.4939283943486331, 0.0, 1.0, 71001.07012544415], 
processed observation next is [1.0, 0.9565217391304348, 0.461218836565097, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6220868488695125, 0.6646427981162111, 0.0, 1.0, 0.33810033393068645], 
reward next is 0.6619, 
noisyNet noise sample is [array([-1.622198], dtype=float32), -1.0697783]. 
=============================================
[2019-04-04 06:40:44,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0443491e-09 1.5297719e-09 6.0708407e-23 1.9398334e-10 3.7367029e-10
 2.5528726e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:44,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7787
[2019-04-04 06:40:44,856] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 81.5, 0.0, 0.0, 26.0, 25.00010321736327, 0.4790111191039538, 0.0, 1.0, 131664.7001740243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4739400.0000, 
sim time next is 4740000.0000, 
raw observation next is [-1.666666666666667, 82.66666666666667, 0.0, 0.0, 26.0, 25.09965711197946, 0.5021835011278694, 0.0, 1.0, 73892.41417245126], 
processed observation next is [1.0, 0.8695652173913043, 0.4164358264081256, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.591638092664955, 0.6673945003759565, 0.0, 1.0, 0.35186863891643455], 
reward next is 0.6481, 
noisyNet noise sample is [array([-0.74303997], dtype=float32), -0.02608616]. 
=============================================
[2019-04-04 06:40:44,876] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.51882]
 [79.36571]
 [79.70083]
 [81.69839]
 [84.53776]], R is [[79.54516602]
 [79.1227417 ]
 [78.38169861]
 [78.1129837 ]
 [77.93742371]].
[2019-04-04 06:40:55,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3761123e-10 2.3416313e-10 1.5917234e-22 4.3632853e-10 1.9618350e-10
 4.9017626e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:40:55,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7912
[2019-04-04 06:40:55,897] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 79.00000000000001, 273.6666666666667, 26.0, 25.17829018794109, 0.3492753189653137, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4899000.0000, 
sim time next is 4899600.0000, 
raw observation next is [3.0, 45.0, 67.5, 252.0, 26.0, 25.14214810492108, 0.339012537510585, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.7391304347826086, 0.5457063711911359, 0.45, 0.225, 0.27845303867403315, 0.6666666666666666, 0.5951790087434233, 0.613004179170195, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.3642526], dtype=float32), -0.34086463]. 
=============================================
[2019-04-04 06:40:57,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:40:57,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:40:57,483] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run11
[2019-04-04 06:40:58,479] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:40:58,480] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:40:58,492] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run11
[2019-04-04 06:40:59,882] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:40:59,883] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:40:59,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run11
[2019-04-04 06:40:59,999] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:00,000] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:00,006] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run11
[2019-04-04 06:41:00,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:00,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:00,937] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run11
[2019-04-04 06:41:01,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:01,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:01,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run11
[2019-04-04 06:41:05,352] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85000, global step 1357828: loss 0.0613
[2019-04-04 06:41:05,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85000, global step 1357828: learning rate 0.0000
[2019-04-04 06:41:06,584] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85000, global step 1358209: loss 0.0487
[2019-04-04 06:41:06,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85000, global step 1358209: learning rate 0.0000
[2019-04-04 06:41:07,560] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:07,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:07,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run11
[2019-04-04 06:41:07,909] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85000, global step 1358692: loss 0.0508
[2019-04-04 06:41:07,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85000, global step 1358692: learning rate 0.0000
[2019-04-04 06:41:08,119] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85000, global step 1358777: loss 0.0444
[2019-04-04 06:41:08,120] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85000, global step 1358777: learning rate 0.0000
[2019-04-04 06:41:08,596] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85000, global step 1358963: loss 0.0380
[2019-04-04 06:41:08,599] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85000, global step 1358965: learning rate 0.0000
[2019-04-04 06:41:09,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:09,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:09,067] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run11
[2019-04-04 06:41:09,230] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85000, global step 1359178: loss 0.0343
[2019-04-04 06:41:09,261] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85000, global step 1359178: learning rate 0.0000
[2019-04-04 06:41:09,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:09,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:09,471] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run11
[2019-04-04 06:41:10,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:10,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:10,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run11
[2019-04-04 06:41:11,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:11,405] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:11,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run11
[2019-04-04 06:41:12,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:12,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:12,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run11
[2019-04-04 06:41:12,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0934022e-12 6.1808189e-12 9.8352145e-29 7.0018318e-13 6.0450898e-14
 1.9815424e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:12,775] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8203
[2019-04-04 06:41:12,808] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 26.0, 28.86685249552239, 1.175828408824559, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5073000.0000, 
sim time next is 5073600.0000, 
raw observation next is [11.66666666666667, 17.0, 42.66666666666666, 340.8333333333333, 26.0, 29.05810252422319, 0.9868706874026586, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.785780240073869, 0.17, 0.1422222222222222, 0.3766114180478821, 0.6666666666666666, 0.9215085436852659, 0.8289568958008862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36189595], dtype=float32), 0.44400257]. 
=============================================
[2019-04-04 06:41:12,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:12,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:12,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run11
[2019-04-04 06:41:14,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:14,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:14,056] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run11
[2019-04-04 06:41:14,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.5814574e-09 2.4117697e-09 2.3180905e-23 2.4702007e-10 7.6930712e-10
 1.3069273e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:14,411] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8963
[2019-04-04 06:41:14,438] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.516666666666667, 91.0, 0.0, 0.0, 26.0, 24.25385380400035, 0.1254378336943152, 0.0, 1.0, 41816.43060319208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 93000.0000, 
sim time next is 93600.0000, 
raw observation next is [-1.7, 91.0, 0.0, 0.0, 26.0, 24.24658010880745, 0.1239493097257282, 0.0, 1.0, 41971.55930925649], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5205483424006209, 0.5413164365752428, 0.0, 1.0, 0.1998645681393166], 
reward next is 0.8001, 
noisyNet noise sample is [array([-0.42139775], dtype=float32), -0.9598022]. 
=============================================
[2019-04-04 06:41:15,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:15,593] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:15,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run11
[2019-04-04 06:41:15,969] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85000, global step 1360479: loss 0.0524
[2019-04-04 06:41:15,969] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85000, global step 1360479: learning rate 0.0000
[2019-04-04 06:41:16,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:41:16,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:41:16,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run11
[2019-04-04 06:41:17,753] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85000, global step 1360739: loss 0.0612
[2019-04-04 06:41:17,801] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85000, global step 1360742: learning rate 0.0000
[2019-04-04 06:41:17,883] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85000, global step 1360751: loss 0.0572
[2019-04-04 06:41:17,893] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85000, global step 1360751: learning rate 0.0000
[2019-04-04 06:41:19,326] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85000, global step 1360933: loss 0.0388
[2019-04-04 06:41:19,327] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85000, global step 1360933: learning rate 0.0000
[2019-04-04 06:41:21,101] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85000, global step 1361236: loss 0.0334
[2019-04-04 06:41:21,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85000, global step 1361236: learning rate 0.0000
[2019-04-04 06:41:21,565] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85000, global step 1361324: loss 0.0398
[2019-04-04 06:41:21,565] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85000, global step 1361324: learning rate 0.0000
[2019-04-04 06:41:22,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3476329e-10 1.4648190e-10 3.3402996e-26 2.7799487e-11 6.1881797e-12
 1.0354739e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:22,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2489
[2019-04-04 06:41:22,158] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.9, 90.66666666666667, 92.5, 0.0, 26.0, 24.31302030504014, 0.1092863644306539, 0.0, 1.0, 35410.2030073198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [8.0, 89.5, 96.0, 0.0, 26.0, 24.33680959862573, 0.1155099110610531, 0.0, 1.0, 24220.52529228139], 
processed observation next is [0.0, 0.5217391304347826, 0.6842105263157896, 0.895, 0.32, 0.0, 0.6666666666666666, 0.5280674665521442, 0.5385033036870177, 0.0, 1.0, 0.11533583472514947], 
reward next is 0.8847, 
noisyNet noise sample is [array([0.89111024], dtype=float32), 0.2696598]. 
=============================================
[2019-04-04 06:41:22,163] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.77263]
 [87.68861]
 [87.58696]
 [87.50007]
 [87.47894]], R is [[87.80207062]
 [87.75543213]
 [87.69142151]
 [87.64445496]
 [87.65722656]].
[2019-04-04 06:41:22,437] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85000, global step 1361521: loss 0.0321
[2019-04-04 06:41:22,438] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85000, global step 1361521: learning rate 0.0000
[2019-04-04 06:41:23,252] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85000, global step 1361685: loss 0.0359
[2019-04-04 06:41:23,253] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85000, global step 1361685: learning rate 0.0000
[2019-04-04 06:41:24,807] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85000, global step 1362108: loss 0.0204
[2019-04-04 06:41:24,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85000, global step 1362111: learning rate 0.0000
[2019-04-04 06:41:25,479] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85000, global step 1362372: loss 0.0180
[2019-04-04 06:41:25,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85000, global step 1362372: learning rate 0.0000
[2019-04-04 06:41:30,562] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.2478296e-10 7.8629442e-10 4.5160068e-25 2.1526661e-10 1.3592603e-10
 3.6143384e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:30,562] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1900
[2019-04-04 06:41:30,612] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 89.0, 0.0, 0.0, 26.0, 24.5805628082141, 0.2031754109643814, 0.0, 1.0, 40458.85387966829], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 64800.0000, 
sim time next is 65400.0000, 
raw observation next is [4.300000000000001, 88.5, 0.0, 0.0, 26.0, 24.59167654511074, 0.2044933421061403, 0.0, 1.0, 36781.64338993765], 
processed observation next is [0.0, 0.782608695652174, 0.5817174515235458, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5493063787592284, 0.5681644473687134, 0.0, 1.0, 0.1751506828092269], 
reward next is 0.8248, 
noisyNet noise sample is [array([0.5428117], dtype=float32), -0.48624107]. 
=============================================
[2019-04-04 06:41:30,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5813929e-09 1.2135105e-08 6.0990464e-22 2.4075630e-10 1.1964155e-09
 6.0989378e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:30,777] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3170
[2019-04-04 06:41:30,820] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.483333333333333, 66.66666666666667, 0.0, 0.0, 26.0, 24.73134698482353, 0.2137034001661745, 0.0, 1.0, 44754.75766822971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 249000.0000, 
sim time next is 249600.0000, 
raw observation next is [-3.566666666666667, 68.33333333333334, 0.0, 0.0, 26.0, 24.67922968918035, 0.2038219626555708, 0.0, 1.0, 44714.14158718628], 
processed observation next is [1.0, 0.9130434782608695, 0.3638042474607572, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5566024740983625, 0.5679406542185236, 0.0, 1.0, 0.2129244837485061], 
reward next is 0.7871, 
noisyNet noise sample is [array([-0.27936623], dtype=float32), 1.6148707]. 
=============================================
[2019-04-04 06:41:31,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2228887e-11 8.9050364e-11 1.0472554e-23 1.0914571e-10 1.1857099e-11
 5.2245117e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:31,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4647
[2019-04-04 06:41:31,786] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.15283085441152, 0.2601125162460585, 1.0, 1.0, 20477.29346676412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 238200.0000, 
sim time next is 238800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.15626221264526, 0.2234963997485702, 1.0, 1.0, 26339.56838424996], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.596355184387105, 0.57449879991619, 1.0, 1.0, 0.125426516115476], 
reward next is 0.8746, 
noisyNet noise sample is [array([-1.22454], dtype=float32), 0.03365377]. 
=============================================
[2019-04-04 06:41:33,855] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2395795e-09 4.2016847e-08 6.4456686e-20 6.0567040e-09 4.2794244e-09
 8.8404062e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:33,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5060
[2019-04-04 06:41:33,871] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.13294418838927, 0.09929121173859172, 0.0, 1.0, 44962.67839370439], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 168600.0000, 
sim time next is 169200.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 24.08626046875003, 0.08904929802124084, 0.0, 1.0, 44914.4591407251], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5071883723958358, 0.5296830993404136, 0.0, 1.0, 0.21387837686059571], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.30610564], dtype=float32), -0.38961232]. 
=============================================
[2019-04-04 06:41:36,777] A3C_AGENT_WORKER-Thread-2 INFO:Local step 85500, global step 1365287: loss 0.2767
[2019-04-04 06:41:36,779] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 85500, global step 1365287: learning rate 0.0000
[2019-04-04 06:41:38,601] A3C_AGENT_WORKER-Thread-18 INFO:Local step 85500, global step 1365689: loss 0.2552
[2019-04-04 06:41:38,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 85500, global step 1365689: learning rate 0.0000
[2019-04-04 06:41:40,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0335554e-10 1.5549682e-09 6.1040676e-24 1.6117466e-10 3.2949109e-11
 5.9691496e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:40,052] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0400
[2019-04-04 06:41:40,132] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.76510273051029, 0.2363001808258915, 0.0, 1.0, 167882.4920124775], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 244200.0000, 
sim time next is 244800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.78058751770632, 0.260012827471464, 0.0, 1.0, 96692.4636488201], 
processed observation next is [1.0, 0.8695652173913043, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5650489598088599, 0.586670942490488, 0.0, 1.0, 0.46044030308961953], 
reward next is 0.5396, 
noisyNet noise sample is [array([0.11383588], dtype=float32), 0.27804726]. 
=============================================
[2019-04-04 06:41:40,803] A3C_AGENT_WORKER-Thread-20 INFO:Local step 85500, global step 1366210: loss 0.2980
[2019-04-04 06:41:40,805] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 85500, global step 1366210: learning rate 0.0000
[2019-04-04 06:41:40,943] A3C_AGENT_WORKER-Thread-12 INFO:Local step 85500, global step 1366253: loss 0.2707
[2019-04-04 06:41:40,944] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 85500, global step 1366253: learning rate 0.0000
[2019-04-04 06:41:41,091] A3C_AGENT_WORKER-Thread-16 INFO:Local step 85500, global step 1366297: loss 0.2807
[2019-04-04 06:41:41,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 85500, global step 1366297: learning rate 0.0000
[2019-04-04 06:41:41,411] A3C_AGENT_WORKER-Thread-5 INFO:Local step 85500, global step 1366384: loss 0.2638
[2019-04-04 06:41:41,412] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 85500, global step 1366384: learning rate 0.0000
[2019-04-04 06:41:48,511] A3C_AGENT_WORKER-Thread-19 INFO:Local step 85500, global step 1368151: loss 0.3860
[2019-04-04 06:41:48,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 85500, global step 1368151: learning rate 0.0000
[2019-04-04 06:41:49,615] A3C_AGENT_WORKER-Thread-15 INFO:Local step 85500, global step 1368423: loss 0.4383
[2019-04-04 06:41:49,616] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 85500, global step 1368423: learning rate 0.0000
[2019-04-04 06:41:49,946] A3C_AGENT_WORKER-Thread-17 INFO:Local step 85500, global step 1368515: loss 0.4504
[2019-04-04 06:41:49,947] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 85500, global step 1368515: learning rate 0.0000
[2019-04-04 06:41:50,314] A3C_AGENT_WORKER-Thread-3 INFO:Local step 85500, global step 1368616: loss 0.4890
[2019-04-04 06:41:50,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 85500, global step 1368616: learning rate 0.0000
[2019-04-04 06:41:51,348] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.2686880e-09 1.4957356e-08 5.4207387e-21 4.8957274e-09 2.1371538e-09
 2.9251376e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 06:41:51,348] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5840
[2019-04-04 06:41:51,391] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 70.0, 0.0, 0.0, 26.0, 23.41080842201293, -0.09330872821628405, 0.0, 1.0, 46524.46015133633], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 273600.0000, 
sim time next is 274200.0000, 
raw observation next is [-9.683333333333334, 69.5, 0.0, 0.0, 26.0, 23.3323253809277, -0.1032875480473283, 0.0, 1.0, 46717.53386556815], 
processed observation next is [1.0, 0.17391304347826086, 0.19436749769159742, 0.695, 0.0, 0.0, 0.6666666666666666, 0.4443604484106416, 0.46557081731755723, 0.0, 1.0, 0.22246444697889595], 
reward next is 0.7775, 
noisyNet noise sample is [array([-0.2274979], dtype=float32), -0.21759735]. 
=============================================
[2019-04-04 06:41:52,683] A3C_AGENT_WORKER-Thread-14 INFO:Local step 85500, global step 1369272: loss 0.5247
[2019-04-04 06:41:52,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 85500, global step 1369272: learning rate 0.0000
[2019-04-04 06:41:53,142] A3C_AGENT_WORKER-Thread-13 INFO:Local step 85500, global step 1369417: loss 0.5337
[2019-04-04 06:41:53,148] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 85500, global step 1369417: learning rate 0.0000
[2019-04-04 06:41:54,068] A3C_AGENT_WORKER-Thread-4 INFO:Local step 85500, global step 1369683: loss 0.5079
[2019-04-04 06:41:54,099] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 85500, global step 1369683: learning rate 0.0000
[2019-04-04 06:41:55,937] A3C_AGENT_WORKER-Thread-6 INFO:Local step 85500, global step 1370071: loss 0.5022
[2019-04-04 06:41:55,937] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 85500, global step 1370071: learning rate 0.0000
[2019-04-04 06:41:56,777] A3C_AGENT_WORKER-Thread-11 INFO:Local step 85500, global step 1370286: loss 0.4933
[2019-04-04 06:41:56,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 85500, global step 1370286: learning rate 0.0000
[2019-04-04 06:41:57,329] A3C_AGENT_WORKER-Thread-10 INFO:Local step 85500, global step 1370416: loss 0.5191
[2019-04-04 06:41:57,330] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 85500, global step 1370416: learning rate 0.0000
[2019-04-04 06:42:00,865] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1338886e-10 6.6724815e-10 1.9193888e-25 3.6047467e-11 8.3305418e-11
 5.1086721e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:42:00,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5703
[2019-04-04 06:42:00,906] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.633333333333333, 96.66666666666666, 0.0, 0.0, 26.0, 24.84276599367649, 0.2405629281880052, 0.0, 1.0, 39923.88670582083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 517200.0000, 
sim time next is 517800.0000, 
raw observation next is [3.716666666666666, 96.83333333333334, 0.0, 0.0, 26.0, 24.88396069463973, 0.2412304247799812, 0.0, 1.0, 39846.87247087662], 
processed observation next is [1.0, 1.0, 0.5655586334256695, 0.9683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5736633912199774, 0.580410141593327, 0.0, 1.0, 0.18974701176607914], 
reward next is 0.8103, 
noisyNet noise sample is [array([1.2057824], dtype=float32), 0.14716573]. 
=============================================
[2019-04-04 06:42:05,782] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86000, global step 1372748: loss 0.4523
[2019-04-04 06:42:05,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86000, global step 1372748: learning rate 0.0000
[2019-04-04 06:42:06,931] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86000, global step 1373054: loss 0.4984
[2019-04-04 06:42:06,990] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86000, global step 1373059: learning rate 0.0000
[2019-04-04 06:42:09,330] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86000, global step 1373859: loss 0.5193
[2019-04-04 06:42:09,331] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86000, global step 1373859: learning rate 0.0000
[2019-04-04 06:42:09,553] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86000, global step 1373938: loss 0.4626
[2019-04-04 06:42:09,554] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86000, global step 1373938: learning rate 0.0000
[2019-04-04 06:42:09,732] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86000, global step 1373993: loss 0.4813
[2019-04-04 06:42:09,744] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86000, global step 1373993: learning rate 0.0000
[2019-04-04 06:42:10,058] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86000, global step 1374102: loss 0.4552
[2019-04-04 06:42:10,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86000, global step 1374104: learning rate 0.0000
[2019-04-04 06:42:16,300] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86000, global step 1375899: loss 0.4376
[2019-04-04 06:42:16,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86000, global step 1375900: learning rate 0.0000
[2019-04-04 06:42:17,673] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86000, global step 1376456: loss 0.3743
[2019-04-04 06:42:17,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86000, global step 1376456: learning rate 0.0000
[2019-04-04 06:42:17,712] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86000, global step 1376467: loss 0.3896
[2019-04-04 06:42:17,714] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86000, global step 1376467: learning rate 0.0000
[2019-04-04 06:42:18,452] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86000, global step 1376658: loss 0.3822
[2019-04-04 06:42:18,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86000, global step 1376658: learning rate 0.0000
[2019-04-04 06:42:20,676] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86000, global step 1377351: loss 0.3136
[2019-04-04 06:42:20,681] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86000, global step 1377351: learning rate 0.0000
[2019-04-04 06:42:21,079] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86000, global step 1377466: loss 0.3210
[2019-04-04 06:42:21,101] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86000, global step 1377466: learning rate 0.0000
[2019-04-04 06:42:21,184] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86000, global step 1377493: loss 0.3444
[2019-04-04 06:42:21,187] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86000, global step 1377493: learning rate 0.0000
[2019-04-04 06:42:23,658] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86000, global step 1378270: loss 0.3527
[2019-04-04 06:42:23,668] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86000, global step 1378276: learning rate 0.0000
[2019-04-04 06:42:23,732] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86000, global step 1378306: loss 0.3328
[2019-04-04 06:42:23,746] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86000, global step 1378309: learning rate 0.0000
[2019-04-04 06:42:24,000] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.6303542e-10 9.7074593e-10 1.6139231e-23 9.8003806e-11 1.3938585e-10
 2.0698126e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:42:24,002] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3892
[2019-04-04 06:42:24,012] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86000, global step 1378421: loss 0.3294
[2019-04-04 06:42:24,013] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86000, global step 1378421: learning rate 0.0000
[2019-04-04 06:42:24,058] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 59.0, 147.0, 96.5, 26.0, 24.89957174885221, 0.2384947708124454, 0.0, 1.0, 35235.9469115984], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 651600.0000, 
sim time next is 652200.0000, 
raw observation next is [-2.116666666666667, 59.16666666666667, 158.6666666666667, 95.33333333333334, 26.0, 24.90796154084256, 0.2411389744377384, 0.0, 1.0, 30024.64238288365], 
processed observation next is [0.0, 0.5652173913043478, 0.40397045244690677, 0.5916666666666667, 0.5288888888888891, 0.10534069981583795, 0.6666666666666666, 0.57566346173688, 0.5803796581459127, 0.0, 1.0, 0.1429744875375412], 
reward next is 0.8570, 
noisyNet noise sample is [array([-2.553264], dtype=float32), 1.3000606]. 
=============================================
[2019-04-04 06:42:24,127] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1105074e-09 6.3651449e-09 6.2747213e-22 8.4020185e-10 1.3378815e-09
 2.0974578e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:42:24,128] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0697
[2019-04-04 06:42:24,158] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 73.5, 0.0, 0.0, 26.0, 23.53173753858626, -0.0640618500852324, 0.0, 1.0, 43922.24593437873], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 631800.0000, 
sim time next is 632400.0000, 
raw observation next is [-4.5, 75.33333333333333, 0.0, 0.0, 26.0, 23.50690790829686, -0.07007529894353944, 0.0, 1.0, 43947.62732059176], 
processed observation next is [0.0, 0.30434782608695654, 0.3379501385041552, 0.7533333333333333, 0.0, 0.0, 0.6666666666666666, 0.45890899235807164, 0.4766415670188202, 0.0, 1.0, 0.2092744158123417], 
reward next is 0.7907, 
noisyNet noise sample is [array([1.341087], dtype=float32), -0.9749185]. 
=============================================
[2019-04-04 06:42:31,131] A3C_AGENT_WORKER-Thread-2 INFO:Local step 86500, global step 1380792: loss 0.0321
[2019-04-04 06:42:31,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 86500, global step 1380792: learning rate 0.0000
[2019-04-04 06:42:32,023] A3C_AGENT_WORKER-Thread-18 INFO:Local step 86500, global step 1381113: loss 0.0299
[2019-04-04 06:42:32,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 86500, global step 1381114: learning rate 0.0000
[2019-04-04 06:42:33,850] A3C_AGENT_WORKER-Thread-16 INFO:Local step 86500, global step 1381755: loss 0.0255
[2019-04-04 06:42:33,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 86500, global step 1381756: learning rate 0.0000
[2019-04-04 06:42:34,756] A3C_AGENT_WORKER-Thread-12 INFO:Local step 86500, global step 1382085: loss 0.0202
[2019-04-04 06:42:34,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 86500, global step 1382085: learning rate 0.0000
[2019-04-04 06:42:35,101] A3C_AGENT_WORKER-Thread-5 INFO:Local step 86500, global step 1382182: loss 0.0107
[2019-04-04 06:42:35,102] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 86500, global step 1382182: learning rate 0.0000
[2019-04-04 06:42:35,127] A3C_AGENT_WORKER-Thread-20 INFO:Local step 86500, global step 1382189: loss 0.0150
[2019-04-04 06:42:35,127] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 86500, global step 1382189: learning rate 0.0000
[2019-04-04 06:42:40,309] A3C_AGENT_WORKER-Thread-19 INFO:Local step 86500, global step 1384298: loss 0.0032
[2019-04-04 06:42:40,314] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 86500, global step 1384299: learning rate 0.0000
[2019-04-04 06:42:41,338] A3C_AGENT_WORKER-Thread-15 INFO:Local step 86500, global step 1384825: loss 0.0088
[2019-04-04 06:42:41,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 86500, global step 1384825: learning rate 0.0000
[2019-04-04 06:42:41,964] A3C_AGENT_WORKER-Thread-3 INFO:Local step 86500, global step 1385139: loss 0.0064
[2019-04-04 06:42:41,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 86500, global step 1385139: learning rate 0.0000
[2019-04-04 06:42:42,303] A3C_AGENT_WORKER-Thread-17 INFO:Local step 86500, global step 1385294: loss 0.0080
[2019-04-04 06:42:42,308] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 86500, global step 1385294: learning rate 0.0000
[2019-04-04 06:42:42,803] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.0426078e-12 1.2704024e-12 1.0095174e-29 8.6137309e-13 2.5451980e-14
 1.3578770e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 06:42:42,807] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1674
[2019-04-04 06:42:42,816] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.73333333333333, 59.0, 179.3333333333333, 264.1666666666667, 26.0, 27.06157530267827, 0.860843552149818, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1082400.0000, 
sim time next is 1083000.0000, 
raw observation next is [18.01666666666667, 57.5, 177.6666666666667, 211.3333333333333, 26.0, 26.70058189723366, 0.8670106749837531, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9616805170821794, 0.575, 0.5922222222222224, 0.23351749539594838, 0.6666666666666666, 0.7250484914361385, 0.7890035583279177, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6420333], dtype=float32), 0.090977125]. 
=============================================
[2019-04-04 06:42:42,825] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[102.09487 ]
 [103.562584]
 [105.08741 ]
 [106.66175 ]
 [108.0821  ]], R is [[100.50251007]
 [100.4974823 ]
 [100.49250793]
 [100.48758698]
 [100.48271179]].
[2019-04-04 06:42:44,129] A3C_AGENT_WORKER-Thread-14 INFO:Local step 86500, global step 1386343: loss 0.0060
[2019-04-04 06:42:44,129] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 86500, global step 1386343: learning rate 0.0000
[2019-04-04 06:42:44,279] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87000, global step 1386441: loss 0.2561
[2019-04-04 06:42:44,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87000, global step 1386442: learning rate 0.0000
[2019-04-04 06:42:44,386] A3C_AGENT_WORKER-Thread-13 INFO:Local step 86500, global step 1386506: loss 0.0056
[2019-04-04 06:42:44,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 86500, global step 1386506: learning rate 0.0000
[2019-04-04 06:42:44,502] A3C_AGENT_WORKER-Thread-4 INFO:Local step 86500, global step 1386582: loss 0.0039
[2019-04-04 06:42:44,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 86500, global step 1386583: learning rate 0.0000
[2019-04-04 06:42:45,039] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87000, global step 1386871: loss 0.2597
[2019-04-04 06:42:45,042] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87000, global step 1386871: learning rate 0.0000
[2019-04-04 06:42:45,923] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.81089168e-11 1.57577371e-11 1.33944847e-25 3.39308304e-11
 1.02993855e-11 4.68322653e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 06:42:45,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5986
[2019-04-04 06:42:45,934] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 163.0, 0.0, 26.0, 25.06707635928555, 0.4990340065878165, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1170600.0000, 
sim time next is 1171200.0000, 
raw observation next is [18.3, 65.0, 161.0, 0.0, 26.0, 25.05552311625718, 0.4982244882622078, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.5366666666666666, 0.0, 0.6666666666666666, 0.5879602596880984, 0.6660748294207359, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7917415], dtype=float32), 1.2118243]. 
=============================================
[2019-04-04 06:42:46,663] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87000, global step 1387850: loss 0.2648
[2019-04-04 06:42:46,697] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87000, global step 1387870: learning rate 0.0000
[2019-04-04 06:42:46,946] A3C_AGENT_WORKER-Thread-6 INFO:Local step 86500, global step 1388034: loss 0.0007
[2019-04-04 06:42:46,948] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 86500, global step 1388034: learning rate 0.0000
[2019-04-04 06:42:47,055] A3C_AGENT_WORKER-Thread-10 INFO:Local step 86500, global step 1388107: loss 0.0012
[2019-04-04 06:42:47,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 86500, global step 1388107: learning rate 0.0000
[2019-04-04 06:42:47,102] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87000, global step 1388132: loss 0.2501
[2019-04-04 06:42:47,103] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87000, global step 1388133: learning rate 0.0000
[2019-04-04 06:42:47,114] A3C_AGENT_WORKER-Thread-11 INFO:Local step 86500, global step 1388141: loss 0.0015
[2019-04-04 06:42:47,116] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 86500, global step 1388141: learning rate 0.0000
[2019-04-04 06:42:47,391] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87000, global step 1388321: loss 0.2478
[2019-04-04 06:42:47,392] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87000, global step 1388321: learning rate 0.0000
[2019-04-04 06:42:48,008] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87000, global step 1388636: loss 0.2214
[2019-04-04 06:42:48,019] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87000, global step 1388641: learning rate 0.0000
[2019-04-04 06:42:53,218] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87000, global step 1391562: loss 0.3042
[2019-04-04 06:42:53,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87000, global step 1391563: learning rate 0.0000
[2019-04-04 06:42:53,780] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87000, global step 1391946: loss 0.2950
[2019-04-04 06:42:53,781] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87000, global step 1391947: learning rate 0.0000
[2019-04-04 06:42:55,065] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87000, global step 1392721: loss 0.3094
[2019-04-04 06:42:55,068] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87000, global step 1392722: learning rate 0.0000
[2019-04-04 06:42:55,246] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87000, global step 1392827: loss 0.2979
[2019-04-04 06:42:55,247] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87000, global step 1392827: learning rate 0.0000
[2019-04-04 06:42:56,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.5203614e-10 5.4832628e-11 1.4572606e-26 4.0425954e-11 3.2772299e-11
 6.3585243e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:42:56,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6163
[2019-04-04 06:42:56,230] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.45726126206696, 0.4259845133211693, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248600.0000, 
sim time next is 1249200.0000, 
raw observation next is [14.4, 100.0, 86.5, 0.0, 26.0, 24.70938387216763, 0.4496120431941544, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 1.0, 0.28833333333333333, 0.0, 0.6666666666666666, 0.5591153226806359, 0.6498706810647181, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9336835], dtype=float32), -1.0907458]. 
=============================================
[2019-04-04 06:42:56,797] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87000, global step 1393776: loss 0.2949
[2019-04-04 06:42:56,798] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87000, global step 1393777: learning rate 0.0000
[2019-04-04 06:42:57,319] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87000, global step 1394093: loss 0.2957
[2019-04-04 06:42:57,321] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87000, global step 1394093: learning rate 0.0000
[2019-04-04 06:42:57,450] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87000, global step 1394172: loss 0.2666
[2019-04-04 06:42:57,452] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87000, global step 1394172: learning rate 0.0000
[2019-04-04 06:43:02,709] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87000, global step 1395683: loss 0.2704
[2019-04-04 06:43:02,716] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87000, global step 1395683: learning rate 0.0000
[2019-04-04 06:43:02,967] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87000, global step 1395773: loss 0.2908
[2019-04-04 06:43:02,968] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87000, global step 1395773: learning rate 0.0000
[2019-04-04 06:43:03,014] A3C_AGENT_WORKER-Thread-2 INFO:Local step 87500, global step 1395785: loss 0.0313
[2019-04-04 06:43:03,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 87500, global step 1395785: learning rate 0.0000
[2019-04-04 06:43:03,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9900102e-10 5.9341754e-10 2.0731949e-24 1.2200028e-10 1.3056554e-10
 7.7822585e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:43:03,181] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87000, global step 1395838: loss 0.2745
[2019-04-04 06:43:03,184] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2872
[2019-04-04 06:43:03,202] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87000, global step 1395844: learning rate 0.0000
[2019-04-04 06:43:03,203] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.7, 92.0, 0.0, 0.0, 26.0, 25.55200904325831, 0.4861174752065753, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1473000.0000, 
sim time next is 1473600.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 26.0, 25.53233881240478, 0.4730685558888039, 0.0, 1.0, 18745.7086354989], 
processed observation next is [1.0, 0.043478260869565216, 0.5124653739612189, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6276949010337317, 0.6576895186296013, 0.0, 1.0, 0.08926527921666143], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.6299018], dtype=float32), 0.8691159]. 
=============================================
[2019-04-04 06:43:05,209] A3C_AGENT_WORKER-Thread-18 INFO:Local step 87500, global step 1396367: loss 0.0418
[2019-04-04 06:43:05,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 87500, global step 1396367: learning rate 0.0000
[2019-04-04 06:43:08,786] A3C_AGENT_WORKER-Thread-12 INFO:Local step 87500, global step 1397039: loss 0.0293
[2019-04-04 06:43:08,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 87500, global step 1397039: learning rate 0.0000
[2019-04-04 06:43:09,287] A3C_AGENT_WORKER-Thread-16 INFO:Local step 87500, global step 1397150: loss 0.0139
[2019-04-04 06:43:09,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 87500, global step 1397150: learning rate 0.0000
[2019-04-04 06:43:10,842] A3C_AGENT_WORKER-Thread-5 INFO:Local step 87500, global step 1397485: loss 0.0167
[2019-04-04 06:43:10,842] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 87500, global step 1397485: learning rate 0.0000
[2019-04-04 06:43:11,547] A3C_AGENT_WORKER-Thread-20 INFO:Local step 87500, global step 1397656: loss 0.0078
[2019-04-04 06:43:11,548] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 87500, global step 1397656: learning rate 0.0000
[2019-04-04 06:43:17,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9729583e-11 1.0677691e-10 2.7603688e-25 1.4043692e-11 7.1375922e-12
 8.4683352e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:43:17,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7293
[2019-04-04 06:43:17,807] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.83333333333333, 35.33333333333334, 0.0, 26.0, 25.69840150119893, 0.5183014194193509, 1.0, 1.0, 47307.2556761329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1353000.0000, 
sim time next is 1353600.0000, 
raw observation next is [1.1, 93.0, 31.0, 0.0, 26.0, 25.70597884838442, 0.5190710957447235, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.93, 0.10333333333333333, 0.0, 0.6666666666666666, 0.6421649040320349, 0.6730236985815745, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34480897], dtype=float32), -0.26070392]. 
=============================================
[2019-04-04 06:43:17,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5239088e-10 2.6189650e-09 4.4746474e-24 7.8529135e-11 8.9867336e-11
 1.0813997e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:43:17,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1754
[2019-04-04 06:43:17,865] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 90.0, 0.0, 0.0, 26.0, 25.10493229176598, 0.5231709872932774, 0.0, 1.0, 71120.44818159031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1456800.0000, 
sim time next is 1457400.0000, 
raw observation next is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.31118574703884, 0.5472547384332428, 0.0, 1.0, 50075.17392464632], 
processed observation next is [1.0, 0.8695652173913043, 0.5046168051708219, 0.895, 0.0, 0.0, 0.6666666666666666, 0.6092654789199035, 0.6824182461444143, 0.0, 1.0, 0.23845320916498247], 
reward next is 0.7615, 
noisyNet noise sample is [array([-0.5421037], dtype=float32), -0.9630154]. 
=============================================
[2019-04-04 06:43:20,970] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 06:43:20,974] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:43:20,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:43:20,974] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:43:20,984] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:43:20,986] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run15
[2019-04-04 06:43:20,974] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:43:21,014] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:43:21,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run15
[2019-04-04 06:43:21,068] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run15
[2019-04-04 06:46:31,475] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 06:46:58,655] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 06:47:04,456] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6735 275798561.6764 1233.0993
[2019-04-04 06:47:05,489] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 1400000, evaluation results [1400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.673515826508, 275798561.67643094, 1233.0993326628943]
[2019-04-04 06:47:05,656] A3C_AGENT_WORKER-Thread-19 INFO:Local step 87500, global step 1400065: loss 0.0243
[2019-04-04 06:47:05,656] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 87500, global step 1400065: learning rate 0.0000
[2019-04-04 06:47:06,830] A3C_AGENT_WORKER-Thread-15 INFO:Local step 87500, global step 1400453: loss 0.0182
[2019-04-04 06:47:06,831] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 87500, global step 1400453: learning rate 0.0000
[2019-04-04 06:47:08,263] A3C_AGENT_WORKER-Thread-17 INFO:Local step 87500, global step 1400854: loss 0.0224
[2019-04-04 06:47:08,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 87500, global step 1400854: learning rate 0.0000
[2019-04-04 06:47:08,528] A3C_AGENT_WORKER-Thread-3 INFO:Local step 87500, global step 1400952: loss 0.0145
[2019-04-04 06:47:08,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 87500, global step 1400952: learning rate 0.0000
[2019-04-04 06:47:11,347] A3C_AGENT_WORKER-Thread-14 INFO:Local step 87500, global step 1401782: loss 0.0029
[2019-04-04 06:47:11,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 87500, global step 1401782: learning rate 0.0000
[2019-04-04 06:47:11,406] A3C_AGENT_WORKER-Thread-13 INFO:Local step 87500, global step 1401801: loss 0.0041
[2019-04-04 06:47:11,445] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 87500, global step 1401801: learning rate 0.0000
[2019-04-04 06:47:14,234] A3C_AGENT_WORKER-Thread-4 INFO:Local step 87500, global step 1402576: loss 0.0105
[2019-04-04 06:47:14,259] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 87500, global step 1402576: learning rate 0.0000
[2019-04-04 06:47:19,020] A3C_AGENT_WORKER-Thread-10 INFO:Local step 87500, global step 1403933: loss 0.0045
[2019-04-04 06:47:19,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 87500, global step 1403933: learning rate 0.0000
[2019-04-04 06:47:19,162] A3C_AGENT_WORKER-Thread-6 INFO:Local step 87500, global step 1403968: loss 0.0039
[2019-04-04 06:47:19,162] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 87500, global step 1403968: learning rate 0.0000
[2019-04-04 06:47:19,362] A3C_AGENT_WORKER-Thread-11 INFO:Local step 87500, global step 1404007: loss 0.0042
[2019-04-04 06:47:19,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 87500, global step 1404007: learning rate 0.0000
[2019-04-04 06:47:19,752] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88000, global step 1404112: loss 0.0029
[2019-04-04 06:47:19,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88000, global step 1404112: learning rate 0.0000
[2019-04-04 06:47:21,485] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88000, global step 1404655: loss 0.0013
[2019-04-04 06:47:21,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88000, global step 1404655: learning rate 0.0000
[2019-04-04 06:47:23,342] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88000, global step 1405196: loss 0.0009
[2019-04-04 06:47:23,362] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88000, global step 1405208: learning rate 0.0000
[2019-04-04 06:47:24,195] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88000, global step 1405443: loss 0.0012
[2019-04-04 06:47:24,198] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88000, global step 1405443: learning rate 0.0000
[2019-04-04 06:47:24,399] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88000, global step 1405499: loss 0.0034
[2019-04-04 06:47:24,400] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88000, global step 1405499: learning rate 0.0000
[2019-04-04 06:47:24,594] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88000, global step 1405577: loss 0.0040
[2019-04-04 06:47:24,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88000, global step 1405577: learning rate 0.0000
[2019-04-04 06:47:29,821] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.6910505e-09 5.9108962e-09 1.0098605e-21 1.3449045e-09 3.2681258e-09
 4.3253721e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:47:29,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7466
[2019-04-04 06:47:29,890] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.300000000000001, 80.0, 0.0, 0.0, 26.0, 24.33114289453402, 0.09093148515902176, 0.0, 1.0, 44918.02198627093], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1898400.0000, 
sim time next is 1899000.0000, 
raw observation next is [-7.3, 80.5, 0.0, 0.0, 26.0, 24.29085584415692, 0.08285143275370438, 0.0, 1.0, 44932.73832919998], 
processed observation next is [0.0, 1.0, 0.26038781163434904, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5242379870130766, 0.5276171442512348, 0.0, 1.0, 0.213965420615238], 
reward next is 0.7860, 
noisyNet noise sample is [array([0.26502046], dtype=float32), 0.6293588]. 
=============================================
[2019-04-04 06:47:29,893] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.47874 ]
 [69.636024]
 [69.81902 ]
 [70.016045]
 [70.21364 ]], R is [[69.42880249]
 [69.52061462]
 [69.61155701]
 [69.70160675]
 [69.79075623]].
[2019-04-04 06:47:31,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6910866e-08 3.4877714e-09 6.1911500e-21 1.5633962e-09 2.6092075e-09
 9.5646442e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:47:31,826] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4808
[2019-04-04 06:47:31,850] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.55, 76.5, 0.0, 0.0, 26.0, 24.05540584620114, 0.02239092134065847, 0.0, 1.0, 45193.15189838954], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1906200.0000, 
sim time next is 1906800.0000, 
raw observation next is [-7.633333333333333, 77.0, 0.0, 0.0, 26.0, 24.04182751803177, 0.02211545887414687, 0.0, 1.0, 45144.29041296341], 
processed observation next is [1.0, 0.043478260869565216, 0.2511542012927055, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5034856265026475, 0.5073718196247156, 0.0, 1.0, 0.21497281149030195], 
reward next is 0.7850, 
noisyNet noise sample is [array([0.22862403], dtype=float32), -0.51181424]. 
=============================================
[2019-04-04 06:47:33,139] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88000, global step 1408075: loss 0.0072
[2019-04-04 06:47:33,157] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88000, global step 1408075: learning rate 0.0000
[2019-04-04 06:47:33,749] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88000, global step 1408262: loss 0.0085
[2019-04-04 06:47:33,762] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88000, global step 1408262: learning rate 0.0000
[2019-04-04 06:47:34,333] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88000, global step 1408462: loss 0.0107
[2019-04-04 06:47:34,334] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88000, global step 1408462: learning rate 0.0000
[2019-04-04 06:47:34,923] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88000, global step 1408617: loss 0.0142
[2019-04-04 06:47:34,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88000, global step 1408617: learning rate 0.0000
[2019-04-04 06:47:37,274] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88000, global step 1409222: loss 0.0026
[2019-04-04 06:47:37,275] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88000, global step 1409222: learning rate 0.0000
[2019-04-04 06:47:38,175] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88000, global step 1409436: loss 0.0105
[2019-04-04 06:47:38,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88000, global step 1409438: learning rate 0.0000
[2019-04-04 06:47:40,375] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88000, global step 1409979: loss 0.0216
[2019-04-04 06:47:40,375] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88000, global step 1409979: learning rate 0.0000
[2019-04-04 06:47:43,383] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88000, global step 1410919: loss 0.0418
[2019-04-04 06:47:43,386] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88000, global step 1410919: learning rate 0.0000
[2019-04-04 06:47:43,491] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88000, global step 1410949: loss 0.0420
[2019-04-04 06:47:43,493] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88000, global step 1410950: learning rate 0.0000
[2019-04-04 06:47:44,015] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88000, global step 1411102: loss 0.0289
[2019-04-04 06:47:44,015] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88000, global step 1411102: learning rate 0.0000
[2019-04-04 06:47:49,589] A3C_AGENT_WORKER-Thread-2 INFO:Local step 88500, global step 1412565: loss 0.9269
[2019-04-04 06:47:49,591] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 88500, global step 1412565: learning rate 0.0000
[2019-04-04 06:47:51,931] A3C_AGENT_WORKER-Thread-18 INFO:Local step 88500, global step 1413303: loss 0.9389
[2019-04-04 06:47:51,933] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 88500, global step 1413303: learning rate 0.0000
[2019-04-04 06:47:52,804] A3C_AGENT_WORKER-Thread-16 INFO:Local step 88500, global step 1413569: loss 0.8926
[2019-04-04 06:47:52,805] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 88500, global step 1413569: learning rate 0.0000
[2019-04-04 06:47:53,690] A3C_AGENT_WORKER-Thread-12 INFO:Local step 88500, global step 1413798: loss 0.8601
[2019-04-04 06:47:53,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 88500, global step 1413798: learning rate 0.0000
[2019-04-04 06:47:53,875] A3C_AGENT_WORKER-Thread-5 INFO:Local step 88500, global step 1413839: loss 0.8730
[2019-04-04 06:47:53,878] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 88500, global step 1413839: learning rate 0.0000
[2019-04-04 06:47:54,156] A3C_AGENT_WORKER-Thread-20 INFO:Local step 88500, global step 1413907: loss 0.8762
[2019-04-04 06:47:54,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 88500, global step 1413907: learning rate 0.0000
[2019-04-04 06:47:56,344] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2966201e-09 4.0980148e-09 2.5531906e-22 1.0758527e-09 9.8288022e-10
 8.2781373e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:47:56,345] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9314
[2019-04-04 06:47:56,361] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33137021276675, 0.1340124437852609, 0.0, 1.0, 41640.45409983567], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1996200.0000, 
sim time next is 1996800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.35721537062151, 0.133788875829777, 0.0, 1.0, 41581.57025277321], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5297679475517926, 0.544596291943259, 0.0, 1.0, 0.19800747739415814], 
reward next is 0.8020, 
noisyNet noise sample is [array([1.6605211], dtype=float32), 0.15966499]. 
=============================================
[2019-04-04 06:48:02,539] A3C_AGENT_WORKER-Thread-15 INFO:Local step 88500, global step 1416169: loss 0.7992
[2019-04-04 06:48:02,540] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 88500, global step 1416169: learning rate 0.0000
[2019-04-04 06:48:03,279] A3C_AGENT_WORKER-Thread-19 INFO:Local step 88500, global step 1416348: loss 0.8680
[2019-04-04 06:48:03,281] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 88500, global step 1416348: learning rate 0.0000
[2019-04-04 06:48:04,475] A3C_AGENT_WORKER-Thread-3 INFO:Local step 88500, global step 1416622: loss 0.9732
[2019-04-04 06:48:04,477] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 88500, global step 1416623: learning rate 0.0000
[2019-04-04 06:48:04,559] A3C_AGENT_WORKER-Thread-17 INFO:Local step 88500, global step 1416642: loss 0.9459
[2019-04-04 06:48:04,561] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 88500, global step 1416642: learning rate 0.0000
[2019-04-04 06:48:06,070] A3C_AGENT_WORKER-Thread-14 INFO:Local step 88500, global step 1417079: loss 0.9330
[2019-04-04 06:48:06,071] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 88500, global step 1417079: learning rate 0.0000
[2019-04-04 06:48:07,092] A3C_AGENT_WORKER-Thread-13 INFO:Local step 88500, global step 1417364: loss 0.9375
[2019-04-04 06:48:07,093] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 88500, global step 1417364: learning rate 0.0000
[2019-04-04 06:48:09,438] A3C_AGENT_WORKER-Thread-4 INFO:Local step 88500, global step 1418101: loss 0.8611
[2019-04-04 06:48:09,450] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 88500, global step 1418104: learning rate 0.0000
[2019-04-04 06:48:10,877] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4175825e-11 1.4483388e-10 2.0269162e-24 5.2595716e-11 8.1460394e-12
 6.6757011e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:48:10,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2354
[2019-04-04 06:48:10,921] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.483333333333333, 66.0, 138.6666666666667, 0.0, 26.0, 25.3281973689602, 0.2938113233641302, 1.0, 1.0, 28353.55562051856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2207400.0000, 
sim time next is 2208000.0000, 
raw observation next is [-3.566666666666667, 67.0, 141.3333333333333, 0.0, 26.0, 25.17812822144177, 0.3031984220141926, 1.0, 1.0, 61531.18895158705], 
processed observation next is [1.0, 0.5652173913043478, 0.3638042474607572, 0.67, 0.471111111111111, 0.0, 0.6666666666666666, 0.5981773517868142, 0.6010661406713975, 1.0, 1.0, 0.29300566167422404], 
reward next is 0.7070, 
noisyNet noise sample is [array([-0.07749776], dtype=float32), -0.21639134]. 
=============================================
[2019-04-04 06:48:10,927] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.645096]
 [83.43082 ]
 [83.21336 ]
 [83.01715 ]
 [82.93733 ]], R is [[83.89430237]
 [83.92034149]
 [83.96651459]
 [84.01064301]
 [84.05414581]].
[2019-04-04 06:48:11,812] A3C_AGENT_WORKER-Thread-6 INFO:Local step 88500, global step 1418770: loss 0.7573
[2019-04-04 06:48:11,813] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 88500, global step 1418770: learning rate 0.0000
[2019-04-04 06:48:12,222] A3C_AGENT_WORKER-Thread-11 INFO:Local step 88500, global step 1418875: loss 0.7531
[2019-04-04 06:48:12,224] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 88500, global step 1418875: learning rate 0.0000
[2019-04-04 06:48:12,926] A3C_AGENT_WORKER-Thread-10 INFO:Local step 88500, global step 1419055: loss 0.7331
[2019-04-04 06:48:12,926] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 88500, global step 1419055: learning rate 0.0000
[2019-04-04 06:48:17,616] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89000, global step 1420479: loss 0.0018
[2019-04-04 06:48:17,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89000, global step 1420479: learning rate 0.0000
[2019-04-04 06:48:18,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.06750422e-11 6.42892614e-11 2.72953308e-24 4.15720514e-11
 8.70248144e-12 1.06569206e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:48:18,876] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3955
[2019-04-04 06:48:18,928] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 71.0, 114.5, 75.16666666666664, 26.0, 25.99344212876348, 0.3980989083825205, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2197200.0000, 
sim time next is 2197800.0000, 
raw observation next is [-4.75, 71.0, 117.0, 0.0, 26.0, 25.96420758326445, 0.3852730551576281, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3310249307479225, 0.71, 0.39, 0.0, 0.6666666666666666, 0.6636839652720375, 0.6284243517192093, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08618596], dtype=float32), -0.47227103]. 
=============================================
[2019-04-04 06:48:20,327] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89000, global step 1421186: loss 0.0005
[2019-04-04 06:48:20,349] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89000, global step 1421186: learning rate 0.0000
[2019-04-04 06:48:20,617] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89000, global step 1421281: loss 0.0009
[2019-04-04 06:48:20,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89000, global step 1421281: learning rate 0.0000
[2019-04-04 06:48:21,199] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89000, global step 1421466: loss 0.0008
[2019-04-04 06:48:21,203] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89000, global step 1421468: learning rate 0.0000
[2019-04-04 06:48:21,457] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89000, global step 1421547: loss 0.0023
[2019-04-04 06:48:21,461] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89000, global step 1421547: learning rate 0.0000
[2019-04-04 06:48:22,447] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89000, global step 1421877: loss 0.0007
[2019-04-04 06:48:22,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89000, global step 1421877: learning rate 0.0000
[2019-04-04 06:48:24,214] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0815899e-11 1.3097785e-10 3.4466382e-24 2.3218923e-11 7.3073015e-12
 3.7552359e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:48:24,214] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7250
[2019-04-04 06:48:24,260] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.9166666666666667, 43.16666666666667, 132.3333333333333, 48.0, 26.0, 26.31915576063546, 0.3974217330195591, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2301000.0000, 
sim time next is 2301600.0000, 
raw observation next is [0.7333333333333335, 43.33333333333334, 135.1666666666667, 45.0, 26.0, 25.8458250894587, 0.4285893580609319, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4829178208679595, 0.4333333333333334, 0.4505555555555557, 0.049723756906077346, 0.6666666666666666, 0.6538187574548916, 0.6428631193536439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.420555], dtype=float32), 0.3136835]. 
=============================================
[2019-04-04 06:48:25,118] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4923180e-09 1.0550444e-08 7.4414736e-22 1.3011936e-09 6.0190541e-10
 9.5789533e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:48:25,121] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2770
[2019-04-04 06:48:25,151] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 91.00000000000001, 0.0, 0.0, 26.0, 23.57431751789938, -0.0357199833851567, 0.0, 1.0, 43182.43247501701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2268600.0000, 
sim time next is 2269200.0000, 
raw observation next is [-9.100000000000001, 91.0, 0.0, 0.0, 26.0, 23.59970606435493, -0.04778790735273014, 0.0, 1.0, 43156.98722186944], 
processed observation next is [1.0, 0.2608695652173913, 0.21052631578947364, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4666421720295775, 0.48407069754909, 0.0, 1.0, 0.20550946296128306], 
reward next is 0.7945, 
noisyNet noise sample is [array([-1.6554908], dtype=float32), 1.1559262]. 
=============================================
[2019-04-04 06:48:30,099] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89000, global step 1424377: loss 0.0205
[2019-04-04 06:48:30,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89000, global step 1424377: learning rate 0.0000
[2019-04-04 06:48:30,454] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89000, global step 1424535: loss 0.0231
[2019-04-04 06:48:30,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89000, global step 1424540: learning rate 0.0000
[2019-04-04 06:48:30,813] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89000, global step 1424674: loss 0.0124
[2019-04-04 06:48:30,814] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89000, global step 1424674: learning rate 0.0000
[2019-04-04 06:48:31,327] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89000, global step 1424898: loss 0.0014
[2019-04-04 06:48:31,332] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89000, global step 1424900: learning rate 0.0000
[2019-04-04 06:48:32,888] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89000, global step 1425454: loss 0.0006
[2019-04-04 06:48:32,888] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89000, global step 1425454: learning rate 0.0000
[2019-04-04 06:48:34,287] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89000, global step 1425912: loss 0.0004
[2019-04-04 06:48:34,287] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89000, global step 1425912: learning rate 0.0000
[2019-04-04 06:48:35,911] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89000, global step 1426411: loss 0.0013
[2019-04-04 06:48:35,915] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89000, global step 1426413: learning rate 0.0000
[2019-04-04 06:48:38,212] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89000, global step 1427249: loss 0.0016
[2019-04-04 06:48:38,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89000, global step 1427249: learning rate 0.0000
[2019-04-04 06:48:38,580] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89000, global step 1427395: loss 0.0023
[2019-04-04 06:48:38,582] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89000, global step 1427396: learning rate 0.0000
[2019-04-04 06:48:38,989] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89000, global step 1427529: loss 0.0018
[2019-04-04 06:48:38,995] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89000, global step 1427529: learning rate 0.0000
[2019-04-04 06:48:40,652] A3C_AGENT_WORKER-Thread-2 INFO:Local step 89500, global step 1428237: loss 0.6561
[2019-04-04 06:48:40,653] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 89500, global step 1428237: learning rate 0.0000
[2019-04-04 06:48:42,709] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5952287e-12 5.3079152e-11 7.0236181e-25 7.1246164e-12 6.6793506e-12
 1.0371778e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:48:42,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0756
[2019-04-04 06:48:42,776] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.86483538246843, 0.3619502188077265, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2723400.0000, 
sim time next is 2724000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 26.0, 25.29426394596282, 0.4015509190729253, 1.0, 1.0, 166630.5289785643], 
processed observation next is [1.0, 0.5217391304347826, 0.27793167128347185, 0.6066666666666666, 0.37444444444444436, 0.8808471454880296, 0.6666666666666666, 0.6078553288302349, 0.6338503063576417, 1.0, 1.0, 0.7934787094217348], 
reward next is 0.2065, 
noisyNet noise sample is [array([0.71832746], dtype=float32), -1.6466656]. 
=============================================
[2019-04-04 06:48:42,830] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.04123 ]
 [85.99105 ]
 [85.927284]
 [85.88447 ]
 [85.86319 ]], R is [[86.31958771]
 [86.45639038]
 [86.59182739]
 [86.72590637]
 [86.85865021]].
[2019-04-04 06:48:42,830] A3C_AGENT_WORKER-Thread-18 INFO:Local step 89500, global step 1428957: loss 0.6087
[2019-04-04 06:48:42,835] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 89500, global step 1428957: learning rate 0.0000
[2019-04-04 06:48:43,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.12365515e-11 8.58562596e-11 5.91449943e-26 1.25213945e-11
 5.79944686e-12 5.86939216e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:48:43,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4289
[2019-04-04 06:48:43,083] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.266666666666667, 53.66666666666667, 121.8333333333333, 30.5, 26.0, 25.65561870697538, 0.2824691061694298, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2539200.0000, 
sim time next is 2539800.0000, 
raw observation next is [-2.0, 52.5, 136.0, 33.0, 26.0, 25.70124486553331, 0.2930395433123135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.40720221606648205, 0.525, 0.4533333333333333, 0.036464088397790057, 0.6666666666666666, 0.6417704054611093, 0.5976798477707712, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5283251], dtype=float32), 1.1073734]. 
=============================================
[2019-04-04 06:48:43,215] A3C_AGENT_WORKER-Thread-16 INFO:Local step 89500, global step 1429103: loss 0.6279
[2019-04-04 06:48:43,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 89500, global step 1429103: learning rate 0.0000
[2019-04-04 06:48:43,435] A3C_AGENT_WORKER-Thread-12 INFO:Local step 89500, global step 1429183: loss 0.5887
[2019-04-04 06:48:43,437] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 89500, global step 1429183: learning rate 0.0000
[2019-04-04 06:48:44,169] A3C_AGENT_WORKER-Thread-5 INFO:Local step 89500, global step 1429440: loss 0.6133
[2019-04-04 06:48:44,174] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 89500, global step 1429441: learning rate 0.0000
[2019-04-04 06:48:45,610] A3C_AGENT_WORKER-Thread-20 INFO:Local step 89500, global step 1429939: loss 0.6120
[2019-04-04 06:48:45,623] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 89500, global step 1429939: learning rate 0.0000
[2019-04-04 06:48:46,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.28397096e-10 1.73176340e-09 1.05373926e-23 1.87655752e-10
 1.50150239e-10 3.55534043e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:48:46,012] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5413
[2019-04-04 06:48:46,034] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 61.5, 0.0, 0.0, 26.0, 25.36814895002933, 0.442780105730884, 0.0, 1.0, 67315.21772803208], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2755800.0000, 
sim time next is 2756400.0000, 
raw observation next is [-6.0, 60.66666666666666, 0.0, 0.0, 26.0, 25.34786031254403, 0.4407071039054395, 0.0, 1.0, 57475.21611992049], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6066666666666666, 0.0, 0.0, 0.6666666666666666, 0.6123216927120024, 0.6469023679684799, 0.0, 1.0, 0.2736915053329547], 
reward next is 0.7263, 
noisyNet noise sample is [array([1.0171517], dtype=float32), 1.947794]. 
=============================================
[2019-04-04 06:48:52,478] A3C_AGENT_WORKER-Thread-17 INFO:Local step 89500, global step 1432412: loss 0.6854
[2019-04-04 06:48:52,480] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 89500, global step 1432412: learning rate 0.0000
[2019-04-04 06:48:52,580] A3C_AGENT_WORKER-Thread-19 INFO:Local step 89500, global step 1432449: loss 0.6814
[2019-04-04 06:48:52,581] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 89500, global step 1432449: learning rate 0.0000
[2019-04-04 06:48:52,740] A3C_AGENT_WORKER-Thread-15 INFO:Local step 89500, global step 1432506: loss 0.6720
[2019-04-04 06:48:52,740] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 89500, global step 1432506: learning rate 0.0000
[2019-04-04 06:48:52,911] A3C_AGENT_WORKER-Thread-3 INFO:Local step 89500, global step 1432569: loss 0.6908
[2019-04-04 06:48:52,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 89500, global step 1432573: learning rate 0.0000
[2019-04-04 06:48:55,305] A3C_AGENT_WORKER-Thread-14 INFO:Local step 89500, global step 1433366: loss 0.7208
[2019-04-04 06:48:55,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 89500, global step 1433366: learning rate 0.0000
[2019-04-04 06:48:56,241] A3C_AGENT_WORKER-Thread-13 INFO:Local step 89500, global step 1433749: loss 0.7580
[2019-04-04 06:48:56,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 89500, global step 1433749: learning rate 0.0000
[2019-04-04 06:48:58,647] A3C_AGENT_WORKER-Thread-4 INFO:Local step 89500, global step 1434563: loss 0.8513
[2019-04-04 06:48:58,671] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 89500, global step 1434563: learning rate 0.0000
[2019-04-04 06:49:00,332] A3C_AGENT_WORKER-Thread-6 INFO:Local step 89500, global step 1435175: loss 0.8437
[2019-04-04 06:49:00,333] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 89500, global step 1435175: learning rate 0.0000
[2019-04-04 06:49:00,900] A3C_AGENT_WORKER-Thread-11 INFO:Local step 89500, global step 1435348: loss 0.8318
[2019-04-04 06:49:00,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 89500, global step 1435348: learning rate 0.0000
[2019-04-04 06:49:01,248] A3C_AGENT_WORKER-Thread-10 INFO:Local step 89500, global step 1435455: loss 0.8392
[2019-04-04 06:49:01,248] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 89500, global step 1435455: learning rate 0.0000
[2019-04-04 06:49:03,412] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90000, global step 1436172: loss 0.0628
[2019-04-04 06:49:03,412] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90000, global step 1436172: learning rate 0.0000
[2019-04-04 06:49:03,748] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2808739e-09 2.0964253e-09 2.4010457e-22 2.3597541e-10 3.2190894e-10
 4.2604418e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:03,750] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5945
[2019-04-04 06:49:03,765] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64593222764496, 0.2545383653904763, 0.0, 1.0, 42918.2494894645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2767200.0000, 
sim time next is 2767800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.72449360062238, 0.2563514570973985, 0.0, 1.0, 42694.63230404101], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5603744667185317, 0.5854504856991328, 0.0, 1.0, 0.20330777287638577], 
reward next is 0.7967, 
noisyNet noise sample is [array([-1.0337868], dtype=float32), -2.0445561]. 
=============================================
[2019-04-04 06:49:05,657] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90000, global step 1436957: loss 0.0546
[2019-04-04 06:49:05,661] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90000, global step 1436957: learning rate 0.0000
[2019-04-04 06:49:06,072] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90000, global step 1437125: loss 0.0492
[2019-04-04 06:49:06,074] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90000, global step 1437125: learning rate 0.0000
[2019-04-04 06:49:06,446] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90000, global step 1437266: loss 0.0431
[2019-04-04 06:49:06,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90000, global step 1437266: learning rate 0.0000
[2019-04-04 06:49:06,786] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90000, global step 1437380: loss 0.0385
[2019-04-04 06:49:06,794] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90000, global step 1437380: learning rate 0.0000
[2019-04-04 06:49:08,168] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90000, global step 1437859: loss 0.0449
[2019-04-04 06:49:08,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90000, global step 1437859: learning rate 0.0000
[2019-04-04 06:49:12,125] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1726925e-10 1.4202983e-10 3.9043609e-24 1.0761170e-10 4.0880334e-11
 4.2087807e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:12,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4793
[2019-04-04 06:49:12,173] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 256.0, 284.0, 26.0, 24.96448985456088, 0.3549425951612621, 0.0, 1.0, 49123.43056640511], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2979000.0000, 
sim time next is 2979600.0000, 
raw observation next is [-3.0, 65.0, 243.5, 351.8333333333333, 26.0, 24.96635207908687, 0.3649378369042164, 0.0, 1.0, 44526.96263569329], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8116666666666666, 0.3887661141804788, 0.6666666666666666, 0.5805293399239059, 0.6216459456347389, 0.0, 1.0, 0.2120331554080633], 
reward next is 0.7880, 
noisyNet noise sample is [array([-0.44208726], dtype=float32), -0.31146452]. 
=============================================
[2019-04-04 06:49:13,242] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6207755e-10 3.6806866e-10 2.3259917e-23 9.8208518e-11 3.0830813e-10
 2.4150284e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:13,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2766
[2019-04-04 06:49:13,319] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 92.0, 709.0, 26.0, 25.11883347090845, 0.4058185718941061, 0.0, 1.0, 38762.15874797017], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2991600.0000, 
sim time next is 2992200.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 89.0, 695.0, 26.0, 25.10451745250051, 0.4088535299218514, 0.0, 1.0, 34788.48057573079], 
processed observation next is [0.0, 0.6521739130434783, 0.41181902123730385, 0.5916666666666666, 0.2966666666666667, 0.7679558011049724, 0.6666666666666666, 0.5920431210417091, 0.6362845099739505, 0.0, 1.0, 0.16565943131300376], 
reward next is 0.8343, 
noisyNet noise sample is [array([-1.197656], dtype=float32), 0.6526679]. 
=============================================
[2019-04-04 06:49:15,515] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90000, global step 1440544: loss 0.0262
[2019-04-04 06:49:15,515] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90000, global step 1440544: learning rate 0.0000
[2019-04-04 06:49:15,588] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90000, global step 1440576: loss 0.0332
[2019-04-04 06:49:15,590] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90000, global step 1440576: learning rate 0.0000
[2019-04-04 06:49:15,936] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90000, global step 1440737: loss 0.0410
[2019-04-04 06:49:15,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90000, global step 1440737: learning rate 0.0000
[2019-04-04 06:49:16,080] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90000, global step 1440796: loss 0.0461
[2019-04-04 06:49:16,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90000, global step 1440796: learning rate 0.0000
[2019-04-04 06:49:16,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0852253e-08 2.4083459e-08 6.4478535e-21 5.5050120e-09 1.5387153e-08
 1.8549558e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 06:49:16,093] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2622
[2019-04-04 06:49:16,127] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.30815391177491, 0.1228513130703935, 0.0, 1.0, 39015.23046695278], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3033600.0000, 
sim time next is 3034200.0000, 
raw observation next is [-5.833333333333333, 76.0, 0.0, 0.0, 26.0, 24.28032196950821, 0.1154848450227981, 0.0, 1.0, 39188.49980922794], 
processed observation next is [0.0, 0.08695652173913043, 0.30101569713758086, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5233601641256843, 0.5384949483409327, 0.0, 1.0, 0.1866119038534664], 
reward next is 0.8134, 
noisyNet noise sample is [array([-0.818697], dtype=float32), -2.0055244]. 
=============================================
[2019-04-04 06:49:18,765] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90000, global step 1441929: loss 0.0871
[2019-04-04 06:49:18,769] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90000, global step 1441931: learning rate 0.0000
[2019-04-04 06:49:19,054] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90000, global step 1442011: loss 0.0879
[2019-04-04 06:49:19,055] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90000, global step 1442011: learning rate 0.0000
[2019-04-04 06:49:21,680] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90000, global step 1442961: loss 0.0545
[2019-04-04 06:49:21,681] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90000, global step 1442961: learning rate 0.0000
[2019-04-04 06:49:22,773] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90000, global step 1443396: loss 0.0288
[2019-04-04 06:49:22,774] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90000, global step 1443396: learning rate 0.0000
[2019-04-04 06:49:22,919] A3C_AGENT_WORKER-Thread-2 INFO:Local step 90500, global step 1443474: loss 0.1627
[2019-04-04 06:49:22,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 90500, global step 1443474: learning rate 0.0000
[2019-04-04 06:49:23,461] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90000, global step 1443727: loss 0.0184
[2019-04-04 06:49:23,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90000, global step 1443727: learning rate 0.0000
[2019-04-04 06:49:23,896] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90000, global step 1443898: loss 0.0208
[2019-04-04 06:49:23,898] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90000, global step 1443898: learning rate 0.0000
[2019-04-04 06:49:24,979] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.4892083e-10 1.5443924e-09 1.2950398e-22 5.3594240e-10 4.6775517e-10
 1.3089705e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:24,979] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8197
[2019-04-04 06:49:25,025] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 58.33333333333334, 0.0, 0.0, 26.0, 25.13085973846691, 0.4519952687849213, 0.0, 1.0, 198934.9750302327], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3356400.0000, 
sim time next is 3357000.0000, 
raw observation next is [-3.5, 60.0, 0.0, 0.0, 26.0, 25.11035871527098, 0.4744873411394977, 0.0, 1.0, 156192.9459286674], 
processed observation next is [1.0, 0.8695652173913043, 0.36565096952908593, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5925298929392483, 0.6581624470464992, 0.0, 1.0, 0.7437759329936543], 
reward next is 0.2562, 
noisyNet noise sample is [array([-0.56835324], dtype=float32), -0.7256973]. 
=============================================
[2019-04-04 06:49:25,029] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.56957 ]
 [78.04764 ]
 [80.63904 ]
 [78.399666]
 [76.39065 ]], R is [[74.8026886 ]
 [74.10735321]
 [74.03093719]
 [74.06712341]
 [74.20632935]].
[2019-04-04 06:49:25,395] A3C_AGENT_WORKER-Thread-12 INFO:Local step 90500, global step 1444519: loss 0.1832
[2019-04-04 06:49:25,397] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 90500, global step 1444519: learning rate 0.0000
[2019-04-04 06:49:25,717] A3C_AGENT_WORKER-Thread-16 INFO:Local step 90500, global step 1444656: loss 0.1607
[2019-04-04 06:49:25,725] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 90500, global step 1444659: learning rate 0.0000
[2019-04-04 06:49:26,049] A3C_AGENT_WORKER-Thread-5 INFO:Local step 90500, global step 1444802: loss 0.1755
[2019-04-04 06:49:26,050] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 90500, global step 1444802: learning rate 0.0000
[2019-04-04 06:49:26,130] A3C_AGENT_WORKER-Thread-18 INFO:Local step 90500, global step 1444841: loss 0.1524
[2019-04-04 06:49:26,130] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 90500, global step 1444841: learning rate 0.0000
[2019-04-04 06:49:27,896] A3C_AGENT_WORKER-Thread-20 INFO:Local step 90500, global step 1445580: loss 0.1909
[2019-04-04 06:49:27,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 90500, global step 1445581: learning rate 0.0000
[2019-04-04 06:49:28,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.1172700e-10 1.3990290e-09 8.5307759e-23 3.9625242e-10 2.2304318e-10
 7.2389902e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:28,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0701
[2019-04-04 06:49:28,866] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.96371026169886, 0.3429652785585509, 0.0, 1.0, 41423.37609965451], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3371400.0000, 
sim time next is 3372000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.93181866707538, 0.3565012904215064, 0.0, 1.0, 41701.04912511476], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.577651555589615, 0.6188337634738355, 0.0, 1.0, 0.1985764244053084], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.92739356], dtype=float32), 1.8971153]. 
=============================================
[2019-04-04 06:49:28,895] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.92352 ]
 [75.029175]
 [75.076866]
 [75.1684  ]
 [75.19305 ]], R is [[74.91538239]
 [74.96897888]
 [75.02188873]
 [75.07411194]
 [75.12563324]].
[2019-04-04 06:49:34,036] A3C_AGENT_WORKER-Thread-19 INFO:Local step 90500, global step 1448380: loss 0.1823
[2019-04-04 06:49:34,037] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 90500, global step 1448380: learning rate 0.0000
[2019-04-04 06:49:34,445] A3C_AGENT_WORKER-Thread-15 INFO:Local step 90500, global step 1448553: loss 0.1651
[2019-04-04 06:49:34,446] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 90500, global step 1448553: learning rate 0.0000
[2019-04-04 06:49:34,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4288339e-11 1.2362900e-10 1.0574277e-23 4.6612866e-11 1.5789146e-11
 5.6470484e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:34,742] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1097
[2019-04-04 06:49:34,748] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 52.0, 114.0, 800.0, 26.0, 26.01127376304867, 0.5806633954306307, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3331800.0000, 
sim time next is 3332400.0000, 
raw observation next is [-4.333333333333334, 51.33333333333333, 112.6666666666667, 792.0, 26.0, 26.08714840171888, 0.5906292111970463, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3425669436749769, 0.5133333333333333, 0.37555555555555564, 0.8751381215469614, 0.6666666666666666, 0.6739290334765734, 0.6968764037323488, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4537803], dtype=float32), 0.67458993]. 
=============================================
[2019-04-04 06:49:34,986] A3C_AGENT_WORKER-Thread-17 INFO:Local step 90500, global step 1448786: loss 0.1682
[2019-04-04 06:49:34,988] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 90500, global step 1448787: learning rate 0.0000
[2019-04-04 06:49:35,318] A3C_AGENT_WORKER-Thread-3 INFO:Local step 90500, global step 1448946: loss 0.1557
[2019-04-04 06:49:35,330] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 90500, global step 1448950: learning rate 0.0000
[2019-04-04 06:49:37,055] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5628452e-11 1.3345522e-10 5.8616512e-25 5.9576107e-12 2.7338951e-12
 4.7490373e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:37,055] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5024
[2019-04-04 06:49:37,074] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 50.5, 115.0, 806.0, 26.0, 26.01872094874965, 0.625899037072052, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3504600.0000, 
sim time next is 3505200.0000, 
raw observation next is [2.666666666666667, 50.0, 112.8333333333333, 801.8333333333334, 26.0, 25.51533412456113, 0.5944203165102196, 1.0, 1.0, 18680.41167023055], 
processed observation next is [1.0, 0.5652173913043478, 0.5364727608494922, 0.5, 0.376111111111111, 0.8860036832412523, 0.6666666666666666, 0.6262778437134274, 0.6981401055034065, 1.0, 1.0, 0.08895434128681215], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.52581024], dtype=float32), 1.5260708]. 
=============================================
[2019-04-04 06:49:37,193] A3C_AGENT_WORKER-Thread-13 INFO:Local step 90500, global step 1449749: loss 0.1930
[2019-04-04 06:49:37,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 90500, global step 1449749: learning rate 0.0000
[2019-04-04 06:49:38,507] A3C_AGENT_WORKER-Thread-14 INFO:Local step 90500, global step 1450324: loss 0.1670
[2019-04-04 06:49:38,508] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 90500, global step 1450324: learning rate 0.0000
[2019-04-04 06:49:39,846] A3C_AGENT_WORKER-Thread-4 INFO:Local step 90500, global step 1450896: loss 0.1453
[2019-04-04 06:49:39,858] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 90500, global step 1450898: learning rate 0.0000
[2019-04-04 06:49:41,104] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91000, global step 1451402: loss 0.0125
[2019-04-04 06:49:41,111] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91000, global step 1451402: learning rate 0.0000
[2019-04-04 06:49:41,624] A3C_AGENT_WORKER-Thread-6 INFO:Local step 90500, global step 1451599: loss 0.1270
[2019-04-04 06:49:41,628] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 90500, global step 1451599: learning rate 0.0000
[2019-04-04 06:49:42,318] A3C_AGENT_WORKER-Thread-10 INFO:Local step 90500, global step 1451889: loss 0.1236
[2019-04-04 06:49:42,323] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 90500, global step 1451891: learning rate 0.0000
[2019-04-04 06:49:42,589] A3C_AGENT_WORKER-Thread-11 INFO:Local step 90500, global step 1452014: loss 0.1260
[2019-04-04 06:49:42,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 90500, global step 1452014: learning rate 0.0000
[2019-04-04 06:49:43,072] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91000, global step 1452247: loss 0.0030
[2019-04-04 06:49:43,073] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91000, global step 1452247: learning rate 0.0000
[2019-04-04 06:49:43,528] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91000, global step 1452449: loss 0.0025
[2019-04-04 06:49:43,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91000, global step 1452449: learning rate 0.0000
[2019-04-04 06:49:44,125] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91000, global step 1452721: loss 0.0017
[2019-04-04 06:49:44,125] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91000, global step 1452721: learning rate 0.0000
[2019-04-04 06:49:44,543] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91000, global step 1452902: loss 0.0025
[2019-04-04 06:49:44,545] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91000, global step 1452902: learning rate 0.0000
[2019-04-04 06:49:46,177] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91000, global step 1453753: loss 0.0016
[2019-04-04 06:49:46,180] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91000, global step 1453754: learning rate 0.0000
[2019-04-04 06:49:47,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3709414e-09 6.9217604e-10 6.9985886e-24 1.9791536e-10 2.6197530e-10
 5.5462628e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:47,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2647
[2019-04-04 06:49:47,707] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.51760917396136, 0.3763097101429968, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3719400.0000, 
sim time next is 3720000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.49755936171024, 0.3642848149286946, 0.0, 1.0, 25941.32677294801], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6247966134758535, 0.6214282716428982, 0.0, 1.0, 0.12353012749022863], 
reward next is 0.8765, 
noisyNet noise sample is [array([0.7663311], dtype=float32), 0.8693288]. 
=============================================
[2019-04-04 06:49:47,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.17001]
 [80.56492]
 [80.83181]
 [81.27963]
 [81.37977]], R is [[80.12043762]
 [80.31923676]
 [80.51604462]
 [80.71088409]
 [80.70475769]].
[2019-04-04 06:49:51,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4903356e-09 1.3088951e-09 5.3681770e-23 2.7249905e-10 2.4261682e-10
 8.1667420e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:51,205] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4165
[2019-04-04 06:49:51,218] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.39774539797779, 0.3339450714021434, 0.0, 1.0, 35423.14202899583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3729000.0000, 
sim time next is 3729600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.35886388068787, 0.3341375041788204, 0.0, 1.0, 52830.14143953903], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6132386567239893, 0.6113791680596068, 0.0, 1.0, 0.251572102093043], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.15317804], dtype=float32), -0.59753215]. 
=============================================
[2019-04-04 06:49:52,275] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91000, global step 1456616: loss 0.0048
[2019-04-04 06:49:52,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91000, global step 1456616: learning rate 0.0000
[2019-04-04 06:49:52,287] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91000, global step 1456622: loss 0.0072
[2019-04-04 06:49:52,288] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91000, global step 1456622: learning rate 0.0000
[2019-04-04 06:49:53,069] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91000, global step 1457008: loss 0.0021
[2019-04-04 06:49:53,070] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91000, global step 1457009: learning rate 0.0000
[2019-04-04 06:49:53,101] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91000, global step 1457024: loss 0.0029
[2019-04-04 06:49:53,104] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91000, global step 1457028: learning rate 0.0000
[2019-04-04 06:49:53,942] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3973137e-09 7.2597972e-10 9.0344425e-26 1.0197414e-10 2.0681701e-10
 1.4156902e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:53,942] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5742
[2019-04-04 06:49:53,953] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.5, 31.0, 60.66666666666668, 309.0, 26.0, 25.53216865925349, 0.4071791970996729, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3658200.0000, 
sim time next is 3658800.0000, 
raw observation next is [9.0, 30.0, 74.83333333333334, 356.0000000000001, 26.0, 25.55889386973467, 0.4153065923730512, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.7119113573407203, 0.3, 0.24944444444444447, 0.3933701657458565, 0.6666666666666666, 0.6299078224778892, 0.6384355307910171, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17664763], dtype=float32), -1.5096349]. 
=============================================
[2019-04-04 06:49:55,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4338357e-12 1.2015398e-10 1.4256832e-25 5.5887101e-12 2.1242515e-12
 1.8088054e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:49:55,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3259
[2019-04-04 06:49:55,224] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91000, global step 1458044: loss 0.0006
[2019-04-04 06:49:55,225] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91000, global step 1458045: learning rate 0.0000
[2019-04-04 06:49:55,230] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.0, 117.0, 824.1666666666666, 26.0, 26.52724809273352, 0.6456706944149783, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3843600.0000, 
sim time next is 3844200.0000, 
raw observation next is [-1.0, 60.0, 117.0, 826.3333333333334, 26.0, 26.54666554460726, 0.6529293972341699, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9130755064456723, 0.6666666666666666, 0.7122221287172718, 0.71764313241139, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0131578], dtype=float32), 0.18589821]. 
=============================================
[2019-04-04 06:49:56,941] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91000, global step 1458925: loss 0.0014
[2019-04-04 06:49:56,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91000, global step 1458925: learning rate 0.0000
[2019-04-04 06:49:57,184] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91000, global step 1459024: loss 0.0021
[2019-04-04 06:49:57,185] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91000, global step 1459024: learning rate 0.0000
[2019-04-04 06:49:57,407] A3C_AGENT_WORKER-Thread-2 INFO:Local step 91500, global step 1459132: loss 0.0818
[2019-04-04 06:49:57,407] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 91500, global step 1459132: learning rate 0.0000
[2019-04-04 06:49:59,459] A3C_AGENT_WORKER-Thread-16 INFO:Local step 91500, global step 1460108: loss 0.0759
[2019-04-04 06:49:59,460] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 91500, global step 1460108: learning rate 0.0000
[2019-04-04 06:49:59,521] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91000, global step 1460134: loss 0.0011
[2019-04-04 06:49:59,524] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91000, global step 1460134: learning rate 0.0000
[2019-04-04 06:49:59,874] A3C_AGENT_WORKER-Thread-12 INFO:Local step 91500, global step 1460309: loss 0.0819
[2019-04-04 06:49:59,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 91500, global step 1460309: learning rate 0.0000
[2019-04-04 06:49:59,920] A3C_AGENT_WORKER-Thread-5 INFO:Local step 91500, global step 1460329: loss 0.0856
[2019-04-04 06:49:59,927] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 91500, global step 1460329: learning rate 0.0000
[2019-04-04 06:50:00,117] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91000, global step 1460441: loss 0.0016
[2019-04-04 06:50:00,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91000, global step 1460441: learning rate 0.0000
[2019-04-04 06:50:00,199] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91000, global step 1460484: loss 0.0005
[2019-04-04 06:50:00,201] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91000, global step 1460484: learning rate 0.0000
[2019-04-04 06:50:00,794] A3C_AGENT_WORKER-Thread-18 INFO:Local step 91500, global step 1460763: loss 0.0715
[2019-04-04 06:50:00,796] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 91500, global step 1460763: learning rate 0.0000
[2019-04-04 06:50:02,635] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.4311216e-11 2.1693168e-11 2.0185586e-25 1.1066269e-11 2.8929270e-12
 5.5337854e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:02,635] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9129
[2019-04-04 06:50:02,643] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 42.33333333333334, 113.6666666666667, 819.8333333333334, 26.0, 25.29096786394497, 0.4540702993860884, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3676800.0000, 
sim time next is 3677400.0000, 
raw observation next is [5.5, 42.5, 113.0, 818.0, 26.0, 25.29794716278786, 0.4564897372560201, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6149584487534627, 0.425, 0.37666666666666665, 0.9038674033149171, 0.6666666666666666, 0.6081622635656551, 0.6521632457520067, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3036005], dtype=float32), -1.2242832]. 
=============================================
[2019-04-04 06:50:02,919] A3C_AGENT_WORKER-Thread-20 INFO:Local step 91500, global step 1461747: loss 0.0635
[2019-04-04 06:50:02,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 91500, global step 1461747: learning rate 0.0000
[2019-04-04 06:50:05,262] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8581519e-09 2.0152435e-09 3.5834871e-23 1.5940968e-10 2.3671154e-10
 6.5346957e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:05,273] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5134
[2019-04-04 06:50:05,306] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.8789199302566, 0.2837647253928801, 0.0, 1.0, 43895.20222547666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3820200.0000, 
sim time next is 3820800.0000, 
raw observation next is [-4.333333333333334, 73.0, 0.0, 0.0, 26.0, 24.87511278554539, 0.2788298912983603, 0.0, 1.0, 43832.59121650484], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5729260654621159, 0.5929432970994534, 0.0, 1.0, 0.20872662484049925], 
reward next is 0.7913, 
noisyNet noise sample is [array([1.1283133], dtype=float32), -0.46643707]. 
=============================================
[2019-04-04 06:50:08,725] A3C_AGENT_WORKER-Thread-19 INFO:Local step 91500, global step 1464249: loss 0.0159
[2019-04-04 06:50:08,726] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 91500, global step 1464249: learning rate 0.0000
[2019-04-04 06:50:08,747] A3C_AGENT_WORKER-Thread-15 INFO:Local step 91500, global step 1464259: loss 0.0156
[2019-04-04 06:50:08,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 91500, global step 1464261: learning rate 0.0000
[2019-04-04 06:50:09,341] A3C_AGENT_WORKER-Thread-3 INFO:Local step 91500, global step 1464508: loss 0.0145
[2019-04-04 06:50:09,342] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 91500, global step 1464508: learning rate 0.0000
[2019-04-04 06:50:09,379] A3C_AGENT_WORKER-Thread-17 INFO:Local step 91500, global step 1464529: loss 0.0104
[2019-04-04 06:50:09,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 91500, global step 1464530: learning rate 0.0000
[2019-04-04 06:50:09,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1315232e-12 1.4858212e-11 1.9168342e-26 1.3767802e-12 6.8185984e-13
 3.1637083e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:09,485] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1054
[2019-04-04 06:50:09,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 49.0, 113.0, 775.0, 26.0, 26.35343452713694, 0.5923768954713317, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3925800.0000, 
sim time next is 3926400.0000, 
raw observation next is [-6.333333333333334, 49.0, 114.1666666666667, 780.8333333333334, 26.0, 26.36614822870682, 0.5870093045014827, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.28716528162511545, 0.49, 0.38055555555555565, 0.8627992633517496, 0.6666666666666666, 0.6971790190589017, 0.695669768167161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.891441], dtype=float32), 1.2942253]. 
=============================================
[2019-04-04 06:50:09,561] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0033065e-08 8.6177131e-08 2.1269336e-21 9.9179944e-09 3.2747582e-09
 1.1980789e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 06:50:09,573] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2547
[2019-04-04 06:50:09,583] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 34.0, 0.0, 0.0, 26.0, 25.37168074095932, 0.4142602705575822, 0.0, 1.0, 83673.72738513813], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4055400.0000, 
sim time next is 4056000.0000, 
raw observation next is [-5.666666666666667, 35.0, 0.0, 0.0, 26.0, 25.29366769778441, 0.4072557176919962, 0.0, 1.0, 61632.31284424493], 
processed observation next is [1.0, 0.9565217391304348, 0.30563250230840255, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6078056414820342, 0.6357519058973321, 0.0, 1.0, 0.29348720402021394], 
reward next is 0.7065, 
noisyNet noise sample is [array([1.6855745], dtype=float32), -0.98845863]. 
=============================================
[2019-04-04 06:50:09,596] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.14259]
 [73.95231]
 [73.98886]
 [74.0073 ]
 [73.82215]], R is [[74.06575012]
 [73.926651  ]
 [73.98859406]
 [74.01817322]
 [73.97317505]].
[2019-04-04 06:50:11,741] A3C_AGENT_WORKER-Thread-13 INFO:Local step 91500, global step 1465463: loss 0.0085
[2019-04-04 06:50:11,744] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 91500, global step 1465465: learning rate 0.0000
[2019-04-04 06:50:12,329] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.67296363e-09 7.63210029e-09 1.76092394e-21 7.88461518e-10
 8.45248982e-10 1.02042185e-11 1.00000000e+00], sum to 1.0000
[2019-04-04 06:50:12,335] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6095
[2019-04-04 06:50:12,359] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.333333333333334, 46.33333333333334, 0.0, 0.0, 26.0, 25.50432609080821, 0.5096789711342912, 0.0, 1.0, 33780.32079258478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3964800.0000, 
sim time next is 3965400.0000, 
raw observation next is [-7.5, 47.0, 0.0, 0.0, 26.0, 25.49136482627049, 0.5031581938710973, 0.0, 1.0, 42044.96048438217], 
processed observation next is [1.0, 0.9130434782608695, 0.2548476454293629, 0.47, 0.0, 0.0, 0.6666666666666666, 0.6242804021892076, 0.6677193979570325, 0.0, 1.0, 0.200214097544677], 
reward next is 0.7998, 
noisyNet noise sample is [array([-1.3499856], dtype=float32), 1.7192386]. 
=============================================
[2019-04-04 06:50:13,662] A3C_AGENT_WORKER-Thread-4 INFO:Local step 91500, global step 1466412: loss 0.0009
[2019-04-04 06:50:13,663] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 91500, global step 1466413: learning rate 0.0000
[2019-04-04 06:50:14,750] A3C_AGENT_WORKER-Thread-14 INFO:Local step 91500, global step 1466838: loss 0.0018
[2019-04-04 06:50:14,760] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 91500, global step 1466842: learning rate 0.0000
[2019-04-04 06:50:16,182] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92000, global step 1467409: loss 0.0353
[2019-04-04 06:50:16,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92000, global step 1467409: learning rate 0.0000
[2019-04-04 06:50:16,653] A3C_AGENT_WORKER-Thread-6 INFO:Local step 91500, global step 1467594: loss 0.0033
[2019-04-04 06:50:16,653] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 91500, global step 1467594: learning rate 0.0000
[2019-04-04 06:50:16,793] A3C_AGENT_WORKER-Thread-11 INFO:Local step 91500, global step 1467655: loss 0.0089
[2019-04-04 06:50:16,794] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 91500, global step 1467655: learning rate 0.0000
[2019-04-04 06:50:16,920] A3C_AGENT_WORKER-Thread-10 INFO:Local step 91500, global step 1467716: loss 0.0027
[2019-04-04 06:50:16,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 91500, global step 1467716: learning rate 0.0000
[2019-04-04 06:50:18,152] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92000, global step 1468274: loss 0.0344
[2019-04-04 06:50:18,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92000, global step 1468280: learning rate 0.0000
[2019-04-04 06:50:18,177] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92000, global step 1468287: loss 0.0331
[2019-04-04 06:50:18,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92000, global step 1468289: learning rate 0.0000
[2019-04-04 06:50:18,717] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92000, global step 1468534: loss 0.0318
[2019-04-04 06:50:18,722] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92000, global step 1468534: learning rate 0.0000
[2019-04-04 06:50:19,500] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1022814e-09 7.3058176e-10 3.5482139e-24 3.1542025e-10 9.7239328e-10
 1.3053423e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:19,500] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4091
[2019-04-04 06:50:19,515] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 46.5, 0.0, 0.0, 26.0, 25.3994941135691, 0.3476176788052936, 0.0, 1.0, 36757.02658301219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4239000.0000, 
sim time next is 4239600.0000, 
raw observation next is [2.666666666666667, 46.0, 0.0, 0.0, 26.0, 25.40966319154165, 0.3474612814894543, 0.0, 1.0, 32081.60865117364], 
processed observation next is [0.0, 0.043478260869565216, 0.5364727608494922, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6174719326284709, 0.6158204271631514, 0.0, 1.0, 0.15276956500558875], 
reward next is 0.8472, 
noisyNet noise sample is [array([-0.91689485], dtype=float32), -0.39290687]. 
=============================================
[2019-04-04 06:50:19,629] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92000, global step 1468925: loss 0.0221
[2019-04-04 06:50:19,631] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92000, global step 1468926: learning rate 0.0000
[2019-04-04 06:50:20,282] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9431245e-11 1.3279580e-10 4.9993906e-26 2.3590102e-11 6.6239722e-12
 4.7668219e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:20,282] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1633
[2019-04-04 06:50:20,334] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 35.33333333333334, 76.66666666666667, 390.8333333333334, 26.0, 25.42176909839235, 0.3729026919293974, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4090800.0000, 
sim time next is 4091400.0000, 
raw observation next is [-3.5, 36.0, 92.0, 469.0, 26.0, 25.56916814868671, 0.4187367194330388, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.36565096952908593, 0.36, 0.30666666666666664, 0.518232044198895, 0.6666666666666666, 0.6307640123905592, 0.6395789064776797, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28627566], dtype=float32), 3.1726258]. 
=============================================
[2019-04-04 06:50:21,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5482398e-11 7.2990447e-11 2.7395145e-25 1.6442464e-11 2.8355879e-12
 1.2618120e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:21,711] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6781
[2019-04-04 06:50:21,720] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 36.0, 34.66666666666666, 120.3333333333333, 26.0, 26.74727186397172, 0.6518332303552766, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4123200.0000, 
sim time next is 4123800.0000, 
raw observation next is [3.0, 35.5, 23.0, 57.0, 26.0, 26.636558246562, 0.6274873714718255, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.355, 0.07666666666666666, 0.06298342541436464, 0.6666666666666666, 0.7197131872135, 0.7091624571572752, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3342023], dtype=float32), -0.45687923]. 
=============================================
[2019-04-04 06:50:21,906] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92000, global step 1469943: loss 0.0159
[2019-04-04 06:50:21,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92000, global step 1469943: learning rate 0.0000
[2019-04-04 06:50:24,519] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0960723e-09 6.3309002e-10 1.1612901e-24 6.8396025e-11 1.2681667e-10
 8.3830011e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:24,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-04 06:50:24,539] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.8, 76.0, 0.0, 0.0, 26.0, 25.50574536854075, 0.4144086941815637, 0.0, 1.0, 62524.51731677207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4312800.0000, 
sim time next is 4313400.0000, 
raw observation next is [4.733333333333333, 75.83333333333334, 0.0, 0.0, 26.0, 25.49981648676922, 0.4175732998085318, 0.0, 1.0, 48108.14706495027], 
processed observation next is [0.0, 0.9565217391304348, 0.5937211449676825, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6249847072307683, 0.6391910999361773, 0.0, 1.0, 0.22908641459500131], 
reward next is 0.7709, 
noisyNet noise sample is [array([1.1022282], dtype=float32), 0.8671912]. 
=============================================
[2019-04-04 06:50:26,648] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92000, global step 1472248: loss 0.0231
[2019-04-04 06:50:26,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92000, global step 1472248: learning rate 0.0000
[2019-04-04 06:50:27,299] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92000, global step 1472585: loss 0.0278
[2019-04-04 06:50:27,313] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92000, global step 1472585: learning rate 0.0000
[2019-04-04 06:50:27,728] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92000, global step 1472784: loss 0.0189
[2019-04-04 06:50:27,730] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92000, global step 1472784: learning rate 0.0000
[2019-04-04 06:50:27,892] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92000, global step 1472862: loss 0.0162
[2019-04-04 06:50:27,895] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92000, global step 1472863: learning rate 0.0000
[2019-04-04 06:50:30,194] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92000, global step 1474048: loss 0.0171
[2019-04-04 06:50:30,196] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92000, global step 1474048: learning rate 0.0000
[2019-04-04 06:50:31,623] A3C_AGENT_WORKER-Thread-2 INFO:Local step 92500, global step 1474775: loss 6.1839
[2019-04-04 06:50:31,624] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 92500, global step 1474775: learning rate 0.0000
[2019-04-04 06:50:31,869] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92000, global step 1474897: loss 0.0488
[2019-04-04 06:50:31,871] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92000, global step 1474898: learning rate 0.0000
[2019-04-04 06:50:33,338] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92000, global step 1475603: loss 0.0605
[2019-04-04 06:50:33,339] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92000, global step 1475603: learning rate 0.0000
[2019-04-04 06:50:34,144] A3C_AGENT_WORKER-Thread-12 INFO:Local step 92500, global step 1475995: loss 6.3188
[2019-04-04 06:50:34,145] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 92500, global step 1475997: learning rate 0.0000
[2019-04-04 06:50:34,249] A3C_AGENT_WORKER-Thread-16 INFO:Local step 92500, global step 1476031: loss 6.3122
[2019-04-04 06:50:34,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 92500, global step 1476034: learning rate 0.0000
[2019-04-04 06:50:34,729] A3C_AGENT_WORKER-Thread-5 INFO:Local step 92500, global step 1476251: loss 6.2893
[2019-04-04 06:50:34,731] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 92500, global step 1476253: learning rate 0.0000
[2019-04-04 06:50:34,948] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92000, global step 1476379: loss 0.0629
[2019-04-04 06:50:34,950] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92000, global step 1476379: learning rate 0.0000
[2019-04-04 06:50:34,954] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92000, global step 1476380: loss 0.0577
[2019-04-04 06:50:34,957] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92000, global step 1476380: learning rate 0.0000
[2019-04-04 06:50:35,125] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92000, global step 1476467: loss 0.0528
[2019-04-04 06:50:35,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92000, global step 1476467: learning rate 0.0000
[2019-04-04 06:50:35,536] A3C_AGENT_WORKER-Thread-18 INFO:Local step 92500, global step 1476706: loss 6.2875
[2019-04-04 06:50:35,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 92500, global step 1476706: learning rate 0.0000
[2019-04-04 06:50:36,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9334689e-10 5.0948090e-10 6.9558397e-25 1.7400398e-10 3.0959613e-11
 1.1966984e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:36,943] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4487
[2019-04-04 06:50:36,980] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.83333333333334, 0.0, 0.0, 26.0, 24.98969206164253, 0.4242190907436027, 1.0, 1.0, 18702.19783458726], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4565400.0000, 
sim time next is 4566000.0000, 
raw observation next is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.02470255925676, 0.4375907792396387, 0.0, 1.0, 198462.995736183], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.5366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5853918799380633, 0.6458635930798796, 0.0, 1.0, 0.9450618844580142], 
reward next is 0.0549, 
noisyNet noise sample is [array([0.77923155], dtype=float32), -1.3374372]. 
=============================================
[2019-04-04 06:50:36,991] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.21792 ]
 [83.93781 ]
 [81.922775]
 [84.74288 ]
 [83.50962 ]], R is [[82.6207428 ]
 [82.70547485]
 [82.73655701]
 [82.66005707]
 [82.56423187]].
[2019-04-04 06:50:37,134] A3C_AGENT_WORKER-Thread-20 INFO:Local step 92500, global step 1477607: loss 6.3800
[2019-04-04 06:50:37,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 92500, global step 1477607: learning rate 0.0000
[2019-04-04 06:50:37,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6727971e-10 1.5982160e-10 2.3632764e-25 7.5977863e-11 2.7635283e-11
 7.7336019e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:37,730] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2015
[2019-04-04 06:50:37,744] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 55.0, 162.5, 713.0, 26.0, 25.27084845983894, 0.398165575875906, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4273200.0000, 
sim time next is 4273800.0000, 
raw observation next is [5.333333333333334, 54.5, 148.6666666666667, 749.3333333333334, 26.0, 25.25425645486551, 0.3972033807040876, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6103416435826409, 0.545, 0.4955555555555557, 0.8279926335174954, 0.6666666666666666, 0.6045213712387923, 0.6324011269013625, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1923678], dtype=float32), 0.7319235]. 
=============================================
[2019-04-04 06:50:39,291] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.76404080e-09 5.36357891e-09 1.19136314e-23 5.31731104e-10
 3.17731064e-10 9.84516722e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:50:39,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7531
[2019-04-04 06:50:39,305] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 72.0, 0.0, 0.0, 26.0, 25.6546704228174, 0.4591525256196293, 0.0, 1.0, 18728.50836786075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4677600.0000, 
sim time next is 4678200.0000, 
raw observation next is [1.0, 77.0, 0.0, 0.0, 26.0, 25.57391221904671, 0.4455285577842235, 0.0, 1.0, 53848.20031037353], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6311593515872259, 0.6485095192614079, 0.0, 1.0, 0.25642000147796923], 
reward next is 0.7436, 
noisyNet noise sample is [array([-0.6622715], dtype=float32), -0.7734534]. 
=============================================
[2019-04-04 06:50:41,611] A3C_AGENT_WORKER-Thread-19 INFO:Local step 92500, global step 1480046: loss 6.4999
[2019-04-04 06:50:41,612] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 92500, global step 1480046: learning rate 0.0000
[2019-04-04 06:50:42,097] A3C_AGENT_WORKER-Thread-15 INFO:Local step 92500, global step 1480311: loss 6.4752
[2019-04-04 06:50:42,098] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 92500, global step 1480311: learning rate 0.0000
[2019-04-04 06:50:42,800] A3C_AGENT_WORKER-Thread-17 INFO:Local step 92500, global step 1480663: loss 6.4772
[2019-04-04 06:50:42,802] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 92500, global step 1480663: learning rate 0.0000
[2019-04-04 06:50:43,070] A3C_AGENT_WORKER-Thread-3 INFO:Local step 92500, global step 1480801: loss 6.4637
[2019-04-04 06:50:43,073] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 92500, global step 1480802: learning rate 0.0000
[2019-04-04 06:50:44,968] A3C_AGENT_WORKER-Thread-13 INFO:Local step 92500, global step 1481820: loss 6.5040
[2019-04-04 06:50:44,970] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 92500, global step 1481820: learning rate 0.0000
[2019-04-04 06:50:46,753] A3C_AGENT_WORKER-Thread-4 INFO:Local step 92500, global step 1482648: loss 6.4170
[2019-04-04 06:50:46,757] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 92500, global step 1482648: learning rate 0.0000
[2019-04-04 06:50:46,790] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7226744e-10 8.9556468e-10 6.1852391e-24 5.4195152e-11 5.2062733e-11
 2.5225904e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:46,791] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9989
[2019-04-04 06:50:46,850] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.06072901153695, 0.464276401202135, 0.0, 1.0, 18706.19033046643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4477800.0000, 
sim time next is 4478400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.0615544309861, 0.4575205022404298, 1.0, 1.0, 24200.12243433108], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5884628692488416, 0.6525068340801433, 1.0, 1.0, 0.11523867825871942], 
reward next is 0.8848, 
noisyNet noise sample is [array([-0.6209668], dtype=float32), -0.08889728]. 
=============================================
[2019-04-04 06:50:47,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.6494116e-09 8.3959995e-09 4.4187601e-22 2.0817379e-09 1.5301250e-09
 6.0413727e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:47,142] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9157
[2019-04-04 06:50:47,159] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.033333333333333, 92.16666666666667, 0.0, 0.0, 26.0, 24.13428911377499, 0.1495331681212612, 0.0, 1.0, 41588.18560989928], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4774200.0000, 
sim time next is 4774800.0000, 
raw observation next is [-6.066666666666666, 92.33333333333334, 0.0, 0.0, 26.0, 24.09174086750239, 0.1410417408938902, 0.0, 1.0, 41648.13431327002], 
processed observation next is [0.0, 0.2608695652173913, 0.2945521698984303, 0.9233333333333335, 0.0, 0.0, 0.6666666666666666, 0.507645072291866, 0.5470139136312967, 0.0, 1.0, 0.19832444911080963], 
reward next is 0.8017, 
noisyNet noise sample is [array([0.34104425], dtype=float32), -0.5485985]. 
=============================================
[2019-04-04 06:50:47,696] A3C_AGENT_WORKER-Thread-2 INFO:Local step 93000, global step 1483086: loss 1.0602
[2019-04-04 06:50:47,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 93000, global step 1483086: learning rate 0.0000
[2019-04-04 06:50:48,440] A3C_AGENT_WORKER-Thread-14 INFO:Local step 92500, global step 1483456: loss 6.4593
[2019-04-04 06:50:48,441] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 92500, global step 1483457: learning rate 0.0000
[2019-04-04 06:50:49,307] A3C_AGENT_WORKER-Thread-6 INFO:Local step 92500, global step 1483907: loss 6.4458
[2019-04-04 06:50:49,308] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 92500, global step 1483907: learning rate 0.0000
[2019-04-04 06:50:49,635] A3C_AGENT_WORKER-Thread-10 INFO:Local step 92500, global step 1484061: loss 6.4526
[2019-04-04 06:50:49,644] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 92500, global step 1484061: learning rate 0.0000
[2019-04-04 06:50:49,771] A3C_AGENT_WORKER-Thread-11 INFO:Local step 92500, global step 1484131: loss 6.4173
[2019-04-04 06:50:49,773] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 92500, global step 1484131: learning rate 0.0000
[2019-04-04 06:50:50,233] A3C_AGENT_WORKER-Thread-12 INFO:Local step 93000, global step 1484357: loss 1.0144
[2019-04-04 06:50:50,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 93000, global step 1484357: learning rate 0.0000
[2019-04-04 06:50:50,237] A3C_AGENT_WORKER-Thread-16 INFO:Local step 93000, global step 1484357: loss 1.0003
[2019-04-04 06:50:50,240] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 93000, global step 1484359: learning rate 0.0000
[2019-04-04 06:50:50,701] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8653081e-11 5.7145819e-11 3.4962667e-26 7.8543326e-12 3.1470421e-12
 1.4580674e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:50,701] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5700
[2019-04-04 06:50:50,732] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 51.33333333333334, 167.0, 16.0, 26.0, 25.256686478145, 0.4441886260079554, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4539000.0000, 
sim time next is 4539600.0000, 
raw observation next is [2.0, 52.0, 187.0, 24.0, 26.0, 25.41242610805704, 0.473907574850384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 0.52, 0.6233333333333333, 0.026519337016574586, 0.6666666666666666, 0.61770217567142, 0.6579691916167947, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7992115], dtype=float32), 0.720413]. 
=============================================
[2019-04-04 06:50:50,890] A3C_AGENT_WORKER-Thread-5 INFO:Local step 93000, global step 1484698: loss 0.9602
[2019-04-04 06:50:50,891] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 93000, global step 1484698: learning rate 0.0000
[2019-04-04 06:50:52,072] A3C_AGENT_WORKER-Thread-18 INFO:Local step 93000, global step 1485287: loss 0.9700
[2019-04-04 06:50:52,073] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 93000, global step 1485287: learning rate 0.0000
[2019-04-04 06:50:52,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1911728e-11 6.2921460e-11 3.9025405e-26 1.2158556e-11 1.8069564e-12
 5.3577239e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:52,487] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5799
[2019-04-04 06:50:52,570] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 81.66666666666667, 130.5, 0.0, 26.0, 24.91409252301909, 0.390569767586216, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4713600.0000, 
sim time next is 4714200.0000, 
raw observation next is [1.5, 79.5, 135.0, 0.0, 26.0, 24.35176183055236, 0.3895289635008682, 1.0, 1.0, 196918.5028107268], 
processed observation next is [1.0, 0.5652173913043478, 0.5041551246537397, 0.795, 0.45, 0.0, 0.6666666666666666, 0.5293134858793632, 0.6298429878336228, 1.0, 1.0, 0.9377071562415561], 
reward next is 0.0623, 
noisyNet noise sample is [array([1.0682031], dtype=float32), 1.4649733]. 
=============================================
[2019-04-04 06:50:53,870] A3C_AGENT_WORKER-Thread-20 INFO:Local step 93000, global step 1486124: loss 1.0198
[2019-04-04 06:50:53,879] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 93000, global step 1486124: learning rate 0.0000
[2019-04-04 06:50:54,836] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9814913e-09 3.5387722e-09 6.7185005e-23 2.6959017e-09 5.1589566e-10
 1.4659691e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:54,836] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4338
[2019-04-04 06:50:54,859] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.5, 0.0, 0.0, 26.0, 25.12713939175858, 0.3479835325957756, 0.0, 1.0, 152652.7670043305], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4908600.0000, 
sim time next is 4909200.0000, 
raw observation next is [1.0, 42.33333333333333, 0.0, 0.0, 26.0, 25.17728557210885, 0.3679739124701962, 0.0, 1.0, 82808.14351107727], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5981071310090709, 0.6226579708233987, 0.0, 1.0, 0.3943244929098918], 
reward next is 0.6057, 
noisyNet noise sample is [array([0.06426788], dtype=float32), 0.2390582]. 
=============================================
[2019-04-04 06:50:56,988] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.5525092e-10 2.3360118e-09 2.9824313e-23 2.1766042e-10 2.2772795e-10
 7.6385912e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:50:56,990] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3075
[2019-04-04 06:50:57,008] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.4772184056705, 0.507690522076037, 0.0, 1.0, 45586.71376037322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4667400.0000, 
sim time next is 4668000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.48425309024827, 0.5123761932454156, 0.0, 1.0, 33612.84438551131], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6236877575206892, 0.6707920644151386, 0.0, 1.0, 0.16006116374053003], 
reward next is 0.8399, 
noisyNet noise sample is [array([-0.06063307], dtype=float32), -3.4364808]. 
=============================================
[2019-04-04 06:50:57,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.28067]
 [81.236  ]
 [81.01552]
 [80.83815]
 [80.72531]], R is [[81.2896347 ]
 [81.25965881]
 [81.14636993]
 [81.03981781]
 [81.02380371]].
[2019-04-04 06:50:57,770] A3C_AGENT_WORKER-Thread-19 INFO:Local step 93000, global step 1488068: loss 0.9490
[2019-04-04 06:50:57,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 93000, global step 1488069: learning rate 0.0000
[2019-04-04 06:50:58,597] A3C_AGENT_WORKER-Thread-15 INFO:Local step 93000, global step 1488486: loss 0.9624
[2019-04-04 06:50:58,600] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 93000, global step 1488487: learning rate 0.0000
[2019-04-04 06:50:58,869] A3C_AGENT_WORKER-Thread-17 INFO:Local step 93000, global step 1488615: loss 0.9297
[2019-04-04 06:50:58,871] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 93000, global step 1488617: learning rate 0.0000
[2019-04-04 06:50:58,980] A3C_AGENT_WORKER-Thread-3 INFO:Local step 93000, global step 1488666: loss 0.9752
[2019-04-04 06:50:58,980] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 93000, global step 1488666: learning rate 0.0000
[2019-04-04 06:51:00,850] A3C_AGENT_WORKER-Thread-13 INFO:Local step 93000, global step 1489625: loss 1.0563
[2019-04-04 06:51:00,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 93000, global step 1489625: learning rate 0.0000
[2019-04-04 06:51:02,622] A3C_AGENT_WORKER-Thread-4 INFO:Local step 93000, global step 1490450: loss 0.9864
[2019-04-04 06:51:02,623] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 93000, global step 1490450: learning rate 0.0000
[2019-04-04 06:51:03,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:03,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:03,108] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run12
[2019-04-04 06:51:04,417] A3C_AGENT_WORKER-Thread-14 INFO:Local step 93000, global step 1491327: loss 1.0075
[2019-04-04 06:51:04,418] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 93000, global step 1491328: learning rate 0.0000
[2019-04-04 06:51:04,833] A3C_AGENT_WORKER-Thread-6 INFO:Local step 93000, global step 1491530: loss 0.9840
[2019-04-04 06:51:04,835] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 93000, global step 1491531: learning rate 0.0000
[2019-04-04 06:51:05,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:05,078] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:05,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:05,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:05,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run12
[2019-04-04 06:51:05,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run12
[2019-04-04 06:51:05,523] A3C_AGENT_WORKER-Thread-11 INFO:Local step 93000, global step 1491794: loss 1.0095
[2019-04-04 06:51:05,524] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 93000, global step 1491794: learning rate 0.0000
[2019-04-04 06:51:05,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:05,867] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:05,887] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run12
[2019-04-04 06:51:05,888] A3C_AGENT_WORKER-Thread-10 INFO:Local step 93000, global step 1491936: loss 0.9967
[2019-04-04 06:51:05,907] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 93000, global step 1491936: learning rate 0.0000
[2019-04-04 06:51:07,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:07,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:07,478] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run12
[2019-04-04 06:51:08,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:08,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:08,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run12
[2019-04-04 06:51:12,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:12,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:12,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run12
[2019-04-04 06:51:13,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:13,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:13,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run12
[2019-04-04 06:51:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:14,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:14,191] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run12
[2019-04-04 06:51:14,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:14,693] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:14,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run12
[2019-04-04 06:51:15,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4224492e-11 9.7173644e-11 1.5958247e-25 2.5640830e-11 5.8950261e-11
 2.7126947e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:51:15,589] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7489
[2019-04-04 06:51:15,605] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.5, 19.0, 0.0, 0.0, 26.0, 27.26932197662799, 0.887369831666908, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5085000.0000, 
sim time next is 5085600.0000, 
raw observation next is [9.333333333333334, 19.0, 0.0, 0.0, 26.0, 27.20477274652416, 0.8799240333733672, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.7211449676823639, 0.19, 0.0, 0.0, 0.6666666666666666, 0.76706439554368, 0.7933080111244557, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7908884], dtype=float32), -2.07216]. 
=============================================
[2019-04-04 06:51:17,538] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:17,539] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:17,559] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run12
[2019-04-04 06:51:21,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:21,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:21,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run12
[2019-04-04 06:51:23,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:23,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:23,701] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run12
[2019-04-04 06:51:23,789] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:23,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:23,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run12
[2019-04-04 06:51:26,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:26,328] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:26,332] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run12
[2019-04-04 06:51:26,672] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 06:51:26,672] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:26,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run12
[2019-04-04 06:51:40,706] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 06:51:40,739] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 06:51:40,740] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 06:51:40,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:40,756] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 06:51:40,768] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:40,780] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 06:51:40,866] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run16
[2019-04-04 06:51:40,912] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run16
[2019-04-04 06:51:41,048] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run16
[2019-04-04 06:52:49,286] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2217608], dtype=float32), -0.019456396]
[2019-04-04 06:52:49,286] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.01666666666666667, 58.0, 153.0, 547.6666666666666, 26.0, 25.0379402121482, 0.3543721019589487, 0.0, 1.0, 0.0]
[2019-04-04 06:52:49,286] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:52:49,287] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.1594103e-10 1.0836245e-10 9.8017131e-25 3.2572136e-11 2.1016076e-11
 3.1012993e-14 1.0000000e+00], sampled 0.27418778171645286
[2019-04-04 06:53:30,568] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2217608], dtype=float32), -0.019456396]
[2019-04-04 06:53:30,568] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.183333333333334, 80.66666666666667, 0.0, 0.0, 26.0, 24.35678824477185, 0.1219610098525142, 0.0, 1.0, 40514.43429153777]
[2019-04-04 06:53:30,569] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 06:53:30,570] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.3642613e-09 1.8004742e-09 3.6272639e-23 4.6809701e-10 3.4880093e-10
 7.6037082e-13 1.0000000e+00], sampled 0.3720996454855139
[2019-04-04 06:54:38,838] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2217608], dtype=float32), -0.019456396]
[2019-04-04 06:54:38,839] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.666666666666666, 43.0, 116.3333333333333, 827.1666666666667, 26.0, 25.33545008051765, 0.455047184441693, 0.0, 1.0, 0.0]
[2019-04-04 06:54:38,839] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:54:38,839] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.9570703e-11 3.0582412e-11 2.9541756e-26 1.0962238e-11 4.9168599e-12
 4.2478584e-15 1.0000000e+00], sampled 0.8570734602149923
[2019-04-04 06:54:52,369] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 06:54:58,945] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2217608], dtype=float32), -0.019456396]
[2019-04-04 06:54:58,945] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.7, 41.0, 0.0, 0.0, 26.0, 25.02938354850775, 0.324828081367016, 0.0, 1.0, 47189.9575559425]
[2019-04-04 06:54:58,946] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:54:58,947] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.3018887e-09 2.0121977e-09 4.0218232e-22 5.7026406e-10 3.1153330e-10
 1.3740990e-12 1.0000000e+00], sampled 0.003824136009254464
[2019-04-04 06:55:07,146] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2217608], dtype=float32), -0.019456396]
[2019-04-04 06:55:07,146] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-0.4333333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.37946088852097, 0.4775966046069735, 0.0, 1.0, 42899.37215551726]
[2019-04-04 06:55:07,146] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 06:55:07,147] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.6447138e-10 5.0744137e-10 7.0646273e-24 1.3033341e-10 1.1106960e-10
 2.0971568e-13 1.0000000e+00], sampled 0.2932554530237578
[2019-04-04 06:55:28,097] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 06:55:36,107] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 06:55:37,154] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1500000, evaluation results [1500000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 06:56:05,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.43760240e-10 5.68766874e-11 1.16595535e-24 1.23231134e-10
 2.90278218e-12 3.30096329e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 06:56:05,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-04 06:56:05,230] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.38333333333333, 67.5, 51.66666666666666, 385.0, 26.0, 25.79070427502599, 0.3113413593197087, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 291000.0000, 
sim time next is 291600.0000, 
raw observation next is [-12.3, 67.0, 62.5, 384.5, 26.0, 25.70795429258868, 0.3036182209135867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.12188365650969527, 0.67, 0.20833333333333334, 0.4248618784530387, 0.6666666666666666, 0.6423295243823901, 0.6012060736378623, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39533302], dtype=float32), 0.35618815]. 
=============================================
[2019-04-04 06:56:06,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6157274e-11 3.9616307e-11 7.2663876e-26 3.0894887e-12 7.4876943e-13
 3.2636762e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:06,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6076
[2019-04-04 06:56:06,610] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.23333333333333, 82.0, 36.66666666666666, 694.5, 26.0, 25.35711787261956, 0.220502922722089, 1.0, 1.0, 38015.83886369481], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 379200.0000, 
sim time next is 379800.0000, 
raw observation next is [-15.05, 78.0, 39.0, 738.0, 26.0, 25.38449185070468, 0.2460616842839414, 1.0, 1.0, 53266.67675694892], 
processed observation next is [1.0, 0.391304347826087, 0.0457063711911357, 0.78, 0.13, 0.8154696132596685, 0.6666666666666666, 0.6153743208920567, 0.5820205614279804, 1.0, 1.0, 0.25365084169975677], 
reward next is 0.7463, 
noisyNet noise sample is [array([0.6330247], dtype=float32), 0.5447597]. 
=============================================
[2019-04-04 06:56:14,863] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7414497e-09 8.6894083e-09 2.3734088e-21 2.2404869e-09 8.9788393e-10
 7.3012551e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:14,864] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1055
[2019-04-04 06:56:14,903] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.5874010208961, -0.07478199377538243, 0.0, 1.0, 45486.13177114395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 436800.0000, 
sim time next is 437400.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.55342212076916, -0.09114813129938894, 0.0, 1.0, 45547.7176947196], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.4627851767307633, 0.4696172895668704, 0.0, 1.0, 0.21689389378437904], 
reward next is 0.7831, 
noisyNet noise sample is [array([1.7675157], dtype=float32), -1.4026034]. 
=============================================
[2019-04-04 06:56:20,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7953951e-10 1.3083827e-10 3.2641292e-26 2.1332745e-11 1.0152288e-11
 2.4006250e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:20,489] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1501
[2019-04-04 06:56:20,585] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.08878445193561, 0.2881411100600778, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 551400.0000, 
sim time next is 552000.0000, 
raw observation next is [-0.2, 89.66666666666667, 125.6666666666667, 103.1666666666667, 26.0, 25.03771370157387, 0.279485018707, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4570637119113574, 0.8966666666666667, 0.418888888888889, 0.11399631675874773, 0.6666666666666666, 0.5864761417978226, 0.5931616729023333, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.24317469], dtype=float32), 0.35154837]. 
=============================================
[2019-04-04 06:56:20,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.37656 ]
 [89.162994]
 [88.903015]
 [88.67403 ]
 [88.45938 ]], R is [[89.62451935]
 [89.72827148]
 [89.83098602]
 [89.93267822]
 [90.03335571]].
[2019-04-04 06:56:23,745] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3455983e-11 3.0905467e-10 2.9407300e-25 4.2253302e-11 7.3433246e-11
 2.3381990e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:23,747] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4078
[2019-04-04 06:56:23,791] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.3, 96.0, 0.0, 0.0, 26.0, 24.83589931883902, 0.2335312388634035, 0.0, 1.0, 40390.96628448059], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 514800.0000, 
sim time next is 515400.0000, 
raw observation next is [3.383333333333333, 96.16666666666666, 0.0, 0.0, 26.0, 24.83846333596739, 0.2335783803970969, 0.0, 1.0, 40247.04249967411], 
processed observation next is [1.0, 1.0, 0.5563250230840259, 0.9616666666666666, 0.0, 0.0, 0.6666666666666666, 0.5698719446639492, 0.5778594601323657, 0.0, 1.0, 0.19165258333178148], 
reward next is 0.8083, 
noisyNet noise sample is [array([1.2020879], dtype=float32), 1.2895128]. 
=============================================
[2019-04-04 06:56:27,769] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0866088e-10 2.8553679e-10 7.5946898e-25 1.2486270e-10 1.0383333e-11
 4.6292414e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:27,769] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9464
[2019-04-04 06:56:27,893] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 25.0, 125.5, 0.0, 26.0, 25.14699155037999, 0.1672624198964032, 1.0, 1.0, 18707.68748912897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 475200.0000, 
sim time next is 475800.0000, 
raw observation next is [-1.616666666666667, 25.5, 126.6666666666667, 0.0, 26.0, 25.19110648647009, 0.1731500644629232, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4178208679593721, 0.255, 0.42222222222222233, 0.0, 0.6666666666666666, 0.5992588738725075, 0.5577166881543077, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69912493], dtype=float32), 0.30096963]. 
=============================================
[2019-04-04 06:56:36,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1263584e-10 1.6907978e-10 6.4999989e-25 4.0018509e-11 2.6627128e-11
 2.5220504e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:36,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4521
[2019-04-04 06:56:36,188] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 84.33333333333333, 0.0, 0.0, 26.0, 25.11884961143441, 0.2999147592959887, 0.0, 1.0, 42885.29268548699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 592800.0000, 
sim time next is 593400.0000, 
raw observation next is [-2.8, 83.66666666666667, 0.0, 0.0, 26.0, 25.0987207397736, 0.2927237319419615, 0.0, 1.0, 42941.76721682448], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5915600616478001, 0.5975745773139872, 0.0, 1.0, 0.2044846057944023], 
reward next is 0.7955, 
noisyNet noise sample is [array([-1.0086132], dtype=float32), 0.5529757]. 
=============================================
[2019-04-04 06:56:39,311] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9124054e-11 6.2635522e-11 8.5701602e-27 1.6061873e-12 2.6271862e-12
 5.3849349e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:39,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3279
[2019-04-04 06:56:39,346] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.65, 88.5, 0.0, 0.0, 26.0, 25.02892288957022, 0.2584819736073531, 0.0, 1.0, 39520.74215865227], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 523800.0000, 
sim time next is 524400.0000, 
raw observation next is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.99946733032607, 0.2527773886542353, 0.0, 1.0, 39560.50337170452], 
processed observation next is [0.0, 0.043478260869565216, 0.5881809787626964, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5832889441938391, 0.5842591295514118, 0.0, 1.0, 0.18838334938906914], 
reward next is 0.8116, 
noisyNet noise sample is [array([0.5661993], dtype=float32), -0.92268384]. 
=============================================
[2019-04-04 06:56:40,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7563023e-09 2.7838887e-09 6.4089553e-23 3.3710554e-10 9.4176562e-11
 4.0491430e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:40,645] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2397
[2019-04-04 06:56:40,662] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.80262632817244, -0.0109433667884249, 0.0, 1.0, 41711.92404707265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 795600.0000, 
sim time next is 796200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.75714056657963, -0.02325356944930174, 0.0, 1.0, 41821.61051507924], 
processed observation next is [1.0, 0.21739130434782608, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.47976171388163574, 0.4922488101835661, 0.0, 1.0, 0.1991505262622821], 
reward next is 0.8008, 
noisyNet noise sample is [array([-0.9081268], dtype=float32), 0.26658723]. 
=============================================
[2019-04-04 06:56:49,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5420902e-12 4.8320887e-12 1.1646200e-27 9.9510406e-13 9.1064721e-13
 1.1974004e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:56:49,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7487
[2019-04-04 06:56:49,490] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 26.0, 25.8369665905087, 0.4438218008294426, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 742800.0000, 
sim time next is 743400.0000, 
raw observation next is [0.25, 46.0, 80.0, 714.0, 26.0, 25.89073588865145, 0.4519577484267781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46952908587257625, 0.46, 0.26666666666666666, 0.7889502762430939, 0.6666666666666666, 0.6575613240542874, 0.650652582808926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.76300347], dtype=float32), 0.4385086]. 
=============================================
[2019-04-04 06:57:05,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0021697e-12 7.6362970e-12 1.8708376e-27 1.8530136e-12 6.3559238e-13
 5.8638476e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:05,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7239
[2019-04-04 06:57:05,913] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 92.83333333333333, 30.0, 0.0, 26.0, 25.78394147640775, 0.4027152533576151, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922200.0000, 
sim time next is 922800.0000, 
raw observation next is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 25.76920356991501, 0.3958484809577676, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5900277008310251, 0.9266666666666667, 0.08, 0.0, 0.6666666666666666, 0.6474336308262508, 0.6319494936525892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47454026], dtype=float32), 1.108849]. 
=============================================
[2019-04-04 06:57:07,309] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5916094e-09 5.2018490e-10 7.7096658e-26 2.8890907e-11 1.8266921e-11
 5.8743537e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:07,318] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6008
[2019-04-04 06:57:07,341] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 82.0, 0.0, 0.0, 26.0, 25.66200814051557, 0.6073169228567901, 0.0, 1.0, 31205.84340241796], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1144200.0000, 
sim time next is 1144800.0000, 
raw observation next is [11.6, 83.0, 0.0, 0.0, 26.0, 25.64113713318578, 0.6069644821957371, 0.0, 1.0, 38194.73090686744], 
processed observation next is [0.0, 0.2608695652173913, 0.7839335180055402, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6367614277654816, 0.7023214940652457, 0.0, 1.0, 0.18187967098508304], 
reward next is 0.8181, 
noisyNet noise sample is [array([0.50120014], dtype=float32), 1.5456369]. 
=============================================
[2019-04-04 06:57:08,601] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.7374253e-09 5.5349636e-10 2.4497555e-23 2.1118754e-10 1.1349337e-10
 7.5938055e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:08,604] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7588
[2019-04-04 06:57:08,609] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.38333333333333, 64.66666666666667, 19.33333333333334, 0.0, 26.0, 24.95630396905947, 0.4613817278594322, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1183800.0000, 
sim time next is 1184400.0000, 
raw observation next is [18.3, 65.0, 14.5, 0.0, 26.0, 24.93528120249126, 0.4565269821222449, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.9695290858725764, 0.65, 0.04833333333333333, 0.0, 0.6666666666666666, 0.5779401002076051, 0.652175660707415, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.178906], dtype=float32), 0.5323207]. 
=============================================
[2019-04-04 06:57:15,945] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2273889e-09 6.7896810e-10 4.1828600e-24 3.8200082e-10 2.2185095e-10
 5.7882274e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:15,952] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5437
[2019-04-04 06:57:15,956] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.78333333333333, 73.66666666666666, 0.0, 0.0, 26.0, 24.50793344844714, 0.3517342008147948, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1201800.0000, 
sim time next is 1202400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.47428833839148, 0.3473750986283024, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5395240281992901, 0.6157916995427675, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5377088], dtype=float32), 0.80103874]. 
=============================================
[2019-04-04 06:57:26,805] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4062982e-11 1.2516434e-10 2.3892829e-25 1.7901892e-11 2.6953335e-11
 8.7402328e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:26,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6819
[2019-04-04 06:57:26,817] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 78.0, 0.0, 26.0, 25.87647037112631, 0.496283320158637, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1421400.0000, 
sim time next is 1422000.0000, 
raw observation next is [0.0, 95.0, 81.0, 0.0, 26.0, 25.87049334222059, 0.4879157180201378, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.27, 0.0, 0.6666666666666666, 0.6558744451850492, 0.6626385726733792, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7644132], dtype=float32), 0.22845188]. 
=============================================
[2019-04-04 06:57:26,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[88.77924 ]
 [89.04767 ]
 [89.033646]
 [89.25441 ]
 [89.441414]], R is [[88.82190704]
 [88.9336853 ]
 [89.04434967]
 [89.15390778]
 [89.26236725]].
[2019-04-04 06:57:27,423] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9662049e-11 7.5451048e-11 1.9640430e-26 9.8040404e-12 3.0155444e-12
 7.3356103e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:27,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6214
[2019-04-04 06:57:27,456] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 98.33333333333334, 41.33333333333334, 0.0, 26.0, 25.91877907585159, 0.5087338201462407, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1416000.0000, 
sim time next is 1416600.0000, 
raw observation next is [-0.3, 97.5, 46.0, 0.0, 26.0, 25.94719382034214, 0.5155366606126869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4542936288088643, 0.975, 0.15333333333333332, 0.0, 0.6666666666666666, 0.6622661516951783, 0.6718455535375623, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1368084], dtype=float32), 1.9895123]. 
=============================================
[2019-04-04 06:57:29,766] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5756950e-11 1.9479115e-11 3.2434506e-26 2.2346155e-12 1.0701310e-12
 2.9934083e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:29,770] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1755
[2019-04-04 06:57:29,787] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 59.0, 0.0, 26.0, 26.01572942356153, 0.5191180858962029, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1418400.0000, 
sim time next is 1419000.0000, 
raw observation next is [0.0, 95.0, 63.33333333333334, 0.0, 26.0, 26.02197147757976, 0.5150375394708272, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.21111111111111114, 0.0, 0.6666666666666666, 0.6684976231316467, 0.671679179823609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.82467735], dtype=float32), 0.8402981]. 
=============================================
[2019-04-04 06:57:29,810] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.39206]
 [87.57107]
 [87.69734]
 [87.83265]
 [87.9447 ]], R is [[87.31013489]
 [87.43703461]
 [87.56266785]
 [87.68704224]
 [87.81017303]].
[2019-04-04 06:57:44,843] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.4302799e-10 1.6347448e-09 1.4378158e-22 4.7195825e-10 2.7934560e-10
 2.8971569e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:44,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1126
[2019-04-04 06:57:44,901] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.55, 83.0, 126.0, 0.0, 26.0, 24.94465363222898, 0.3516515443336399, 0.0, 1.0, 45027.94158808923], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1773000.0000, 
sim time next is 1773600.0000, 
raw observation next is [-2.633333333333333, 83.0, 124.8333333333333, 0.0, 26.0, 24.9674101224044, 0.3565215298460506, 0.0, 1.0, 33893.49559521452], 
processed observation next is [0.0, 0.5217391304347826, 0.38965835641735924, 0.83, 0.416111111111111, 0.0, 0.6666666666666666, 0.5806175102003666, 0.6188405099486836, 0.0, 1.0, 0.16139759807245008], 
reward next is 0.8386, 
noisyNet noise sample is [array([-0.12853193], dtype=float32), 1.0587656]. 
=============================================
[2019-04-04 06:57:46,995] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.6511575e-11 5.9632181e-11 4.2925487e-26 1.1773558e-11 7.7355422e-12
 8.2244934e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:46,996] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8029
[2019-04-04 06:57:47,013] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 88.33333333333334, 0.0, 0.0, 26.0, 25.57728977731026, 0.5911557370161137, 0.0, 1.0, 56411.82757265789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1642800.0000, 
sim time next is 1643400.0000, 
raw observation next is [6.9, 89.5, 0.0, 0.0, 26.0, 25.62261450105436, 0.5987663214042377, 0.0, 1.0, 18730.31523188282], 
processed observation next is [1.0, 0.0, 0.6537396121883658, 0.895, 0.0, 0.0, 0.6666666666666666, 0.6352178750878634, 0.6995887738014126, 0.0, 1.0, 0.0891919772946801], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.5869752], dtype=float32), -0.14222607]. 
=============================================
[2019-04-04 06:57:52,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6664952e-09 5.4110099e-09 5.8100350e-22 1.0144459e-09 1.0838049e-09
 3.8037399e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:57:52,728] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4545
[2019-04-04 06:57:52,774] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 80.5, 0.0, 0.0, 26.0, 24.62126590546995, 0.2120908213666982, 0.0, 1.0, 45483.34585050339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1812600.0000, 
sim time next is 1813200.0000, 
raw observation next is [-5.0, 80.0, 0.0, 0.0, 26.0, 24.59074110280127, 0.2054997158288671, 0.0, 1.0, 45486.24304835903], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5492284252334393, 0.568499905276289, 0.0, 1.0, 0.21660115737313823], 
reward next is 0.7834, 
noisyNet noise sample is [array([-1.1225662], dtype=float32), -1.192358]. 
=============================================
[2019-04-04 06:58:02,192] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4832619e-09 1.5805290e-08 8.1280880e-22 2.2777062e-09 8.9862406e-10
 2.0674107e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:58:02,196] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8826
[2019-04-04 06:58:02,230] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 84.0, 0.0, 0.0, 26.0, 24.9470655442345, 0.2173525773555441, 0.0, 1.0, 44525.56735636412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1888800.0000, 
sim time next is 1889400.0000, 
raw observation next is [-5.6, 83.5, 0.0, 0.0, 26.0, 24.91184248868616, 0.2097648681887673, 0.0, 1.0, 44658.27536145842], 
processed observation next is [0.0, 0.8695652173913043, 0.30747922437673136, 0.835, 0.0, 0.0, 0.6666666666666666, 0.5759868740571799, 0.5699216227295891, 0.0, 1.0, 0.21265845410218295], 
reward next is 0.7873, 
noisyNet noise sample is [array([-0.24409725], dtype=float32), -0.42388278]. 
=============================================
[2019-04-04 06:58:03,261] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5428261e-09 5.4722040e-09 5.6031440e-22 1.3666439e-09 9.6530839e-10
 1.5174885e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:58:03,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8849
[2019-04-04 06:58:03,301] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 80.5, 0.0, 0.0, 26.0, 25.24754887614342, 0.382502738812836, 0.0, 1.0, 64349.57585513288], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [-5.6, 79.66666666666667, 0.0, 0.0, 26.0, 25.24228613064496, 0.3809376311804593, 0.0, 1.0, 51247.72345694351], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6035238442204133, 0.6269792103934865, 0.0, 1.0, 0.24403677836639767], 
reward next is 0.7560, 
noisyNet noise sample is [array([-0.9161494], dtype=float32), -0.8888942]. 
=============================================
[2019-04-04 06:58:14,673] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.7463877e-11 6.9264323e-11 1.1222768e-24 1.4740785e-11 7.4546662e-12
 1.0426348e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:58:14,674] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9450
[2019-04-04 06:58:14,725] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 26.0, 25.50084851172112, 0.3160404774809554, 1.0, 1.0, 33520.62074970889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1959600.0000, 
sim time next is 1960200.0000, 
raw observation next is [-3.35, 68.5, 30.0, 0.0, 26.0, 25.53503542476353, 0.3250331878569039, 1.0, 1.0, 34846.71876804351], 
processed observation next is [1.0, 0.6956521739130435, 0.3698060941828255, 0.685, 0.1, 0.0, 0.6666666666666666, 0.6279196187302943, 0.6083443959523013, 1.0, 1.0, 0.16593675603830244], 
reward next is 0.8341, 
noisyNet noise sample is [array([-1.8410003], dtype=float32), 0.42731404]. 
=============================================
[2019-04-04 06:58:24,230] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.5772688e-10 1.0251360e-09 3.3782870e-23 1.5428342e-10 1.2060171e-10
 7.3529176e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:58:24,233] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5416
[2019-04-04 06:58:24,272] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.67120873571065, 0.2435578951541035, 0.0, 1.0, 44124.53194704583], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.61607529525443, 0.2322844172711413, 0.0, 1.0, 44119.31403432272], 
processed observation next is [1.0, 1.0, 0.28393351800554023, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5513396079378691, 0.5774281390903805, 0.0, 1.0, 0.21009197159201295], 
reward next is 0.7899, 
noisyNet noise sample is [array([2.7379882], dtype=float32), -0.4386262]. 
=============================================
[2019-04-04 06:58:48,168] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.8859726e-11 1.5179880e-10 9.2987857e-25 9.1143357e-11 4.1877868e-12
 1.3893202e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:58:48,169] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5477
[2019-04-04 06:58:48,213] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 24.89548908209278, 0.3161692003843071, 1.0, 1.0, 135165.556747875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2309400.0000, 
sim time next is 2310000.0000, 
raw observation next is [-1.0, 51.0, 0.0, 0.0, 26.0, 25.14712304855039, 0.3949133395880278, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4349030470914128, 0.51, 0.0, 0.0, 0.6666666666666666, 0.5955935873791992, 0.6316377798626759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5755511], dtype=float32), 0.17613082]. 
=============================================
[2019-04-04 06:58:48,234] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.86126 ]
 [84.14308 ]
 [82.94835 ]
 [82.36785 ]
 [82.238556]], R is [[85.10614777]
 [84.61144257]
 [83.82738495]
 [83.61898804]
 [83.78279877]].
[2019-04-04 06:58:48,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1477622e-09 3.4990790e-09 1.5331180e-22 1.0863277e-09 7.1234962e-10
 2.4131267e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:58:48,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3215
[2019-04-04 06:58:48,633] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 48.16666666666667, 0.0, 0.0, 26.0, 24.99354677001551, 0.1776982852619892, 0.0, 1.0, 38414.98633304137], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2519400.0000, 
sim time next is 2520000.0000, 
raw observation next is [-1.7, 49.0, 0.0, 0.0, 26.0, 24.97477623761025, 0.1691429071736763, 0.0, 1.0, 38367.70237698045], 
processed observation next is [1.0, 0.17391304347826086, 0.4155124653739613, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5812313531341875, 0.5563809690578921, 0.0, 1.0, 0.18270334465228785], 
reward next is 0.8173, 
noisyNet noise sample is [array([-1.9259387], dtype=float32), -0.16788569]. 
=============================================
[2019-04-04 06:58:48,642] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.3388 ]
 [77.298  ]
 [77.24713]
 [77.19977]
 [77.16275]], R is [[77.41926575]
 [77.46214294]
 [77.50453949]
 [77.54636383]
 [77.58751678]].
[2019-04-04 06:58:57,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1966426e-11 5.3649238e-11 4.2291622e-25 1.8598057e-11 6.8807898e-12
 2.0120238e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:58:57,045] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8197
[2019-04-04 06:58:57,061] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 54.0, 234.5, 159.0, 26.0, 25.75122151300686, 0.3944796894202997, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2635200.0000, 
sim time next is 2635800.0000, 
raw observation next is [-2.016666666666667, 52.83333333333334, 238.0, 155.0, 26.0, 25.75410879292788, 0.3978622046072235, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4067405355493998, 0.5283333333333334, 0.7933333333333333, 0.1712707182320442, 0.6666666666666666, 0.64617573274399, 0.6326207348690746, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5889587], dtype=float32), -0.7277316]. 
=============================================
[2019-04-04 06:59:09,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.88418650e-10 9.91225990e-10 1.09590412e-22 2.07628845e-10
 1.04228244e-10 4.24845407e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 06:59:09,827] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9794
[2019-04-04 06:59:09,873] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76888677290717, 0.2720552368837709, 0.0, 1.0, 44134.24324881612], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2764200.0000, 
sim time next is 2764800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.73270997945984, 0.2636259061317492, 0.0, 1.0, 43861.36954513732], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5610591649549868, 0.5878753020439164, 0.0, 1.0, 0.2088636645006539], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.36051986], dtype=float32), -0.4033904]. 
=============================================
[2019-04-04 06:59:14,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0099314e-11 7.0386481e-11 8.2215614e-26 3.6900830e-11 1.1393658e-12
 4.6880905e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:14,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9109
[2019-04-04 06:59:14,629] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.8, 24.5, 95.0, 0.0, 26.0, 25.6605554475612, 0.3820006900448127, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2820600.0000, 
sim time next is 2821200.0000, 
raw observation next is [6.733333333333333, 24.66666666666666, 91.0, 12.66666666666666, 26.0, 25.72998452382454, 0.402434267505035, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.24666666666666662, 0.30333333333333334, 0.013996316758747691, 0.6666666666666666, 0.6441653769853785, 0.6341447558350116, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9465819], dtype=float32), -0.14011316]. 
=============================================
[2019-04-04 06:59:15,237] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6693083e-11 2.1289009e-11 8.1469181e-25 6.3887797e-12 1.5362257e-12
 4.8354468e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:15,237] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8530
[2019-04-04 06:59:15,274] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 55.66666666666667, 104.3333333333333, 751.3333333333334, 26.0, 26.59198747165387, 0.6425008479013279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2729400.0000, 
sim time next is 2730000.0000, 
raw observation next is [-4.533333333333333, 55.33333333333334, 103.1666666666667, 742.1666666666667, 26.0, 26.66551651425414, 0.652406935405158, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3370267774699908, 0.5533333333333335, 0.343888888888889, 0.8200736648250462, 0.6666666666666666, 0.722126376187845, 0.717468978468386, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14962009], dtype=float32), -0.11744117]. 
=============================================
[2019-04-04 06:59:15,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.805214]
 [85.81023 ]
 [85.83666 ]
 [85.91266 ]
 [85.936226]], R is [[85.96318054]
 [86.10354614]
 [86.24250793]
 [86.38008118]
 [86.51628113]].
[2019-04-04 06:59:20,917] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5710620e-11 1.0216337e-10 7.5297578e-24 6.1750015e-11 5.3142638e-12
 2.6553344e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:20,918] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8748
[2019-04-04 06:59:20,952] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.133333333333333, 54.33333333333333, 96.66666666666667, 693.3333333333334, 26.0, 26.77187813220095, 0.5559290625408814, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2731800.0000, 
sim time next is 2732400.0000, 
raw observation next is [-4.0, 54.0, 94.0, 673.5, 26.0, 26.03710124918757, 0.5715649199430146, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3518005540166205, 0.54, 0.31333333333333335, 0.7441988950276243, 0.6666666666666666, 0.6697584374322977, 0.6905216399810049, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3077333], dtype=float32), -0.96239245]. 
=============================================
[2019-04-04 06:59:24,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.1674867e-10 7.3294892e-10 3.2910640e-24 1.0183672e-10 1.8518813e-10
 1.8133947e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:24,605] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6992
[2019-04-04 06:59:24,632] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.11922425207557, 0.2957752791113055, 0.0, 1.0, 56223.8237837162], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2859600.0000, 
sim time next is 2860200.0000, 
raw observation next is [1.0, 82.5, 0.0, 0.0, 26.0, 25.1067870447082, 0.2943147519210703, 0.0, 1.0, 56190.29543009695], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.825, 0.0, 0.0, 0.6666666666666666, 0.5922322537256832, 0.5981049173070234, 0.0, 1.0, 0.2675728353814141], 
reward next is 0.7324, 
noisyNet noise sample is [array([-0.9658578], dtype=float32), -1.4648081]. 
=============================================
[2019-04-04 06:59:24,651] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8722482e-10 2.1285458e-09 3.2935162e-22 5.0377938e-11 3.0862821e-10
 1.7873146e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:24,652] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8985
[2019-04-04 06:59:24,693] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 62.33333333333334, 0.0, 0.0, 26.0, 25.40096759305561, 0.4453351316565221, 0.0, 1.0, 55247.92384989408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2755200.0000, 
sim time next is 2755800.0000, 
raw observation next is [-6.0, 61.5, 0.0, 0.0, 26.0, 25.36828191723548, 0.4428042979780642, 0.0, 1.0, 67435.26221898962], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.615, 0.0, 0.0, 0.6666666666666666, 0.6140234931029566, 0.6476014326593548, 0.0, 1.0, 0.32112029628090294], 
reward next is 0.6789, 
noisyNet noise sample is [array([0.36625957], dtype=float32), 0.4689174]. 
=============================================
[2019-04-04 06:59:30,969] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7582078e-10 9.3190622e-10 1.2341796e-24 1.6756094e-10 2.3694416e-10
 2.0308902e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:30,969] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4870
[2019-04-04 06:59:31,004] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.13627731932959, 0.3070852976943839, 0.0, 1.0, 54032.04765298268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2857200.0000, 
sim time next is 2857800.0000, 
raw observation next is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.15195411214892, 0.3102157696109882, 0.0, 1.0, 55492.44893933398], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7783333333333334, 0.0, 0.0, 0.6666666666666666, 0.5959961760124101, 0.6034052565369961, 0.0, 1.0, 0.26424975685397134], 
reward next is 0.7358, 
noisyNet noise sample is [array([-2.2491665], dtype=float32), 1.5426183]. 
=============================================
[2019-04-04 06:59:45,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.7996061e-10 2.7126484e-10 3.0552066e-24 2.2168641e-10 1.8045990e-10
 1.0266234e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:45,161] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0052
[2019-04-04 06:59:45,199] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 100.0, 0.0, 0.0, 26.0, 25.3783904239165, 0.5653839629524948, 0.0, 1.0, 60892.10093284738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3205200.0000, 
sim time next is 3205800.0000, 
raw observation next is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.4725548184096, 0.5807206194626013, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.44875346260387816, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6227129015341332, 0.6935735398208672, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29581678], dtype=float32), -1.983017]. 
=============================================
[2019-04-04 06:59:55,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1150610e-09 6.0202971e-09 3.2282619e-22 7.8212870e-10 9.8891084e-10
 2.6170335e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:55,055] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1844
[2019-04-04 06:59:55,078] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 61.0, 0.0, 0.0, 26.0, 25.38369422212313, 0.4715065689972149, 0.0, 1.0, 39515.20286036744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3541800.0000, 
sim time next is 3542400.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.37524098580605, 0.4702295807232408, 0.0, 1.0, 45681.79263908399], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6146034154838377, 0.6567431935744136, 0.0, 1.0, 0.21753234590039996], 
reward next is 0.7825, 
noisyNet noise sample is [array([-2.8864536], dtype=float32), 0.008577931]. 
=============================================
[2019-04-04 06:59:56,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4656264e-11 9.1534995e-11 6.3822859e-24 1.2349130e-11 6.1820453e-12
 8.3386206e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 06:59:56,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4862
[2019-04-04 06:59:56,307] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 26.0, 25.63472722284086, 0.6048344637402776, 1.0, 1.0, 70333.60784504382], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3334800.0000, 
sim time next is 3335400.0000, 
raw observation next is [-3.5, 50.0, 106.0, 752.0, 26.0, 26.09413109133474, 0.6513439148545158, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.36565096952908593, 0.5, 0.35333333333333333, 0.830939226519337, 0.6666666666666666, 0.6745109242778952, 0.7171146382848387, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3933342], dtype=float32), 0.56770366]. 
=============================================
[2019-04-04 07:00:01,872] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2975768e-09 5.6956537e-09 4.0282105e-22 1.9604258e-09 1.5554992e-09
 3.0149980e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:00:01,874] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6252
[2019-04-04 07:00:01,897] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.0, 0.0, 0.0, 26.0, 24.97751382424077, 0.342961551266677, 0.0, 1.0, 40934.28562353843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.97033722668381, 0.3352563306694469, 0.0, 1.0, 40884.05439789644], 
processed observation next is [0.0, 0.17391304347826086, 0.33333333333333337, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5808614355569842, 0.611752110223149, 0.0, 1.0, 0.19468597332331639], 
reward next is 0.8053, 
noisyNet noise sample is [array([0.74305487], dtype=float32), -1.0106841]. 
=============================================
[2019-04-04 07:00:03,147] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4403140e-10 2.3179289e-10 6.9769438e-24 5.2613976e-11 2.6412457e-11
 3.5029916e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:00:03,152] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0826
[2019-04-04 07:00:03,159] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 26.05117353689159, 0.5920386197321464, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3520800.0000, 
sim time next is 3521400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 26.07696212066681, 0.5824806875534329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6730801767222342, 0.6941602291844776, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0258133], dtype=float32), -1.3091918]. 
=============================================
[2019-04-04 07:00:08,239] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.3772851e-10 8.6886409e-10 9.1929758e-25 5.5178112e-11 7.8794395e-11
 1.0121304e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:00:08,240] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5765
[2019-04-04 07:00:08,256] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 74.33333333333334, 0.0, 0.0, 26.0, 25.38768939122411, 0.4250506837539163, 0.0, 1.0, 52335.73696617677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3465600.0000, 
sim time next is 3466200.0000, 
raw observation next is [1.0, 73.16666666666666, 0.0, 0.0, 26.0, 25.36121620903838, 0.422126991212988, 0.0, 1.0, 53888.69913922653], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7316666666666666, 0.0, 0.0, 0.6666666666666666, 0.6134346840865316, 0.640708997070996, 0.0, 1.0, 0.2566128530439359], 
reward next is 0.7434, 
noisyNet noise sample is [array([0.38985753], dtype=float32), 0.20846233]. 
=============================================
[2019-04-04 07:00:10,529] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.44471729e-10 1.39356404e-09 1.02520695e-23 1.98463107e-10
 9.33848762e-11 1.29573246e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 07:00:10,530] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1294
[2019-04-04 07:00:10,548] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.25181239167098, 0.3792896494471843, 0.0, 1.0, 41806.94853437308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3478200.0000, 
sim time next is 3478800.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23692757623222, 0.3775296839661653, 0.0, 1.0, 41756.37014034262], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6030772980193516, 0.6258432279887217, 0.0, 1.0, 0.19883985781115532], 
reward next is 0.8012, 
noisyNet noise sample is [array([-0.890597], dtype=float32), 1.3354437]. 
=============================================
[2019-04-04 07:00:15,927] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2254567e-10 1.6555131e-10 1.3626253e-24 7.9920362e-11 1.0824202e-10
 4.9206732e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:00:15,928] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6040
[2019-04-04 07:00:15,978] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 67.5, 100.0, 676.0, 26.0, 25.89933642343454, 0.526986532207871, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3576600.0000, 
sim time next is 3577200.0000, 
raw observation next is [-5.333333333333333, 66.66666666666666, 101.8333333333333, 689.6666666666666, 26.0, 25.86295704968023, 0.5216357898017118, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3148661126500462, 0.6666666666666665, 0.3394444444444443, 0.7620626151012891, 0.6666666666666666, 0.6552464208066858, 0.6738785966005706, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9905807], dtype=float32), -0.73560715]. 
=============================================
[2019-04-04 07:00:24,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3168622e-11 3.0954180e-11 1.5465612e-25 2.6834125e-11 3.6380304e-12
 8.3084047e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:00:24,479] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2676
[2019-04-04 07:00:24,573] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 56.33333333333334, 76.83333333333334, 415.5000000000001, 26.0, 25.66589219082464, 0.4578083325299748, 1.0, 1.0, 9370.644181409836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3918000.0000, 
sim time next is 3918600.0000, 
raw observation next is [-8.0, 55.5, 91.0, 466.0, 26.0, 25.77096228570781, 0.4680180863573584, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.555, 0.30333333333333334, 0.5149171270718232, 0.6666666666666666, 0.6475801904756509, 0.6560060287857862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7813706], dtype=float32), 0.6565752]. 
=============================================
[2019-04-04 07:00:29,178] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 07:00:29,182] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:00:29,182] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:00:29,188] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:00:29,197] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:00:29,199] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run17
[2019-04-04 07:00:29,246] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:00:29,248] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:00:29,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run17
[2019-04-04 07:00:29,299] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run17
[2019-04-04 07:00:56,637] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2309187], dtype=float32), -0.034605943]
[2019-04-04 07:00:56,638] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-15.64044301, 65.04773838, 0.0, 0.0, 26.0, 23.99820924381184, 0.02260130583817222, 0.0, 1.0, 46080.08509885157]
[2019-04-04 07:00:56,638] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:00:56,639] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7196975e-08 1.6538515e-08 5.5283098e-20 4.7854205e-09 3.6051904e-09
 2.9710262e-11 1.0000000e+00], sampled 0.6490564636627464
[2019-04-04 07:01:35,211] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2309187], dtype=float32), -0.034605943]
[2019-04-04 07:01:35,211] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [10.25, 98.0, 0.0, 0.0, 26.0, 24.59008743476455, 0.4330187011210192, 0.0, 1.0, 45080.17091632891]
[2019-04-04 07:01:35,211] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:01:35,212] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.2070378e-11 2.7851020e-11 5.9461673e-27 6.2405814e-12 6.6797839e-12
 3.7350329e-15 1.0000000e+00], sampled 0.8390151569249296
[2019-04-04 07:03:13,734] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.2309187], dtype=float32), -0.034605943]
[2019-04-04 07:03:13,734] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.166666666666667, 48.16666666666666, 105.6666666666667, 728.6666666666666, 26.0, 26.57625097970191, 0.6127545354310061, 1.0, 1.0, 0.0]
[2019-04-04 07:03:13,734] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:03:13,735] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.0056703e-11 1.2826646e-11 3.9393480e-27 4.5137804e-12 1.3892096e-12
 1.8654899e-15 1.0000000e+00], sampled 0.978003229566205
[2019-04-04 07:03:38,171] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 07:04:12,202] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:04:14,884] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.6651 275800324.5281 1233.1507
[2019-04-04 07:04:15,912] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1600000, evaluation results [1600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.665121294629, 275800324.5281271, 1233.1506611100626]
[2019-04-04 07:04:21,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3267911e-12 2.2400276e-11 1.4216970e-25 1.1634395e-11 1.0771508e-12
 7.7821313e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:04:21,434] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7970
[2019-04-04 07:04:21,505] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 41.5, 116.0, 824.0, 26.0, 26.30913480052656, 0.6939995124403638, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3936600.0000, 
sim time next is 3937200.0000, 
raw observation next is [-5.333333333333333, 40.33333333333334, 114.1666666666667, 818.0, 26.0, 26.5681600492984, 0.7215807174674339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3148661126500462, 0.40333333333333343, 0.38055555555555565, 0.9038674033149171, 0.6666666666666666, 0.7140133374415333, 0.7405269058224779, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1722766], dtype=float32), -0.5222216]. 
=============================================
[2019-04-04 07:04:30,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4533420e-10 1.7720864e-10 4.1241710e-25 2.4395728e-11 3.7599576e-11
 1.6471504e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:04:30,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1926
[2019-04-04 07:04:31,021] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 35.0, 114.0, 774.0, 26.0, 25.35910522538893, 0.418912807871487, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4185000.0000, 
sim time next is 4185600.0000, 
raw observation next is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 26.0, 25.31765189058514, 0.4136882483093113, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.42566943674976926, 0.35, 0.38277777777777766, 0.8640883977900552, 0.6666666666666666, 0.6098043242154283, 0.6378960827697705, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5819332], dtype=float32), 0.2993314]. 
=============================================
[2019-04-04 07:04:34,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5950374e-09 1.8739226e-09 9.7634622e-25 5.3283772e-10 2.1213586e-10
 2.9975135e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:04:34,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3844
[2019-04-04 07:04:34,542] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.38308340702506, 0.3252484532164843, 0.0, 1.0, 38400.32162562429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4254600.0000, 
sim time next is 4255200.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.37650431555439, 0.3237305469099407, 0.0, 1.0, 41776.44133995981], 
processed observation next is [0.0, 0.2608695652173913, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6147086929628657, 0.6079101823033136, 0.0, 1.0, 0.19893543495218957], 
reward next is 0.8011, 
noisyNet noise sample is [array([1.4451935], dtype=float32), 1.1158322]. 
=============================================
[2019-04-04 07:04:36,197] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1378987e-10 3.1905797e-10 6.9574850e-25 1.5515596e-11 7.6374372e-11
 3.8012846e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:04:36,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5112
[2019-04-04 07:04:36,233] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 34.16666666666667, 117.3333333333333, 806.0, 26.0, 25.20616980640071, 0.4062611170215805, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4187400.0000, 
sim time next is 4188000.0000, 
raw observation next is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 26.0, 25.19769071103834, 0.4008791761719141, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4533702677747, 0.3333333333333334, 0.393888888888889, 0.8994475138121547, 0.6666666666666666, 0.5998075592531951, 0.6336263920573048, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.43840173], dtype=float32), 0.86098605]. 
=============================================
[2019-04-04 07:04:36,248] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.46688 ]
 [85.522514]
 [85.677   ]
 [85.8022  ]
 [85.825264]], R is [[85.49641418]
 [85.64144897]
 [85.78503418]
 [85.92718506]
 [86.06791687]].
[2019-04-04 07:04:58,807] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.3752580e-12 1.8474456e-11 3.0285960e-26 3.6426062e-12 4.3765000e-12
 8.2302371e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:04:58,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4226
[2019-04-04 07:04:58,834] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 65.5, 164.0, 509.0, 26.0, 26.10875153836963, 0.5709754578668617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4613400.0000, 
sim time next is 4614000.0000, 
raw observation next is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 26.0, 26.24458003417372, 0.5850884707709499, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.44413665743305636, 0.6366666666666667, 0.5272222222222224, 0.6099447513812155, 0.6666666666666666, 0.6870483361811432, 0.6950294902569834, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11358006], dtype=float32), 0.12534413]. 
=============================================
[2019-04-04 07:04:58,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[93.32589 ]
 [92.812096]
 [92.33231 ]
 [91.875565]
 [91.47337 ]], R is [[93.64903259]
 [93.71253967]
 [93.77541351]
 [93.83766174]
 [93.89928436]].
[2019-04-04 07:05:07,024] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6945535e-09 6.2665695e-09 1.4968494e-22 2.0993572e-10 7.8913703e-10
 1.5500562e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:07,024] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0270
[2019-04-04 07:05:07,052] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 25.37180257439245, 0.5115746697157878, 0.0, 1.0, 71757.2252603846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4744800.0000, 
sim time next is 4745400.0000, 
raw observation next is [-3.0, 82.83333333333333, 0.0, 0.0, 26.0, 25.38896982263256, 0.4697545276493393, 0.0, 1.0, 45711.62260755041], 
processed observation next is [1.0, 0.9565217391304348, 0.3795013850415513, 0.8283333333333333, 0.0, 0.0, 0.6666666666666666, 0.61574748521938, 0.6565848425497798, 0.0, 1.0, 0.21767439336928765], 
reward next is 0.7823, 
noisyNet noise sample is [array([-0.8336113], dtype=float32), 0.42391464]. 
=============================================
[2019-04-04 07:05:19,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4745895e-12 6.8378771e-12 1.8812422e-28 9.0287142e-13 1.2671445e-13
 5.9731181e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:19,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0233
[2019-04-04 07:05:19,136] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 26.33333333333333, 122.1666666666667, 848.3333333333334, 26.0, 26.90875565421588, 0.6818096888782174, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4966800.0000, 
sim time next is 4967400.0000, 
raw observation next is [5.5, 25.66666666666667, 122.3333333333333, 851.6666666666666, 26.0, 26.98869829286843, 0.6889759436091585, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6149584487534627, 0.2566666666666667, 0.4077777777777777, 0.9410681399631675, 0.6666666666666666, 0.7490581910723693, 0.7296586478697195, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2280854], dtype=float32), 0.08171574]. 
=============================================
[2019-04-04 07:05:21,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0356604e-09 3.1994543e-09 3.2189286e-22 9.8400388e-10 4.5135437e-10
 4.7448230e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:21,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8271
[2019-04-04 07:05:21,306] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 40.0, 0.0, 0.0, 26.0, 25.55734154275144, 0.4395867572490961, 0.0, 1.0, 37451.80728335716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5016000.0000, 
sim time next is 5016600.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.51876903750599, 0.429902354945731, 0.0, 1.0, 53993.18092946603], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6265640864588325, 0.6433007849819103, 0.0, 1.0, 0.25711038537840963], 
reward next is 0.7429, 
noisyNet noise sample is [array([0.6794333], dtype=float32), -0.50097317]. 
=============================================
[2019-04-04 07:05:21,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:21,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:21,888] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run13
[2019-04-04 07:05:25,240] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:25,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:25,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run13
[2019-04-04 07:05:26,834] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6844919e-10 3.7346864e-10 8.5672602e-25 5.0858644e-11 3.9509604e-11
 5.6662780e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:26,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0480
[2019-04-04 07:05:26,882] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 65.0, 65.33333333333334, 173.0, 26.0, 25.58488314034332, 0.415561014469757, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5038800.0000, 
sim time next is 5039400.0000, 
raw observation next is [-2.166666666666667, 65.0, 71.66666666666666, 245.0, 26.0, 25.58879736805824, 0.422966792438294, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.65, 0.23888888888888885, 0.27071823204419887, 0.6666666666666666, 0.6323997806715201, 0.6409889308127646, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7733035], dtype=float32), 0.22634064]. 
=============================================
[2019-04-04 07:05:26,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:26,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:26,963] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run13
[2019-04-04 07:05:27,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:27,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:27,074] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run13
[2019-04-04 07:05:27,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:27,764] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:27,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run13
[2019-04-04 07:05:27,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:27,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:27,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run13
[2019-04-04 07:05:28,537] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6815932e-10 1.0210687e-10 2.5983633e-26 3.9119229e-11 6.1947812e-12
 9.0723793e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:28,538] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8218
[2019-04-04 07:05:28,584] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 78.0, 317.0, 26.0, 25.58479751900864, 0.4291991560370779, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5040000.0000, 
sim time next is 5040600.0000, 
raw observation next is [-1.5, 62.00000000000001, 84.33333333333333, 389.0, 26.0, 25.5939281719825, 0.4388007604374109, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4210526315789474, 0.6200000000000001, 0.2811111111111111, 0.4298342541436464, 0.6666666666666666, 0.6328273476652084, 0.6462669201458037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3023722], dtype=float32), 0.101453945]. 
=============================================
[2019-04-04 07:05:31,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:31,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:31,830] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run13
[2019-04-04 07:05:32,372] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8113043e-12 1.6107624e-11 3.1579737e-27 4.4396748e-12 8.0798833e-13
 1.2060600e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:32,372] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7676
[2019-04-04 07:05:32,449] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 27.0, 122.0, 845.0, 26.0, 26.82753252111416, 0.6637460303462425, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4966200.0000, 
sim time next is 4966800.0000, 
raw observation next is [5.0, 26.33333333333333, 122.1666666666667, 848.3333333333334, 26.0, 26.9087933177935, 0.681842083812452, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6011080332409973, 0.2633333333333333, 0.4072222222222223, 0.9373848987108656, 0.6666666666666666, 0.7423994431494583, 0.7272806946041507, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.431901], dtype=float32), -0.7033029]. 
=============================================
[2019-04-04 07:05:33,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:33,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:33,096] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run13
[2019-04-04 07:05:34,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:34,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:34,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run13
[2019-04-04 07:05:37,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.3801892e-10 6.4781164e-10 1.2112177e-25 4.3880285e-11 1.5416987e-10
 6.8752336e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:37,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1498
[2019-04-04 07:05:37,179] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 70.0, 0.0, 26.0, 24.17403531359248, 0.07398854028318086, 0.0, 1.0, 39424.96371927889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 40200.0000, 
sim time next is 40800.0000, 
raw observation next is [7.7, 93.0, 72.5, 0.0, 26.0, 24.23880036722083, 0.08161802986169825, 0.0, 1.0, 18780.08396786054], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.24166666666666667, 0.0, 0.6666666666666666, 0.5199000306017357, 0.5272060099538994, 0.0, 1.0, 0.08942897127552639], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.02505077], dtype=float32), -0.5311593]. 
=============================================
[2019-04-04 07:05:37,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:37,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:37,339] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run13
[2019-04-04 07:05:37,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:37,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:37,760] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run13
[2019-04-04 07:05:38,033] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1724532e-10 5.1633347e-11 1.4965845e-25 4.6402344e-12 9.4331661e-12
 9.4836468e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:38,034] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7649
[2019-04-04 07:05:38,118] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.800000000000001, 91.83333333333333, 89.0, 0.0, 26.0, 24.3048746577388, 0.1010041400136539, 0.0, 1.0, 39155.29926228961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 43800.0000, 
sim time next is 44400.0000, 
raw observation next is [7.9, 90.66666666666667, 92.5, 0.0, 26.0, 24.31302030504014, 0.1092863644306539, 0.0, 1.0, 35410.2030073198], 
processed observation next is [0.0, 0.5217391304347826, 0.6814404432132966, 0.9066666666666667, 0.30833333333333335, 0.0, 0.6666666666666666, 0.5260850254200117, 0.5364287881435513, 0.0, 1.0, 0.16862001432057047], 
reward next is 0.8314, 
noisyNet noise sample is [array([0.15337336], dtype=float32), -1.3653541]. 
=============================================
[2019-04-04 07:05:38,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:38,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:38,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run13
[2019-04-04 07:05:41,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:41,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:41,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run13
[2019-04-04 07:05:43,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:43,357] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:43,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run13
[2019-04-04 07:05:44,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:44,144] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:44,147] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run13
[2019-04-04 07:05:45,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:05:45,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:05:45,069] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run13
[2019-04-04 07:05:58,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7749354e-09 2.4941340e-09 7.7314717e-23 4.0808293e-10 2.5632335e-10
 2.6911969e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:05:58,797] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4739
[2019-04-04 07:05:58,821] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.1194446531076, 0.08530681515516757, 0.0, 1.0, 42938.9092388928], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 97200.0000, 
sim time next is 97800.0000, 
raw observation next is [-2.9, 85.66666666666667, 0.0, 0.0, 26.0, 24.05372368500042, 0.08485897023462564, 0.0, 1.0, 43091.05595795492], 
processed observation next is [1.0, 0.13043478260869565, 0.38227146814404434, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5044769737500351, 0.5282863234115419, 0.0, 1.0, 0.2051955045616901], 
reward next is 0.7948, 
noisyNet noise sample is [array([-0.30620596], dtype=float32), 0.056613352]. 
=============================================
[2019-04-04 07:06:14,052] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.01836866e-10 1.34477363e-09 1.10533754e-23 2.98125663e-10
 2.15837972e-11 4.86568353e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 07:06:14,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2043
[2019-04-04 07:06:14,178] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.55, 68.5, 30.0, 386.0, 26.0, 25.33754497422313, 0.2898121107301926, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 289800.0000, 
sim time next is 290400.0000, 
raw observation next is [-12.46666666666667, 68.0, 40.83333333333333, 385.5, 26.0, 25.63172786091829, 0.3217546741508705, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.11726685133887339, 0.68, 0.1361111111111111, 0.42596685082872926, 0.6666666666666666, 0.6359773217431908, 0.6072515580502902, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9451618], dtype=float32), -0.52876747]. 
=============================================
[2019-04-04 07:06:17,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3477934e-09 2.1550868e-09 2.5485200e-22 4.3376119e-10 5.2186214e-11
 3.9927913e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:06:17,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8632
[2019-04-04 07:06:17,220] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.56189653091911, 0.3671376187320992, 1.0, 1.0, 34523.80684485524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 411000.0000, 
sim time next is 411600.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.60628907505891, 0.3549069539082217, 1.0, 1.0, 41764.94630336553], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6338574229215759, 0.6183023179694073, 1.0, 1.0, 0.19888069668269298], 
reward next is 0.8011, 
noisyNet noise sample is [array([-0.31833863], dtype=float32), 0.5327241]. 
=============================================
[2019-04-04 07:06:26,393] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.3886836e-10 2.7501737e-10 3.3569698e-26 5.7510153e-11 4.1760796e-11
 3.6336927e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:06:26,394] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5399
[2019-04-04 07:06:26,419] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 26.0, 24.78598796809966, 0.2244863980426726, 0.0, 1.0, 39818.71670178534], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 529200.0000, 
sim time next is 529800.0000, 
raw observation next is [3.616666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.76780303131051, 0.2213770200632138, 0.0, 1.0, 39865.96741854811], 
processed observation next is [0.0, 0.13043478260869565, 0.5627885503231764, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5639835859425425, 0.5737923400210713, 0.0, 1.0, 0.18983794008832436], 
reward next is 0.8102, 
noisyNet noise sample is [array([-0.6002059], dtype=float32), -0.40191787]. 
=============================================
[2019-04-04 07:06:34,871] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.2597823e-09 9.9912842e-09 3.9886420e-20 5.8108047e-09 4.6065112e-09
 1.0271396e-10 1.0000000e+00], sum to 1.0000
[2019-04-04 07:06:34,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7832
[2019-04-04 07:06:34,898] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.9, 52.0, 0.0, 0.0, 26.0, 22.70720036682388, -0.2579608486824492, 0.0, 1.0, 46680.93349287169], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 448200.0000, 
sim time next is 448800.0000, 
raw observation next is [-10.8, 52.0, 0.0, 0.0, 26.0, 22.75349889931037, -0.2630341997599627, 0.0, 1.0, 46692.12022955227], 
processed observation next is [1.0, 0.17391304347826086, 0.1634349030470914, 0.52, 0.0, 0.0, 0.6666666666666666, 0.39612490827586405, 0.41232193341334583, 0.0, 1.0, 0.2223434296645346], 
reward next is 0.7777, 
noisyNet noise sample is [array([0.19076827], dtype=float32), -1.3616508]. 
=============================================
[2019-04-04 07:06:42,591] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5373740e-10 5.6173355e-10 1.4638150e-23 2.3010842e-10 9.0921284e-11
 6.5657424e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:06:42,591] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6625
[2019-04-04 07:06:42,626] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.716666666666667, 64.5, 0.0, 0.0, 26.0, 24.95965574496929, 0.2223776757820963, 0.0, 1.0, 47123.56784056078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 676200.0000, 
sim time next is 676800.0000, 
raw observation next is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.97453586691586, 0.2208420876917097, 0.0, 1.0, 44502.79508118616], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5812113222429884, 0.5736140292305699, 0.0, 1.0, 0.2119180718151722], 
reward next is 0.7881, 
noisyNet noise sample is [array([-0.8534441], dtype=float32), -2.273347]. 
=============================================
[2019-04-04 07:06:43,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4121979e-09 8.0725171e-10 6.1828094e-24 4.5620729e-10 1.2419149e-10
 2.5093171e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:06:43,139] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8741
[2019-04-04 07:06:43,196] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.89815365870973, 0.2185547890900477, 0.0, 1.0, 78056.75658630079], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 675000.0000, 
sim time next is 675600.0000, 
raw observation next is [-2.633333333333333, 64.0, 0.0, 0.0, 26.0, 24.9275366089732, 0.2216815487113497, 0.0, 1.0, 55354.85640882267], 
processed observation next is [0.0, 0.8260869565217391, 0.38965835641735924, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5772947174144333, 0.5738938495704499, 0.0, 1.0, 0.263594554327727], 
reward next is 0.7364, 
noisyNet noise sample is [array([0.49305323], dtype=float32), 0.44766542]. 
=============================================
[2019-04-04 07:06:52,750] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.23602818e-12 1.26652959e-11 3.77752577e-26 1.30693295e-11
 1.41989328e-12 2.79086599e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:06:52,750] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7760
[2019-04-04 07:06:52,839] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 55.0, 0.0, 0.0, 26.0, 25.55570783160082, 0.363752024601955, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 757200.0000, 
sim time next is 757800.0000, 
raw observation next is [-3.9, 54.5, 0.0, 0.0, 26.0, 25.37174340936694, 0.3351221347842195, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.545, 0.0, 0.0, 0.6666666666666666, 0.6143119507805782, 0.6117073782614065, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4755482], dtype=float32), -0.36811745]. 
=============================================
[2019-04-04 07:06:56,582] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3673619e-09 2.4908109e-09 2.0001721e-23 2.6504773e-10 1.2852587e-10
 9.6694506e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:06:56,583] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8648
[2019-04-04 07:06:56,599] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 79.5, 0.0, 0.0, 26.0, 24.75598126875038, 0.224115933538322, 0.0, 1.0, 39419.24739487388], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 869400.0000, 
sim time next is 870000.0000, 
raw observation next is [-1.9, 79.33333333333334, 0.0, 0.0, 26.0, 24.80114415025999, 0.2180461436806142, 0.0, 1.0, 39391.48422265166], 
processed observation next is [1.0, 0.043478260869565216, 0.4099722991689751, 0.7933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5667620125216658, 0.572682047893538, 0.0, 1.0, 0.18757849629834125], 
reward next is 0.8124, 
noisyNet noise sample is [array([-0.8364204], dtype=float32), -0.8606867]. 
=============================================
[2019-04-04 07:06:56,602] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.22558 ]
 [83.1304  ]
 [83.039566]
 [82.9165  ]
 [82.81447 ]], R is [[83.26560211]
 [83.24523163]
 [83.22498322]
 [83.20480347]
 [83.18464661]].
[2019-04-04 07:06:59,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0545175e-10 1.6460452e-10 1.6685302e-24 4.6647287e-11 2.9989292e-11
 8.2539478e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:06:59,657] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1276
[2019-04-04 07:06:59,673] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57566399822124, 0.1843559917088464, 0.0, 1.0, 39268.00146015653], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 876000.0000, 
sim time next is 876600.0000, 
raw observation next is [-1.45, 77.5, 0.0, 0.0, 26.0, 24.61344081981355, 0.1847178269616885, 0.0, 1.0, 39216.82556991155], 
processed observation next is [1.0, 0.13043478260869565, 0.422437673130194, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5511200683177959, 0.5615726089872295, 0.0, 1.0, 0.18674678842815023], 
reward next is 0.8133, 
noisyNet noise sample is [array([-1.5038881], dtype=float32), -0.79372984]. 
=============================================
[2019-04-04 07:07:13,404] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3776042e-11 5.7109970e-11 8.2622450e-26 1.2563714e-11 6.5319325e-12
 1.2239556e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:13,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3873
[2019-04-04 07:07:13,423] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23897602048101, 0.408950040818914, 0.0, 1.0, 38645.2547826621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 948000.0000, 
sim time next is 948600.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23977803303761, 0.4092096379802662, 0.0, 1.0, 38549.2671142858], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6033148360864674, 0.6364032126600887, 0.0, 1.0, 0.1835679386394562], 
reward next is 0.8164, 
noisyNet noise sample is [array([0.63555914], dtype=float32), -2.3767734]. 
=============================================
[2019-04-04 07:07:15,165] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.3915036e-12 9.8881688e-13 5.8185475e-31 4.2247133e-13 7.3330400e-14
 5.0624760e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:15,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6680
[2019-04-04 07:07:15,208] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.5, 92.83333333333333, 18.0, 0.0, 26.0, 25.40585924682344, 0.4252915335922909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 979800.0000, 
sim time next is 980400.0000, 
raw observation next is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33550761313693, 0.487207385585406, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7285318559556788, 0.9266666666666667, 0.075, 0.0, 0.6666666666666666, 0.6112923010947441, 0.662402461861802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3044742], dtype=float32), 1.2913309]. 
=============================================
[2019-04-04 07:07:17,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3323447e-10 1.1618650e-11 3.3295718e-28 1.7078936e-12 2.0205981e-12
 2.0928071e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:17,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.31935066e-12 1.03263090e-12 1.35635629e-30 3.15478709e-13
 5.20982183e-14 1.12807525e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 07:07:17,761] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1572
[2019-04-04 07:07:17,761] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7376
[2019-04-04 07:07:17,787] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.03198810878187, 0.6330752544101285, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1040400.0000, 
sim time next is 1041000.0000, 
raw observation next is [14.3, 75.5, 0.0, 0.0, 26.0, 26.04438381511459, 0.6293779687063353, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.8587257617728533, 0.755, 0.0, 0.0, 0.6666666666666666, 0.6703653179262158, 0.7097926562354452, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.71068496], dtype=float32), 2.2104914]. 
=============================================
[2019-04-04 07:07:17,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[97.95191]
 [97.98106]
 [97.92481]
 [97.83455]
 [97.76391]], R is [[97.95116425]
 [97.9716568 ]
 [97.99194336]
 [98.01202393]
 [98.03190613]].
[2019-04-04 07:07:17,817] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.600000000000001, 92.66666666666667, 22.5, 0.0, 26.0, 25.33560477142252, 0.4872269988851476, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 980400.0000, 
sim time next is 981000.0000, 
raw observation next is [9.7, 92.5, 27.0, 0.0, 26.0, 25.71165627584688, 0.5083386088486214, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7313019390581719, 0.925, 0.09, 0.0, 0.6666666666666666, 0.64263802298724, 0.6694462029495405, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35276812], dtype=float32), 0.6718537]. 
=============================================
[2019-04-04 07:07:17,824] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[104.953476]
 [105.171265]
 [105.22003 ]
 [105.05806 ]
 [104.04789 ]], R is [[104.76699829]
 [104.71932983]
 [104.6721344 ]
 [104.62541199]
 [104.5791626 ]].
[2019-04-04 07:07:18,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2224896e-11 4.9028993e-11 3.8449488e-29 4.9208849e-12 1.6880388e-11
 5.1424757e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:18,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4616
[2019-04-04 07:07:18,756] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.55, 79.0, 0.0, 0.0, 26.0, 25.68678727327605, 0.6097492840629134, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1056600.0000, 
sim time next is 1057200.0000, 
raw observation next is [13.46666666666667, 79.33333333333333, 0.0, 0.0, 26.0, 25.7885090264852, 0.6124780990050941, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.8356417359187445, 0.7933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6490424188737668, 0.7041593663350314, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21217495], dtype=float32), -1.270175]. 
=============================================
[2019-04-04 07:07:20,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.85064808e-10 4.33458269e-11 6.19853126e-26 8.86475008e-12
 1.05676475e-11 6.71271281e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:07:20,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1399
[2019-04-04 07:07:20,566] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.466666666666667, 92.33333333333333, 0.0, 0.0, 26.0, 25.38970078195608, 0.552692464230826, 0.0, 1.0, 109330.7317154679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1302000.0000, 
sim time next is 1302600.0000, 
raw observation next is [3.383333333333333, 92.16666666666667, 0.0, 0.0, 26.0, 25.33094390210919, 0.5575661081219495, 0.0, 1.0, 80990.84256700113], 
processed observation next is [1.0, 0.043478260869565216, 0.5563250230840259, 0.9216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6109119918424325, 0.6858553693739832, 0.0, 1.0, 0.38567067889048157], 
reward next is 0.6143, 
noisyNet noise sample is [array([0.87995774], dtype=float32), -1.026327]. 
=============================================
[2019-04-04 07:07:20,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3866810e-09 1.6418655e-10 4.7519645e-25 7.0936097e-11 6.3100053e-11
 1.7240470e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:20,789] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5454
[2019-04-04 07:07:20,795] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.33333333333334, 69.66666666666667, 0.0, 0.0, 26.0, 24.52554860983278, 0.3596607959925437, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1200000.0000, 
sim time next is 1200600.0000, 
raw observation next is [17.15, 71.0, 0.0, 0.0, 26.0, 24.49777400354039, 0.3628261785481878, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9376731301939059, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5414811669616991, 0.6209420595160626, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15848409], dtype=float32), 1.0421185]. 
=============================================
[2019-04-04 07:07:29,698] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2637570e-11 1.8703213e-11 4.8635967e-25 4.9988065e-12 6.4428679e-12
 8.9095034e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:29,698] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3654
[2019-04-04 07:07:29,711] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 50.33333333333333, 0.0, 26.0, 25.93922737627055, 0.5155020282349231, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1437600.0000, 
sim time next is 1438200.0000, 
raw observation next is [1.1, 92.0, 46.0, 0.0, 26.0, 25.90895956556707, 0.5153033617895116, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.15333333333333332, 0.0, 0.6666666666666666, 0.6590799637972559, 0.6717677872631705, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46996948], dtype=float32), -0.91276217]. 
=============================================
[2019-04-04 07:07:32,705] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.93829067e-11 2.16588518e-11 1.39052789e-26 1.49284252e-12
 2.23916784e-12 1.27903966e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 07:07:32,707] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2767
[2019-04-04 07:07:32,753] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 100.0, 0.0, 0.0, 26.0, 25.59916014823041, 0.4606866331560492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1496400.0000, 
sim time next is 1497000.0000, 
raw observation next is [1.1, 100.0, 5.999999999999998, 0.0, 26.0, 25.52699937654192, 0.4523950947032175, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.49307479224376743, 1.0, 0.019999999999999993, 0.0, 0.6666666666666666, 0.62724994804516, 0.6507983649010725, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7370963], dtype=float32), -0.60200804]. 
=============================================
[2019-04-04 07:07:32,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.701225]
 [84.49033 ]
 [85.896416]
 [81.71134 ]
 [81.54899 ]], R is [[89.64003754]
 [89.74363708]
 [89.84619904]
 [89.94773865]
 [89.65648651]].
[2019-04-04 07:07:33,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5156593e-11 5.4922122e-11 2.3452979e-24 1.6250779e-11 9.8947586e-12
 1.5598783e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:33,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7568
[2019-04-04 07:07:33,161] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.36588651344519, 0.4621588090405963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1452000.0000, 
sim time next is 1452600.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.24407659849852, 0.4377978246175859, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6036730498748767, 0.645932608205862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9240673], dtype=float32), -0.48033744]. 
=============================================
[2019-04-04 07:07:33,546] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.16307394e-10 2.00012049e-10 3.51324775e-25 2.34050505e-11
 2.31344783e-12 7.51326610e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:07:33,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2684
[2019-04-04 07:07:33,568] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 59.0, 0.0, 26.0, 26.01352269239699, 0.539302010494577, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.05053112323795, 0.5166125840202299, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.1822222222222222, 0.0, 0.6666666666666666, 0.6708775936031625, 0.67220419467341, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1796247], dtype=float32), 1.3122424]. 
=============================================
[2019-04-04 07:07:33,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[85.76069]
 [85.86031]
 [85.99215]
 [86.22955]
 [86.58541]], R is [[85.77628326]
 [85.91851807]
 [86.0593338 ]
 [86.1987381 ]
 [86.33675385]].
[2019-04-04 07:07:38,039] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.24902993e-12 1.42731495e-11 4.12079915e-28 1.95275003e-12
 1.24590908e-12 3.23057546e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 07:07:38,041] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9075
[2019-04-04 07:07:38,061] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.116666666666667, 62.16666666666666, 205.3333333333333, 141.6666666666667, 26.0, 26.83624278007356, 0.7474969011430944, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [9.4, 61.0, 208.0, 168.5, 26.0, 26.82862144157125, 0.7603841826108401, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7229916897506927, 0.61, 0.6933333333333334, 0.1861878453038674, 0.6666666666666666, 0.7357184534642709, 0.7534613942036134, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40336335], dtype=float32), -0.11928964]. 
=============================================
[2019-04-04 07:07:39,956] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5341931e-10 6.7339273e-10 4.1690009e-24 5.4598440e-11 2.9309288e-11
 1.0561189e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:39,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7675
[2019-04-04 07:07:39,973] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 54.66666666666666, 0.0, 26.0, 26.0505326614984, 0.5166118387399898, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1437000.0000, 
sim time next is 1437600.0000, 
raw observation next is [1.1, 92.0, 50.33333333333333, 0.0, 26.0, 25.93922939116679, 0.5155013388505522, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.16777777777777778, 0.0, 0.6666666666666666, 0.6616024492638992, 0.6718337796168506, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00243703], dtype=float32), -1.0055578]. 
=============================================
[2019-04-04 07:07:40,248] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.6619345e-10 1.9431737e-10 2.6260679e-24 5.0099001e-11 7.6355290e-11
 8.5938208e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:40,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5563
[2019-04-04 07:07:40,264] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.65225850590992, 0.51796389238267, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [5.083333333333334, 81.50000000000001, 0.0, 0.0, 26.0, 25.69092879868365, 0.5051554527401692, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.6034164358264081, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.6409107332236376, 0.6683851509133897, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20980294], dtype=float32), 1.2408936]. 
=============================================
[2019-04-04 07:07:42,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8764994e-12 5.0827294e-12 3.7639739e-27 1.3976346e-12 9.2690937e-13
 1.6518879e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:42,760] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0718
[2019-04-04 07:07:42,774] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 57.0, 182.5, 186.5, 26.0, 27.19698426638199, 0.8360717786781809, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1598400.0000, 
sim time next is 1599000.0000, 
raw observation next is [11.96666666666667, 55.66666666666666, 171.3333333333333, 165.6666666666667, 26.0, 27.22048950903403, 0.8444457644427888, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.7940904893813484, 0.5566666666666665, 0.5711111111111109, 0.18305709023941075, 0.6666666666666666, 0.7683741257528359, 0.7814819214809297, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1422002], dtype=float32), -0.20848529]. 
=============================================
[2019-04-04 07:07:42,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[93.502266]
 [94.25165 ]
 [94.92416 ]
 [95.29836 ]
 [95.127815]], R is [[92.83786011]
 [92.90948486]
 [92.98039246]
 [93.05059052]
 [93.12008667]].
[2019-04-04 07:07:44,313] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.7898147e-10 2.9945180e-10 1.5668053e-24 6.6562457e-11 9.8063453e-11
 3.2038567e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:44,313] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9147
[2019-04-04 07:07:44,324] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.166666666666666, 82.0, 0.0, 0.0, 26.0, 25.54141504264789, 0.5573080741555562, 0.0, 1.0, 40270.01561688387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1554000.0000, 
sim time next is 1554600.0000, 
raw observation next is [5.083333333333334, 82.0, 0.0, 0.0, 26.0, 25.50739217271266, 0.5538804051265597, 0.0, 1.0, 54057.8863930665], 
processed observation next is [1.0, 1.0, 0.6034164358264081, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6256160143927216, 0.6846268017088533, 0.0, 1.0, 0.25741850663365], 
reward next is 0.7426, 
noisyNet noise sample is [array([-0.5386478], dtype=float32), 1.6498071]. 
=============================================
[2019-04-04 07:07:47,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2553782e-11 4.3702542e-11 1.9367613e-25 4.6982848e-11 6.8546440e-12
 7.8561637e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:47,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2191
[2019-04-04 07:07:47,028] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.55589144774026, 0.5682233046479146, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1557600.0000, 
sim time next is 1558200.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.60267047374836, 0.5735105929606357, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6335558728123635, 0.6911701976535453, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7281929], dtype=float32), 1.3672869]. 
=============================================
[2019-04-04 07:07:51,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.1846501e-11 3.0230224e-10 3.6441109e-25 1.8597242e-11 3.8827316e-11
 9.1087907e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:51,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8691
[2019-04-04 07:07:51,859] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 24.05002153471001, 0.4185959065501003, 1.0, 1.0, 199729.2167776584], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1710000.0000, 
sim time next is 1710600.0000, 
raw observation next is [1.1, 88.00000000000001, 0.0, 0.0, 26.0, 24.64939031393575, 0.5124090589204943, 1.0, 1.0, 77888.86755605576], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.8800000000000001, 0.0, 0.0, 0.6666666666666666, 0.5541158594946459, 0.6708030196401648, 1.0, 1.0, 0.37089936931455125], 
reward next is 0.6291, 
noisyNet noise sample is [array([0.4377002], dtype=float32), 0.07805499]. 
=============================================
[2019-04-04 07:07:56,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0723042e-10 1.4786672e-09 3.1126427e-21 9.0479668e-10 6.6982996e-11
 9.6368985e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:56,121] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1295
[2019-04-04 07:07:56,157] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.566666666666666, 63.0, 132.8333333333333, 0.0, 26.0, 25.66003501195884, 0.3377456549579564, 1.0, 1.0, 23106.17931630252], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1950000.0000, 
sim time next is 1950600.0000, 
raw observation next is [-3.483333333333333, 62.5, 128.6666666666667, 0.0, 26.0, 25.69665176071489, 0.3450545576041857, 1.0, 1.0, 23003.9806099329], 
processed observation next is [1.0, 0.5652173913043478, 0.3661126500461681, 0.625, 0.42888888888888904, 0.0, 0.6666666666666666, 0.6413876467262408, 0.6150181858680619, 1.0, 1.0, 0.10954276480920429], 
reward next is 0.8905, 
noisyNet noise sample is [array([0.9303087], dtype=float32), -0.19437125]. 
=============================================
[2019-04-04 07:07:56,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8788924e-09 4.9977555e-09 7.6189643e-21 1.4656637e-09 8.3997592e-10
 1.1831954e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:07:56,433] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6753
[2019-04-04 07:07:56,478] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 81.66666666666667, 19.66666666666667, 0.0, 26.0, 25.03009228933469, 0.2506010356942625, 0.0, 1.0, 46018.72325876683], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1875000.0000, 
sim time next is 1875600.0000, 
raw observation next is [-4.5, 83.0, 15.0, 0.0, 26.0, 25.01208707985355, 0.2497854512403863, 0.0, 1.0, 54554.82386625389], 
processed observation next is [0.0, 0.7391304347826086, 0.3379501385041552, 0.83, 0.05, 0.0, 0.6666666666666666, 0.5843405899877959, 0.5832618170801288, 0.0, 1.0, 0.25978487555359], 
reward next is 0.7402, 
noisyNet noise sample is [array([0.7793936], dtype=float32), -0.59786594]. 
=============================================
[2019-04-04 07:08:06,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0108996e-11 4.1050278e-11 1.4521783e-25 1.1102612e-11 6.1182674e-12
 4.1926257e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:06,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2260
[2019-04-04 07:08:06,248] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.633333333333333, 84.83333333333334, 67.66666666666667, 373.0000000000001, 26.0, 25.52066288246231, 0.2643640190126533, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1933800.0000, 
sim time next is 1934400.0000, 
raw observation next is [-8.366666666666667, 83.66666666666667, 76.33333333333334, 461.0, 26.0, 25.56360285144506, 0.2870699733305909, 1.0, 1.0, 18744.42614334656], 
processed observation next is [1.0, 0.391304347826087, 0.23084025854108958, 0.8366666666666667, 0.2544444444444445, 0.5093922651933702, 0.6666666666666666, 0.6303002376204218, 0.595689991110197, 1.0, 1.0, 0.08925917211117411], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.30087936], dtype=float32), 1.2801569]. 
=============================================
[2019-04-04 07:08:08,370] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1565757e-08 8.7224690e-09 2.7023205e-21 2.1533282e-09 5.9097771e-10
 5.6942683e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:08,370] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8928
[2019-04-04 07:08:08,388] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.2, 78.0, 0.0, 0.0, 26.0, 23.94058100398517, 0.00342531778932476, 0.0, 1.0, 44742.46258679492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1910400.0000, 
sim time next is 1911000.0000, 
raw observation next is [-8.3, 78.0, 0.0, 0.0, 26.0, 24.03163609780182, 0.01282613638708691, 0.0, 1.0, 44658.31806991061], 
processed observation next is [1.0, 0.08695652173913043, 0.23268698060941828, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5026363414834849, 0.5042753787956956, 0.0, 1.0, 0.21265865747576482], 
reward next is 0.7873, 
noisyNet noise sample is [array([1.2015941], dtype=float32), -0.57024384]. 
=============================================
[2019-04-04 07:08:08,394] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.34446 ]
 [71.49177 ]
 [71.564026]
 [71.63201 ]
 [71.74555 ]], R is [[71.42780304]
 [71.50046539]
 [71.572052  ]
 [71.64266968]
 [71.71231079]].
[2019-04-04 07:08:15,594] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5046647e-09 1.6357866e-09 3.0245957e-22 1.3106231e-10 2.5169281e-10
 5.9945842e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:15,595] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6392
[2019-04-04 07:08:15,616] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.37407286567034, 0.1177945325656538, 0.0, 1.0, 42391.93842068542], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2172600.0000, 
sim time next is 2173200.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.25330206943455, 0.1093102283590046, 0.0, 1.0, 42168.97484472753], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5211085057862125, 0.5364367427863349, 0.0, 1.0, 0.20080464211775012], 
reward next is 0.7992, 
noisyNet noise sample is [array([-0.37406865], dtype=float32), -0.28443334]. 
=============================================
[2019-04-04 07:08:31,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0394727e-09 2.7708214e-08 4.8637550e-21 3.6597574e-09 2.0995023e-09
 1.3709451e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:31,703] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9752
[2019-04-04 07:08:31,721] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.2, 91.0, 0.0, 0.0, 26.0, 23.52940267812638, -0.05221051918452832, 0.0, 1.0, 43142.95608755995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2269800.0000, 
sim time next is 2270400.0000, 
raw observation next is [-9.3, 91.0, 0.0, 0.0, 26.0, 23.53296136701925, -0.06176326669961821, 0.0, 1.0, 43108.37511783088], 
processed observation next is [1.0, 0.2608695652173913, 0.20498614958448752, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4610801139182709, 0.4794122444334606, 0.0, 1.0, 0.20527797675157564], 
reward next is 0.7947, 
noisyNet noise sample is [array([-0.51101375], dtype=float32), -0.61724466]. 
=============================================
[2019-04-04 07:08:37,299] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0852986e-09 1.6090015e-09 6.1399184e-23 2.3696270e-10 1.7483702e-10
 3.3986256e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:37,300] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4191
[2019-04-04 07:08:37,329] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 83.0, 0.0, 0.0, 26.0, 25.21737653277829, 0.3976263854053873, 0.0, 1.0, 42206.82806102424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2152800.0000, 
sim time next is 2153400.0000, 
raw observation next is [-6.800000000000001, 82.83333333333334, 0.0, 0.0, 26.0, 25.19147858450103, 0.3490897065955352, 0.0, 1.0, 42519.48520479897], 
processed observation next is [1.0, 0.9565217391304348, 0.2742382271468144, 0.8283333333333335, 0.0, 0.0, 0.6666666666666666, 0.5992898820417526, 0.616363235531845, 0.0, 1.0, 0.20247373907047128], 
reward next is 0.7975, 
noisyNet noise sample is [array([0.27096233], dtype=float32), -0.46352562]. 
=============================================
[2019-04-04 07:08:37,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8937206e-09 5.7612537e-09 2.2361945e-22 3.4358194e-10 6.3155592e-10
 7.2073068e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:37,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0673
[2019-04-04 07:08:37,534] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.00136814149153, 0.0544200177485234, 0.0, 1.0, 41985.49149567245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97494249058295, 0.04286466206617429, 0.0, 1.0, 41960.29159501617], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.4979118742152459, 0.5142882206887248, 0.0, 1.0, 0.19981091235721984], 
reward next is 0.8002, 
noisyNet noise sample is [array([0.61242664], dtype=float32), -0.4027504]. 
=============================================
[2019-04-04 07:08:42,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3318344e-11 1.4694811e-10 5.6130861e-24 5.5672220e-11 5.1883675e-12
 1.9951528e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:42,210] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0867
[2019-04-04 07:08:42,253] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3166666666666667, 44.66666666666666, 154.3333333333333, 63.0, 26.0, 26.05202319657091, 0.4833436670673492, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2297400.0000, 
sim time next is 2298000.0000, 
raw observation next is [-0.03333333333333333, 44.33333333333334, 137.6666666666667, 61.5, 26.0, 26.24207935206509, 0.4944745245145388, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46168051708217916, 0.4433333333333334, 0.45888888888888907, 0.06795580110497237, 0.6666666666666666, 0.6868399460054242, 0.6648248415048462, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13940297], dtype=float32), -0.96630746]. 
=============================================
[2019-04-04 07:08:42,259] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.13222 ]
 [82.582375]
 [83.00736 ]
 [83.583885]
 [83.0328  ]], R is [[82.00341797]
 [82.18338776]
 [82.36155701]
 [82.53794098]
 [81.76717377]].
[2019-04-04 07:08:46,974] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.9393185e-09 1.0304938e-09 2.3976409e-22 5.4304139e-10 1.5863927e-10
 7.1687231e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:46,975] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4358
[2019-04-04 07:08:47,027] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.883333333333333, 44.16666666666667, 66.0, 702.3333333333333, 26.0, 25.22071208438101, 0.2465256550473158, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2454600.0000, 
sim time next is 2455200.0000, 
raw observation next is [-5.6, 43.0, 68.5, 721.0, 26.0, 25.16846676756765, 0.2416536059008002, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.30747922437673136, 0.43, 0.22833333333333333, 0.7966850828729282, 0.6666666666666666, 0.5973722306306376, 0.5805512019669333, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.727342], dtype=float32), 1.7867609]. 
=============================================
[2019-04-04 07:08:48,952] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8558728e-10 7.0736977e-10 1.9240751e-23 6.6883034e-11 6.5519382e-11
 2.1809559e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:48,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4059
[2019-04-04 07:08:49,005] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.00152840105993, 0.3570679787250625, 0.0, 1.0, 18730.56746775761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2318400.0000, 
sim time next is 2319000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.03416956154145, 0.3545413231372411, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.5861807967951208, 0.6181804410457471, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5199017], dtype=float32), 0.31627873]. 
=============================================
[2019-04-04 07:08:49,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.31557 ]
 [80.65925 ]
 [76.901695]
 [78.754524]
 [81.4748  ]], R is [[82.87431335]
 [82.95637512]
 [82.91904449]
 [82.69428253]
 [82.30960083]].
[2019-04-04 07:08:55,142] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5419671e-09 6.5968581e-10 5.1269806e-24 1.5481992e-10 8.9667010e-11
 1.3017825e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:55,142] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6215
[2019-04-04 07:08:55,252] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 67.66666666666667, 107.0, 300.0, 26.0, 25.30114488455369, 0.319978095259171, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2366400.0000, 
sim time next is 2367000.0000, 
raw observation next is [-3.1, 67.0, 121.0, 360.0, 26.0, 25.26066344444438, 0.3198570494199313, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.37673130193905824, 0.67, 0.4033333333333333, 0.39779005524861877, 0.6666666666666666, 0.6050552870370316, 0.6066190164733104, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4156274], dtype=float32), 0.11381962]. 
=============================================
[2019-04-04 07:08:55,270] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.13479 ]
 [81.48278 ]
 [80.66502 ]
 [79.88239 ]
 [79.115974]], R is [[82.81394958]
 [82.98580933]
 [83.15595245]
 [83.32439423]
 [83.4911499 ]].
[2019-04-04 07:08:57,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4638521e-09 1.5379724e-09 6.2314710e-23 1.6167877e-09 6.7011524e-10
 4.6046331e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:08:57,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2625
[2019-04-04 07:08:57,544] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 38.0, 0.0, 0.0, 26.0, 25.08002713818947, 0.2199969943787259, 0.0, 1.0, 39481.49811096125], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2505600.0000, 
sim time next is 2506200.0000, 
raw observation next is [-1.7, 38.33333333333334, 0.0, 0.0, 26.0, 25.05171601158786, 0.2141952198722425, 0.0, 1.0, 39430.52585610429], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3833333333333334, 0.0, 0.0, 0.6666666666666666, 0.587643000965655, 0.5713984066240808, 0.0, 1.0, 0.18776440883859183], 
reward next is 0.8122, 
noisyNet noise sample is [array([-0.29361978], dtype=float32), -0.37613392]. 
=============================================
[2019-04-04 07:09:07,140] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 07:09:07,141] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:09:07,141] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:09:07,143] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:09:07,143] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:09:07,143] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:09:07,144] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:09:07,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run18
[2019-04-04 07:09:07,198] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run18
[2019-04-04 07:09:07,252] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run18
[2019-04-04 07:10:02,823] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.24104339], dtype=float32), -0.050967425]
[2019-04-04 07:10:02,823] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.933333333333334, 61.66666666666667, 0.0, 0.0, 26.0, 24.6536629902506, 0.2120375940245597, 1.0, 1.0, 18704.22290601432]
[2019-04-04 07:10:02,823] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:10:02,824] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.3587182e-11 2.0062708e-11 2.3162736e-27 7.0005937e-12 1.5101920e-12
 4.2952757e-15 1.0000000e+00], sampled 0.4326878563425154
[2019-04-04 07:12:15,713] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 07:12:44,552] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:12:50,574] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:12:51,607] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 1700000, evaluation results [1700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:13:01,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.13400304e-11 1.12060354e-10 7.36596947e-25 9.44841774e-12
 3.61099995e-12 2.37890897e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:13:01,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8077
[2019-04-04 07:13:01,305] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 26.0, 26.08157597999655, 0.4731931516495056, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2716800.0000, 
sim time next is 2717400.0000, 
raw observation next is [-9.5, 66.0, 115.3333333333333, 709.6666666666666, 26.0, 26.07939661712038, 0.4789665873910186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.1994459833795014, 0.66, 0.3844444444444443, 0.7841620626151012, 0.6666666666666666, 0.6732830514266984, 0.6596555291303395, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7706165], dtype=float32), 0.9362104]. 
=============================================
[2019-04-04 07:13:18,623] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.84041929e-10 1.13309945e-10 1.55780021e-24 5.34092041e-11
 3.02590522e-11 2.65394210e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 07:13:18,623] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0889
[2019-04-04 07:13:18,740] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 89.5, 0.0, 0.0, 26.0, 25.14082316053243, 0.2858345543807895, 0.0, 1.0, 52355.85977041578], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2863800.0000, 
sim time next is 2864400.0000, 
raw observation next is [1.0, 90.66666666666667, 0.0, 0.0, 26.0, 25.06778124890726, 0.2727778166079531, 0.0, 1.0, 54115.74575272306], 
processed observation next is [1.0, 0.13043478260869565, 0.4903047091412743, 0.9066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5889817707422716, 0.5909259388693177, 0.0, 1.0, 0.25769402739391933], 
reward next is 0.7423, 
noisyNet noise sample is [array([0.95638424], dtype=float32), -1.7509657]. 
=============================================
[2019-04-04 07:13:23,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9510942e-09 5.4756288e-09 2.4805079e-21 1.0607440e-09 6.2578831e-10
 1.0261013e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:13:23,357] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8142
[2019-04-04 07:13:23,481] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.16271926825017, 0.05847881694646709, 0.0, 1.0, 47870.20819638183], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2787600.0000, 
sim time next is 2788200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.09896658900318, 0.02217592947156554, 0.0, 1.0, 56215.32361038332], 
processed observation next is [1.0, 0.2608695652173913, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.508247215750265, 0.5073919764905218, 0.0, 1.0, 0.2676920171923015], 
reward next is 0.7323, 
noisyNet noise sample is [array([-1.2658033], dtype=float32), 0.5913614]. 
=============================================
[2019-04-04 07:14:08,820] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1984150e-11 6.1322898e-11 8.5018863e-25 1.8797915e-11 4.4133330e-12
 6.3990419e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:08,820] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8480
[2019-04-04 07:14:08,843] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 45.66666666666667, 400.3333333333334, 26.0, 26.59119442901292, 0.528344110048962, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3516600.0000, 
sim time next is 3517200.0000, 
raw observation next is [3.0, 49.0, 37.5, 338.0, 26.0, 26.54643684672604, 0.6852137358052599, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.49, 0.125, 0.3734806629834254, 0.6666666666666666, 0.7122030705605032, 0.7284045786017533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8427286], dtype=float32), -0.6046354]. 
=============================================
[2019-04-04 07:14:11,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0645911e-09 1.7889408e-10 2.8068042e-24 6.8746675e-11 4.0256291e-11
 1.3884115e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:11,151] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2735
[2019-04-04 07:14:11,165] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 62.66666666666667, 0.0, 0.0, 26.0, 25.69521083805924, 0.4747009394150573, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3702000.0000, 
sim time next is 3702600.0000, 
raw observation next is [2.5, 62.5, 0.0, 0.0, 26.0, 25.74935884001472, 0.4686151033708792, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5318559556786704, 0.625, 0.0, 0.0, 0.6666666666666666, 0.6457799033345601, 0.6562050344569598, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6321473], dtype=float32), 0.595698]. 
=============================================
[2019-04-04 07:14:12,342] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0335147e-10 1.4789400e-10 1.2781120e-23 3.6106989e-11 1.2484312e-11
 1.6826174e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:12,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2249
[2019-04-04 07:14:12,356] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 51.0, 10.83333333333333, 125.8333333333333, 26.0, 25.91894397386335, 0.6019822178323636, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3519600.0000, 
sim time next is 3520200.0000, 
raw observation next is [2.166666666666667, 51.5, 0.0, 0.0, 26.0, 25.91658238135804, 0.5894107585029151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5226223453370269, 0.515, 0.0, 0.0, 0.6666666666666666, 0.6597151984465034, 0.6964702528343051, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.023764], dtype=float32), -0.89240086]. 
=============================================
[2019-04-04 07:14:15,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3087348e-09 2.6863289e-09 3.0488760e-22 9.5688324e-10 3.7313755e-10
 6.6248276e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:15,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4035
[2019-04-04 07:14:15,452] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 26.0, 24.72145400173088, 0.2231336036753243, 0.0, 1.0, 42664.5514999299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3388800.0000, 
sim time next is 3389400.0000, 
raw observation next is [-4.0, 65.5, 0.0, 0.0, 26.0, 24.65540965378807, 0.2429058055935412, 0.0, 1.0, 42892.47305937942], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.655, 0.0, 0.0, 0.6666666666666666, 0.5546174711490058, 0.5809686018645137, 0.0, 1.0, 0.20424987171133058], 
reward next is 0.7958, 
noisyNet noise sample is [array([1.079485], dtype=float32), -2.2653582]. 
=============================================
[2019-04-04 07:14:17,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3734154e-10 2.9114142e-09 3.9223493e-24 3.6802514e-10 1.9437148e-10
 2.0719772e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:17,148] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3489
[2019-04-04 07:14:17,164] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 68.0, 0.0, 0.0, 26.0, 25.20343254415619, 0.4618102159853303, 0.0, 1.0, 83894.40143223676], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3538200.0000, 
sim time next is 3538800.0000, 
raw observation next is [-1.0, 66.0, 0.0, 0.0, 26.0, 25.221606514845, 0.4759455774062478, 0.0, 1.0, 56891.00739327454], 
processed observation next is [1.0, 1.0, 0.4349030470914128, 0.66, 0.0, 0.0, 0.6666666666666666, 0.60180054290375, 0.6586485258020826, 0.0, 1.0, 0.27090955901559305], 
reward next is 0.7291, 
noisyNet noise sample is [array([-0.95848376], dtype=float32), -1.3473967]. 
=============================================
[2019-04-04 07:14:29,744] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.8576911e-12 2.7496845e-11 1.6761399e-25 1.2346917e-11 6.6350237e-12
 1.4650563e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:29,746] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6329
[2019-04-04 07:14:29,784] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 60.0, 112.3333333333333, 790.6666666666667, 26.0, 26.57284296561863, 0.633255038399389, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3840600.0000, 
sim time next is 3841200.0000, 
raw observation next is [-1.0, 60.0, 113.5, 798.5, 26.0, 26.57345376064314, 0.6404835248795125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.37833333333333335, 0.8823204419889503, 0.6666666666666666, 0.714454480053595, 0.7134945082931708, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5547852], dtype=float32), -0.42751968]. 
=============================================
[2019-04-04 07:14:39,118] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3985076e-08 8.2665132e-08 2.3031898e-19 2.0524807e-08 7.7900406e-09
 5.8167086e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 07:14:39,119] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0844
[2019-04-04 07:14:39,145] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.33333333333333, 65.0, 0.0, 0.0, 26.0, 23.48536702626612, -0.03242442852947218, 0.0, 1.0, 43420.88807369488], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3997200.0000, 
sim time next is 3997800.0000, 
raw observation next is [-13.5, 66.0, 0.0, 0.0, 26.0, 23.47516346952275, -0.04456081146964771, 0.0, 1.0, 43308.99658483426], 
processed observation next is [1.0, 0.2608695652173913, 0.0886426592797784, 0.66, 0.0, 0.0, 0.6666666666666666, 0.4562636224602293, 0.4851463961767841, 0.0, 1.0, 0.20623331707063935], 
reward next is 0.7938, 
noisyNet noise sample is [array([-0.27845573], dtype=float32), -0.8054719]. 
=============================================
[2019-04-04 07:14:44,182] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1415515e-09 4.4757464e-10 6.3910809e-24 8.3264749e-11 8.0891287e-11
 3.2716803e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:44,182] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7809
[2019-04-04 07:14:44,200] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.3379848590677, 0.3166756967995695, 0.0, 1.0, 39188.08804844959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4259400.0000, 
sim time next is 4260000.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.33097823201356, 0.3152990720268202, 0.0, 1.0, 39170.06881807988], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6109148526677967, 0.6050996906756068, 0.0, 1.0, 0.1865241372289518], 
reward next is 0.8135, 
noisyNet noise sample is [array([-2.1828222], dtype=float32), -0.5892669]. 
=============================================
[2019-04-04 07:14:44,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.67616 ]
 [83.700806]
 [83.7207  ]
 [83.736305]
 [83.74284 ]], R is [[83.64022827]
 [83.61721802]
 [83.59429932]
 [83.57130432]
 [83.54776001]].
[2019-04-04 07:14:46,009] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.1935898e-10 4.2633644e-10 8.1150366e-24 1.1997925e-10 2.9888560e-11
 2.9022547e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:46,010] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3283
[2019-04-04 07:14:46,056] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 36.66666666666666, 110.0, 698.0, 26.0, 25.50978639058848, 0.4280329913388436, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4182600.0000, 
sim time next is 4183200.0000, 
raw observation next is [-2.0, 35.0, 111.0, 717.0, 26.0, 25.46126764311798, 0.4242309930729164, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.40720221606648205, 0.35, 0.37, 0.7922651933701658, 0.6666666666666666, 0.6217723035931652, 0.6414103310243054, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9932275], dtype=float32), 0.008816614]. 
=============================================
[2019-04-04 07:14:49,871] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9390836e-09 3.4376697e-09 1.0688771e-22 5.4580246e-10 1.4504177e-10
 1.7176835e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:49,871] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9262
[2019-04-04 07:14:49,892] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 30.0, 0.0, 0.0, 26.0, 25.44476501122643, 0.5102788438930119, 0.0, 1.0, 129278.3860222225], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4048200.0000, 
sim time next is 4048800.0000, 
raw observation next is [-4.0, 29.66666666666666, 0.0, 0.0, 26.0, 25.47823706627354, 0.521852799170247, 0.0, 1.0, 51730.45138291249], 
processed observation next is [1.0, 0.8695652173913043, 0.3518005540166205, 0.29666666666666663, 0.0, 0.0, 0.6666666666666666, 0.6231864221894616, 0.6739509330567489, 0.0, 1.0, 0.24633548277577377], 
reward next is 0.7537, 
noisyNet noise sample is [array([0.01522469], dtype=float32), 0.7579433]. 
=============================================
[2019-04-04 07:14:55,391] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.1982470e-10 3.5264064e-10 2.9414927e-25 8.8118048e-11 3.6610732e-11
 1.4021671e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:14:55,401] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5212
[2019-04-04 07:14:55,412] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 69.66666666666667, 0.0, 0.0, 26.0, 25.63364673631331, 0.3875449804300069, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4335600.0000, 
sim time next is 4336200.0000, 
raw observation next is [3.75, 69.5, 0.0, 0.0, 26.0, 25.58688397944891, 0.3780071888103713, 0.0, 1.0, 18736.74420343309], 
processed observation next is [1.0, 0.17391304347826086, 0.5664819944598338, 0.695, 0.0, 0.0, 0.6666666666666666, 0.6322403316207424, 0.6260023962701238, 0.0, 1.0, 0.08922259144491947], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.5954502], dtype=float32), 0.20667085]. 
=============================================
[2019-04-04 07:15:05,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4537614e-09 3.3901737e-10 1.4061585e-24 3.6097847e-10 8.8993486e-11
 4.6561192e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:15:05,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2793
[2019-04-04 07:15:05,020] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.2, 75.0, 0.0, 0.0, 26.0, 25.50350591352824, 0.4048620937261207, 0.0, 1.0, 37295.7139293152], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4323600.0000, 
sim time next is 4324200.0000, 
raw observation next is [4.25, 74.5, 0.0, 0.0, 26.0, 25.53303326583459, 0.4099569925614534, 0.0, 1.0, 19298.16890308583], 
processed observation next is [1.0, 0.043478260869565216, 0.5803324099722993, 0.745, 0.0, 0.0, 0.6666666666666666, 0.6277527721528825, 0.6366523308538178, 0.0, 1.0, 0.0918960423956468], 
reward next is 0.9081, 
noisyNet noise sample is [array([0.39480153], dtype=float32), -0.5808332]. 
=============================================
[2019-04-04 07:15:07,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1579746e-09 1.6208697e-09 5.9533033e-24 2.9994285e-10 1.2883315e-10
 4.8286407e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:15:07,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1441
[2019-04-04 07:15:07,580] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.9, 75.0, 0.0, 0.0, 26.0, 25.40143386246854, 0.3773275289214276, 0.0, 1.0, 48588.16871055894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4345200.0000, 
sim time next is 4345800.0000, 
raw observation next is [2.916666666666667, 74.83333333333333, 0.0, 0.0, 26.0, 25.49379098706451, 0.3856785219767691, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.543397968605725, 0.7483333333333333, 0.0, 0.0, 0.6666666666666666, 0.6244825822553759, 0.6285595073255897, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96715313], dtype=float32), -1.3953005]. 
=============================================
[2019-04-04 07:15:10,735] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5538749e-09 1.3825795e-09 8.6083864e-22 4.9679960e-10 2.5493085e-10
 1.5771703e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:15:10,738] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0421
[2019-04-04 07:15:10,797] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 25.16462895835867, 0.4179323106837782, 0.0, 1.0, 41895.62186974448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4749600.0000, 
sim time next is 4750200.0000, 
raw observation next is [-3.5, 80.5, 0.0, 0.0, 26.0, 25.13333981593473, 0.4102831689391341, 0.0, 1.0, 41770.82818855055], 
processed observation next is [1.0, 1.0, 0.36565096952908593, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5944449846612274, 0.6367610563130447, 0.0, 1.0, 0.1989087056597645], 
reward next is 0.8011, 
noisyNet noise sample is [array([-0.9189251], dtype=float32), 2.8034184]. 
=============================================
[2019-04-04 07:15:11,055] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.01830645e-11 1.33280401e-11 7.49752635e-28 2.02367203e-12
 4.60507012e-13 1.39582358e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:15:11,055] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8694
[2019-04-04 07:15:11,084] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.416666666666667, 83.33333333333333, 135.0, 165.0, 26.0, 26.03915394114394, 0.5998196624173474, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4438200.0000, 
sim time next is 4438800.0000, 
raw observation next is [1.3, 84.0, 142.5, 131.5, 26.0, 26.17496112394989, 0.5985749268683356, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49861495844875353, 0.84, 0.475, 0.1453038674033149, 0.6666666666666666, 0.6812467603291573, 0.6995249756227785, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09331466], dtype=float32), 0.2892361]. 
=============================================
[2019-04-04 07:15:11,122] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.3648379e-12 1.1530651e-11 1.6084621e-27 1.6954562e-12 4.2245683e-13
 1.4491917e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:15:11,125] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1322
[2019-04-04 07:15:11,147] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.25, 84.33333333333333, 150.0, 97.99999999999999, 26.0, 26.16226613389876, 0.5924372373356519, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4439400.0000, 
sim time next is 4440000.0000, 
raw observation next is [1.2, 84.66666666666667, 157.5, 64.5, 26.0, 26.15356765839344, 0.6020814672286695, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4958448753462604, 0.8466666666666667, 0.525, 0.0712707182320442, 0.6666666666666666, 0.6794639715327868, 0.7006938224095566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.09331466], dtype=float32), 0.2892361]. 
=============================================
[2019-04-04 07:15:11,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[93.42663 ]
 [94.193634]
 [94.91207 ]
 [95.4947  ]
 [95.72585 ]], R is [[92.85488129]
 [92.92633057]
 [92.99707031]
 [93.06710052]
 [93.13642883]].
[2019-04-04 07:15:24,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1317115e-13 1.1000353e-12 3.3419516e-29 1.7053602e-13 4.0222386e-14
 9.1891216e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 07:15:24,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7275
[2019-04-04 07:15:24,242] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.333333333333334, 25.66666666666667, 94.83333333333334, 781.5, 26.0, 26.24934143355232, 0.7429722611791032, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4980000.0000, 
sim time next is 4980600.0000, 
raw observation next is [8.5, 25.5, 92.0, 774.0, 26.0, 26.79393048728001, 0.811827658382259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.698060941828255, 0.255, 0.30666666666666664, 0.8552486187845304, 0.6666666666666666, 0.7328275406066677, 0.770609219460753, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.59299237], dtype=float32), 1.1914413]. 
=============================================
[2019-04-04 07:15:29,414] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3032528e-10 3.6041298e-10 8.7965703e-23 3.0971736e-10 8.9535414e-11
 1.2167313e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:15:29,417] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7818
[2019-04-04 07:15:29,436] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 44.16666666666667, 260.0, 383.3333333333333, 26.0, 25.10472383474497, 0.3704792037217491, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4888200.0000, 
sim time next is 4888800.0000, 
raw observation next is [2.0, 44.0, 254.0, 381.0, 26.0, 25.10122610408689, 0.3683403172937123, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.44, 0.8466666666666667, 0.42099447513812155, 0.6666666666666666, 0.5917688420072409, 0.6227801057645708, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40373152], dtype=float32), 0.7719879]. 
=============================================
[2019-04-04 07:15:31,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:31,576] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:31,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run14
[2019-04-04 07:15:37,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:37,434] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:37,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run14
[2019-04-04 07:15:38,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:38,353] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:38,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run14
[2019-04-04 07:15:38,584] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:38,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:38,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run14
[2019-04-04 07:15:39,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:39,199] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:39,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run14
[2019-04-04 07:15:41,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:41,669] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:41,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run14
[2019-04-04 07:15:41,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:41,781] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:41,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run14
[2019-04-04 07:15:44,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:44,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:44,066] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run14
[2019-04-04 07:15:46,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:46,005] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:46,009] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run14
[2019-04-04 07:15:46,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.2530612e-12 3.1642080e-12 2.6867387e-28 3.9640302e-13 2.5027388e-14
 7.9981569e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 07:15:46,733] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1162
[2019-04-04 07:15:46,755] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.833333333333332, 25.16666666666667, 122.0, 863.3333333333334, 26.0, 27.41002871097812, 0.8786266132331164, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5057400.0000, 
sim time next is 5058000.0000, 
raw observation next is [9.0, 25.0, 121.0, 862.5, 26.0, 27.5799890564953, 0.9038773215667116, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7119113573407203, 0.25, 0.4033333333333333, 0.9530386740331491, 0.6666666666666666, 0.7983324213746084, 0.8012924405222371, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.51740927], dtype=float32), 0.20947339]. 
=============================================
[2019-04-04 07:15:46,761] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[100.24464]
 [100.0868 ]
 [ 99.90101]
 [ 99.60781]
 [ 99.4417 ]], R is [[100.50972748]
 [100.50463104]
 [100.49958801]
 [100.49459076]
 [100.48964691]].
[2019-04-04 07:15:48,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:48,361] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:48,364] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run14
[2019-04-04 07:15:50,206] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:50,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:50,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run14
[2019-04-04 07:15:50,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:50,709] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:50,712] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run14
[2019-04-04 07:15:51,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:51,473] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:51,490] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run14
[2019-04-04 07:15:51,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:51,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:51,888] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run14
[2019-04-04 07:15:53,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:53,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:53,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run14
[2019-04-04 07:15:59,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:15:59,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:15:59,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run14
[2019-04-04 07:16:00,715] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9073318e-10 1.4939322e-10 1.3845172e-23 7.3586075e-11 4.5155307e-11
 1.2147000e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:16:00,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2959
[2019-04-04 07:16:00,782] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 92.33333333333334, 0.0, 0.0, 26.0, 24.37928643998555, 0.1548284254176366, 0.0, 1.0, 40375.49937103171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 88800.0000, 
sim time next is 89400.0000, 
raw observation next is [-0.5, 91.66666666666667, 0.0, 0.0, 26.0, 24.35419017580823, 0.1528319096119378, 0.0, 1.0, 40551.69227925447], 
processed observation next is [1.0, 0.0, 0.44875346260387816, 0.9166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5295158479840193, 0.5509439698706459, 0.0, 1.0, 0.19310329656787845], 
reward next is 0.8069, 
noisyNet noise sample is [array([-0.48293957], dtype=float32), -2.5062082]. 
=============================================
[2019-04-04 07:16:39,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5046732e-10 8.9049684e-11 1.7295443e-25 2.2866013e-11 1.5604988e-11
 5.6532745e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:16:39,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4710
[2019-04-04 07:16:39,738] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 88.66666666666667, 0.0, 0.0, 26.0, 24.4617828459481, 0.1609451262503209, 0.0, 1.0, 40802.64454175421], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 540600.0000, 
sim time next is 541200.0000, 
raw observation next is [0.9000000000000001, 89.33333333333334, 0.0, 0.0, 26.0, 24.4303735245241, 0.1844054652343594, 0.0, 1.0, 41107.19654509719], 
processed observation next is [0.0, 0.2608695652173913, 0.48753462603878117, 0.8933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5358644603770083, 0.5614684884114531, 0.0, 1.0, 0.19574855497665328], 
reward next is 0.8043, 
noisyNet noise sample is [array([-0.7012954], dtype=float32), -0.4689429]. 
=============================================
[2019-04-04 07:16:40,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4504019e-11 8.2230556e-10 9.5216292e-24 1.5268212e-10 1.8726307e-11
 2.2346570e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:16:40,275] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-04 07:16:40,350] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.583333333333334, 40.33333333333334, 0.0, 0.0, 26.0, 25.12967832192227, 0.2873775679768276, 1.0, 1.0, 108038.2074852404], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 414600.0000, 
sim time next is 415200.0000, 
raw observation next is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 26.0, 25.11212679063894, 0.2897074471181182, 1.0, 1.0, 95333.88043480816], 
processed observation next is [1.0, 0.8260869565217391, 0.19482917820867957, 0.40666666666666673, 0.0, 0.0, 0.6666666666666666, 0.5926772325532449, 0.5965691490393727, 1.0, 1.0, 0.4539708592133722], 
reward next is 0.5460, 
noisyNet noise sample is [array([1.8477726], dtype=float32), -0.42588925]. 
=============================================
[2019-04-04 07:16:43,560] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1794374e-09 5.8782357e-09 1.8075859e-21 5.7329147e-10 7.3081591e-10
 1.1653094e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:16:43,561] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7463
[2019-04-04 07:16:43,586] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.39861733054365, 0.1286303989315176, 0.0, 1.0, 44914.29303314945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 424200.0000, 
sim time next is 424800.0000, 
raw observation next is [-10.6, 49.0, 0.0, 0.0, 26.0, 24.35338979924169, 0.117486072294204, 0.0, 1.0, 44894.85890227435], 
processed observation next is [1.0, 0.9565217391304348, 0.1689750692520776, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5294491499368075, 0.539162024098068, 0.0, 1.0, 0.21378504239178261], 
reward next is 0.7862, 
noisyNet noise sample is [array([0.9108656], dtype=float32), -0.35686308]. 
=============================================
[2019-04-04 07:16:44,267] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9256229e-11 3.8793781e-11 3.0850578e-25 1.1918977e-11 1.3718024e-12
 4.7503759e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:16:44,267] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5797
[2019-04-04 07:16:44,347] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.4, 60.0, 64.5, 746.5, 26.0, 25.87398962055461, 0.3450403505080835, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 385200.0000, 
sim time next is 385800.0000, 
raw observation next is [-13.3, 58.5, 62.33333333333333, 752.3333333333333, 26.0, 25.83821537888281, 0.34130228769736, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.09418282548476452, 0.585, 0.20777777777777776, 0.8313075506445672, 0.6666666666666666, 0.6531846149069009, 0.6137674292324533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6761634], dtype=float32), -0.6180233]. 
=============================================
[2019-04-04 07:16:46,806] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9218349e-11 1.0367467e-10 5.6707161e-26 1.4383037e-11 2.5763816e-12
 3.2222035e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:16:46,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6982
[2019-04-04 07:16:46,871] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 38.5, 20.0, 0.0, 26.0, 24.34788797131685, 0.1720396030547403, 1.0, 1.0, 197728.5200991297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 491400.0000, 
sim time next is 492000.0000, 
raw observation next is [1.1, 40.0, 16.66666666666667, 0.0, 26.0, 24.8683784027478, 0.2325869169961444, 1.0, 1.0, 58600.04206612171], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.4, 0.05555555555555557, 0.0, 0.6666666666666666, 0.5723648668956501, 0.5775289723320481, 1.0, 1.0, 0.2790478193624843], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.92361987], dtype=float32), 1.1603438]. 
=============================================
[2019-04-04 07:16:46,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[89.46887 ]
 [88.40299 ]
 [88.33116 ]
 [88.35346 ]
 [88.439644]], R is [[89.42810059]
 [88.59225464]
 [88.54138184]
 [88.65596771]
 [88.76940918]].
[2019-04-04 07:16:50,310] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1731696e-09 5.6288652e-10 5.7987587e-24 1.4934906e-10 1.6434884e-10
 9.3973885e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:16:50,310] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3590
[2019-04-04 07:16:50,370] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 65.0, 111.6666666666667, 17.0, 26.0, 24.8973302396802, 0.1823476564539273, 0.0, 1.0, 73376.39247394196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 641400.0000, 
sim time next is 642000.0000, 
raw observation next is [-3.733333333333333, 65.0, 105.8333333333333, 8.499999999999998, 26.0, 24.83814619027102, 0.1838788261953926, 0.0, 1.0, 94871.23515256731], 
processed observation next is [0.0, 0.43478260869565216, 0.35918744228993543, 0.65, 0.3527777777777777, 0.009392265193370164, 0.6666666666666666, 0.5698455158559182, 0.5612929420651308, 0.0, 1.0, 0.45176778644079674], 
reward next is 0.5482, 
noisyNet noise sample is [array([-1.3521338], dtype=float32), 1.0191905]. 
=============================================
[2019-04-04 07:16:50,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.75865 ]
 [81.27897 ]
 [82.22687 ]
 [83.012634]
 [83.34912 ]], R is [[80.20302582]
 [80.05158234]
 [80.25106812]
 [80.44855499]
 [80.64407349]].
[2019-04-04 07:16:55,305] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.03182865e-10 3.17423171e-11 4.15984844e-25 1.78446199e-11
 3.16597681e-12 1.36293957e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 07:16:55,306] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2519
[2019-04-04 07:16:55,374] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 81.0, 128.8333333333333, 488.3333333333333, 26.0, 24.9946501321666, 0.3471977419198642, 0.0, 1.0, 32031.58099451198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 570000.0000, 
sim time next is 570600.0000, 
raw observation next is [-1.2, 81.5, 127.0, 467.0, 26.0, 24.98975120024329, 0.3543634204015632, 0.0, 1.0, 34572.10916183866], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.815, 0.42333333333333334, 0.5160220994475138, 0.6666666666666666, 0.5824792666869408, 0.6181211401338543, 0.0, 1.0, 0.16462909124685077], 
reward next is 0.8354, 
noisyNet noise sample is [array([-0.87770224], dtype=float32), -0.25672367]. 
=============================================
[2019-04-04 07:16:56,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.15089549e-09 2.11714271e-10 1.59224405e-24 6.67112893e-11
 3.07096050e-11 5.26372836e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 07:16:56,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8200
[2019-04-04 07:16:56,037] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 83.0, 0.0, 0.0, 26.0, 24.8582481376911, 0.2414491535464535, 0.0, 1.0, 42839.56651582883], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [-3.2, 83.0, 0.0, 0.0, 26.0, 24.83144655275812, 0.2380029900464108, 0.0, 1.0, 42774.52817178831], 
processed observation next is [0.0, 0.9565217391304348, 0.37396121883656513, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5692872127298433, 0.5793343300154703, 0.0, 1.0, 0.20368822938946815], 
reward next is 0.7963, 
noisyNet noise sample is [array([0.7453197], dtype=float32), -3.0220368]. 
=============================================
[2019-04-04 07:16:56,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.72016 ]
 [81.81836 ]
 [81.920105]
 [81.98544 ]
 [82.06594 ]], R is [[81.57891083]
 [81.55912781]
 [81.53924561]
 [81.5193634 ]
 [81.49954224]].
[2019-04-04 07:17:21,242] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4825773e-13 3.5793523e-13 8.4310658e-31 1.2540813e-13 5.1993488e-14
 5.0749860e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 07:17:21,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5858
[2019-04-04 07:17:21,267] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.16666666666667, 62.0, 193.1666666666667, 300.0, 26.0, 26.52854067969509, 0.8631603599170906, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1081200.0000, 
sim time next is 1081800.0000, 
raw observation next is [17.45, 60.5, 181.0, 317.0, 26.0, 27.00579470250378, 0.7243764907867782, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9459833795013851, 0.605, 0.6033333333333334, 0.35027624309392263, 0.6666666666666666, 0.7504828918753151, 0.7414588302622594, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.024677], dtype=float32), 1.1151444]. 
=============================================
[2019-04-04 07:17:22,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9380105e-12 2.8608982e-12 1.2005261e-27 5.8243525e-13 5.3684470e-14
 9.7494392e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:17:22,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7720
[2019-04-04 07:17:22,131] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 84.0, 62.5, 0.0, 26.0, 25.4405895176471, 0.2949414888262283, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 900000.0000, 
sim time next is 900600.0000, 
raw observation next is [1.1, 84.0, 67.33333333333333, 0.0, 26.0, 25.48181843740219, 0.300458209213927, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.49307479224376743, 0.84, 0.22444444444444442, 0.0, 0.6666666666666666, 0.6234848697835158, 0.6001527364046423, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8429467], dtype=float32), 0.29411802]. 
=============================================
[2019-04-04 07:17:22,883] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.3330810e-12 3.4569298e-12 4.5697036e-28 3.8190043e-13 2.1332805e-13
 5.4973232e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:17:22,886] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8302
[2019-04-04 07:17:22,898] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 92.83333333333333, 30.0, 0.0, 26.0, 25.78394138106896, 0.4027152089433052, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 922200.0000, 
sim time next is 922800.0000, 
raw observation next is [4.600000000000001, 92.66666666666667, 24.0, 0.0, 26.0, 25.76920346665583, 0.3958484349374678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5900277008310251, 0.9266666666666667, 0.08, 0.0, 0.6666666666666666, 0.6474336222213193, 0.6319494783124893, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45656422], dtype=float32), -1.7375708]. 
=============================================
[2019-04-04 07:17:27,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5366629e-10 1.4625856e-10 7.5640998e-25 1.3913351e-10 2.6417899e-11
 9.4899850e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:17:27,253] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6724
[2019-04-04 07:17:27,261] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 78.66666666666667, 0.0, 0.0, 26.0, 24.13840733661447, 0.2807690691417994, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1210800.0000, 
sim time next is 1211400.0000, 
raw observation next is [16.1, 79.0, 0.0, 0.0, 26.0, 24.11465165540062, 0.2809293106884665, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5095543046167185, 0.5936431035628221, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85995626], dtype=float32), 0.30227244]. 
=============================================
[2019-04-04 07:17:32,786] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5573278e-09 7.0136452e-10 7.2396645e-25 2.1978594e-10 6.5463045e-11
 8.7921153e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:17:32,800] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6880
[2019-04-04 07:17:32,876] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 80.5, 0.0, 0.0, 26.0, 24.09920847016056, 0.2681616709624834, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1213800.0000, 
sim time next is 1214400.0000, 
raw observation next is [16.1, 81.0, 0.0, 0.0, 26.0, 24.07148002383564, 0.2632064915061604, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5059566686529701, 0.5877354971687202, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.57138836], dtype=float32), -0.90187824]. 
=============================================
[2019-04-04 07:17:38,799] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2735320e-12 2.4144079e-12 3.8894598e-27 1.5534243e-12 2.7205668e-13
 1.4479595e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:17:38,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2819
[2019-04-04 07:17:38,815] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.71666666666667, 54.33333333333333, 160.6666666666667, 0.0, 26.0, 27.13214309712052, 0.9009332964752269, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1086600.0000, 
sim time next is 1087200.0000, 
raw observation next is [18.8, 54.0, 155.5, 0.0, 26.0, 26.56257305671717, 0.8770586970082886, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9833795013850417, 0.54, 0.5183333333333333, 0.0, 0.6666666666666666, 0.7135477547264308, 0.792352899002763, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47386432], dtype=float32), 0.458326]. 
=============================================
[2019-04-04 07:17:44,989] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-04 07:17:45,025] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:17:45,026] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:17:45,028] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run19
[2019-04-04 07:17:45,100] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:17:45,100] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:17:45,102] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run19
[2019-04-04 07:17:45,131] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:17:45,132] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:17:45,134] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run19
[2019-04-04 07:18:28,464] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.2309767], dtype=float32), -0.06737358]
[2019-04-04 07:18:28,464] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-9.316666666666666, 43.16666666666667, 124.6666666666667, 324.6666666666666, 26.0, 25.13505885343567, 0.23119481877607, 0.0, 1.0, 44030.66850317904]
[2019-04-04 07:18:28,464] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:18:28,465] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.2411843e-09 3.6949392e-09 2.0730093e-20 1.4774661e-09 5.0175242e-10
 9.1267697e-12 1.0000000e+00], sampled 0.8683299205376935
[2019-04-04 07:18:39,255] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2309767], dtype=float32), -0.06737358]
[2019-04-04 07:18:39,256] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.108737532666667, 82.55740258833333, 6.448902286666666, 0.0, 26.0, 24.61219207797524, 0.1786137584074045, 0.0, 1.0, 103182.7013477549]
[2019-04-04 07:18:39,256] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:18:39,256] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1418128e-09 5.3799312e-09 2.9680527e-22 1.2571806e-09 5.0136784e-10
 4.3460296e-12 1.0000000e+00], sampled 0.5452293438990847
[2019-04-04 07:18:54,153] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.2309767], dtype=float32), -0.06737358]
[2019-04-04 07:18:54,153] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [0.9122917963333332, 90.18544739000001, 112.7486689916667, 0.0, 26.0, 24.9996230934419, 0.4323582069078797, 0.0, 1.0, 0.0]
[2019-04-04 07:18:54,153] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:18:54,154] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.1984582e-10 1.7918264e-10 4.8443790e-24 4.7752219e-11 2.9143448e-11
 1.3204924e-13 1.0000000e+00], sampled 0.7703151973132134
[2019-04-04 07:20:54,793] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 07:21:24,391] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 07:21:30,177] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 07:21:31,211] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1800000, evaluation results [1800000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 07:21:34,132] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1791292e-10 3.6515260e-11 2.8679969e-26 2.8518761e-12 1.4476007e-11
 6.5503158e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:21:34,132] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3429
[2019-04-04 07:21:34,156] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.9, 98.66666666666667, 0.0, 0.0, 26.0, 24.5841763306522, 0.42735952483161, 0.0, 1.0, 45616.4650487844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1272000.0000, 
sim time next is 1272600.0000, 
raw observation next is [10.25, 98.0, 0.0, 0.0, 26.0, 24.59008743476455, 0.4330187011210192, 0.0, 1.0, 45080.17091632891], 
processed observation next is [0.0, 0.7391304347826086, 0.7465373961218837, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5491739528970457, 0.6443395670403397, 0.0, 1.0, 0.21466748055394722], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.6258761], dtype=float32), 0.24120215]. 
=============================================
[2019-04-04 07:21:38,676] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.4977754e-12 1.9797039e-11 1.6771104e-27 1.5688094e-12 3.8453323e-13
 1.8889923e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:21:38,676] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6829
[2019-04-04 07:21:38,736] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 31.5, 0.0, 26.0, 26.07973968565456, 0.5890791945889431, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1328400.0000, 
sim time next is 1329000.0000, 
raw observation next is [0.5, 92.0, 36.0, 0.0, 26.0, 26.05969484757576, 0.5780948508084677, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.12, 0.0, 0.6666666666666666, 0.67164123729798, 0.6926982836028226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8547619], dtype=float32), -0.47645748]. 
=============================================
[2019-04-04 07:21:38,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[92.994354]
 [93.166824]
 [93.35304 ]
 [93.660225]
 [94.032135]], R is [[92.8629303 ]
 [92.93430328]
 [93.00495911]
 [93.07491302]
 [93.14416504]].
[2019-04-04 07:21:55,092] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9025676e-11 5.1857088e-11 2.2585826e-26 1.3108050e-11 2.6698210e-12
 1.1033879e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:21:55,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9815
[2019-04-04 07:21:55,111] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.116666666666667, 96.66666666666666, 78.0, 235.9999999999999, 26.0, 26.04645600683811, 0.5472631076609404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1507800.0000, 
sim time next is 1508400.0000, 
raw observation next is [3.3, 96.0, 80.5, 354.0, 26.0, 26.06658065194552, 0.5663758513927363, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.554016620498615, 0.96, 0.2683333333333333, 0.3911602209944751, 0.6666666666666666, 0.6722150543287935, 0.6887919504642454, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3289914], dtype=float32), 0.88112146]. 
=============================================
[2019-04-04 07:21:56,849] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1989810e-12 2.7871745e-11 2.8937674e-26 9.2296336e-12 3.7677235e-12
 1.0967952e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:21:56,849] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9449
[2019-04-04 07:21:56,865] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.06666666666667, 58.66666666666667, 0.0, 0.0, 26.0, 26.77745517967717, 0.7527409961793613, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1618800.0000, 
sim time next is 1619400.0000, 
raw observation next is [10.78333333333333, 59.83333333333334, 0.0, 0.0, 26.0, 26.63983537318604, 0.7460857319379434, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7613111726685133, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.7199862810988366, 0.7486952439793145, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.06911091], dtype=float32), -1.1878936]. 
=============================================
[2019-04-04 07:21:57,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4838699e-11 2.2479768e-10 9.5263109e-25 3.2311077e-11 2.0557491e-11
 9.7710434e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:21:57,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8417
[2019-04-04 07:21:58,026] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.1, 82.66666666666667, 0.0, 0.0, 26.0, 25.67220544148928, 0.5864123950220548, 0.0, 1.0, 70871.9775822074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1637400.0000, 
sim time next is 1638000.0000, 
raw observation next is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6324866818770666, 0.6945378394286904, 0.0, 1.0, 0.4448516215070642], 
reward next is 0.5551, 
noisyNet noise sample is [array([-1.4449278], dtype=float32), 0.5293252]. 
=============================================
[2019-04-04 07:21:58,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.463  ]
 [84.25403]
 [84.45909]
 [84.50447]
 [84.5957 ]], R is [[84.63956451]
 [84.45568085]
 [84.61112213]
 [84.76501465]
 [84.91736603]].
[2019-04-04 07:22:10,047] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3585977e-12 4.4018912e-12 7.3375564e-26 2.4385481e-12 3.8357872e-13
 6.2055688e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:22:10,047] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9887
[2019-04-04 07:22:10,075] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 93.33333333333333, 0.0, 26.0, 26.19253713161497, 0.6223947678742777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1691400.0000, 
sim time next is 1692000.0000, 
raw observation next is [1.1, 88.0, 90.0, 0.0, 26.0, 26.27104727888874, 0.6224099630330994, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.3, 0.0, 0.6666666666666666, 0.689253939907395, 0.7074699876776998, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0665396], dtype=float32), -0.8955955]. 
=============================================
[2019-04-04 07:22:10,079] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.00583 ]
 [86.295364]
 [86.571625]
 [86.752884]
 [86.933655]], R is [[85.85469055]
 [85.99614716]
 [86.13618469]
 [86.27482605]
 [86.41207886]].
[2019-04-04 07:22:34,874] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7057087e-10 2.1792264e-09 1.9393951e-23 4.6836579e-10 8.2732363e-11
 1.4797907e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:22:34,875] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8615
[2019-04-04 07:22:34,962] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.62482582125511, 0.009840905635030853, 1.0, 1.0, 203280.4378791368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1927800.0000, 
sim time next is 1928400.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 26.0, 24.4452978489698, 0.1105834951628297, 1.0, 1.0, 155932.3830473522], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5371081540808168, 0.5368611650542766, 1.0, 1.0, 0.7425351573683437], 
reward next is 0.2575, 
noisyNet noise sample is [array([-1.1084598], dtype=float32), -2.9918053]. 
=============================================
[2019-04-04 07:22:41,279] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.6419740e-10 8.0709738e-11 4.5612465e-25 6.4137883e-11 1.6759472e-11
 3.7059192e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:22:41,279] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7580
[2019-04-04 07:22:41,370] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 24.0, 18.0, 26.0, 25.05102100262842, 0.2444802709955953, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2275200.0000, 
sim time next is 2275800.0000, 
raw observation next is [-9.316666666666666, 90.33333333333334, 31.0, 17.33333333333333, 26.0, 25.13767757272259, 0.2469490737246974, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20452446906740537, 0.9033333333333334, 0.10333333333333333, 0.01915285451197053, 0.6666666666666666, 0.5948064643935492, 0.5823163579082324, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.51323783], dtype=float32), 1.1152568]. 
=============================================
[2019-04-04 07:22:52,039] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0206706e-08 8.4267224e-09 4.7519927e-21 1.9974300e-09 9.7103658e-10
 5.0415210e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:22:52,039] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2662
[2019-04-04 07:22:52,162] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.40948940203087, -0.09018449569682807, 0.0, 1.0, 43041.82098807226], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2271600.0000, 
sim time next is 2272200.0000, 
raw observation next is [-9.5, 91.00000000000001, 0.0, 0.0, 26.0, 23.32747807033447, -0.1014317316874055, 0.0, 1.0, 43019.49739951664], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.443956505861206, 0.4661894227708648, 0.0, 1.0, 0.20485474952150778], 
reward next is 0.7951, 
noisyNet noise sample is [array([0.7119187], dtype=float32), -0.31851527]. 
=============================================
[2019-04-04 07:23:01,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9224858e-09 6.3998973e-10 9.9027079e-23 4.0227682e-10 3.3111855e-10
 1.5580534e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:01,069] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3542
[2019-04-04 07:23:01,120] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 34.33333333333334, 0.0, 0.0, 26.0, 25.18230652060769, 0.247660480971791, 0.0, 1.0, 40253.21457258502], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2500800.0000, 
sim time next is 2501400.0000, 
raw observation next is [-0.7, 34.66666666666666, 0.0, 0.0, 26.0, 25.16526033954659, 0.2447614276913105, 0.0, 1.0, 40175.74353785173], 
processed observation next is [0.0, 0.9565217391304348, 0.443213296398892, 0.34666666666666657, 0.0, 0.0, 0.6666666666666666, 0.5971050282955493, 0.5815871425637702, 0.0, 1.0, 0.19131306446596064], 
reward next is 0.8087, 
noisyNet noise sample is [array([1.0464847], dtype=float32), -0.091562375]. 
=============================================
[2019-04-04 07:23:08,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3714388e-10 4.0001971e-10 7.7051834e-22 4.0436465e-10 4.6666417e-11
 1.9330724e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:08,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1638
[2019-04-04 07:23:08,452] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 53.33333333333334, 201.1666666666667, 70.66666666666666, 26.0, 24.99199780840915, 0.297906145564304, 0.0, 1.0, 42924.709393446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2380800.0000, 
sim time next is 2381400.0000, 
raw observation next is [-0.3, 53.0, 191.0, 0.0, 26.0, 24.93904132143366, 0.290751477293717, 0.0, 1.0, 62167.99423955715], 
processed observation next is [0.0, 0.5652173913043478, 0.4542936288088643, 0.53, 0.6366666666666667, 0.0, 0.6666666666666666, 0.5782534434528049, 0.5969171590979057, 0.0, 1.0, 0.296038067807415], 
reward next is 0.7040, 
noisyNet noise sample is [array([1.3044811], dtype=float32), 0.9836256]. 
=============================================
[2019-04-04 07:23:09,992] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7319460e-08 5.7344813e-09 5.2995499e-22 2.4763347e-09 5.9456406e-10
 1.1266848e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:09,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9372
[2019-04-04 07:23:10,077] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 50.0, 50.5, 540.5, 26.0, 24.92156052500962, 0.2356530634984777, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2451600.0000, 
sim time next is 2452200.0000, 
raw observation next is [-7.016666666666667, 48.83333333333334, 54.0, 582.0, 26.0, 25.21421849242588, 0.2521130004909745, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2682363804247461, 0.48833333333333345, 0.18, 0.6430939226519337, 0.6666666666666666, 0.6011848743688232, 0.5840376668303248, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.727219], dtype=float32), -0.23343836]. 
=============================================
[2019-04-04 07:23:14,118] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0881151e-09 3.0334146e-09 1.8652779e-21 9.4809216e-10 6.0534328e-10
 4.0500333e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:14,119] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3332
[2019-04-04 07:23:14,174] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 48.66666666666667, 147.6666666666667, 56.83333333333332, 26.0, 24.95437476284992, 0.2923739763397237, 0.0, 1.0, 38527.31887305374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2385600.0000, 
sim time next is 2386200.0000, 
raw observation next is [0.0, 47.83333333333334, 135.3333333333333, 113.6666666666666, 26.0, 24.93904919763082, 0.2967338361799097, 0.0, 1.0, 43348.21827574441], 
processed observation next is [0.0, 0.6086956521739131, 0.46260387811634357, 0.47833333333333344, 0.45111111111111096, 0.125598526703499, 0.6666666666666666, 0.5782540998025683, 0.5989112787266365, 0.0, 1.0, 0.20642008702735434], 
reward next is 0.7936, 
noisyNet noise sample is [array([-0.22623752], dtype=float32), 0.44189203]. 
=============================================
[2019-04-04 07:23:29,781] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1178566e-11 5.3075606e-11 1.4582075e-24 2.0762558e-11 4.0472378e-12
 1.7508427e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:29,781] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9379
[2019-04-04 07:23:29,841] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 72.0, 113.6666666666667, 663.6666666666667, 26.0, 26.10218855146347, 0.4645319525927477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [-10.5, 70.0, 117.0, 674.0, 26.0, 26.07611993835756, 0.4683942589864222, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.17174515235457063, 0.7, 0.39, 0.7447513812154696, 0.6666666666666666, 0.6730099948631301, 0.6561314196621407, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2527409], dtype=float32), -0.5853669]. 
=============================================
[2019-04-04 07:23:35,646] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2166474e-11 6.8916553e-11 5.9737291e-25 1.6351455e-11 1.6033345e-12
 4.5640741e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:35,647] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6329
[2019-04-04 07:23:35,661] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.8, 24.5, 95.0, 0.0, 26.0, 25.66029353979286, 0.3819401940219726, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2820600.0000, 
sim time next is 2821200.0000, 
raw observation next is [6.733333333333333, 24.66666666666666, 91.0, 12.66666666666666, 26.0, 25.72972600116392, 0.4023823603209566, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.24666666666666662, 0.30333333333333334, 0.013996316758747691, 0.6666666666666666, 0.6441438334303268, 0.6341274534403188, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.40509897], dtype=float32), 1.9381738]. 
=============================================
[2019-04-04 07:23:38,313] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3867530e-11 1.8549594e-11 3.8625095e-25 1.7122390e-11 7.4096404e-13
 2.8800411e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:38,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0419
[2019-04-04 07:23:38,356] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.5, 25.5, 79.0, 50.66666666666667, 26.0, 26.00370750318855, 0.332426486067981, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2823000.0000, 
sim time next is 2823600.0000, 
raw observation next is [6.4, 26.0, 75.0, 63.33333333333334, 26.0, 25.58841175966843, 0.3377719914785254, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.6398891966759004, 0.26, 0.25, 0.0699815837937385, 0.6666666666666666, 0.6323676466390357, 0.6125906638261751, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.50676954], dtype=float32), 0.19936493]. 
=============================================
[2019-04-04 07:23:39,375] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.8665584e-10 5.1106830e-10 4.6757435e-24 1.5850649e-10 4.1748053e-11
 3.1611782e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:39,378] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2289
[2019-04-04 07:23:39,410] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 24.80369969194923, 0.2325661396546132, 0.0, 1.0, 52862.59878605009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2876400.0000, 
sim time next is 2877000.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 24.79532144103628, 0.2347997913178192, 0.0, 1.0, 51848.468990095], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5662767867530233, 0.5782665971059398, 0.0, 1.0, 0.24689747138140478], 
reward next is 0.7531, 
noisyNet noise sample is [array([0.6982054], dtype=float32), 0.058888774]. 
=============================================
[2019-04-04 07:23:39,445] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[81.280106]
 [81.26713 ]
 [81.24073 ]
 [81.21703 ]
 [81.21012 ]], R is [[81.21837616]
 [81.15447235]
 [81.08624268]
 [81.01415253]
 [80.94029999]].
[2019-04-04 07:23:50,992] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1009133e-10 1.5150672e-09 4.5038994e-23 7.0222751e-11 9.2213855e-11
 6.8470484e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:51,019] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4822
[2019-04-04 07:23:51,035] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.00000000000001, 0.0, 0.0, 26.0, 25.03733944521629, 0.3788874816967112, 0.0, 1.0, 43585.23188445978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2934600.0000, 
sim time next is 2935200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.02390479330337, 0.3735206406567502, 0.0, 1.0, 43516.06123076618], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5853253994419475, 0.6245068802189168, 0.0, 1.0, 0.20721933919412466], 
reward next is 0.7928, 
noisyNet noise sample is [array([-0.48471677], dtype=float32), 0.6383826]. 
=============================================
[2019-04-04 07:23:51,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8903237e-09 1.5122437e-09 1.4196241e-21 2.8978064e-10 2.6840535e-10
 8.6627477e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:51,249] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3895
[2019-04-04 07:23:51,295] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 39.33333333333334, 349.0000000000001, 26.0, 25.0758258313669, 0.3750747288986738, 0.0, 1.0, 37866.8857293471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2998200.0000, 
sim time next is 2998800.0000, 
raw observation next is [-1.0, 55.0, 31.0, 286.5, 26.0, 25.09527684634763, 0.3689477632659163, 0.0, 1.0, 20023.77097813785], 
processed observation next is [0.0, 0.7391304347826086, 0.4349030470914128, 0.55, 0.10333333333333333, 0.3165745856353591, 0.6666666666666666, 0.5912730705289692, 0.6229825877553054, 0.0, 1.0, 0.095351290372085], 
reward next is 0.9046, 
noisyNet noise sample is [array([-0.36764085], dtype=float32), 0.14744711]. 
=============================================
[2019-04-04 07:23:53,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1176626e-11 8.2856014e-11 9.3728252e-25 7.5928569e-12 3.3116181e-12
 3.4049011e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:53,069] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1012
[2019-04-04 07:23:53,092] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 50.83333333333334, 157.6666666666667, 593.0, 26.0, 26.0231780871031, 0.4632928843119255, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2803800.0000, 
sim time next is 2804400.0000, 
raw observation next is [-1.0, 50.0, 149.5, 635.5, 26.0, 26.04318363990414, 0.4737178041032116, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.5, 0.49833333333333335, 0.7022099447513812, 0.6666666666666666, 0.670265303325345, 0.6579059347010706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7045892], dtype=float32), -0.94134617]. 
=============================================
[2019-04-04 07:23:53,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.2065721e-10 1.8763833e-09 1.4539764e-22 5.2966109e-10 2.4990660e-10
 2.8168166e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:53,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2693
[2019-04-04 07:23:53,980] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 56.5, 99.0, 635.0, 26.0, 25.33916211697487, 0.3257630532219768, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3058200.0000, 
sim time next is 3058800.0000, 
raw observation next is [-4.666666666666666, 55.66666666666667, 100.1666666666667, 655.6666666666667, 26.0, 25.31958303626982, 0.3236640304274999, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.33333333333333337, 0.5566666666666668, 0.333888888888889, 0.7244935543278086, 0.6666666666666666, 0.6099652530224849, 0.6078880101425, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35035485], dtype=float32), -1.9696679]. 
=============================================
[2019-04-04 07:23:59,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2174372e-09 1.9055684e-09 6.0238264e-23 4.4929555e-10 7.4075003e-11
 1.9635476e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:23:59,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9411
[2019-04-04 07:23:59,523] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 230.0, 197.3333333333333, 26.0, 24.9963368883564, 0.3424755225193072, 0.0, 1.0, 18739.89428127718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2977800.0000, 
sim time next is 2978400.0000, 
raw observation next is [-3.0, 65.0, 243.0, 240.6666666666667, 26.0, 24.98306857364187, 0.3449768345701803, 0.0, 1.0, 33519.52984960682], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.81, 0.26593001841620634, 0.6666666666666666, 0.5819223811368225, 0.6149922781900601, 0.0, 1.0, 0.15961680880765153], 
reward next is 0.8404, 
noisyNet noise sample is [array([0.20780209], dtype=float32), -0.15896979]. 
=============================================
[2019-04-04 07:24:01,262] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8884794e-12 2.2439403e-11 9.3440870e-25 4.9991209e-12 1.0097591e-12
 2.5594324e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:01,264] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4681
[2019-04-04 07:24:01,284] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 71.0, 114.0, 817.0, 26.0, 26.63800912173961, 0.7595149352770812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3240000.0000, 
sim time next is 3240600.0000, 
raw observation next is [-2.0, 73.33333333333334, 114.3333333333333, 819.0, 26.0, 26.66906271743749, 0.7655589425682305, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.40720221606648205, 0.7333333333333334, 0.381111111111111, 0.9049723756906077, 0.6666666666666666, 0.7224218931197909, 0.7551863141894102, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17881368], dtype=float32), -0.39562228]. 
=============================================
[2019-04-04 07:24:13,342] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2607089e-11 4.7986803e-10 4.2605967e-23 2.5505719e-11 1.4736455e-11
 3.3838890e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:13,343] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4500
[2019-04-04 07:24:13,365] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2027028e-12 1.3548336e-11 6.4110726e-26 1.8419207e-12 5.5110425e-13
 7.5858678e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:13,371] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6095
[2019-04-04 07:24:13,373] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.66666666666667, 69.0, 558.6666666666667, 26.0, 26.39040021824707, 0.6772872881692384, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3341400.0000, 
sim time next is 3342000.0000, 
raw observation next is [-2.0, 47.33333333333334, 64.5, 529.8333333333333, 26.0, 26.5730525684216, 0.6803965637273612, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.47333333333333344, 0.215, 0.5854511970534069, 0.6666666666666666, 0.7144210473684666, 0.7267988545757871, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2408342], dtype=float32), 0.7843168]. 
=============================================
[2019-04-04 07:24:13,380] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.82578 ]
 [80.06649 ]
 [80.42973 ]
 [80.387146]
 [80.70256 ]], R is [[79.77895355]
 [79.98116302]
 [80.18135071]
 [80.08250427]
 [80.28167725]].
[2019-04-04 07:24:13,429] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 8.0, 116.0, 26.0, 27.24153708891369, 0.8477939859413324, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3173400.0000, 
sim time next is 3174000.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 26.57025950595279, 0.860109869587166, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6288088642659281, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7141882921627326, 0.7867032898623886, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8593508], dtype=float32), -0.48648295]. 
=============================================
[2019-04-04 07:24:13,449] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.80148 ]
 [87.43039 ]
 [88.28905 ]
 [88.98299 ]
 [89.672554]], R is [[85.94456482]
 [86.08512115]
 [86.22427368]
 [86.36203003]
 [86.49841309]].
[2019-04-04 07:24:14,115] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.2197248e-10 9.3218711e-10 8.7042045e-24 5.1871531e-11 9.7948677e-11
 9.4792978e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:14,115] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8404
[2019-04-04 07:24:14,133] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 100.0, 0.0, 0.0, 26.0, 25.28647435386438, 0.5063269993313723, 0.0, 1.0, 46900.32316017478], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3213000.0000, 
sim time next is 3213600.0000, 
raw observation next is [-1.666666666666667, 100.0, 0.0, 0.0, 26.0, 25.30027865718768, 0.506841977966908, 0.0, 1.0, 42609.47676954906], 
processed observation next is [1.0, 0.17391304347826086, 0.4164358264081256, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6083565547656399, 0.6689473259889693, 0.0, 1.0, 0.202902270331186], 
reward next is 0.7971, 
noisyNet noise sample is [array([-0.02821635], dtype=float32), -0.008683134]. 
=============================================
[2019-04-04 07:24:24,161] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6329716e-09 3.6336465e-09 2.1451617e-21 8.0827939e-10 4.0362105e-10
 1.8303881e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:24,162] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5268
[2019-04-04 07:24:24,177] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.03649961977214, 0.3349439289010403, 0.0, 1.0, 41203.24934384927], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3374400.0000, 
sim time next is 3375000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.99297466959001, 0.3250055337524751, 0.0, 1.0, 41207.04918012386], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5827478891325008, 0.6083351779174917, 0.0, 1.0, 0.19622404371487553], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.9856363], dtype=float32), -0.5223765]. 
=============================================
[2019-04-04 07:24:24,194] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.2615  ]
 [74.34287 ]
 [74.382286]
 [74.43757 ]
 [74.478806]], R is [[74.27119446]
 [74.33227539]
 [74.39281464]
 [74.45285034]
 [74.5111084 ]].
[2019-04-04 07:24:25,208] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4549671e-11 7.2694754e-12 3.9763850e-26 9.5857749e-12 9.0043966e-13
 9.7712247e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:25,208] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7023
[2019-04-04 07:24:25,253] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 90.83333333333334, 470.0, 26.0, 25.50303599465797, 0.3722898786126607, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3746400.0000, 
sim time next is 3747000.0000, 
raw observation next is [-4.0, 76.0, 92.66666666666667, 511.0, 26.0, 25.5667168178241, 0.3727021512212846, 1.0, 1.0, 18710.48831578535], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.76, 0.3088888888888889, 0.5646408839779006, 0.6666666666666666, 0.6305597348186751, 0.6242340504070949, 1.0, 1.0, 0.08909756340850167], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.3194677], dtype=float32), 0.6376418]. 
=============================================
[2019-04-04 07:24:25,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[89.44424 ]
 [88.813194]
 [87.501305]
 [86.243286]
 [84.675224]], R is [[89.43675995]
 [89.54239655]
 [89.64697266]
 [89.70593262]
 [89.80887604]].
[2019-04-04 07:24:25,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7080357e-09 7.0132034e-10 9.3424166e-24 7.5256046e-10 1.9810005e-10
 1.0472752e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:25,430] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0138
[2019-04-04 07:24:25,450] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.21945494825468, 0.3898761112925737, 0.0, 1.0, 41924.89455352424], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3475800.0000, 
sim time next is 3476400.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.26774394784553, 0.3860617360472606, 0.0, 1.0, 41906.98576001008], 
processed observation next is [1.0, 0.21739130434782608, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6056453289871276, 0.6286872453490869, 0.0, 1.0, 0.19955707504766704], 
reward next is 0.8004, 
noisyNet noise sample is [array([0.27348524], dtype=float32), -0.2673537]. 
=============================================
[2019-04-04 07:24:35,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2627367e-10 1.6004161e-09 1.0205524e-22 7.3951623e-11 1.5331654e-10
 6.8710288e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:35,531] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4586
[2019-04-04 07:24:35,545] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.19148635406023, 0.2939195913744179, 0.0, 1.0, 41991.12035032944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.14941033691812, 0.2870688184077063, 0.0, 1.0, 41622.93661162525], 
processed observation next is [1.0, 0.21739130434782608, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5957841947431767, 0.5956896061359022, 0.0, 1.0, 0.19820446005535833], 
reward next is 0.8018, 
noisyNet noise sample is [array([-3.6668358], dtype=float32), -0.7840483]. 
=============================================
[2019-04-04 07:24:38,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0822127e-09 4.3402704e-09 2.3377970e-22 8.8815988e-10 2.8301497e-10
 1.7151210e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:38,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9625
[2019-04-04 07:24:38,802] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.28968135283294, 0.3319272295585285, 0.0, 1.0, 41085.28771637758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3726000.0000, 
sim time next is 3726600.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.25412190983038, 0.3444556607470212, 0.0, 1.0, 41436.28880755237], 
processed observation next is [1.0, 0.13043478260869565, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6045101591525318, 0.6148185535823404, 0.0, 1.0, 0.19731566098834463], 
reward next is 0.8027, 
noisyNet noise sample is [array([-0.6571603], dtype=float32), -0.45465073]. 
=============================================
[2019-04-04 07:24:42,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.4180722e-12 1.1884268e-11 1.3104183e-25 1.8125103e-12 7.3668431e-13
 4.2626624e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:42,512] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0330
[2019-04-04 07:24:42,523] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 47.0, 90.16666666666666, 727.8333333333333, 26.0, 26.08764807972202, 0.6671580491962552, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [2.5, 46.5, 87.0, 717.0, 26.0, 26.39875676377999, 0.7016478586746663, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5318559556786704, 0.465, 0.29, 0.7922651933701658, 0.6666666666666666, 0.6998963969816657, 0.7338826195582221, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.39876577], dtype=float32), 0.7182806]. 
=============================================
[2019-04-04 07:24:44,122] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1620002e-11 1.1283004e-11 1.5834180e-25 7.5184769e-13 6.3199722e-13
 7.9248208e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:44,122] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8650
[2019-04-04 07:24:44,130] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 102.8333333333333, 765.1666666666667, 26.0, 26.63662402940732, 0.6651231089442323, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3768000.0000, 
sim time next is 3768600.0000, 
raw observation next is [0.0, 60.0, 99.66666666666666, 754.3333333333333, 26.0, 26.67866088023206, 0.6741252954594135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.6, 0.3322222222222222, 0.8335174953959483, 0.6666666666666666, 0.7232217400193383, 0.7247084318198045, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6519409], dtype=float32), 0.37569645]. 
=============================================
[2019-04-04 07:24:46,936] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.7095244e-11 6.4611428e-12 4.9960539e-26 1.2426813e-12 1.8075941e-12
 3.4010846e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:46,936] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1124
[2019-04-04 07:24:46,957] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 75.0, 109.1666666666667, 753.3333333333334, 26.0, 26.30274949727895, 0.5386802600285135, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3752400.0000, 
sim time next is 3753000.0000, 
raw observation next is [-3.0, 74.0, 111.0, 769.0, 26.0, 26.34102719621615, 0.5505354992878507, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.74, 0.37, 0.8497237569060774, 0.6666666666666666, 0.6950855996846791, 0.6835118330959502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5632771], dtype=float32), 1.0801599]. 
=============================================
[2019-04-04 07:24:46,963] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[89.80925 ]
 [89.87629 ]
 [89.88386 ]
 [89.942406]
 [89.99575 ]], R is [[89.81916809]
 [89.92097473]
 [90.02176666]
 [90.12155151]
 [90.22033691]].
[2019-04-04 07:24:51,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6474115e-10 1.2918183e-10 8.7157668e-24 6.8198544e-11 2.3421216e-11
 3.0171135e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:51,883] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5400
[2019-04-04 07:24:51,904] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 35.0, 182.0, 728.3333333333333, 26.0, 25.08198645385689, 0.4054934670404842, 0.0, 1.0, 18692.40110156234], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4194600.0000, 
sim time next is 4195200.0000, 
raw observation next is [2.0, 36.0, 198.0, 698.6666666666666, 26.0, 25.0904261311223, 0.4085947868372525, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.518005540166205, 0.36, 0.66, 0.7720073664825046, 0.6666666666666666, 0.5908688442601916, 0.6361982622790842, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32037157], dtype=float32), -0.53476506]. 
=============================================
[2019-04-04 07:24:54,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9762798e-10 1.1320178e-09 2.4862461e-23 3.9525680e-10 7.0941371e-11
 7.8629475e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:54,356] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4392
[2019-04-04 07:24:54,380] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 28.5, 0.0, 0.0, 26.0, 25.76516500908979, 0.4927298755243791, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4041000.0000, 
sim time next is 4041600.0000, 
raw observation next is [-3.666666666666667, 29.33333333333333, 0.0, 0.0, 26.0, 25.61510017148385, 0.4877587912873596, 1.0, 1.0, 165282.8789156084], 
processed observation next is [1.0, 0.782608695652174, 0.3610341643582641, 0.2933333333333333, 0.0, 0.0, 0.6666666666666666, 0.6345916809569875, 0.6625862637624532, 1.0, 1.0, 0.7870613281695638], 
reward next is 0.2129, 
noisyNet noise sample is [array([-1.8179132], dtype=float32), 1.9001974]. 
=============================================
[2019-04-04 07:24:54,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1085878e-10 9.6209760e-09 5.9238138e-22 1.4113497e-09 6.6618505e-10
 1.2799128e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:54,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6554
[2019-04-04 07:24:55,021] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 29.0, 0.0, 0.0, 26.0, 25.6804760927208, 0.5256703533002691, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4050000.0000, 
sim time next is 4050600.0000, 
raw observation next is [-4.166666666666667, 29.33333333333334, 0.0, 0.0, 26.0, 25.68633156435184, 0.5134799098165591, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3471837488457987, 0.2933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6405276303626533, 0.6711599699388531, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08611921], dtype=float32), -0.21142115]. 
=============================================
[2019-04-04 07:24:55,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.55117419e-09 1.43911825e-08 4.09057090e-21 2.49172794e-09
 2.07095008e-09 1.04001408e-11 1.00000000e+00], sum to 1.0000
[2019-04-04 07:24:55,927] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0601
[2019-04-04 07:24:55,950] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.18714140761337, 0.3354875598597832, 0.0, 1.0, 40816.78917764825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4066800.0000, 
sim time next is 4067400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.19470423438424, 0.3274364749790907, 0.0, 1.0, 40834.21605170864], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5995586861986867, 0.6091454916596969, 0.0, 1.0, 0.19444864786527924], 
reward next is 0.8056, 
noisyNet noise sample is [array([-0.42270595], dtype=float32), -0.90127903]. 
=============================================
[2019-04-04 07:24:57,905] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9313580e-09 8.7591590e-10 1.0905447e-23 5.4450999e-10 1.6454174e-10
 6.0588177e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:57,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4221
[2019-04-04 07:24:57,940] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.1, 66.5, 0.0, 0.0, 26.0, 25.13663145339499, 0.3109951346145273, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4300200.0000, 
sim time next is 4300800.0000, 
raw observation next is [6.066666666666666, 67.33333333333334, 0.0, 0.0, 26.0, 25.07424969009377, 0.3000512011197985, 0.0, 1.0, 34506.08166232172], 
processed observation next is [0.0, 0.782608695652174, 0.6306555863342568, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5895208075078141, 0.6000170670399328, 0.0, 1.0, 0.16431467458248436], 
reward next is 0.8357, 
noisyNet noise sample is [array([0.6144165], dtype=float32), -0.90342265]. 
=============================================
[2019-04-04 07:24:59,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.1648245e-10 1.7861289e-09 1.6768012e-22 4.3272991e-10 2.1138058e-10
 1.1455696e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:24:59,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3674
[2019-04-04 07:24:59,719] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.19289308691823, 0.3766089273970907, 0.0, 1.0, 40817.88083280178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4059000.0000, 
sim time next is 4059600.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.17201008482544, 0.3697923687557649, 0.0, 1.0, 40756.54499651513], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.37, 0.0, 0.0, 0.6666666666666666, 0.5976675070687868, 0.6232641229185883, 0.0, 1.0, 0.19407878569769108], 
reward next is 0.8059, 
noisyNet noise sample is [array([-0.33640128], dtype=float32), -0.5837299]. 
=============================================
[2019-04-04 07:25:00,186] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9295675e-10 3.7879100e-10 5.6738813e-24 8.5906060e-11 8.3621735e-11
 1.2444834e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:00,186] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3093
[2019-04-04 07:25:00,196] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 40.66666666666667, 0.0, 0.0, 26.0, 25.35926998264121, 0.4717763838483245, 0.0, 1.0, 196646.6135831886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4134000.0000, 
sim time next is 4134600.0000, 
raw observation next is [1.0, 39.5, 0.0, 0.0, 26.0, 25.32801196048116, 0.4949055331062746, 0.0, 1.0, 198133.9827187007], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.395, 0.0, 0.0, 0.6666666666666666, 0.6106676633734299, 0.6649685110354249, 0.0, 1.0, 0.9434951558033366], 
reward next is 0.0565, 
noisyNet noise sample is [array([1.7747762], dtype=float32), 0.4672729]. 
=============================================
[2019-04-04 07:25:01,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7594966e-12 4.2505231e-12 1.2966585e-26 2.1042098e-12 2.7395729e-13
 1.7095532e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:01,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5192
[2019-04-04 07:25:02,004] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 36.0, 105.6666666666667, 694.0, 26.0, 26.30474245859104, 0.5265798540443202, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4095600.0000, 
sim time next is 4096200.0000, 
raw observation next is [-2.166666666666667, 35.5, 107.3333333333333, 709.0, 26.0, 26.35210976064098, 0.5425145574333192, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4025854108956602, 0.355, 0.3577777777777777, 0.7834254143646409, 0.6666666666666666, 0.6960091467200819, 0.6808381858111064, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9645043], dtype=float32), 0.58812255]. 
=============================================
[2019-04-04 07:25:05,811] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8892990e-11 4.8961122e-12 6.2688434e-27 1.2205672e-12 3.4427630e-12
 5.5940660e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:05,812] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7584
[2019-04-04 07:25:05,823] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.83333333333334, 165.3333333333333, 760.3333333333333, 26.0, 25.35021847950327, 0.4362691250757207, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4281000.0000, 
sim time next is 4281600.0000, 
raw observation next is [7.0, 53.66666666666667, 176.6666666666667, 738.6666666666666, 26.0, 25.35356083532886, 0.4380183055709745, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5366666666666667, 0.588888888888889, 0.8162062615101289, 0.6666666666666666, 0.612796736277405, 0.6460061018569915, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6240222], dtype=float32), -0.19112393]. 
=============================================
[2019-04-04 07:25:12,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.7035936e-14 1.6594686e-13 6.2240131e-30 5.8987964e-14 2.0387134e-14
 3.0490235e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:12,622] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2025
[2019-04-04 07:25:12,636] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 47.0, 0.0, 0.0, 26.0, 27.94701596778084, 1.026292969253263, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4386600.0000, 
sim time next is 4387200.0000, 
raw observation next is [12.13333333333333, 48.0, 0.0, 0.0, 26.0, 27.86333960471612, 1.033211827629049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7987072945521699, 0.48, 0.0, 0.0, 0.6666666666666666, 0.8219449670596767, 0.8444039425430163, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.97540426], dtype=float32), 0.7203264]. 
=============================================
[2019-04-04 07:25:15,807] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1457945e-11 7.7255598e-12 8.8339171e-26 2.2237817e-12 7.2549968e-13
 1.1659702e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:15,807] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-04 07:25:15,817] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 120.5, 834.5, 26.0, 25.21134794371913, 0.4030611469022774, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4276800.0000, 
sim time next is 4277400.0000, 
raw observation next is [7.0, 52.0, 120.3333333333333, 838.6666666666667, 26.0, 25.23102554764804, 0.4070431874518772, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.401111111111111, 0.9267034990791898, 0.6666666666666666, 0.6025854623040034, 0.6356810624839591, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7620688], dtype=float32), -0.251226]. 
=============================================
[2019-04-04 07:25:17,541] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.50644286e-09 1.64478442e-09 8.97157308e-23 2.19349136e-10
 1.07148214e-10 5.64074286e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 07:25:17,541] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9042
[2019-04-04 07:25:17,617] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 71.33333333333334, 0.0, 0.0, 26.0, 25.22246700410926, 0.3829309352345611, 0.0, 1.0, 40436.7964260746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.23561360170128, 0.4292817925176001, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4367497691597415, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6029678001417734, 0.6430939308392, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5561437], dtype=float32), -0.36377427]. 
=============================================
[2019-04-04 07:25:19,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7717052e-12 9.0800398e-13 8.8965006e-29 3.7988514e-13 1.6602601e-13
 2.2931731e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:19,125] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8789
[2019-04-04 07:25:19,167] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.199999999999999, 62.66666666666667, 94.5, 522.0, 26.0, 26.09040411112895, 0.5355582764112072, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4351200.0000, 
sim time next is 4351800.0000, 
raw observation next is [5.75, 59.83333333333333, 97.0, 553.0, 26.0, 26.27802616607842, 0.5608931222683621, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6218836565096953, 0.5983333333333333, 0.3233333333333333, 0.611049723756906, 0.6666666666666666, 0.6898355138398683, 0.6869643740894541, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2191432], dtype=float32), -0.27630025]. 
=============================================
[2019-04-04 07:25:31,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5600051e-09 4.6629722e-09 3.1440144e-22 9.1414304e-10 3.1896305e-10
 2.9714382e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:31,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3977
[2019-04-04 07:25:31,090] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 69.66666666666667, 0.0, 0.0, 26.0, 25.30788319704585, 0.396023787840085, 0.0, 1.0, 50107.02255247395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4594800.0000, 
sim time next is 4595400.0000, 
raw observation next is [-1.75, 70.0, 0.0, 0.0, 26.0, 25.26916054921151, 0.3992736704193492, 0.0, 1.0, 40973.00385203881], 
processed observation next is [1.0, 0.17391304347826086, 0.4141274238227147, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6057633791009591, 0.6330912234731164, 0.0, 1.0, 0.19510954215256576], 
reward next is 0.8049, 
noisyNet noise sample is [array([-0.02857933], dtype=float32), 1.4951262]. 
=============================================
[2019-04-04 07:25:35,704] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.2138283e-10 2.6671885e-09 9.1273573e-24 2.4558300e-10 1.3909530e-10
 1.4464511e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:35,705] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6978
[2019-04-04 07:25:35,722] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 53.66666666666667, 0.0, 0.0, 26.0, 25.97074917543661, 0.6590977798351438, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4656000.0000, 
sim time next is 4656600.0000, 
raw observation next is [2.0, 54.5, 0.0, 0.0, 26.0, 25.98758376127605, 0.653546690260176, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.545, 0.0, 0.0, 0.6666666666666666, 0.6656319801063374, 0.717848896753392, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3047407], dtype=float32), 0.68784845]. 
=============================================
[2019-04-04 07:25:37,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2898196e-09 2.3343993e-09 5.3731374e-22 5.0095589e-10 3.0685848e-10
 3.3631140e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:37,259] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0362
[2019-04-04 07:25:37,280] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 62.5, 0.0, 0.0, 26.0, 24.57684250125638, 0.1767932969518152, 0.0, 1.0, 39480.93899097785], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4861800.0000, 
sim time next is 4862400.0000, 
raw observation next is [-3.666666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 24.561418362849, 0.1711930859669913, 0.0, 1.0, 39478.78124148501], 
processed observation next is [0.0, 0.2608695652173913, 0.3610341643582641, 0.6333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5467848635707501, 0.5570643619889971, 0.0, 1.0, 0.18799419638802387], 
reward next is 0.8120, 
noisyNet noise sample is [array([-0.80509543], dtype=float32), -0.33925053]. 
=============================================
[2019-04-04 07:25:37,882] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0435022e-10 2.4879478e-09 4.6825284e-23 1.4325277e-10 1.0149098e-10
 1.0670248e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:37,884] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1674
[2019-04-04 07:25:37,914] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 25.44797411807832, 0.5084998112620461, 0.0, 1.0, 59773.83512763988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4743600.0000, 
sim time next is 4744200.0000, 
raw observation next is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 25.40071387069114, 0.5084918386924403, 0.0, 1.0, 74879.18414594691], 
processed observation next is [1.0, 0.9130434782608695, 0.3841181902123731, 0.8416666666666666, 0.0, 0.0, 0.6666666666666666, 0.6167261558909282, 0.6694972795641467, 0.0, 1.0, 0.35656754355212816], 
reward next is 0.6434, 
noisyNet noise sample is [array([0.6620053], dtype=float32), -0.76552314]. 
=============================================
[2019-04-04 07:25:39,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4741911e-09 6.5871264e-10 1.1498960e-22 5.1791482e-10 2.5541416e-10
 4.2857809e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:39,986] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8432
[2019-04-04 07:25:40,039] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 0.0, 0.0, 26.0, 24.97307284129002, 0.2913881259848979, 0.0, 1.0, 31997.31200273814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4906800.0000, 
sim time next is 4907400.0000, 
raw observation next is [1.0, 45.83333333333334, 0.0, 0.0, 26.0, 24.97247002444077, 0.3079689306922782, 0.0, 1.0, 29035.4195853215], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4583333333333334, 0.0, 0.0, 0.6666666666666666, 0.5810391687033976, 0.6026563102307594, 0.0, 1.0, 0.13826390278724524], 
reward next is 0.8617, 
noisyNet noise sample is [array([-1.1548772], dtype=float32), -0.5585797]. 
=============================================
[2019-04-04 07:25:41,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:25:41,238] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:25:41,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run15
[2019-04-04 07:25:45,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.3417077e-09 4.2093831e-09 6.5086190e-22 2.0273028e-09 6.8519712e-10
 8.1136584e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:25:45,728] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6625
[2019-04-04 07:25:45,755] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 24.78060795889647, 0.2227844876729617, 0.0, 1.0, 39435.71227414166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4857000.0000, 
sim time next is 4857600.0000, 
raw observation next is [-3.666666666666667, 67.33333333333334, 0.0, 0.0, 26.0, 24.7790003560034, 0.217262363353719, 0.0, 1.0, 39495.63758729546], 
processed observation next is [0.0, 0.21739130434782608, 0.3610341643582641, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5649166963336167, 0.572420787784573, 0.0, 1.0, 0.18807446470140696], 
reward next is 0.8119, 
noisyNet noise sample is [array([1.916866], dtype=float32), 0.7594496]. 
=============================================
[2019-04-04 07:25:47,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:25:47,394] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:25:47,398] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run15
[2019-04-04 07:25:48,217] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7748678e-07 1.9096328e-08 1.6781021e-19 7.0370820e-09 4.2611297e-09
 2.4925279e-11 9.9999964e-01], sum to 1.0000
[2019-04-04 07:25:48,218] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0489
[2019-04-04 07:25:48,241] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 20.45050668558164, -0.760783523060803, 0.0, 1.0, 43011.68768994651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 9000.0000, 
sim time next is 9600.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.49340673354557, -0.7524726760767653, 0.0, 1.0, 42776.37397270938], 
processed observation next is [0.0, 0.08695652173913043, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.20778389446213077, 0.24917577464107823, 0.0, 1.0, 0.2036970189176637], 
reward next is 0.7963, 
noisyNet noise sample is [array([-0.5602904], dtype=float32), -1.4194343]. 
=============================================
[2019-04-04 07:25:49,338] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 07:25:49,338] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:25:49,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:25:49,339] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:25:49,340] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:25:49,341] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run20
[2019-04-04 07:25:49,383] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:25:49,384] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:25:49,403] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run20
[2019-04-04 07:25:49,440] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run20
[2019-04-04 07:25:50,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:25:50,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:25:50,248] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run15
[2019-04-04 07:28:29,953] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.22830497], dtype=float32), -0.076950714]
[2019-04-04 07:28:29,953] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.1, 47.0, 0.0, 0.0, 26.0, 25.32311245479088, 0.4395586152370323, 1.0, 1.0, 24674.57612928067]
[2019-04-04 07:28:29,953] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:28:29,954] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [6.1230215e-11 7.1776099e-11 4.9794084e-25 2.1308264e-11 3.7350813e-12
 3.6645047e-14 1.0000000e+00], sampled 0.03262929887257926
[2019-04-04 07:28:56,121] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 07:29:28,986] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 07:29:36,676] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 07:29:37,738] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1900000, evaluation results [1900000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 07:29:41,123] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.1727552e-10 2.8671177e-10 3.0129617e-24 8.5319994e-11 3.7629781e-11
 2.9183134e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:29:41,124] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3673
[2019-04-04 07:29:41,218] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 29.5, 0.0, 26.0, 22.74370010213037, -0.2202450098611307, 0.0, 1.0, 60569.43646857025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 32400.0000, 
sim time next is 33000.0000, 
raw observation next is [7.7, 93.0, 32.33333333333334, 0.0, 26.0, 22.92809495101843, -0.1963248254049741, 0.0, 1.0, 59374.37423446235], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.1077777777777778, 0.0, 0.6666666666666666, 0.41067457925153583, 0.4345583915316753, 0.0, 1.0, 0.2827351154022017], 
reward next is 0.7173, 
noisyNet noise sample is [array([1.0666493], dtype=float32), 1.6471334]. 
=============================================
[2019-04-04 07:29:41,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[83.52465 ]
 [83.20541 ]
 [82.88974 ]
 [82.477715]
 [81.952484]], R is [[83.71378326]
 [83.58821869]
 [83.4473877 ]
 [83.25861359]
 [82.9493103 ]].
[2019-04-04 07:29:41,288] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:29:41,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:29:41,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run15
[2019-04-04 07:29:41,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:29:41,608] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:29:41,611] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run15
[2019-04-04 07:29:43,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:29:43,365] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:29:43,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run15
[2019-04-04 07:29:43,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:29:43,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:29:43,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run15
[2019-04-04 07:29:49,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:29:49,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:29:49,686] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run15
[2019-04-04 07:29:51,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:29:51,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:29:51,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run15
[2019-04-04 07:29:57,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:29:57,706] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:29:57,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run15
[2019-04-04 07:30:02,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:30:02,321] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:02,324] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run15
[2019-04-04 07:30:02,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:30:02,884] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:02,894] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run15
[2019-04-04 07:30:02,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:30:02,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:02,978] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run15
[2019-04-04 07:30:05,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:30:05,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:05,084] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run15
[2019-04-04 07:30:08,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:30:08,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:08,592] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run15
[2019-04-04 07:30:14,762] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4274238e-09 2.2568709e-09 3.9866127e-23 6.0666672e-10 1.3535015e-10
 2.2954752e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:30:14,762] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4576
[2019-04-04 07:30:14,792] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.02428530240748, -0.6260026937834978, 0.0, 1.0, 40681.43349759776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 18000.0000, 
sim time next is 18600.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.04394758394561, -0.6103791906739388, 0.0, 1.0, 40601.89671485911], 
processed observation next is [0.0, 0.21739130434782608, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.25366229866213413, 0.29654026977535375, 0.0, 1.0, 0.1933423653088529], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.6531288], dtype=float32), 0.13596343]. 
=============================================
[2019-04-04 07:30:15,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:30:15,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:30:15,676] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run15
[2019-04-04 07:30:18,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.9988627e-09 6.8491914e-09 1.7173793e-20 1.8285800e-09 1.6259849e-09
 3.5319764e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:30:18,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8418
[2019-04-04 07:30:18,821] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.54692993417455, -0.2980968945670965, 0.0, 1.0, 45023.98128446237], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 198600.0000, 
sim time next is 199200.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.4828627300495, -0.2224464604171648, 1.0, 1.0, 202343.6168360426], 
processed observation next is [1.0, 0.30434782608695654, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.37357189417079173, 0.42585117986094506, 1.0, 1.0, 0.9635410325525838], 
reward next is 0.0365, 
noisyNet noise sample is [array([1.4830221], dtype=float32), -2.0967858]. 
=============================================
[2019-04-04 07:30:22,945] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4079934e-11 8.6696700e-10 3.7644537e-24 5.6162290e-11 1.3323312e-11
 4.2980312e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:30:22,946] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5745
[2019-04-04 07:30:23,024] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.3, 67.33333333333334, 0.0, 0.0, 26.0, 24.98629882654301, 0.3043687129411883, 1.0, 1.0, 110398.8680526219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.98698591364652, 0.3143169160407694, 0.0, 1.0, 83795.70213747746], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5822488261372101, 0.6047723053469231, 0.0, 1.0, 0.399027153035607], 
reward next is 0.6010, 
noisyNet noise sample is [array([-0.48285896], dtype=float32), 0.7452475]. 
=============================================
[2019-04-04 07:30:30,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6053744e-08 4.8710650e-08 5.2653815e-19 2.4760556e-09 8.4836982e-09
 6.2473367e-11 9.9999988e-01], sum to 1.0000
[2019-04-04 07:30:30,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3458
[2019-04-04 07:30:30,710] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 69.0, 0.0, 0.0, 26.0, 22.84816844898067, -0.2343334286594067, 0.0, 1.0, 49193.72667434888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 355800.0000, 
sim time next is 356400.0000, 
raw observation next is [-15.0, 69.0, 0.0, 0.0, 26.0, 22.74698748185054, -0.2488739774801119, 0.0, 1.0, 49272.93254944009], 
processed observation next is [1.0, 0.13043478260869565, 0.04709141274238226, 0.69, 0.0, 0.0, 0.6666666666666666, 0.3955822901542116, 0.4170420075066294, 0.0, 1.0, 0.23463301214019092], 
reward next is 0.7654, 
noisyNet noise sample is [array([1.3385365], dtype=float32), 0.55918694]. 
=============================================
[2019-04-04 07:30:32,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.3865977e-11 3.8663275e-10 5.2832577e-23 9.1909376e-11 2.0071360e-11
 2.6668473e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:30:32,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3431
[2019-04-04 07:30:32,461] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 62.0, 75.5, 55.16666666666666, 26.0, 25.23193833972149, 0.3567404184145255, 1.0, 1.0, 63991.03643899944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 141600.0000, 
sim time next is 142200.0000, 
raw observation next is [-6.7, 62.5, 61.0, 45.0, 26.0, 24.72922986193058, 0.3497290367595245, 1.0, 1.0, 199727.7743145048], 
processed observation next is [1.0, 0.6521739130434783, 0.2770083102493075, 0.625, 0.20333333333333334, 0.049723756906077346, 0.6666666666666666, 0.5607691551608817, 0.6165763455865082, 1.0, 1.0, 0.9510846395928799], 
reward next is 0.0489, 
noisyNet noise sample is [array([-0.8027677], dtype=float32), 0.81917953]. 
=============================================
[2019-04-04 07:30:36,278] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.1657380e-11 1.8345632e-10 1.6213589e-23 8.9073592e-12 4.8185414e-12
 3.4206426e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:30:36,278] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6232
[2019-04-04 07:30:36,366] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 43.0, 82.0, 623.0, 26.0, 26.34605469163317, 0.4093901572061058, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 311400.0000, 
sim time next is 312000.0000, 
raw observation next is [-9.5, 42.66666666666667, 80.0, 598.6666666666667, 26.0, 25.74361116269202, 0.4019455470529199, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4266666666666667, 0.26666666666666666, 0.661510128913444, 0.6666666666666666, 0.645300930224335, 0.63398184901764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.3372533], dtype=float32), 0.32581702]. 
=============================================
[2019-04-04 07:30:36,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.98007 ]
 [81.284515]
 [81.460144]
 [81.71347 ]
 [81.95958 ]], R is [[80.52766418]
 [80.72238922]
 [80.91516876]
 [81.10601807]
 [81.29496002]].
[2019-04-04 07:30:54,400] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0744542e-09 1.2445310e-09 1.9961942e-22 1.3865559e-10 1.4181545e-10
 1.8365561e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:30:54,426] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2414
[2019-04-04 07:30:54,553] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 73.0, 0.0, 0.0, 26.0, 24.09648718493674, 0.08487438865873446, 0.0, 1.0, 44487.24752742679], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 261000.0000, 
sim time next is 261600.0000, 
raw observation next is [-5.966666666666667, 71.0, 0.0, 0.0, 26.0, 24.12413343229557, 0.08545579177673328, 0.0, 1.0, 44565.29230686714], 
processed observation next is [1.0, 0.0, 0.2973222530009234, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5103444526912974, 0.5284852639255778, 0.0, 1.0, 0.21221567765174829], 
reward next is 0.7878, 
noisyNet noise sample is [array([0.03384237], dtype=float32), 0.36389023]. 
=============================================
[2019-04-04 07:31:03,867] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.09591496e-10 9.51348722e-11 3.85825361e-25 9.31539706e-11
 1.98013329e-12 3.00426594e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 07:31:03,867] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3856
[2019-04-04 07:31:03,954] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 31.5, 119.0, 0.0, 26.0, 24.95089509436038, 0.2028666009498425, 1.0, 1.0, 46392.28670483363], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 480600.0000, 
sim time next is 481200.0000, 
raw observation next is [-0.8, 32.66666666666666, 114.8333333333333, 0.0, 26.0, 25.27990577364826, 0.2358166122246518, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4404432132963989, 0.32666666666666655, 0.38277777777777766, 0.0, 0.6666666666666666, 0.6066588144706883, 0.5786055374082173, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0231414], dtype=float32), 0.30828214]. 
=============================================
[2019-04-04 07:31:15,363] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2587833e-09 9.7531192e-09 1.7513433e-22 2.0935072e-10 2.2499157e-10
 1.0757547e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:15,363] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1727
[2019-04-04 07:31:15,424] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 64.5, 0.0, 0.0, 26.0, 24.7230253266854, 0.2425597041003881, 0.0, 1.0, 43307.27906572285], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 771000.0000, 
sim time next is 771600.0000, 
raw observation next is [-6.366666666666667, 65.0, 0.0, 0.0, 26.0, 24.68861631029452, 0.2354355156752032, 0.0, 1.0, 43195.55863670514], 
processed observation next is [1.0, 0.9565217391304348, 0.28624192059095105, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5573846925245434, 0.5784785052250677, 0.0, 1.0, 0.20569313636526254], 
reward next is 0.7943, 
noisyNet noise sample is [array([-1.2430712], dtype=float32), 1.1579199]. 
=============================================
[2019-04-04 07:31:17,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8060062e-11 7.5941788e-11 1.5061778e-25 1.1673360e-11 2.1419986e-12
 3.1617673e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:17,294] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0459
[2019-04-04 07:31:17,357] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 54.5, 0.0, 0.0, 26.0, 25.37174397532249, 0.3351223662542246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 757800.0000, 
sim time next is 758400.0000, 
raw observation next is [-3.899999999999999, 54.0, 0.0, 0.0, 26.0, 25.16721004485214, 0.3117636008116184, 1.0, 1.0, 18721.65987551382], 
processed observation next is [1.0, 0.782608695652174, 0.35457063711911363, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5972675037376783, 0.6039212002705395, 1.0, 1.0, 0.08915076131197057], 
reward next is 0.9108, 
noisyNet noise sample is [array([-2.7774167], dtype=float32), 0.14036892]. 
=============================================
[2019-04-04 07:31:19,927] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4658454e-09 2.2777933e-09 9.4205094e-23 5.8902305e-10 1.7918503e-10
 5.5845758e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:19,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3497
[2019-04-04 07:31:19,958] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.16666666666667, 6.333333333333332, 0.0, 26.0, 23.48042435014039, -0.07486640598425524, 0.0, 1.0, 43965.90021468028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 633000.0000, 
sim time next is 633600.0000, 
raw observation next is [-4.5, 79.0, 9.5, 0.0, 26.0, 23.45535208591805, -0.07952867824251297, 0.0, 1.0, 43972.51526968516], 
processed observation next is [0.0, 0.34782608695652173, 0.3379501385041552, 0.79, 0.03166666666666667, 0.0, 0.6666666666666666, 0.4546126738265042, 0.47349044058582895, 0.0, 1.0, 0.20939292985564362], 
reward next is 0.7906, 
noisyNet noise sample is [array([-0.13495314], dtype=float32), 0.686848]. 
=============================================
[2019-04-04 07:31:22,112] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.42745987e-11 4.72385776e-11 6.28055123e-26 1.17182436e-11
 3.57727910e-12 2.11215204e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:31:22,113] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9011
[2019-04-04 07:31:22,166] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 75.0, 36.83333333333333, 0.0, 26.0, 25.75384630888435, 0.3308070796805104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 808800.0000, 
sim time next is 809400.0000, 
raw observation next is [-6.283333333333333, 75.0, 41.66666666666666, 0.0, 26.0, 25.87169287220818, 0.3152049878042367, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.288550323176362, 0.75, 0.13888888888888887, 0.0, 0.6666666666666666, 0.6559744060173482, 0.6050683292680789, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3319868], dtype=float32), 2.3737488]. 
=============================================
[2019-04-04 07:31:23,557] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1635246e-10 4.0999906e-10 3.4626482e-25 1.8015442e-11 1.3775426e-11
 4.2716336e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:23,558] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3519
[2019-04-04 07:31:23,583] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.15, 83.5, 0.0, 0.0, 26.0, 24.63665028590652, 0.1964241464142303, 0.0, 1.0, 40302.14530255074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 534600.0000, 
sim time next is 535200.0000, 
raw observation next is [1.966666666666667, 84.0, 0.0, 0.0, 26.0, 24.61195633232596, 0.1915809887686618, 0.0, 1.0, 40381.73522624399], 
processed observation next is [0.0, 0.17391304347826086, 0.5170821791320407, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5509963610271633, 0.5638603295895539, 0.0, 1.0, 0.19229397726782851], 
reward next is 0.8077, 
noisyNet noise sample is [array([-1.7272975], dtype=float32), -0.42996427]. 
=============================================
[2019-04-04 07:31:25,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.8615304e-10 1.8003712e-09 1.5521117e-24 9.6754937e-11 7.2599045e-11
 2.1317267e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:25,492] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7690
[2019-04-04 07:31:25,504] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 72.0, 0.0, 0.0, 26.0, 24.60651028923008, 0.1814875982419966, 0.0, 1.0, 38983.51591765817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 882600.0000, 
sim time next is 883200.0000, 
raw observation next is [-0.4, 72.0, 0.0, 0.0, 26.0, 24.65130646676486, 0.1837869309338891, 0.0, 1.0, 38936.79432229135], 
processed observation next is [1.0, 0.21739130434782608, 0.45152354570637127, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5542755388970718, 0.5612623103112964, 0.0, 1.0, 0.1854133062966255], 
reward next is 0.8146, 
noisyNet noise sample is [array([-0.4412011], dtype=float32), -0.016128642]. 
=============================================
[2019-04-04 07:31:29,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5735455e-10 6.2816807e-10 3.3724013e-24 7.1842116e-11 1.7941716e-11
 2.6001158e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:29,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7746
[2019-04-04 07:31:29,754] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.24286685675699, 0.03671712284704738, 0.0, 1.0, 41705.4822498903], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 712200.0000, 
sim time next is 712800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.27509109812181, 0.04171043175588392, 0.0, 1.0, 41737.62339154108], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5229242581768174, 0.5139034772519613, 0.0, 1.0, 0.19875058757876704], 
reward next is 0.8012, 
noisyNet noise sample is [array([-0.5211152], dtype=float32), -1.9607089]. 
=============================================
[2019-04-04 07:31:42,581] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2152389e-11 3.8329829e-11 5.7953691e-26 8.1095001e-12 1.5514582e-12
 1.4856772e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:42,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6100
[2019-04-04 07:31:42,623] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 76.33333333333333, 97.66666666666666, 0.0, 26.0, 25.43856129370378, 0.2890968198612372, 1.0, 1.0, 27884.98156795038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 823200.0000, 
sim time next is 823800.0000, 
raw observation next is [-4.5, 77.66666666666667, 96.33333333333334, 0.0, 26.0, 25.40727646492848, 0.2860388446890448, 1.0, 1.0, 27950.83812545352], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7766666666666667, 0.3211111111111111, 0.0, 0.6666666666666666, 0.6172730387440399, 0.595346281563015, 1.0, 1.0, 0.13309922916882627], 
reward next is 0.8669, 
noisyNet noise sample is [array([1.2988636], dtype=float32), 0.048261058]. 
=============================================
[2019-04-04 07:31:44,006] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2538633e-10 3.6818291e-11 2.7422420e-26 1.9399250e-11 9.3366669e-12
 9.2913517e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:44,006] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7735
[2019-04-04 07:31:44,010] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.2, 67.0, 106.5, 0.0, 26.0, 25.53199208070804, 0.5501607789542569, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1159200.0000, 
sim time next is 1159800.0000, 
raw observation next is [17.38333333333333, 66.66666666666667, 114.3333333333333, 0.0, 26.0, 25.45547887523529, 0.5388838557542975, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.9441366574330563, 0.6666666666666667, 0.381111111111111, 0.0, 0.6666666666666666, 0.6212899062696075, 0.6796279519180991, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5493538], dtype=float32), 0.38483918]. 
=============================================
[2019-04-04 07:31:59,570] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5266599e-12 2.3966438e-11 2.1888833e-27 2.2406332e-12 1.2014126e-12
 3.8765649e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:31:59,571] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8269
[2019-04-04 07:31:59,619] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.35, 100.0, 18.0, 0.0, 26.0, 25.56787339404686, 0.4768331306539985, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1499400.0000, 
sim time next is 1500000.0000, 
raw observation next is [1.433333333333333, 100.0, 22.83333333333333, 0.0, 26.0, 25.7263029371947, 0.4824349009428717, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.502308402585411, 1.0, 0.0761111111111111, 0.0, 0.6666666666666666, 0.6438585780995583, 0.6608116336476239, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75039905], dtype=float32), -0.03037626]. 
=============================================
[2019-04-04 07:31:59,649] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[94.19664 ]
 [94.348694]
 [94.688126]
 [94.115105]
 [92.94785 ]], R is [[93.95967865]
 [94.02008057]
 [94.07987976]
 [94.13908386]
 [94.19769287]].
[2019-04-04 07:32:02,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8631463e-12 2.8502190e-11 9.1704501e-26 1.3514725e-12 5.3926796e-13
 9.6900823e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:02,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6296
[2019-04-04 07:32:02,483] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 107.1666666666667, 0.0, 26.0, 25.67275469348057, 0.5326804542606867, 1.0, 1.0, 42508.27961054981], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1344000.0000, 
sim time next is 1344600.0000, 
raw observation next is [1.1, 92.0, 106.0, 0.0, 26.0, 25.71353115963649, 0.5298969784318149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.35333333333333333, 0.0, 0.6666666666666666, 0.6427942633030407, 0.6766323261439383, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6352115], dtype=float32), -0.5718076]. 
=============================================
[2019-04-04 07:32:05,721] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.0995072e-10 2.3801405e-10 1.5712464e-24 4.3926508e-11 6.8706950e-11
 2.9418627e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:05,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9987
[2019-04-04 07:32:05,737] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.42212111991987, 0.3356611343468749, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1203600.0000, 
sim time next is 1204200.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.38783406561935, 0.3296011041979662, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5323195054682793, 0.6098670347326554, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2762028], dtype=float32), -1.560404]. 
=============================================
[2019-04-04 07:32:07,859] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.1507634e-11 3.0653365e-11 1.0827955e-26 7.2579486e-12 4.5028083e-12
 2.1037249e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:07,860] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3781
[2019-04-04 07:32:07,875] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 96.0, 0.0, 26.0, 24.84909131709885, 0.4571268188895259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1257600.0000, 
sim time next is 1258200.0000, 
raw observation next is [13.8, 100.0, 95.0, 0.0, 26.0, 24.817401372178, 0.4563202398397088, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.31666666666666665, 0.0, 0.6666666666666666, 0.5681167810148334, 0.6521067466132363, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.88293004], dtype=float32), -1.3532878]. 
=============================================
[2019-04-04 07:32:11,421] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0270764e-11 2.2052528e-11 1.4876085e-25 2.7248267e-12 7.7897725e-13
 2.8135599e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:11,421] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1776
[2019-04-04 07:32:11,440] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 81.0, 0.0, 26.0, 25.8704933764913, 0.487915727223342, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1422000.0000, 
sim time next is 1422600.0000, 
raw observation next is [0.0, 95.0, 84.0, 0.0, 26.0, 25.81351680817056, 0.4775429834776723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.28, 0.0, 0.6666666666666666, 0.6511264006808801, 0.6591809944925574, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5918002], dtype=float32), -0.9213554]. 
=============================================
[2019-04-04 07:32:26,184] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2552925e-12 1.3308353e-11 5.5002735e-26 2.2767344e-12 8.9170267e-13
 2.0429915e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:26,185] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6126
[2019-04-04 07:32:26,208] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 63.0, 80.0, 682.0, 26.0, 26.63110348157565, 0.7308035579246727, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1519200.0000, 
sim time next is 1519800.0000, 
raw observation next is [10.26666666666667, 61.16666666666666, 78.33333333333334, 675.6666666666667, 26.0, 26.74008258594818, 0.7442441529906914, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.746999076638966, 0.6116666666666666, 0.2611111111111111, 0.7465930018416207, 0.6666666666666666, 0.7283402154956816, 0.7480813843302304, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06890621], dtype=float32), -0.09127824]. 
=============================================
[2019-04-04 07:32:26,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.8141457e-10 1.1020678e-09 2.3839582e-24 1.6238363e-10 7.9455567e-11
 2.7726489e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:26,350] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2753
[2019-04-04 07:32:26,363] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.1, 82.66666666666667, 0.0, 0.0, 26.0, 25.67220544148928, 0.5864123950220548, 0.0, 1.0, 70871.9775822074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1637400.0000, 
sim time next is 1638000.0000, 
raw observation next is [7.2, 82.0, 0.0, 0.0, 26.0, 25.5898401825248, 0.583613518286071, 0.0, 1.0, 93418.84051648348], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6324866818770666, 0.6945378394286904, 0.0, 1.0, 0.4448516215070642], 
reward next is 0.5551, 
noisyNet noise sample is [array([0.22416012], dtype=float32), -1.8263181]. 
=============================================
[2019-04-04 07:32:26,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[84.04101 ]
 [83.837685]
 [84.036095]
 [84.10002 ]
 [84.177734]], R is [[84.20546722]
 [84.02592468]
 [84.18566895]
 [84.34381104]
 [84.50037384]].
[2019-04-04 07:32:35,458] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.6456030e-10 8.9811686e-10 6.3445939e-22 9.2678303e-11 1.3451271e-10
 1.7645352e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:35,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6368
[2019-04-04 07:32:35,526] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 73.0, 180.6666666666667, 69.33333333333334, 26.0, 25.03441142584728, 0.2807187251872977, 0.0, 1.0, 33619.74796497702], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1865400.0000, 
sim time next is 1866000.0000, 
raw observation next is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 26.0, 25.02868317847993, 0.2816130712064742, 0.0, 1.0, 41838.01953091958], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.75, 0.6111111111111109, 0.0847145488029466, 0.6666666666666666, 0.5857235982066609, 0.5938710237354914, 0.0, 1.0, 0.19922866443295037], 
reward next is 0.8008, 
noisyNet noise sample is [array([-0.36073515], dtype=float32), 2.5913749]. 
=============================================
[2019-04-04 07:32:35,552] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.948555]
 [72.98949 ]
 [73.09567 ]
 [73.07326 ]
 [72.896225]], R is [[73.05049133]
 [73.15988922]
 [73.29385376]
 [73.3691864 ]
 [73.44782257]].
[2019-04-04 07:32:45,718] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6591840e-09 4.5839856e-09 3.7175925e-22 5.9402677e-10 7.2893236e-10
 1.9913064e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:45,730] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9954
[2019-04-04 07:32:45,765] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.20775087106731, 0.08949037669847333, 0.0, 1.0, 41328.10787200006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2003400.0000, 
sim time next is 2004000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.2384020077733, 0.08508005896669135, 0.0, 1.0, 41093.73673040397], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5198668339811082, 0.5283600196555638, 0.0, 1.0, 0.1956844606209713], 
reward next is 0.8043, 
noisyNet noise sample is [array([-1.3834709], dtype=float32), -0.18512991]. 
=============================================
[2019-04-04 07:32:45,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.0926  ]
 [74.12585 ]
 [74.17747 ]
 [74.232544]
 [74.26777 ]], R is [[74.12317657]
 [74.18514252]
 [74.24519348]
 [74.30615234]
 [74.36660767]].
[2019-04-04 07:32:46,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4958187e-09 9.9484176e-10 5.9869039e-21 4.5930418e-10 5.1985460e-10
 1.0378239e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:46,122] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0402
[2019-04-04 07:32:46,184] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 77.66666666666667, 64.83333333333333, 0.0, 26.0, 25.04672624853971, 0.2626318113096386, 0.0, 1.0, 45191.44755567658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1870800.0000, 
sim time next is 1871400.0000, 
raw observation next is [-4.5, 76.33333333333333, 57.66666666666667, 0.0, 26.0, 25.02830136689433, 0.259418259302713, 0.0, 1.0, 52481.31523078598], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.7633333333333333, 0.19222222222222224, 0.0, 0.6666666666666666, 0.5856917805745274, 0.5864727531009043, 0.0, 1.0, 0.24991102490850467], 
reward next is 0.7501, 
noisyNet noise sample is [array([-0.89125264], dtype=float32), 0.20998609]. 
=============================================
[2019-04-04 07:32:54,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3517437e-09 9.9646358e-10 3.7850900e-23 1.9335389e-10 9.2287761e-11
 4.5692416e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:54,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9775
[2019-04-04 07:32:54,157] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.15, 78.0, 148.0, 94.0, 26.0, 25.18804859057562, 0.2623632594079757, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1848600.0000, 
sim time next is 1849200.0000, 
raw observation next is [-5.966666666666667, 78.0, 143.3333333333333, 86.83333333333334, 26.0, 25.15029351127778, 0.2491431242883055, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.2973222530009234, 0.78, 0.47777777777777763, 0.09594843462246778, 0.6666666666666666, 0.5958577926064818, 0.5830477080961018, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.874924], dtype=float32), 1.0170394]. 
=============================================
[2019-04-04 07:32:59,452] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5364787e-10 2.2849207e-09 5.5387315e-22 1.6875792e-10 2.8575253e-10
 6.6502654e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:32:59,452] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8804
[2019-04-04 07:32:59,515] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 26.0, 25.22183804657253, 0.3792861524186223, 0.0, 1.0, 44447.24493997368], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1976400.0000, 
sim time next is 1977000.0000, 
raw observation next is [-5.7, 78.83333333333333, 0.0, 0.0, 26.0, 25.2283603001295, 0.3738151513143783, 0.0, 1.0, 43894.48435728387], 
processed observation next is [1.0, 0.9130434782608695, 0.30470914127423826, 0.7883333333333333, 0.0, 0.0, 0.6666666666666666, 0.6023633583441249, 0.6246050504381261, 0.0, 1.0, 0.20902135408230416], 
reward next is 0.7910, 
noisyNet noise sample is [array([-0.22640704], dtype=float32), 0.43967986]. 
=============================================
[2019-04-04 07:32:59,519] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.77745 ]
 [75.5368  ]
 [75.297615]
 [75.0902  ]
 [74.81011 ]], R is [[75.99964905]
 [76.02799988]
 [76.04808044]
 [76.04356384]
 [75.97669983]].
[2019-04-04 07:33:09,221] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5264725e-09 9.8268715e-10 1.4382710e-22 2.5092442e-10 1.6135214e-10
 1.3980346e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:09,221] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8661
[2019-04-04 07:33:09,236] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13805997751053, 0.05771806164269403, 0.0, 1.0, 41132.93142092844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008800.0000, 
sim time next is 2009400.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.14033430504187, 0.05484005060591123, 0.0, 1.0, 41137.88448876359], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5116945254201557, 0.5182800168686371, 0.0, 1.0, 0.1958946880417314], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.5099837], dtype=float32), 1.9018747]. 
=============================================
[2019-04-04 07:33:21,551] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.30948765e-11 6.36144817e-11 1.26908925e-24 2.15335076e-11
 9.77556491e-12 8.04722771e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 07:33:21,551] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6355
[2019-04-04 07:33:21,620] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 67.5, 18.0, 0.0, 26.0, 26.08606395070724, 0.4624121682323021, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2134200.0000, 
sim time next is 2134800.0000, 
raw observation next is [-4.5, 68.0, 14.0, 0.0, 26.0, 26.09740607255883, 0.4572353093997617, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3379501385041552, 0.68, 0.04666666666666667, 0.0, 0.6666666666666666, 0.6747838393799025, 0.6524117697999205, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4165063], dtype=float32), 0.16750717]. 
=============================================
[2019-04-04 07:33:33,383] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5812023e-08 4.2577271e-09 1.1567021e-22 2.7901452e-09 5.9033672e-10
 5.2027467e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:33,386] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6183
[2019-04-04 07:33:33,409] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 40.0, 0.0, 0.0, 26.0, 25.14670300291356, 0.2158134236149967, 0.0, 1.0, 39139.22048356795], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2509200.0000, 
sim time next is 2509800.0000, 
raw observation next is [-1.7, 39.66666666666667, 0.0, 0.0, 26.0, 25.08519437022937, 0.2072826198701153, 0.0, 1.0, 39122.89690497024], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.3966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5904328641857809, 0.5690942066233717, 0.0, 1.0, 0.18629950907128687], 
reward next is 0.8137, 
noisyNet noise sample is [array([-1.7225106], dtype=float32), -0.21182586]. 
=============================================
[2019-04-04 07:33:43,087] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5527557e-12 4.5335992e-12 3.2857921e-26 2.3244657e-12 1.3282271e-13
 1.5496155e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:43,087] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6700
[2019-04-04 07:33:43,137] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.533333333333333, 26.66666666666667, 167.0, 413.0, 26.0, 25.78976682844762, 0.3643415257647311, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2555400.0000, 
sim time next is 2556000.0000, 
raw observation next is [3.8, 26.0, 165.0, 378.5, 26.0, 25.72783229577758, 0.3669711005998975, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5678670360110805, 0.26, 0.55, 0.41823204419889504, 0.6666666666666666, 0.6439860246481318, 0.6223237001999659, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8327885], dtype=float32), -1.3191913]. 
=============================================
[2019-04-04 07:33:43,149] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[91.42038]
 [92.02936]
 [92.2321 ]
 [91.68607]
 [91.09701]], R is [[90.8210144 ]
 [90.91280365]
 [91.00367737]
 [91.09364319]
 [91.18270874]].
[2019-04-04 07:33:46,491] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2387196e-08 3.2879161e-08 8.6583890e-20 4.0194417e-09 4.2650745e-09
 1.6326211e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:46,491] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3676
[2019-04-04 07:33:46,510] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.83333333333333, 83.0, 0.0, 0.0, 26.0, 23.08757945945528, -0.1099431370454848, 0.0, 1.0, 43425.96799933787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2699400.0000, 
sim time next is 2700000.0000, 
raw observation next is [-16.0, 83.0, 0.0, 0.0, 26.0, 23.08813308390158, -0.1239352117490782, 0.0, 1.0, 43343.73676136578], 
processed observation next is [1.0, 0.2608695652173913, 0.01939058171745151, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4240110903251318, 0.4586882627503073, 0.0, 1.0, 0.20639874648269418], 
reward next is 0.7936, 
noisyNet noise sample is [array([-1.6798137], dtype=float32), -0.061526988]. 
=============================================
[2019-04-04 07:33:46,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[66.245735]
 [66.4466  ]
 [66.64527 ]
 [66.84722 ]
 [67.059105]], R is [[66.17288971]
 [66.30437469]
 [66.43424225]
 [66.56239319]
 [66.68870544]].
[2019-04-04 07:33:52,759] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3012111e-09 4.1553938e-09 3.0542907e-21 1.2213259e-09 3.6679737e-10
 1.6404802e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:52,759] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0662
[2019-04-04 07:33:52,777] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.8117834258425, 0.01640609877650047, 0.0, 1.0, 44211.87012584207], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2614200.0000, 
sim time next is 2614800.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 23.78101884551919, 0.01809258304384623, 0.0, 1.0, 44385.53418171554], 
processed observation next is [1.0, 0.2608695652173913, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.4817515704599324, 0.5060308610146155, 0.0, 1.0, 0.21135968657959783], 
reward next is 0.7886, 
noisyNet noise sample is [array([-1.7639929], dtype=float32), -0.66532606]. 
=============================================
[2019-04-04 07:33:53,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.9779018e-09 1.5012461e-08 1.3833624e-21 8.0911233e-10 3.0152253e-10
 3.2124435e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:53,251] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6823
[2019-04-04 07:33:53,282] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 62.33333333333333, 0.0, 0.0, 26.0, 24.48153983410489, 0.1410469830617614, 0.0, 1.0, 40847.2151555495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2781600.0000, 
sim time next is 2782200.0000, 
raw observation next is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.44104532868745, 0.1290428895273478, 0.0, 1.0, 40924.88978116163], 
processed observation next is [1.0, 0.17391304347826086, 0.27331486611265005, 0.6316666666666666, 0.0, 0.0, 0.6666666666666666, 0.5367537773906209, 0.5430142965091159, 0.0, 1.0, 0.19488042752934112], 
reward next is 0.8051, 
noisyNet noise sample is [array([-0.16319476], dtype=float32), 0.61545444]. 
=============================================
[2019-04-04 07:33:55,838] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.1014340e-11 1.3843600e-10 2.6203841e-24 1.8996549e-11 4.4433008e-12
 3.7898265e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:55,842] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8123
[2019-04-04 07:33:55,916] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 61.5, 113.0, 799.0, 26.0, 25.86483538246843, 0.3619502188077265, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2723400.0000, 
sim time next is 2724000.0000, 
raw observation next is [-6.666666666666667, 60.66666666666666, 112.3333333333333, 797.1666666666667, 26.0, 25.29426394596282, 0.4015509190729253, 1.0, 1.0, 166630.5289785643], 
processed observation next is [1.0, 0.5217391304347826, 0.27793167128347185, 0.6066666666666666, 0.37444444444444436, 0.8808471454880296, 0.6666666666666666, 0.6078553288302349, 0.6338503063576417, 1.0, 1.0, 0.7934787094217348], 
reward next is 0.2065, 
noisyNet noise sample is [array([0.5630335], dtype=float32), -0.9813253]. 
=============================================
[2019-04-04 07:33:55,925] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.692245]
 [83.676056]
 [83.637054]
 [83.622574]
 [83.64361 ]], R is [[83.92353058]
 [84.08429718]
 [84.24345398]
 [84.40102386]
 [84.55701447]].
[2019-04-04 07:33:57,438] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4726096e-10 2.4163360e-10 1.6902934e-23 5.1346576e-11 9.1886021e-12
 7.8298430e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:57,441] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3823
[2019-04-04 07:33:57,482] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 41.66666666666666, 0.0, 0.0, 26.0, 25.38914949686406, 0.3721796435990419, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2835600.0000, 
sim time next is 2836200.0000, 
raw observation next is [2.166666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.29020063896814, 0.3456921250661811, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5226223453370269, 0.42833333333333345, 0.0, 0.0, 0.6666666666666666, 0.6075167199140118, 0.6152307083553937, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2568524], dtype=float32), -0.5524599]. 
=============================================
[2019-04-04 07:33:59,278] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8002269e-09 9.9363806e-09 2.5239952e-21 4.6284385e-10 7.1182532e-10
 4.6898214e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:33:59,279] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9445
[2019-04-04 07:33:59,312] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64554662655244, 0.2544611136274689, 0.0, 1.0, 42918.24202176111], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2767200.0000, 
sim time next is 2767800.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.72411067863519, 0.2562760719155276, 0.0, 1.0, 42694.63821931047], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5603425565529324, 0.5854253573051759, 0.0, 1.0, 0.20330780104433557], 
reward next is 0.7967, 
noisyNet noise sample is [array([1.2034779], dtype=float32), -0.64131147]. 
=============================================
[2019-04-04 07:34:05,029] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2494316e-11 2.3429616e-11 6.9479368e-25 4.7771145e-12 1.1950111e-12
 1.3647077e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:05,032] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4345
[2019-04-04 07:34:05,054] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 149.5, 635.5, 26.0, 26.04318363990414, 0.4737178041032116, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2804400.0000, 
sim time next is 2805000.0000, 
raw observation next is [-0.5, 49.00000000000001, 141.3333333333333, 678.0, 26.0, 26.07244515113236, 0.4738635288171969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44875346260387816, 0.49000000000000005, 0.471111111111111, 0.7491712707182321, 0.6666666666666666, 0.6727037625943634, 0.6579545096057323, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.90512794], dtype=float32), -0.28245807]. 
=============================================
[2019-04-04 07:34:05,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.16892 ]
 [87.17311 ]
 [87.151054]
 [86.94219 ]
 [86.53109 ]], R is [[87.32529449]
 [87.45204163]
 [87.57752228]
 [87.70174408]
 [87.82472992]].
[2019-04-04 07:34:10,385] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5764812e-11 2.5196118e-11 1.0112148e-26 3.4398224e-12 1.9546394e-12
 2.9941507e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:10,394] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7088
[2019-04-04 07:34:10,444] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 17.5, 48.49999999999999, 26.0, 24.98154432525494, 0.2711033902909603, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2878800.0000, 
sim time next is 2879400.0000, 
raw observation next is [2.0, 93.0, 34.99999999999999, 69.99999999999999, 26.0, 25.00294439653596, 0.2652678112307157, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.11666666666666664, 0.07734806629834252, 0.6666666666666666, 0.5835786997113299, 0.5884226037435719, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0687287], dtype=float32), -1.4450569]. 
=============================================
[2019-04-04 07:34:15,777] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0622610e-08 4.0134052e-09 8.4575507e-22 1.5687939e-09 5.5145205e-10
 5.2585857e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:15,777] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6564
[2019-04-04 07:34:15,809] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 14.0, 15.66666666666666, 26.0, 23.55692559871305, 0.03588896650406052, 0.0, 1.0, 60784.70272629883], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2965200.0000, 
sim time next is 2965800.0000, 
raw observation next is [-4.0, 77.0, 27.99999999999999, 22.33333333333333, 26.0, 23.5567018309863, 0.03318141816965941, 0.0, 1.0, 60770.28930127715], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0933333333333333, 0.024677716390423567, 0.6666666666666666, 0.4630584859155249, 0.5110604727232199, 0.0, 1.0, 0.28938233000608166], 
reward next is 0.7106, 
noisyNet noise sample is [array([0.6437884], dtype=float32), 0.07135925]. 
=============================================
[2019-04-04 07:34:23,600] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.00404362e-12 1.31549615e-11 4.45964911e-25 5.33299793e-12
 4.06930609e-12 3.08662724e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:34:23,601] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8429
[2019-04-04 07:34:23,633] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 108.0, 790.5, 26.0, 26.47924979548013, 0.5327821233253499, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3420000.0000, 
sim time next is 3420600.0000, 
raw observation next is [3.0, 50.5, 106.3333333333333, 785.3333333333334, 26.0, 26.03706773890016, 0.5783663949245811, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5457063711911359, 0.505, 0.35444444444444434, 0.8677716390423573, 0.6666666666666666, 0.6697556449083466, 0.6927887983081936, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6301873], dtype=float32), -0.6024218]. 
=============================================
[2019-04-04 07:34:29,663] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.4503017e-10 1.4421345e-09 1.1152240e-21 2.1205010e-10 2.5335450e-10
 1.6634275e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:29,665] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0013
[2019-04-04 07:34:29,702] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.05979096061659, 0.3686610657588961, 0.0, 1.0, 41564.77680470695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3369000.0000, 
sim time next is 3369600.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.02201006522967, 0.3635289959096875, 0.0, 1.0, 41524.74962145172], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5851675054358058, 0.6211763319698959, 0.0, 1.0, 0.19773690295929391], 
reward next is 0.8023, 
noisyNet noise sample is [array([-0.15740678], dtype=float32), 0.01789321]. 
=============================================
[2019-04-04 07:34:29,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.8054165e-10 1.7020458e-09 4.5876881e-23 7.3769886e-11 2.7104918e-10
 1.2850558e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:29,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1329
[2019-04-04 07:34:29,744] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333333, 74.0, 0.0, 0.0, 26.0, 25.93424668358892, 0.6160344868219034, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3532800.0000, 
sim time next is 3533400.0000, 
raw observation next is [-0.5, 75.0, 0.0, 0.0, 26.0, 25.91929336846743, 0.6035285232649453, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.44875346260387816, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6599411140389524, 0.7011761744216485, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5290539], dtype=float32), -0.4819062]. 
=============================================
[2019-04-04 07:34:42,020] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5395720e-09 6.2388145e-10 1.1094908e-22 8.1739071e-10 7.1687750e-10
 3.6808512e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:42,028] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3357
[2019-04-04 07:34:42,052] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 70.0, 0.0, 0.0, 26.0, 25.51621355081851, 0.3761387452740439, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3719400.0000, 
sim time next is 3720000.0000, 
raw observation next is [-3.0, 69.0, 0.0, 0.0, 26.0, 25.49642285089065, 0.3642085930372396, 0.0, 1.0, 26516.25343408304], 
processed observation next is [1.0, 0.043478260869565216, 0.3795013850415513, 0.69, 0.0, 0.0, 0.6666666666666666, 0.6247019042408875, 0.6214028643457465, 0.0, 1.0, 0.1262678734956335], 
reward next is 0.8737, 
noisyNet noise sample is [array([0.6850832], dtype=float32), -0.45455024]. 
=============================================
[2019-04-04 07:34:42,056] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.33288]
 [77.65504]
 [77.84736]
 [78.21768]
 [78.22839]], R is [[77.3633194 ]
 [77.58968353]
 [77.81378937]
 [78.03565216]
 [78.0565567 ]].
[2019-04-04 07:34:44,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.8516764e-11 4.0328169e-10 1.1933560e-22 6.1161819e-11 2.1939254e-11
 4.0460165e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:44,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8253
[2019-04-04 07:34:44,253] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.89154695464559, 0.5717233682293625, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3349200.0000, 
sim time next is 3349800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.91147384609242, 0.5418926876548795, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6592894871743683, 0.6806308958849598, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0991814], dtype=float32), 0.40367842]. 
=============================================
[2019-04-04 07:34:48,028] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.8350358e-10 1.0489896e-10 1.0975634e-25 1.1656702e-10 3.5103524e-11
 3.6874529e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:48,029] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6968
[2019-04-04 07:34:48,048] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 28.0, 91.0, 446.3333333333334, 26.0, 25.6045740301603, 0.4311587351745405, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3660000.0000, 
sim time next is 3660600.0000, 
raw observation next is [10.5, 27.0, 93.0, 489.6666666666666, 26.0, 25.62421579810986, 0.4366900730960031, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.7534626038781165, 0.27, 0.31, 0.5410681399631675, 0.6666666666666666, 0.6353513165091549, 0.6455633576986677, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01988667], dtype=float32), 0.6145021]. 
=============================================
[2019-04-04 07:34:48,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.3423089e-10 1.0788997e-10 7.3713419e-24 3.9330809e-11 2.7713245e-11
 3.8774712e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:34:48,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0206
[2019-04-04 07:34:48,691] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 48.5, 87.0, 705.0, 26.0, 25.5168452757637, 0.4879658518173212, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3684600.0000, 
sim time next is 3685200.0000, 
raw observation next is [5.333333333333333, 49.0, 83.16666666666666, 674.5, 26.0, 25.50916102837557, 0.4844639669247988, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6103416435826409, 0.49, 0.2772222222222222, 0.7453038674033149, 0.6666666666666666, 0.6257634190312974, 0.6614879889749329, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5101275], dtype=float32), -1.3481437]. 
=============================================
[2019-04-04 07:34:49,172] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 07:34:49,173] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:34:49,175] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:34:49,185] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:34:49,202] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:34:49,210] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:34:49,245] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:34:49,315] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run21
[2019-04-04 07:34:49,383] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run21
[2019-04-04 07:34:49,488] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run21
[2019-04-04 07:36:21,009] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.21903643], dtype=float32), -0.0955593]
[2019-04-04 07:36:21,009] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.116666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 24.45067985141205, 0.1163085635620002, 0.0, 1.0, 44905.18816883703]
[2019-04-04 07:36:21,009] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:36:21,010] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.6549056e-09 2.0019764e-09 5.0771613e-22 3.8978556e-10 2.7145841e-10
 1.6066898e-12 1.0000000e+00], sampled 0.009930430325847728
[2019-04-04 07:37:55,222] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 07:38:30,822] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:38:37,188] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.4547 275844504.2563 1233.4233
[2019-04-04 07:38:38,220] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2000000, evaluation results [2000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.454741636738, 275844504.2562892, 1233.4232607117071]
[2019-04-04 07:38:44,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.1217960e-10 3.1655151e-10 9.7753977e-23 8.0170745e-11 6.4169815e-11
 3.1442926e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:38:44,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9836
[2019-04-04 07:38:44,364] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.29142121467439, 0.4323552386426101, 0.0, 1.0, 43777.40971902688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3799800.0000, 
sim time next is 3800400.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.28351086145505, 0.4300565890963537, 0.0, 1.0, 43705.70385441517], 
processed observation next is [1.0, 1.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6069592384545874, 0.6433521963654513, 0.0, 1.0, 0.20812239930673893], 
reward next is 0.7919, 
noisyNet noise sample is [array([-0.00168963], dtype=float32), -0.9160946]. 
=============================================
[2019-04-04 07:38:45,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2306290e-10 4.5577266e-09 8.9576592e-22 2.6557703e-10 3.7772108e-10
 2.2336226e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:38:45,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6018
[2019-04-04 07:38:45,405] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 76.0, 0.0, 0.0, 26.0, 25.42809067746823, 0.3986429812038229, 0.0, 1.0, 32285.89191055142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3809400.0000, 
sim time next is 3810000.0000, 
raw observation next is [-4.0, 75.0, 0.0, 0.0, 26.0, 25.4224020661117, 0.3925592622634031, 0.0, 1.0, 40879.16923544391], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6185335055093084, 0.6308530874211343, 0.0, 1.0, 0.19466271064497098], 
reward next is 0.8053, 
noisyNet noise sample is [array([0.59948057], dtype=float32), -0.19918866]. 
=============================================
[2019-04-04 07:38:45,409] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.790474]
 [76.91096 ]
 [77.10283 ]
 [77.21738 ]
 [77.28648 ]], R is [[76.75673676]
 [76.83543396]
 [76.97773743]
 [77.04601288]
 [77.06547546]].
[2019-04-04 07:39:01,299] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0618408e-11 9.5223822e-11 1.1565294e-23 5.1941132e-11 8.7157703e-12
 1.4526048e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:01,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4824
[2019-04-04 07:39:01,370] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 48.5, 0.0, 0.0, 26.0, 25.61920228760544, 0.5403690646692779, 1.0, 1.0, 73963.42234868222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3867000.0000, 
sim time next is 3867600.0000, 
raw observation next is [1.666666666666667, 49.0, 0.0, 0.0, 26.0, 24.97219156296409, 0.4801047369321038, 1.0, 1.0, 184952.3626066606], 
processed observation next is [1.0, 0.782608695652174, 0.5087719298245615, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5810159635803407, 0.6600349123107013, 1.0, 1.0, 0.8807255362221934], 
reward next is 0.1193, 
noisyNet noise sample is [array([-2.6079526], dtype=float32), 1.32062]. 
=============================================
[2019-04-04 07:39:04,011] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.17028495e-11 1.94488210e-11 4.18841199e-26 1.14269835e-11
 1.40328677e-12 6.30507185e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:39:04,011] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8585
[2019-04-04 07:39:04,018] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666666, 38.0, 116.1666666666667, 818.1666666666666, 26.0, 25.64454608562064, 0.4868234354974899, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3670800.0000, 
sim time next is 3671400.0000, 
raw observation next is [5.333333333333333, 41.5, 116.3333333333333, 820.3333333333334, 26.0, 25.59435414030743, 0.4762448429782067, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6103416435826409, 0.415, 0.38777777777777767, 0.9064456721915286, 0.6666666666666666, 0.6328628450256192, 0.6587482809927355, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04639004], dtype=float32), -0.21800593]. 
=============================================
[2019-04-04 07:39:07,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6448249e-12 1.5756204e-11 3.8159211e-26 1.0312157e-12 6.8979344e-13
 1.2765677e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:07,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2480
[2019-04-04 07:39:07,714] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 58.5, 117.0, 830.6666666666667, 26.0, 26.62593362447512, 0.6711748041064179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3845400.0000, 
sim time next is 3846000.0000, 
raw observation next is [-0.3333333333333334, 57.00000000000001, 117.0, 832.8333333333334, 26.0, 26.67164117318834, 0.6743821986860853, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4533702677747, 0.5700000000000001, 0.39, 0.9202578268876612, 0.6666666666666666, 0.7226367644323618, 0.7247940662286951, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7985692], dtype=float32), -1.5703174]. 
=============================================
[2019-04-04 07:39:07,751] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[90.984436]
 [91.00528 ]
 [91.05748 ]
 [91.08903 ]
 [91.15178 ]], R is [[91.07362366]
 [91.16288757]
 [91.25125885]
 [91.33874512]
 [91.42536163]].
[2019-04-04 07:39:10,370] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2222556e-09 9.0396352e-10 1.4621861e-22 3.8727707e-10 9.6388383e-11
 1.0450584e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:10,378] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1131
[2019-04-04 07:39:10,405] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 51.5, 0.0, 0.0, 26.0, 24.61591257362522, 0.201778269886199, 0.0, 1.0, 40279.58528421688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4174200.0000, 
sim time next is 4174800.0000, 
raw observation next is [-5.0, 52.33333333333334, 15.33333333333333, 81.33333333333331, 26.0, 24.57065641360904, 0.207466541901585, 0.0, 1.0, 40324.97979725989], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.5233333333333334, 0.0511111111111111, 0.0898710865561694, 0.6666666666666666, 0.5475547011340867, 0.569155513967195, 0.0, 1.0, 0.1920237133202852], 
reward next is 0.8080, 
noisyNet noise sample is [array([-0.55611205], dtype=float32), -0.11019776]. 
=============================================
[2019-04-04 07:39:14,746] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9649387e-09 1.1763739e-09 4.7337321e-23 2.0113718e-10 1.5997988e-10
 8.1609182e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:14,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8586
[2019-04-04 07:39:14,793] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 47.16666666666667, 0.0, 0.0, 26.0, 25.40236525135658, 0.3682735621055941, 0.0, 1.0, 45632.88066068075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4230600.0000, 
sim time next is 4231200.0000, 
raw observation next is [1.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.39879744887244, 0.3674889667885972, 0.0, 1.0, 43230.54125326015], 
processed observation next is [0.0, 1.0, 0.4995383194829178, 0.47333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6165664540727033, 0.6224963222628658, 0.0, 1.0, 0.20585972025361976], 
reward next is 0.7941, 
noisyNet noise sample is [array([-0.4594296], dtype=float32), -1.508199]. 
=============================================
[2019-04-04 07:39:16,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0471448e-10 3.9837128e-10 6.5686992e-24 6.3281928e-11 1.2896094e-11
 1.9612110e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:16,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7532
[2019-04-04 07:39:16,914] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 36.66666666666666, 0.0, 0.0, 26.0, 25.86550842223248, 0.5939934594095875, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4137000.0000, 
sim time next is 4137600.0000, 
raw observation next is [1.0, 37.33333333333334, 0.0, 0.0, 26.0, 25.92050040810451, 0.5908294607137937, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.6600417006753757, 0.6969431535712646, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6068211], dtype=float32), 0.1295918]. 
=============================================
[2019-04-04 07:39:19,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2804953e-09 1.8480467e-09 3.4488229e-23 3.3624120e-10 2.8709427e-10
 5.3272512e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:19,804] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6822
[2019-04-04 07:39:19,838] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 46.0, 0.0, 0.0, 26.0, 25.34535674835589, 0.3996130103610825, 0.0, 1.0, 39390.36556653557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4154400.0000, 
sim time next is 4155000.0000, 
raw observation next is [-2.166666666666667, 46.66666666666667, 0.0, 0.0, 26.0, 25.33295408909064, 0.3944931054136555, 0.0, 1.0, 39282.31894555877], 
processed observation next is [0.0, 0.08695652173913043, 0.4025854108956602, 0.46666666666666673, 0.0, 0.0, 0.6666666666666666, 0.6110795074242201, 0.6314977018045519, 0.0, 1.0, 0.18705866164551796], 
reward next is 0.8129, 
noisyNet noise sample is [array([1.2762432], dtype=float32), -2.4982564]. 
=============================================
[2019-04-04 07:39:19,850] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.97159 ]
 [78.93699 ]
 [78.997734]
 [78.914154]
 [78.873146]], R is [[79.0080719 ]
 [79.0304184 ]
 [79.05001831]
 [79.06089783]
 [79.04755402]].
[2019-04-04 07:39:23,672] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5276643e-12 2.4803879e-11 1.2044814e-25 7.6738503e-12 3.1304576e-13
 1.0572027e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:23,672] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0561
[2019-04-04 07:39:23,683] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 38.0, 106.8333333333333, 794.0, 26.0, 26.96964222180745, 0.7699698840369908, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3939600.0000, 
sim time next is 3940200.0000, 
raw observation next is [-4.5, 38.0, 105.0, 788.0, 26.0, 26.9787364305754, 0.7710180941056747, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3379501385041552, 0.38, 0.35, 0.8707182320441988, 0.6666666666666666, 0.7482280358812833, 0.7570060313685582, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5937808], dtype=float32), 1.3495727]. 
=============================================
[2019-04-04 07:39:26,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9634411e-09 3.3300931e-09 2.0909175e-21 1.0382849e-09 8.1126184e-10
 7.1745266e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:26,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8723
[2019-04-04 07:39:26,828] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18022256450548, 0.1137711891745468, 0.0, 1.0, 43643.89971858619], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988800.0000, 
sim time next is 3989400.0000, 
raw observation next is [-12.16666666666667, 64.0, 0.0, 0.0, 26.0, 24.11401463913138, 0.1050442692036519, 0.0, 1.0, 43686.3789886024], 
processed observation next is [1.0, 0.17391304347826086, 0.12557710064635264, 0.64, 0.0, 0.0, 0.6666666666666666, 0.509501219927615, 0.5350147564012173, 0.0, 1.0, 0.20803037613620193], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.90043205], dtype=float32), -0.98303586]. 
=============================================
[2019-04-04 07:39:26,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3961481e-08 1.2783387e-08 1.3526681e-21 1.0027630e-09 8.3388285e-10
 2.3790397e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:26,959] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4811
[2019-04-04 07:39:26,992] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 38.0, 0.0, 0.0, 26.0, 24.71024340140844, 0.1949178310500141, 0.0, 1.0, 40164.89355677031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4082400.0000, 
sim time next is 4083000.0000, 
raw observation next is [-4.166666666666667, 38.5, 0.0, 0.0, 26.0, 24.6851568521959, 0.1920669973210452, 0.0, 1.0, 40143.74606340181], 
processed observation next is [1.0, 0.2608695652173913, 0.3471837488457987, 0.385, 0.0, 0.0, 0.6666666666666666, 0.5570964043496582, 0.5640223324403484, 0.0, 1.0, 0.19116069554000864], 
reward next is 0.8088, 
noisyNet noise sample is [array([-0.6391208], dtype=float32), 0.025010388]. 
=============================================
[2019-04-04 07:39:27,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.93547]
 [75.94074]
 [75.93458]
 [75.9193 ]
 [75.91008]], R is [[75.96122742]
 [76.01035309]
 [76.05898285]
 [76.10707855]
 [76.15462494]].
[2019-04-04 07:39:31,824] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.4329732e-10 1.2093462e-09 3.9870081e-23 9.3824615e-11 8.1231431e-11
 3.6263987e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:31,824] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1752
[2019-04-04 07:39:31,840] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.31376599819518, 0.4070834796339953, 0.0, 1.0, 41318.37563167084], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.30032335840804, 0.4021826450626667, 0.0, 1.0, 41236.68667362652], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6083602798673367, 0.6340608816875556, 0.0, 1.0, 0.19636517463631678], 
reward next is 0.8036, 
noisyNet noise sample is [array([2.4932601], dtype=float32), 3.466758]. 
=============================================
[2019-04-04 07:39:32,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0799447e-08 3.6853951e-09 7.4261604e-22 6.4674743e-10 9.1134189e-10
 1.8296935e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:32,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4010
[2019-04-04 07:39:32,181] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 52.33333333333334, 15.33333333333333, 81.33333333333331, 26.0, 24.64942613646074, 0.2154504997965918, 0.0, 1.0, 40288.6637722446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4174800.0000, 
sim time next is 4175400.0000, 
raw observation next is [-5.0, 53.16666666666666, 30.66666666666666, 162.6666666666666, 26.0, 24.63528992600519, 0.2207685764860946, 0.0, 1.0, 40190.72173323314], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.5316666666666666, 0.1022222222222222, 0.17974217311233878, 0.6666666666666666, 0.5529408271670991, 0.5735895254953648, 0.0, 1.0, 0.1913843892058721], 
reward next is 0.8086, 
noisyNet noise sample is [array([-1.5049785], dtype=float32), 1.5866344]. 
=============================================
[2019-04-04 07:39:36,320] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5869387e-09 1.6090107e-09 2.7439309e-23 6.6215661e-10 2.1897292e-10
 7.6261046e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:36,320] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3582
[2019-04-04 07:39:36,376] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.47031442092987, 0.3417794953387102, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4248000.0000, 
sim time next is 4248600.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.46727433403557, 0.3307676001708169, 0.0, 1.0, 18756.66004864531], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6222728611696308, 0.6102558667236057, 0.0, 1.0, 0.0893174288030729], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.6094166], dtype=float32), -1.4981202]. 
=============================================
[2019-04-04 07:39:38,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0871886e-10 3.4140987e-10 8.2808346e-25 2.7325099e-11 2.9870151e-11
 1.0588640e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:38,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3030
[2019-04-04 07:39:38,344] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.33333333333333, 58.83333333333334, 0.0, 0.0, 26.0, 26.92705155307286, 0.8566239763717615, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4395000.0000, 
sim time next is 4395600.0000, 
raw observation next is [10.2, 59.0, 0.0, 0.0, 26.0, 26.8629855582455, 0.8471213551343898, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7451523545706372, 0.59, 0.0, 0.0, 0.6666666666666666, 0.7385821298537918, 0.7823737850447966, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7108382], dtype=float32), -0.9422387]. 
=============================================
[2019-04-04 07:39:39,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7740040e-10 4.8939419e-10 1.5881858e-24 2.6331071e-11 3.7177126e-11
 2.0320216e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:39,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1346
[2019-04-04 07:39:39,865] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.45, 75.5, 0.0, 0.0, 26.0, 25.56261208545037, 0.4022590905078445, 0.0, 1.0, 34439.57085027609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4318200.0000, 
sim time next is 4318800.0000, 
raw observation next is [4.466666666666667, 75.66666666666667, 0.0, 0.0, 26.0, 25.51420964680865, 0.3988155343136519, 0.0, 1.0, 58488.04852007503], 
processed observation next is [0.0, 1.0, 0.5863342566943676, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6261841372340541, 0.6329385114378839, 0.0, 1.0, 0.2785145167622621], 
reward next is 0.7215, 
noisyNet noise sample is [array([1.2253072], dtype=float32), -0.60284424]. 
=============================================
[2019-04-04 07:39:42,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5406064e-09 1.1251982e-09 3.6619675e-22 3.2988806e-10 5.0565591e-10
 3.5333106e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:42,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1250
[2019-04-04 07:39:42,244] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.31306197291286, 0.1882172752299994, 0.0, 1.0, 41334.45094117964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4771800.0000, 
sim time next is 4772400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.2756651356463, 0.1772580492208637, 0.0, 1.0, 41386.36022344339], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5229720946371916, 0.5590860164069545, 0.0, 1.0, 0.1970779058259209], 
reward next is 0.8029, 
noisyNet noise sample is [array([0.27694598], dtype=float32), -1.475344]. 
=============================================
[2019-04-04 07:39:42,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4179636e-10 5.7574717e-10 7.4907660e-24 9.1022273e-11 1.4686733e-11
 1.2878088e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:42,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9101
[2019-04-04 07:39:42,881] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 74.0, 0.0, 0.0, 26.0, 25.09834889539604, 0.3656884426556035, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4606200.0000, 
sim time next is 4606800.0000, 
raw observation next is [-2.333333333333333, 73.0, 20.5, 28.49999999999999, 26.0, 25.26527552020707, 0.3874840251500494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3979686057248385, 0.73, 0.06833333333333333, 0.03149171270718231, 0.6666666666666666, 0.6054396266839225, 0.6291613417166831, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2402254], dtype=float32), 0.7044005]. 
=============================================
[2019-04-04 07:39:43,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2074296e-11 2.8097365e-11 2.6506138e-25 8.7307783e-12 2.1449791e-12
 2.4097621e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:43,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3225
[2019-04-04 07:39:43,700] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 68.5, 48.0, 26.0, 25.08478688397965, 0.4514129315541849, 1.0, 1.0, 124611.5272293077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4554000.0000, 
sim time next is 4554600.0000, 
raw observation next is [2.0, 52.0, 55.0, 41.33333333333333, 26.0, 25.50102180748812, 0.4823489253431909, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.18333333333333332, 0.04567219152854511, 0.6666666666666666, 0.6250851506240099, 0.660782975114397, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5256649], dtype=float32), 0.7785293]. 
=============================================
[2019-04-04 07:39:53,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8345851e-10 7.1518402e-10 1.1903416e-22 8.0147812e-11 5.0972459e-11
 2.1720803e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:39:53,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7783
[2019-04-04 07:39:53,335] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.59355077598256, 0.4596468463091535, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4686600.0000, 
sim time next is 4687200.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.5938897577379, 0.4506667087063628, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6328241464781584, 0.6502222362354543, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20560004], dtype=float32), -0.65011233]. 
=============================================
[2019-04-04 07:39:59,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.21975452e-09 1.11137677e-09 1.23657014e-22 1.00851674e-10
 5.96121694e-11 4.16773603e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 07:39:59,740] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0032
[2019-04-04 07:39:59,752] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.5938897577379, 0.4506667087063628, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4687200.0000, 
sim time next is 4687800.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.57942982625696, 0.4374596303206384, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.63161915218808, 0.6458198767735461, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2641585], dtype=float32), 0.80093014]. 
=============================================
[2019-04-04 07:40:02,375] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:02,375] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:02,379] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run16
[2019-04-04 07:40:02,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5147371e-10 1.3685658e-10 4.8352242e-23 9.0982347e-11 8.0045267e-12
 2.9858616e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:40:02,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3488
[2019-04-04 07:40:02,965] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 199.5, 398.0, 26.0, 25.07151881637233, 0.3709688122588122, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4892400.0000, 
sim time next is 4893000.0000, 
raw observation next is [3.0, 45.0, 187.3333333333333, 406.0, 26.0, 25.08049739751291, 0.3743944478452382, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.45, 0.6244444444444442, 0.4486187845303867, 0.6666666666666666, 0.5900414497927425, 0.624798149281746, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2496232], dtype=float32), 0.2695157]. 
=============================================
[2019-04-04 07:40:03,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.6431  ]
 [81.784676]
 [81.9347  ]
 [81.95829 ]
 [81.98087 ]], R is [[81.68241882]
 [81.86559296]
 [82.04693604]
 [82.13747406]
 [82.22710419]].
[2019-04-04 07:40:04,503] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0232582e-09 1.0400351e-09 2.8520520e-22 2.1091542e-10 3.4080935e-10
 2.8610673e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:40:04,507] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0555
[2019-04-04 07:40:04,521] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 23.91151306125134, 0.1026816816919423, 0.0, 1.0, 41873.73495651734], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4777800.0000, 
sim time next is 4778400.0000, 
raw observation next is [-6.133333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 23.88071131513524, 0.09586130466501486, 0.0, 1.0, 41902.30819521914], 
processed observation next is [0.0, 0.30434782608695654, 0.2927054478301016, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.49005927626127005, 0.5319537682216716, 0.0, 1.0, 0.19953480092961495], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.9656462], dtype=float32), -1.0845151]. 
=============================================
[2019-04-04 07:40:06,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:06,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:06,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run16
[2019-04-04 07:40:07,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:07,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:07,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run16
[2019-04-04 07:40:10,965] A3C_AGENT_WORKER-Thread-2 INFO:Local step 127500, global step 2035617: loss 0.3282
[2019-04-04 07:40:10,966] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 127500, global step 2035617: learning rate 0.0000
[2019-04-04 07:40:12,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:12,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:12,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run16
[2019-04-04 07:40:12,815] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:12,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:12,819] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run16
[2019-04-04 07:40:14,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:14,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:14,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run16
[2019-04-04 07:40:15,517] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3997525e-09 5.7196146e-09 3.4261918e-22 1.6760960e-09 2.8425221e-10
 1.5165654e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:40:15,517] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3597
[2019-04-04 07:40:15,531] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.21210644186593, 0.335744812322218, 0.0, 1.0, 39400.18692792133], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4842600.0000, 
sim time next is 4843200.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.19567657049034, 0.3314249725347352, 0.0, 1.0, 39303.09353778009], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5996397142075282, 0.6104749908449117, 0.0, 1.0, 0.1871575882751433], 
reward next is 0.8128, 
noisyNet noise sample is [array([-1.0842893], dtype=float32), -0.43573934]. 
=============================================
[2019-04-04 07:40:15,641] A3C_AGENT_WORKER-Thread-12 INFO:Local step 127500, global step 2037214: loss 0.3034
[2019-04-04 07:40:15,642] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 127500, global step 2037214: learning rate 0.0000
[2019-04-04 07:40:15,656] A3C_AGENT_WORKER-Thread-5 INFO:Local step 127500, global step 2037217: loss 0.2970
[2019-04-04 07:40:15,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 127500, global step 2037217: learning rate 0.0000
[2019-04-04 07:40:15,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:15,898] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:15,907] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run16
[2019-04-04 07:40:16,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:16,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:16,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run16
[2019-04-04 07:40:19,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:19,086] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:19,105] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run16
[2019-04-04 07:40:20,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:20,281] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:20,290] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run16
[2019-04-04 07:40:21,302] A3C_AGENT_WORKER-Thread-19 INFO:Local step 127500, global step 2038624: loss 0.3011
[2019-04-04 07:40:21,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 127500, global step 2038625: learning rate 0.0000
[2019-04-04 07:40:22,147] A3C_AGENT_WORKER-Thread-20 INFO:Local step 127500, global step 2038844: loss 0.2964
[2019-04-04 07:40:22,148] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 127500, global step 2038844: learning rate 0.0000
[2019-04-04 07:40:24,641] A3C_AGENT_WORKER-Thread-3 INFO:Local step 127500, global step 2039459: loss 0.2811
[2019-04-04 07:40:24,642] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 127500, global step 2039459: learning rate 0.0000
[2019-04-04 07:40:25,680] A3C_AGENT_WORKER-Thread-18 INFO:Local step 127500, global step 2039756: loss 0.2543
[2019-04-04 07:40:25,681] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 127500, global step 2039756: learning rate 0.0000
[2019-04-04 07:40:25,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:25,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:25,961] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run16
[2019-04-04 07:40:26,236] A3C_AGENT_WORKER-Thread-16 INFO:Local step 127500, global step 2039924: loss 0.2548
[2019-04-04 07:40:26,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 127500, global step 2039924: learning rate 0.0000
[2019-04-04 07:40:26,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:26,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:26,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run16
[2019-04-04 07:40:27,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:27,705] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:27,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run16
[2019-04-04 07:40:28,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:28,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:28,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run16
[2019-04-04 07:40:28,536] A3C_AGENT_WORKER-Thread-17 INFO:Local step 127500, global step 2040532: loss 0.2539
[2019-04-04 07:40:28,536] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 127500, global step 2040532: learning rate 0.0000
[2019-04-04 07:40:29,557] A3C_AGENT_WORKER-Thread-11 INFO:Local step 127500, global step 2040728: loss 0.2575
[2019-04-04 07:40:29,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 127500, global step 2040728: learning rate 0.0000
[2019-04-04 07:40:33,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:33,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:33,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run16
[2019-04-04 07:40:34,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:40:34,493] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:40:34,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run16
[2019-04-04 07:40:35,753] A3C_AGENT_WORKER-Thread-15 INFO:Local step 127500, global step 2042067: loss 0.2624
[2019-04-04 07:40:35,754] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 127500, global step 2042067: learning rate 0.0000
[2019-04-04 07:40:36,846] A3C_AGENT_WORKER-Thread-6 INFO:Local step 127500, global step 2042315: loss 0.2406
[2019-04-04 07:40:36,847] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 127500, global step 2042315: learning rate 0.0000
[2019-04-04 07:40:37,759] A3C_AGENT_WORKER-Thread-13 INFO:Local step 127500, global step 2042468: loss 0.2389
[2019-04-04 07:40:37,760] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 127500, global step 2042468: learning rate 0.0000
[2019-04-04 07:40:38,592] A3C_AGENT_WORKER-Thread-4 INFO:Local step 127500, global step 2042625: loss 0.2316
[2019-04-04 07:40:38,609] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 127500, global step 2042625: learning rate 0.0000
[2019-04-04 07:40:41,603] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128000, global step 2043304: loss 0.1661
[2019-04-04 07:40:41,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128000, global step 2043304: learning rate 0.0000
[2019-04-04 07:40:43,481] A3C_AGENT_WORKER-Thread-10 INFO:Local step 127500, global step 2043896: loss 0.2274
[2019-04-04 07:40:43,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 127500, global step 2043896: learning rate 0.0000
[2019-04-04 07:40:44,162] A3C_AGENT_WORKER-Thread-14 INFO:Local step 127500, global step 2044083: loss 0.2165
[2019-04-04 07:40:44,165] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 127500, global step 2044085: learning rate 0.0000
[2019-04-04 07:40:46,740] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128000, global step 2044657: loss 0.1534
[2019-04-04 07:40:46,742] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128000, global step 2044657: learning rate 0.0000
[2019-04-04 07:40:47,676] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.2492000e-11 1.1920399e-09 4.9074933e-22 1.0043071e-10 1.5948198e-11
 1.8585663e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:40:47,676] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8184
[2019-04-04 07:40:47,732] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.1, 61.0, 0.0, 0.0, 26.0, 25.32286418168126, 0.3422585518887196, 1.0, 1.0, 82596.34471488331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 326400.0000, 
sim time next is 327000.0000, 
raw observation next is [-12.2, 62.0, 0.0, 0.0, 26.0, 25.22380161669614, 0.3387076214628316, 1.0, 1.0, 76568.10625246541], 
processed observation next is [1.0, 0.782608695652174, 0.12465373961218838, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6019834680580116, 0.6129025404876106, 1.0, 1.0, 0.3646100297736448], 
reward next is 0.6354, 
noisyNet noise sample is [array([-0.11317956], dtype=float32), -1.8080198]. 
=============================================
[2019-04-04 07:40:47,742] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.65219 ]
 [77.67882 ]
 [77.35443 ]
 [77.618546]
 [77.2144  ]], R is [[77.60573578]
 [77.43636322]
 [77.16267395]
 [77.39104462]
 [77.21026611]].
[2019-04-04 07:40:48,819] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128000, global step 2045143: loss 0.1705
[2019-04-04 07:40:48,820] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128000, global step 2045143: learning rate 0.0000
[2019-04-04 07:40:52,911] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128000, global step 2046245: loss 0.2203
[2019-04-04 07:40:52,912] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128000, global step 2046245: learning rate 0.0000
[2019-04-04 07:40:53,066] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128000, global step 2046296: loss 0.2157
[2019-04-04 07:40:53,066] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128000, global step 2046296: learning rate 0.0000
[2019-04-04 07:40:56,114] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128000, global step 2047043: loss 0.2123
[2019-04-04 07:40:56,133] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128000, global step 2047043: learning rate 0.0000
[2019-04-04 07:40:57,525] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128000, global step 2047338: loss 0.1816
[2019-04-04 07:40:57,525] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128000, global step 2047338: learning rate 0.0000
[2019-04-04 07:40:58,460] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9580613e-08 2.3705244e-08 6.1708520e-20 2.5212248e-09 1.8867081e-09
 3.7354394e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:40:58,461] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2153
[2019-04-04 07:40:58,515] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 43.0, 0.0, 0.0, 26.0, 22.52259763361867, -0.3383250864803483, 0.0, 1.0, 46090.61025001643], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [-8.3, 42.5, 0.0, 0.0, 26.0, 22.49669682434098, -0.3338514588320365, 0.0, 1.0, 46024.76147984101], 
processed observation next is [1.0, 0.30434782608695654, 0.23268698060941828, 0.425, 0.0, 0.0, 0.6666666666666666, 0.3747247353617483, 0.38871618038932115, 0.0, 1.0, 0.21916553085638574], 
reward next is 0.7808, 
noisyNet noise sample is [array([0.26593336], dtype=float32), -0.22798294]. 
=============================================
[2019-04-04 07:40:58,781] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128000, global step 2047678: loss 0.1647
[2019-04-04 07:40:58,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128000, global step 2047678: learning rate 0.0000
[2019-04-04 07:40:59,361] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128000, global step 2047843: loss 0.1751
[2019-04-04 07:40:59,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128000, global step 2047843: learning rate 0.0000
[2019-04-04 07:41:02,153] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128000, global step 2048596: loss 0.1574
[2019-04-04 07:41:02,153] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128000, global step 2048597: learning rate 0.0000
[2019-04-04 07:41:03,465] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.8197552e-09 4.9444129e-09 1.4286259e-21 8.6052709e-10 5.8800492e-10
 1.0476834e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:03,466] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5191
[2019-04-04 07:41:03,512] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.866666666666667, 69.0, 0.0, 0.0, 26.0, 23.29231472117385, -0.1181317589167583, 0.0, 1.0, 46905.96264448686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 274800.0000, 
sim time next is 275400.0000, 
raw observation next is [-10.05, 68.5, 0.0, 0.0, 26.0, 23.20702877872595, -0.1286294762887642, 0.0, 1.0, 47094.57120203454], 
processed observation next is [1.0, 0.17391304347826086, 0.18421052631578946, 0.685, 0.0, 0.0, 0.6666666666666666, 0.4339190648938291, 0.4571235079037453, 0.0, 1.0, 0.22425986286683114], 
reward next is 0.7757, 
noisyNet noise sample is [array([1.079989], dtype=float32), -1.8343718]. 
=============================================
[2019-04-04 07:41:06,864] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128000, global step 2049732: loss 0.1720
[2019-04-04 07:41:06,865] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128000, global step 2049732: learning rate 0.0000
[2019-04-04 07:41:07,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0098914e-10 6.7365735e-10 2.1464036e-23 1.9750471e-10 8.5991957e-11
 4.6874402e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:07,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6182
[2019-04-04 07:41:07,579] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.00550338770974, 0.2778597255165293, 0.0, 1.0, 43015.20856417853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 595200.0000, 
sim time next is 595800.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.002657330653, 0.2728347826545032, 0.0, 1.0, 43010.45263479936], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5835547775544168, 0.5909449275515011, 0.0, 1.0, 0.2048116792133303], 
reward next is 0.7952, 
noisyNet noise sample is [array([-1.8085485], dtype=float32), -1.3930799]. 
=============================================
[2019-04-04 07:41:08,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4259760e-09 2.1985826e-08 3.3781507e-20 3.3305823e-09 9.3793173e-10
 6.0870376e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:08,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9358
[2019-04-04 07:41:08,257] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.8, 52.0, 0.0, 0.0, 26.0, 22.75398344609933, -0.262915112693216, 0.0, 1.0, 46691.79055058275], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 448800.0000, 
sim time next is 449400.0000, 
raw observation next is [-10.7, 52.0, 0.0, 0.0, 26.0, 22.72961020272939, -0.271855484382738, 0.0, 1.0, 46717.23109055762], 
processed observation next is [1.0, 0.17391304347826086, 0.1662049861495845, 0.52, 0.0, 0.0, 0.6666666666666666, 0.3941341835607825, 0.40938150520575406, 0.0, 1.0, 0.22246300519313153], 
reward next is 0.7775, 
noisyNet noise sample is [array([0.898416], dtype=float32), -0.909788]. 
=============================================
[2019-04-04 07:41:08,608] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128000, global step 2050311: loss 0.1587
[2019-04-04 07:41:08,608] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128000, global step 2050311: learning rate 0.0000
[2019-04-04 07:41:09,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0751748e-11 2.1464833e-10 2.2505434e-24 2.3088691e-11 1.2249985e-12
 2.9521941e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:09,469] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5690
[2019-04-04 07:41:09,529] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 35.5, 32.0, 0.0, 26.0, 25.63991937534581, 0.1645693290090091, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 490200.0000, 
sim time next is 490800.0000, 
raw observation next is [1.1, 37.0, 26.0, 0.0, 26.0, 25.06583838558965, 0.178426747180553, 1.0, 1.0, 34638.81225628891], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.37, 0.08666666666666667, 0.0, 0.6666666666666666, 0.5888198654658042, 0.5594755823935177, 1.0, 1.0, 0.1649467250299472], 
reward next is 0.8351, 
noisyNet noise sample is [array([-0.353202], dtype=float32), -0.2213823]. 
=============================================
[2019-04-04 07:41:09,611] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1127534e-11 1.2324899e-10 2.8958892e-25 2.6059870e-11 2.0861706e-12
 6.5480890e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:09,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5449
[2019-04-04 07:41:09,677] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6000000000000001, 86.0, 0.0, 0.0, 26.0, 24.41755777659319, 0.185242059864534, 1.0, 1.0, 199595.3142240786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 497400.0000, 
sim time next is 498000.0000, 
raw observation next is [0.7000000000000001, 88.0, 0.0, 0.0, 26.0, 24.64567423507843, 0.211124418576759, 1.0, 1.0, 99170.26201165038], 
processed observation next is [1.0, 0.782608695652174, 0.4819944598337951, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5538061862565359, 0.570374806192253, 1.0, 1.0, 0.4722393429126209], 
reward next is 0.5278, 
noisyNet noise sample is [array([1.6073703], dtype=float32), 1.2084675]. 
=============================================
[2019-04-04 07:41:09,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[89.79661]
 [88.55306]
 [87.3851 ]
 [87.02148]
 [86.89655]], R is [[89.91926575]
 [89.06961823]
 [88.3115387 ]
 [88.20249939]
 [88.32047272]].
[2019-04-04 07:41:10,148] A3C_AGENT_WORKER-Thread-2 INFO:Local step 128500, global step 2050699: loss 2.1021
[2019-04-04 07:41:10,149] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 128500, global step 2050699: learning rate 0.0000
[2019-04-04 07:41:10,917] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128000, global step 2050858: loss 0.1170
[2019-04-04 07:41:10,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128000, global step 2050858: learning rate 0.0000
[2019-04-04 07:41:11,329] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128000, global step 2050955: loss 0.1248
[2019-04-04 07:41:11,337] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128000, global step 2050959: learning rate 0.0000
[2019-04-04 07:41:13,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4398672e-10 6.7196949e-11 4.6559180e-23 1.5847830e-11 1.8365474e-11
 5.1042720e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:13,046] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4641
[2019-04-04 07:41:13,101] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 87.0, 20.5, 22.5, 26.0, 24.98320799786399, 0.3050336883395189, 0.0, 1.0, 26381.24648879339], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 579600.0000, 
sim time next is 580200.0000, 
raw observation next is [-1.8, 87.0, 0.0, 0.0, 26.0, 24.98344802229088, 0.2928499985149806, 0.0, 1.0, 32535.5878971461], 
processed observation next is [0.0, 0.7391304347826086, 0.41274238227146814, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5819540018575733, 0.5976166661716602, 0.0, 1.0, 0.15493137093879095], 
reward next is 0.8451, 
noisyNet noise sample is [array([-0.42777485], dtype=float32), 1.0042392]. 
=============================================
[2019-04-04 07:41:15,201] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128000, global step 2051938: loss 0.1154
[2019-04-04 07:41:15,205] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128000, global step 2051938: learning rate 0.0000
[2019-04-04 07:41:15,401] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128000, global step 2051998: loss 0.1202
[2019-04-04 07:41:15,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128000, global step 2051998: learning rate 0.0000
[2019-04-04 07:41:15,995] A3C_AGENT_WORKER-Thread-12 INFO:Local step 128500, global step 2052175: loss 2.0566
[2019-04-04 07:41:15,998] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 128500, global step 2052175: learning rate 0.0000
[2019-04-04 07:41:17,523] A3C_AGENT_WORKER-Thread-5 INFO:Local step 128500, global step 2052710: loss 2.0579
[2019-04-04 07:41:17,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 128500, global step 2052711: learning rate 0.0000
[2019-04-04 07:41:18,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.6850416e-11 1.0068600e-10 6.9558694e-24 2.8221651e-11 3.7212313e-11
 2.9995940e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:18,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1460
[2019-04-04 07:41:18,959] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 84.33333333333333, 0.0, 0.0, 26.0, 25.11884961143441, 0.2999147592959887, 0.0, 1.0, 42885.29268548699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 592800.0000, 
sim time next is 593400.0000, 
raw observation next is [-2.8, 83.66666666666667, 0.0, 0.0, 26.0, 25.0987207397736, 0.2927237319419615, 0.0, 1.0, 42941.76721682448], 
processed observation next is [0.0, 0.8695652173913043, 0.38504155124653744, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5915600616478001, 0.5975745773139872, 0.0, 1.0, 0.2044846057944023], 
reward next is 0.7955, 
noisyNet noise sample is [array([-0.63243836], dtype=float32), 0.6902412]. 
=============================================
[2019-04-04 07:41:21,808] A3C_AGENT_WORKER-Thread-19 INFO:Local step 128500, global step 2053818: loss 2.0887
[2019-04-04 07:41:21,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 128500, global step 2053818: learning rate 0.0000
[2019-04-04 07:41:23,256] A3C_AGENT_WORKER-Thread-20 INFO:Local step 128500, global step 2054230: loss 2.0917
[2019-04-04 07:41:23,257] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 128500, global step 2054230: learning rate 0.0000
[2019-04-04 07:41:25,756] A3C_AGENT_WORKER-Thread-3 INFO:Local step 128500, global step 2055057: loss 2.0789
[2019-04-04 07:41:25,759] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 128500, global step 2055057: learning rate 0.0000
[2019-04-04 07:41:25,837] A3C_AGENT_WORKER-Thread-18 INFO:Local step 128500, global step 2055082: loss 2.0887
[2019-04-04 07:41:25,838] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 128500, global step 2055082: learning rate 0.0000
[2019-04-04 07:41:26,413] A3C_AGENT_WORKER-Thread-17 INFO:Local step 128500, global step 2055235: loss 2.0861
[2019-04-04 07:41:26,414] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 128500, global step 2055235: learning rate 0.0000
[2019-04-04 07:41:26,870] A3C_AGENT_WORKER-Thread-16 INFO:Local step 128500, global step 2055348: loss 2.0760
[2019-04-04 07:41:26,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 128500, global step 2055348: learning rate 0.0000
[2019-04-04 07:41:29,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2599253e-10 2.4740685e-09 2.3721585e-22 4.3224815e-10 2.3564842e-10
 1.3876049e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:29,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6325
[2019-04-04 07:41:29,195] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 55.00000000000001, 36.33333333333333, 18.83333333333333, 26.0, 24.8986967784748, 0.2153159390080196, 0.0, 1.0, 37991.63021781467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 663600.0000, 
sim time next is 664200.0000, 
raw observation next is [-0.8999999999999999, 55.5, 27.0, 15.0, 26.0, 24.88801076382936, 0.2118458221844686, 0.0, 1.0, 47468.55708739007], 
processed observation next is [0.0, 0.6956521739130435, 0.43767313019390586, 0.555, 0.09, 0.016574585635359115, 0.6666666666666666, 0.57400089698578, 0.5706152740614895, 0.0, 1.0, 0.2260407480351908], 
reward next is 0.7740, 
noisyNet noise sample is [array([-0.02520171], dtype=float32), 0.6657534]. 
=============================================
[2019-04-04 07:41:31,621] A3C_AGENT_WORKER-Thread-11 INFO:Local step 128500, global step 2056731: loss 2.0632
[2019-04-04 07:41:31,623] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 128500, global step 2056731: learning rate 0.0000
[2019-04-04 07:41:34,128] A3C_AGENT_WORKER-Thread-15 INFO:Local step 128500, global step 2057563: loss 2.0606
[2019-04-04 07:41:34,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 128500, global step 2057563: learning rate 0.0000
[2019-04-04 07:41:37,055] A3C_AGENT_WORKER-Thread-6 INFO:Local step 128500, global step 2058414: loss 2.0561
[2019-04-04 07:41:37,058] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 128500, global step 2058414: learning rate 0.0000
[2019-04-04 07:41:37,650] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129000, global step 2058603: loss 15.3804
[2019-04-04 07:41:37,651] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129000, global step 2058603: learning rate 0.0000
[2019-04-04 07:41:38,815] A3C_AGENT_WORKER-Thread-13 INFO:Local step 128500, global step 2058971: loss 2.0693
[2019-04-04 07:41:38,818] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 128500, global step 2058972: learning rate 0.0000
[2019-04-04 07:41:39,775] A3C_AGENT_WORKER-Thread-4 INFO:Local step 128500, global step 2059312: loss 2.0602
[2019-04-04 07:41:39,776] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 128500, global step 2059312: learning rate 0.0000
[2019-04-04 07:41:42,024] A3C_AGENT_WORKER-Thread-10 INFO:Local step 128500, global step 2060014: loss 2.0604
[2019-04-04 07:41:42,030] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 128500, global step 2060014: learning rate 0.0000
[2019-04-04 07:41:42,709] A3C_AGENT_WORKER-Thread-14 INFO:Local step 128500, global step 2060206: loss 2.0557
[2019-04-04 07:41:42,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 128500, global step 2060206: learning rate 0.0000
[2019-04-04 07:41:43,163] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129000, global step 2060325: loss 15.5775
[2019-04-04 07:41:43,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129000, global step 2060325: learning rate 0.0000
[2019-04-04 07:41:43,869] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129000, global step 2060528: loss 15.6362
[2019-04-04 07:41:43,869] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129000, global step 2060528: learning rate 0.0000
[2019-04-04 07:41:46,852] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129000, global step 2061616: loss 15.5456
[2019-04-04 07:41:46,852] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129000, global step 2061616: learning rate 0.0000
[2019-04-04 07:41:48,312] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8007144e-13 2.8095140e-13 5.4862646e-30 1.2589249e-13 8.8847615e-15
 8.0425770e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:48,313] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1243
[2019-04-04 07:41:48,319] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.77511016562056, 0.5307559688764998, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1017000.0000, 
sim time next is 1017600.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.65853643939625, 0.5225276721938997, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6382113699496875, 0.6741758907312999, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8188223], dtype=float32), -1.5210927]. 
=============================================
[2019-04-04 07:41:49,601] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129000, global step 2062766: loss 15.5019
[2019-04-04 07:41:49,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129000, global step 2062766: learning rate 0.0000
[2019-04-04 07:41:50,391] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129000, global step 2063064: loss 15.6222
[2019-04-04 07:41:50,394] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129000, global step 2063066: learning rate 0.0000
[2019-04-04 07:41:51,497] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129000, global step 2063586: loss 15.3864
[2019-04-04 07:41:51,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129000, global step 2063588: learning rate 0.0000
[2019-04-04 07:41:52,015] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129000, global step 2063811: loss 15.3654
[2019-04-04 07:41:52,016] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129000, global step 2063811: learning rate 0.0000
[2019-04-04 07:41:52,024] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129000, global step 2063817: loss 15.4285
[2019-04-04 07:41:52,025] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129000, global step 2063817: learning rate 0.0000
[2019-04-04 07:41:52,107] A3C_AGENT_WORKER-Thread-2 INFO:Local step 129500, global step 2063857: loss 0.1679
[2019-04-04 07:41:52,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 129500, global step 2063857: learning rate 0.0000
[2019-04-04 07:41:56,155] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129000, global step 2065654: loss 15.2341
[2019-04-04 07:41:56,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129000, global step 2065654: learning rate 0.0000
[2019-04-04 07:41:57,509] A3C_AGENT_WORKER-Thread-12 INFO:Local step 129500, global step 2066357: loss 0.1849
[2019-04-04 07:41:57,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 129500, global step 2066357: learning rate 0.0000
[2019-04-04 07:41:57,546] A3C_AGENT_WORKER-Thread-5 INFO:Local step 129500, global step 2066384: loss 0.1787
[2019-04-04 07:41:57,553] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 129500, global step 2066385: learning rate 0.0000
[2019-04-04 07:41:58,650] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129000, global step 2066945: loss 15.3693
[2019-04-04 07:41:58,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129000, global step 2066945: learning rate 0.0000
[2019-04-04 07:41:59,556] A3C_AGENT_WORKER-Thread-19 INFO:Local step 129500, global step 2067443: loss 0.1880
[2019-04-04 07:41:59,566] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 129500, global step 2067443: learning rate 0.0000
[2019-04-04 07:41:59,762] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8043090e-11 2.6078814e-11 5.1148027e-27 1.2062304e-11 1.1431643e-12
 1.4609480e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:41:59,765] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6797
[2019-04-04 07:41:59,811] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1833333333333333, 73.33333333333333, 19.33333333333334, 0.0, 26.0, 24.92862906911081, 0.2199286404121082, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 893400.0000, 
sim time next is 894000.0000, 
raw observation next is [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 26.0, 24.90942006954935, 0.2508762519185078, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4727608494921515, 0.7466666666666667, 0.08055555555555557, 0.0, 0.6666666666666666, 0.5757850057957791, 0.5836254173061692, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37604618], dtype=float32), -0.49977112]. 
=============================================
[2019-04-04 07:41:59,833] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[93.64428 ]
 [93.861565]
 [94.04891 ]
 [93.40553 ]
 [91.52738 ]], R is [[93.42003632]
 [93.48583984]
 [93.55097961]
 [93.61547089]
 [93.67931366]].
[2019-04-04 07:42:01,057] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129000, global step 2068251: loss 15.1940
[2019-04-04 07:42:01,060] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129000, global step 2068253: learning rate 0.0000
[2019-04-04 07:42:01,540] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9911610e-11 3.1012332e-11 5.8746989e-26 2.3255876e-12 7.8322478e-13
 3.1958521e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:01,543] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3554
[2019-04-04 07:42:01,573] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.933333333333334, 96.0, 0.0, 0.0, 26.0, 24.70595252723841, 0.4436874704457701, 0.0, 1.0, 22475.70143230641], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1275600.0000, 
sim time next is 1276200.0000, 
raw observation next is [7.75, 96.0, 0.0, 0.0, 26.0, 24.69452020864159, 0.4428092080384611, 0.0, 1.0, 33316.79794061215], 
processed observation next is [0.0, 0.782608695652174, 0.6772853185595569, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5578766840534657, 0.6476030693461537, 0.0, 1.0, 0.15865141876481978], 
reward next is 0.8413, 
noisyNet noise sample is [array([0.23370178], dtype=float32), -1.6797898]. 
=============================================
[2019-04-04 07:42:02,559] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129000, global step 2069024: loss 15.1394
[2019-04-04 07:42:02,560] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129000, global step 2069025: learning rate 0.0000
[2019-04-04 07:42:02,740] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129000, global step 2069136: loss 15.2600
[2019-04-04 07:42:02,742] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129000, global step 2069136: learning rate 0.0000
[2019-04-04 07:42:02,919] A3C_AGENT_WORKER-Thread-20 INFO:Local step 129500, global step 2069247: loss 0.1862
[2019-04-04 07:42:02,921] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 129500, global step 2069249: learning rate 0.0000
[2019-04-04 07:42:03,412] A3C_AGENT_WORKER-Thread-18 INFO:Local step 129500, global step 2069538: loss 0.1753
[2019-04-04 07:42:03,413] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 129500, global step 2069539: learning rate 0.0000
[2019-04-04 07:42:04,903] A3C_AGENT_WORKER-Thread-16 INFO:Local step 129500, global step 2070351: loss 0.1492
[2019-04-04 07:42:04,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 129500, global step 2070351: learning rate 0.0000
[2019-04-04 07:42:04,919] A3C_AGENT_WORKER-Thread-3 INFO:Local step 129500, global step 2070358: loss 0.1467
[2019-04-04 07:42:04,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 129500, global step 2070358: learning rate 0.0000
[2019-04-04 07:42:05,105] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129000, global step 2070444: loss 15.2558
[2019-04-04 07:42:05,110] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129000, global step 2070446: learning rate 0.0000
[2019-04-04 07:42:05,304] A3C_AGENT_WORKER-Thread-17 INFO:Local step 129500, global step 2070546: loss 0.1510
[2019-04-04 07:42:05,312] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 129500, global step 2070546: learning rate 0.0000
[2019-04-04 07:42:05,485] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6961666e-12 3.4248715e-11 3.7760707e-26 6.1934581e-12 1.9170746e-12
 8.8128628e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:05,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7023
[2019-04-04 07:42:05,504] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 99.33333333333334, 0.0, 0.0, 26.0, 25.05745246748339, 0.3817582680037359, 0.0, 1.0, 41148.35474733582], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 940200.0000, 
sim time next is 940800.0000, 
raw observation next is [5.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.09173173021423, 0.3861298187811557, 0.0, 1.0, 40655.88188712835], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5909776441845193, 0.6287099395937186, 0.0, 1.0, 0.19359943755775405], 
reward next is 0.8064, 
noisyNet noise sample is [array([-0.00441757], dtype=float32), 0.52135324]. 
=============================================
[2019-04-04 07:42:05,637] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129000, global step 2070726: loss 15.0935
[2019-04-04 07:42:05,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129000, global step 2070726: learning rate 0.0000
[2019-04-04 07:42:08,647] A3C_AGENT_WORKER-Thread-11 INFO:Local step 129500, global step 2072386: loss 0.2135
[2019-04-04 07:42:08,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 129500, global step 2072388: learning rate 0.0000
[2019-04-04 07:42:09,212] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.2458966e-12 2.2800165e-11 2.9344657e-25 6.4772450e-12 2.4020931e-12
 2.3193647e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:09,212] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6942
[2019-04-04 07:42:09,295] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 36.66666666666667, 0.0, 26.0, 25.3902400130752, 0.4281415742427714, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1439400.0000, 
sim time next is 1440000.0000, 
raw observation next is [1.1, 92.0, 32.0, 0.0, 26.0, 24.68832476926903, 0.4062036376556888, 1.0, 1.0, 196601.4990475629], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.10666666666666667, 0.0, 0.6666666666666666, 0.5573603974390858, 0.6354012125518963, 1.0, 1.0, 0.9361976145122042], 
reward next is 0.0638, 
noisyNet noise sample is [array([0.13948685], dtype=float32), -0.5877084]. 
=============================================
[2019-04-04 07:42:09,299] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[86.99196 ]
 [87.1654  ]
 [87.323586]
 [87.41337 ]
 [87.49693 ]], R is [[87.0582962 ]
 [87.18771362]
 [87.31583405]
 [87.44267273]
 [87.56824493]].
[2019-04-04 07:42:09,341] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130000, global step 2072769: loss 2.0578
[2019-04-04 07:42:09,345] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130000, global step 2072772: learning rate 0.0000
[2019-04-04 07:42:10,594] A3C_AGENT_WORKER-Thread-15 INFO:Local step 129500, global step 2073400: loss 0.2091
[2019-04-04 07:42:10,597] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 129500, global step 2073400: learning rate 0.0000
[2019-04-04 07:42:13,387] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0255816e-11 5.2344219e-11 8.6938793e-25 3.8702045e-12 1.9652112e-12
 7.3002975e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:13,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4255
[2019-04-04 07:42:13,406] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.35270305277118, 0.510984316300366, 0.0, 1.0, 37502.10951607786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1470600.0000, 
sim time next is 1471200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.54763399325637, 0.5095550608457321, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6289694994380307, 0.6698516869485774, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4189947], dtype=float32), 0.20215218]. 
=============================================
[2019-04-04 07:42:13,623] A3C_AGENT_WORKER-Thread-6 INFO:Local step 129500, global step 2075057: loss 0.2147
[2019-04-04 07:42:13,626] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 129500, global step 2075058: learning rate 0.0000
[2019-04-04 07:42:14,826] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130000, global step 2075723: loss 2.0631
[2019-04-04 07:42:14,827] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130000, global step 2075723: learning rate 0.0000
[2019-04-04 07:42:14,930] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130000, global step 2075785: loss 2.0260
[2019-04-04 07:42:14,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130000, global step 2075785: learning rate 0.0000
[2019-04-04 07:42:15,236] A3C_AGENT_WORKER-Thread-13 INFO:Local step 129500, global step 2075964: loss 0.2019
[2019-04-04 07:42:15,237] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 129500, global step 2075964: learning rate 0.0000
[2019-04-04 07:42:15,381] A3C_AGENT_WORKER-Thread-4 INFO:Local step 129500, global step 2076036: loss 0.1976
[2019-04-04 07:42:15,396] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 129500, global step 2076040: learning rate 0.0000
[2019-04-04 07:42:16,813] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130000, global step 2076755: loss 2.0777
[2019-04-04 07:42:16,816] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130000, global step 2076756: learning rate 0.0000
[2019-04-04 07:42:17,964] A3C_AGENT_WORKER-Thread-10 INFO:Local step 129500, global step 2077331: loss 0.1781
[2019-04-04 07:42:17,966] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 129500, global step 2077331: learning rate 0.0000
[2019-04-04 07:42:18,123] A3C_AGENT_WORKER-Thread-14 INFO:Local step 129500, global step 2077411: loss 0.1813
[2019-04-04 07:42:18,123] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 129500, global step 2077411: learning rate 0.0000
[2019-04-04 07:42:18,129] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0533874e-12 4.3995077e-12 7.7920653e-27 5.9125643e-13 1.1467387e-13
 6.5404967e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:18,133] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-04 07:42:18,154] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.06666666666667, 51.66666666666667, 153.5, 103.3333333333333, 26.0, 26.71843497170012, 0.7474405578072387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1600800.0000, 
sim time next is 1601400.0000, 
raw observation next is [13.43333333333334, 50.33333333333334, 158.0, 82.66666666666667, 26.0, 25.89628161470464, 0.6939506492156906, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8347183748845802, 0.5033333333333334, 0.5266666666666666, 0.09134438305709025, 0.6666666666666666, 0.6580234678920535, 0.7313168830718969, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.34863108], dtype=float32), -0.8246863]. 
=============================================
[2019-04-04 07:42:18,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4619258e-11 1.2356866e-10 1.5368808e-25 1.4172046e-11 5.8348409e-12
 2.1202663e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:18,788] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1050
[2019-04-04 07:42:18,813] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.36637952735969, 0.4476490250655195, 0.0, 1.0, 57751.85611704543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483200.0000, 
sim time next is 1483800.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31714148285139, 0.4543666885083774, 0.0, 1.0, 47884.2282630358], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6097617902376159, 0.6514555628361258, 0.0, 1.0, 0.22802013458588477], 
reward next is 0.7720, 
noisyNet noise sample is [array([-1.6600053], dtype=float32), -0.648155]. 
=============================================
[2019-04-04 07:42:20,497] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130000, global step 2078558: loss 2.2300
[2019-04-04 07:42:20,497] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130000, global step 2078558: learning rate 0.0000
[2019-04-04 07:42:20,655] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130000, global step 2078633: loss 2.2131
[2019-04-04 07:42:20,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130000, global step 2078634: learning rate 0.0000
[2019-04-04 07:42:21,619] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130000, global step 2079150: loss 2.3736
[2019-04-04 07:42:21,620] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130000, global step 2079150: learning rate 0.0000
[2019-04-04 07:42:21,717] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130000, global step 2079214: loss 2.3486
[2019-04-04 07:42:21,718] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130000, global step 2079214: learning rate 0.0000
[2019-04-04 07:42:21,909] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0397161e-11 9.6724670e-11 9.0601544e-26 5.0956006e-12 1.5306014e-12
 8.1454059e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:21,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0859
[2019-04-04 07:42:21,953] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.41666666666667, 58.33333333333334, 30.33333333333333, 13.33333333333333, 26.0, 26.76207434458045, 0.7309271535362023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1530600.0000, 
sim time next is 1531200.0000, 
raw observation next is [10.33333333333333, 58.66666666666667, 0.0, 0.0, 26.0, 26.91343028335547, 0.6978328815122944, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7488457987072946, 0.5866666666666667, 0.0, 0.0, 0.6666666666666666, 0.7427858569462892, 0.7326109605040981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5389418], dtype=float32), -0.63838214]. 
=============================================
[2019-04-04 07:42:22,148] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130000, global step 2079420: loss 2.3598
[2019-04-04 07:42:22,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130000, global step 2079420: learning rate 0.0000
[2019-04-04 07:42:23,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8177797e-12 1.1893996e-11 1.0766338e-26 1.3615658e-12 3.0645500e-13
 1.2475628e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:23,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9522
[2019-04-04 07:42:23,624] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 176.0, 0.0, 26.0, 26.92384879918462, 0.8084208514286141, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1603800.0000, 
sim time next is 1604400.0000, 
raw observation next is [13.8, 49.0, 170.8333333333333, 0.0, 26.0, 27.06728734039572, 0.8296891560218187, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.844875346260388, 0.49, 0.5694444444444443, 0.0, 0.6666666666666666, 0.7556072783663099, 0.7765630520072729, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9849474], dtype=float32), 1.301247]. 
=============================================
[2019-04-04 07:42:25,703] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130000, global step 2080997: loss 2.2593
[2019-04-04 07:42:25,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130000, global step 2080997: learning rate 0.0000
[2019-04-04 07:42:27,047] A3C_AGENT_WORKER-Thread-2 INFO:Local step 130500, global step 2081660: loss 0.0001
[2019-04-04 07:42:27,048] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 130500, global step 2081661: learning rate 0.0000
[2019-04-04 07:42:27,721] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130000, global step 2081974: loss 2.3226
[2019-04-04 07:42:27,722] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130000, global step 2081974: learning rate 0.0000
[2019-04-04 07:42:31,271] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130000, global step 2083268: loss 2.3978
[2019-04-04 07:42:31,272] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130000, global step 2083268: learning rate 0.0000
[2019-04-04 07:42:31,465] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4701454e-10 2.4497390e-10 2.1451501e-24 2.8160568e-11 2.0397005e-11
 8.7851757e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:31,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0616
[2019-04-04 07:42:31,496] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 93.5, 0.0, 0.0, 26.0, 25.36254282977009, 0.4795915284125747, 0.0, 1.0, 45727.17319783988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1726200.0000, 
sim time next is 1726800.0000, 
raw observation next is [0.3333333333333333, 93.0, 0.0, 0.0, 26.0, 25.35972591543482, 0.4775027574178108, 0.0, 1.0, 44091.09238731928], 
processed observation next is [1.0, 1.0, 0.4718374884579871, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6133104929529015, 0.6591675858059369, 0.0, 1.0, 0.20995758279675847], 
reward next is 0.7900, 
noisyNet noise sample is [array([0.6709882], dtype=float32), -0.19716139]. 
=============================================
[2019-04-04 07:42:32,792] A3C_AGENT_WORKER-Thread-5 INFO:Local step 130500, global step 2084016: loss 0.0021
[2019-04-04 07:42:32,793] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 130500, global step 2084016: learning rate 0.0000
[2019-04-04 07:42:32,908] A3C_AGENT_WORKER-Thread-12 INFO:Local step 130500, global step 2084068: loss 0.0023
[2019-04-04 07:42:32,910] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 130500, global step 2084070: learning rate 0.0000
[2019-04-04 07:42:33,214] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130000, global step 2084201: loss 2.5280
[2019-04-04 07:42:33,220] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130000, global step 2084201: learning rate 0.0000
[2019-04-04 07:42:33,417] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130000, global step 2084298: loss 2.5110
[2019-04-04 07:42:33,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130000, global step 2084298: learning rate 0.0000
[2019-04-04 07:42:35,258] A3C_AGENT_WORKER-Thread-19 INFO:Local step 130500, global step 2085009: loss 0.0041
[2019-04-04 07:42:35,263] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 130500, global step 2085009: learning rate 0.0000
[2019-04-04 07:42:35,295] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130000, global step 2085019: loss 2.5461
[2019-04-04 07:42:35,309] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130000, global step 2085019: learning rate 0.0000
[2019-04-04 07:42:36,356] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130000, global step 2085366: loss 2.6815
[2019-04-04 07:42:36,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130000, global step 2085366: learning rate 0.0000
[2019-04-04 07:42:36,762] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6776360e-09 2.9681688e-09 4.2255746e-21 1.2084700e-09 5.9824401e-10
 2.1984146e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:36,765] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3558
[2019-04-04 07:42:36,820] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 79.0, 29.0, 0.0, 26.0, 25.04282917204261, 0.2619375271604173, 0.0, 1.0, 26588.13201205072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1873800.0000, 
sim time next is 1874400.0000, 
raw observation next is [-4.5, 80.33333333333333, 24.33333333333334, 0.0, 26.0, 25.04510848708372, 0.255459612836952, 0.0, 1.0, 32612.02636076955], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.8033333333333332, 0.08111111111111113, 0.0, 0.6666666666666666, 0.5870923739236433, 0.585153204278984, 0.0, 1.0, 0.15529536362271212], 
reward next is 0.8447, 
noisyNet noise sample is [array([-1.395286], dtype=float32), 0.84013456]. 
=============================================
[2019-04-04 07:42:38,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5181841e-11 9.1635616e-11 6.5363511e-25 1.9043239e-11 3.8884360e-12
 1.4734860e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:38,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6644
[2019-04-04 07:42:38,492] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.699999999999999, 84.33333333333334, 0.0, 0.0, 26.0, 26.07773941404336, 0.7180296338465721, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1633800.0000, 
sim time next is 1634400.0000, 
raw observation next is [6.6, 86.0, 0.0, 0.0, 26.0, 26.06690919481409, 0.7110992303113286, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6454293628808865, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6722424329011742, 0.7370330767704428, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3184069], dtype=float32), 0.62115824]. 
=============================================
[2019-04-04 07:42:38,676] A3C_AGENT_WORKER-Thread-20 INFO:Local step 130500, global step 2086312: loss 0.0023
[2019-04-04 07:42:38,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 130500, global step 2086313: learning rate 0.0000
[2019-04-04 07:42:39,229] A3C_AGENT_WORKER-Thread-18 INFO:Local step 130500, global step 2086576: loss 0.0018
[2019-04-04 07:42:39,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 130500, global step 2086577: learning rate 0.0000
[2019-04-04 07:42:40,143] A3C_AGENT_WORKER-Thread-17 INFO:Local step 130500, global step 2087011: loss 0.0034
[2019-04-04 07:42:40,144] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 130500, global step 2087011: learning rate 0.0000
[2019-04-04 07:42:40,280] A3C_AGENT_WORKER-Thread-3 INFO:Local step 130500, global step 2087062: loss 0.0039
[2019-04-04 07:42:40,284] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 130500, global step 2087062: learning rate 0.0000
[2019-04-04 07:42:40,761] A3C_AGENT_WORKER-Thread-16 INFO:Local step 130500, global step 2087234: loss 0.0022
[2019-04-04 07:42:40,770] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 130500, global step 2087234: learning rate 0.0000
[2019-04-04 07:42:44,146] A3C_AGENT_WORKER-Thread-11 INFO:Local step 130500, global step 2088275: loss 0.0036
[2019-04-04 07:42:44,147] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 130500, global step 2088275: learning rate 0.0000
[2019-04-04 07:42:46,536] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.15273044e-08 8.39343794e-09 1.46540882e-21 4.48122428e-09
 7.79611653e-10 2.21246025e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 07:42:46,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0058
[2019-04-04 07:42:46,551] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.199999999999999, 87.0, 0.0, 0.0, 26.0, 24.21321926536091, 0.06389952976119186, 0.0, 1.0, 41079.61300070263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2007600.0000, 
sim time next is 2008200.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574456735, 0.0587030164282273, 0.0, 1.0, 41111.63103505894], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5132646453806124, 0.5195676721427425, 0.0, 1.0, 0.19576967159551878], 
reward next is 0.8042, 
noisyNet noise sample is [array([0.33018172], dtype=float32), -0.83077085]. 
=============================================
[2019-04-04 07:42:46,901] A3C_AGENT_WORKER-Thread-15 INFO:Local step 130500, global step 2089270: loss 0.0022
[2019-04-04 07:42:46,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 130500, global step 2089270: learning rate 0.0000
[2019-04-04 07:42:47,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4558735e-09 1.3763077e-08 1.1653396e-20 1.2637390e-09 9.5536412e-10
 2.4271932e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:47,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6260
[2019-04-04 07:42:47,510] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 83.50000000000001, 0.0, 0.0, 26.0, 23.28357726944897, -0.1585070296034638, 0.0, 1.0, 44772.84372460052], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1923000.0000, 
sim time next is 1923600.0000, 
raw observation next is [-9.100000000000001, 85.0, 0.0, 0.0, 26.0, 23.25883988788627, -0.1686694702427577, 0.0, 1.0, 44687.21542111969], 
processed observation next is [1.0, 0.2608695652173913, 0.21052631578947364, 0.85, 0.0, 0.0, 0.6666666666666666, 0.4382366573238559, 0.4437768432524141, 0.0, 1.0, 0.21279626391009376], 
reward next is 0.7872, 
noisyNet noise sample is [array([0.61633354], dtype=float32), 0.61539173]. 
=============================================
[2019-04-04 07:42:50,776] A3C_AGENT_WORKER-Thread-6 INFO:Local step 130500, global step 2090394: loss 0.0030
[2019-04-04 07:42:50,777] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 130500, global step 2090394: learning rate 0.0000
[2019-04-04 07:42:51,900] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131000, global step 2090703: loss 0.7462
[2019-04-04 07:42:51,900] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131000, global step 2090703: learning rate 0.0000
[2019-04-04 07:42:52,769] A3C_AGENT_WORKER-Thread-4 INFO:Local step 130500, global step 2090960: loss 0.0015
[2019-04-04 07:42:52,772] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 130500, global step 2090963: learning rate 0.0000
[2019-04-04 07:42:52,837] A3C_AGENT_WORKER-Thread-13 INFO:Local step 130500, global step 2090988: loss 0.0015
[2019-04-04 07:42:52,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 130500, global step 2090997: learning rate 0.0000
[2019-04-04 07:42:54,844] A3C_AGENT_WORKER-Thread-10 INFO:Local step 130500, global step 2091749: loss 0.0010
[2019-04-04 07:42:54,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 130500, global step 2091749: learning rate 0.0000
[2019-04-04 07:42:55,866] A3C_AGENT_WORKER-Thread-14 INFO:Local step 130500, global step 2092020: loss 0.0006
[2019-04-04 07:42:55,867] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 130500, global step 2092020: learning rate 0.0000
[2019-04-04 07:42:56,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3241698e-11 1.2645292e-10 2.4240508e-24 2.2173366e-11 3.3274594e-12
 2.7437184e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:42:56,099] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8357
[2019-04-04 07:42:56,148] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 79.0, 133.3333333333333, 0.0, 26.0, 26.13050848547026, 0.4760679570549104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2037000.0000, 
sim time next is 2037600.0000, 
raw observation next is [-3.9, 79.0, 126.0, 0.0, 26.0, 26.29660625242786, 0.4902889879887155, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.79, 0.42, 0.0, 0.6666666666666666, 0.6913838543689884, 0.6634296626629052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1945356], dtype=float32), -0.82472956]. 
=============================================
[2019-04-04 07:42:58,693] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131000, global step 2092788: loss 0.6601
[2019-04-04 07:42:58,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131000, global step 2092788: learning rate 0.0000
[2019-04-04 07:42:59,170] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131000, global step 2092904: loss 0.6567
[2019-04-04 07:42:59,171] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131000, global step 2092904: learning rate 0.0000
[2019-04-04 07:43:00,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.11525580e-10 6.77131684e-10 1.15592956e-23 5.53528820e-11
 2.18508441e-11 7.63234745e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 07:43:00,269] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3536
[2019-04-04 07:43:00,348] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.00858437287111, 0.33764734937311, 1.0, 1.0, 141966.6330281707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2058600.0000, 
sim time next is 2059200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.91251592916637, 0.3660793151504452, 0.0, 1.0, 152091.1019251902], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5760429940971976, 0.6220264383834817, 0.0, 1.0, 0.7242433425009056], 
reward next is 0.2758, 
noisyNet noise sample is [array([-0.39011115], dtype=float32), 0.22613302]. 
=============================================
[2019-04-04 07:43:01,531] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131000, global step 2093687: loss 0.6380
[2019-04-04 07:43:01,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131000, global step 2093687: learning rate 0.0000
[2019-04-04 07:43:02,094] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2325753e-09 2.6811438e-09 4.6669274e-22 2.3178007e-10 2.6089922e-10
 5.2993618e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:43:02,094] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1359
[2019-04-04 07:43:02,109] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.12438756386508, 0.07403210653297965, 0.0, 1.0, 42051.50385924979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2176200.0000, 
sim time next is 2176800.0000, 
raw observation next is [-6.366666666666667, 76.0, 0.0, 0.0, 26.0, 24.0649240530966, 0.0649092266023231, 0.0, 1.0, 42038.93308073103], 
processed observation next is [1.0, 0.17391304347826086, 0.28624192059095105, 0.76, 0.0, 0.0, 0.6666666666666666, 0.50541033775805, 0.521636408867441, 0.0, 1.0, 0.20018539562252874], 
reward next is 0.7998, 
noisyNet noise sample is [array([-0.44667378], dtype=float32), -2.3566065]. 
=============================================
[2019-04-04 07:43:05,399] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131000, global step 2094865: loss 0.6115
[2019-04-04 07:43:05,399] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131000, global step 2094865: learning rate 0.0000
[2019-04-04 07:43:05,902] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131000, global step 2095017: loss 0.5933
[2019-04-04 07:43:05,904] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131000, global step 2095017: learning rate 0.0000
[2019-04-04 07:43:06,582] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131000, global step 2095222: loss 0.5407
[2019-04-04 07:43:06,583] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131000, global step 2095222: learning rate 0.0000
[2019-04-04 07:43:07,129] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131000, global step 2095375: loss 0.4869
[2019-04-04 07:43:07,143] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131000, global step 2095381: learning rate 0.0000
[2019-04-04 07:43:07,212] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131000, global step 2095401: loss 0.5030
[2019-04-04 07:43:07,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131000, global step 2095401: learning rate 0.0000
[2019-04-04 07:43:10,437] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131000, global step 2096586: loss 0.5326
[2019-04-04 07:43:10,438] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131000, global step 2096586: learning rate 0.0000
[2019-04-04 07:43:10,631] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.3711714e-12 3.8804363e-11 5.9149082e-25 4.8858226e-12 2.7865457e-12
 5.1631372e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:43:10,631] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7914
[2019-04-04 07:43:10,679] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 82.66666666666667, 58.0, 0.0, 26.0, 25.24049646352648, 0.3828629981204698, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2044200.0000, 
sim time next is 2044800.0000, 
raw observation next is [-3.9, 82.0, 51.5, 0.0, 26.0, 25.6591874945291, 0.4100955332805907, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.17166666666666666, 0.0, 0.6666666666666666, 0.6382656245440916, 0.6366985110935303, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2431199], dtype=float32), 0.2107343]. 
=============================================
[2019-04-04 07:43:11,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2964811e-10 1.0211531e-09 5.4107755e-23 6.9269208e-11 2.0286004e-11
 3.3665589e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:43:11,573] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0419
[2019-04-04 07:43:11,630] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.600000000000001, 83.0, 0.0, 0.0, 26.0, 25.03829442769483, 0.3901295052077132, 0.0, 1.0, 72001.69722823374], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2146200.0000, 
sim time next is 2146800.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.09447048694729, 0.3992791467493133, 0.0, 1.0, 109835.5440945107], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5912058739122742, 0.6330930489164378, 0.0, 1.0, 0.5230264004500509], 
reward next is 0.4770, 
noisyNet noise sample is [array([-0.43445364], dtype=float32), -1.272582]. 
=============================================
[2019-04-04 07:43:13,431] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131000, global step 2097355: loss 0.5479
[2019-04-04 07:43:13,432] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131000, global step 2097355: learning rate 0.0000
[2019-04-04 07:43:16,303] A3C_AGENT_WORKER-Thread-2 INFO:Local step 131500, global step 2098168: loss 0.0006
[2019-04-04 07:43:16,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 131500, global step 2098168: learning rate 0.0000
[2019-04-04 07:43:18,061] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131000, global step 2098696: loss 0.6450
[2019-04-04 07:43:18,082] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131000, global step 2098696: learning rate 0.0000
[2019-04-04 07:43:19,711] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131000, global step 2099083: loss 0.6367
[2019-04-04 07:43:19,712] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131000, global step 2099083: learning rate 0.0000
[2019-04-04 07:43:21,232] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131000, global step 2099426: loss 0.5562
[2019-04-04 07:43:21,233] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131000, global step 2099426: learning rate 0.0000
[2019-04-04 07:43:21,855] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131000, global step 2099585: loss 0.4955
[2019-04-04 07:43:21,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131000, global step 2099585: learning rate 0.0000
[2019-04-04 07:43:23,277] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 07:43:23,290] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:43:23,290] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:43:23,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run22
[2019-04-04 07:43:23,322] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:43:23,323] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:43:23,323] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:43:23,324] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:43:23,327] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run22
[2019-04-04 07:43:23,356] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run22
[2019-04-04 07:44:29,071] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21036714], dtype=float32), -0.08788652]
[2019-04-04 07:44:29,071] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.6, 81.0, 0.0, 0.0, 26.0, 25.35655151456546, 0.3798270355019502, 0.0, 1.0, 51466.7741821065]
[2019-04-04 07:44:29,071] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:44:29,072] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.0367340e-10 3.4445727e-10 9.1648992e-24 6.4055358e-11 4.8440415e-11
 1.0426686e-13 1.0000000e+00], sampled 0.7963869169373855
[2019-04-04 07:44:29,119] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21036714], dtype=float32), -0.08788652]
[2019-04-04 07:44:29,120] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [17.65877959666667, 60.54293427333333, 218.2881420816667, 0.0, 26.0, 26.55433200323621, 0.7971458959267674, 1.0, 0.0, 0.0]
[2019-04-04 07:44:29,120] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:44:29,121] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.2645189e-11 8.2376935e-12 6.8871840e-27 2.7393446e-12 4.5991536e-13
 6.4445755e-16 1.0000000e+00], sampled 0.1881079053225101
[2019-04-04 07:45:07,009] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21036714], dtype=float32), -0.08788652]
[2019-04-04 07:45:07,009] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-7.744378965833334, 85.08147811833332, 0.0, 0.0, 26.0, 24.48859938652512, 0.1347437455663827, 0.0, 1.0, 40512.21869298568]
[2019-04-04 07:45:07,009] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:45:07,010] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.5195153e-09 2.1807398e-09 1.9318499e-22 2.8030203e-10 2.6681132e-10
 8.8309432e-13 1.0000000e+00], sampled 0.7562492218696438
[2019-04-04 07:46:12,263] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21036714], dtype=float32), -0.08788652]
[2019-04-04 07:46:12,264] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.4, 56.66666666666667, 46.66666666666666, 275.6666666666666, 26.0, 25.87759619761776, 0.4978622637735812, 1.0, 1.0, 0.0]
[2019-04-04 07:46:12,264] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 07:46:12,265] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.1381551e-11 1.0572633e-10 1.5355111e-23 2.1691845e-11 5.7397689e-12
 5.1172957e-14 1.0000000e+00], sampled 0.7344487906289379
[2019-04-04 07:46:27,569] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.21036714], dtype=float32), -0.08788652]
[2019-04-04 07:46:27,569] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-4.333333333333334, 38.0, 102.1666666666667, 777.3333333333334, 26.0, 26.98063411735293, 0.7798111095581369, 1.0, 1.0, 0.0]
[2019-04-04 07:46:27,569] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:46:27,570] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.5470230e-11 7.1133502e-11 7.1978114e-24 1.5493830e-11 3.7010208e-12
 2.3702506e-14 1.0000000e+00], sampled 0.6335176233767553
[2019-04-04 07:46:32,893] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 07:47:01,089] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:47:05,909] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:47:06,942] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2100000, evaluation results [2100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:47:07,787] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131000, global step 2100202: loss 0.4641
[2019-04-04 07:47:07,788] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131000, global step 2100202: learning rate 0.0000
[2019-04-04 07:47:09,111] A3C_AGENT_WORKER-Thread-12 INFO:Local step 131500, global step 2100592: loss 0.0004
[2019-04-04 07:47:09,114] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 131500, global step 2100593: learning rate 0.0000
[2019-04-04 07:47:09,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1208195e-11 2.1825521e-11 1.3260168e-25 6.6899329e-12 1.1733692e-12
 3.3984131e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:47:09,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1172
[2019-04-04 07:47:09,539] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.533333333333334, 54.83333333333334, 107.6666666666667, 28.0, 26.0, 25.64517903464868, 0.2743400407364544, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2538600.0000, 
sim time next is 2539200.0000, 
raw observation next is [-2.266666666666667, 53.66666666666667, 121.8333333333333, 30.5, 26.0, 25.65561870693443, 0.2824691061484974, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.39981532779316714, 0.5366666666666667, 0.406111111111111, 0.03370165745856354, 0.6666666666666666, 0.6379682255778691, 0.5941563687161658, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2694331], dtype=float32), 0.9990716]. 
=============================================
[2019-04-04 07:47:09,767] A3C_AGENT_WORKER-Thread-5 INFO:Local step 131500, global step 2100792: loss 0.0003
[2019-04-04 07:47:09,768] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 131500, global step 2100792: learning rate 0.0000
[2019-04-04 07:47:12,938] A3C_AGENT_WORKER-Thread-19 INFO:Local step 131500, global step 2101525: loss 0.0002
[2019-04-04 07:47:12,986] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 131500, global step 2101530: learning rate 0.0000
[2019-04-04 07:47:14,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.1861240e-10 4.9531318e-10 4.0562902e-22 4.4214429e-10 2.1823698e-10
 2.3789206e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:47:14,950] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1821
[2019-04-04 07:47:15,030] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.449999999999999, 46.5, 61.0, 665.0, 26.0, 25.30083131482226, 0.2563186569058648, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2453400.0000, 
sim time next is 2454000.0000, 
raw observation next is [-6.166666666666666, 45.33333333333333, 63.5, 683.6666666666667, 26.0, 25.26752740538911, 0.2518065774858636, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.29178208679593726, 0.4533333333333333, 0.21166666666666667, 0.7554327808471456, 0.6666666666666666, 0.6056272837824258, 0.5839355258286212, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97569627], dtype=float32), 0.49076417]. 
=============================================
[2019-04-04 07:47:15,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[76.76484]
 [76.6818 ]
 [76.54992]
 [76.40771]
 [76.27943]], R is [[76.93249512]
 [77.16316986]
 [77.39154053]
 [77.61762238]
 [77.84144592]].
[2019-04-04 07:47:17,950] A3C_AGENT_WORKER-Thread-20 INFO:Local step 131500, global step 2102795: loss 0.0006
[2019-04-04 07:47:17,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 131500, global step 2102795: learning rate 0.0000
[2019-04-04 07:47:18,173] A3C_AGENT_WORKER-Thread-18 INFO:Local step 131500, global step 2102868: loss 0.0004
[2019-04-04 07:47:18,176] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 131500, global step 2102868: learning rate 0.0000
[2019-04-04 07:47:19,128] A3C_AGENT_WORKER-Thread-16 INFO:Local step 131500, global step 2103136: loss 0.0005
[2019-04-04 07:47:19,129] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 131500, global step 2103136: learning rate 0.0000
[2019-04-04 07:47:19,561] A3C_AGENT_WORKER-Thread-17 INFO:Local step 131500, global step 2103248: loss 0.0007
[2019-04-04 07:47:19,562] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 131500, global step 2103248: learning rate 0.0000
[2019-04-04 07:47:20,003] A3C_AGENT_WORKER-Thread-3 INFO:Local step 131500, global step 2103355: loss 0.0006
[2019-04-04 07:47:20,077] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 131500, global step 2103359: learning rate 0.0000
[2019-04-04 07:47:23,694] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2139677e-10 2.9513894e-10 2.3144301e-25 2.2135505e-11 3.7714333e-12
 6.6286671e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:47:23,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8236
[2019-04-04 07:47:23,736] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.733333333333333, 51.33333333333333, 135.5, 35.0, 26.0, 25.75344469288618, 0.3003357389352001, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2540400.0000, 
sim time next is 2541000.0000, 
raw observation next is [-1.466666666666667, 50.16666666666667, 135.0, 37.0, 26.0, 25.79614832423049, 0.3046325756474622, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.42197599261311175, 0.5016666666666667, 0.45, 0.04088397790055249, 0.6666666666666666, 0.6496790270192075, 0.6015441918824874, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0955999], dtype=float32), 0.23285528]. 
=============================================
[2019-04-04 07:47:23,742] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[89.72129 ]
 [89.801094]
 [89.61971 ]
 [89.435936]
 [89.265205]], R is [[89.61104584]
 [89.7149353 ]
 [89.81778717]
 [89.91960907]
 [90.02041626]].
[2019-04-04 07:47:24,507] A3C_AGENT_WORKER-Thread-11 INFO:Local step 131500, global step 2104517: loss 0.0025
[2019-04-04 07:47:24,507] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 131500, global step 2104517: learning rate 0.0000
[2019-04-04 07:47:28,479] A3C_AGENT_WORKER-Thread-15 INFO:Local step 131500, global step 2105540: loss 0.0042
[2019-04-04 07:47:28,480] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 131500, global step 2105540: learning rate 0.0000
[2019-04-04 07:47:29,089] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132000, global step 2105696: loss 0.0324
[2019-04-04 07:47:29,090] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132000, global step 2105696: learning rate 0.0000
[2019-04-04 07:47:35,709] A3C_AGENT_WORKER-Thread-6 INFO:Local step 131500, global step 2107038: loss 0.0014
[2019-04-04 07:47:35,711] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 131500, global step 2107038: learning rate 0.0000
[2019-04-04 07:47:36,719] A3C_AGENT_WORKER-Thread-4 INFO:Local step 131500, global step 2107287: loss 0.0007
[2019-04-04 07:47:36,720] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 131500, global step 2107287: learning rate 0.0000
[2019-04-04 07:47:38,657] A3C_AGENT_WORKER-Thread-13 INFO:Local step 131500, global step 2107757: loss 0.0002
[2019-04-04 07:47:38,661] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 131500, global step 2107762: learning rate 0.0000
[2019-04-04 07:47:39,961] A3C_AGENT_WORKER-Thread-10 INFO:Local step 131500, global step 2107989: loss 0.0002
[2019-04-04 07:47:40,001] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 131500, global step 2107989: learning rate 0.0000
[2019-04-04 07:47:42,288] A3C_AGENT_WORKER-Thread-14 INFO:Local step 131500, global step 2108502: loss 0.0007
[2019-04-04 07:47:42,290] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 131500, global step 2108502: learning rate 0.0000
[2019-04-04 07:47:42,616] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132000, global step 2108602: loss 0.0490
[2019-04-04 07:47:42,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132000, global step 2108602: learning rate 0.0000
[2019-04-04 07:47:43,325] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132000, global step 2108826: loss 0.0501
[2019-04-04 07:47:43,325] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132000, global step 2108826: learning rate 0.0000
[2019-04-04 07:47:46,030] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132000, global step 2109669: loss 0.0427
[2019-04-04 07:47:46,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132000, global step 2109669: learning rate 0.0000
[2019-04-04 07:47:46,862] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5575684e-09 8.4731833e-10 4.3121535e-22 3.3653186e-10 1.7115868e-10
 1.0094991e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:47:46,862] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4868
[2019-04-04 07:47:46,879] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 38.33333333333334, 0.0, 0.0, 26.0, 25.05171601150306, 0.2141952198774993, 0.0, 1.0, 39430.52585624298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2506200.0000, 
sim time next is 2506800.0000, 
raw observation next is [-1.7, 38.66666666666667, 0.0, 0.0, 26.0, 25.02289631816561, 0.2389719353252175, 0.0, 1.0, 39649.20202203539], 
processed observation next is [1.0, 0.0, 0.4155124653739613, 0.3866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5852413598471342, 0.5796573117750725, 0.0, 1.0, 0.18880572391445422], 
reward next is 0.8112, 
noisyNet noise sample is [array([-0.9747896], dtype=float32), 0.17804328]. 
=============================================
[2019-04-04 07:47:49,625] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132000, global step 2110830: loss 0.0246
[2019-04-04 07:47:49,626] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132000, global step 2110830: learning rate 0.0000
[2019-04-04 07:47:50,136] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132000, global step 2111031: loss 0.0224
[2019-04-04 07:47:50,139] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132000, global step 2111033: learning rate 0.0000
[2019-04-04 07:47:50,179] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132000, global step 2111045: loss 0.0228
[2019-04-04 07:47:50,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132000, global step 2111046: learning rate 0.0000
[2019-04-04 07:47:50,783] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132000, global step 2111267: loss 0.0301
[2019-04-04 07:47:50,785] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132000, global step 2111267: learning rate 0.0000
[2019-04-04 07:47:51,029] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132000, global step 2111346: loss 0.0322
[2019-04-04 07:47:51,029] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132000, global step 2111346: learning rate 0.0000
[2019-04-04 07:47:54,219] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132000, global step 2112483: loss 0.0341
[2019-04-04 07:47:54,220] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132000, global step 2112484: learning rate 0.0000
[2019-04-04 07:47:56,979] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132000, global step 2113385: loss 0.0392
[2019-04-04 07:47:56,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132000, global step 2113385: learning rate 0.0000
[2019-04-04 07:47:57,226] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2113477: loss 0.0052
[2019-04-04 07:47:57,226] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2113477: learning rate 0.0000
[2019-04-04 07:48:01,832] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132000, global step 2115109: loss 0.0609
[2019-04-04 07:48:01,832] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132000, global step 2115109: learning rate 0.0000
[2019-04-04 07:48:02,191] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132000, global step 2115245: loss 0.0661
[2019-04-04 07:48:02,192] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132000, global step 2115245: learning rate 0.0000
[2019-04-04 07:48:04,151] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132000, global step 2115847: loss 0.0648
[2019-04-04 07:48:04,152] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132000, global step 2115847: learning rate 0.0000
[2019-04-04 07:48:04,793] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132000, global step 2116068: loss 0.0682
[2019-04-04 07:48:04,793] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132000, global step 2116068: learning rate 0.0000
[2019-04-04 07:48:05,554] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2116369: loss 0.0110
[2019-04-04 07:48:05,555] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2116369: learning rate 0.0000
[2019-04-04 07:48:05,912] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132000, global step 2116509: loss 0.0629
[2019-04-04 07:48:05,916] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132000, global step 2116511: learning rate 0.0000
[2019-04-04 07:48:07,321] A3C_AGENT_WORKER-Thread-5 INFO:Local step 132500, global step 2117078: loss 0.0162
[2019-04-04 07:48:07,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 132500, global step 2117078: learning rate 0.0000
[2019-04-04 07:48:08,331] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7339212e-09 7.5631368e-09 5.8779640e-21 3.5424796e-09 1.5419671e-09
 3.2140553e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:08,332] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4520
[2019-04-04 07:48:08,363] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.72411067863519, 0.2562760719155276, 0.0, 1.0, 42694.63821931047], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2767800.0000, 
sim time next is 2768400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.75007351587733, 0.2486432145298395, 0.0, 1.0, 42516.00349726364], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5625061263231107, 0.5828810715099465, 0.0, 1.0, 0.20245715951077925], 
reward next is 0.7975, 
noisyNet noise sample is [array([0.6502491], dtype=float32), 0.4979439]. 
=============================================
[2019-04-04 07:48:09,661] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2117784: loss 0.0058
[2019-04-04 07:48:09,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2117784: learning rate 0.0000
[2019-04-04 07:48:12,441] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2118760: loss 0.0005
[2019-04-04 07:48:12,446] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2118762: learning rate 0.0000
[2019-04-04 07:48:12,942] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2118985: loss 0.0011
[2019-04-04 07:48:12,943] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2118985: learning rate 0.0000
[2019-04-04 07:48:13,161] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2119082: loss 0.0019
[2019-04-04 07:48:13,163] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2119083: learning rate 0.0000
[2019-04-04 07:48:13,299] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2119142: loss 0.0030
[2019-04-04 07:48:13,300] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2119142: learning rate 0.0000
[2019-04-04 07:48:13,394] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2119187: loss 0.0027
[2019-04-04 07:48:13,396] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2119188: learning rate 0.0000
[2019-04-04 07:48:15,120] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6561519e-09 2.8159683e-09 2.0638150e-22 4.6319093e-10 3.9914630e-10
 2.5965092e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:15,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0610
[2019-04-04 07:48:15,187] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 70.0, 170.0, 59.99999999999999, 26.0, 24.88829556260841, 0.2887273547324194, 0.0, 1.0, 104611.8469062672], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2974200.0000, 
sim time next is 2974800.0000, 
raw observation next is [-3.666666666666667, 69.0, 174.0, 42.0, 26.0, 24.84692871960326, 0.3061027346643935, 0.0, 1.0, 97469.73404623813], 
processed observation next is [0.0, 0.43478260869565216, 0.3610341643582641, 0.69, 0.58, 0.04640883977900553, 0.6666666666666666, 0.5705773933002716, 0.6020342448881312, 0.0, 1.0, 0.464141590696372], 
reward next is 0.5359, 
noisyNet noise sample is [array([1.1630583], dtype=float32), 0.5510788]. 
=============================================
[2019-04-04 07:48:16,891] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2120435: loss 0.0022
[2019-04-04 07:48:16,892] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2120435: learning rate 0.0000
[2019-04-04 07:48:17,120] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2120530: loss 0.2808
[2019-04-04 07:48:17,125] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2120531: learning rate 0.0000
[2019-04-04 07:48:18,548] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3776911e-09 2.9785017e-09 1.3129220e-22 1.7424709e-10 2.0566913e-10
 2.3823987e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:18,559] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4127
[2019-04-04 07:48:18,572] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 100.0, 0.0, 0.0, 26.0, 25.3822345654604, 0.3391669568655208, 0.0, 1.0, 45801.64325044757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3104400.0000, 
sim time next is 3105000.0000, 
raw observation next is [-0.5, 100.0, 0.0, 0.0, 26.0, 25.36171329894693, 0.3415638514317478, 0.0, 1.0, 48868.71029177491], 
processed observation next is [0.0, 0.9565217391304348, 0.44875346260387816, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6134761082455776, 0.613854617143916, 0.0, 1.0, 0.23270814424654718], 
reward next is 0.7673, 
noisyNet noise sample is [array([1.3181207], dtype=float32), 0.8799639]. 
=============================================
[2019-04-04 07:48:18,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.41878 ]
 [79.34951 ]
 [79.31542 ]
 [79.392525]
 [79.369896]], R is [[79.46806335]
 [79.45528412]
 [79.49306488]
 [79.5328064 ]
 [79.48477936]].
[2019-04-04 07:48:18,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5144378e-10 2.9671879e-10 4.3582079e-23 2.9049161e-11 4.6851301e-11
 5.1543063e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:18,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3970
[2019-04-04 07:48:18,865] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.39424758595381, 0.4813716004089487, 0.0, 1.0, 48336.88507245939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2928600.0000, 
sim time next is 2929200.0000, 
raw observation next is [-1.0, 80.33333333333334, 0.0, 0.0, 26.0, 25.38140993122097, 0.4817287228267869, 0.0, 1.0, 52595.77672821701], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.615117494268414, 0.6605762409422623, 0.0, 1.0, 0.25045607965817623], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.7208825], dtype=float32), 0.18734214]. 
=============================================
[2019-04-04 07:48:19,614] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2121580: loss 0.0108
[2019-04-04 07:48:19,623] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2121580: learning rate 0.0000
[2019-04-04 07:48:24,734] A3C_AGENT_WORKER-Thread-6 INFO:Local step 132500, global step 2123705: loss 0.0009
[2019-04-04 07:48:24,735] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 132500, global step 2123705: learning rate 0.0000
[2019-04-04 07:48:24,972] A3C_AGENT_WORKER-Thread-4 INFO:Local step 132500, global step 2123804: loss 0.0007
[2019-04-04 07:48:24,981] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 132500, global step 2123805: learning rate 0.0000
[2019-04-04 07:48:25,031] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3984428e-08 9.3146069e-09 8.6451497e-21 1.3466577e-09 1.4830593e-09
 8.4765537e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:25,031] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8776
[2019-04-04 07:48:25,069] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.18255941671924, 0.09056237140621488, 0.0, 1.0, 39745.01731946161], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3036600.0000, 
sim time next is 3037200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.14886031487977, 0.08258158635137416, 0.0, 1.0, 39861.80790593582], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5124050262399807, 0.527527195450458, 0.0, 1.0, 0.18981813288540866], 
reward next is 0.8102, 
noisyNet noise sample is [array([-1.19928], dtype=float32), -0.7429102]. 
=============================================
[2019-04-04 07:48:25,241] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2123917: loss 0.5004
[2019-04-04 07:48:25,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2123917: learning rate 0.0000
[2019-04-04 07:48:26,182] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2124334: loss 0.0007
[2019-04-04 07:48:26,183] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2124334: learning rate 0.0000
[2019-04-04 07:48:26,927] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2124646: loss 0.0007
[2019-04-04 07:48:26,930] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2124646: learning rate 0.0000
[2019-04-04 07:48:27,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2260396e-11 7.3561379e-11 4.6896408e-24 4.6669244e-12 6.6751734e-12
 3.0844867e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:27,188] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0174
[2019-04-04 07:48:27,204] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 55.33333333333334, 115.8333333333333, 820.8333333333334, 26.0, 25.62531378798655, 0.5505423014175864, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3500400.0000, 
sim time next is 3501000.0000, 
raw observation next is [2.0, 54.5, 116.0, 823.0, 26.0, 25.82062535487243, 0.5747918826441064, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.545, 0.38666666666666666, 0.9093922651933701, 0.6666666666666666, 0.6517187795727025, 0.6915972942147022, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10582098], dtype=float32), 0.8112309]. 
=============================================
[2019-04-04 07:48:27,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[83.757454]
 [83.8352  ]
 [83.682   ]
 [83.84034 ]
 [83.95201 ]], R is [[83.83776093]
 [83.91043091]
 [83.75744629]
 [83.9198761 ]
 [84.08068085]].
[2019-04-04 07:48:27,752] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2124963: loss 0.0011
[2019-04-04 07:48:27,753] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2124963: learning rate 0.0000
[2019-04-04 07:48:27,818] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133000, global step 2124997: loss 0.5835
[2019-04-04 07:48:27,823] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133000, global step 2124997: learning rate 0.0000
[2019-04-04 07:48:28,694] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2125353: loss 0.5667
[2019-04-04 07:48:28,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2125354: learning rate 0.0000
[2019-04-04 07:48:31,344] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2126372: loss 0.5583
[2019-04-04 07:48:31,345] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2126372: learning rate 0.0000
[2019-04-04 07:48:31,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0403882e-09 9.1488261e-10 5.5587552e-22 2.4669755e-10 4.6222255e-11
 1.5364513e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:31,621] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3853
[2019-04-04 07:48:31,667] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 61.33333333333334, 48.66666666666666, 419.6666666666667, 26.0, 25.07818130908914, 0.3277695817826191, 0.0, 1.0, 49360.22542125468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3084000.0000, 
sim time next is 3084600.0000, 
raw observation next is [0.1666666666666666, 66.66666666666667, 40.33333333333334, 353.3333333333334, 26.0, 25.05455214936956, 0.3280904695440376, 0.0, 1.0, 50386.04530588872], 
processed observation next is [0.0, 0.6956521739130435, 0.4672206832871654, 0.6666666666666667, 0.13444444444444448, 0.39042357274401485, 0.6666666666666666, 0.5878793457807966, 0.6093634898480126, 0.0, 1.0, 0.23993354907566058], 
reward next is 0.7601, 
noisyNet noise sample is [array([-0.4468073], dtype=float32), 0.73876745]. 
=============================================
[2019-04-04 07:48:31,933] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2126628: loss 0.5091
[2019-04-04 07:48:31,938] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2126628: learning rate 0.0000
[2019-04-04 07:48:32,362] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2126810: loss 0.4847
[2019-04-04 07:48:32,365] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2126810: learning rate 0.0000
[2019-04-04 07:48:32,488] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2126859: loss 0.4788
[2019-04-04 07:48:32,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2126859: learning rate 0.0000
[2019-04-04 07:48:32,906] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2127041: loss 0.4864
[2019-04-04 07:48:32,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2127041: learning rate 0.0000
[2019-04-04 07:48:33,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9269566e-10 5.7905578e-09 1.2518967e-21 5.6721905e-10 1.1049576e-09
 1.5291424e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:33,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8300
[2019-04-04 07:48:33,108] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45429022948466, 0.4932090624715694, 0.0, 1.0, 71281.22641698869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.41009979984726, 0.5008420114672724, 0.0, 1.0, 77110.78655913717], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6175083166539382, 0.6669473371557575, 0.0, 1.0, 0.36719422171017696], 
reward next is 0.6328, 
noisyNet noise sample is [array([-0.2050262], dtype=float32), -1.3487183]. 
=============================================
[2019-04-04 07:48:35,344] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2128203: loss 0.4233
[2019-04-04 07:48:35,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2128203: learning rate 0.0000
[2019-04-04 07:48:35,643] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2128349: loss 0.5575
[2019-04-04 07:48:35,644] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2128350: learning rate 0.0000
[2019-04-04 07:48:37,807] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2129343: loss 0.6843
[2019-04-04 07:48:37,807] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2129343: learning rate 0.0000
[2019-04-04 07:48:43,307] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133000, global step 2131781: loss 0.6916
[2019-04-04 07:48:43,309] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133000, global step 2131781: learning rate 0.0000
[2019-04-04 07:48:43,527] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2131895: loss 0.4504
[2019-04-04 07:48:43,536] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2131899: learning rate 0.0000
[2019-04-04 07:48:43,904] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133000, global step 2132080: loss 0.6744
[2019-04-04 07:48:43,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133000, global step 2132080: learning rate 0.0000
[2019-04-04 07:48:45,118] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2132615: loss 0.6479
[2019-04-04 07:48:45,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2132615: learning rate 0.0000
[2019-04-04 07:48:45,911] A3C_AGENT_WORKER-Thread-5 INFO:Local step 133500, global step 2132995: loss 0.3396
[2019-04-04 07:48:45,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 133500, global step 2132996: learning rate 0.0000
[2019-04-04 07:48:45,942] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2133010: loss 0.6364
[2019-04-04 07:48:45,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2133010: learning rate 0.0000
[2019-04-04 07:48:46,271] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5137067e-09 4.2844146e-09 3.0384034e-22 2.2845142e-10 1.3530266e-10
 2.2691766e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:46,272] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7597
[2019-04-04 07:48:46,281] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 67.0, 0.0, 0.0, 26.0, 24.97029658988136, 0.3352026593399478, 0.0, 1.0, 40884.11115528287], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3559200.0000, 
sim time next is 3559800.0000, 
raw observation next is [-4.833333333333334, 66.0, 0.0, 0.0, 26.0, 24.94985404733506, 0.3251060399933215, 0.0, 1.0, 40867.2268146779], 
processed observation next is [0.0, 0.17391304347826086, 0.32871652816251157, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5791545039445882, 0.6083686799977738, 0.0, 1.0, 0.1946058419746567], 
reward next is 0.8054, 
noisyNet noise sample is [array([0.78458154], dtype=float32), -0.57315284]. 
=============================================
[2019-04-04 07:48:46,386] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2133196: loss 0.3426
[2019-04-04 07:48:46,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2133196: learning rate 0.0000
[2019-04-04 07:48:46,465] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2133225: loss 0.6190
[2019-04-04 07:48:46,466] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2133225: learning rate 0.0000
[2019-04-04 07:48:47,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1608715e-09 1.5967329e-09 3.7300931e-22 1.7940321e-10 2.0266175e-10
 4.1547671e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:47,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4143
[2019-04-04 07:48:47,487] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 116.0, 819.5, 26.0, 25.17893675922013, 0.4453046177711999, 0.0, 1.0, 18712.60917189331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3585600.0000, 
sim time next is 3586200.0000, 
raw observation next is [-2.833333333333333, 54.16666666666667, 116.6666666666667, 820.6666666666667, 26.0, 25.17543895453002, 0.4457421583507086, 0.0, 1.0, 18712.0621233015], 
processed observation next is [0.0, 0.5217391304347826, 0.3841181902123731, 0.5416666666666667, 0.388888888888889, 0.9068139963167589, 0.6666666666666666, 0.5979532462108349, 0.6485807194502362, 0.0, 1.0, 0.08910505773000715], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.7145017], dtype=float32), 1.2573149]. 
=============================================
[2019-04-04 07:48:49,213] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2134503: loss 0.2389
[2019-04-04 07:48:49,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2134507: learning rate 0.0000
[2019-04-04 07:48:50,001] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2134888: loss 0.2342
[2019-04-04 07:48:50,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2134889: learning rate 0.0000
[2019-04-04 07:48:50,084] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2134937: loss 0.2306
[2019-04-04 07:48:50,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2134937: learning rate 0.0000
[2019-04-04 07:48:50,232] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2135006: loss 0.2182
[2019-04-04 07:48:50,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2135006: learning rate 0.0000
[2019-04-04 07:48:50,980] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2135381: loss 0.2053
[2019-04-04 07:48:50,984] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2135381: learning rate 0.0000
[2019-04-04 07:48:51,761] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2135757: loss 0.1108
[2019-04-04 07:48:51,761] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2135757: learning rate 0.0000
[2019-04-04 07:48:52,926] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2136356: loss 0.2392
[2019-04-04 07:48:52,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2136356: learning rate 0.0000
[2019-04-04 07:48:54,102] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.4102720e-12 2.3024483e-11 1.4033445e-25 3.6356859e-12 9.3986429e-13
 8.6762273e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:54,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9987
[2019-04-04 07:48:54,126] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 60.33333333333334, 107.3333333333333, 753.3333333333334, 26.0, 26.26324909136737, 0.576302124206741, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3493200.0000, 
sim time next is 3493800.0000, 
raw observation next is [0.5, 60.5, 109.0, 770.0, 26.0, 26.21589331091788, 0.580951000485432, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.605, 0.36333333333333334, 0.850828729281768, 0.6666666666666666, 0.6846577759098235, 0.6936503334951439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04971345], dtype=float32), 0.16458507]. 
=============================================
[2019-04-04 07:48:55,324] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2137561: loss 0.3119
[2019-04-04 07:48:55,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2137561: learning rate 0.0000
[2019-04-04 07:48:58,100] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0171496e-11 2.5403906e-11 3.1545695e-25 1.9595079e-11 3.8224147e-13
 3.1102799e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:48:58,109] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8376
[2019-04-04 07:48:58,148] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 77.0, 96.33333333333333, 593.0, 26.0, 25.72234013241103, 0.4388169994078718, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3748200.0000, 
sim time next is 3748800.0000, 
raw observation next is [-3.666666666666667, 77.0, 98.16666666666667, 634.0, 26.0, 25.96601288647788, 0.4707802293512444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3610341643582641, 0.77, 0.32722222222222225, 0.7005524861878453, 0.6666666666666666, 0.66383440720649, 0.6569267431170814, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35150018], dtype=float32), -0.4687022]. 
=============================================
[2019-04-04 07:48:59,451] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2139501: loss 0.1350
[2019-04-04 07:48:59,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2139503: learning rate 0.0000
[2019-04-04 07:49:00,472] A3C_AGENT_WORKER-Thread-6 INFO:Local step 133500, global step 2139959: loss 0.2948
[2019-04-04 07:49:00,474] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 133500, global step 2139959: learning rate 0.0000
[2019-04-04 07:49:01,356] A3C_AGENT_WORKER-Thread-4 INFO:Local step 133500, global step 2140302: loss 0.2554
[2019-04-04 07:49:01,357] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 133500, global step 2140302: learning rate 0.0000
[2019-04-04 07:49:02,199] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2140704: loss 0.1987
[2019-04-04 07:49:02,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2140704: learning rate 0.0000
[2019-04-04 07:49:02,746] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134000, global step 2140983: loss 0.0605
[2019-04-04 07:49:02,747] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134000, global step 2140984: learning rate 0.0000
[2019-04-04 07:49:02,966] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2141088: loss 0.0577
[2019-04-04 07:49:02,967] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2141089: learning rate 0.0000
[2019-04-04 07:49:03,380] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.7062504e-11 3.0514355e-11 9.9469844e-27 1.4204060e-11 1.8224977e-12
 5.8279916e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:03,381] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9076
[2019-04-04 07:49:03,404] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 26.33333333333334, 97.0, 576.3333333333334, 26.0, 25.70303739299125, 0.4562860390511013, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3661800.0000, 
sim time next is 3662400.0000, 
raw observation next is [11.0, 26.66666666666667, 99.0, 619.6666666666666, 26.0, 25.73551144756321, 0.4608863043007618, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.2666666666666667, 0.33, 0.6847145488029466, 0.6666666666666666, 0.6446259539636007, 0.6536287681002539, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.972554], dtype=float32), -0.14498869]. 
=============================================
[2019-04-04 07:49:04,071] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2141663: loss 0.2551
[2019-04-04 07:49:04,078] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2141665: learning rate 0.0000
[2019-04-04 07:49:04,397] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2141813: loss 0.2762
[2019-04-04 07:49:04,398] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2141813: learning rate 0.0000
[2019-04-04 07:49:05,150] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2142151: loss 0.0663
[2019-04-04 07:49:05,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2142151: learning rate 0.0000
[2019-04-04 07:49:05,640] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6376834e-11 4.6304318e-11 1.0931384e-25 2.3443920e-11 1.1288121e-12
 9.9618116e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:05,641] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9164
[2019-04-04 07:49:05,658] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 27.0, 101.0, 663.0, 26.0, 25.7187415348685, 0.4641192911650581, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3663000.0000, 
sim time next is 3663600.0000, 
raw observation next is [11.0, 27.33333333333333, 102.6666666666667, 679.6666666666666, 26.0, 25.69879184526723, 0.4653706312471503, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.7673130193905818, 0.27333333333333326, 0.3422222222222223, 0.751012891344383, 0.6666666666666666, 0.6415659871056025, 0.65512354374905, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0102315], dtype=float32), -2.3270507]. 
=============================================
[2019-04-04 07:49:06,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.93166228e-12 4.99903764e-11 1.03215596e-24 1.09210835e-11
 1.06021051e-12 5.30045817e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:49:06,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5841
[2019-04-04 07:49:06,125] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.5, 51.0, 100.0, 692.0, 26.0, 25.99699474141102, 0.5308271020086107, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3922200.0000, 
sim time next is 3922800.0000, 
raw observation next is [-7.333333333333333, 50.33333333333333, 102.1666666666667, 705.8333333333334, 26.0, 26.12609539108662, 0.5481502667382544, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.25946445060018475, 0.5033333333333333, 0.34055555555555567, 0.779926335174954, 0.6666666666666666, 0.6771746159238848, 0.6827167555794181, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0269936], dtype=float32), 1.0095716]. 
=============================================
[2019-04-04 07:49:06,244] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2142663: loss 0.1103
[2019-04-04 07:49:06,245] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2142663: learning rate 0.0000
[2019-04-04 07:49:06,247] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2142664: loss 0.0585
[2019-04-04 07:49:06,248] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2142664: learning rate 0.0000
[2019-04-04 07:49:06,923] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2142959: loss 0.0505
[2019-04-04 07:49:06,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2142959: learning rate 0.0000
[2019-04-04 07:49:07,168] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2143049: loss 0.0485
[2019-04-04 07:49:07,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2143049: learning rate 0.0000
[2019-04-04 07:49:08,962] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2143814: loss 0.0350
[2019-04-04 07:49:08,962] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2143814: learning rate 0.0000
[2019-04-04 07:49:10,012] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2144277: loss 0.0908
[2019-04-04 07:49:10,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2144277: learning rate 0.0000
[2019-04-04 07:49:12,021] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2145066: loss 0.0253
[2019-04-04 07:49:12,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2145066: learning rate 0.0000
[2019-04-04 07:49:16,034] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.7594647e-10 5.3223337e-10 6.7672950e-23 1.5667656e-10 1.3232528e-10
 3.6302930e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:16,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8117
[2019-04-04 07:49:16,073] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 36.66666666666666, 0.0, 0.0, 26.0, 24.81842458596958, 0.2128651160840288, 0.0, 1.0, 40176.57983001737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4081200.0000, 
sim time next is 4081800.0000, 
raw observation next is [-4.0, 37.33333333333334, 0.0, 0.0, 26.0, 24.77224608224579, 0.2012292781225057, 0.0, 1.0, 40166.93253667688], 
processed observation next is [1.0, 0.21739130434782608, 0.3518005540166205, 0.3733333333333334, 0.0, 0.0, 0.6666666666666666, 0.5643538401871492, 0.5670764260408353, 0.0, 1.0, 0.19127110731750896], 
reward next is 0.8087, 
noisyNet noise sample is [array([-1.1660812], dtype=float32), 1.9609857]. 
=============================================
[2019-04-04 07:49:17,043] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134000, global step 2147301: loss 0.0099
[2019-04-04 07:49:17,050] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134000, global step 2147302: learning rate 0.0000
[2019-04-04 07:49:17,963] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134000, global step 2147708: loss 0.0199
[2019-04-04 07:49:17,964] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134000, global step 2147708: learning rate 0.0000
[2019-04-04 07:49:18,223] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2147828: loss 0.0581
[2019-04-04 07:49:18,225] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2147828: learning rate 0.0000
[2019-04-04 07:49:18,376] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2147906: loss 0.0158
[2019-04-04 07:49:18,377] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2147910: learning rate 0.0000
[2019-04-04 07:49:20,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0758860e-12 1.0049159e-11 3.8141163e-26 1.2374259e-12 1.7769270e-13
 6.7086391e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:20,120] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5653
[2019-04-04 07:49:20,146] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 28.0, 120.5, 828.5, 26.0, 26.54601884201412, 0.4077076637400346, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4104000.0000, 
sim time next is 4104600.0000, 
raw observation next is [1.333333333333333, 28.16666666666667, 120.3333333333333, 832.6666666666667, 26.0, 26.61868396526148, 0.6264745561622386, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4995383194829178, 0.28166666666666673, 0.401111111111111, 0.9200736648250462, 0.6666666666666666, 0.7182236637717899, 0.7088248520540795, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.321579], dtype=float32), 1.2306341]. 
=============================================
[2019-04-04 07:49:20,519] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.5348362e-10 9.3194885e-10 2.1898984e-23 3.2717040e-10 1.7535133e-10
 1.3688169e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:20,524] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2955
[2019-04-04 07:49:20,549] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.133333333333333, 42.66666666666666, 0.0, 0.0, 26.0, 25.09763990507345, 0.366232494091296, 0.0, 1.0, 100587.2262973448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4218000.0000, 
sim time next is 4218600.0000, 
raw observation next is [1.066666666666667, 42.83333333333334, 0.0, 0.0, 26.0, 25.21595587443852, 0.3867083310794987, 0.0, 1.0, 60183.5639228952], 
processed observation next is [0.0, 0.8260869565217391, 0.49215143120960303, 0.42833333333333345, 0.0, 0.0, 0.6666666666666666, 0.6013296562032101, 0.6289027770264995, 0.0, 1.0, 0.2865883996328343], 
reward next is 0.7134, 
noisyNet noise sample is [array([-1.0578965], dtype=float32), -0.48818895]. 
=============================================
[2019-04-04 07:49:20,790] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2148993: loss 0.0087
[2019-04-04 07:49:20,791] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2148993: learning rate 0.0000
[2019-04-04 07:49:21,309] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2149246: loss 0.0080
[2019-04-04 07:49:21,310] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2149248: learning rate 0.0000
[2019-04-04 07:49:21,416] A3C_AGENT_WORKER-Thread-5 INFO:Local step 134500, global step 2149292: loss 0.0816
[2019-04-04 07:49:21,431] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 134500, global step 2149293: learning rate 0.0000
[2019-04-04 07:49:22,031] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2149561: loss 0.0917
[2019-04-04 07:49:22,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2149561: learning rate 0.0000
[2019-04-04 07:49:23,928] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2150313: loss 0.0615
[2019-04-04 07:49:23,931] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2150314: learning rate 0.0000
[2019-04-04 07:49:25,026] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2150855: loss 0.0683
[2019-04-04 07:49:25,027] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2150857: learning rate 0.0000
[2019-04-04 07:49:25,225] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2150963: loss 0.0493
[2019-04-04 07:49:25,225] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2150963: learning rate 0.0000
[2019-04-04 07:49:25,396] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2151041: loss 0.0463
[2019-04-04 07:49:25,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2151041: learning rate 0.0000
[2019-04-04 07:49:25,716] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2151175: loss 0.0449
[2019-04-04 07:49:25,716] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2151175: learning rate 0.0000
[2019-04-04 07:49:26,468] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2151564: loss 0.3719
[2019-04-04 07:49:26,470] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2151564: learning rate 0.0000
[2019-04-04 07:49:27,325] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2151950: loss 0.0380
[2019-04-04 07:49:27,327] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2151950: learning rate 0.0000
[2019-04-04 07:49:30,777] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2153708: loss 0.0636
[2019-04-04 07:49:30,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2153708: learning rate 0.0000
[2019-04-04 07:49:34,045] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2155393: loss 0.3765
[2019-04-04 07:49:34,046] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2155393: learning rate 0.0000
[2019-04-04 07:49:34,076] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.3939171e-13 4.1411375e-12 8.3211960e-28 3.8851203e-13 2.2018799e-13
 6.8939217e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:34,084] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3130
[2019-04-04 07:49:34,094] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 232.8333333333333, 121.6666666666667, 26.0, 26.48175174314926, 0.6653892875525865, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4444800.0000, 
sim time next is 4445400.0000, 
raw observation next is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 26.0, 26.48342636703084, 0.6621030946930381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.7155555555555557, 0.10755064456721916, 0.6666666666666666, 0.7069521972525701, 0.720701031564346, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1661073], dtype=float32), 0.34820127]. 
=============================================
[2019-04-04 07:49:35,027] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.3146115e-10 6.9247186e-10 5.0598734e-23 4.6539311e-11 5.7328572e-11
 2.0731552e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:35,039] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7189
[2019-04-04 07:49:35,053] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.55, 73.0, 0.0, 0.0, 26.0, 25.37101066150809, 0.4939353317380669, 0.0, 1.0, 46975.42924133546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4494600.0000, 
sim time next is 4495200.0000, 
raw observation next is [-0.5666666666666667, 73.0, 0.0, 0.0, 26.0, 25.50618524269612, 0.4903724470802154, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.44690674053554946, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6255154368913433, 0.6634574823600717, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.63743085], dtype=float32), 0.9239965]. 
=============================================
[2019-04-04 07:49:35,145] A3C_AGENT_WORKER-Thread-6 INFO:Local step 134500, global step 2155970: loss 0.1291
[2019-04-04 07:49:35,145] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 134500, global step 2155972: learning rate 0.0000
[2019-04-04 07:49:36,105] A3C_AGENT_WORKER-Thread-4 INFO:Local step 134500, global step 2156460: loss 0.1413
[2019-04-04 07:49:36,107] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 134500, global step 2156460: learning rate 0.0000
[2019-04-04 07:49:36,780] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2156714: loss 0.1333
[2019-04-04 07:49:36,780] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2156714: learning rate 0.0000
[2019-04-04 07:49:37,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4071602e-09 2.1718733e-09 1.0552236e-23 5.7258664e-10 8.5274114e-11
 4.7709747e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:37,296] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5740
[2019-04-04 07:49:37,318] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.3479696571227, 0.322700217066376, 0.0, 1.0, 39250.52119839998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4258200.0000, 
sim time next is 4258800.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.34201229534833, 0.3211354549138404, 0.0, 1.0, 39205.49783687342], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6118343579456941, 0.6070451516379468, 0.0, 1.0, 0.1866928468422544], 
reward next is 0.8133, 
noisyNet noise sample is [array([1.0076356], dtype=float32), -1.1043128]. 
=============================================
[2019-04-04 07:49:37,695] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135000, global step 2157128: loss 0.3942
[2019-04-04 07:49:37,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135000, global step 2157128: learning rate 0.0000
[2019-04-04 07:49:37,718] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2157138: loss 0.3973
[2019-04-04 07:49:37,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2157138: learning rate 0.0000
[2019-04-04 07:49:39,313] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2157916: loss 0.1042
[2019-04-04 07:49:39,314] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2157916: learning rate 0.0000
[2019-04-04 07:49:39,566] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2158066: loss 0.3942
[2019-04-04 07:49:39,580] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2158075: learning rate 0.0000
[2019-04-04 07:49:39,708] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2158142: loss 0.1220
[2019-04-04 07:49:39,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2158142: learning rate 0.0000
[2019-04-04 07:49:40,940] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2158704: loss 0.4035
[2019-04-04 07:49:40,940] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2158704: learning rate 0.0000
[2019-04-04 07:49:40,954] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2158711: loss 0.4066
[2019-04-04 07:49:40,956] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2158711: learning rate 0.0000
[2019-04-04 07:49:41,182] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2158817: loss 0.4075
[2019-04-04 07:49:41,183] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2158817: learning rate 0.0000
[2019-04-04 07:49:41,377] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2158913: loss 0.4139
[2019-04-04 07:49:41,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2158913: learning rate 0.0000
[2019-04-04 07:49:43,034] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2159730: loss 0.0454
[2019-04-04 07:49:43,035] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2159731: learning rate 0.0000
[2019-04-04 07:49:43,117] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2159771: loss 0.3897
[2019-04-04 07:49:43,123] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2159771: learning rate 0.0000
[2019-04-04 07:49:46,368] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2161396: loss 0.3841
[2019-04-04 07:49:46,369] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2161397: learning rate 0.0000
[2019-04-04 07:49:46,969] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4773991e-12 3.3813295e-12 2.8431823e-26 1.4065239e-12 3.4114248e-13
 2.2933667e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:46,970] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4029
[2019-04-04 07:49:46,983] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 192.5, 5.0, 26.0, 26.43329049910738, 0.5862933539945291, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4702800.0000, 
sim time next is 4703400.0000, 
raw observation next is [0.0, 92.0, 208.0, 6.0, 26.0, 26.44882807218339, 0.5934658210484952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.6933333333333334, 0.0066298342541436465, 0.6666666666666666, 0.7040690060152824, 0.6978219403494984, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0719407], dtype=float32), 0.46464318]. 
=============================================
[2019-04-04 07:49:47,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1374329e-09 3.8396601e-09 5.0614950e-23 3.4775399e-10 2.2695004e-10
 2.3141439e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:47,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0546
[2019-04-04 07:49:47,325] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.4972833345559, 0.548312644312596, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4652400.0000, 
sim time next is 4653000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.44665100256761, 0.5687048158849636, 0.0, 1.0, 197390.8111345668], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6205542502139675, 0.6895682719616545, 0.0, 1.0, 0.939956243497937], 
reward next is 0.0600, 
noisyNet noise sample is [array([-0.9744599], dtype=float32), -1.5069382]. 
=============================================
[2019-04-04 07:49:47,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.285645]
 [78.96079 ]
 [80.64685 ]
 [83.15456 ]
 [86.30291 ]], R is [[79.55587769]
 [78.82595062]
 [78.9146347 ]
 [78.92334747]
 [79.13411713]].
[2019-04-04 07:49:50,812] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135000, global step 2163577: loss 0.4038
[2019-04-04 07:49:50,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135000, global step 2163577: learning rate 0.0000
[2019-04-04 07:49:51,254] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2163763: loss 0.0408
[2019-04-04 07:49:51,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2163764: learning rate 0.0000
[2019-04-04 07:49:52,093] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135000, global step 2164099: loss 0.4139
[2019-04-04 07:49:52,095] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135000, global step 2164099: learning rate 0.0000
[2019-04-04 07:49:52,704] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2164378: loss 0.4088
[2019-04-04 07:49:52,705] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2164378: learning rate 0.0000
[2019-04-04 07:49:54,414] A3C_AGENT_WORKER-Thread-5 INFO:Local step 135500, global step 2165184: loss 0.0325
[2019-04-04 07:49:54,415] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 135500, global step 2165184: learning rate 0.0000
[2019-04-04 07:49:54,553] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2165238: loss 0.0274
[2019-04-04 07:49:54,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2165238: learning rate 0.0000
[2019-04-04 07:49:54,793] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2165331: loss 0.4081
[2019-04-04 07:49:54,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2165331: learning rate 0.0000
[2019-04-04 07:49:55,649] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2165713: loss 0.3935
[2019-04-04 07:49:55,657] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2165715: learning rate 0.0000
[2019-04-04 07:49:56,897] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2166244: loss 0.0456
[2019-04-04 07:49:56,898] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2166244: learning rate 0.0000
[2019-04-04 07:49:57,989] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2166750: loss 0.0463
[2019-04-04 07:49:57,999] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2166753: learning rate 0.0000
[2019-04-04 07:49:58,400] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2166969: loss 0.0442
[2019-04-04 07:49:58,403] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2166970: learning rate 0.0000
[2019-04-04 07:49:58,538] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2167041: loss 0.0348
[2019-04-04 07:49:58,540] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2167043: learning rate 0.0000
[2019-04-04 07:49:58,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:49:58,565] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:49:58,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run17
[2019-04-04 07:49:58,700] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2167101: loss 0.0300
[2019-04-04 07:49:58,700] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2167101: learning rate 0.0000
[2019-04-04 07:49:59,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4714400e-12 2.8837050e-12 2.7260637e-27 4.7114445e-13 8.6451808e-14
 2.5216920e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:49:59,693] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9940
[2019-04-04 07:49:59,705] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.833333333333334, 25.66666666666667, 114.3333333333333, 846.3333333333333, 26.0, 27.02264999765517, 0.772420361414207, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4974600.0000, 
sim time next is 4975200.0000, 
raw observation next is [8.0, 26.0, 113.0, 839.5, 26.0, 27.22092676500987, 0.7999699107979333, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.37666666666666665, 0.9276243093922651, 0.6666666666666666, 0.7684105637508226, 0.7666566369326445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8551972], dtype=float32), -0.7762141]. 
=============================================
[2019-04-04 07:50:00,351] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2167826: loss 0.0409
[2019-04-04 07:50:00,352] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2167826: learning rate 0.0000
[2019-04-04 07:50:03,486] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2169093: loss 0.0470
[2019-04-04 07:50:03,486] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2169093: learning rate 0.0000
[2019-04-04 07:50:07,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:07,282] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:07,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run17
[2019-04-04 07:50:08,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5027270e-12 2.4630296e-11 3.4469897e-26 2.5256249e-12 3.8948697e-13
 1.2545203e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:08,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2398
[2019-04-04 07:50:08,078] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 25.5, 34.0, 304.0, 26.0, 27.50523803938095, 0.8256053838767308, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4987800.0000, 
sim time next is 4988400.0000, 
raw observation next is [6.666666666666667, 25.33333333333333, 28.33333333333334, 253.3333333333333, 26.0, 27.33721509660292, 0.8476868517210754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6472760849492153, 0.2533333333333333, 0.09444444444444447, 0.2799263351749539, 0.6666666666666666, 0.7781012580502434, 0.7825622839070251, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.88853854], dtype=float32), -0.049228303]. 
=============================================
[2019-04-04 07:50:08,120] A3C_AGENT_WORKER-Thread-6 INFO:Local step 135500, global step 2171109: loss 0.0381
[2019-04-04 07:50:08,123] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 135500, global step 2171110: learning rate 0.0000
[2019-04-04 07:50:09,633] A3C_AGENT_WORKER-Thread-4 INFO:Local step 135500, global step 2171794: loss 0.0225
[2019-04-04 07:50:09,635] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 135500, global step 2171794: learning rate 0.0000
[2019-04-04 07:50:10,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:10,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:10,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run17
[2019-04-04 07:50:10,541] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2172084: loss 0.0357
[2019-04-04 07:50:10,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2172085: learning rate 0.0000
[2019-04-04 07:50:10,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:10,574] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:10,578] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run17
[2019-04-04 07:50:12,398] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2172768: loss 0.0330
[2019-04-04 07:50:12,401] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2172768: learning rate 0.0000
[2019-04-04 07:50:12,842] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2172933: loss 0.0368
[2019-04-04 07:50:12,844] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2172933: learning rate 0.0000
[2019-04-04 07:50:13,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:13,093] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:13,098] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run17
[2019-04-04 07:50:13,859] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:13,861] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:13,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run17
[2019-04-04 07:50:14,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:14,219] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:14,223] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run17
[2019-04-04 07:50:14,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:14,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:14,266] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run17
[2019-04-04 07:50:14,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:14,863] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:14,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run17
[2019-04-04 07:50:16,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:16,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:16,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run17
[2019-04-04 07:50:20,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:20,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:20,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run17
[2019-04-04 07:50:21,293] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.0597670e-10 3.0209200e-09 1.5153490e-22 5.0561250e-10 1.4974925e-10
 7.7921502e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:21,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3981
[2019-04-04 07:50:21,321] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 25.33591323692676, 0.3581783695203959, 0.0, 1.0, 46067.94785970592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5034000.0000, 
sim time next is 5034600.0000, 
raw observation next is [-2.0, 55.5, 0.0, 0.0, 26.0, 25.28980104778433, 0.3499908693783726, 0.0, 1.0, 39980.64969332508], 
processed observation next is [1.0, 0.2608695652173913, 0.40720221606648205, 0.555, 0.0, 0.0, 0.6666666666666666, 0.6074834206486942, 0.6166636231261242, 0.0, 1.0, 0.19038404615869087], 
reward next is 0.8096, 
noisyNet noise sample is [array([-1.2475291], dtype=float32), 1.4684476]. 
=============================================
[2019-04-04 07:50:23,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0215869e-10 4.5502282e-10 2.0157818e-24 1.3911228e-10 2.8101492e-11
 2.1920147e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:23,397] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8756
[2019-04-04 07:50:23,477] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 49.0, 0.0, 26.0, 23.48396747548868, -0.07784156757973969, 0.0, 1.0, 57952.71270752177], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 36000.0000, 
sim time next is 36600.0000, 
raw observation next is [7.7, 93.0, 52.66666666666667, 0.0, 26.0, 23.58465297994817, -0.05738324692424232, 0.0, 1.0, 57320.74950573059], 
processed observation next is [0.0, 0.43478260869565216, 0.6759002770083103, 0.93, 0.17555555555555558, 0.0, 0.6666666666666666, 0.46538774832901425, 0.48087225102525255, 0.0, 1.0, 0.27295595002728856], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.78471184], dtype=float32), 0.47244126]. 
=============================================
[2019-04-04 07:50:26,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:26,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:26,920] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run17
[2019-04-04 07:50:28,696] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:28,696] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:28,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run17
[2019-04-04 07:50:29,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:29,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:29,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run17
[2019-04-04 07:50:31,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:31,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:31,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run17
[2019-04-04 07:50:31,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 07:50:31,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:50:31,698] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run17
[2019-04-04 07:50:37,895] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.6329832e-11 9.4320962e-10 8.3115363e-24 8.2222354e-11 1.8003522e-11
 1.8254225e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:37,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8220
[2019-04-04 07:50:37,982] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.25497710880117, 0.3127776681159637, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 241800.0000, 
sim time next is 242400.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.35157373065526, 0.2820659360000954, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6126311442212717, 0.5940219786666985, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9833003], dtype=float32), 1.2849635]. 
=============================================
[2019-04-04 07:50:42,676] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.6845631e-12 6.9319300e-11 5.1384319e-25 1.0589358e-11 7.5457875e-13
 4.5437317e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:42,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1554
[2019-04-04 07:50:42,742] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.800000000000001, 69.66666666666667, 148.1666666666667, 0.0, 26.0, 25.30753995549852, 0.2242858613696216, 1.0, 1.0, 25955.41236959328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 213600.0000, 
sim time next is 214200.0000, 
raw observation next is [-5.6, 68.5, 153.0, 0.0, 26.0, 25.27854299550894, 0.2277315188078578, 1.0, 1.0, 25377.87353041201], 
processed observation next is [1.0, 0.4782608695652174, 0.30747922437673136, 0.685, 0.51, 0.0, 0.6666666666666666, 0.606545249625745, 0.575910506269286, 1.0, 1.0, 0.12084701681148576], 
reward next is 0.8792, 
noisyNet noise sample is [array([0.6253531], dtype=float32), -1.1106793]. 
=============================================
[2019-04-04 07:50:44,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.60652602e-10 1.00246991e-10 3.50735763e-24 1.21646339e-11
 3.67479935e-11 1.05566374e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 07:50:44,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0500
[2019-04-04 07:50:44,943] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.316666666666667, 86.5, 0.0, 0.0, 26.0, 24.59527120127176, 0.1945476055171012, 0.0, 1.0, 18737.71282386491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 61800.0000, 
sim time next is 62400.0000, 
raw observation next is [5.133333333333334, 87.0, 0.0, 0.0, 26.0, 24.59089897093159, 0.1898883015005877, 0.0, 1.0, 33416.89176956963], 
processed observation next is [0.0, 0.7391304347826086, 0.6048014773776548, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5492415809109659, 0.5632961005001959, 0.0, 1.0, 0.15912805604556965], 
reward next is 0.8409, 
noisyNet noise sample is [array([0.6598181], dtype=float32), 0.3634768]. 
=============================================
[2019-04-04 07:50:47,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5476951e-09 1.7810499e-09 1.2428457e-22 1.2023767e-10 5.0835561e-11
 5.2076485e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:47,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9593
[2019-04-04 07:50:47,133] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666667, 95.33333333333333, 0.0, 0.0, 26.0, 24.46798957150236, 0.1834816950130772, 0.0, 1.0, 40127.40013445167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 81600.0000, 
sim time next is 82200.0000, 
raw observation next is [0.3333333333333333, 95.16666666666667, 0.0, 0.0, 26.0, 24.44366966321932, 0.1789108912398988, 0.0, 1.0, 40102.08782873357], 
processed observation next is [0.0, 0.9565217391304348, 0.4718374884579871, 0.9516666666666667, 0.0, 0.0, 0.6666666666666666, 0.5369724719349435, 0.559636963746633, 0.0, 1.0, 0.1909623229939694], 
reward next is 0.8090, 
noisyNet noise sample is [array([0.86147034], dtype=float32), -0.63751686]. 
=============================================
[2019-04-04 07:50:50,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.2755521e-12 4.8416583e-11 2.5372263e-24 1.1342731e-11 1.4337344e-12
 1.5995087e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:50,312] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8692
[2019-04-04 07:50:50,404] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 44.00000000000001, 91.0, 673.3333333333333, 26.0, 25.77142620877013, 0.4708756682267164, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 306600.0000, 
sim time next is 307200.0000, 
raw observation next is [-9.5, 44.0, 93.0, 652.1666666666666, 26.0, 25.97107000882829, 0.5033108253199056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31, 0.7206261510128913, 0.6666666666666666, 0.664255834069024, 0.6677702751066352, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85722584], dtype=float32), 0.21800624]. 
=============================================
[2019-04-04 07:50:55,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.1727733e-09 1.1914745e-08 1.1594119e-19 1.9571680e-09 1.5114334e-09
 8.1012913e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:50:55,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4440
[2019-04-04 07:50:55,900] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.2, 67.5, 0.0, 0.0, 26.0, 23.39078049263844, -0.0872996104838607, 0.0, 1.0, 47518.70152752692], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 347400.0000, 
sim time next is 348000.0000, 
raw observation next is [-14.3, 68.0, 0.0, 0.0, 26.0, 23.32025584524159, -0.09512990840352377, 0.0, 1.0, 47583.1332056958], 
processed observation next is [1.0, 0.0, 0.06648199445983377, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4433546537701325, 0.46829003053215873, 0.0, 1.0, 0.22658634859855142], 
reward next is 0.7734, 
noisyNet noise sample is [array([-0.8712952], dtype=float32), -1.3248061]. 
=============================================
[2019-04-04 07:50:55,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[64.535675]
 [64.56879 ]
 [64.6028  ]
 [64.672966]
 [64.65932 ]], R is [[64.67046356]
 [64.79747772]
 [64.92355347]
 [65.04870605]
 [65.17295837]].
[2019-04-04 07:51:02,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0866528e-10 2.9911631e-09 3.6425043e-22 1.0322047e-10 8.9213061e-11
 8.2801740e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:51:02,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4481
[2019-04-04 07:51:02,893] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 79.66666666666667, 0.0, 0.0, 26.0, 24.39487747930847, 0.1466998827211072, 0.0, 1.0, 44189.51689022614], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 254400.0000, 
sim time next is 255000.0000, 
raw observation next is [-3.9, 80.83333333333334, 0.0, 0.0, 26.0, 24.3715619244595, 0.1466708100118445, 0.0, 1.0, 44204.05894099727], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.8083333333333335, 0.0, 0.0, 0.6666666666666666, 0.5309634937049582, 0.5488902700039482, 0.0, 1.0, 0.21049551876665365], 
reward next is 0.7895, 
noisyNet noise sample is [array([0.37925443], dtype=float32), -0.19802316]. 
=============================================
[2019-04-04 07:51:02,922] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.35079 ]
 [75.39049 ]
 [75.32491 ]
 [75.320625]
 [75.28547 ]], R is [[75.48096466]
 [75.51573181]
 [75.55020142]
 [75.58432007]
 [75.61804199]].
[2019-04-04 07:51:11,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.11188087e-10 6.31054653e-11 7.30382153e-26 6.14542445e-12
 1.19604474e-11 6.23396937e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 07:51:11,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7805
[2019-04-04 07:51:11,109] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.616666666666667, 85.33333333333334, 0.0, 0.0, 26.0, 24.76780303131051, 0.2213770200632138, 0.0, 1.0, 39865.96741854811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 529800.0000, 
sim time next is 530400.0000, 
raw observation next is [3.433333333333334, 84.66666666666667, 0.0, 0.0, 26.0, 24.74927466293748, 0.2180913858887323, 0.0, 1.0, 39917.03401161416], 
processed observation next is [0.0, 0.13043478260869565, 0.5577100646352725, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.5624395552447901, 0.5726971286295774, 0.0, 1.0, 0.19008111434101982], 
reward next is 0.8099, 
noisyNet noise sample is [array([2.0185387], dtype=float32), 0.22821245]. 
=============================================
[2019-04-04 07:51:23,977] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.2785001e-10 1.6882401e-09 6.2447971e-23 1.5690235e-10 2.6220312e-11
 6.8940150e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:51:23,981] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3978
[2019-04-04 07:51:24,051] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.86734112436041, 0.2034636400330516, 0.0, 1.0, 55076.37726380645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 669600.0000, 
sim time next is 670200.0000, 
raw observation next is [-1.383333333333333, 57.83333333333333, 0.0, 0.0, 26.0, 24.86993841729884, 0.2055641925384671, 0.0, 1.0, 51417.03919983044], 
processed observation next is [0.0, 0.782608695652174, 0.4242843951985227, 0.5783333333333333, 0.0, 0.0, 0.6666666666666666, 0.5724948681082367, 0.5685213975128224, 0.0, 1.0, 0.24484304380871638], 
reward next is 0.7552, 
noisyNet noise sample is [array([-0.00059015], dtype=float32), 1.1642314]. 
=============================================
[2019-04-04 07:51:25,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.3192591e-11 2.5836477e-10 1.9015574e-23 3.4405000e-11 5.5521295e-12
 3.1398681e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:51:25,822] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5808
[2019-04-04 07:51:25,858] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 75.0, 0.0, 0.0, 26.0, 24.31524411975339, 0.04604349976472411, 0.0, 1.0, 41637.81344966263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 705000.0000, 
sim time next is 705600.0000, 
raw observation next is [-2.8, 75.0, 0.0, 0.0, 26.0, 24.26489777017506, 0.03775164833085152, 0.0, 1.0, 41647.60703996824], 
processed observation next is [1.0, 0.17391304347826086, 0.38504155124653744, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5220748141812551, 0.5125838827769506, 0.0, 1.0, 0.19832193828556305], 
reward next is 0.8017, 
noisyNet noise sample is [array([-0.54238755], dtype=float32), 0.67784977]. 
=============================================
[2019-04-04 07:51:27,611] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6968241e-10 5.3836852e-10 1.3517228e-23 7.6558725e-11 2.3449197e-11
 1.7478674e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:51:27,612] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5764
[2019-04-04 07:51:27,650] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 75.0, 0.0, 0.0, 26.0, 24.29300458240709, 0.06332563422430218, 0.0, 1.0, 41551.88809463752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 703200.0000, 
sim time next is 703800.0000, 
raw observation next is [-3.1, 75.0, 0.0, 0.0, 26.0, 24.36372609058783, 0.05828376540233238, 0.0, 1.0, 41572.6597455455], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5303105075489857, 0.5194279218007775, 0.0, 1.0, 0.1979650464073595], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.42292926], dtype=float32), -1.1693571]. 
=============================================
[2019-04-04 07:51:28,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0140460e-10 1.1257606e-09 6.6947556e-24 1.4703642e-10 2.2902241e-11
 7.5188174e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:51:28,799] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9361
[2019-04-04 07:51:28,815] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.21187285909551, 0.02527812369025825, 0.0, 1.0, 41644.02707152075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 711000.0000, 
sim time next is 711600.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.18729137398718, 0.03180880312454996, 0.0, 1.0, 41683.46491966456], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5156076144989316, 0.51060293437485, 0.0, 1.0, 0.19849269009364076], 
reward next is 0.8015, 
noisyNet noise sample is [array([-1.0050676], dtype=float32), -0.89920294]. 
=============================================
[2019-04-04 07:51:42,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7746807e-10 1.7880983e-10 2.6224154e-23 9.1223015e-11 1.6916647e-11
 2.0093287e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:51:42,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1894
[2019-04-04 07:51:43,022] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 82.0, 47.0, 26.0, 24.88112943336666, 0.2292775556857284, 0.0, 1.0, 35054.52766636142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 658800.0000, 
sim time next is 659400.0000, 
raw observation next is [-0.6, 54.0, 82.33333333333334, 44.0, 26.0, 24.90537562417295, 0.2280504114303763, 0.0, 1.0, 24212.01118608043], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.2744444444444445, 0.04861878453038674, 0.6666666666666666, 0.5754479686810793, 0.5760168038101254, 0.0, 1.0, 0.11529529136228778], 
reward next is 0.8847, 
noisyNet noise sample is [array([1.1057081], dtype=float32), -0.48040256]. 
=============================================
[2019-04-04 07:51:56,245] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 07:51:56,311] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 07:51:56,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:56,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run23
[2019-04-04 07:51:56,371] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 07:51:56,371] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:56,373] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run23
[2019-04-04 07:51:56,484] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 07:51:56,485] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 07:51:56,492] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run23
[2019-04-04 07:52:16,214] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1935343], dtype=float32), -0.09640057]
[2019-04-04 07:52:16,215] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-10.00435827583333, 78.52981741833332, 0.0, 0.0, 26.0, 22.9445752432997, -0.2009997467022619, 0.0, 1.0, 44018.25176064837]
[2019-04-04 07:52:16,215] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 07:52:16,215] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.1344162e-09 2.3878992e-09 2.9294035e-22 4.8938670e-10 1.5280797e-10
 9.1741502e-13 1.0000000e+00], sampled 0.3832026319201929
[2019-04-04 07:53:24,288] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1935343], dtype=float32), -0.09640057]
[2019-04-04 07:53:24,289] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.1, 88.0, 83.33333333333334, 0.0, 26.0, 26.30586506898782, 0.6246989675444394, 1.0, 1.0, 0.0]
[2019-04-04 07:53:24,289] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 07:53:24,290] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.9167567e-12 7.8093686e-12 8.9921469e-27 1.7224021e-12 2.5846491e-13
 1.0876949e-15 1.0000000e+00], sampled 0.7885563852255214
[2019-04-04 07:55:13,582] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 07:55:43,889] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 07:55:48,094] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 07:55:49,143] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2200000, evaluation results [2200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 07:55:49,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8012496e-10 8.1915125e-10 1.7110403e-23 9.3392537e-11 1.7366074e-11
 1.8447853e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:55:49,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8170
[2019-04-04 07:55:49,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2285651e-11 4.7934455e-11 1.5433484e-26 5.7217414e-13 1.1865463e-12
 6.6137074e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:55:49,369] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.633333333333333, 74.33333333333334, 0.0, 0.0, 26.0, 23.9321068257886, 0.04251415032067431, 0.0, 1.0, 41209.84469082539], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 789600.0000, 
sim time next is 790200.0000, 
raw observation next is [-7.55, 74.5, 0.0, 0.0, 26.0, 23.93427236708271, 0.03514395388021724, 0.0, 1.0, 41202.29714630164], 
processed observation next is [1.0, 0.13043478260869565, 0.25346260387811637, 0.745, 0.0, 0.0, 0.6666666666666666, 0.4945226972568924, 0.5117146512934058, 0.0, 1.0, 0.19620141498238874], 
reward next is 0.8038, 
noisyNet noise sample is [array([0.6762306], dtype=float32), -0.07030899]. 
=============================================
[2019-04-04 07:55:49,373] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9217
[2019-04-04 07:55:49,380] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 83.0, 0.0, 0.0, 26.0, 25.45043610125441, 0.4488844861757624, 0.0, 1.0, 61194.93981076749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 975600.0000, 
sim time next is 976200.0000, 
raw observation next is [9.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.4388775968943, 0.459051720860788, 0.0, 1.0, 49411.89516701935], 
processed observation next is [1.0, 0.30434782608695654, 0.7368421052631581, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6199064664078584, 0.6530172402869293, 0.0, 1.0, 0.23529473889056832], 
reward next is 0.7647, 
noisyNet noise sample is [array([-0.9173244], dtype=float32), -0.3462845]. 
=============================================
[2019-04-04 07:56:06,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1393451e-12 4.7776907e-11 7.2291699e-27 5.2291405e-12 4.9883838e-13
 2.7053303e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:06,839] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9056
[2019-04-04 07:56:06,854] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 67.66666666666667, 0.0, 0.0, 26.0, 25.71078473151037, 0.669300627801188, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1120800.0000, 
sim time next is 1121400.0000, 
raw observation next is [11.9, 68.5, 0.0, 0.0, 26.0, 25.73737980849132, 0.6660460143941404, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7922437673130196, 0.685, 0.0, 0.0, 0.6666666666666666, 0.64478165070761, 0.7220153381313801, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97451705], dtype=float32), 1.7321391]. 
=============================================
[2019-04-04 07:56:09,612] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6472321e-10 1.2795766e-10 4.7573879e-25 6.4248286e-12 6.4155833e-12
 3.5229015e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:09,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7780
[2019-04-04 07:56:09,625] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.25, 94.5, 0.0, 0.0, 26.0, 23.7408336412213, 0.196514944794371, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1225800.0000, 
sim time next is 1226400.0000, 
raw observation next is [15.16666666666667, 95.0, 0.0, 0.0, 26.0, 23.71701098420755, 0.1914931679634454, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.17391304347826086, 0.8827331486611266, 0.95, 0.0, 0.0, 0.6666666666666666, 0.47641758201729595, 0.5638310559878151, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.058761], dtype=float32), -0.21904038]. 
=============================================
[2019-04-04 07:56:12,173] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0627232e-11 1.6559670e-11 9.5826493e-27 1.4316384e-12 5.0015217e-13
 4.6624039e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:12,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6413
[2019-04-04 07:56:12,226] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.9, 98.66666666666667, 0.0, 0.0, 26.0, 24.5841763306522, 0.42735952483161, 0.0, 1.0, 45616.4650487844], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1272000.0000, 
sim time next is 1272600.0000, 
raw observation next is [10.25, 98.0, 0.0, 0.0, 26.0, 24.59008743476455, 0.4330187011210192, 0.0, 1.0, 45080.17091632891], 
processed observation next is [0.0, 0.7391304347826086, 0.7465373961218837, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5491739528970457, 0.6443395670403397, 0.0, 1.0, 0.21466748055394722], 
reward next is 0.7853, 
noisyNet noise sample is [array([0.7374099], dtype=float32), 0.22888474]. 
=============================================
[2019-04-04 07:56:19,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7967777e-11 7.8215073e-11 2.8993033e-26 1.3955915e-12 7.5150503e-13
 2.8705521e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:19,413] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4368
[2019-04-04 07:56:19,443] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.08333333333333, 78.66666666666667, 0.0, 0.0, 26.0, 25.65863464026014, 0.633501668783965, 0.0, 1.0, 24076.08069678874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1129800.0000, 
sim time next is 1130400.0000, 
raw observation next is [10.0, 79.0, 0.0, 0.0, 26.0, 25.64847153902357, 0.6311922600853407, 0.0, 1.0, 29239.60968491439], 
processed observation next is [0.0, 0.08695652173913043, 0.739612188365651, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6373726282519643, 0.710397420028447, 0.0, 1.0, 0.13923623659483042], 
reward next is 0.8608, 
noisyNet noise sample is [array([0.15473303], dtype=float32), -0.19463997]. 
=============================================
[2019-04-04 07:56:23,423] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.1765410e-10 4.2073503e-10 8.8531556e-24 3.4180912e-11 3.1328620e-11
 1.4381232e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:23,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6966
[2019-04-04 07:56:23,484] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.45273500032571, 0.4729432189747944, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1409400.0000, 
sim time next is 1410000.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.56989485602632, 0.4634661032222374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6308245713355266, 0.6544887010740791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40703538], dtype=float32), -0.76535475]. 
=============================================
[2019-04-04 07:56:23,503] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.09817 ]
 [86.942566]
 [82.12349 ]
 [82.1375  ]
 [82.15846 ]], R is [[88.60800171]
 [88.72192383]
 [88.83470917]
 [88.76239014]
 [88.69080353]].
[2019-04-04 07:56:38,387] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.9724708e-10 5.7961130e-10 1.0903123e-22 4.5545501e-11 1.8130922e-11
 1.0213677e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:38,388] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3329
[2019-04-04 07:56:38,424] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01102720880633, 0.332258723262512, 0.0, 1.0, 79134.87633459154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1798200.0000, 
sim time next is 1798800.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.08115871112037, 0.3342703463961108, 0.0, 1.0, 57553.53650542837], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5900965592600308, 0.6114234487987036, 0.0, 1.0, 0.2740644595496589], 
reward next is 0.7259, 
noisyNet noise sample is [array([-0.3033702], dtype=float32), -0.14799725]. 
=============================================
[2019-04-04 07:56:42,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2783160e-12 1.8888772e-11 1.0488055e-25 4.5671618e-12 1.2333141e-12
 1.0411140e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:42,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1991
[2019-04-04 07:56:42,174] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 74.5, 0.0, 26.0, 25.89499361336951, 0.5372925768771402, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [1.1, 90.66666666666667, 77.33333333333334, 0.0, 26.0, 25.90716680374458, 0.5332132650117086, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.49307479224376743, 0.9066666666666667, 0.25777777777777783, 0.0, 0.6666666666666666, 0.658930566978715, 0.6777377550039029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11521838], dtype=float32), 2.0722473]. 
=============================================
[2019-04-04 07:56:55,016] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2117363e-09 4.4962154e-09 4.5619342e-21 1.5580914e-09 7.5047629e-10
 4.2099067e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:56:55,016] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5085
[2019-04-04 07:56:55,038] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 82.0, 0.0, 0.0, 26.0, 24.17710118677645, 0.05934080920024202, 0.0, 1.0, 45012.91543737747], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1900800.0000, 
sim time next is 1901400.0000, 
raw observation next is [-7.3, 80.83333333333334, 0.0, 0.0, 26.0, 24.14097215774492, 0.05156156409778412, 0.0, 1.0, 45049.20743142415], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.8083333333333335, 0.0, 0.0, 0.6666666666666666, 0.5117476798120766, 0.5171871880325947, 0.0, 1.0, 0.21452003538773404], 
reward next is 0.7855, 
noisyNet noise sample is [array([-0.63136256], dtype=float32), 0.11473978]. 
=============================================
[2019-04-04 07:57:16,987] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8173353e-09 8.3879002e-09 2.5062837e-21 8.0572887e-10 2.8665106e-10
 4.5175677e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:57:16,987] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8348
[2019-04-04 07:57:17,008] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.51514084383789, -0.1256645801738149, 0.0, 1.0, 44997.6479346148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1921200.0000, 
sim time next is 1921800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.43377019517114, -0.1391846257816527, 0.0, 1.0, 44929.38587972718], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.6666666666666666, 0.45281418293092823, 0.4536051247394491, 0.0, 1.0, 0.21394945657012943], 
reward next is 0.7861, 
noisyNet noise sample is [array([0.3006312], dtype=float32), -0.34550792]. 
=============================================
[2019-04-04 07:57:25,051] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.1079971e-10 2.2960522e-09 1.5460386e-22 7.3132489e-10 6.7074152e-11
 3.2623210e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:57:25,055] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1934
[2019-04-04 07:57:25,082] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574456735, 0.0587030164282273, 0.0, 1.0, 41111.63103505894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008200.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13805997661969, 0.05771806141708869, 0.0, 1.0, 41132.93142159054], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5115049980516408, 0.5192393538056962, 0.0, 1.0, 0.195871102007574], 
reward next is 0.8041, 
noisyNet noise sample is [array([1.3074932], dtype=float32), 0.28992772]. 
=============================================
[2019-04-04 07:57:29,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9539526e-12 7.5805029e-11 2.4227196e-24 8.3782781e-12 1.2979569e-12
 1.0192843e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:57:29,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1922
[2019-04-04 07:57:29,135] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 71.0, 121.3333333333333, 0.0, 26.0, 25.93190664173905, 0.3783519087939954, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2198400.0000, 
sim time next is 2199000.0000, 
raw observation next is [-4.583333333333333, 71.0, 125.6666666666667, 0.0, 26.0, 25.89299549151895, 0.3506213797450933, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3356417359187443, 0.71, 0.418888888888889, 0.0, 0.6666666666666666, 0.6577496242932458, 0.6168737932483644, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56629723], dtype=float32), -1.2901344]. 
=============================================
[2019-04-04 07:57:29,139] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.96222 ]
 [82.955154]
 [83.790375]
 [84.83783 ]
 [86.17154 ]], R is [[83.43566132]
 [83.6013031 ]
 [83.76528931]
 [83.92763519]
 [84.08835602]].
[2019-04-04 07:57:37,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.0864462e-11 1.5840523e-10 4.6675101e-24 1.5869245e-11 5.9941297e-12
 1.9015988e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:57:37,980] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2587
[2019-04-04 07:57:38,052] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 50.0, 11.0, 0.0, 26.0, 24.50627878462837, 0.2626086404935054, 1.0, 1.0, 196968.1535768699], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2308800.0000, 
sim time next is 2309400.0000, 
raw observation next is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 24.89548908209278, 0.3161692003843071, 1.0, 1.0, 135165.556747875], 
processed observation next is [1.0, 0.7391304347826086, 0.43767313019390586, 0.505, 0.0, 0.0, 0.6666666666666666, 0.5746240901743983, 0.6053897334614357, 1.0, 1.0, 0.6436455083232143], 
reward next is 0.3564, 
noisyNet noise sample is [array([-0.5511417], dtype=float32), 0.93216455]. 
=============================================
[2019-04-04 07:57:42,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8721492e-09 4.4347646e-09 2.0143233e-22 5.2789612e-10 2.7815836e-10
 1.5717116e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:57:42,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0496
[2019-04-04 07:57:42,799] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.716666666666667, 64.5, 0.0, 0.0, 26.0, 24.45850954278087, 0.171624166073775, 0.0, 1.0, 40099.56517515703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2346600.0000, 
sim time next is 2347200.0000, 
raw observation next is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.42486263714591, 0.1717043926285801, 0.0, 1.0, 40215.70993558245], 
processed observation next is [0.0, 0.17391304347826086, 0.38504155124653744, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5354052197621592, 0.5572347975428601, 0.0, 1.0, 0.1915033806456307], 
reward next is 0.8085, 
noisyNet noise sample is [array([-0.28631145], dtype=float32), 0.53760415]. 
=============================================
[2019-04-04 07:58:04,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4309948e-10 3.6961587e-09 5.0071877e-22 3.5275299e-10 1.5591192e-10
 2.3040946e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:58:04,246] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8532
[2019-04-04 07:58:04,307] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.516666666666667, 42.66666666666667, 0.0, 0.0, 26.0, 24.98104561169529, 0.3284642584726459, 0.0, 1.0, 76036.09413439865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2577000.0000, 
sim time next is 2577600.0000, 
raw observation next is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.95026502417611, 0.3355338535142305, 0.0, 1.0, 76772.56585334317], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.44, 0.0, 0.0, 0.6666666666666666, 0.579188752014676, 0.6118446178380769, 0.0, 1.0, 0.36558364692068174], 
reward next is 0.6344, 
noisyNet noise sample is [array([0.8042605], dtype=float32), -2.4477906]. 
=============================================
[2019-04-04 07:58:07,866] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2735634e-11 1.0431416e-10 2.4016478e-24 2.0728686e-11 5.0796472e-12
 1.8797626e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:58:07,867] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4182
[2019-04-04 07:58:07,926] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.66666666666667, 88.50000000000001, 91.33333333333333, 518.0, 26.0, 26.01044182239573, 0.416240722952304, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2711400.0000, 
sim time next is 2712000.0000, 
raw observation next is [-13.33333333333333, 86.0, 94.16666666666667, 565.0, 26.0, 25.9535641151597, 0.4212913386040193, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.09325946445060027, 0.86, 0.3138888888888889, 0.6243093922651933, 0.6666666666666666, 0.6627970095966417, 0.6404304462013398, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10204039], dtype=float32), -0.029250085]. 
=============================================
[2019-04-04 07:58:07,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.88798 ]
 [83.98043 ]
 [84.07659 ]
 [84.118355]
 [84.059326]], R is [[84.00530243]
 [84.16525269]
 [84.32360077]
 [84.48036194]
 [84.63555908]].
[2019-04-04 07:58:31,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5891205e-11 2.3255413e-10 8.9477463e-24 8.8414814e-12 3.6519906e-12
 2.0361098e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:58:31,599] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3033
[2019-04-04 07:58:31,649] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.04050753596379, 0.3775653204153253, 1.0, 1.0, 63032.74195368057], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2921400.0000, 
sim time next is 2922000.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 24.93941954111542, 0.3765635391815049, 1.0, 1.0, 105100.5311598866], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5782849617596183, 0.6255211797271684, 1.0, 1.0, 0.5004787198089838], 
reward next is 0.4995, 
noisyNet noise sample is [array([0.92494667], dtype=float32), 0.9682787]. 
=============================================
[2019-04-04 07:58:31,672] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.59822 ]
 [78.894394]
 [84.30394 ]
 [84.54111 ]
 [84.934395]], R is [[82.05089569]
 [81.93022919]
 [82.1109314 ]
 [82.28982544]
 [82.46692657]].
[2019-04-04 07:58:34,706] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0620195e-09 4.1280925e-09 1.8562135e-21 4.2258239e-10 1.8354313e-10
 1.9798805e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:58:34,736] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2191
[2019-04-04 07:58:34,751] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14614501331151, 0.3207366178417431, 0.0, 1.0, 38897.56796914036], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3016800.0000, 
sim time next is 3017400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.12210939207488, 0.3212913611870472, 0.0, 1.0, 38766.97824046833], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5935091160062399, 0.6070971203956824, 0.0, 1.0, 0.18460465828794442], 
reward next is 0.8154, 
noisyNet noise sample is [array([-0.3730586], dtype=float32), 0.5063029]. 
=============================================
[2019-04-04 07:58:53,557] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.07397267e-11 1.40265133e-10 1.04925935e-22 3.27568007e-11
 3.09969411e-12 1.06090985e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 07:58:53,562] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5628
[2019-04-04 07:58:53,590] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 50.0, 102.8333333333333, 739.0, 26.0, 26.36828984600321, 0.6730847606401636, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [-3.166666666666667, 50.0, 99.66666666666666, 726.0, 26.0, 26.51813685043662, 0.6924886092527304, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3748845798707295, 0.5, 0.3322222222222222, 0.8022099447513812, 0.6666666666666666, 0.7098447375363849, 0.7308295364175769, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6858872], dtype=float32), 1.144597]. 
=============================================
[2019-04-04 07:58:55,241] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5114990e-10 1.0869392e-09 1.4287077e-21 8.7020377e-11 6.3886611e-11
 9.2569030e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:58:55,242] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9959
[2019-04-04 07:58:55,268] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 52.5, 11.0, 133.0, 26.0, 25.71167221545306, 0.471482319031125, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3346200.0000, 
sim time next is 3346800.0000, 
raw observation next is [-2.666666666666667, 53.33333333333333, 9.166666666666668, 110.8333333333333, 26.0, 25.51120611111508, 0.4916682270770402, 1.0, 1.0, 123454.4263152411], 
processed observation next is [1.0, 0.7391304347826086, 0.38873499538319484, 0.5333333333333333, 0.030555555555555558, 0.12246777163904232, 0.6666666666666666, 0.6259338425929233, 0.66388940902568, 1.0, 1.0, 0.5878782205487671], 
reward next is 0.4121, 
noisyNet noise sample is [array([0.07753184], dtype=float32), -1.284868]. 
=============================================
[2019-04-04 07:58:57,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8993149e-10 3.8068158e-09 1.5263788e-21 2.6616884e-10 2.0469897e-10
 3.6413767e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 07:58:57,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2671
[2019-04-04 07:58:57,236] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.8515705172232, 0.272577583115875, 0.0, 1.0, 41268.77728834154], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3383400.0000, 
sim time next is 3384000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.86433303947717, 0.2653706237064508, 0.0, 1.0, 41382.65975984687], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5720277532897642, 0.5884568745688169, 0.0, 1.0, 0.19706028457069938], 
reward next is 0.8029, 
noisyNet noise sample is [array([-0.26636592], dtype=float32), -0.20048763]. 
=============================================
[2019-04-04 07:58:57,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.49494 ]
 [73.50714 ]
 [73.522545]
 [73.53957 ]
 [73.561104]], R is [[73.54148865]
 [73.60955811]
 [73.67733765]
 [73.74464417]
 [73.81141663]].
[2019-04-04 07:59:01,380] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6613249e-11 2.8717861e-10 3.6582935e-23 8.7832283e-11 9.9918598e-12
 8.0894135e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:01,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6805
[2019-04-04 07:59:01,438] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.65599998130639, 0.6190256363119716, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3265200.0000, 
sim time next is 3265800.0000, 
raw observation next is [-4.0, 66.0, 0.0, 0.0, 26.0, 25.80498863774775, 0.6355447593732673, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.66, 0.0, 0.0, 0.6666666666666666, 0.6504157198123126, 0.7118482531244225, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16779692], dtype=float32), 2.7419329]. 
=============================================
[2019-04-04 07:59:14,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.87071469e-10 3.61636610e-10 2.88924945e-23 1.02282106e-10
 1.60850881e-11 4.37307424e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 07:59:14,737] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5490
[2019-04-04 07:59:14,754] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.666666666666667, 53.0, 67.83333333333333, 552.5, 26.0, 25.50174911250342, 0.4736384600501942, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3687600.0000, 
sim time next is 3688200.0000, 
raw observation next is [4.5, 54.5, 64.0, 522.0, 26.0, 25.49465389744457, 0.4674638167414282, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5872576177285319, 0.545, 0.21333333333333335, 0.5767955801104973, 0.6666666666666666, 0.6245544914537143, 0.6558212722471427, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02796607], dtype=float32), -0.3063262]. 
=============================================
[2019-04-04 07:59:15,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.5255856e-12 9.3906202e-12 6.9240185e-25 1.2412316e-12 1.2534578e-12
 7.9321705e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:15,349] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3239
[2019-04-04 07:59:15,354] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8333333333333334, 60.83333333333334, 111.0, 783.3333333333333, 26.0, 26.27458422646531, 0.5802662089988812, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3495000.0000, 
sim time next is 3495600.0000, 
raw observation next is [1.0, 61.0, 112.0, 790.0, 26.0, 26.17833164058857, 0.5841821870726277, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.61, 0.37333333333333335, 0.8729281767955801, 0.6666666666666666, 0.6815276367157143, 0.6947273956908759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52469903], dtype=float32), 0.9726123]. 
=============================================
[2019-04-04 07:59:24,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7174010e-11 4.3504523e-11 1.0465200e-24 2.1676045e-11 3.4829398e-12
 1.2158630e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:24,415] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6869
[2019-04-04 07:59:24,466] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 56.33333333333334, 76.83333333333334, 415.5000000000001, 26.0, 25.66757814434812, 0.4581106897049016, 1.0, 1.0, 9370.64502307263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3918000.0000, 
sim time next is 3918600.0000, 
raw observation next is [-8.0, 55.5, 91.0, 466.0, 26.0, 25.77258075045681, 0.4679726537523053, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.555, 0.30333333333333334, 0.5149171270718232, 0.6666666666666666, 0.6477150625380675, 0.6559908845841017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5996428], dtype=float32), -1.2347927]. 
=============================================
[2019-04-04 07:59:42,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0034472e-10 5.4422550e-10 1.2389020e-23 8.7093104e-11 6.6174561e-12
 1.7641921e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:42,627] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0130
[2019-04-04 07:59:42,665] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 42.33333333333334, 0.0, 0.0, 26.0, 24.98459631677888, 0.3094680506145677, 0.0, 1.0, 198502.8380438001], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4216800.0000, 
sim time next is 4217400.0000, 
raw observation next is [1.2, 42.5, 0.0, 0.0, 26.0, 25.02412834567826, 0.3400845261247968, 0.0, 1.0, 185347.3007419609], 
processed observation next is [0.0, 0.8260869565217391, 0.4958448753462604, 0.425, 0.0, 0.0, 0.6666666666666666, 0.5853440288065217, 0.6133615087082656, 0.0, 1.0, 0.8826061940093376], 
reward next is 0.1174, 
noisyNet noise sample is [array([-1.5468608], dtype=float32), 0.104799144]. 
=============================================
[2019-04-04 07:59:44,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9550112e-10 4.2324463e-10 8.5519504e-24 7.6288038e-11 1.3136834e-11
 1.8827427e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:44,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5368
[2019-04-04 07:59:44,244] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 47.83333333333333, 0.0, 0.0, 26.0, 25.41376486019184, 0.3641398650635496, 0.0, 1.0, 36279.2637527503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4233000.0000, 
sim time next is 4233600.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42744188314253, 0.3628921095087436, 0.0, 1.0, 30638.16308405545], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6189534902618776, 0.6209640365029145, 0.0, 1.0, 0.14589601468597832], 
reward next is 0.8541, 
noisyNet noise sample is [array([1.2152606], dtype=float32), -1.6430941]. 
=============================================
[2019-04-04 07:59:52,114] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.5573762e-11 2.8056946e-10 3.7612383e-24 1.5110945e-11 6.8155074e-12
 3.8000090e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:52,115] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4682
[2019-04-04 07:59:52,156] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.266666666666667, 68.0, 0.0, 0.0, 26.0, 25.69314505212219, 0.5290747662552454, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4426800.0000, 
sim time next is 4427400.0000, 
raw observation next is [3.133333333333333, 68.0, 0.0, 0.0, 26.0, 25.62454132536067, 0.5170219224366304, 0.0, 1.0, 36690.21893839842], 
processed observation next is [1.0, 0.21739130434782608, 0.5493998153277933, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6353784437800559, 0.6723406408122101, 0.0, 1.0, 0.17471532827808772], 
reward next is 0.8253, 
noisyNet noise sample is [array([2.0601337], dtype=float32), 0.35626537]. 
=============================================
[2019-04-04 07:59:58,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6269074e-10 8.1524520e-10 1.1418488e-22 5.0538181e-11 3.6661882e-11
 1.1769550e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:58,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1485
[2019-04-04 07:59:58,323] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.29177047844758, 0.4406595535139126, 0.0, 1.0, 59451.28494261286], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4499400.0000, 
sim time next is 4500000.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.27526229705645, 0.44032460495667, 0.0, 1.0, 48146.87393250204], 
processed observation next is [1.0, 0.08695652173913043, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6062718580880375, 0.64677486831889, 0.0, 1.0, 0.22927082825000972], 
reward next is 0.7707, 
noisyNet noise sample is [array([0.88255066], dtype=float32), 0.34100917]. 
=============================================
[2019-04-04 07:59:58,348] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.263084]
 [80.171074]
 [79.901054]
 [79.43814 ]
 [79.46901 ]], R is [[80.25769806]
 [80.17201996]
 [79.95734406]
 [79.61948395]
 [79.68408203]].
[2019-04-04 07:59:59,030] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5404259e-10 1.9599836e-10 1.7430968e-24 5.2132590e-11 6.5919839e-12
 2.8738256e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 07:59:59,034] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1461
[2019-04-04 07:59:59,122] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 50.33333333333334, 91.66666666666667, 44.16666666666667, 26.0, 25.33211087118874, 0.3673355334058985, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4263600.0000, 
sim time next is 4264200.0000, 
raw observation next is [3.0, 51.0, 110.0, 53.0, 26.0, 25.55005820192433, 0.3944669983793156, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.5457063711911359, 0.51, 0.36666666666666664, 0.05856353591160221, 0.6666666666666666, 0.6291715168270274, 0.6314889994597719, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1267438], dtype=float32), 0.54649484]. 
=============================================
[2019-04-04 08:00:01,335] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2110862e-11 4.2672754e-11 6.0991999e-25 6.7668553e-12 6.3588978e-12
 3.1266907e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:00:01,335] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1653
[2019-04-04 08:00:01,355] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.916666666666666, 67.83333333333334, 0.0, 0.0, 26.0, 25.59646853743672, 0.5321480220212081, 0.0, 1.0, 65767.74913870385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4423800.0000, 
sim time next is 4424400.0000, 
raw observation next is [3.8, 68.0, 0.0, 0.0, 26.0, 25.55630572915067, 0.53268996589476, 0.0, 1.0, 66074.86926284489], 
processed observation next is [1.0, 0.21739130434782608, 0.5678670360110805, 0.68, 0.0, 0.0, 0.6666666666666666, 0.629692144095889, 0.6775633219649201, 0.0, 1.0, 0.3146422345849757], 
reward next is 0.6854, 
noisyNet noise sample is [array([0.0027393], dtype=float32), 0.3408908]. 
=============================================
[2019-04-04 08:00:03,038] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.9384750e-11 3.9601744e-10 5.0815570e-23 4.4269848e-11 1.7587645e-11
 6.7912994e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:00:03,042] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2323
[2019-04-04 08:00:03,054] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.47649264658408, 0.5056794874646059, 0.0, 1.0, 63151.90519822166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4666800.0000, 
sim time next is 4667400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.47709899577727, 0.5077615418601624, 0.0, 1.0, 45592.41195161594], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6230915829814391, 0.6692538472867208, 0.0, 1.0, 0.21710672357912353], 
reward next is 0.7829, 
noisyNet noise sample is [array([-0.41357875], dtype=float32), -1.2914158]. 
=============================================
[2019-04-04 08:00:03,416] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5518755e-12 1.7121042e-12 3.0875740e-27 6.0244659e-13 3.5422139e-14
 1.6408712e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:00:03,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9222
[2019-04-04 08:00:03,442] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 63.66666666666667, 158.1666666666667, 552.0, 26.0, 26.24458003417372, 0.5850884707709499, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4614000.0000, 
sim time next is 4614600.0000, 
raw observation next is [-0.3333333333333333, 61.83333333333333, 152.3333333333333, 595.0, 26.0, 26.36525466127706, 0.6097516791277625, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.6183333333333333, 0.5077777777777777, 0.6574585635359116, 0.6666666666666666, 0.6971045551064217, 0.7032505597092542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13728863], dtype=float32), 0.13218494]. 
=============================================
[2019-04-04 08:00:10,798] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 08:00:10,799] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:00:10,799] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:00:10,799] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:00:10,800] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:00:10,800] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:00:10,800] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:00:10,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run24
[2019-04-04 08:00:10,830] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run24
[2019-04-04 08:00:10,852] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run24
[2019-04-04 08:01:44,858] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.19285993], dtype=float32), -0.10215317]
[2019-04-04 08:01:44,858] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-5.868482337166667, 85.79357646166667, 0.0, 0.0, 26.0, 23.8591617269206, -0.002963002329930043, 0.0, 1.0, 40805.94095967646]
[2019-04-04 08:01:44,858] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:01:44,859] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6970024e-09 3.3989809e-09 9.8315364e-22 5.8763938e-10 2.7843597e-10
 1.7685053e-12 1.0000000e+00], sampled 0.9216941740714845
[2019-04-04 08:02:46,966] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.19285993], dtype=float32), -0.10215317]
[2019-04-04 08:02:46,966] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.65, 45.5, 21.0, 311.0, 26.0, 25.79225753058568, 0.4763560640046591, 1.0, 1.0, 0.0]
[2019-04-04 08:02:46,966] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:02:46,967] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.3046640e-11 2.8992147e-11 3.7896806e-25 6.4264956e-12 8.5459488e-13
 6.1335761e-15 1.0000000e+00], sampled 0.8968007500465098
[2019-04-04 08:03:09,670] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.19285993], dtype=float32), -0.10215317]
[2019-04-04 08:03:09,670] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.15846558, 57.46388681, 6.499459804000001, 315.24521375, 26.0, 25.21782922726767, 0.3837766922877092, 0.0, 1.0, 18680.41167023054]
[2019-04-04 08:03:09,670] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:03:09,671] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.0593161e-10 4.0233977e-10 1.4697244e-22 8.9789717e-11 3.3948244e-11
 1.9417035e-13 1.0000000e+00], sampled 0.4122384014058533
[2019-04-04 08:03:17,656] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.19285993], dtype=float32), -0.10215317]
[2019-04-04 08:03:17,656] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.729296965, 54.17268981, 42.067840045, 798.51868365, 26.0, 25.87250799995969, 0.4952542020875219, 1.0, 1.0, 0.0]
[2019-04-04 08:03:17,657] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:03:17,658] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.08195935e-11 3.06274762e-11 2.91875774e-25 5.11097978e-12
 8.44071666e-13 4.12452233e-15 1.00000000e+00], sampled 0.5621514810157299
[2019-04-04 08:03:18,025] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:03:48,432] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 08:03:52,092] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:03:53,128] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2300000, evaluation results [2300000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:04:01,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6711302e-10 8.0748275e-10 4.3364167e-23 2.8053951e-10 2.3801303e-11
 1.7464544e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:01,928] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5598
[2019-04-04 08:04:01,952] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02368101859592, 0.3753852027088445, 0.0, 1.0, 164404.3176276435], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4822200.0000, 
sim time next is 4822800.0000, 
raw observation next is [1.0, 45.66666666666667, 0.0, 0.0, 26.0, 25.09485906149022, 0.3988946180337247, 0.0, 1.0, 89555.17651352371], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5912382551241849, 0.6329648726779082, 0.0, 1.0, 0.42645322149297005], 
reward next is 0.5735, 
noisyNet noise sample is [array([1.0203775], dtype=float32), 0.56001675]. 
=============================================
[2019-04-04 08:04:09,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1566073e-10 8.1679075e-10 1.2702233e-23 2.2847409e-10 4.0947679e-11
 1.2193613e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:09,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7090
[2019-04-04 08:04:09,527] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 38.66666666666667, 0.0, 0.0, 26.0, 25.536672121215, 0.3992174026060463, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4911600.0000, 
sim time next is 4912200.0000, 
raw observation next is [1.0, 38.0, 0.0, 0.0, 26.0, 25.54324057653847, 0.3897209323935988, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6286033813782058, 0.629906977464533, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01869588], dtype=float32), 0.06933127]. 
=============================================
[2019-04-04 08:04:09,631] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:09,631] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:09,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run18
[2019-04-04 08:04:26,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:26,973] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:26,976] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run18
[2019-04-04 08:04:27,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:27,107] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:27,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run18
[2019-04-04 08:04:28,241] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1782780e-09 2.5452480e-09 5.1973618e-23 4.0859230e-10 8.3668801e-11
 2.9502309e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:28,252] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0886
[2019-04-04 08:04:28,295] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8333333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 25.39974913508444, 0.4005063475768026, 0.0, 1.0, 42991.0823753345], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4830600.0000, 
sim time next is 4831200.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.38949725837604, 0.4004439696458449, 0.0, 1.0, 45993.85083615073], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6157914381980033, 0.6334813232152816, 0.0, 1.0, 0.21901833731500345], 
reward next is 0.7810, 
noisyNet noise sample is [array([0.5058709], dtype=float32), -1.131372]. 
=============================================
[2019-04-04 08:04:29,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:29,881] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:29,886] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run18
[2019-04-04 08:04:32,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:32,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:32,372] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run18
[2019-04-04 08:04:32,790] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.04115785e-10 1.06966325e-09 8.40918941e-24 5.92954436e-11
 1.97268174e-11 3.61429203e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 08:04:32,790] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7780
[2019-04-04 08:04:32,798] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.25, 27.5, 0.0, 0.0, 26.0, 25.93534602878092, 0.5558817985628172, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5095800.0000, 
sim time next is 5096400.0000, 
raw observation next is [8.2, 30.0, 0.0, 0.0, 26.0, 25.8586162867003, 0.544308307600441, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6897506925207757, 0.3, 0.0, 0.0, 0.6666666666666666, 0.6548846905583584, 0.6814361025334804, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.408297], dtype=float32), 0.021079984]. 
=============================================
[2019-04-04 08:04:33,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6636688e-11 5.1864409e-11 4.2026285e-24 3.4266291e-11 4.1350760e-12
 2.4701675e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:33,657] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9812
[2019-04-04 08:04:33,679] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 47.0, 282.0, 349.0, 26.0, 25.05703618440602, 0.3513981057688051, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4881600.0000, 
sim time next is 4882200.0000, 
raw observation next is [1.066666666666667, 46.66666666666667, 281.6666666666666, 362.6666666666667, 26.0, 25.05281940325503, 0.3570393889020651, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.49215143120960303, 0.46666666666666673, 0.9388888888888886, 0.4007366482504604, 0.6666666666666666, 0.5877349502712524, 0.6190131296340217, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5292357], dtype=float32), 0.12233431]. 
=============================================
[2019-04-04 08:04:33,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:33,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:33,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run18
[2019-04-04 08:04:33,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:33,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:33,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run18
[2019-04-04 08:04:35,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:35,597] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:35,600] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run18
[2019-04-04 08:04:37,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:37,552] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:37,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run18
[2019-04-04 08:04:38,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:38,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:38,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run18
[2019-04-04 08:04:41,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.4890582e-11 2.9727720e-11 4.0452509e-25 6.9379715e-12 2.7143899e-12
 4.6400878e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:41,299] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7485
[2019-04-04 08:04:41,373] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.9, 86.0, 85.66666666666666, 0.0, 26.0, 24.38095614677756, 0.1410785218270942, 0.0, 1.0, 41198.96603819998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 49200.0000, 
sim time next is 49800.0000, 
raw observation next is [7.8, 86.0, 84.33333333333334, 0.0, 26.0, 24.39847624105709, 0.1467568366699223, 0.0, 1.0, 33126.88310640195], 
processed observation next is [0.0, 0.5652173913043478, 0.6786703601108034, 0.86, 0.28111111111111114, 0.0, 0.6666666666666666, 0.5332063534214241, 0.5489189455566408, 0.0, 1.0, 0.15774706241143785], 
reward next is 0.8423, 
noisyNet noise sample is [array([0.35348764], dtype=float32), 1.7596604]. 
=============================================
[2019-04-04 08:04:42,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:42,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:42,208] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run18
[2019-04-04 08:04:51,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:04:51,101] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:04:51,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run18
[2019-04-04 08:04:54,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1397782e-10 1.3226176e-09 6.7927487e-22 1.8158203e-10 4.7978541e-11
 5.1508174e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:54,372] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1996
[2019-04-04 08:04:54,474] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.38570689878538, 0.1410276341777169, 0.0, 1.0, 44220.47196980369], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 255600.0000, 
sim time next is 256200.0000, 
raw observation next is [-4.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.3566064680519, 0.1320707039739155, 0.0, 1.0, 44256.88106710286], 
processed observation next is [1.0, 1.0, 0.3518005540166205, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.5297172056709917, 0.5440235679913051, 0.0, 1.0, 0.21074705270048982], 
reward next is 0.7893, 
noisyNet noise sample is [array([1.2798316], dtype=float32), 2.5280256]. 
=============================================
[2019-04-04 08:04:54,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2544024e-12 4.8834656e-12 6.1781552e-27 1.2558101e-12 5.0129704e-14
 4.9342396e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:54,678] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0588
[2019-04-04 08:04:54,778] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 42.0, 113.3333333333333, 735.0, 26.0, 26.91671859952339, 0.6924570134389462, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5046600.0000, 
sim time next is 5047200.0000, 
raw observation next is [3.0, 41.0, 114.0, 753.5, 26.0, 26.96853831076995, 0.718967737758439, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5457063711911359, 0.41, 0.38, 0.8325966850828729, 0.6666666666666666, 0.7473781925641626, 0.7396559125861463, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4784773], dtype=float32), 0.29142088]. 
=============================================
[2019-04-04 08:04:56,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0941941e-10 3.4096603e-10 1.2548208e-24 7.4514998e-11 1.6753880e-11
 5.4847606e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:04:56,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5890
[2019-04-04 08:04:56,351] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.9, 90.66666666666667, 92.5, 0.0, 26.0, 24.31302030504014, 0.1092863644306539, 0.0, 1.0, 35410.2030073198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [8.0, 89.5, 96.0, 0.0, 26.0, 24.33680959862573, 0.1155099110610531, 0.0, 1.0, 24220.52529228139], 
processed observation next is [0.0, 0.5217391304347826, 0.6842105263157896, 0.895, 0.32, 0.0, 0.6666666666666666, 0.5280674665521442, 0.5385033036870177, 0.0, 1.0, 0.11533583472514947], 
reward next is 0.8847, 
noisyNet noise sample is [array([1.8428656], dtype=float32), -0.64123416]. 
=============================================
[2019-04-04 08:04:56,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.025024]
 [84.97724 ]
 [84.915436]
 [84.88009 ]
 [84.90627 ]], R is [[85.05691528]
 [85.03772736]
 [85.00089264]
 [84.98083496]
 [85.02024078]].
[2019-04-04 08:05:01,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:05:01,877] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:05:01,880] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run18
[2019-04-04 08:05:02,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:05:02,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:05:02,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run18
[2019-04-04 08:05:02,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:05:02,413] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:05:02,417] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run18
[2019-04-04 08:05:02,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:05:02,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:05:02,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run18
[2019-04-04 08:05:07,671] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4710740e-10 1.0542454e-09 1.4855344e-23 1.2462359e-10 1.3423737e-11
 1.1403050e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:05:07,671] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3007
[2019-04-04 08:05:07,793] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.2, 66.66666666666666, 0.0, 0.0, 26.0, 25.03464077039532, 0.2968240949860606, 1.0, 1.0, 97352.64524469146], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 157200.0000, 
sim time next is 157800.0000, 
raw observation next is [-8.3, 67.33333333333334, 0.0, 0.0, 26.0, 24.98629882654301, 0.3043687129411883, 1.0, 1.0, 110398.8680526219], 
processed observation next is [1.0, 0.8260869565217391, 0.23268698060941828, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5821915688785841, 0.6014562376470628, 1.0, 1.0, 0.5257088954886757], 
reward next is 0.4743, 
noisyNet noise sample is [array([-1.236515], dtype=float32), -0.29876474]. 
=============================================
[2019-04-04 08:05:20,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5915676e-12 1.2492583e-10 2.5410535e-23 1.5547116e-11 4.7280378e-12
 1.1113300e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:05:20,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0084
[2019-04-04 08:05:20,907] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.683333333333334, 44.83333333333334, 87.0, 715.6666666666666, 26.0, 25.06840703956814, 0.4037761711770327, 1.0, 1.0, 101953.4833047579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 305400.0000, 
sim time next is 306000.0000, 
raw observation next is [-9.5, 44.0, 89.0, 694.5, 26.0, 25.47955481038625, 0.4482820464085409, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.2966666666666667, 0.7674033149171271, 0.6666666666666666, 0.6232962341988543, 0.6494273488028469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2297107], dtype=float32), -0.13755961]. 
=============================================
[2019-04-04 08:05:20,911] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.98904 ]
 [83.048416]
 [82.61315 ]
 [82.19048 ]
 [82.49815 ]], R is [[82.68899536]
 [82.3766098 ]
 [81.59754944]
 [81.10179901]
 [81.29077911]].
[2019-04-04 08:05:34,567] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0354242e-11 3.1462349e-10 3.5263963e-23 2.0268445e-11 3.4866355e-12
 4.1564611e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:05:34,567] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9534
[2019-04-04 08:05:34,619] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.100000000000001, 38.66666666666667, 34.33333333333334, 656.3333333333334, 26.0, 26.20811688913422, 0.4764981768221332, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 402000.0000, 
sim time next is 402600.0000, 
raw observation next is [-9.0, 38.33333333333334, 31.66666666666666, 605.6666666666667, 26.0, 25.91610289816067, 0.3046761938644502, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.21329639889196678, 0.3833333333333334, 0.10555555555555554, 0.6692449355432781, 0.6666666666666666, 0.6596752415133892, 0.6015587312881501, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0668508], dtype=float32), -0.10195157]. 
=============================================
[2019-04-04 08:05:36,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.0231057e-11 5.6964735e-11 6.0231769e-25 1.3118930e-11 2.1524503e-12
 4.5253591e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:05:36,751] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7577
[2019-04-04 08:05:36,822] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.916666666666667, 32.83333333333334, 49.0, 0.0, 26.0, 25.55872239528254, 0.2006265602407246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 465000.0000, 
sim time next is 465600.0000, 
raw observation next is [-5.633333333333334, 32.66666666666667, 55.50000000000001, 0.0, 26.0, 25.53908626653034, 0.1981979462165576, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30655586334256696, 0.3266666666666667, 0.18500000000000003, 0.0, 0.6666666666666666, 0.6282571888775283, 0.5660659820721858, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1028791], dtype=float32), 1.326363]. 
=============================================
[2019-04-04 08:05:41,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0882146e-10 5.0152691e-09 3.0797023e-21 6.8529254e-10 1.8090376e-10
 2.0174556e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:05:41,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3618
[2019-04-04 08:05:41,349] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.57892224971059, -0.06679103742811132, 0.0, 1.0, 45445.56087529077], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 436200.0000, 
sim time next is 436800.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.5874010208961, -0.07478199377538243, 0.0, 1.0, 45486.13177114395], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.4656167517413416, 0.4750726687415392, 0.0, 1.0, 0.21660062748163786], 
reward next is 0.7834, 
noisyNet noise sample is [array([-2.1481466], dtype=float32), 0.625631]. 
=============================================
[2019-04-04 08:05:43,543] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9656247e-10 1.7430303e-09 1.7083085e-22 4.7376386e-10 1.4209378e-10
 4.9597137e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:05:43,544] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0606
[2019-04-04 08:05:43,626] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.53252691059418, 0.1198662740117538, 0.0, 1.0, 41378.19040206765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 689400.0000, 
sim time next is 690000.0000, 
raw observation next is [-3.899999999999999, 71.0, 0.0, 0.0, 26.0, 24.49943159256046, 0.1139856128266562, 0.0, 1.0, 41302.88256601729], 
processed observation next is [0.0, 1.0, 0.35457063711911363, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5416192993800383, 0.537995204275552, 0.0, 1.0, 0.1966803931715109], 
reward next is 0.8033, 
noisyNet noise sample is [array([-0.11614332], dtype=float32), -2.5940077]. 
=============================================
[2019-04-04 08:05:43,659] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.007774]
 [79.081116]
 [79.1522  ]
 [79.238625]
 [79.32769 ]], R is [[78.95168304]
 [78.96513367]
 [78.97811127]
 [78.99064636]
 [79.00276947]].
[2019-04-04 08:05:43,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3803878e-12 2.6525089e-11 5.9025788e-25 4.5662558e-12 5.4852450e-13
 2.3579574e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:05:43,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2533
[2019-04-04 08:05:43,841] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.05, 46.5, 83.0, 758.0, 26.0, 25.40248485823452, 0.3540170195324577, 1.0, 1.0, 142753.8981698468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 304200.0000, 
sim time next is 304800.0000, 
raw observation next is [-9.866666666666667, 45.66666666666667, 85.0, 736.8333333333334, 26.0, 24.60864735638268, 0.3296979623693908, 1.0, 1.0, 200611.3222299295], 
processed observation next is [1.0, 0.5217391304347826, 0.18928901200369344, 0.4566666666666667, 0.2833333333333333, 0.8141804788213628, 0.6666666666666666, 0.5507206130318899, 0.6098993207897969, 1.0, 1.0, 0.9552920106187119], 
reward next is 0.0447, 
noisyNet noise sample is [array([-0.65480137], dtype=float32), -0.048929214]. 
=============================================
[2019-04-04 08:06:02,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8848430e-11 7.0267368e-11 1.3386830e-24 1.0269961e-11 3.0863730e-12
 5.5827886e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:02,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9503
[2019-04-04 08:06:02,367] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.45, 55.5, 0.0, 0.0, 26.0, 25.14303542058654, 0.3185369845387978, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 761400.0000, 
sim time next is 762000.0000, 
raw observation next is [-4.633333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 25.06787072106033, 0.2990073418884059, 1.0, 1.0, 45505.01498298821], 
processed observation next is [1.0, 0.8260869565217391, 0.3342566943674977, 0.5633333333333332, 0.0, 0.0, 0.6666666666666666, 0.5889892267550275, 0.599669113962802, 1.0, 1.0, 0.21669054753803907], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.02752843], dtype=float32), -0.14851777]. 
=============================================
[2019-04-04 08:06:02,378] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[87.778496]
 [88.02767 ]
 [88.3097  ]
 [87.91589 ]
 [87.498184]], R is [[87.58709717]
 [87.71122742]
 [87.83411407]
 [87.48099518]
 [87.07331848]].
[2019-04-04 08:06:03,524] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4464974e-10 2.7860703e-10 1.6408707e-24 2.2766226e-11 1.0512845e-11
 2.1178089e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:03,524] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2818
[2019-04-04 08:06:03,555] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.18729244031784, 0.03180908812882842, 0.0, 1.0, 41683.46460290565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 711600.0000, 
sim time next is 712200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.24286685675699, 0.03671712284704738, 0.0, 1.0, 41705.4822498903], 
processed observation next is [1.0, 0.21739130434782608, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5202389047297492, 0.5122390409490157, 0.0, 1.0, 0.19859753452328716], 
reward next is 0.8014, 
noisyNet noise sample is [array([0.1137654], dtype=float32), 2.2097156]. 
=============================================
[2019-04-04 08:06:20,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.09824816e-10 2.11025281e-10 3.69352998e-24 1.86827463e-11
 7.96846963e-12 3.55809268e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:06:20,002] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7923
[2019-04-04 08:06:20,023] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.383333333333333, 79.83333333333334, 0.0, 0.0, 26.0, 24.64769187562052, 0.2217595932227064, 0.0, 1.0, 40362.19214438719], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 863400.0000, 
sim time next is 864000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.67344829966706, 0.2192631006344836, 0.0, 1.0, 40190.89419845599], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5561206916389217, 0.5730877002114946, 0.0, 1.0, 0.19138521046883805], 
reward next is 0.8086, 
noisyNet noise sample is [array([1.6446224], dtype=float32), -0.12264875]. 
=============================================
[2019-04-04 08:06:20,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.5193 ]
 [83.4913 ]
 [83.4762 ]
 [83.44908]
 [83.44733]], R is [[83.6684494 ]
 [83.63956451]
 [83.61023712]
 [83.58052063]
 [83.55046082]].
[2019-04-04 08:06:26,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.9712203e-10 1.2369488e-09 6.9831779e-23 1.4246584e-10 2.4582048e-11
 1.6086729e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:26,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2507
[2019-04-04 08:06:26,486] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.55, 71.0, 0.0, 0.0, 26.0, 24.20673328394815, 0.1122078271517264, 0.0, 1.0, 41562.75883908949], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 783000.0000, 
sim time next is 783600.0000, 
raw observation next is [-7.633333333333333, 71.0, 0.0, 0.0, 26.0, 24.19540021573315, 0.1016159226079102, 0.0, 1.0, 41543.54125666724], 
processed observation next is [1.0, 0.043478260869565216, 0.2511542012927055, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5162833513110959, 0.5338719742026368, 0.0, 1.0, 0.19782638693651067], 
reward next is 0.8022, 
noisyNet noise sample is [array([-0.19756718], dtype=float32), -1.2900965]. 
=============================================
[2019-04-04 08:06:32,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9698777e-11 4.6302288e-11 5.0356612e-26 8.2952430e-12 1.8575926e-12
 5.8873369e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:32,169] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3155
[2019-04-04 08:06:32,186] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 100.0, 0.0, 0.0, 26.0, 25.50432957085948, 0.5964899583831936, 0.0, 1.0, 18752.32145978201], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1290600.0000, 
sim time next is 1291200.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.50209512323323, 0.5909746035305775, 0.0, 1.0, 18749.96661130094], 
processed observation next is [0.0, 0.9565217391304348, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6251745936027691, 0.6969915345101926, 0.0, 1.0, 0.08928555529190924], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.18582287], dtype=float32), 1.9855244]. 
=============================================
[2019-04-04 08:06:34,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1939756e-11 2.4394611e-11 8.1253082e-27 4.1936858e-12 1.5787000e-12
 3.5185751e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:34,061] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3354
[2019-04-04 08:06:34,081] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.51666666666667, 77.0, 0.0, 0.0, 26.0, 25.63861434214405, 0.6169288765287078, 0.0, 1.0, 27973.96584652528], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1140600.0000, 
sim time next is 1141200.0000, 
raw observation next is [11.6, 77.0, 0.0, 0.0, 26.0, 25.64744354767834, 0.6187837538314601, 0.0, 1.0, 21317.82871265694], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6372869623065283, 0.7062612512771533, 0.0, 1.0, 0.10151347006027114], 
reward next is 0.8985, 
noisyNet noise sample is [array([-0.68087596], dtype=float32), 0.7156848]. 
=============================================
[2019-04-04 08:06:37,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1912645e-12 3.4482653e-11 2.3144189e-27 6.6993655e-13 7.9269260e-14
 6.3889864e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:37,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9355
[2019-04-04 08:06:37,816] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.9000000000000001, 92.0, 106.1666666666667, 0.0, 26.0, 26.09989361132651, 0.5899688169544077, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1334400.0000, 
sim time next is 1335000.0000, 
raw observation next is [1.0, 92.0, 110.3333333333333, 0.0, 26.0, 26.10695419515282, 0.591398615179604, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.92, 0.36777777777777765, 0.0, 0.6666666666666666, 0.675579516262735, 0.6971328717265347, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0612442], dtype=float32), -0.9378025]. 
=============================================
[2019-04-04 08:06:37,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[93.8915  ]
 [93.88007 ]
 [93.92213 ]
 [93.94311 ]
 [93.957924]], R is [[93.92097473]
 [93.98176575]
 [94.04194641]
 [94.10152435]
 [94.1605072 ]].
[2019-04-04 08:06:44,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2284015e-12 2.0165642e-11 4.9982082e-26 8.0565424e-12 5.8082890e-13
 9.0221198e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:44,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3001
[2019-04-04 08:06:44,149] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.91666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 26.47470492721046, 0.778358774570118, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1102200.0000, 
sim time next is 1102800.0000, 
raw observation next is [15.73333333333334, 54.33333333333334, 0.0, 0.0, 26.0, 26.4948289712949, 0.7746503675350297, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8984302862419208, 0.5433333333333334, 0.0, 0.0, 0.6666666666666666, 0.707902414274575, 0.7582167891783432, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.528747], dtype=float32), -2.2271476]. 
=============================================
[2019-04-04 08:06:51,878] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3314412e-12 1.6410975e-11 2.7951761e-26 1.8628189e-12 1.3372689e-13
 3.8861001e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:51,878] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6317
[2019-04-04 08:06:51,903] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 100.0, 47.0, 0.0, 26.0, 25.95014959357743, 0.5192077025943368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1503000.0000, 
sim time next is 1503600.0000, 
raw observation next is [2.0, 100.0, 51.33333333333334, 0.0, 26.0, 26.0353517813452, 0.5281498871276882, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.518005540166205, 1.0, 0.17111111111111113, 0.0, 0.6666666666666666, 0.6696126484454332, 0.6760499623758961, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02369626], dtype=float32), 1.3797866]. 
=============================================
[2019-04-04 08:06:52,784] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.41253561e-11 1.43120044e-10 1.03295505e-25 7.69238839e-12
 4.34006520e-12 5.85412803e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:06:52,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7177
[2019-04-04 08:06:52,804] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.766666666666667, 97.33333333333334, 0.0, 0.0, 26.0, 25.44249041392994, 0.5830632159532242, 0.0, 1.0, 45420.59364063265], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1294800.0000, 
sim time next is 1295400.0000, 
raw observation next is [4.583333333333334, 96.66666666666666, 0.0, 0.0, 26.0, 25.42635101509288, 0.5823489811567071, 0.0, 1.0, 48808.11025012079], 
processed observation next is [0.0, 1.0, 0.5895660203139428, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.6188625845910734, 0.6941163270522357, 0.0, 1.0, 0.23241957261962282], 
reward next is 0.7676, 
noisyNet noise sample is [array([-0.16999234], dtype=float32), 2.0105171]. 
=============================================
[2019-04-04 08:06:53,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0674868e-11 2.7830894e-11 2.5226123e-26 1.7182774e-12 2.1139034e-13
 2.2125625e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:53,211] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4351
[2019-04-04 08:06:53,242] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.566666666666667, 92.0, 33.83333333333333, 0.0, 26.0, 25.72070642803859, 0.5391370572208104, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1672800.0000, 
sim time next is 1673400.0000, 
raw observation next is [2.383333333333333, 92.0, 37.66666666666667, 0.0, 26.0, 25.76732370719096, 0.5333769447236373, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5286241920590952, 0.92, 0.12555555555555556, 0.0, 0.6666666666666666, 0.6472769755992468, 0.6777923149078791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3134031], dtype=float32), -0.1919087]. 
=============================================
[2019-04-04 08:06:53,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7048029e-10 3.0275288e-10 5.0481872e-24 2.9588505e-11 3.1363533e-11
 7.8179603e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:06:53,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6334
[2019-04-04 08:06:53,759] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.48759701868731, 0.5661307196371765, 0.0, 1.0, 22133.58303279427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1557000.0000, 
sim time next is 1557600.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.55589103910478, 0.5682060296994981, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6296575865920649, 0.6894020098998327, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3807089], dtype=float32), -1.2042862]. 
=============================================
[2019-04-04 08:07:03,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3225309e-10 1.1503161e-09 1.2464202e-23 1.5199146e-10 7.2486420e-11
 1.4987761e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:03,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7711
[2019-04-04 08:07:03,122] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 91.66666666666667, 0.0, 26.0, 24.9483436036005, 0.3450353020913646, 0.0, 1.0, 18741.8142451959], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1765200.0000, 
sim time next is 1765800.0000, 
raw observation next is [-2.3, 87.0, 97.0, 0.0, 26.0, 24.96955792847374, 0.3400224375561655, 0.0, 1.0, 18738.88830610051], 
processed observation next is [0.0, 0.43478260869565216, 0.3988919667590028, 0.87, 0.3233333333333333, 0.0, 0.6666666666666666, 0.5807964940394784, 0.6133408125187219, 0.0, 1.0, 0.08923280145762148], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.47998026], dtype=float32), -0.6525828]. 
=============================================
[2019-04-04 08:07:08,567] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1860155e-11 1.1445116e-10 1.6288344e-24 1.6148784e-11 1.1431234e-11
 3.7208707e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:08,593] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9162
[2019-04-04 08:07:08,604] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 82.0, 0.0, 0.0, 26.0, 25.58434253957916, 0.5739347260831248, 0.0, 1.0, 18739.46116390277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1552800.0000, 
sim time next is 1553400.0000, 
raw observation next is [5.25, 82.0, 0.0, 0.0, 26.0, 25.57274552742539, 0.5644402270737611, 0.0, 1.0, 18738.2382081111], 
processed observation next is [1.0, 1.0, 0.60803324099723, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6310621272854492, 0.6881467423579203, 0.0, 1.0, 0.08922970575290999], 
reward next is 0.9108, 
noisyNet noise sample is [array([2.3290145], dtype=float32), 1.0587736]. 
=============================================
[2019-04-04 08:07:09,688] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0482801e-09 9.4759500e-10 1.7760412e-22 8.5741213e-11 7.5367337e-11
 7.5972133e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:09,689] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4696
[2019-04-04 08:07:09,734] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1631699e-09 1.0646337e-09 8.0986238e-22 2.0531482e-10 1.4698427e-10
 1.5585698e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:09,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6719
[2019-04-04 08:07:09,746] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.00880322946809, 0.3151774601432937, 0.0, 1.0, 48147.30965864663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1796400.0000, 
sim time next is 1797000.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01150164596977, 0.3143140389017838, 0.0, 1.0, 45585.51167848399], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5842918038308141, 0.6047713463005946, 0.0, 1.0, 0.21707386513563806], 
reward next is 0.7829, 
noisyNet noise sample is [array([-2.1710806], dtype=float32), -0.115008496]. 
=============================================
[2019-04-04 08:07:09,751] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.35208 ]
 [74.303764]
 [74.23551 ]
 [74.14131 ]
 [74.02846 ]], R is [[74.43772125]
 [74.46407318]
 [74.48841858]
 [74.50989532]
 [74.52976227]].
[2019-04-04 08:07:09,804] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 82.16666666666667, 0.0, 0.0, 26.0, 25.00136244819462, 0.3178396201755533, 0.0, 1.0, 47112.49945803238], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1793400.0000, 
sim time next is 1794000.0000, 
raw observation next is [-4.1, 82.33333333333334, 0.0, 0.0, 26.0, 25.00122570024737, 0.3169906606674067, 0.0, 1.0, 48805.01440963073], 
processed observation next is [0.0, 0.782608695652174, 0.3490304709141275, 0.8233333333333335, 0.0, 0.0, 0.6666666666666666, 0.5834354750206142, 0.6056635535558023, 0.0, 1.0, 0.23240483052205108], 
reward next is 0.7676, 
noisyNet noise sample is [array([-1.7640549], dtype=float32), -0.15522459]. 
=============================================
[2019-04-04 08:07:09,807] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.09744]
 [73.93045]
 [73.7368 ]
 [73.54213]
 [73.3911 ]], R is [[74.27567291]
 [74.30857086]
 [74.35231781]
 [74.39187622]
 [74.42610168]].
[2019-04-04 08:07:12,155] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8969975e-09 9.1151104e-09 1.8568650e-21 2.2477635e-09 1.6038539e-09
 2.6599601e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:12,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8247
[2019-04-04 08:07:12,191] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.283333333333333, 78.83333333333334, 0.0, 0.0, 26.0, 23.5627916583503, -0.03732287942367261, 0.0, 1.0, 47067.67103482843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1836600.0000, 
sim time next is 1837200.0000, 
raw observation next is [-6.366666666666667, 78.66666666666667, 0.0, 0.0, 26.0, 23.52472462646916, -0.04494160968814705, 0.0, 1.0, 47063.60924292093], 
processed observation next is [0.0, 0.2608695652173913, 0.28624192059095105, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.46039371887242986, 0.48501946343728436, 0.0, 1.0, 0.22411242496629014], 
reward next is 0.7759, 
noisyNet noise sample is [array([3.5243864], dtype=float32), 0.44046256]. 
=============================================
[2019-04-04 08:07:20,055] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.9079031e-12 8.5639508e-12 3.9842046e-26 9.0241687e-13 3.2828132e-13
 3.3167126e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:20,055] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6290
[2019-04-04 08:07:20,135] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.633333333333333, 84.83333333333334, 67.66666666666667, 373.0000000000001, 26.0, 25.52066288189265, 0.2643639713221327, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1933800.0000, 
sim time next is 1934400.0000, 
raw observation next is [-8.366666666666667, 83.66666666666667, 76.33333333333334, 461.0, 26.0, 25.56360285088055, 0.2870699258834242, 1.0, 1.0, 18744.42614329578], 
processed observation next is [1.0, 0.391304347826087, 0.23084025854108958, 0.8366666666666667, 0.2544444444444445, 0.5093922651933702, 0.6666666666666666, 0.6303002375733792, 0.5956899752944748, 1.0, 1.0, 0.08925917211093227], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.82136065], dtype=float32), -1.0793958]. 
=============================================
[2019-04-04 08:07:35,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3008288e-09 1.0142195e-09 1.3640932e-22 5.9231016e-11 9.2571596e-11
 1.4740517e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:35,996] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7307
[2019-04-04 08:07:36,015] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.13590744970157, 0.07610601019141214, 0.0, 1.0, 43500.84823640242], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094600.0000, 
sim time next is 2095200.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.06241220271301, 0.07261120130865788, 0.0, 1.0, 43563.55716969544], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5052010168927508, 0.5242037337695526, 0.0, 1.0, 0.20744551033188305], 
reward next is 0.7926, 
noisyNet noise sample is [array([-1.0365407], dtype=float32), -0.87598056]. 
=============================================
[2019-04-04 08:07:36,981] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7732408e-10 9.1241503e-10 1.4089197e-22 1.0264033e-10 6.8133145e-11
 6.9826805e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:36,981] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3968
[2019-04-04 08:07:37,000] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574408238, 0.0587030327048619, 0.0, 1.0, 41111.63103543982], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008200.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13805997613019, 0.05771807760116118, 0.0, 1.0, 41132.93142197821], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5115049980108491, 0.5192393592003871, 0.0, 1.0, 0.19587110200942004], 
reward next is 0.8041, 
noisyNet noise sample is [array([-0.54152185], dtype=float32), 2.3147988]. 
=============================================
[2019-04-04 08:07:41,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7833426e-11 4.5748363e-11 4.7102564e-25 1.7575103e-11 3.2215476e-13
 9.3000702e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:41,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3495
[2019-04-04 08:07:41,584] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.49605243267775, 0.348666740919426, 1.0, 1.0, 18715.60489475307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1964400.0000, 
sim time next is 1965000.0000, 
raw observation next is [-4.816666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 25.33151688927054, 0.3483495085572961, 1.0, 1.0, 24675.21412799677], 
processed observation next is [1.0, 0.7391304347826086, 0.32917820867959374, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.6109597407725449, 0.616116502852432, 1.0, 1.0, 0.11750101965712748], 
reward next is 0.8825, 
noisyNet noise sample is [array([0.5125853], dtype=float32), 1.5204749]. 
=============================================
[2019-04-04 08:07:41,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.149895]
 [86.25451 ]
 [86.58173 ]
 [86.70661 ]
 [86.836845]], R is [[86.13664246]
 [86.18615723]
 [86.32429504]
 [86.46105194]
 [86.59644318]].
[2019-04-04 08:07:47,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3948908e-10 3.1556362e-09 2.4773385e-22 2.3243174e-10 1.7356314e-10
 1.1183583e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:47,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6317
[2019-04-04 08:07:47,869] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333334, 77.0, 0.0, 0.0, 26.0, 24.14886341213963, 0.08516802762487825, 0.0, 1.0, 42074.23045600582], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2175600.0000, 
sim time next is 2176200.0000, 
raw observation next is [-6.45, 76.5, 0.0, 0.0, 26.0, 24.12438756386508, 0.07403210653297965, 0.0, 1.0, 42051.50385924979], 
processed observation next is [1.0, 0.17391304347826086, 0.28393351800554023, 0.765, 0.0, 0.0, 0.6666666666666666, 0.5103656303220901, 0.5246773688443266, 0.0, 1.0, 0.20024525647261804], 
reward next is 0.7998, 
noisyNet noise sample is [array([-0.4640235], dtype=float32), 0.3752203]. 
=============================================
[2019-04-04 08:07:48,271] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5060104e-11 1.0094155e-10 3.8726618e-24 1.3775794e-11 3.0215032e-12
 2.0018453e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:07:48,272] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4694
[2019-04-04 08:07:48,319] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.3, 70.0, 29.66666666666667, 0.0, 26.0, 25.59671174051197, 0.3716831090272638, 1.0, 1.0, 88844.7537249696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2220000.0000, 
sim time next is 2220600.0000, 
raw observation next is [-4.399999999999999, 70.5, 24.33333333333334, 0.0, 26.0, 25.5529608886534, 0.3928279556158598, 1.0, 1.0, 60932.82778710546], 
processed observation next is [1.0, 0.6956521739130435, 0.3407202216066483, 0.705, 0.08111111111111113, 0.0, 0.6666666666666666, 0.6294134073877832, 0.6309426518719533, 1.0, 1.0, 0.2901563227957403], 
reward next is 0.7098, 
noisyNet noise sample is [array([-1.0062106], dtype=float32), -1.3506445]. 
=============================================
[2019-04-04 08:08:07,954] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1635825e-10 2.7855616e-09 2.1993429e-22 1.7266569e-10 8.4388739e-11
 1.5157318e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:07,956] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6084
[2019-04-04 08:08:07,971] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 72.33333333333334, 0.0, 0.0, 26.0, 25.16496973485071, 0.3266315870859962, 0.0, 1.0, 44971.0784355768], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2240400.0000, 
sim time next is 2241000.0000, 
raw observation next is [-5.9, 73.0, 0.0, 0.0, 26.0, 25.03924028569166, 0.3090821555550189, 0.0, 1.0, 44492.89274786998], 
processed observation next is [1.0, 0.9565217391304348, 0.2991689750692521, 0.73, 0.0, 0.0, 0.6666666666666666, 0.5866033571409716, 0.6030273851850063, 0.0, 1.0, 0.21187091784699988], 
reward next is 0.7881, 
noisyNet noise sample is [array([-0.7268194], dtype=float32), -0.17271696]. 
=============================================
[2019-04-04 08:08:07,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.19378]
 [76.2977 ]
 [76.33068]
 [76.38096]
 [76.44884]], R is [[76.18051147]
 [76.20455933]
 [76.22893524]
 [76.25449371]
 [76.27922058]].
[2019-04-04 08:08:10,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6711672e-09 1.5806487e-09 9.3076322e-23 2.9759789e-10 1.1211594e-10
 4.9082651e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:10,421] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5838
[2019-04-04 08:08:10,434] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 34.33333333333334, 0.0, 0.0, 26.0, 25.32132116890053, 0.2763853623533309, 0.0, 1.0, 40086.4326701298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2497200.0000, 
sim time next is 2497800.0000, 
raw observation next is [-1.2, 33.66666666666666, 0.0, 0.0, 26.0, 25.30794126128997, 0.2697897823867564, 0.0, 1.0, 40138.9004130668], 
processed observation next is [0.0, 0.9130434782608695, 0.42936288088642666, 0.33666666666666656, 0.0, 0.0, 0.6666666666666666, 0.6089951051074974, 0.5899299274622521, 0.0, 1.0, 0.19113762101460383], 
reward next is 0.8089, 
noisyNet noise sample is [array([-1.1972789], dtype=float32), 0.838264]. 
=============================================
[2019-04-04 08:08:10,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9196697e-10 6.2388534e-09 2.1154022e-22 3.5747727e-10 8.4562433e-11
 1.3984152e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:10,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9403
[2019-04-04 08:08:10,796] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.41433630299266, 0.4157892446457611, 0.0, 1.0, 42287.6070419565], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2583000.0000, 
sim time next is 2583600.0000, 
raw observation next is [-2.8, 56.0, 0.0, 0.0, 26.0, 25.41089788570654, 0.4135741988907586, 0.0, 1.0, 43958.3740873539], 
processed observation next is [1.0, 0.9130434782608695, 0.38504155124653744, 0.56, 0.0, 0.0, 0.6666666666666666, 0.6175748238088783, 0.6378580662969195, 0.0, 1.0, 0.20932559089216143], 
reward next is 0.7907, 
noisyNet noise sample is [array([1.5677725], dtype=float32), 0.3963634]. 
=============================================
[2019-04-04 08:08:19,525] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5476621e-09 1.6861933e-09 1.8203032e-22 3.3760292e-10 7.1512636e-11
 7.7508388e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:19,527] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9580
[2019-04-04 08:08:19,547] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.95, 33.0, 0.0, 0.0, 26.0, 25.31341518960269, 0.2849892866562533, 0.0, 1.0, 40192.38084724758], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2493000.0000, 
sim time next is 2493600.0000, 
raw observation next is [-1.033333333333333, 34.33333333333333, 0.0, 0.0, 26.0, 25.31390745215406, 0.2828378454466705, 0.0, 1.0, 40064.83428380277], 
processed observation next is [0.0, 0.8695652173913043, 0.43397968605724846, 0.34333333333333327, 0.0, 0.0, 0.6666666666666666, 0.6094922876795049, 0.5942792818155568, 0.0, 1.0, 0.19078492516096557], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.4205496], dtype=float32), 0.37815267]. 
=============================================
[2019-04-04 08:08:19,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7390026e-12 6.6476435e-11 3.2610319e-25 4.8573801e-12 7.9999890e-13
 1.9952041e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:19,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4294
[2019-04-04 08:08:19,918] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.416666666666667, 73.33333333333333, 87.66666666666667, 60.66666666666667, 26.0, 25.83658951517027, 0.3531739161202775, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2625000.0000, 
sim time next is 2625600.0000, 
raw observation next is [-6.133333333333335, 71.66666666666667, 90.33333333333333, 75.83333333333334, 26.0, 25.82779172405074, 0.3593648272580678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.29270544783010155, 0.7166666666666667, 0.3011111111111111, 0.0837937384898711, 0.6666666666666666, 0.6523159770042284, 0.6197882757526892, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9023892], dtype=float32), -0.60971034]. 
=============================================
[2019-04-04 08:08:29,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3186188e-10 2.0189330e-09 7.1342315e-23 6.0424936e-11 2.3657918e-11
 7.2472756e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:29,856] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1677
[2019-04-04 08:08:29,872] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 56.00000000000001, 0.0, 0.0, 26.0, 24.72175207042626, 0.1221395393964346, 0.0, 1.0, 38693.8301045395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2528400.0000, 
sim time next is 2529000.0000, 
raw observation next is [-2.55, 55.5, 0.0, 0.0, 26.0, 24.69978072443601, 0.1124263894478241, 0.0, 1.0, 38754.05764574005], 
processed observation next is [1.0, 0.2608695652173913, 0.3919667590027701, 0.555, 0.0, 0.0, 0.6666666666666666, 0.5583150603696675, 0.5374754631492747, 0.0, 1.0, 0.1845431316463812], 
reward next is 0.8155, 
noisyNet noise sample is [array([-1.4419171], dtype=float32), 1.8759172]. 
=============================================
[2019-04-04 08:08:29,877] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.93171 ]
 [78.00362 ]
 [78.064545]
 [78.107086]
 [78.13317 ]], R is [[77.88814545]
 [77.92501068]
 [77.96182251]
 [77.99866486]
 [78.03552246]].
[2019-04-04 08:08:30,333] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3302241e-10 3.4542196e-09 7.7421264e-23 3.8171749e-10 6.5984905e-11
 5.6073391e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:30,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8261
[2019-04-04 08:08:30,379] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 243.0, 240.6666666666667, 26.0, 24.98309457362, 0.3450438624177863, 0.0, 1.0, 33500.55658236261], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2978400.0000, 
sim time next is 2979000.0000, 
raw observation next is [-3.0, 65.0, 256.0, 284.0, 26.0, 24.96448985456088, 0.3549425951612621, 0.0, 1.0, 49123.43056640511], 
processed observation next is [0.0, 0.4782608695652174, 0.3795013850415513, 0.65, 0.8533333333333334, 0.3138121546961326, 0.6666666666666666, 0.5803741545467401, 0.6183141983870873, 0.0, 1.0, 0.23392109793526245], 
reward next is 0.7661, 
noisyNet noise sample is [array([0.4833014], dtype=float32), -1.045016]. 
=============================================
[2019-04-04 08:08:30,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.40843]
 [80.90443]
 [80.24641]
 [79.45785]
 [78.7143 ]], R is [[81.76535797]
 [81.78818512]
 [81.88106537]
 [81.97299957]
 [82.06399536]].
[2019-04-04 08:08:30,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.8646915e-12 4.5516497e-11 4.8160585e-24 1.0629019e-11 3.3420697e-12
 7.7950265e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:30,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9193
[2019-04-04 08:08:31,047] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.5, 70.0, 117.0, 674.0, 26.0, 26.07611993835756, 0.4683942589864222, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2716200.0000, 
sim time next is 2716800.0000, 
raw observation next is [-10.0, 68.0, 116.1666666666667, 691.8333333333334, 26.0, 26.08157597999655, 0.4731931516495056, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.18559556786703602, 0.68, 0.38722222222222236, 0.7644567219152855, 0.6666666666666666, 0.6734646649997126, 0.6577310505498352, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5227666], dtype=float32), -0.35930857]. 
=============================================
[2019-04-04 08:08:31,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8398260e-12 1.4419350e-11 6.9108474e-26 6.2129414e-13 7.3238846e-14
 2.9548122e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:31,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8198
[2019-04-04 08:08:31,088] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.110223024625157e-16, 48.0, 133.1666666666667, 720.5, 26.0, 26.06103626506909, 0.4604721514724188, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2805600.0000, 
sim time next is 2806200.0000, 
raw observation next is [0.5, 47.0, 125.0, 763.0, 26.0, 26.02819598106496, 0.4536780886750782, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4764542936288089, 0.47, 0.4166666666666667, 0.8430939226519337, 0.6666666666666666, 0.6690163317554134, 0.6512260295583594, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39110655], dtype=float32), 0.28286043]. 
=============================================
[2019-04-04 08:08:34,156] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.9150141e-12 7.7292048e-11 1.9194410e-24 1.2577429e-11 1.2184086e-12
 6.7533971e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:34,158] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3873
[2019-04-04 08:08:34,218] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.33333333333333, 86.0, 94.16666666666667, 565.0, 26.0, 25.9535641151597, 0.4212913386040193, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2712000.0000, 
sim time next is 2712600.0000, 
raw observation next is [-13.0, 83.5, 97.0, 612.0, 26.0, 25.9420198626813, 0.4320479121745586, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.10249307479224376, 0.835, 0.3233333333333333, 0.6762430939226519, 0.6666666666666666, 0.6618349885567749, 0.6440159707248528, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01296537], dtype=float32), -0.6139132]. 
=============================================
[2019-04-04 08:08:34,924] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0272949e-09 2.4009974e-09 1.1629898e-21 2.8465749e-10 8.0140936e-11
 8.0632414e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:34,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0068
[2019-04-04 08:08:34,942] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.366666666666667, 81.33333333333334, 0.0, 0.0, 26.0, 24.13916472200472, 0.07090284669676022, 0.0, 1.0, 43267.94138871566], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2611200.0000, 
sim time next is 2611800.0000, 
raw observation next is [-6.45, 80.5, 0.0, 0.0, 26.0, 24.06125077468531, 0.05793485594257933, 0.0, 1.0, 43426.67511904789], 
processed observation next is [1.0, 0.21739130434782608, 0.28393351800554023, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5051042312237758, 0.5193116186475265, 0.0, 1.0, 0.2067936910430852], 
reward next is 0.7932, 
noisyNet noise sample is [array([1.2147259], dtype=float32), 0.32887602]. 
=============================================
[2019-04-04 08:08:42,645] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4713420e-11 5.2027990e-11 3.4710744e-24 8.8884360e-12 9.6632923e-13
 2.5908126e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:42,645] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9244
[2019-04-04 08:08:42,667] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666666, 98.83333333333334, 58.66666666666666, 0.0, 26.0, 25.46394615576764, 0.3076971888866248, 1.0, 1.0, 18682.08336876821], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2886600.0000, 
sim time next is 2887200.0000, 
raw observation next is [0.0, 100.0, 63.5, 0.0, 26.0, 25.43695900402948, 0.3047936961678733, 1.0, 1.0, 18680.85237545778], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 1.0, 0.21166666666666667, 0.0, 0.6666666666666666, 0.6197465836691233, 0.6015978987226244, 1.0, 1.0, 0.08895643988313229], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.09221166], dtype=float32), -0.8319009]. 
=============================================
[2019-04-04 08:08:43,083] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1295286e-09 4.3200195e-09 2.8955290e-22 4.1420581e-10 1.6154676e-10
 2.9682420e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:43,084] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2341
[2019-04-04 08:08:43,197] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 70.0, 42.33333333333334, 26.0, 23.54062491934664, 0.1261045856029406, 0.0, 1.0, 202096.013069987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2967600.0000, 
sim time next is 2968200.0000, 
raw observation next is [-4.0, 74.0, 84.0, 49.0, 26.0, 23.92224321119537, 0.2280813120363678, 0.0, 1.0, 203175.0198782216], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.74, 0.28, 0.05414364640883978, 0.6666666666666666, 0.49352026759961404, 0.5760271040121226, 0.0, 1.0, 0.9675000946581981], 
reward next is 0.0325, 
noisyNet noise sample is [array([-1.9078965], dtype=float32), 0.13204415]. 
=============================================
[2019-04-04 08:08:46,077] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.4180604e-10 3.3195977e-09 6.0534768e-22 1.2543774e-10 3.5658929e-10
 6.9095223e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:46,077] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5390
[2019-04-04 08:08:46,092] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 26.0, 25.13414082462772, 0.3920150271776124, 0.0, 1.0, 43771.63623237996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2933400.0000, 
sim time next is 2934000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.08401743300012, 0.383035797943971, 0.0, 1.0, 43663.304490666], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5903347860833433, 0.627678599314657, 0.0, 1.0, 0.2079204975746], 
reward next is 0.7921, 
noisyNet noise sample is [array([0.13530378], dtype=float32), 1.2278864]. 
=============================================
[2019-04-04 08:08:46,106] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[75.881454]
 [75.859474]
 [75.97169 ]
 [75.958305]
 [75.9315  ]], R is [[75.91108704]
 [75.94354248]
 [75.97464752]
 [76.00170135]
 [76.02093506]].
[2019-04-04 08:08:56,226] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2839456e-10 8.8890045e-10 1.2743445e-22 9.9647256e-11 5.0609756e-11
 2.1934043e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:08:56,228] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4611
[2019-04-04 08:08:56,248] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.916666666666667, 65.0, 0.0, 0.0, 26.0, 25.15992859925283, 0.326180367153369, 0.0, 1.0, 39034.22012658595], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3016200.0000, 
sim time next is 3016800.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.14613755258074, 0.3207069145249564, 0.0, 1.0, 38897.39630813452], 
processed observation next is [0.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5955114627150616, 0.6069023048416521, 0.0, 1.0, 0.18522569670540248], 
reward next is 0.8148, 
noisyNet noise sample is [array([0.04382283], dtype=float32), -0.9022221]. 
=============================================
[2019-04-04 08:09:00,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0289206e-09 5.4536047e-09 3.1097825e-21 2.9215314e-10 1.5775457e-10
 8.3633014e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:09:00,721] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4844
[2019-04-04 08:09:00,756] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 55.83333333333334, 22.66666666666666, 224.0, 26.0, 25.09202974464196, 0.3585795498795973, 0.0, 1.0, 27198.70935165413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2999400.0000, 
sim time next is 3000000.0000, 
raw observation next is [-1.333333333333333, 56.66666666666667, 14.33333333333333, 161.5, 26.0, 25.07558730985142, 0.3484521427317866, 0.0, 1.0, 39517.52968874524], 
processed observation next is [0.0, 0.7391304347826086, 0.42566943674976926, 0.5666666666666668, 0.047777777777777766, 0.17845303867403314, 0.6666666666666666, 0.5896322758209518, 0.6161507142439289, 0.0, 1.0, 0.18817871280354878], 
reward next is 0.8118, 
noisyNet noise sample is [array([-0.61590105], dtype=float32), -1.2357851]. 
=============================================
[2019-04-04 08:09:00,767] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.823265]
 [70.31474 ]
 [70.87698 ]
 [71.33266 ]
 [71.58914 ]], R is [[69.5813446 ]
 [69.75601196]
 [69.96309662]
 [70.08314514]
 [70.16610718]].
[2019-04-04 08:09:02,033] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1030032e-11 2.3966906e-10 1.2663728e-22 3.7447632e-11 6.3100263e-12
 6.0790254e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:09:02,034] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2132
[2019-04-04 08:09:02,112] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 48.0, 87.0, 674.0, 26.0, 26.70901119396065, 0.6052728418123406, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3339000.0000, 
sim time next is 3339600.0000, 
raw observation next is [-2.333333333333333, 47.33333333333333, 82.5, 645.1666666666667, 26.0, 26.0506796920674, 0.6325985856478772, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3979686057248385, 0.4733333333333333, 0.275, 0.7128913443830571, 0.6666666666666666, 0.6708899743389501, 0.7108661952159591, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2132678], dtype=float32), -1.8400154]. 
=============================================
[2019-04-04 08:09:02,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9354111e-09 8.9679757e-09 1.5756905e-21 4.7243826e-10 8.2393520e-10
 2.6518601e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:09:02,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5365
[2019-04-04 08:09:02,783] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079208640976, 0.2752204650914976, 0.0, 1.0, 38150.93490819046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93795231173934, 0.2698984712044595, 0.0, 1.0, 38075.94099414045], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5781626926449451, 0.5899661570681531, 0.0, 1.0, 0.18131400473400217], 
reward next is 0.8187, 
noisyNet noise sample is [array([-0.02561159], dtype=float32), -1.8885076]. 
=============================================
[2019-04-04 08:09:13,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.4975139e-11 1.0922824e-09 2.6939103e-22 6.5419607e-11 1.0249726e-11
 1.6553653e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:09:13,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8230
[2019-04-04 08:09:13,799] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.89154695464559, 0.5717233682293625, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3349200.0000, 
sim time next is 3349800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.91147384609242, 0.5418926876548795, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6592894871743683, 0.6806308958849598, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.06272297], dtype=float32), -0.702527]. 
=============================================
[2019-04-04 08:09:17,183] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-04 08:09:17,215] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:09:17,215] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:09:17,230] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:09:17,230] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:09:17,232] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:09:17,232] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:09:17,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run25
[2019-04-04 08:09:17,321] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run25
[2019-04-04 08:09:17,358] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run25
[2019-04-04 08:11:50,698] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.19406669], dtype=float32), -0.121250905]
[2019-04-04 08:11:50,698] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.666666666666667, 80.66666666666666, 98.33333333333334, 755.1666666666667, 26.0, 26.95454187629523, 0.8335455569140363, 1.0, 1.0, 0.0]
[2019-04-04 08:11:50,698] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 08:11:50,700] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.1255163e-12 3.1886681e-11 4.4353058e-24 3.5092127e-12 1.2999761e-12
 9.2316088e-15 1.0000000e+00], sampled 0.1703135970315024
[2019-04-04 08:12:22,981] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:12:55,395] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.19406669], dtype=float32), -0.121250905]
[2019-04-04 08:12:55,395] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.0, 50.0, 0.0, 0.0, 26.0, 25.38558196431818, 0.3938177509568068, 0.0, 1.0, 45968.34540372325]
[2019-04-04 08:12:55,395] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 08:12:55,396] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.3529673e-10 1.6522144e-09 4.6163733e-23 1.9659667e-10 6.0990574e-11
 2.6815781e-13 1.0000000e+00], sampled 0.01566826154232659
[2019-04-04 08:12:57,613] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:13:03,873] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:13:04,909] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2400000, evaluation results [2400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:13:11,743] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4799622e-11 2.6363298e-10 2.6970037e-23 9.9586780e-12 9.5519738e-12
 1.1216967e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:11,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2500
[2019-04-04 08:13:11,758] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 26.76494238257215, 0.8204252162971954, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3177000.0000, 
sim time next is 3177600.0000, 
raw observation next is [4.666666666666666, 100.0, 0.0, 0.0, 26.0, 26.59149729799517, 0.8119925146855115, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5918744228993538, 1.0, 0.0, 0.0, 0.6666666666666666, 0.715958108166264, 0.7706641715618372, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6077626], dtype=float32), -0.39489108]. 
=============================================
[2019-04-04 08:13:14,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1564046e-11 3.0426439e-10 1.3317116e-22 2.3777435e-11 1.6140092e-12
 4.7266353e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:14,091] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6761
[2019-04-04 08:13:14,135] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 26.04723469976449, 0.5910601754436543, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3520800.0000, 
sim time next is 3521400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 26.07308788381162, 0.581515339282301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6727573236509684, 0.6938384464274336, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7402577], dtype=float32), 0.014663261]. 
=============================================
[2019-04-04 08:13:18,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1785676e-11 8.7251054e-11 7.9806959e-24 7.2895388e-12 2.7129405e-12
 2.9012183e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:18,291] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7378
[2019-04-04 08:13:18,370] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 60.33333333333334, 113.0, 796.6666666666667, 26.0, 26.1609878533207, 0.6014188847117291, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3496200.0000, 
sim time next is 3496800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 26.0, 26.2374795602468, 0.6118321854371301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4995383194829178, 0.5966666666666667, 0.38, 0.8876611418047882, 0.6666666666666666, 0.6864566300205667, 0.7039440618123768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3050626], dtype=float32), -1.822227]. 
=============================================
[2019-04-04 08:13:21,639] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4042832e-09 1.7572104e-09 1.7443824e-22 1.6355424e-10 1.8501231e-10
 4.1184577e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:21,715] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7361
[2019-04-04 08:13:21,822] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 68.0, 0.0, 0.0, 26.0, 25.1353547924457, 0.3742403547029178, 0.0, 1.0, 41174.88382638222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3555000.0000, 
sim time next is 3555600.0000, 
raw observation next is [-3.666666666666667, 69.0, 0.0, 0.0, 26.0, 25.09443277799304, 0.3664113441988723, 0.0, 1.0, 41198.06064739273], 
processed observation next is [0.0, 0.13043478260869565, 0.3610341643582641, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5912027314994198, 0.6221371147329574, 0.0, 1.0, 0.19618124117806063], 
reward next is 0.8038, 
noisyNet noise sample is [array([-0.7989975], dtype=float32), 0.7484235]. 
=============================================
[2019-04-04 08:13:28,643] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.0389445e-10 1.0401203e-09 8.7618749e-23 1.1224090e-10 8.2493484e-11
 2.8489916e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:28,643] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5828
[2019-04-04 08:13:28,735] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23060516118292, 0.3778978792417891, 0.0, 1.0, 41684.79724863172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3479400.0000, 
sim time next is 3480000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.23975604722202, 0.379206387888924, 0.0, 1.0, 41599.78994990349], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6033130039351683, 0.626402129296308, 0.0, 1.0, 0.1980942378566833], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.80599844], dtype=float32), 0.008365302]. 
=============================================
[2019-04-04 08:13:28,759] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[78.69146 ]
 [78.729454]
 [78.78316 ]
 [78.83855 ]
 [78.876366]], R is [[78.69432831]
 [78.70888519]
 [78.72296143]
 [78.73664856]
 [78.75      ]].
[2019-04-04 08:13:31,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4094264e-10 7.2651862e-10 2.0229766e-24 1.4024293e-10 1.0077221e-11
 3.3689082e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:31,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4670
[2019-04-04 08:13:31,606] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.333333333333334, 26.33333333333334, 0.0, 0.0, 26.0, 25.51847692355643, 0.3583661050034131, 0.0, 1.0, 18747.38505472707], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3648000.0000, 
sim time next is 3648600.0000, 
raw observation next is [9.5, 26.0, 0.0, 0.0, 26.0, 25.50674497954367, 0.3558895845662004, 0.0, 1.0, 29498.77847128849], 
processed observation next is [0.0, 0.21739130434782608, 0.7257617728531857, 0.26, 0.0, 0.0, 0.6666666666666666, 0.625562081628639, 0.6186298615220668, 0.0, 1.0, 0.14047037367280232], 
reward next is 0.8595, 
noisyNet noise sample is [array([0.23439695], dtype=float32), -0.90806675]. 
=============================================
[2019-04-04 08:13:35,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6855727e-12 2.1782227e-11 1.4514812e-24 1.1220531e-11 1.1788877e-12
 4.3375596e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:35,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4818
[2019-04-04 08:13:35,481] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 62.5, 84.0, 704.0, 26.0, 26.59393394823696, 0.6707037780802683, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3425400.0000, 
sim time next is 3426000.0000, 
raw observation next is [2.333333333333333, 64.0, 80.16666666666666, 672.1666666666667, 26.0, 26.65636252127855, 0.6759470882386149, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5272391505078486, 0.64, 0.2672222222222222, 0.7427255985267036, 0.6666666666666666, 0.7213635434398791, 0.7253156960795383, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17069831], dtype=float32), -0.52744883]. 
=============================================
[2019-04-04 08:13:35,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.89993 ]
 [85.91783 ]
 [85.896835]
 [85.91574 ]
 [85.92943 ]], R is [[85.88059998]
 [86.02179718]
 [86.16158295]
 [86.2999649 ]
 [86.43696594]].
[2019-04-04 08:13:45,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3193395e-09 1.5094541e-09 4.1328393e-22 1.3452296e-10 1.0072826e-10
 4.3678166e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:45,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7547
[2019-04-04 08:13:45,333] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.74317303274378, 0.2320758487912781, 0.0, 1.0, 43000.81728540175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3825600.0000, 
sim time next is 3826200.0000, 
raw observation next is [-5.0, 77.0, 0.0, 0.0, 26.0, 24.66974247407493, 0.2347356846393642, 0.0, 1.0, 42926.2490648885], 
processed observation next is [1.0, 0.2608695652173913, 0.32409972299168976, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5558118728395774, 0.5782452282131214, 0.0, 1.0, 0.20441070983280238], 
reward next is 0.7956, 
noisyNet noise sample is [array([-0.16477302], dtype=float32), 0.09397458]. 
=============================================
[2019-04-04 08:13:47,648] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6968497e-09 7.0848141e-09 1.1320001e-21 2.0545037e-10 3.3117731e-10
 7.3814571e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:47,656] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3243
[2019-04-04 08:13:47,672] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333333, 60.66666666666667, 0.0, 0.0, 26.0, 24.92799566016194, 0.2920715495732042, 0.0, 1.0, 42025.06065439757], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3910800.0000, 
sim time next is 3911400.0000, 
raw observation next is [-6.5, 61.5, 0.0, 0.0, 26.0, 24.91647843306871, 0.2834527412304775, 0.0, 1.0, 42172.9947809158], 
processed observation next is [1.0, 0.2608695652173913, 0.28254847645429365, 0.615, 0.0, 0.0, 0.6666666666666666, 0.5763732027557259, 0.5944842470768258, 0.0, 1.0, 0.2008237846710276], 
reward next is 0.7992, 
noisyNet noise sample is [array([-0.1307986], dtype=float32), 0.60481876]. 
=============================================
[2019-04-04 08:13:47,791] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0833502e-09 3.2615313e-09 4.0823399e-21 3.4173758e-10 2.2610405e-10
 9.4023194e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:13:47,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3572
[2019-04-04 08:13:47,810] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.833333333333334, 63.16666666666666, 0.0, 0.0, 26.0, 24.80060352091012, 0.2530881031385069, 0.0, 1.0, 42487.29870437142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3912600.0000, 
sim time next is 3913200.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.72032240647716, 0.2394712276671384, 0.0, 1.0, 42633.23920550437], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5600268672064299, 0.5798237425557128, 0.0, 1.0, 0.20301542478811604], 
reward next is 0.7970, 
noisyNet noise sample is [array([-0.04724828], dtype=float32), -2.2484121]. 
=============================================
[2019-04-04 08:14:00,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2501849e-11 1.5203264e-10 2.6686845e-24 1.3957466e-11 3.2349848e-12
 5.0094531e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:00,594] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9560
[2019-04-04 08:14:00,602] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 40.0, 0.0, 0.0, 26.0, 25.58696242769682, 0.5146442112313704, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4131000.0000, 
sim time next is 4131600.0000, 
raw observation next is [1.666666666666667, 41.0, 0.0, 0.0, 26.0, 25.56240307182996, 0.5053777446587594, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5087719298245615, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6302002559858298, 0.6684592482195865, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86650157], dtype=float32), 1.4442127]. 
=============================================
[2019-04-04 08:14:03,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1567703e-11 3.9364356e-11 5.2481979e-26 5.9930780e-12 2.2892259e-13
 2.0203252e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:03,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2234
[2019-04-04 08:14:03,623] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 36.5, 104.0, 679.0, 26.0, 26.20421056989942, 0.5158958057679439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4095000.0000, 
sim time next is 4095600.0000, 
raw observation next is [-2.333333333333333, 36.0, 105.6666666666667, 694.0, 26.0, 26.30474245859104, 0.5265798540443202, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.3979686057248385, 0.36, 0.3522222222222223, 0.7668508287292818, 0.6666666666666666, 0.6920618715492534, 0.6755266180147733, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.69307005], dtype=float32), -0.29641318]. 
=============================================
[2019-04-04 08:14:06,521] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0438179e-10 1.7471277e-09 1.5612395e-23 6.8461153e-11 4.9829019e-11
 4.5596017e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:06,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1090
[2019-04-04 08:14:06,554] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 41.33333333333334, 0.0, 0.0, 26.0, 25.36482908441861, 0.411588537990923, 0.0, 1.0, 56447.80966451103], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4152000.0000, 
sim time next is 4152600.0000, 
raw observation next is [-1.5, 42.5, 0.0, 0.0, 26.0, 25.3391475820235, 0.411579838436313, 0.0, 1.0, 46773.3717495204], 
processed observation next is [0.0, 0.043478260869565216, 0.4210526315789474, 0.425, 0.0, 0.0, 0.6666666666666666, 0.6115956318352916, 0.637193279478771, 0.0, 1.0, 0.22273034166438285], 
reward next is 0.7773, 
noisyNet noise sample is [array([-0.12297315], dtype=float32), 0.9994094]. 
=============================================
[2019-04-04 08:14:06,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8261706e-10 5.3527532e-10 4.7652027e-23 1.4829228e-10 1.5582473e-11
 4.7344131e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:06,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3144
[2019-04-04 08:14:06,721] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 41.5, 0.0, 0.0, 26.0, 25.38724420683905, 0.4341621081977253, 0.0, 1.0, 60733.81480205471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4147800.0000, 
sim time next is 4148400.0000, 
raw observation next is [-1.0, 41.0, 0.0, 0.0, 26.0, 25.37455134778207, 0.4340385280483436, 0.0, 1.0, 54887.67942544416], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6145459456485058, 0.6446795093494478, 0.0, 1.0, 0.2613699020259246], 
reward next is 0.7386, 
noisyNet noise sample is [array([1.3182703], dtype=float32), -1.360513]. 
=============================================
[2019-04-04 08:14:13,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0169135e-13 7.5877611e-12 1.0947868e-25 5.2521377e-13 5.3283868e-14
 5.9190179e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:13,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8134
[2019-04-04 08:14:13,135] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 26.0, 26.32047466380178, 0.5795275273239565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4020000.0000, 
sim time next is 4020600.0000, 
raw observation next is [-4.333333333333333, 30.33333333333333, 116.6666666666667, 837.3333333333334, 26.0, 26.26514644790995, 0.474369026033973, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.342566943674977, 0.3033333333333333, 0.388888888888889, 0.9252302025782689, 0.6666666666666666, 0.6887622039924958, 0.658123008677991, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26785952], dtype=float32), 0.2548076]. 
=============================================
[2019-04-04 08:14:17,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0468150e-11 4.5226933e-11 1.0967849e-25 6.7845608e-12 1.0522629e-12
 1.9001657e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:17,822] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0454
[2019-04-04 08:14:17,834] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.466666666666667, 63.33333333333333, 0.0, 0.0, 26.0, 25.75275380873055, 0.6362097682929312, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4407000.0000, 
sim time next is 4407600.0000, 
raw observation next is [7.333333333333334, 63.66666666666667, 0.0, 0.0, 26.0, 25.80422250319698, 0.6330053927409977, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6657433056325024, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6503518752664149, 0.7110017975803326, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3544986], dtype=float32), 1.264014]. 
=============================================
[2019-04-04 08:14:23,959] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9655503e-11 4.4050621e-10 1.4645180e-24 2.1535971e-11 1.5869214e-11
 3.6422830e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:23,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6898
[2019-04-04 08:14:23,976] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.05, 62.5, 0.0, 0.0, 26.0, 25.64159423997291, 0.5986162173265589, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4404600.0000, 
sim time next is 4405200.0000, 
raw observation next is [7.9, 62.66666666666667, 0.0, 0.0, 26.0, 25.62174092010765, 0.621365800175056, 0.0, 1.0, 143190.9370672884], 
processed observation next is [1.0, 1.0, 0.6814404432132966, 0.6266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6351450766756376, 0.7071219333916853, 0.0, 1.0, 0.6818616050823257], 
reward next is 0.3181, 
noisyNet noise sample is [array([1.2438873], dtype=float32), -1.0380994]. 
=============================================
[2019-04-04 08:14:31,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4189013e-10 7.5628448e-10 6.7432266e-23 2.9548253e-10 1.2245768e-10
 1.5816710e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:31,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4030
[2019-04-04 08:14:31,323] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.98549736123022, 0.2753512409130817, 0.0, 1.0, 39195.20197238576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4849800.0000, 
sim time next is 4850400.0000, 
raw observation next is [-3.0, 60.00000000000001, 0.0, 0.0, 26.0, 24.95535528703432, 0.2694200731582167, 0.0, 1.0, 39209.77867085164], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5796129405861933, 0.5898066910527389, 0.0, 1.0, 0.18671323176596022], 
reward next is 0.8133, 
noisyNet noise sample is [array([0.49639517], dtype=float32), 0.195115]. 
=============================================
[2019-04-04 08:14:41,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3623822e-11 4.9766929e-10 1.3791503e-24 1.9682093e-11 7.7701853e-13
 3.6586873e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:14:41,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7552
[2019-04-04 08:14:41,128] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.01826159268358, 0.4458151396379726, 1.0, 1.0, 95487.84026258503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4734000.0000, 
sim time next is 4734600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.07691521782707, 0.4731473005908312, 1.0, 1.0, 25704.85573417582], 
processed observation next is [1.0, 0.8260869565217391, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5897429348189226, 0.6577157668636104, 1.0, 1.0, 0.12240407492464676], 
reward next is 0.8776, 
noisyNet noise sample is [array([-1.7225052], dtype=float32), 1.1882865]. 
=============================================
[2019-04-04 08:14:45,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:14:45,982] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:14:46,001] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run19
[2019-04-04 08:14:55,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:14:55,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:14:55,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run19
[2019-04-04 08:14:57,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:14:57,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:14:57,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run19
[2019-04-04 08:14:58,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:14:58,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:14:58,192] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run19
[2019-04-04 08:14:58,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:14:58,841] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:14:58,844] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run19
[2019-04-04 08:14:59,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:14:59,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:14:59,295] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run19
[2019-04-04 08:15:00,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:00,553] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:00,557] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run19
[2019-04-04 08:15:00,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:00,957] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:00,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run19
[2019-04-04 08:15:01,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:01,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:01,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run19
[2019-04-04 08:15:02,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:02,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:02,614] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run19
[2019-04-04 08:15:03,718] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:03,719] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:03,722] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run19
[2019-04-04 08:15:06,296] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1261233e-09 2.3153850e-09 1.7239677e-22 2.6828356e-10 6.9990083e-11
 1.1684375e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:06,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8607
[2019-04-04 08:15:06,316] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 50.0, 0.0, 0.0, 26.0, 25.29345449288046, 0.2949968820420242, 0.0, 1.0, 54685.57491511018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4935600.0000, 
sim time next is 4936200.0000, 
raw observation next is [-1.166666666666667, 50.0, 0.0, 0.0, 26.0, 25.24035676822307, 0.2936526625105207, 0.0, 1.0, 43873.47483789016], 
processed observation next is [1.0, 0.13043478260869565, 0.43028624192059095, 0.5, 0.0, 0.0, 0.6666666666666666, 0.6033630640185891, 0.5978842208368402, 0.0, 1.0, 0.20892130875185788], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.64210296], dtype=float32), 0.31960887]. 
=============================================
[2019-04-04 08:15:08,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:08,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:08,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run19
[2019-04-04 08:15:14,842] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0562258e-09 7.7059209e-10 1.8179022e-22 1.7325652e-10 9.0251077e-11
 2.3669189e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:14,845] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6431
[2019-04-04 08:15:14,858] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.883333333333333, 90.33333333333334, 0.0, 0.0, 26.0, 24.24191458592058, 0.1157433132302564, 0.0, 1.0, 42122.56147887168], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 94200.0000, 
sim time next is 94800.0000, 
raw observation next is [-2.066666666666667, 89.66666666666667, 0.0, 0.0, 26.0, 24.19659016770169, 0.1171144668110194, 0.0, 1.0, 42284.32194256665], 
processed observation next is [1.0, 0.08695652173913043, 0.40535549399815335, 0.8966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5163825139751408, 0.5390381556036731, 0.0, 1.0, 0.20135391401222213], 
reward next is 0.7986, 
noisyNet noise sample is [array([1.30826], dtype=float32), -0.21090622]. 
=============================================
[2019-04-04 08:15:15,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1555655e-13 9.5450167e-12 8.9711733e-28 2.3478084e-13 2.3502311e-14
 3.0699965e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:15,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1115
[2019-04-04 08:15:15,568] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 121.0, 862.5, 26.0, 27.5800405575392, 0.9038891927213477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5058000.0000, 
sim time next is 5058600.0000, 
raw observation next is [9.333333333333334, 24.16666666666667, 120.0, 861.6666666666666, 26.0, 27.69511786659898, 0.9321891198165847, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7211449676823639, 0.24166666666666672, 0.4, 0.9521178637200737, 0.6666666666666666, 0.8079264888832484, 0.8107297066055282, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10061569], dtype=float32), -1.6926446]. 
=============================================
[2019-04-04 08:15:15,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3754296e-09 3.4042231e-09 8.3071746e-21 7.8248236e-10 3.7619180e-10
 3.4438598e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:15,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7213
[2019-04-04 08:15:15,694] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 71.5, 0.0, 0.0, 26.0, 23.23618467633299, -0.09023655364389764, 0.0, 1.0, 45590.33572720978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 109800.0000, 
sim time next is 110400.0000, 
raw observation next is [-7.1, 70.33333333333333, 0.0, 0.0, 26.0, 23.22454968517724, -0.1012307581125648, 0.0, 1.0, 45700.42877450833], 
processed observation next is [1.0, 0.2608695652173913, 0.2659279778393352, 0.7033333333333333, 0.0, 0.0, 0.6666666666666666, 0.4353791404314367, 0.46625641396247836, 0.0, 1.0, 0.21762108940242061], 
reward next is 0.7824, 
noisyNet noise sample is [array([-1.4517206], dtype=float32), 0.1401777]. 
=============================================
[2019-04-04 08:15:15,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.2689868e-13 8.1598331e-12 7.4900080e-28 2.0919952e-13 2.1102484e-14
 2.6452306e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:15,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2510
[2019-04-04 08:15:15,772] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.33333333333333, 21.66666666666666, 116.8333333333333, 853.1666666666667, 26.0, 27.5557498860302, 0.927308026672684, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5060400.0000, 
sim time next is 5061000.0000, 
raw observation next is [10.66666666666667, 20.83333333333334, 115.6666666666667, 846.3333333333333, 26.0, 26.95937100429299, 0.8919099132647196, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.7580794090489382, 0.2083333333333334, 0.38555555555555565, 0.9351749539594842, 0.6666666666666666, 0.7466142503577492, 0.7973033044215732, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.10061569], dtype=float32), -1.6926446]. 
=============================================
[2019-04-04 08:15:15,784] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[97.86302]
 [97.89401]
 [97.74572]
 [97.56076]
 [97.52377]], R is [[97.89400482]
 [97.91506195]
 [97.93591309]
 [97.95655823]
 [97.97699738]].
[2019-04-04 08:15:16,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:16,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:16,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run19
[2019-04-04 08:15:18,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:18,537] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:18,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run19
[2019-04-04 08:15:18,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:18,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:18,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run19
[2019-04-04 08:15:19,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2141095e-09 5.7729355e-09 5.5687972e-21 6.5130173e-10 8.7371957e-11
 7.4145638e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:19,206] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3081
[2019-04-04 08:15:19,250] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.04751746464618, 0.3007762505710571, 0.0, 1.0, 23212.17394460347], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 159600.0000, 
sim time next is 160200.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.99991040220249, 0.2805014425501773, 0.0, 1.0, 35186.5295509158], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5833258668502076, 0.5935004808500591, 0.0, 1.0, 0.16755490262340858], 
reward next is 0.8324, 
noisyNet noise sample is [array([-0.19653738], dtype=float32), -0.5512232]. 
=============================================
[2019-04-04 08:15:19,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:15:19,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:15:19,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run19
[2019-04-04 08:15:30,350] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0556612e-12 8.6968273e-11 1.5990796e-24 8.6707525e-12 1.0807605e-12
 3.4931268e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:30,368] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1950
[2019-04-04 08:15:30,418] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.34662369829584, 0.2853937248080233, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 237000.0000, 
sim time next is 237600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.22764118358315, 0.2588438997133118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6023034319652624, 0.5862812999044372, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41627344], dtype=float32), -1.1622105]. 
=============================================
[2019-04-04 08:15:33,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1906213e-09 1.6638465e-08 3.5276532e-20 2.5526281e-09 5.3276661e-10
 4.7568477e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:33,775] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4868
[2019-04-04 08:15:33,788] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.33333333333333, 69.0, 0.0, 0.0, 26.0, 22.90008239251475, -0.2005072357952256, 0.0, 1.0, 47811.22820896592], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 279600.0000, 
sim time next is 280200.0000, 
raw observation next is [-11.51666666666667, 69.5, 0.0, 0.0, 26.0, 22.83587296583839, -0.213237055273605, 0.0, 1.0, 47829.34408748151], 
processed observation next is [1.0, 0.21739130434782608, 0.14358264081255764, 0.695, 0.0, 0.0, 0.6666666666666666, 0.4029894138198659, 0.42892098157546504, 0.0, 1.0, 0.22775878136895958], 
reward next is 0.7722, 
noisyNet noise sample is [array([-0.7656534], dtype=float32), 0.086940706]. 
=============================================
[2019-04-04 08:15:42,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6697103e-10 1.1323331e-09 7.4285365e-23 1.3099834e-10 2.6414926e-11
 4.4066584e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:42,703] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9611
[2019-04-04 08:15:42,803] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.9, 64.66666666666667, 0.0, 0.0, 26.0, 25.07133613239284, 0.3346424472551301, 1.0, 1.0, 21983.12619771644], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 155400.0000, 
sim time next is 156000.0000, 
raw observation next is [-8.0, 65.33333333333334, 0.0, 0.0, 26.0, 25.16535250428964, 0.3231470301526972, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.24099722991689754, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5971127086908034, 0.6077156767175658, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0317492], dtype=float32), 0.12417131]. 
=============================================
[2019-04-04 08:15:42,815] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.3933 ]
 [78.58916]
 [78.46645]
 [78.06423]
 [77.60415]], R is [[78.36248779]
 [78.47418213]
 [78.296875  ]
 [77.82131958]
 [77.37294006]].
[2019-04-04 08:15:43,965] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2676799e-11 7.7934513e-11 1.0651929e-23 1.8162522e-11 3.6550012e-12
 7.2699779e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:15:43,968] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3378
[2019-04-04 08:15:44,062] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 78.0, 56.5, 148.0, 26.0, 25.30907388712632, 0.2201463160441554, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 205200.0000, 
sim time next is 205800.0000, 
raw observation next is [-8.216666666666667, 77.5, 64.0, 98.66666666666664, 26.0, 25.30847573761607, 0.2240116417004669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.23499538319482918, 0.775, 0.21333333333333335, 0.10902394106813994, 0.6666666666666666, 0.6090396448013392, 0.574670547233489, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5427328], dtype=float32), 0.46634543]. 
=============================================
[2019-04-04 08:15:55,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.14096475e-11 2.92379870e-11 3.63586312e-25 3.73973639e-12
 1.27098169e-12 4.70676007e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:15:55,200] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7046
[2019-04-04 08:15:55,268] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.96666666666667, 61.0, 90.16666666666666, 536.3333333333334, 26.0, 25.82335594581672, 0.3602859564727177, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 297600.0000, 
sim time next is 298200.0000, 
raw observation next is [-10.78333333333333, 60.5, 93.33333333333334, 560.6666666666666, 26.0, 25.80098501585081, 0.3582060068369762, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.1638965835641737, 0.605, 0.3111111111111111, 0.6195211786372007, 0.6666666666666666, 0.6500820846542341, 0.619402002278992, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54532474], dtype=float32), 0.07183676]. 
=============================================
[2019-04-04 08:16:02,941] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.8838653e-11 1.4810543e-10 1.1059232e-24 4.4273817e-11 5.2701632e-12
 4.2880721e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:02,942] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3596
[2019-04-04 08:16:03,020] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.916666666666667, 32.83333333333334, 49.0, 0.0, 26.0, 25.55872239528254, 0.2006265602407246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 465000.0000, 
sim time next is 465600.0000, 
raw observation next is [-5.633333333333334, 32.66666666666667, 55.50000000000001, 0.0, 26.0, 25.53908626653034, 0.1981979462165576, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.30655586334256696, 0.3266666666666667, 0.18500000000000003, 0.0, 0.6666666666666666, 0.6282571888775283, 0.5660659820721858, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3053179], dtype=float32), 0.40866065]. 
=============================================
[2019-04-04 08:16:11,124] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9868064e-10 1.1719106e-09 5.9075408e-23 1.2270482e-10 3.1196361e-11
 2.2663651e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:11,125] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8220
[2019-04-04 08:16:11,150] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 67.0, 0.0, 0.0, 26.0, 25.02531936966371, 0.2196593467886331, 0.0, 1.0, 42740.31806773723], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 678600.0000, 
sim time next is 679200.0000, 
raw observation next is [-3.2, 67.66666666666667, 0.0, 0.0, 26.0, 24.99765895773892, 0.2123492821102044, 0.0, 1.0, 42483.78807146917], 
processed observation next is [0.0, 0.8695652173913043, 0.37396121883656513, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5831382464782434, 0.5707830940367348, 0.0, 1.0, 0.20230375272128176], 
reward next is 0.7977, 
noisyNet noise sample is [array([-0.73877937], dtype=float32), -2.1355257]. 
=============================================
[2019-04-04 08:16:17,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0854842e-10 1.4887666e-10 4.2263578e-24 6.9399868e-11 9.5278464e-12
 3.3899211e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:17,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1849
[2019-04-04 08:16:17,119] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.25095308869876, 0.02894175085831643, 0.0, 1.0, 41956.42196727282], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 715200.0000, 
sim time next is 715800.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.2438440065476, 0.01718394564063278, 0.0, 1.0, 41981.82350274587], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5203203338789667, 0.505727981880211, 0.0, 1.0, 0.19991344525117083], 
reward next is 0.8001, 
noisyNet noise sample is [array([0.7588004], dtype=float32), 0.29165405]. 
=============================================
[2019-04-04 08:16:19,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1862059e-10 2.3283063e-10 1.1047637e-24 1.5633887e-11 1.0649617e-11
 2.3482283e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:19,883] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2254
[2019-04-04 08:16:19,961] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 87.0, 110.5, 122.0, 26.0, 24.80441938866144, 0.2552977528583804, 0.0, 1.0, 70318.22719244647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 554400.0000, 
sim time next is 555000.0000, 
raw observation next is [-0.6, 86.33333333333333, 99.33333333333331, 128.3333333333333, 26.0, 24.78496987236842, 0.2622479733920772, 0.0, 1.0, 66139.12541096906], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.8633333333333333, 0.331111111111111, 0.14180478821362794, 0.6666666666666666, 0.5654141560307018, 0.5874159911306924, 0.0, 1.0, 0.3149482162427098], 
reward next is 0.6851, 
noisyNet noise sample is [array([-0.72856915], dtype=float32), -0.9151148]. 
=============================================
[2019-04-04 08:16:19,986] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.389946]
 [85.95444 ]
 [86.45717 ]
 [86.88627 ]
 [86.88265 ]], R is [[84.65522003]
 [84.47382355]
 [84.32524109]
 [84.39276886]
 [84.54884338]].
[2019-04-04 08:16:21,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8144190e-11 3.5772673e-11 3.6427924e-24 6.2542254e-12 2.5236360e-12
 5.4572304e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:21,872] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9685
[2019-04-04 08:16:21,948] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 82.0, 122.5, 401.3333333333334, 26.0, 25.03092311104506, 0.3528107761441155, 0.0, 1.0, 18721.57134378249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 571200.0000, 
sim time next is 571800.0000, 
raw observation next is [-1.2, 82.5, 118.0, 335.6666666666667, 26.0, 25.03052409941997, 0.3434422971871475, 0.0, 1.0, 18720.33246282479], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.825, 0.3933333333333333, 0.370902394106814, 0.6666666666666666, 0.5858770082849976, 0.6144807657290492, 0.0, 1.0, 0.08914444029916567], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.7619573], dtype=float32), 0.0032219994]. 
=============================================
[2019-04-04 08:16:24,931] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1830714e-12 1.3297798e-11 4.2768271e-25 1.1975918e-12 1.2541795e-13
 9.7981366e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:24,932] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7919
[2019-04-04 08:16:24,986] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 25.06787072106033, 0.2990073418884059, 1.0, 1.0, 45505.01498298821], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 762000.0000, 
sim time next is 762600.0000, 
raw observation next is [-4.816666666666666, 57.16666666666667, 0.0, 0.0, 26.0, 24.98154187192152, 0.2987186756393406, 1.0, 1.0, 96870.94378082962], 
processed observation next is [1.0, 0.8260869565217391, 0.32917820867959374, 0.5716666666666668, 0.0, 0.0, 0.6666666666666666, 0.5817951559934601, 0.5995728918797801, 1.0, 1.0, 0.4612902084801411], 
reward next is 0.5387, 
noisyNet noise sample is [array([-0.39335224], dtype=float32), -0.9449178]. 
=============================================
[2019-04-04 08:16:27,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5217442e-11 3.1594453e-11 3.3116403e-25 2.6604422e-12 4.1654554e-13
 3.5542316e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:27,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8527
[2019-04-04 08:16:27,574] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 84.66666666666666, 24.16666666666667, 0.0, 26.0, 25.4160565924734, 0.3116281893515619, 1.0, 1.0, 46841.89470733552], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 837600.0000, 
sim time next is 838200.0000, 
raw observation next is [-3.9, 85.33333333333334, 19.33333333333334, 0.0, 26.0, 24.68187115577426, 0.3121933402055614, 1.0, 1.0, 198364.5779226428], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.8533333333333334, 0.06444444444444447, 0.0, 0.6666666666666666, 0.5568225963145217, 0.6040644467351871, 1.0, 1.0, 0.944593228203061], 
reward next is 0.0554, 
noisyNet noise sample is [array([0.53557587], dtype=float32), 1.0393542]. 
=============================================
[2019-04-04 08:16:48,394] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.1880703e-12 2.4306772e-12 1.8033584e-27 5.1199149e-13 1.7606591e-13
 1.4409231e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:48,394] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4030
[2019-04-04 08:16:48,414] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 95.33333333333333, 0.0, 0.0, 26.0, 25.19585159205904, 0.3158075948779777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 928200.0000, 
sim time next is 928800.0000, 
raw observation next is [4.4, 96.0, 0.0, 0.0, 26.0, 25.13118846047306, 0.3063244988459518, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5942657050394216, 0.6021081662819839, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08759463], dtype=float32), -1.0467654]. 
=============================================
[2019-04-04 08:16:49,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9778830e-13 1.9152544e-12 8.0687490e-28 2.0605205e-13 3.8696321e-14
 2.4857712e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:49,313] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8393
[2019-04-04 08:16:49,326] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.066666666666667, 95.66666666666667, 102.8333333333333, 0.0, 26.0, 25.29662173522826, 0.2642491131497371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 908400.0000, 
sim time next is 909000.0000, 
raw observation next is [3.25, 95.0, 104.0, 0.0, 26.0, 25.22382137352477, 0.2588479350609945, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5526315789473685, 0.95, 0.3466666666666667, 0.0, 0.6666666666666666, 0.6019851144603976, 0.5862826450203314, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0421276], dtype=float32), -0.3179607]. 
=============================================
[2019-04-04 08:16:49,330] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[95.44695 ]
 [95.320045]
 [95.201935]
 [95.09118 ]
 [95.00457 ]], R is [[95.5943985 ]
 [95.63845825]
 [95.6820755 ]
 [95.72525787]
 [95.76800537]].
[2019-04-04 08:16:51,706] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2091420e-11 2.2221378e-11 1.4939036e-25 2.3764684e-12 2.1199206e-12
 9.1929180e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:51,709] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7573
[2019-04-04 08:16:51,723] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.55343473028335, 0.5325912291564134, 0.0, 1.0, 18744.20024836863], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1315800.0000, 
sim time next is 1316400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.48195015539194, 0.5365200858597478, 0.0, 1.0, 55749.92580718879], 
processed observation next is [1.0, 0.21739130434782608, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6234958462826615, 0.6788400286199159, 0.0, 1.0, 0.26547583717708945], 
reward next is 0.7345, 
noisyNet noise sample is [array([0.38926247], dtype=float32), 0.12516971]. 
=============================================
[2019-04-04 08:16:52,045] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0471973e-13 5.7389998e-13 1.5845577e-28 1.0442130e-13 1.6667382e-14
 1.8650469e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:16:52,046] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3113
[2019-04-04 08:16:52,052] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.7, 86.0, 112.0, 0.0, 26.0, 26.66259078833274, 0.6705095089722938, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 990600.0000, 
sim time next is 991200.0000, 
raw observation next is [11.8, 86.0, 116.0, 0.0, 26.0, 26.65443562555125, 0.6782173170095579, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.7894736842105264, 0.86, 0.38666666666666666, 0.0, 0.6666666666666666, 0.7212029687959376, 0.726072439003186, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.59534204], dtype=float32), 0.1959997]. 
=============================================
[2019-04-04 08:17:00,134] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1468333e-12 4.9077339e-12 4.3442083e-26 8.3887151e-13 6.8407269e-14
 1.1433435e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:17:00,134] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9885
[2019-04-04 08:17:00,165] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 45.0, 0.0, 26.0, 26.03730379980853, 0.588597221355959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1330200.0000, 
sim time next is 1330800.0000, 
raw observation next is [0.5, 92.0, 54.5, 0.0, 26.0, 26.08792225195037, 0.5895144679483473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4764542936288089, 0.92, 0.18166666666666667, 0.0, 0.6666666666666666, 0.6739935209958642, 0.696504822649449, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28332648], dtype=float32), 0.12628436]. 
=============================================
[2019-04-04 08:17:12,349] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0311573e-12 6.4323116e-11 2.6312909e-25 2.5888293e-12 1.0893658e-12
 2.4729569e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:17:12,351] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4626
[2019-04-04 08:17:12,363] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 82.0, 0.0, 0.0, 26.0, 25.58434253957916, 0.5739347260831248, 0.0, 1.0, 18739.46116390277], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1552800.0000, 
sim time next is 1553400.0000, 
raw observation next is [5.25, 82.0, 0.0, 0.0, 26.0, 25.57274552742539, 0.5644402270737611, 0.0, 1.0, 18738.2382081111], 
processed observation next is [1.0, 1.0, 0.60803324099723, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6310621272854492, 0.6881467423579203, 0.0, 1.0, 0.08922970575290999], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.1139971], dtype=float32), 1.8847473]. 
=============================================
[2019-04-04 08:17:36,564] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5829941e-10 7.9851681e-10 2.0358917e-22 8.8741098e-11 6.8615016e-11
 1.6323923e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:17:36,564] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8441
[2019-04-04 08:17:36,615] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.0880739883594, 0.3184138557922362, 0.0, 1.0, 46589.48388229196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1801200.0000, 
sim time next is 1801800.0000, 
raw observation next is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.0602835331803, 0.3128778378271226, 0.0, 1.0, 46167.18341318843], 
processed observation next is [0.0, 0.8695652173913043, 0.3310249307479225, 0.845, 0.0, 0.0, 0.6666666666666666, 0.5883569610983583, 0.6042926126090409, 0.0, 1.0, 0.21984373053899253], 
reward next is 0.7802, 
noisyNet noise sample is [array([-0.02623415], dtype=float32), -0.32485327]. 
=============================================
[2019-04-04 08:17:40,323] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.5387392e-11 4.9396781e-10 1.2577550e-22 3.6415385e-11 1.0491531e-11
 5.4613981e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:17:40,323] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5112
[2019-04-04 08:17:40,408] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.2, 77.0, 0.0, 0.0, 26.0, 25.49245182559808, 0.4043834773180189, 0.0, 1.0, 31743.99861523229], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [-5.3, 78.5, 0.0, 0.0, 26.0, 25.38064546644452, 0.3814161330258899, 0.0, 1.0, 34459.72555863416], 
processed observation next is [1.0, 0.8260869565217391, 0.31578947368421056, 0.785, 0.0, 0.0, 0.6666666666666666, 0.6150537888703766, 0.62713871100863, 0.0, 1.0, 0.16409393123159124], 
reward next is 0.8359, 
noisyNet noise sample is [array([-0.63448024], dtype=float32), -0.35164973]. 
=============================================
[2019-04-04 08:17:42,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2352853e-11 7.0204675e-11 1.4670677e-24 1.0452862e-11 1.6768097e-12
 3.2822527e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:17:42,795] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4132
[2019-04-04 08:17:42,856] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.300000000000001, 79.0, 149.3333333333333, 0.0, 26.0, 24.24189828603048, 0.3108668898766903, 1.0, 1.0, 200301.7797366625], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2035200.0000, 
sim time next is 2035800.0000, 
raw observation next is [-4.2, 79.0, 148.0, 0.0, 26.0, 25.08356972004286, 0.3985518609932586, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.34626038781163443, 0.79, 0.49333333333333335, 0.0, 0.6666666666666666, 0.5902974766702384, 0.6328506203310862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65839475], dtype=float32), 1.6636094]. 
=============================================
[2019-04-04 08:17:43,108] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 08:17:43,117] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:17:43,117] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:17:43,139] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:17:43,156] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:17:43,162] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:17:43,163] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:17:43,325] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run26
[2019-04-04 08:17:43,569] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run26
[2019-04-04 08:17:43,581] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run26
[2019-04-04 08:18:32,335] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.187723], dtype=float32), -0.13504909]
[2019-04-04 08:18:32,335] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.95, 82.0, 165.0, 413.0, 26.0, 25.50119523149499, 0.3478634793116954, 1.0, 1.0, 0.0]
[2019-04-04 08:18:32,335] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:18:32,337] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.8713355e-12 1.0704296e-11 6.4320203e-25 2.4056785e-12 4.2732495e-13
 2.7908872e-15 1.0000000e+00], sampled 0.35302889467348564
[2019-04-04 08:20:49,855] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:21:21,260] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:21:25,796] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:21:26,831] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 2500000, evaluation results [2500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:21:36,545] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4123017e-12 1.8339125e-10 9.6309275e-24 2.1005378e-11 4.2412345e-12
 1.4616853e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:21:36,545] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9058
[2019-04-04 08:21:36,587] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 66.0, 36.0, 0.0, 26.0, 25.83404012440165, 0.453138989876085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2132400.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 26.0, 0.0, 26.0, 25.98545423765922, 0.458471065821533, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.665, 0.08666666666666667, 0.0, 0.6666666666666666, 0.665454519804935, 0.6528236886071777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21846646], dtype=float32), 0.71107256]. 
=============================================
[2019-04-04 08:21:36,592] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[82.160965]
 [82.23693 ]
 [82.40817 ]
 [81.75754 ]
 [81.60807 ]], R is [[82.22163391]
 [82.39942169]
 [82.57543182]
 [81.80474091]
 [81.62644958]].
[2019-04-04 08:21:37,227] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.0457993e-12 1.1535277e-10 3.7024173e-24 9.9469842e-12 1.3754284e-12
 5.1579792e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:21:37,236] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7342
[2019-04-04 08:21:37,362] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.866666666666667, 75.0, 0.0, 0.0, 26.0, 25.33035619803988, 0.3386403916266121, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1970400.0000, 
sim time next is 1971000.0000, 
raw observation next is [-5.05, 77.0, 0.0, 0.0, 26.0, 25.194966006035, 0.3099923397652823, 1.0, 1.0, 25671.08504881978], 
processed observation next is [1.0, 0.8260869565217391, 0.32271468144044324, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5995805005029166, 0.6033307799217608, 1.0, 1.0, 0.12224326213723705], 
reward next is 0.8778, 
noisyNet noise sample is [array([0.63770103], dtype=float32), 0.6730681]. 
=============================================
[2019-04-04 08:21:37,373] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[83.82489 ]
 [83.92366 ]
 [84.08661 ]
 [84.26077 ]
 [84.131744]], R is [[83.84306335]
 [84.00463104]
 [84.16458893]
 [84.32294464]
 [84.22091675]].
[2019-04-04 08:21:40,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2382072e-12 1.1200603e-11 5.5701035e-25 3.1735764e-12 1.2496497e-13
 8.7117847e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:21:40,152] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9384
[2019-04-04 08:21:40,205] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.1, 68.0, 112.0, 0.0, 26.0, 26.25935664052702, 0.5102294774850981, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2127000.0000, 
sim time next is 2127600.0000, 
raw observation next is [-5.0, 68.0, 105.5, 0.0, 26.0, 26.29070465045418, 0.5085977847540678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.32409972299168976, 0.68, 0.3516666666666667, 0.0, 0.6666666666666666, 0.690892054204515, 0.6695325949180226, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4563262], dtype=float32), 0.46352476]. 
=============================================
[2019-04-04 08:21:53,529] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5656812e-10 5.0442772e-10 4.2721213e-22 3.5565446e-11 1.6504614e-11
 3.3632214e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:21:53,530] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7723
[2019-04-04 08:21:53,558] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 91.0, 0.0, 0.0, 26.0, 23.69355740815719, -0.00880450846326535, 0.0, 1.0, 43320.51480658642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2265600.0000, 
sim time next is 2266200.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.68407757142769, -0.01663578602112774, 0.0, 1.0, 43271.28039951844], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.47367313095230745, 0.4944547379929574, 0.0, 1.0, 0.20605371618818302], 
reward next is 0.7939, 
noisyNet noise sample is [array([0.7580198], dtype=float32), 0.7003864]. 
=============================================
[2019-04-04 08:22:19,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5682583e-09 6.6904882e-10 1.1291913e-23 1.2188375e-10 2.7295615e-11
 3.3887251e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:19,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1139
[2019-04-04 08:22:19,806] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 64.99999999999999, 120.0, 26.0, 25.20015590421547, 0.314459814883175, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2364600.0000, 
sim time next is 2365200.0000, 
raw observation next is [-3.4, 69.0, 79.0, 180.0, 26.0, 25.33190534754004, 0.3229324590057953, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.368421052631579, 0.69, 0.2633333333333333, 0.19889502762430938, 0.6666666666666666, 0.6109921122950034, 0.6076441530019318, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3459991], dtype=float32), 0.25862378]. 
=============================================
[2019-04-04 08:22:22,570] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.3170168e-10 9.9676578e-10 7.2805216e-23 8.2367550e-11 8.0311091e-11
 2.1911798e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:22,570] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1938
[2019-04-04 08:22:22,605] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 73.0, 0.0, 0.0, 26.0, 24.90176689921308, 0.2551997886396772, 0.0, 1.0, 41645.68953058771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2598600.0000, 
sim time next is 2599200.0000, 
raw observation next is [-5.0, 74.0, 0.0, 0.0, 26.0, 24.97989800314202, 0.252424487895651, 0.0, 1.0, 41599.9774153428], 
processed observation next is [1.0, 0.08695652173913043, 0.32409972299168976, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5816581669285016, 0.584141495965217, 0.0, 1.0, 0.19809513054925143], 
reward next is 0.8019, 
noisyNet noise sample is [array([-0.40090975], dtype=float32), 0.505505]. 
=============================================
[2019-04-04 08:22:25,274] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8506923e-09 1.3600936e-09 3.2183884e-22 5.1300003e-10 7.0246198e-11
 8.9996402e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:25,274] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3927
[2019-04-04 08:22:25,421] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.766666666666667, 55.33333333333334, 33.83333333333334, 353.3333333333334, 26.0, 22.92071626667022, -0.1193043384827321, 0.0, 1.0, 202369.3775151992], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2449200.0000, 
sim time next is 2449800.0000, 
raw observation next is [-8.4, 54.0, 40.0, 416.0, 26.0, 23.30715263994946, -0.005743958792985865, 0.0, 1.0, 203134.9941282829], 
processed observation next is [0.0, 0.34782608695652173, 0.2299168975069252, 0.54, 0.13333333333333333, 0.45966850828729283, 0.6666666666666666, 0.4422627199957884, 0.4980853470690047, 0.0, 1.0, 0.9673094958489662], 
reward next is 0.0327, 
noisyNet noise sample is [array([-0.12691894], dtype=float32), -1.1944765]. 
=============================================
[2019-04-04 08:22:27,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4416021e-10 5.4401067e-10 6.7410663e-23 1.1544940e-10 1.9701135e-11
 1.7263578e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:27,983] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9479
[2019-04-04 08:22:28,062] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 62.5, 0.0, 0.0, 26.0, 24.9714935240008, 0.3584046472876619, 0.0, 1.0, 81202.30901040214], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2663400.0000, 
sim time next is 2664000.0000, 
raw observation next is [-1.2, 63.0, 0.0, 0.0, 26.0, 24.95541146598098, 0.372744739425137, 0.0, 1.0, 64368.91402166285], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5796176221650816, 0.6242482464750457, 0.0, 1.0, 0.30651863819839453], 
reward next is 0.6935, 
noisyNet noise sample is [array([-1.3074894], dtype=float32), -1.1411216]. 
=============================================
[2019-04-04 08:22:28,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.865204]
 [80.93381 ]
 [76.94998 ]
 [79.88457 ]
 [85.025185]], R is [[77.64686584]
 [77.48371887]
 [77.29397583]
 [77.39875793]
 [77.62477112]].
[2019-04-04 08:22:37,488] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4311251e-10 1.0940254e-09 4.4954222e-22 1.4923515e-10 9.0150949e-11
 3.8217004e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:37,488] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0645
[2019-04-04 08:22:37,540] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 63.16666666666667, 0.0, 0.0, 26.0, 25.42303151967523, 0.4510676523878006, 0.0, 1.0, 35089.86393125188], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2754600.0000, 
sim time next is 2755200.0000, 
raw observation next is [-6.0, 62.33333333333334, 0.0, 0.0, 26.0, 25.40074099102904, 0.4453222742252606, 0.0, 1.0, 55241.59919473575], 
processed observation next is [1.0, 0.9130434782608695, 0.296398891966759, 0.6233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6167284159190866, 0.6484407580750869, 0.0, 1.0, 0.2630552342606464], 
reward next is 0.7369, 
noisyNet noise sample is [array([0.2838185], dtype=float32), 0.90786004]. 
=============================================
[2019-04-04 08:22:40,801] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1225074e-10 1.8914461e-09 1.1431955e-22 5.0153305e-11 3.5640414e-11
 7.2015592e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:40,806] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4522
[2019-04-04 08:22:40,849] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 63.33333333333333, 0.0, 0.0, 26.0, 25.0022096479458, 0.3740183522085482, 0.0, 1.0, 23329.42446317054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [-1.2, 63.66666666666667, 0.0, 0.0, 26.0, 25.02916427203987, 0.3829862441688923, 0.0, 1.0, 189214.8487388082], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5857636893366559, 0.6276620813896308, 0.0, 1.0, 0.90102308923242], 
reward next is 0.0990, 
noisyNet noise sample is [array([0.32351378], dtype=float32), -0.9332603]. 
=============================================
[2019-04-04 08:22:41,509] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.63449343e-10 5.76521098e-09 6.37287404e-23 9.61249275e-11
 7.14923190e-11 1.10911924e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 08:22:41,510] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1409
[2019-04-04 08:22:41,526] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.38448137357232, 0.4321434539896289, 0.0, 1.0, 42575.65767306398], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2841600.0000, 
sim time next is 2842200.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.43974332867863, 0.4360251709378224, 0.0, 1.0, 18761.70884112401], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6199786107232192, 0.6453417236459408, 0.0, 1.0, 0.0893414706720191], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.5948092], dtype=float32), -0.1808698]. 
=============================================
[2019-04-04 08:22:45,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0767037e-11 7.5877651e-11 3.5888892e-24 1.5419382e-12 2.9530701e-12
 6.8277924e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:45,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7942
[2019-04-04 08:22:45,662] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.1869947454226, 0.3700268441267016, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2917200.0000, 
sim time next is 2917800.0000, 
raw observation next is [0.0, 92.5, 0.0, 0.0, 26.0, 25.00831032712875, 0.3672727387030481, 1.0, 1.0, 85162.16511579743], 
processed observation next is [1.0, 0.782608695652174, 0.46260387811634357, 0.925, 0.0, 0.0, 0.6666666666666666, 0.5840258605940626, 0.6224242462343493, 1.0, 1.0, 0.40553411959903535], 
reward next is 0.5945, 
noisyNet noise sample is [array([-0.01067418], dtype=float32), -0.6349853]. 
=============================================
[2019-04-04 08:22:54,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0844091e-11 3.0605826e-10 8.5074057e-24 2.0478907e-11 7.5490595e-12
 4.7665153e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:22:54,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2411
[2019-04-04 08:22:55,015] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.35666602636819, 0.5049432921417821, 0.0, 1.0, 40665.95458741377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3215400.0000, 
sim time next is 3216000.0000, 
raw observation next is [-2.333333333333333, 100.0, 0.0, 0.0, 26.0, 25.35122355476976, 0.4969229862247934, 0.0, 1.0, 40590.4851680076], 
processed observation next is [1.0, 0.21739130434782608, 0.3979686057248385, 1.0, 0.0, 0.0, 0.6666666666666666, 0.61260196289748, 0.6656409954082645, 0.0, 1.0, 0.19328802460956002], 
reward next is 0.8067, 
noisyNet noise sample is [array([2.075167], dtype=float32), -1.0487247]. 
=============================================
[2019-04-04 08:22:55,027] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.12038]
 [81.28542]
 [81.4226 ]
 [81.48332]
 [81.51137]], R is [[80.93804932]
 [80.93502045]
 [80.93115234]
 [80.92528534]
 [80.91313171]].
[2019-04-04 08:23:10,258] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9094681e-10 3.4546857e-10 5.2229236e-22 1.7832005e-10 5.4278842e-11
 2.6542928e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:10,258] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9469
[2019-04-04 08:23:10,301] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.36361217295942, 0.1326868515724793, 0.0, 1.0, 38846.10037409302], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3033000.0000, 
sim time next is 3033600.0000, 
raw observation next is [-5.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.30812124347784, 0.1228126126485738, 0.0, 1.0, 39015.22109962886], 
processed observation next is [0.0, 0.08695652173913043, 0.30563250230840255, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5256767702898199, 0.5409375375495246, 0.0, 1.0, 0.18578676714108983], 
reward next is 0.8142, 
noisyNet noise sample is [array([-0.18261348], dtype=float32), 0.58829033]. 
=============================================
[2019-04-04 08:23:15,020] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.0660421e-10 3.9439776e-09 5.4119342e-22 1.6119434e-10 1.6737216e-10
 9.9739943e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:15,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1615
[2019-04-04 08:23:15,037] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 24.80905968377819, 0.2318460603342052, 0.0, 1.0, 42730.83549243791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3388200.0000, 
sim time next is 3388800.0000, 
raw observation next is [-4.333333333333334, 67.33333333333334, 0.0, 0.0, 26.0, 24.72116847519943, 0.2230561651181839, 0.0, 1.0, 42664.65866836238], 
processed observation next is [1.0, 0.21739130434782608, 0.3425669436749769, 0.6733333333333335, 0.0, 0.0, 0.6666666666666666, 0.5600973729332859, 0.5743520550393947, 0.0, 1.0, 0.2031650412779161], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.199439], dtype=float32), 0.76940656]. 
=============================================
[2019-04-04 08:23:20,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0196554e-11 6.8172142e-11 4.4714930e-23 1.6853267e-11 2.6289706e-12
 4.2220286e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:20,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3145
[2019-04-04 08:23:20,570] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 64.0, 80.16666666666666, 672.1666666666667, 26.0, 26.65636252127855, 0.6759470882386149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3426000.0000, 
sim time next is 3426600.0000, 
raw observation next is [2.166666666666667, 65.5, 76.33333333333334, 640.3333333333334, 26.0, 26.70273931924915, 0.5656510575798792, 1.0, 1.0, 3736.082334046107], 
processed observation next is [1.0, 0.6521739130434783, 0.5226223453370269, 0.655, 0.2544444444444445, 0.7075506445672192, 0.6666666666666666, 0.725228276604096, 0.6885503525266264, 1.0, 1.0, 0.017790868257362414], 
reward next is 0.9822, 
noisyNet noise sample is [array([-0.33859605], dtype=float32), 0.7298929]. 
=============================================
[2019-04-04 08:23:22,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.3531936e-10 9.4192953e-10 5.7644881e-23 1.0213064e-10 8.0388640e-11
 5.9622426e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:22,145] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7693
[2019-04-04 08:23:22,175] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.55508375939783, 0.4575419545566774, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3462000.0000, 
sim time next is 3462600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.56379045140961, 0.4486918080607731, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6303158709508008, 0.6495639360202577, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03367925], dtype=float32), 0.20552345]. 
=============================================
[2019-04-04 08:23:23,335] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8901628e-10 3.1888963e-09 3.8579966e-21 2.2527756e-10 1.8642711e-10
 3.1153162e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:23,342] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4520
[2019-04-04 08:23:23,371] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 68.66666666666666, 0.0, 0.0, 26.0, 25.10038256035005, 0.4461173675141495, 0.0, 1.0, 91031.95761190004], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3527400.0000, 
sim time next is 3528000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.03506243903738, 0.456141600769205, 0.0, 1.0, 103012.3548216757], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5862552032531149, 0.6520472002564016, 0.0, 1.0, 0.4905350229603605], 
reward next is 0.5095, 
noisyNet noise sample is [array([2.0465932], dtype=float32), -1.1062149]. 
=============================================
[2019-04-04 08:23:23,408] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[72.00707 ]
 [71.589676]
 [72.415955]
 [75.04884 ]
 [79.79292 ]], R is [[72.83908844]
 [72.67720795]
 [72.66454315]
 [72.79670715]
 [72.9258728 ]].
[2019-04-04 08:23:26,035] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1286118e-10 3.6673442e-10 5.9683140e-23 2.9202262e-11 2.0333884e-11
 1.9586092e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:26,036] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8681
[2019-04-04 08:23:26,054] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.5, 70.0, 3.0, 121.0, 26.0, 24.28495810116007, 0.190334928596643, 0.0, 1.0, 41542.06047585433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3569400.0000, 
sim time next is 3570000.0000, 
raw observation next is [-6.666666666666666, 70.0, 17.16666666666666, 171.6666666666667, 26.0, 24.24928226857997, 0.1925944391811048, 0.0, 1.0, 41553.42177211324], 
processed observation next is [0.0, 0.30434782608695654, 0.2779316712834719, 0.7, 0.0572222222222222, 0.18968692449355437, 0.6666666666666666, 0.5207735223816643, 0.5641981463937016, 0.0, 1.0, 0.19787343701006305], 
reward next is 0.8021, 
noisyNet noise sample is [array([-0.35504636], dtype=float32), -1.2822897]. 
=============================================
[2019-04-04 08:23:26,090] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[74.88985]
 [73.93899]
 [74.02595]
 [74.09613]
 [74.15155]], R is [[75.92855072]
 [75.97144318]
 [76.01428986]
 [76.05728149]
 [76.1003952 ]].
[2019-04-04 08:23:29,346] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9100360e-10 2.6921598e-10 1.9908329e-22 6.6666006e-11 4.3740404e-11
 7.9124099e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:29,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0303
[2019-04-04 08:23:29,393] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 54.83333333333334, 115.3333333333333, 818.3333333333334, 26.0, 25.17219074579951, 0.443965731799616, 0.0, 1.0, 18712.8961443272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3585000.0000, 
sim time next is 3585600.0000, 
raw observation next is [-3.0, 55.0, 116.0, 819.5, 26.0, 25.17921808236447, 0.445315906348072, 0.0, 1.0, 18712.6155525783], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.55, 0.38666666666666666, 0.905524861878453, 0.6666666666666666, 0.5982681735303726, 0.6484386354493573, 0.0, 1.0, 0.0891076931075157], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.81425387], dtype=float32), 1.9061092]. 
=============================================
[2019-04-04 08:23:29,830] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.3595058e-12 5.3694178e-11 2.1108295e-24 1.8266015e-11 2.5457590e-12
 2.5921126e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:29,830] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2836
[2019-04-04 08:23:29,846] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 60.16666666666666, 105.6666666666667, 736.6666666666666, 26.0, 26.21649742089709, 0.5769524173653423, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3492600.0000, 
sim time next is 3493200.0000, 
raw observation next is [0.3333333333333333, 60.33333333333334, 107.3333333333333, 753.3333333333334, 26.0, 26.26353078514955, 0.5763754897358138, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4718374884579871, 0.6033333333333334, 0.3577777777777777, 0.8324125230202579, 0.6666666666666666, 0.6886275654291291, 0.6921251632452713, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9604003], dtype=float32), -0.07836415]. 
=============================================
[2019-04-04 08:23:32,882] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.5350310e-10 6.8845513e-10 1.8204819e-24 1.2997220e-10 1.6236280e-11
 2.2878455e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:32,883] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6212
[2019-04-04 08:23:32,899] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.2, 27.0, 0.0, 0.0, 26.0, 25.47924138024652, 0.3600298546074405, 0.0, 1.0, 37425.47131989399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3639600.0000, 
sim time next is 3640200.0000, 
raw observation next is [8.166666666666666, 27.33333333333333, 0.0, 0.0, 26.0, 25.47446365088431, 0.3782090915202834, 0.0, 1.0, 36729.34325316867], 
processed observation next is [0.0, 0.13043478260869565, 0.6888273314866113, 0.27333333333333326, 0.0, 0.0, 0.6666666666666666, 0.6228719709070258, 0.6260696971734278, 0.0, 1.0, 0.17490163453889843], 
reward next is 0.8251, 
noisyNet noise sample is [array([0.11323814], dtype=float32), -0.039639477]. 
=============================================
[2019-04-04 08:23:41,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0289796e-11 2.9558794e-10 1.2466485e-23 2.7577310e-11 1.0783396e-11
 1.3786366e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:41,477] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3538
[2019-04-04 08:23:41,492] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.36522951713013, 0.4470787477215231, 0.0, 1.0, 56671.10350364526], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3546000.0000, 
sim time next is 3546600.0000, 
raw observation next is [-2.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 25.33771420791134, 0.4425421275368251, 0.0, 1.0, 48188.34168120848], 
processed observation next is [0.0, 0.043478260869565216, 0.4025854108956602, 0.6183333333333333, 0.0, 0.0, 0.6666666666666666, 0.6114761839926116, 0.6475140425122751, 0.0, 1.0, 0.2294682937200404], 
reward next is 0.7705, 
noisyNet noise sample is [array([0.6939954], dtype=float32), 0.28460744]. 
=============================================
[2019-04-04 08:23:45,070] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8236226e-10 1.7738414e-10 4.1830331e-25 3.9561108e-11 1.1170073e-11
 5.7079687e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:45,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5422
[2019-04-04 08:23:45,097] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.60078620067299, 0.3734109537857981, 0.0, 1.0, 18735.48893980787], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3631800.0000, 
sim time next is 3632400.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.56355117718372, 0.3672650490254382, 0.0, 1.0, 37560.19093003786], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6302959314319766, 0.6224216830084793, 0.0, 1.0, 0.17885805204779934], 
reward next is 0.8211, 
noisyNet noise sample is [array([2.1966865], dtype=float32), -1.0145983]. 
=============================================
[2019-04-04 08:23:45,839] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.5240127e-12 2.1497640e-11 5.9649846e-25 2.5795483e-12 1.4076027e-12
 2.7221627e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:45,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6448
[2019-04-04 08:23:45,889] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 49.0, 106.5, 733.5, 26.0, 26.26187894412664, 0.5651341056092571, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3924000.0000, 
sim time next is 3924600.0000, 
raw observation next is [-6.833333333333334, 49.0, 108.6666666666667, 747.3333333333334, 26.0, 26.31310518176711, 0.5777571459698381, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.27331486611265005, 0.49, 0.36222222222222233, 0.8257826887661143, 0.6666666666666666, 0.6927587651472592, 0.6925857153232794, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.937774], dtype=float32), -1.0849065]. 
=============================================
[2019-04-04 08:23:48,746] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.2903091e-10 3.3125327e-09 1.1271691e-22 9.7955782e-11 2.2033105e-11
 1.8955631e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:48,747] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1010
[2019-04-04 08:23:48,789] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.166666666666667, 45.66666666666666, 0.0, 0.0, 26.0, 25.50887045707706, 0.5117555441151449, 0.0, 1.0, 46558.24047541543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3964200.0000, 
sim time next is 3964800.0000, 
raw observation next is [-7.333333333333334, 46.33333333333334, 0.0, 0.0, 26.0, 25.49149766570373, 0.5083847918267971, 0.0, 1.0, 49747.09863012226], 
processed observation next is [1.0, 0.9130434782608695, 0.2594644506001847, 0.46333333333333343, 0.0, 0.0, 0.6666666666666666, 0.6242914721419774, 0.669461597275599, 0.0, 1.0, 0.23689094585772505], 
reward next is 0.7631, 
noisyNet noise sample is [array([-0.603787], dtype=float32), 1.0317966]. 
=============================================
[2019-04-04 08:23:50,183] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0583286e-11 3.5149890e-11 1.6554816e-25 8.3749865e-12 1.2537519e-12
 2.1110566e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:50,184] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9421
[2019-04-04 08:23:50,244] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 72.0, 61.00000000000001, 331.3333333333334, 26.0, 25.19161835805996, 0.3035227619881227, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3744600.0000, 
sim time next is 3745200.0000, 
raw observation next is [-4.0, 73.0, 75.0, 380.1666666666667, 26.0, 25.18966495568438, 0.3319790735082151, 1.0, 1.0, 9360.018379773128], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.73, 0.25, 0.42007366482504604, 0.6666666666666666, 0.5991387463070316, 0.610659691169405, 1.0, 1.0, 0.04457151609415775], 
reward next is 0.9554, 
noisyNet noise sample is [array([0.6432204], dtype=float32), 0.99557316]. 
=============================================
[2019-04-04 08:23:50,272] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3530550e-10 5.0723599e-11 4.8208375e-24 2.6625554e-11 3.2996386e-12
 8.7451807e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:50,272] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2119
[2019-04-04 08:23:50,284] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 0.0, 0.0, 26.0, 24.96789006163709, 0.3203569391271602, 0.0, 1.0, 23918.14426993166], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [3.833333333333333, 59.66666666666667, 0.0, 0.0, 26.0, 24.95485126446658, 0.3164377864700692, 0.0, 1.0, 32582.52425026905], 
processed observation next is [0.0, 0.8260869565217391, 0.5687903970452447, 0.5966666666666667, 0.0, 0.0, 0.6666666666666666, 0.5795709387055483, 0.6054792621566897, 0.0, 1.0, 0.15515487738223357], 
reward next is 0.8448, 
noisyNet noise sample is [array([-0.78349006], dtype=float32), 0.20155989]. 
=============================================
[2019-04-04 08:23:53,765] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.7948814e-10 6.0731736e-10 8.2374585e-23 1.6829184e-10 4.0200818e-11
 1.5263558e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:53,766] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4307
[2019-04-04 08:23:53,795] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.11805428492085, 0.3397329588196894, 0.0, 1.0, 44368.07542743854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3813000.0000, 
sim time next is 3813600.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.06428128864625, 0.3404368835370539, 0.0, 1.0, 43785.76177811476], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5886901073871874, 0.613478961179018, 0.0, 1.0, 0.20850362751483217], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.92657477], dtype=float32), -1.2685785]. 
=============================================
[2019-04-04 08:23:58,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6882804e-10 5.0624938e-10 1.9951568e-24 2.1861976e-11 1.1556611e-11
 1.6024157e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:23:58,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3934
[2019-04-04 08:23:58,254] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.42496675981354, 0.3440142848873828, 0.0, 1.0, 31800.97381037433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4240800.0000, 
sim time next is 4241400.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.40156980152739, 0.3408820982517994, 0.0, 1.0, 47443.32739439082], 
processed observation next is [0.0, 0.08695652173913043, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6167974834606159, 0.6136273660839331, 0.0, 1.0, 0.22592060663995628], 
reward next is 0.7741, 
noisyNet noise sample is [array([0.04344537], dtype=float32), 0.13915356]. 
=============================================
[2019-04-04 08:24:02,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.2834568e-10 1.0918637e-09 3.8341366e-23 1.1630717e-10 3.1764244e-11
 3.4239586e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:02,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-04 08:24:02,761] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 50.66666666666667, 0.0, 0.0, 26.0, 25.1872855399786, 0.3384532051761542, 0.0, 1.0, 39467.15835361406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4162200.0000, 
sim time next is 4162800.0000, 
raw observation next is [-3.333333333333333, 51.33333333333334, 0.0, 0.0, 26.0, 25.15531090484548, 0.3294391715060684, 0.0, 1.0, 39461.68332403724], 
processed observation next is [0.0, 0.17391304347826086, 0.37026777469990774, 0.5133333333333334, 0.0, 0.0, 0.6666666666666666, 0.5962759087371232, 0.6098130571686894, 0.0, 1.0, 0.18791277773351067], 
reward next is 0.8121, 
noisyNet noise sample is [array([-1.5754539], dtype=float32), -1.6593739]. 
=============================================
[2019-04-04 08:24:11,383] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.3283497e-12 8.2334677e-12 1.8393273e-26 1.0049883e-12 3.6537795e-13
 8.0039907e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:11,384] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6138
[2019-04-04 08:24:11,408] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.966666666666667, 74.33333333333333, 15.33333333333333, 81.83333333333331, 26.0, 25.46756387411157, 0.3600588004491689, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4347600.0000, 
sim time next is 4348200.0000, 
raw observation next is [2.983333333333333, 74.16666666666667, 30.66666666666666, 163.6666666666666, 26.0, 25.40567959673965, 0.3584968431226234, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.5452446906740537, 0.7416666666666667, 0.1022222222222222, 0.18084714548802938, 0.6666666666666666, 0.6171399663949707, 0.6194989477075411, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9808304], dtype=float32), 0.8129949]. 
=============================================
[2019-04-04 08:24:11,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3377685e-10 1.8248023e-10 3.0711933e-23 1.5453751e-11 2.1271596e-11
 5.0578123e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:11,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6503
[2019-04-04 08:24:11,854] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.53701836909476, 0.441501536019045, 0.0, 1.0, 29232.47208240464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4497600.0000, 
sim time next is 4498200.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.38923995558221, 0.43464144081572, 0.0, 1.0, 113040.3474287508], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6157699962985174, 0.6448804802719067, 0.0, 1.0, 0.5382873687083372], 
reward next is 0.4617, 
noisyNet noise sample is [array([0.18207335], dtype=float32), 1.2300303]. 
=============================================
[2019-04-04 08:24:30,003] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4870694e-10 5.4097071e-10 1.8612042e-22 9.8118833e-11 2.8767022e-11
 4.3063871e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:30,009] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2935
[2019-04-04 08:24:30,023] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 39.0, 22.0, 123.3333333333333, 26.0, 25.0459702156826, 0.3578692441888574, 0.0, 1.0, 44227.45089273881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4816200.0000, 
sim time next is 4816800.0000, 
raw observation next is [2.0, 40.0, 16.5, 92.5, 26.0, 25.01788672249758, 0.3445934054634587, 0.0, 1.0, 49036.23353625807], 
processed observation next is [0.0, 0.782608695652174, 0.518005540166205, 0.4, 0.055, 0.10220994475138122, 0.6666666666666666, 0.5848238935414649, 0.6148644684878196, 0.0, 1.0, 0.23350587398218128], 
reward next is 0.7665, 
noisyNet noise sample is [array([-0.5977774], dtype=float32), -0.06785145]. 
=============================================
[2019-04-04 08:24:34,372] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9138003e-12 4.5447444e-11 4.6889072e-24 1.1255813e-11 1.2646775e-12
 5.1518441e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:34,373] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1288
[2019-04-04 08:24:34,398] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 78.0, 0.0, 0.0, 26.0, 25.47349509111698, 0.4760660046234228, 1.0, 1.0, 19927.11585483117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4730400.0000, 
sim time next is 4731000.0000, 
raw observation next is [-0.1666666666666667, 78.0, 0.0, 0.0, 26.0, 25.46186046092103, 0.4768511406944, 1.0, 1.0, 22939.14917200238], 
processed observation next is [1.0, 0.782608695652174, 0.4579870729455217, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6218217050767526, 0.6589503802314667, 1.0, 1.0, 0.1092340436762018], 
reward next is 0.8908, 
noisyNet noise sample is [array([-0.01473759], dtype=float32), 2.3982954]. 
=============================================
[2019-04-04 08:24:34,412] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[84.24144]
 [84.67774]
 [84.64989]
 [84.78446]
 [85.07936]], R is [[84.20175934]
 [84.2648468 ]
 [84.33324432]
 [84.40096283]
 [84.55695343]].
[2019-04-04 08:24:42,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.05580635e-10 1.48683121e-10 6.30465800e-24 6.93835892e-11
 3.39751079e-12 5.96413042e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:24:42,945] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2663
[2019-04-04 08:24:42,962] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8333333333333334, 36.5, 0.0, 0.0, 26.0, 25.43935808997933, 0.3625531793944313, 0.0, 1.0, 37280.64286759164], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4918200.0000, 
sim time next is 4918800.0000, 
raw observation next is [0.6666666666666667, 37.0, 0.0, 0.0, 26.0, 25.42808167324989, 0.3662770259017269, 0.0, 1.0, 42986.36922649428], 
processed observation next is [0.0, 0.9565217391304348, 0.4810710987996307, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6190068061041574, 0.6220923419672423, 0.0, 1.0, 0.20469699631663943], 
reward next is 0.7953, 
noisyNet noise sample is [array([0.16373144], dtype=float32), 0.9909013]. 
=============================================
[2019-04-04 08:24:44,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8727245e-10 5.3340615e-10 8.5612526e-24 8.2090730e-11 1.2618037e-11
 2.1146293e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:44,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9057
[2019-04-04 08:24:44,028] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 56.16666666666666, 0.0, 0.0, 26.0, 25.49761615841962, 0.5453236702574956, 0.0, 1.0, 22945.19312011731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4662600.0000, 
sim time next is 4663200.0000, 
raw observation next is [2.0, 55.33333333333334, 0.0, 0.0, 26.0, 25.57428606875216, 0.5489898944725858, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.5533333333333335, 0.0, 0.0, 0.6666666666666666, 0.6311905057293465, 0.682996631490862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1356637], dtype=float32), 0.20266216]. 
=============================================
[2019-04-04 08:24:44,389] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.9434863e-10 5.9990085e-10 8.1723257e-23 1.4199462e-10 4.7194269e-11
 3.8247996e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:44,389] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8158
[2019-04-04 08:24:44,403] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03172519049469, 0.2369079584023087, 0.0, 1.0, 38643.64514240235], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4947000.0000, 
sim time next is 4947600.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.05870898965962, 0.2303826483631897, 0.0, 1.0, 38636.31063760172], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.5882257491383017, 0.5767942161210632, 0.0, 1.0, 0.18398243160762723], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.81439835], dtype=float32), -0.28877553]. 
=============================================
[2019-04-04 08:24:45,423] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0871075e-12 4.6846632e-12 7.3892647e-27 7.3093192e-13 1.6092530e-13
 5.0827964e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:45,424] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3361
[2019-04-04 08:24:45,451] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 161.5, 3.0, 26.0, 26.44168955088109, 0.5779535498566761, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4701600.0000, 
sim time next is 4702200.0000, 
raw observation next is [0.0, 92.0, 177.0, 4.0, 26.0, 26.42766164001271, 0.5802062916100533, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.59, 0.004419889502762431, 0.6666666666666666, 0.7023051366677256, 0.6934020972033511, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97077423], dtype=float32), -2.8813977]. 
=============================================
[2019-04-04 08:24:46,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:46,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:46,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run20
[2019-04-04 08:24:47,944] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2700626e-12 1.2691219e-11 4.3003687e-25 1.6574830e-12 6.9801104e-13
 1.5876842e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:24:47,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8532
[2019-04-04 08:24:47,963] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 78.0, 0.0, 0.0, 26.0, 25.29691304876611, 0.4383573089699069, 1.0, 1.0, 25752.05802064527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4732200.0000, 
sim time next is 4732800.0000, 
raw observation next is [-0.6666666666666666, 78.0, 0.0, 0.0, 26.0, 25.17633014363118, 0.4193736174476752, 1.0, 1.0, 58459.89135443052], 
processed observation next is [1.0, 0.782608695652174, 0.44413665743305636, 0.78, 0.0, 0.0, 0.6666666666666666, 0.598027511969265, 0.6397912058158918, 1.0, 1.0, 0.2783804350210977], 
reward next is 0.7216, 
noisyNet noise sample is [array([-0.39002126], dtype=float32), -0.3808444]. 
=============================================
[2019-04-04 08:24:53,466] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:53,466] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:53,506] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run20
[2019-04-04 08:24:53,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:53,531] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:53,566] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run20
[2019-04-04 08:24:53,803] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:53,804] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:53,824] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run20
[2019-04-04 08:24:54,339] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:54,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:54,344] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run20
[2019-04-04 08:24:54,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:54,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:54,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run20
[2019-04-04 08:24:54,543] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:54,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:54,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run20
[2019-04-04 08:24:54,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:54,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:54,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run20
[2019-04-04 08:24:56,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:56,123] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:56,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run20
[2019-04-04 08:24:56,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:24:56,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:24:56,426] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run20
[2019-04-04 08:25:00,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:25:00,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:25:00,677] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run20
[2019-04-04 08:25:03,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1726762e-10 2.4174884e-10 5.3834519e-24 3.1101306e-11 8.3113811e-12
 2.3119823e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:03,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3254
[2019-04-04 08:25:03,608] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 21.29541288100414, -0.5541443627601895, 0.0, 1.0, 40272.25919150066], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 24600.0000, 
sim time next is 25200.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 21.31438586249307, -0.5485331347517629, 0.0, 1.0, 40271.59813724393], 
processed observation next is [0.0, 0.30434782608695654, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2761988218744224, 0.31715562174941236, 0.0, 1.0, 0.19176951493925679], 
reward next is 0.8082, 
noisyNet noise sample is [array([-0.3231081], dtype=float32), 0.044743143]. 
=============================================
[2019-04-04 08:25:03,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:25:03,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:25:03,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run20
[2019-04-04 08:25:10,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:25:10,103] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:25:10,115] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run20
[2019-04-04 08:25:11,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:25:11,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:25:11,124] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run20
[2019-04-04 08:25:11,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:25:11,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:25:11,654] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run20
[2019-04-04 08:25:12,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:25:12,524] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:25:12,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run20
[2019-04-04 08:25:16,481] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8673460e-10 9.2728036e-10 2.6337857e-22 3.4581844e-11 1.2422246e-11
 1.8961488e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:16,482] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6269
[2019-04-04 08:25:16,551] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 65.33333333333334, 0.0, 0.0, 26.0, 25.16535250428964, 0.3231470301526972, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 156000.0000, 
sim time next is 156600.0000, 
raw observation next is [-8.1, 66.0, 0.0, 0.0, 26.0, 25.1238693376273, 0.3019215855228873, 1.0, 1.0, 34609.96962800329], 
processed observation next is [1.0, 0.8260869565217391, 0.23822714681440446, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5936557781356084, 0.6006405285076291, 1.0, 1.0, 0.16480937918096802], 
reward next is 0.8352, 
noisyNet noise sample is [array([-1.1297243], dtype=float32), -1.7885767]. 
=============================================
[2019-04-04 08:25:17,089] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3611965e-09 1.2376732e-09 2.0877843e-22 1.2874350e-10 8.3998350e-11
 1.3077353e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:17,089] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5583
[2019-04-04 08:25:17,136] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.1, 87.5, 0.0, 0.0, 26.0, 24.60387685203898, 0.2060029493106725, 0.0, 1.0, 38881.28373722572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 66600.0000, 
sim time next is 67200.0000, 
raw observation next is [4.0, 87.0, 0.0, 0.0, 26.0, 24.60695077406228, 0.207017534964379, 0.0, 1.0, 40628.10301614374], 
processed observation next is [0.0, 0.782608695652174, 0.5734072022160666, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5505792311718567, 0.5690058449881263, 0.0, 1.0, 0.1934671572197321], 
reward next is 0.8065, 
noisyNet noise sample is [array([1.0283656], dtype=float32), -0.2797067]. 
=============================================
[2019-04-04 08:25:19,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.5523116e-10 3.1813752e-09 4.2496747e-22 1.1711770e-10 4.4887646e-11
 5.1080928e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:19,452] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-04 08:25:19,485] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 74.0, 0.0, 0.0, 26.0, 23.72958116474457, -0.001227074442508682, 0.0, 1.0, 44091.23331460002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 104400.0000, 
sim time next is 105000.0000, 
raw observation next is [-5.283333333333333, 74.16666666666667, 0.0, 0.0, 26.0, 23.64676150861733, -0.01660814366891405, 0.0, 1.0, 44214.39083621311], 
processed observation next is [1.0, 0.21739130434782608, 0.31625115420129274, 0.7416666666666667, 0.0, 0.0, 0.6666666666666666, 0.47056345905144426, 0.494463952110362, 0.0, 1.0, 0.21054471826768148], 
reward next is 0.7895, 
noisyNet noise sample is [array([-0.8277605], dtype=float32), 0.5752811]. 
=============================================
[2019-04-04 08:25:19,498] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[74.0687  ]
 [74.32125 ]
 [74.551056]
 [74.76087 ]
 [74.94849 ]], R is [[73.84992218]
 [73.90146637]
 [73.95301056]
 [74.00457001]
 [74.05610657]].
[2019-04-04 08:25:21,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0553447e-11 2.3133873e-11 3.8226831e-25 3.1963204e-12 5.6441868e-13
 1.0095752e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:21,676] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-04 08:25:21,723] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 86.0, 187.0, 24.5, 26.0, 25.32297745251308, 0.3035808216663944, 1.0, 1.0, 41655.99005941557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 126000.0000, 
sim time next is 126600.0000, 
raw observation next is [-7.9, 81.83333333333334, 186.0, 20.66666666666666, 26.0, 25.34419145045703, 0.3109378772440958, 1.0, 1.0, 44145.7819909243], 
processed observation next is [1.0, 0.4782608695652174, 0.24376731301939059, 0.8183333333333335, 0.62, 0.022836095764272552, 0.6666666666666666, 0.6120159542047524, 0.6036459590813653, 1.0, 1.0, 0.21021800948059188], 
reward next is 0.7898, 
noisyNet noise sample is [array([1.2352225], dtype=float32), -1.4227804]. 
=============================================
[2019-04-04 08:25:27,480] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1798342e-09 1.7622694e-08 3.7171283e-20 8.9005070e-10 3.4191361e-10
 4.2897149e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:27,481] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8233
[2019-04-04 08:25:27,498] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.56666666666667, 78.0, 0.0, 0.0, 26.0, 24.11662308682562, 0.08772625240769967, 0.0, 1.0, 47089.03958747377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 339600.0000, 
sim time next is 340200.0000, 
raw observation next is [-13.65, 76.0, 0.0, 0.0, 26.0, 24.08740137321709, 0.07844651468096826, 0.0, 1.0, 47073.89209964567], 
processed observation next is [1.0, 0.9565217391304348, 0.08448753462603877, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5072834477680909, 0.5261488382269894, 0.0, 1.0, 0.22416139095069368], 
reward next is 0.7758, 
noisyNet noise sample is [array([-0.36101717], dtype=float32), -0.43664494]. 
=============================================
[2019-04-04 08:25:36,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4367053e-11 7.2916562e-11 1.0258406e-23 9.6201181e-12 2.7300430e-13
 1.8744182e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:36,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7503
[2019-04-04 08:25:36,199] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 44.0, 88.5, 627.0, 26.0, 26.43111346035603, 0.5366709230768933, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 309600.0000, 
sim time next is 310200.0000, 
raw observation next is [-9.5, 43.66666666666667, 86.33333333333333, 625.6666666666666, 26.0, 26.44084789944734, 0.5277327456960473, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1994459833795014, 0.4366666666666667, 0.28777777777777774, 0.6913443830570902, 0.6666666666666666, 0.7034039916206117, 0.6759109152320159, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4350406], dtype=float32), -0.68697035]. 
=============================================
[2019-04-04 08:25:37,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4867144e-09 2.1989139e-08 6.9730466e-20 2.1063211e-09 5.9088417e-10
 1.4209967e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:37,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9018
[2019-04-04 08:25:38,010] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.0801167179802, -0.4147324806918082, 0.0, 1.0, 48610.76102454025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.9875273075273, -0.4363105681234463, 0.0, 1.0, 48660.89965763258], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 0.6666666666666666, 0.33229394229394177, 0.35456314395885125, 0.0, 1.0, 0.2317185697982504], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.67222375], dtype=float32), 1.9762988]. 
=============================================
[2019-04-04 08:25:41,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.2247263e-11 3.1719335e-10 2.4490372e-22 3.2708850e-11 6.5949139e-12
 6.3379674e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:41,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4223
[2019-04-04 08:25:41,801] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.75, 41.0, 0.0, 0.0, 26.0, 25.08810795553453, 0.2949925818132944, 1.0, 1.0, 87610.09038443935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 415800.0000, 
sim time next is 416400.0000, 
raw observation next is [-9.833333333333332, 41.33333333333334, 0.0, 0.0, 26.0, 25.08472063433556, 0.2978821066783279, 1.0, 1.0, 73203.56090163538], 
processed observation next is [1.0, 0.8260869565217391, 0.19021237303785785, 0.41333333333333344, 0.0, 0.0, 0.6666666666666666, 0.5903933861946301, 0.5992940355594426, 1.0, 1.0, 0.34858838524588276], 
reward next is 0.6514, 
noisyNet noise sample is [array([0.81979644], dtype=float32), -2.8360152]. 
=============================================
[2019-04-04 08:25:41,906] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1089777e-09 4.7944835e-09 2.2192346e-20 5.1162519e-10 1.7933718e-10
 6.3007898e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:41,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0439
[2019-04-04 08:25:41,931] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 48.0, 0.0, 0.0, 26.0, 24.51736123477623, 0.1531053344212332, 0.0, 1.0, 45631.53209897776], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 423000.0000, 
sim time next is 423600.0000, 
raw observation next is [-10.6, 48.33333333333333, 0.0, 0.0, 26.0, 24.45264075790766, 0.1410772028426763, 0.0, 1.0, 45160.04931476731], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.4833333333333333, 0.0, 0.0, 0.6666666666666666, 0.5377200631589716, 0.5470257342808921, 0.0, 1.0, 0.21504785387984435], 
reward next is 0.7850, 
noisyNet noise sample is [array([-1.0561539], dtype=float32), 1.156152]. 
=============================================
[2019-04-04 08:25:52,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6750459e-12 2.4982641e-11 1.1216728e-25 2.3367927e-12 1.6627636e-13
 6.9736191e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:52,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-04 08:25:52,900] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 84.0, 0.0, 0.0, 26.0, 24.78408258833844, 0.1525679066374762, 1.0, 1.0, 182151.3645860684], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 496800.0000, 
sim time next is 497400.0000, 
raw observation next is [0.6000000000000001, 86.0, 0.0, 0.0, 26.0, 24.41755777659319, 0.185242059864534, 1.0, 1.0, 199595.3142240786], 
processed observation next is [1.0, 0.782608695652174, 0.479224376731302, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5347964813827657, 0.5617473532881779, 1.0, 1.0, 0.9504538772575172], 
reward next is 0.0495, 
noisyNet noise sample is [array([2.801477], dtype=float32), 0.5698684]. 
=============================================
[2019-04-04 08:25:54,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.2931637e-09 2.5565212e-09 4.8332745e-21 6.1192773e-10 1.0346745e-10
 1.0161509e-11 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:54,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3442
[2019-04-04 08:25:54,500] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.3, 71.0, 0.0, 0.0, 26.0, 22.58898839123795, -0.2734914628906601, 0.0, 1.0, 49324.6621694674], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 358200.0000, 
sim time next is 358800.0000, 
raw observation next is [-15.4, 71.66666666666667, 0.0, 0.0, 26.0, 22.58677630738381, -0.2891645997894627, 0.0, 1.0, 49325.2550875931], 
processed observation next is [1.0, 0.13043478260869565, 0.03601108033240995, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.38223135894865096, 0.4036118000701791, 0.0, 1.0, 0.23488216708377666], 
reward next is 0.7651, 
noisyNet noise sample is [array([-0.72055113], dtype=float32), 0.48490682]. 
=============================================
[2019-04-04 08:25:57,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4454813e-10 1.2285288e-09 6.7521621e-22 8.9208127e-11 3.0724895e-11
 2.8189343e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:25:57,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1414
[2019-04-04 08:25:57,596] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.36666666666667, 79.0, 0.0, 0.0, 26.0, 21.49329037909963, -0.4405880206216344, 1.0, 1.0, 202242.6109366239], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 372000.0000, 
sim time next is 372600.0000, 
raw observation next is [-16.45, 79.5, 0.0, 0.0, 26.0, 21.92792010392076, -0.3418148870755788, 1.0, 1.0, 203358.3625419095], 
processed observation next is [1.0, 0.30434782608695654, 0.006925207756232688, 0.795, 0.0, 0.0, 0.6666666666666666, 0.3273266753267299, 0.3860617043081404, 1.0, 1.0, 0.9683731549614738], 
reward next is 0.0316, 
noisyNet noise sample is [array([0.17860855], dtype=float32), -1.4474032]. 
=============================================
[2019-04-04 08:26:08,023] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0192249e-10 2.5259694e-10 8.2592954e-23 8.9117311e-11 3.8326467e-11
 4.7754877e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:26:08,023] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0401
[2019-04-04 08:26:08,134] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.00000000000001, 97.83333333333333, 62.16666666666667, 26.0, 24.90291658099656, 0.2274834089734812, 0.0, 1.0, 41806.55729296053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 656400.0000, 
sim time next is 657000.0000, 
raw observation next is [-0.8999999999999999, 57.0, 81.0, 56.0, 26.0, 24.88794578405737, 0.2247426428662101, 0.0, 1.0, 45935.80554375175], 
processed observation next is [0.0, 0.6086956521739131, 0.43767313019390586, 0.57, 0.27, 0.061878453038674036, 0.6666666666666666, 0.5739954820047807, 0.5749142142887367, 0.0, 1.0, 0.2187419311607226], 
reward next is 0.7813, 
noisyNet noise sample is [array([-0.28305674], dtype=float32), 0.031371705]. 
=============================================
[2019-04-04 08:26:08,188] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.93944]
 [78.41264]
 [79.09247]
 [79.8208 ]
 [80.56996]], R is [[77.62269592]
 [77.64738464]
 [77.70691681]
 [77.7539444 ]
 [77.85105896]].
[2019-04-04 08:26:13,885] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 08:26:13,901] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:26:13,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:13,904] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run27
[2019-04-04 08:26:13,979] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:26:13,980] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:13,982] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run27
[2019-04-04 08:26:14,014] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:26:14,015] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:26:14,017] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run27
[2019-04-04 08:27:05,872] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.21764468], dtype=float32), -0.1511576]
[2019-04-04 08:27:05,872] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.033333333333334, 37.66666666666667, 92.16666666666667, 425.8333333333334, 26.0, 25.73938061280563, 0.2914581938277584, 1.0, 1.0, 0.0]
[2019-04-04 08:27:05,873] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:27:05,873] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.2207470e-11 2.7312071e-11 2.9602591e-24 5.5271196e-12 5.6679748e-13
 5.6191535e-15 1.0000000e+00], sampled 0.6154277680190879
[2019-04-04 08:27:20,702] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21764468], dtype=float32), -0.1511576]
[2019-04-04 08:27:20,703] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.979573284333333, 94.91965057666667, 0.0, 0.0, 26.0, 25.33875483163963, 0.4116259280023975, 0.0, 1.0, 36923.36716043409]
[2019-04-04 08:27:20,703] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:27:20,704] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7938808e-11 2.5464061e-11 6.0723597e-26 2.8423674e-12 1.2419255e-12
 1.8399762e-15 1.0000000e+00], sampled 0.6111143123136329
[2019-04-04 08:28:37,010] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21764468], dtype=float32), -0.1511576]
[2019-04-04 08:28:37,010] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-6.243810494333333, 61.674569495, 207.7215319666667, 840.6031025333334, 26.0, 25.80732160178686, 0.4230917561476824, 1.0, 1.0, 0.0]
[2019-04-04 08:28:37,010] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:28:37,011] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7049253e-12 4.5198194e-12 5.7244437e-26 6.4097214e-13 7.9182673e-14
 4.3112458e-16 1.0000000e+00], sampled 0.20419824272608067
[2019-04-04 08:29:30,283] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:30:00,082] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.21764468], dtype=float32), -0.1511576]
[2019-04-04 08:30:00,082] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-3.056220023833333, 57.90821792333334, 0.0, 0.0, 26.0, 24.83014504039728, 0.2356390851965277, 0.0, 1.0, 38859.1571028766]
[2019-04-04 08:30:00,082] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:30:00,083] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.5379817e-10 4.5069715e-10 1.8322642e-23 7.7125778e-11 1.8707993e-11
 7.3424312e-14 1.0000000e+00], sampled 0.05069811823725623
[2019-04-04 08:30:01,581] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:30:07,150] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:30:08,188] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 2600000, evaluation results [2600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:30:08,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.04953796e-10 1.21859356e-10 1.26415670e-23 1.17202550e-11
 2.57250254e-12 6.03925818e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:30:08,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7969
[2019-04-04 08:30:08,225] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 70.33333333333333, 0.0, 0.0, 26.0, 24.71867761717925, 0.1558110137431093, 0.0, 1.0, 41699.50157116403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 686400.0000, 
sim time next is 687000.0000, 
raw observation next is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 26.0, 24.68520167067255, 0.1480111871376663, 0.0, 1.0, 41630.89268653015], 
processed observation next is [0.0, 0.9565217391304348, 0.3568790397045245, 0.7066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5571001392227126, 0.5493370623792221, 0.0, 1.0, 0.19824234612633404], 
reward next is 0.8018, 
noisyNet noise sample is [array([-1.5431019], dtype=float32), 0.1994561]. 
=============================================
[2019-04-04 08:30:08,261] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.08305 ]
 [80.13325 ]
 [80.19424 ]
 [80.239944]
 [80.257286]], R is [[80.02662659]
 [80.02779388]
 [80.02855682]
 [80.02899933]
 [80.02916718]].
[2019-04-04 08:30:22,625] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.2922543e-11 2.6905067e-10 8.4361464e-24 2.7302176e-11 3.9206529e-12
 3.1038560e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:30:22,626] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6634
[2019-04-04 08:30:22,722] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.7, 61.0, 100.5, 69.0, 26.0, 24.87410637072929, 0.2233090754189258, 0.0, 1.0, 32013.0309343754], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 648000.0000, 
sim time next is 648600.0000, 
raw observation next is [-2.633333333333334, 60.66666666666667, 104.3333333333333, 79.33333333333334, 26.0, 24.88542202668259, 0.2246630112471559, 0.0, 1.0, 31844.54262865201], 
processed observation next is [0.0, 0.5217391304347826, 0.3896583564173592, 0.6066666666666667, 0.3477777777777777, 0.08766114180478822, 0.6666666666666666, 0.5737851688902159, 0.5748876704157186, 0.0, 1.0, 0.1516406791840572], 
reward next is 0.8484, 
noisyNet noise sample is [array([-0.04957649], dtype=float32), -0.41829]. 
=============================================
[2019-04-04 08:30:47,636] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8192683e-12 5.8119043e-12 1.2068994e-27 8.7269108e-13 1.3316744e-13
 6.4605718e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:30:47,636] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6352
[2019-04-04 08:30:47,669] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 26.10142924071922, 0.6387332914151563, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 26.06520965562358, 0.6319593920389058, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6721008046352983, 0.7106531306796353, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24249946], dtype=float32), -0.21735321]. 
=============================================
[2019-04-04 08:30:51,585] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8961620e-11 1.6057111e-10 2.3537127e-25 1.2337971e-11 7.6267924e-12
 1.0068291e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:30:51,587] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5675
[2019-04-04 08:30:51,602] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.52043131390391, 0.1619086637764659, 0.0, 1.0, 38492.44582292824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 888600.0000, 
sim time next is 889200.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.56034095165916, 0.1637790204283981, 0.0, 1.0, 38441.39417127411], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.54669507930493, 0.554593006809466, 0.0, 1.0, 0.18305425795844812], 
reward next is 0.8169, 
noisyNet noise sample is [array([-0.847295], dtype=float32), -1.8531089]. 
=============================================
[2019-04-04 08:31:02,473] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5985256e-12 2.1832544e-12 4.0851762e-28 2.7168746e-13 1.5337743e-13
 1.6284310e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:02,473] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8164
[2019-04-04 08:31:02,483] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 25.93643353753733, 0.6450209979118873, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1048800.0000, 
sim time next is 1049400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 26.0234214506067, 0.6374973555410443, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.13043478260869565, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6686184542172251, 0.7124991185136814, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.03585996], dtype=float32), 0.23260133]. 
=============================================
[2019-04-04 08:31:05,156] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7082034e-13 1.1331826e-12 1.8255767e-27 2.1350227e-13 3.8492121e-14
 5.5422544e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:05,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5843
[2019-04-04 08:31:05,187] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.88333333333334, 63.5, 205.3333333333333, 283.0, 26.0, 27.04366161772251, 0.866321733271692, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1080600.0000, 
sim time next is 1081200.0000, 
raw observation next is [17.16666666666667, 62.0, 193.1666666666667, 300.0, 26.0, 26.52854074854273, 0.8631603328145956, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.9381348107109884, 0.62, 0.6438888888888891, 0.3314917127071823, 0.6666666666666666, 0.7107117290452276, 0.7877201109381985, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9364643], dtype=float32), -0.36609125]. 
=============================================
[2019-04-04 08:31:06,097] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3481951e-12 1.9018918e-11 1.0105400e-26 6.3735383e-13 1.5955423e-13
 1.5283345e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:06,098] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8124
[2019-04-04 08:31:06,117] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.9, 63.33333333333333, 0.0, 0.0, 26.0, 25.86888738047314, 0.69235391897169, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1114800.0000, 
sim time next is 1115400.0000, 
raw observation next is [12.8, 63.66666666666666, 0.0, 0.0, 26.0, 25.85807146590832, 0.684409044622568, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8171745152354571, 0.6366666666666666, 0.0, 0.0, 0.6666666666666666, 0.6548392888256934, 0.7281363482075226, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9023209], dtype=float32), -0.16596954]. 
=============================================
[2019-04-04 08:31:12,924] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3197969e-10 6.0822361e-11 1.0656759e-24 4.9941175e-12 2.3081237e-12
 1.4931371e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:12,925] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5964
[2019-04-04 08:31:12,937] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34033045662806, 0.4491230556780998, 0.0, 1.0, 36924.68176384443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1486800.0000, 
sim time next is 1487400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.3180730559848, 0.4423840637169486, 0.0, 1.0, 36945.80179024974], 
processed observation next is [1.0, 0.21739130434782608, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6098394213320667, 0.6474613545723162, 0.0, 1.0, 0.1759323894773797], 
reward next is 0.8241, 
noisyNet noise sample is [array([2.7917101], dtype=float32), -0.07707343]. 
=============================================
[2019-04-04 08:31:17,290] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.2185062e-12 6.3911563e-12 3.4120983e-26 1.6669617e-13 4.5218153e-14
 1.0749695e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:17,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9465
[2019-04-04 08:31:17,312] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 12.0, 0.0, 26.0, 25.85200607246324, 0.5032279217539041, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1443000.0000, 
sim time next is 1443600.0000, 
raw observation next is [1.1, 92.0, 9.0, 0.0, 26.0, 25.89448892359356, 0.5067921796671271, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.92, 0.03, 0.0, 0.6666666666666666, 0.65787407696613, 0.6689307265557091, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.289739], dtype=float32), -0.36286893]. 
=============================================
[2019-04-04 08:31:21,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8892075e-12 8.7493823e-12 4.2437370e-26 7.2463948e-13 7.9950928e-13
 1.4895638e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:21,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5764
[2019-04-04 08:31:21,616] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.70210624114095, 0.558301611031617, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1657800.0000, 
sim time next is 1658400.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.66918564510176, 0.54079862726254, 0.0, 1.0, 23016.3443362461], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6390988037584799, 0.6802662090875135, 0.0, 1.0, 0.10960163969641], 
reward next is 0.8904, 
noisyNet noise sample is [array([-1.7330163], dtype=float32), -0.98502374]. 
=============================================
[2019-04-04 08:31:24,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4926107e-13 1.1920995e-12 2.6158871e-26 4.4505588e-13 6.6597911e-14
 1.3216646e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:24,554] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4879
[2019-04-04 08:31:24,570] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.85, 92.0, 53.0, 0.0, 26.0, 25.88313748051812, 0.5534475131535236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1675800.0000, 
sim time next is 1676400.0000, 
raw observation next is [1.733333333333333, 92.0, 55.16666666666667, 0.0, 26.0, 25.92848307028386, 0.5577596359608527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.5106186518928902, 0.92, 0.1838888888888889, 0.0, 0.6666666666666666, 0.660706922523655, 0.6859198786536176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1044674], dtype=float32), 0.942706]. 
=============================================
[2019-04-04 08:31:31,264] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.6688009e-11 1.4861422e-10 1.7345600e-24 5.9783962e-12 4.5361163e-12
 4.6728406e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:31,265] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2333
[2019-04-04 08:31:31,276] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 86.0, 0.0, 0.0, 26.0, 25.60156706323648, 0.52570406149953, 0.0, 1.0, 18736.71446081423], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1566000.0000, 
sim time next is 1566600.0000, 
raw observation next is [4.433333333333334, 85.83333333333334, 0.0, 0.0, 26.0, 25.58568150662383, 0.5271026115901148, 0.0, 1.0, 24102.24318860862], 
processed observation next is [1.0, 0.13043478260869565, 0.5854108956602032, 0.8583333333333334, 0.0, 0.0, 0.6666666666666666, 0.6321401255519857, 0.6757008705300382, 0.0, 1.0, 0.114772586612422], 
reward next is 0.8852, 
noisyNet noise sample is [array([-2.3257732], dtype=float32), -0.6258776]. 
=============================================
[2019-04-04 08:31:32,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3675775e-10 7.5524947e-10 7.3650212e-23 6.4329861e-11 7.6768848e-11
 4.1943077e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:32,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4343
[2019-04-04 08:31:32,513] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 87.0, 108.0, 0.0, 26.0, 24.89826071986597, 0.3356726442632931, 0.0, 1.0, 63799.67676843543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1767600.0000, 
sim time next is 1768200.0000, 
raw observation next is [-2.3, 86.33333333333333, 111.6666666666667, 0.0, 26.0, 24.89496021420619, 0.3414904536698115, 0.0, 1.0, 55561.09386660512], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8633333333333333, 0.37222222222222234, 0.0, 0.6666666666666666, 0.5745800178505158, 0.6138301512232706, 0.0, 1.0, 0.2645766374600244], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.2061544], dtype=float32), -0.591377]. 
=============================================
[2019-04-04 08:31:43,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1217403e-10 9.3879082e-10 3.3885514e-21 2.0795463e-10 7.6986008e-11
 1.1089421e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:43,212] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7012
[2019-04-04 08:31:43,338] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333333, 85.0, 0.0, 0.0, 26.0, 25.03765559609259, 0.2482291340162096, 0.0, 1.0, 38938.4311840258], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1878000.0000, 
sim time next is 1878600.0000, 
raw observation next is [-4.916666666666667, 85.5, 0.0, 0.0, 26.0, 25.02548023587752, 0.2454515312722119, 0.0, 1.0, 50271.40685456345], 
processed observation next is [0.0, 0.7391304347826086, 0.32640812557710064, 0.855, 0.0, 0.0, 0.6666666666666666, 0.5854566863231266, 0.5818171770907373, 0.0, 1.0, 0.23938765168839737], 
reward next is 0.7606, 
noisyNet noise sample is [array([-0.6188599], dtype=float32), -0.36096442]. 
=============================================
[2019-04-04 08:31:46,365] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.1249364e-09 4.8995670e-09 2.8318223e-22 2.3459856e-10 1.1969080e-10
 2.9286908e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:31:46,365] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6425
[2019-04-04 08:31:46,463] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01340676877817, 0.3192166867906842, 0.0, 1.0, 121177.5370829508], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1797600.0000, 
sim time next is 1798200.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01102720880633, 0.332258723262512, 0.0, 1.0, 79134.87633459154], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5842522674005274, 0.6107529077541707, 0.0, 1.0, 0.3768327444504359], 
reward next is 0.6232, 
noisyNet noise sample is [array([-0.4353335], dtype=float32), 1.324339]. 
=============================================
[2019-04-04 08:32:04,268] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.01787155e-11 3.01816419e-11 9.41529058e-25 8.14586853e-12
 9.27296651e-13 1.52004403e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:32:04,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5512
[2019-04-04 08:32:04,340] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.266666666666667, 76.33333333333334, 9.166666666666666, 1.666666666666667, 26.0, 25.62245270775747, 0.3501949696072097, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1963200.0000, 
sim time next is 1963800.0000, 
raw observation next is [-4.45, 77.0, 0.0, 0.0, 26.0, 25.70739527823449, 0.3211616721293153, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3393351800554017, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6422829398528741, 0.6070538907097718, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6995349], dtype=float32), -0.032891553]. 
=============================================
[2019-04-04 08:32:07,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8693413e-10 3.2759129e-10 2.0086671e-23 2.9686358e-11 7.4079909e-12
 6.4045855e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:07,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5602
[2019-04-04 08:32:07,634] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 83.66666666666667, 0.0, 0.0, 26.0, 24.1813748647816, 0.07387266318233186, 0.0, 1.0, 41282.91976597568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2002200.0000, 
sim time next is 2002800.0000, 
raw observation next is [-5.8, 84.33333333333334, 0.0, 0.0, 26.0, 24.10572648219625, 0.08828307139544128, 0.0, 1.0, 41600.01590608752], 
processed observation next is [1.0, 0.17391304347826086, 0.30193905817174516, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5088105401830209, 0.529427690465147, 0.0, 1.0, 0.198095313838512], 
reward next is 0.8019, 
noisyNet noise sample is [array([1.2968059], dtype=float32), 1.4472861]. 
=============================================
[2019-04-04 08:32:09,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6702724e-12 1.3639447e-11 1.2087160e-25 7.5638139e-13 3.7176642e-14
 2.4574485e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:09,861] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7274
[2019-04-04 08:32:09,897] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 70.0, 124.0, 0.0, 26.0, 26.26037114906001, 0.5044944589010901, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2211600.0000, 
sim time next is 2212200.0000, 
raw observation next is [-3.9, 69.5, 120.0, 0.0, 26.0, 26.36564049821516, 0.5083436515375371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.695, 0.4, 0.0, 0.6666666666666666, 0.6971367081845967, 0.6694478838458457, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5396191], dtype=float32), 2.2259417]. 
=============================================
[2019-04-04 08:32:21,788] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2038096e-12 1.3882621e-11 3.2011924e-25 1.2081823e-12 1.6789531e-13
 1.7483857e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:21,788] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2842
[2019-04-04 08:32:21,843] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666668, 47.0, 204.3333333333333, 67.5, 26.0, 24.69597617636885, 0.3475088337999195, 1.0, 1.0, 198531.0323868092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [-0.7833333333333332, 46.0, 187.6666666666667, 66.0, 26.0, 25.27813311453504, 0.4151256246957483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.44090489381348114, 0.46, 0.6255555555555558, 0.07292817679558011, 0.6666666666666666, 0.6065110928779202, 0.6383752082319161, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2493842], dtype=float32), 1.3789474]. 
=============================================
[2019-04-04 08:32:22,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1332981e-11 1.9842261e-10 5.5941472e-24 1.0846600e-11 3.8677325e-12
 5.1647844e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:22,136] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7025
[2019-04-04 08:32:22,227] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.59915514780625, 0.09175488005604204, 1.0, 1.0, 202977.48468661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2273400.0000, 
sim time next is 2274000.0000, 
raw observation next is [-9.5, 91.0, 9.999999999999998, 19.33333333333334, 26.0, 24.40465840529729, 0.1884000092838827, 1.0, 1.0, 152803.0860635954], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.033333333333333326, 0.021362799263351755, 0.6666666666666666, 0.5337215337747742, 0.5628000030946275, 1.0, 1.0, 0.7276337431599781], 
reward next is 0.2724, 
noisyNet noise sample is [array([1.1828256], dtype=float32), 1.4371394]. 
=============================================
[2019-04-04 08:32:22,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.28859]
 [78.62634]
 [72.8935 ]
 [73.00304]
 [73.10225]], R is [[83.95831299]
 [83.15216827]
 [82.35688019]
 [82.32845306]
 [82.30020905]].
[2019-04-04 08:32:23,309] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4184249e-10 2.3745417e-09 6.2528574e-22 1.9802336e-10 5.0724369e-11
 5.6180846e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:23,309] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5474
[2019-04-04 08:32:23,323] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 91.0, 0.0, 0.0, 26.0, 23.53296136701925, -0.06176326669961821, 0.0, 1.0, 43108.37511783088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2270400.0000, 
sim time next is 2271000.0000, 
raw observation next is [-9.4, 91.0, 0.0, 0.0, 26.0, 23.47436865908049, -0.07519735986041629, 0.0, 1.0, 43071.84505978855], 
processed observation next is [1.0, 0.2608695652173913, 0.20221606648199447, 0.91, 0.0, 0.0, 0.6666666666666666, 0.45619738825670747, 0.47493421337986125, 0.0, 1.0, 0.2051040240942312], 
reward next is 0.7949, 
noisyNet noise sample is [array([2.0955925], dtype=float32), -2.5627635]. 
=============================================
[2019-04-04 08:32:23,352] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[73.82818 ]
 [73.891   ]
 [73.954834]
 [74.006134]
 [74.0531  ]], R is [[73.80667114]
 [73.86332703]
 [73.91925049]
 [73.97454834]
 [74.0291748 ]].
[2019-04-04 08:32:28,656] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.4715074e-09 3.8750634e-09 7.8872180e-21 7.4073059e-10 3.0048394e-10
 6.3602530e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:28,656] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4384
[2019-04-04 08:32:28,682] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.466666666666667, 53.66666666666667, 0.0, 0.0, 26.0, 24.02108863823883, 0.02710066946084226, 0.0, 1.0, 43605.94557244203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2427600.0000, 
sim time next is 2428200.0000, 
raw observation next is [-7.55, 54.0, 0.0, 0.0, 26.0, 23.99200686391418, 0.01791843072019168, 0.0, 1.0, 43644.39461827743], 
processed observation next is [0.0, 0.08695652173913043, 0.25346260387811637, 0.54, 0.0, 0.0, 0.6666666666666666, 0.49933390532618177, 0.5059728102400639, 0.0, 1.0, 0.20783045056322585], 
reward next is 0.7922, 
noisyNet noise sample is [array([2.9767532], dtype=float32), -1.4163306]. 
=============================================
[2019-04-04 08:32:40,122] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4134067e-10 2.4429500e-10 2.4500918e-23 5.9204586e-11 1.7030670e-11
 1.2700208e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:40,122] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8089
[2019-04-04 08:32:40,173] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.016666666666667, 35.16666666666666, 82.66666666666667, 811.6666666666667, 26.0, 24.98855079778854, 0.2499331062512562, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2459400.0000, 
sim time next is 2460000.0000, 
raw observation next is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 26.0, 24.98140870641038, 0.2498453329389426, 0.0, 1.0, 18734.07632168736], 
processed observation next is [0.0, 0.4782608695652174, 0.41458910433979695, 0.34333333333333343, 0.28111111111111114, 0.9064456721915286, 0.6666666666666666, 0.5817840588675317, 0.5832817776463142, 0.0, 1.0, 0.08920988724613028], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.25159922], dtype=float32), -0.20141897]. 
=============================================
[2019-04-04 08:32:40,184] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.995575]
 [81.94349 ]
 [81.84079 ]
 [81.6844  ]
 [81.51019 ]], R is [[82.16197968]
 [82.34036255]
 [82.42771149]
 [82.41170502]
 [82.39117432]].
[2019-04-04 08:32:46,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5214701e-09 4.5751722e-09 4.0414647e-21 5.5597965e-10 2.0843671e-10
 1.5069147e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:46,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7371
[2019-04-04 08:32:46,264] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.716666666666667, 54.66666666666667, 0.0, 0.0, 26.0, 23.89571443485238, -0.003121688060446525, 0.0, 1.0, 43783.77285202053], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2429400.0000, 
sim time next is 2430000.0000, 
raw observation next is [-7.8, 55.0, 0.0, 0.0, 26.0, 23.84654949357417, -0.01017070406111297, 0.0, 1.0, 43872.65677873355], 
processed observation next is [0.0, 0.13043478260869565, 0.24653739612188366, 0.55, 0.0, 0.0, 0.6666666666666666, 0.48721245779784744, 0.49660976531296236, 0.0, 1.0, 0.20891741323206453], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.11392367], dtype=float32), 1.3731775]. 
=============================================
[2019-04-04 08:32:46,279] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.02151 ]
 [74.129745]
 [74.250626]
 [74.364174]
 [74.48745 ]], R is [[73.97225952]
 [74.02404022]
 [74.07568359]
 [74.12709808]
 [74.17817688]].
[2019-04-04 08:32:48,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.89974286e-10 1.05740444e-10 5.09821618e-23 9.86380422e-11
 8.14897715e-12 3.11993507e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:32:48,281] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2277
[2019-04-04 08:32:48,308] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.8, 27.66666666666667, 87.5, 834.1666666666667, 26.0, 24.96967213800627, 0.2724280747001173, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2467200.0000, 
sim time next is 2467800.0000, 
raw observation next is [1.9, 27.5, 87.0, 832.0, 26.0, 24.96753887339835, 0.2717845567936962, 0.0, 1.0, 18709.64948181783], 
processed observation next is [0.0, 0.5652173913043478, 0.515235457063712, 0.275, 0.29, 0.9193370165745857, 0.6666666666666666, 0.5806282394498625, 0.5905948522645654, 0.0, 1.0, 0.08909356896103728], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.01744069], dtype=float32), -0.574318]. 
=============================================
[2019-04-04 08:32:50,968] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.24716620e-10 1.94383820e-09 8.54853843e-21 4.50832122e-10
 1.00239914e-10 7.17705517e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 08:32:50,969] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0929
[2019-04-04 08:32:50,993] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.5, 83.0, 0.0, 0.0, 26.0, 23.23151621535589, -0.08624045843470984, 0.0, 1.0, 43575.31323283043], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2698200.0000, 
sim time next is 2698800.0000, 
raw observation next is [-15.66666666666667, 83.0, 0.0, 0.0, 26.0, 23.17916832450962, -0.1049998825618459, 0.0, 1.0, 43489.4996356963], 
processed observation next is [1.0, 0.21739130434782608, 0.02862419205909501, 0.83, 0.0, 0.0, 0.6666666666666666, 0.4315973603758018, 0.46500003914605137, 0.0, 1.0, 0.20709285540807762], 
reward next is 0.7929, 
noisyNet noise sample is [array([0.29768], dtype=float32), 0.7337949]. 
=============================================
[2019-04-04 08:32:56,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1731320e-10 1.2566190e-10 1.8010796e-23 5.2060550e-11 2.4646743e-11
 4.0049984e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:56,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4535
[2019-04-04 08:32:56,119] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 83.0, 13.33333333333333, 54.99999999999999, 26.0, 23.95398929691405, 0.1047419193957855, 1.0, 1.0, 154108.8093087058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2706000.0000, 
sim time next is 2706600.0000, 
raw observation next is [-15.0, 83.0, 26.66666666666666, 110.0, 26.0, 24.37992594001125, 0.1695545398380665, 1.0, 1.0, 106132.9580599165], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.08888888888888886, 0.12154696132596685, 0.6666666666666666, 0.5316604950009376, 0.5565181799460222, 1.0, 1.0, 0.5053950383805548], 
reward next is 0.4946, 
noisyNet noise sample is [array([-0.8865636], dtype=float32), 1.2682295]. 
=============================================
[2019-04-04 08:32:57,208] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1936606e-10 4.0863748e-10 1.1664008e-23 7.8552206e-11 6.0715259e-11
 3.9434859e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:57,208] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3713
[2019-04-04 08:32:57,222] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 59.0, 0.0, 0.0, 26.0, 25.21197385973766, 0.354721329943388, 0.0, 1.0, 51853.24727851586], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2847000.0000, 
sim time next is 2847600.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.20929208717166, 0.3546035855707295, 0.0, 1.0, 46079.42593738483], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6007743405976385, 0.6182011951902432, 0.0, 1.0, 0.21942583779707064], 
reward next is 0.7806, 
noisyNet noise sample is [array([-1.6212184], dtype=float32), 0.67961186]. 
=============================================
[2019-04-04 08:32:59,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7725363e-10 7.7317441e-10 1.9539659e-22 7.8500081e-11 1.8162419e-11
 7.5169824e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:32:59,061] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5407
[2019-04-04 08:32:59,096] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.100000000000001, 74.66666666666667, 0.0, 0.0, 26.0, 24.74027480056822, 0.1973623970027864, 0.0, 1.0, 41916.35562082512], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2603400.0000, 
sim time next is 2604000.0000, 
raw observation next is [-5.2, 75.33333333333334, 0.0, 0.0, 26.0, 24.70459247064187, 0.1868456984455138, 0.0, 1.0, 42000.083713449], 
processed observation next is [1.0, 0.13043478260869565, 0.31855955678670367, 0.7533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5587160392201559, 0.5622818994818379, 0.0, 1.0, 0.20000039863547142], 
reward next is 0.8000, 
noisyNet noise sample is [array([-1.1989819], dtype=float32), 0.44845268]. 
=============================================
[2019-04-04 08:32:59,135] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.3846  ]
 [77.32633 ]
 [77.30181 ]
 [77.261856]
 [77.2265  ]], R is [[77.3711319 ]
 [77.39781952]
 [77.42462158]
 [77.45149231]
 [77.478302  ]].
[2019-04-04 08:33:01,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7988958e-10 1.9151405e-09 3.2826666e-22 2.5038802e-10 4.7219121e-11
 2.8625396e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:01,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5946
[2019-04-04 08:33:01,710] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.38263527071474, 0.1178270925205054, 0.0, 1.0, 41020.74741135407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2782800.0000, 
sim time next is 2783400.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 24.3254954628516, 0.1144025669363925, 0.0, 1.0, 41105.60024162859], 
processed observation next is [1.0, 0.21739130434782608, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5271246219042999, 0.5381341889787975, 0.0, 1.0, 0.19574095353156473], 
reward next is 0.8043, 
noisyNet noise sample is [array([-1.467311], dtype=float32), 1.7701229]. 
=============================================
[2019-04-04 08:33:07,459] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.70251918e-12 1.30375355e-11 2.98747403e-24 4.85138058e-12
 5.35640109e-13 1.62960012e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:33:07,459] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1606
[2019-04-04 08:33:07,511] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.66666666666667, 88.50000000000001, 91.33333333333333, 518.0, 26.0, 26.01044182237348, 0.4162407229494334, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2711400.0000, 
sim time next is 2712000.0000, 
raw observation next is [-13.33333333333333, 86.0, 94.16666666666667, 565.0, 26.0, 25.95356411513422, 0.4212913386009268, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.09325946445060027, 0.86, 0.3138888888888889, 0.6243093922651933, 0.6666666666666666, 0.6627970095945184, 0.640430446200309, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20582272], dtype=float32), 0.76540637]. 
=============================================
[2019-04-04 08:33:07,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.49629 ]
 [83.561676]
 [83.684685]
 [83.790535]
 [83.737335]], R is [[83.68133545]
 [83.84452057]
 [84.006073  ]
 [84.16601562]
 [84.32435608]].
[2019-04-04 08:33:10,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.0161329e-12 1.4639021e-11 5.8194091e-25 6.1493258e-13 7.3186097e-13
 1.9224690e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:10,933] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9618
[2019-04-04 08:33:10,994] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 89.16666666666666, 0.0, 26.0, 24.95547864635611, 0.3729836070923357, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2904000.0000, 
sim time next is 2904600.0000, 
raw observation next is [2.0, 100.0, 88.33333333333334, 0.0, 26.0, 25.44156025167431, 0.4167284059185223, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 1.0, 0.29444444444444445, 0.0, 0.6666666666666666, 0.6201300209728592, 0.6389094686395075, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14545742], dtype=float32), 0.84896934]. 
=============================================
[2019-04-04 08:33:16,696] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3899278e-10 1.0402309e-10 4.0551433e-23 3.8759759e-11 7.4768091e-12
 5.4583989e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:16,696] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3482
[2019-04-04 08:33:16,742] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.42029923985563, 0.3339692544564024, 0.0, 1.0, 30594.07848843797], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3106800.0000, 
sim time next is 3107400.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.3673781325299, 0.3279947354101889, 0.0, 1.0, 61303.26493639628], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.613948177710825, 0.6093315784700629, 0.0, 1.0, 0.29192030922093465], 
reward next is 0.7081, 
noisyNet noise sample is [array([-0.05339998], dtype=float32), -0.5050945]. 
=============================================
[2019-04-04 08:33:17,453] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2058760e-11 1.6517440e-10 6.3861370e-23 1.2879599e-11 1.5260255e-11
 1.8884061e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:17,453] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8851
[2019-04-04 08:33:17,480] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 55.83333333333334, 74.0, 602.6666666666667, 26.0, 25.13784346567646, 0.4096038993193159, 0.0, 1.0, 18706.99855834664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2994600.0000, 
sim time next is 2995200.0000, 
raw observation next is [-1.0, 55.0, 69.5, 570.5, 26.0, 25.14849951573077, 0.407805903577534, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.4349030470914128, 0.55, 0.23166666666666666, 0.6303867403314917, 0.6666666666666666, 0.5957082929775641, 0.6359353011925113, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49201924], dtype=float32), 0.27006763]. 
=============================================
[2019-04-04 08:33:18,436] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.0848882e-11 3.3613284e-10 2.1391614e-23 1.5471622e-11 1.8914360e-12
 2.5561930e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:18,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0203
[2019-04-04 08:33:18,450] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 63.66666666666667, 0.0, 0.0, 26.0, 25.23039108213945, 0.3548893402030302, 0.0, 1.0, 43937.55701729131], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2848200.0000, 
sim time next is 2848800.0000, 
raw observation next is [1.666666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.22931550665036, 0.3525981238629425, 0.0, 1.0, 43155.42125199507], 
processed observation next is [1.0, 1.0, 0.5087719298245615, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6024429588875299, 0.6175327079543141, 0.0, 1.0, 0.20550200596188128], 
reward next is 0.7945, 
noisyNet noise sample is [array([-1.3492323], dtype=float32), 1.1686347]. 
=============================================
[2019-04-04 08:33:18,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6530408e-10 1.6435645e-09 1.2742909e-22 1.1507676e-10 4.8498101e-11
 1.2079542e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:18,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5106
[2019-04-04 08:33:18,811] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.46806262929402, 0.1602401886798432, 0.0, 1.0, 38373.37254575094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3030600.0000, 
sim time next is 3031200.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.42346484431128, 0.1540426015147497, 0.0, 1.0, 38521.25483205608], 
processed observation next is [0.0, 0.08695652173913043, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5352887370259399, 0.5513475338382499, 0.0, 1.0, 0.18343454681931468], 
reward next is 0.8166, 
noisyNet noise sample is [array([0.21492021], dtype=float32), 0.6943802]. 
=============================================
[2019-04-04 08:33:29,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7237080e-13 1.1836756e-12 2.0835949e-26 8.0896559e-14 1.9908259e-14
 4.9040841e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:29,257] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1991
[2019-04-04 08:33:29,263] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 98.16666666666667, 749.0, 26.0, 27.63302052376537, 0.9527134640410583, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3163200.0000, 
sim time next is 3163800.0000, 
raw observation next is [7.0, 100.0, 95.33333333333334, 735.0, 26.0, 27.6909242561071, 0.9627956356580049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.3177777777777778, 0.8121546961325967, 0.6666666666666666, 0.8075770213422583, 0.8209318785526682, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.228665], dtype=float32), 0.2563939]. 
=============================================
[2019-04-04 08:33:30,144] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9576333e-12 6.5147623e-12 7.5749319e-24 1.3160703e-12 2.6985047e-13
 5.4081766e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:30,147] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3092
[2019-04-04 08:33:30,158] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 97.5, 107.6666666666667, 797.6666666666666, 26.0, 26.58372646744711, 0.7913281623780845, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3246600.0000, 
sim time next is 3247200.0000, 
raw observation next is [-4.0, 100.0, 106.0, 790.5, 26.0, 26.69081474203973, 0.8015375851617778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 1.0, 0.35333333333333333, 0.8734806629834254, 0.6666666666666666, 0.7242345618366443, 0.767179195053926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50873125], dtype=float32), -0.2076235]. 
=============================================
[2019-04-04 08:33:33,171] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4419702e-12 4.2099952e-12 2.5834617e-24 3.7113836e-12 6.7724689e-13
 1.5358864e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:33,171] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6084
[2019-04-04 08:33:33,190] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 103.6666666666667, 686.0, 26.0, 26.32343525821106, 0.6673036653770427, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3232200.0000, 
sim time next is 3232800.0000, 
raw observation next is [-3.0, 92.0, 105.0, 702.5, 26.0, 26.32286078799157, 0.6766707464643683, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.92, 0.35, 0.7762430939226519, 0.6666666666666666, 0.6935717323326308, 0.7255569154881227, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.61563385], dtype=float32), -1.0792786]. 
=============================================
[2019-04-04 08:33:34,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5004692e-09 2.8747264e-08 1.8998347e-20 1.5031623e-09 3.3423217e-10
 7.8072687e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:34,207] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2848
[2019-04-04 08:33:34,336] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 77.33333333333334, 0.0, 0.0, 26.0, 23.86391429302424, 0.07456711444557412, 0.0, 1.0, 44038.68699561705], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3309000.0000, 
sim time next is 3309600.0000, 
raw observation next is [-11.0, 78.66666666666667, 0.0, 0.0, 26.0, 23.79699428012074, 0.149508528095387, 1.0, 1.0, 202374.3300019872], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.7866666666666667, 0.0, 0.0, 0.6666666666666666, 0.48308285667672823, 0.5498361760317957, 1.0, 1.0, 0.9636872857237485], 
reward next is 0.0363, 
noisyNet noise sample is [array([-1.0445124], dtype=float32), 0.45478776]. 
=============================================
[2019-04-04 08:33:35,185] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1969587e-12 9.5558917e-12 6.6917430e-24 7.1417594e-12 4.5291201e-13
 4.2746636e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:35,185] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0293
[2019-04-04 08:33:35,245] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.833333333333334, 75.83333333333334, 98.0, 542.0, 26.0, 26.12435615181302, 0.5233322289776516, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3316200.0000, 
sim time next is 3316800.0000, 
raw observation next is [-8.666666666666668, 74.66666666666667, 101.0, 578.5, 26.0, 26.19778104253987, 0.5294674312593952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.22253000923361033, 0.7466666666666667, 0.33666666666666667, 0.6392265193370166, 0.6666666666666666, 0.6831484202116558, 0.6764891437531317, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13274153], dtype=float32), -1.5668916]. 
=============================================
[2019-04-04 08:33:37,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3722993e-10 2.4431002e-09 4.2337228e-21 1.0664420e-10 8.2976209e-11
 1.3061797e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:37,657] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5500
[2019-04-04 08:33:37,674] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.9682668612336, 0.3323630676562827, 0.0, 1.0, 43832.82402352744], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3292800.0000, 
sim time next is 3293400.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.89583828553497, 0.3187500087092634, 0.0, 1.0, 43869.76460736858], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5746531904612476, 0.6062500029030878, 0.0, 1.0, 0.20890364098746944], 
reward next is 0.7911, 
noisyNet noise sample is [array([0.52187926], dtype=float32), 0.09009948]. 
=============================================
[2019-04-04 08:33:38,425] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7841829e-11 2.9652023e-10 9.2474275e-24 7.5534232e-12 3.3962410e-12
 1.9217805e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:38,436] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4894
[2019-04-04 08:33:38,455] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 100.0, 0.0, 0.0, 26.0, 25.19657804458682, 0.2861665579296986, 0.0, 1.0, 54906.44004954922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3124200.0000, 
sim time next is 3124800.0000, 
raw observation next is [2.6, 100.0, 0.0, 0.0, 26.0, 25.19707902320241, 0.2903741241292884, 0.0, 1.0, 54296.53210801882], 
processed observation next is [1.0, 0.17391304347826086, 0.5346260387811635, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5997565852668675, 0.5967913747097628, 0.0, 1.0, 0.25855491480008963], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.15635297], dtype=float32), 0.0747822]. 
=============================================
[2019-04-04 08:33:39,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.7043991e-11 4.1520801e-10 1.1737080e-22 4.9394214e-11 3.1773074e-12
 1.6534846e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:39,759] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8203
[2019-04-04 08:33:39,815] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.833333333333334, 69.0, 108.6666666666667, 698.3333333333333, 26.0, 26.36082997064682, 0.5659410292567368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31550079037065, 0.5702048490235775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2502308402585411, 0.68, 0.366111111111111, 0.7946593001841622, 0.6666666666666666, 0.6929583991975541, 0.6900682830078592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82513934], dtype=float32), -0.8913261]. 
=============================================
[2019-04-04 08:33:49,720] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0838469e-11 6.6299699e-10 2.0785820e-22 7.4872018e-11 1.0562892e-11
 3.4818288e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:49,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8899
[2019-04-04 08:33:49,765] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.57171055093519, 0.5116365897872045, 1.0, 1.0, 31643.83781442364], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3352200.0000, 
sim time next is 3352800.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.54512288480654, 0.4952492178570551, 0.0, 1.0, 27162.38975319732], 
processed observation next is [1.0, 0.8260869565217391, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6287602404005449, 0.6650830726190183, 0.0, 1.0, 0.12934471311046342], 
reward next is 0.8707, 
noisyNet noise sample is [array([-0.77890915], dtype=float32), -1.6498691]. 
=============================================
[2019-04-04 08:33:50,237] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2436503e-10 7.5975293e-10 1.2438417e-22 7.3696056e-11 1.0106735e-10
 1.2625487e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:50,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5862
[2019-04-04 08:33:50,258] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 66.0, 0.0, 0.0, 26.0, 24.94985404733506, 0.3251060399933215, 0.0, 1.0, 40867.2268146779], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3559800.0000, 
sim time next is 3560400.0000, 
raw observation next is [-5.0, 65.0, 0.0, 0.0, 26.0, 24.90290005023117, 0.3145639118851966, 0.0, 1.0, 40868.93174547186], 
processed observation next is [0.0, 0.21739130434782608, 0.32409972299168976, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5752416708525976, 0.6048546372950655, 0.0, 1.0, 0.19461396069272316], 
reward next is 0.8054, 
noisyNet noise sample is [array([-1.0111688], dtype=float32), 1.9092151]. 
=============================================
[2019-04-04 08:33:50,754] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.4015052e-12 1.0989642e-11 2.8146833e-25 1.8361577e-12 1.6295839e-12
 4.8671292e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:50,755] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8869
[2019-04-04 08:33:50,764] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 26.0, 26.01452750198677, 0.5421911438660643, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3491400.0000, 
sim time next is 3492000.0000, 
raw observation next is [0.0, 60.0, 104.0, 720.0, 26.0, 26.13614660064907, 0.5618829339627793, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.6, 0.3466666666666667, 0.7955801104972375, 0.6666666666666666, 0.6780122167207558, 0.6872943113209264, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.626842], dtype=float32), 0.9283527]. 
=============================================
[2019-04-04 08:33:50,771] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.817894]
 [87.218506]
 [87.566124]
 [87.68574 ]
 [87.92468 ]], R is [[86.58074951]
 [86.71494293]
 [86.84779358]
 [86.97931671]
 [87.10952759]].
[2019-04-04 08:33:53,500] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6590439e-10 2.2682931e-10 8.6019154e-23 4.7057015e-11 2.8782608e-11
 2.6983090e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:33:53,500] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4399
[2019-04-04 08:33:53,534] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 52.5, 118.0, 823.0, 26.0, 25.18007679361539, 0.4491950383833944, 0.0, 1.0, 18710.10697724189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3587400.0000, 
sim time next is 3588000.0000, 
raw observation next is [-2.333333333333333, 51.66666666666666, 117.3333333333333, 821.1666666666667, 26.0, 25.18669180435771, 0.4511873066794712, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.3979686057248385, 0.5166666666666666, 0.391111111111111, 0.9073664825046042, 0.6666666666666666, 0.5988909836964759, 0.6503957688931571, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5904812], dtype=float32), -0.7261061]. 
=============================================
[2019-04-04 08:33:53,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[79.11204 ]
 [79.12297 ]
 [79.132805]
 [79.16792 ]
 [79.24483 ]], R is [[79.18128967]
 [79.30038452]
 [79.41827393]
 [79.5349884 ]
 [79.65052795]].
[2019-04-04 08:34:01,734] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3578004e-10 1.0791719e-09 2.9447978e-22 2.3243972e-10 4.6073412e-11
 5.2505851e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:01,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0102
[2019-04-04 08:34:01,750] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.53775429480983, 0.2356392945129711, 0.0, 1.0, 40956.02231922626], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3565800.0000, 
sim time next is 3566400.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.49150222375233, 0.2263271719078879, 0.0, 1.0, 41029.51704678917], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5409585186460276, 0.5754423906359626, 0.0, 1.0, 0.19537865260375797], 
reward next is 0.8046, 
noisyNet noise sample is [array([-1.2997024], dtype=float32), 1.0988905]. 
=============================================
[2019-04-04 08:34:01,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.4931943e-11 1.9062109e-10 7.4814603e-23 1.7937199e-11 9.9398536e-12
 3.4429104e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:01,906] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7591
[2019-04-04 08:34:01,923] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 74.0, 0.0, 0.0, 26.0, 25.22037803129468, 0.4140022456962431, 0.0, 1.0, 43839.87055760241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3803400.0000, 
sim time next is 3804000.0000, 
raw observation next is [-3.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.28845096176005, 0.4103251677056098, 0.0, 1.0, 43879.0863943887], 
processed observation next is [1.0, 0.0, 0.3610341643582641, 0.75, 0.0, 0.0, 0.6666666666666666, 0.607370913480004, 0.6367750559018699, 0.0, 1.0, 0.20894803044947], 
reward next is 0.7911, 
noisyNet noise sample is [array([-0.7367569], dtype=float32), -0.74533325]. 
=============================================
[2019-04-04 08:34:01,931] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[77.57126 ]
 [77.71054 ]
 [77.827484]
 [77.937454]
 [77.95521 ]], R is [[77.46692657]
 [77.48349762]
 [77.50013733]
 [77.5168457 ]
 [77.53355408]].
[2019-04-04 08:34:04,992] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1078476e-11 7.2119588e-11 1.3798403e-23 5.4026588e-12 1.6535516e-12
 3.3477317e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:04,993] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3636
[2019-04-04 08:34:05,037] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 66.0, 0.0, 0.0, 26.0, 25.17301354435366, 0.4472586925112759, 1.0, 1.0, 59939.66425945645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3786600.0000, 
sim time next is 3787200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.13619429621621, 0.44870075760736, 1.0, 1.0, 64097.48702938536], 
processed observation next is [1.0, 0.8695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5946828580180176, 0.6495669192024534, 1.0, 1.0, 0.30522612871135885], 
reward next is 0.6948, 
noisyNet noise sample is [array([0.40274978], dtype=float32), -0.8953328]. 
=============================================
[2019-04-04 08:34:06,586] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8932056e-11 1.0238883e-11 9.3612792e-26 5.6945369e-12 3.5798166e-13
 2.0680533e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:06,588] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9520
[2019-04-04 08:34:06,612] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.333333333333334, 31.0, 115.1666666666667, 807.1666666666666, 26.0, 25.68359359334551, 0.5004311870657622, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3669600.0000, 
sim time next is 3670200.0000, 
raw observation next is [8.0, 34.5, 116.0, 816.0, 26.0, 25.6867894692689, 0.4954896167419909, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6842105263157896, 0.345, 0.38666666666666666, 0.901657458563536, 0.6666666666666666, 0.6405657891057416, 0.6651632055806637, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.28243494], dtype=float32), 0.35888433]. 
=============================================
[2019-04-04 08:34:09,579] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2180478e-13 5.5178084e-12 9.2102413e-26 1.1330334e-12 2.5965772e-13
 4.4561154e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:09,579] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4700
[2019-04-04 08:34:09,597] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 26.0, 26.78212553851032, 0.7386094674494806, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3858600.0000, 
sim time next is 3859200.0000, 
raw observation next is [3.0, 45.0, 75.5, 634.0, 26.0, 26.88206741560917, 0.7516072563440529, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.25166666666666665, 0.7005524861878453, 0.6666666666666666, 0.7401722846340976, 0.7505357521146844, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3460352], dtype=float32), 0.9893456]. 
=============================================
[2019-04-04 08:34:16,406] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8768316e-12 1.8582101e-11 2.3257775e-24 3.1283877e-12 6.7850879e-13
 1.7697578e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:16,410] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9122
[2019-04-04 08:34:16,455] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 48.5, 101.0, 698.0, 26.0, 26.49120392159066, 0.5455615340710921, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4008600.0000, 
sim time next is 4009200.0000, 
raw observation next is [-9.666666666666668, 47.0, 102.8333333333333, 711.8333333333334, 26.0, 26.53224643101471, 0.5530006897675276, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.19482917820867957, 0.47, 0.3427777777777777, 0.7865561694290977, 0.6666666666666666, 0.7110205359178924, 0.6843335632558425, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0780786], dtype=float32), 0.22131844]. 
=============================================
[2019-04-04 08:34:25,173] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2405377e-09 2.3179831e-09 9.0678140e-22 7.0906642e-10 6.0857402e-11
 1.1437532e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:25,177] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8463
[2019-04-04 08:34:25,203] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.2533645742412, 0.1290197789530338, 0.0, 1.0, 43609.35916900662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988200.0000, 
sim time next is 3988800.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18022256450548, 0.1137711891745468, 0.0, 1.0, 43643.89971858619], 
processed observation next is [1.0, 0.17391304347826086, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5150185470421235, 0.5379237297248489, 0.0, 1.0, 0.20782809389802948], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.9213996], dtype=float32), -1.2356596]. 
=============================================
[2019-04-04 08:34:26,688] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.1961755e-12 8.9604556e-12 1.0531760e-24 5.0043489e-12 1.4606319e-13
 3.8429800e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:26,689] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9539
[2019-04-04 08:34:26,759] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 53.0, 97.0, 571.0, 26.0, 26.29610168438171, 0.505540269775215, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4006800.0000, 
sim time next is 4007400.0000, 
raw observation next is [-10.66666666666667, 51.50000000000001, 98.33333333333334, 613.3333333333334, 26.0, 26.37689666615852, 0.5200384531896552, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.16712834718374878, 0.5150000000000001, 0.32777777777777783, 0.6777163904235728, 0.6666666666666666, 0.6980747221798765, 0.6733461510632184, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2210544], dtype=float32), -0.45707592]. 
=============================================
[2019-04-04 08:34:28,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5037636e-11 4.7426993e-11 7.9260198e-25 8.4022988e-12 1.6522433e-12
 5.4041964e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:28,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1128
[2019-04-04 08:34:28,937] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 34.16666666666667, 117.3333333333333, 806.0, 26.0, 25.11545053542289, 0.3892565273496273, 0.0, 1.0, 18705.58955302931], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4187400.0000, 
sim time next is 4188000.0000, 
raw observation next is [-0.3333333333333334, 33.33333333333334, 118.1666666666667, 814.0, 26.0, 25.13207899321517, 0.3865265967903277, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.4533702677747, 0.3333333333333334, 0.393888888888889, 0.8994475138121547, 0.6666666666666666, 0.5943399161012642, 0.6288421989301093, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05567217], dtype=float32), 0.65954906]. 
=============================================
[2019-04-04 08:34:28,955] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.26683 ]
 [87.21484 ]
 [87.209885]
 [87.29003 ]
 [87.25521 ]], R is [[87.30204773]
 [87.33995819]
 [87.37748718]
 [87.50371552]
 [87.62867737]].
[2019-04-04 08:34:31,313] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2580253e-10 2.9595576e-10 4.7972091e-24 3.2194070e-11 4.0621950e-12
 1.1826250e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:31,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5311
[2019-04-04 08:34:31,328] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42894863510603, 0.3497558857824769, 0.0, 1.0, 32339.73601528629], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4236600.0000, 
sim time next is 4237200.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42184105846313, 0.3463198504921512, 0.0, 1.0, 39228.68421117693], 
processed observation next is [0.0, 0.043478260869565216, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6184867548719275, 0.6154399501640504, 0.0, 1.0, 0.18680325814846158], 
reward next is 0.8132, 
noisyNet noise sample is [array([1.493684], dtype=float32), -0.4577661]. 
=============================================
[2019-04-04 08:34:36,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1763173e-14 9.0586894e-14 2.9151465e-29 1.1783515e-14 2.1290508e-15
 1.4768375e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:34:36,564] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3703
[2019-04-04 08:34:36,591] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.2, 31.5, 195.0, 629.0, 26.0, 28.73307649749894, 1.061406811481945, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4372200.0000, 
sim time next is 4372800.0000, 
raw observation next is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 26.0, 28.31254101219794, 1.111077629995501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8531855955678671, 0.3166666666666666, 0.598888888888889, 0.5791896869244937, 0.6666666666666666, 0.8593784176831617, 0.8703592099985004, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4448916], dtype=float32), 0.061248906]. 
=============================================
[2019-04-04 08:34:39,421] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 08:34:39,423] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:34:39,424] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:34:39,424] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:34:39,424] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:34:39,424] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:34:39,424] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:34:39,428] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run28
[2019-04-04 08:34:39,451] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run28
[2019-04-04 08:34:39,474] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run28
[2019-04-04 08:35:46,386] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.22803766], dtype=float32), -0.16051027]
[2019-04-04 08:35:46,387] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.1, 92.0, 18.0, 0.0, 26.0, 25.80739975840997, 0.4985340527471129, 1.0, 1.0, 0.0]
[2019-04-04 08:35:46,387] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 08:35:46,388] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [5.7444236e-12 1.0200502e-11 9.2274730e-26 1.4618673e-12 2.5669088e-13
 1.6297058e-15 1.0000000e+00], sampled 0.020872704890082416
[2019-04-04 08:36:50,605] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.22803766], dtype=float32), -0.16051027]
[2019-04-04 08:36:50,605] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-11.54634186, 80.75425233833333, 0.0, 0.0, 26.0, 23.90158620052101, 0.05417182594637935, 0.0, 1.0, 45217.71955771163]
[2019-04-04 08:36:50,606] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:36:50,606] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.9846245e-09 3.8550851e-09 4.3354620e-21 4.0368880e-10 2.5075075e-10
 1.8495919e-12 1.0000000e+00], sampled 0.8643458123878732
[2019-04-04 08:37:43,156] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.22803766], dtype=float32), -0.16051027]
[2019-04-04 08:37:43,156] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [1.1, 39.0, 0.0, 0.0, 26.0, 25.41694656371447, 0.3971367793485487, 0.0, 1.0, 32869.31629543685]
[2019-04-04 08:37:43,156] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:37:43,157] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.5229545e-10 6.2288386e-10 1.7535685e-23 8.5727148e-11 2.1572315e-11
 7.6610755e-14 1.0000000e+00], sampled 0.5348155873068068
[2019-04-04 08:37:46,717] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:38:15,738] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.6417 263415234.9355 1551.9605
[2019-04-04 08:38:21,179] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:38:22,210] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 2700000, evaluation results [2700000.0, 7241.641738402564, 263415234.93545747, 1551.9605289518147, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:38:23,652] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5162487e-12 6.0771770e-12 5.7369922e-26 8.4116752e-13 2.1556292e-13
 9.6276973e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:38:23,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7425
[2019-04-04 08:38:23,700] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.7, 41.0, 9.0, 0.0, 26.0, 27.42327086521936, 0.924039725014833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4383000.0000, 
sim time next is 4383600.0000, 
raw observation next is [12.6, 42.0, 7.5, 0.0, 26.0, 27.44628682231164, 0.9950756068299594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8116343490304709, 0.42, 0.025, 0.0, 0.6666666666666666, 0.7871905685259701, 0.8316918689433198, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7061343], dtype=float32), -0.27741566]. 
=============================================
[2019-04-04 08:38:29,995] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0705503e-12 6.7198838e-12 1.3121346e-24 8.8713254e-13 1.6612073e-13
 1.2107942e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:38:29,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9616
[2019-04-04 08:38:30,007] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 55.0, 41.33333333333333, 26.0, 26.02367656776397, 0.530022349507092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4554600.0000, 
sim time next is 4555200.0000, 
raw observation next is [2.0, 52.0, 41.5, 34.66666666666666, 26.0, 26.07325012575709, 0.5075865406684029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.13833333333333334, 0.03830570902394106, 0.6666666666666666, 0.6727708438130909, 0.6691955135561343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5808002], dtype=float32), 2.2064357]. 
=============================================
[2019-04-04 08:38:38,505] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9146303e-12 2.2113773e-11 4.6477860e-25 1.8717157e-12 6.3332700e-13
 4.0447005e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:38:38,505] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-04 08:38:38,585] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 77.0, 33.0, 36.66666666666667, 26.0, 25.81165902435387, 0.5492567269228625, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4468200.0000, 
sim time next is 4468800.0000, 
raw observation next is [0.0, 76.0, 29.0, 45.83333333333334, 26.0, 26.00930425960902, 0.5333907396933186, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.46260387811634357, 0.76, 0.09666666666666666, 0.050644567219152864, 0.6666666666666666, 0.6674420216340851, 0.6777969132311062, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09179519], dtype=float32), 0.29278225]. 
=============================================
[2019-04-04 08:38:39,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4436137e-11 9.9301567e-11 1.6946505e-24 4.4038731e-12 1.2532499e-12
 1.0582235e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:38:39,695] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6683
[2019-04-04 08:38:39,727] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 26.0, 26.12868891041919, 0.6414891887360449, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4648200.0000, 
sim time next is 4648800.0000, 
raw observation next is [2.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 26.05518100750793, 0.6233009045685032, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6712650839589941, 0.707766968189501, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5436261], dtype=float32), -0.8774104]. 
=============================================
[2019-04-04 08:38:58,118] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.9168505e-11 3.4376859e-11 1.2846620e-23 8.9546643e-12 5.9019169e-13
 6.7166574e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:38:58,118] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9655
[2019-04-04 08:38:58,182] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7666666666666667, 47.83333333333334, 282.3333333333333, 335.3333333333333, 26.0, 25.05630067933696, 0.3521688774037634, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4881000.0000, 
sim time next is 4881600.0000, 
raw observation next is [1.0, 47.0, 282.0, 349.0, 26.0, 25.05721597878423, 0.3514422181836097, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.4903047091412743, 0.47, 0.94, 0.3856353591160221, 0.6666666666666666, 0.5881013315653526, 0.6171474060612032, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2288139], dtype=float32), -0.19048063]. 
=============================================
[2019-04-04 08:39:09,242] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1232586e-11 3.8722497e-12 1.7804546e-25 7.4249235e-12 5.8146733e-13
 2.2627174e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:39:09,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6282
[2019-04-04 08:39:09,289] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 36.0, 105.5, 690.8333333333333, 26.0, 26.00615102632463, 0.4489912368381963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4958400.0000, 
sim time next is 4959000.0000, 
raw observation next is [0.0, 34.5, 108.0, 717.0, 26.0, 26.08216394692588, 0.4704168524656509, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.345, 0.36, 0.7922651933701658, 0.6666666666666666, 0.6735136622438235, 0.6568056174885503, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52030283], dtype=float32), 1.2211818]. 
=============================================
[2019-04-04 08:39:09,315] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[91.64635 ]
 [91.40513 ]
 [91.437004]
 [91.676056]
 [91.98963 ]], R is [[92.12401581]
 [92.20277405]
 [92.28074646]
 [92.35794067]
 [92.43436432]].
[2019-04-04 08:39:12,412] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:12,412] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:12,443] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run21
[2019-04-04 08:39:17,308] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.1575861e-10 5.5723454e-10 2.3487741e-22 8.1378751e-11 2.2770090e-11
 3.3691924e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:39:17,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8933
[2019-04-04 08:39:17,346] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 44.33333333333334, 0.0, 0.0, 26.0, 24.98782309347134, 0.3462943513931711, 0.0, 1.0, 198825.861071015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4821600.0000, 
sim time next is 4822200.0000, 
raw observation next is [1.0, 45.0, 0.0, 0.0, 26.0, 25.02368416907072, 0.3754387872589158, 0.0, 1.0, 164400.8751376054], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.45, 0.0, 0.0, 0.6666666666666666, 0.5853070140892266, 0.6251462624196386, 0.0, 1.0, 0.7828613101790733], 
reward next is 0.2171, 
noisyNet noise sample is [array([-0.65716773], dtype=float32), -0.9547874]. 
=============================================
[2019-04-04 08:39:17,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:17,813] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:17,818] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run21
[2019-04-04 08:39:18,461] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9279103e-12 1.4387921e-11 4.5785921e-25 1.9394245e-12 1.4688203e-13
 1.3920270e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:39:18,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0573
[2019-04-04 08:39:18,489] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.666666666666667, 25.33333333333333, 28.33333333333334, 253.3333333333333, 26.0, 27.33721509660292, 0.8476868517210754, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4988400.0000, 
sim time next is 4989000.0000, 
raw observation next is [6.333333333333333, 25.16666666666667, 22.66666666666667, 202.6666666666667, 26.0, 27.23548611409671, 0.8395088197655118, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6380424746075716, 0.2516666666666667, 0.07555555555555557, 0.22394106813996323, 0.6666666666666666, 0.7696238428413924, 0.7798362732551706, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16158983], dtype=float32), 1.9682293]. 
=============================================
[2019-04-04 08:39:18,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[88.30842]
 [88.90234]
 [89.5766 ]
 [90.29539]
 [90.97165]], R is [[87.93056488]
 [88.0512619 ]
 [88.17075348]
 [88.28904724]
 [88.40615845]].
[2019-04-04 08:39:20,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:20,382] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:20,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run21
[2019-04-04 08:39:21,928] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:21,928] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:21,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run21
[2019-04-04 08:39:22,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:22,854] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:22,857] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run21
[2019-04-04 08:39:23,130] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170000, global step 2717179: loss 0.1436
[2019-04-04 08:39:23,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170000, global step 2717179: learning rate 0.0000
[2019-04-04 08:39:23,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:23,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:23,760] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run21
[2019-04-04 08:39:24,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:24,637] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:24,642] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run21
[2019-04-04 08:39:24,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:24,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:24,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run21
[2019-04-04 08:39:25,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:25,613] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:25,617] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run21
[2019-04-04 08:39:25,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:25,885] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:25,889] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run21
[2019-04-04 08:39:26,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:26,133] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:26,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run21
[2019-04-04 08:39:27,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:27,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:27,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run21
[2019-04-04 08:39:28,573] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170000, global step 2718274: loss 0.1412
[2019-04-04 08:39:28,574] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170000, global step 2718274: learning rate 0.0000
[2019-04-04 08:39:31,798] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170000, global step 2718803: loss 0.1395
[2019-04-04 08:39:31,798] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170000, global step 2718803: learning rate 0.0000
[2019-04-04 08:39:33,700] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.3595897e-11 1.4781778e-11 1.3658206e-24 1.6033713e-12 2.6457066e-12
 1.3443471e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:39:33,705] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2219
[2019-04-04 08:39:33,756] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.766666666666667, 88.0, 0.0, 0.0, 26.0, 24.55331205150782, 0.195689115209195, 0.0, 1.0, 59863.37408746044], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 63600.0000, 
sim time next is 64200.0000, 
raw observation next is [4.583333333333334, 88.5, 0.0, 0.0, 26.0, 24.56812841502387, 0.2006389324496204, 0.0, 1.0, 46479.81782262964], 
processed observation next is [0.0, 0.7391304347826086, 0.5895660203139428, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5473440345853225, 0.5668796441498735, 0.0, 1.0, 0.22133246582204588], 
reward next is 0.7787, 
noisyNet noise sample is [array([-1.5569683], dtype=float32), 1.2819301]. 
=============================================
[2019-04-04 08:39:34,421] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170000, global step 2719353: loss 0.1478
[2019-04-04 08:39:34,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170000, global step 2719354: learning rate 0.0000
[2019-04-04 08:39:34,457] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170000, global step 2719370: loss 0.1504
[2019-04-04 08:39:34,457] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170000, global step 2719370: learning rate 0.0000
[2019-04-04 08:39:35,893] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170000, global step 2719859: loss 0.1608
[2019-04-04 08:39:35,894] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170000, global step 2719859: learning rate 0.0000
[2019-04-04 08:39:36,095] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1848129e-10 4.8494425e-10 6.7273484e-23 5.2548386e-11 3.9783652e-11
 8.7103766e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:39:36,095] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3288
[2019-04-04 08:39:36,108] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.19240022771296, 0.09729269250240746, 0.0, 1.0, 42594.51462153073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [-2.616666666666667, 87.66666666666666, 0.0, 0.0, 26.0, 24.11331844208428, 0.09571525056384377, 0.0, 1.0, 42778.78708577724], 
processed observation next is [1.0, 0.08695652173913043, 0.3901200369344414, 0.8766666666666666, 0.0, 0.0, 0.6666666666666666, 0.5094432035070234, 0.5319050835212812, 0.0, 1.0, 0.20370850993227257], 
reward next is 0.7963, 
noisyNet noise sample is [array([-1.2081883], dtype=float32), -0.09573177]. 
=============================================
[2019-04-04 08:39:36,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:36,425] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:36,430] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run21
[2019-04-04 08:39:36,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:36,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:36,808] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run21
[2019-04-04 08:39:37,025] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170000, global step 2720136: loss 0.1742
[2019-04-04 08:39:37,026] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170000, global step 2720136: learning rate 0.0000
[2019-04-04 08:39:37,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:37,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:37,300] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170000, global step 2720191: loss 0.1752
[2019-04-04 08:39:37,301] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170000, global step 2720191: learning rate 0.0000
[2019-04-04 08:39:37,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run21
[2019-04-04 08:39:37,935] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170000, global step 2720317: loss 0.1579
[2019-04-04 08:39:37,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170000, global step 2720317: learning rate 0.0000
[2019-04-04 08:39:38,188] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170000, global step 2720368: loss 0.1587
[2019-04-04 08:39:38,188] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170000, global step 2720368: learning rate 0.0000
[2019-04-04 08:39:38,341] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170000, global step 2720396: loss 0.1485
[2019-04-04 08:39:38,341] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170000, global step 2720396: learning rate 0.0000
[2019-04-04 08:39:39,979] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:39:39,980] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:39:39,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run21
[2019-04-04 08:39:40,183] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170000, global step 2720733: loss 0.1026
[2019-04-04 08:39:40,184] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170000, global step 2720733: learning rate 0.0000
[2019-04-04 08:39:48,191] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170000, global step 2722471: loss 0.1095
[2019-04-04 08:39:48,212] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170000, global step 2722471: learning rate 0.0000
[2019-04-04 08:39:48,945] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170000, global step 2722655: loss 0.1100
[2019-04-04 08:39:48,954] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170000, global step 2722655: learning rate 0.0000
[2019-04-04 08:39:48,962] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170000, global step 2722662: loss 0.1096
[2019-04-04 08:39:48,964] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170000, global step 2722662: learning rate 0.0000
[2019-04-04 08:39:51,829] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170000, global step 2723345: loss 0.1282
[2019-04-04 08:39:51,831] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170000, global step 2723346: learning rate 0.0000
[2019-04-04 08:39:53,723] A3C_AGENT_WORKER-Thread-2 INFO:Local step 170500, global step 2723952: loss 0.1338
[2019-04-04 08:39:53,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 170500, global step 2723955: learning rate 0.0000
[2019-04-04 08:40:00,899] A3C_AGENT_WORKER-Thread-12 INFO:Local step 170500, global step 2725657: loss 0.1936
[2019-04-04 08:40:00,901] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 170500, global step 2725657: learning rate 0.0000
[2019-04-04 08:40:02,977] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2364580e-11 1.9812801e-10 1.7189038e-23 1.9238007e-11 4.2322968e-12
 3.7675470e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:02,977] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6070
[2019-04-04 08:40:03,048] A3C_AGENT_WORKER-Thread-5 INFO:Local step 170500, global step 2726324: loss 0.1360
[2019-04-04 08:40:03,051] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 170500, global step 2726324: learning rate 0.0000
[2019-04-04 08:40:03,055] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.71666666666667, 69.5, 9.999999999999998, 145.3333333333333, 26.0, 23.89287496817208, 0.06762107324930051, 1.0, 1.0, 113684.2988800308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 287400.0000, 
sim time next is 288000.0000, 
raw observation next is [-12.8, 70.0, 15.0, 205.5, 26.0, 24.41710840335185, 0.1191061538683925, 1.0, 1.0, 93580.93430375599], 
processed observation next is [1.0, 0.34782608695652173, 0.1080332409972299, 0.7, 0.05, 0.22707182320441988, 0.6666666666666666, 0.5347590336126542, 0.5397020512894641, 1.0, 1.0, 0.4456234966845523], 
reward next is 0.5544, 
noisyNet noise sample is [array([0.10559354], dtype=float32), 0.6207817]. 
=============================================
[2019-04-04 08:40:03,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.52182]
 [78.46265]
 [76.61538]
 [72.99979]
 [67.22574]], R is [[81.485466  ]
 [81.1292572 ]
 [80.53374481]
 [79.75972748]
 [78.99887085]].
[2019-04-04 08:40:06,481] A3C_AGENT_WORKER-Thread-17 INFO:Local step 170500, global step 2727077: loss 0.1627
[2019-04-04 08:40:06,481] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 170500, global step 2727077: learning rate 0.0000
[2019-04-04 08:40:06,805] A3C_AGENT_WORKER-Thread-16 INFO:Local step 170500, global step 2727155: loss 0.1647
[2019-04-04 08:40:06,806] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 170500, global step 2727157: learning rate 0.0000
[2019-04-04 08:40:08,524] A3C_AGENT_WORKER-Thread-15 INFO:Local step 170500, global step 2727537: loss 0.1828
[2019-04-04 08:40:08,525] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 170500, global step 2727537: learning rate 0.0000
[2019-04-04 08:40:08,709] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.50827370e-11 1.38995579e-10 1.11049010e-22 1.35141134e-11
 1.92769672e-12 8.95352365e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:40:08,709] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4438
[2019-04-04 08:40:08,808] A3C_AGENT_WORKER-Thread-20 INFO:Local step 170500, global step 2727604: loss 0.1608
[2019-04-04 08:40:08,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 170500, global step 2727604: learning rate 0.0000
[2019-04-04 08:40:08,813] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.16666666666667, 44.0, 49.16666666666666, 841.1666666666667, 26.0, 25.89400550062027, 0.4246978011192106, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 397200.0000, 
sim time next is 397800.0000, 
raw observation next is [-10.0, 43.0, 48.0, 832.0, 26.0, 25.16343722392237, 0.4282634088666383, 1.0, 1.0, 183622.5172362765], 
processed observation next is [1.0, 0.6086956521739131, 0.18559556786703602, 0.43, 0.16, 0.9193370165745857, 0.6666666666666666, 0.5969531019935307, 0.6427544696222127, 1.0, 1.0, 0.8743929392203643], 
reward next is 0.1256, 
noisyNet noise sample is [array([-0.68724203], dtype=float32), -0.11900701]. 
=============================================
[2019-04-04 08:40:09,083] A3C_AGENT_WORKER-Thread-18 INFO:Local step 170500, global step 2727670: loss 0.1485
[2019-04-04 08:40:09,083] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 170500, global step 2727670: learning rate 0.0000
[2019-04-04 08:40:09,787] A3C_AGENT_WORKER-Thread-3 INFO:Local step 170500, global step 2727845: loss 0.1286
[2019-04-04 08:40:09,787] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 170500, global step 2727845: learning rate 0.0000
[2019-04-04 08:40:10,049] A3C_AGENT_WORKER-Thread-19 INFO:Local step 170500, global step 2727928: loss 0.1402
[2019-04-04 08:40:10,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 170500, global step 2727928: learning rate 0.0000
[2019-04-04 08:40:10,694] A3C_AGENT_WORKER-Thread-11 INFO:Local step 170500, global step 2728144: loss 0.0894
[2019-04-04 08:40:10,700] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 170500, global step 2728144: learning rate 0.0000
[2019-04-04 08:40:11,889] A3C_AGENT_WORKER-Thread-6 INFO:Local step 170500, global step 2728542: loss 0.0604
[2019-04-04 08:40:11,891] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 170500, global step 2728542: learning rate 0.0000
[2019-04-04 08:40:12,166] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3893071e-09 4.3819024e-09 5.7849365e-21 6.6830275e-10 1.6689526e-10
 2.3438185e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:12,166] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2995
[2019-04-04 08:40:12,218] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.75, 69.0, 0.0, 0.0, 26.0, 23.29819291892085, -0.1212209936813704, 0.0, 1.0, 47895.43855705271], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 351000.0000, 
sim time next is 351600.0000, 
raw observation next is [-14.83333333333333, 69.0, 0.0, 0.0, 26.0, 23.26524121012938, -0.1259431841864379, 0.0, 1.0, 48037.13860721489], 
processed observation next is [1.0, 0.043478260869565216, 0.05170821791320413, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4387701008441149, 0.45801893860452064, 0.0, 1.0, 0.22874827908197568], 
reward next is 0.7713, 
noisyNet noise sample is [array([0.75545365], dtype=float32), -0.32676774]. 
=============================================
[2019-04-04 08:40:19,594] A3C_AGENT_WORKER-Thread-4 INFO:Local step 170500, global step 2730454: loss 0.0142
[2019-04-04 08:40:19,596] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 170500, global step 2730454: learning rate 0.0000
[2019-04-04 08:40:19,924] A3C_AGENT_WORKER-Thread-14 INFO:Local step 170500, global step 2730557: loss 0.0116
[2019-04-04 08:40:19,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 170500, global step 2730557: learning rate 0.0000
[2019-04-04 08:40:20,726] A3C_AGENT_WORKER-Thread-10 INFO:Local step 170500, global step 2730802: loss 0.0096
[2019-04-04 08:40:20,735] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 170500, global step 2730802: learning rate 0.0000
[2019-04-04 08:40:22,629] A3C_AGENT_WORKER-Thread-13 INFO:Local step 170500, global step 2731292: loss 0.0004
[2019-04-04 08:40:22,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 170500, global step 2731292: learning rate 0.0000
[2019-04-04 08:40:23,106] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171000, global step 2731406: loss 0.0345
[2019-04-04 08:40:23,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171000, global step 2731406: learning rate 0.0000
[2019-04-04 08:40:23,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3799181e-11 9.0312723e-11 9.6113610e-25 6.1736553e-12 8.0931328e-13
 8.2200397e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:23,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4894
[2019-04-04 08:40:23,282] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.33333333333334, 84.0, 20.83333333333334, 405.8333333333333, 26.0, 23.86227881401069, 0.009989288109637081, 1.0, 1.0, 86341.80108719012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 375600.0000, 
sim time next is 376200.0000, 
raw observation next is [-16.15, 85.5, 25.0, 477.0, 26.0, 24.29385977118951, 0.07869889706107712, 1.0, 1.0, 84926.35681455064], 
processed observation next is [1.0, 0.34782608695652173, 0.015235457063711934, 0.855, 0.08333333333333333, 0.5270718232044199, 0.6666666666666666, 0.5244883142657925, 0.5262329656870257, 1.0, 1.0, 0.4044112229264316], 
reward next is 0.5956, 
noisyNet noise sample is [array([-0.02795758], dtype=float32), 0.0855696]. 
=============================================
[2019-04-04 08:40:28,662] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2788465e-11 1.2419758e-11 1.5134808e-25 1.5281394e-12 3.6139749e-13
 1.5471119e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:28,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9902
[2019-04-04 08:40:28,735] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 91.0, 89.0, 103.5, 26.0, 25.12143452922684, 0.2950828730101057, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 107.3333333333333, 103.3333333333333, 26.0, 25.0885339191361, 0.2880852036446447, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4598337950138504, 0.9033333333333334, 0.3577777777777777, 0.11418047882136276, 0.6666666666666666, 0.5907111599280084, 0.5960284012148815, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4925145], dtype=float32), 0.40713185]. 
=============================================
[2019-04-04 08:40:30,391] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171000, global step 2733495: loss 0.0332
[2019-04-04 08:40:30,392] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171000, global step 2733495: learning rate 0.0000
[2019-04-04 08:40:33,115] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171000, global step 2734277: loss 0.0301
[2019-04-04 08:40:33,116] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171000, global step 2734277: learning rate 0.0000
[2019-04-04 08:40:33,397] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.06233904e-10 2.48399690e-10 1.50832966e-23 3.10609906e-11
 8.10143012e-12 4.38884226e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:40:33,397] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1460
[2019-04-04 08:40:33,421] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 86.33333333333333, 0.0, 0.0, 26.0, 24.65282174625434, 0.1945032939196761, 0.0, 1.0, 42341.66524791199], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 604200.0000, 
sim time next is 604800.0000, 
raw observation next is [-3.4, 87.0, 0.0, 0.0, 26.0, 24.62047973466025, 0.1875227367105159, 0.0, 1.0, 42316.78990517924], 
processed observation next is [0.0, 0.0, 0.368421052631579, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5517066445550208, 0.5625075789035053, 0.0, 1.0, 0.20150852335799638], 
reward next is 0.7985, 
noisyNet noise sample is [array([1.2845631], dtype=float32), -1.054401]. 
=============================================
[2019-04-04 08:40:34,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3199908e-10 1.9413732e-10 1.0297418e-23 2.3099482e-11 1.0482810e-11
 2.2327172e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:34,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6534
[2019-04-04 08:40:34,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 26.0, 25.23761189391551, 0.2571782653964879, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 637800.0000, 
sim time next is 638400.0000, 
raw observation next is [-3.9, 69.0, 115.6666666666667, 42.5, 26.0, 25.23055304737941, 0.2512130696630903, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.69, 0.38555555555555565, 0.04696132596685083, 0.6666666666666666, 0.6025460872816174, 0.5837376898876968, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.266152], dtype=float32), 1.3288335]. 
=============================================
[2019-04-04 08:40:34,920] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171000, global step 2734858: loss 0.0184
[2019-04-04 08:40:34,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171000, global step 2734858: learning rate 0.0000
[2019-04-04 08:40:36,004] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171000, global step 2735201: loss 0.0160
[2019-04-04 08:40:36,005] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171000, global step 2735201: learning rate 0.0000
[2019-04-04 08:40:36,919] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171000, global step 2735453: loss 0.0123
[2019-04-04 08:40:36,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171000, global step 2735453: learning rate 0.0000
[2019-04-04 08:40:37,162] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171000, global step 2735519: loss 0.0155
[2019-04-04 08:40:37,162] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171000, global step 2735519: learning rate 0.0000
[2019-04-04 08:40:37,527] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171000, global step 2735609: loss 0.0148
[2019-04-04 08:40:37,528] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171000, global step 2735609: learning rate 0.0000
[2019-04-04 08:40:37,765] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171000, global step 2735670: loss 0.0151
[2019-04-04 08:40:37,771] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171000, global step 2735670: learning rate 0.0000
[2019-04-04 08:40:38,649] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171000, global step 2735942: loss 0.0118
[2019-04-04 08:40:38,652] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171000, global step 2735942: learning rate 0.0000
[2019-04-04 08:40:39,063] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171000, global step 2736051: loss 0.0146
[2019-04-04 08:40:39,064] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171000, global step 2736053: learning rate 0.0000
[2019-04-04 08:40:40,557] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171000, global step 2736461: loss 0.0116
[2019-04-04 08:40:40,559] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171000, global step 2736462: learning rate 0.0000
[2019-04-04 08:40:40,607] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3717717e-11 1.8880814e-11 1.6080096e-25 1.9373431e-12 5.7210757e-13
 5.7440285e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:40,608] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5076
[2019-04-04 08:40:40,711] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 91.66666666666667, 28.5, 87.5, 26.0, 24.30754467152769, 0.2209473390545356, 0.0, 1.0, 202446.3460995434], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 548400.0000, 
sim time next is 549000.0000, 
raw observation next is [0.25, 91.5, 34.0, 104.0, 26.0, 24.62283313580432, 0.2713850057909315, 0.0, 1.0, 10432.47593509457], 
processed observation next is [0.0, 0.34782608695652173, 0.46952908587257625, 0.915, 0.11333333333333333, 0.11491712707182321, 0.6666666666666666, 0.5519027613170268, 0.5904616685969771, 0.0, 1.0, 0.04967845683378366], 
reward next is 0.9503, 
noisyNet noise sample is [array([-0.88334304], dtype=float32), 0.70316267]. 
=============================================
[2019-04-04 08:40:40,723] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[85.032845]
 [83.93733 ]
 [83.61808 ]
 [83.14371 ]
 [82.5974  ]], R is [[85.15632629]
 [84.34073639]
 [84.30224609]
 [84.26386261]
 [84.22557831]].
[2019-04-04 08:40:46,735] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171000, global step 2738591: loss 0.0066
[2019-04-04 08:40:46,736] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171000, global step 2738591: learning rate 0.0000
[2019-04-04 08:40:47,557] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171000, global step 2738843: loss 0.0043
[2019-04-04 08:40:47,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171000, global step 2738843: learning rate 0.0000
[2019-04-04 08:40:47,796] A3C_AGENT_WORKER-Thread-2 INFO:Local step 171500, global step 2738938: loss 0.0091
[2019-04-04 08:40:47,797] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 171500, global step 2738938: learning rate 0.0000
[2019-04-04 08:40:48,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1554240e-10 4.5161616e-10 1.9875463e-23 6.9197766e-11 2.6155779e-11
 1.6633473e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:48,837] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1292
[2019-04-04 08:40:48,862] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 23.60391859889839, -0.04478527756030481, 0.0, 1.0, 42039.13438189814], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 798000.0000, 
sim time next is 798600.0000, 
raw observation next is [-7.299999999999999, 71.0, 0.0, 0.0, 26.0, 23.58526128289462, -0.05009777388124598, 0.0, 1.0, 42069.40627419434], 
processed observation next is [1.0, 0.21739130434782608, 0.2603878116343491, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4654384402412184, 0.4833007420395847, 0.0, 1.0, 0.2003305060675921], 
reward next is 0.7997, 
noisyNet noise sample is [array([1.8073833], dtype=float32), 0.7909843]. 
=============================================
[2019-04-04 08:40:49,518] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171000, global step 2739503: loss 0.0050
[2019-04-04 08:40:49,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171000, global step 2739503: learning rate 0.0000
[2019-04-04 08:40:50,421] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171000, global step 2739745: loss 0.0054
[2019-04-04 08:40:50,421] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171000, global step 2739745: learning rate 0.0000
[2019-04-04 08:40:56,023] A3C_AGENT_WORKER-Thread-12 INFO:Local step 171500, global step 2741485: loss 0.0105
[2019-04-04 08:40:56,023] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 171500, global step 2741485: learning rate 0.0000
[2019-04-04 08:40:58,339] A3C_AGENT_WORKER-Thread-5 INFO:Local step 171500, global step 2742374: loss 0.0089
[2019-04-04 08:40:58,346] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 171500, global step 2742374: learning rate 0.0000
[2019-04-04 08:40:58,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2161218e-12 1.5653760e-11 4.4702923e-26 1.0050650e-12 9.3033613e-14
 5.0286800e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:58,760] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7131
[2019-04-04 08:40:58,802] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.70086450921662, 0.336654431689496, 0.0, 1.0, 103112.1873685872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 937800.0000, 
sim time next is 938400.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.78065646463089, 0.3535528322553141, 0.0, 1.0, 61907.98183083411], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5650547053859075, 0.6178509440851047, 0.0, 1.0, 0.29479991348016243], 
reward next is 0.7052, 
noisyNet noise sample is [array([-1.5832319], dtype=float32), 0.71537364]. 
=============================================
[2019-04-04 08:40:59,324] A3C_AGENT_WORKER-Thread-17 INFO:Local step 171500, global step 2742796: loss 0.0090
[2019-04-04 08:40:59,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 171500, global step 2742796: learning rate 0.0000
[2019-04-04 08:40:59,345] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7173216e-11 5.0768216e-11 5.9298871e-25 5.6792644e-12 9.3084069e-13
 8.4104468e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:40:59,346] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6115
[2019-04-04 08:40:59,390] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.72534146979959, 0.2306545002519585, 0.0, 1.0, 39648.97664565075], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 866400.0000, 
sim time next is 867000.0000, 
raw observation next is [-2.3, 80.0, 0.0, 0.0, 26.0, 24.78194655720925, 0.2267974490107881, 0.0, 1.0, 39550.80090927865], 
processed observation next is [1.0, 0.0, 0.3988919667590028, 0.8, 0.0, 0.0, 0.6666666666666666, 0.5651622131007707, 0.5755991496702627, 0.0, 1.0, 0.1883371471870412], 
reward next is 0.8117, 
noisyNet noise sample is [array([-0.89353573], dtype=float32), 1.1722467]. 
=============================================
[2019-04-04 08:40:59,415] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[84.40715 ]
 [84.32517 ]
 [84.27793 ]
 [84.220894]
 [84.16463 ]], R is [[84.41435242]
 [84.38140869]
 [84.34815216]
 [84.31460571]
 [84.28079224]].
[2019-04-04 08:41:00,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1621528e-12 2.1956418e-11 5.4959924e-27 2.2300243e-13 2.4367940e-13
 2.0803334e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:00,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3633
[2019-04-04 08:41:00,357] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 76.33333333333334, 0.0, 0.0, 26.0, 25.73072201518756, 0.619269484806908, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1027200.0000, 
sim time next is 1027800.0000, 
raw observation next is [14.4, 76.0, 0.0, 0.0, 26.0, 25.81706221988414, 0.6229881632690072, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.8614958448753465, 0.76, 0.0, 0.0, 0.6666666666666666, 0.6514218516570116, 0.707662721089669, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.55817026], dtype=float32), 0.6946705]. 
=============================================
[2019-04-04 08:41:00,874] A3C_AGENT_WORKER-Thread-16 INFO:Local step 171500, global step 2743388: loss 0.0114
[2019-04-04 08:41:00,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 171500, global step 2743388: learning rate 0.0000
[2019-04-04 08:41:01,323] A3C_AGENT_WORKER-Thread-15 INFO:Local step 171500, global step 2743570: loss 0.0093
[2019-04-04 08:41:01,326] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 171500, global step 2743572: learning rate 0.0000
[2019-04-04 08:41:01,752] A3C_AGENT_WORKER-Thread-20 INFO:Local step 171500, global step 2743752: loss 0.0076
[2019-04-04 08:41:01,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 171500, global step 2743752: learning rate 0.0000
[2019-04-04 08:41:02,069] A3C_AGENT_WORKER-Thread-18 INFO:Local step 171500, global step 2743898: loss 0.0066
[2019-04-04 08:41:02,071] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 171500, global step 2743899: learning rate 0.0000
[2019-04-04 08:41:02,393] A3C_AGENT_WORKER-Thread-3 INFO:Local step 171500, global step 2744057: loss 0.0088
[2019-04-04 08:41:02,394] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 171500, global step 2744058: learning rate 0.0000
[2019-04-04 08:41:02,517] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172000, global step 2744102: loss 16.0656
[2019-04-04 08:41:02,519] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172000, global step 2744103: learning rate 0.0000
[2019-04-04 08:41:03,497] A3C_AGENT_WORKER-Thread-19 INFO:Local step 171500, global step 2744502: loss 0.0082
[2019-04-04 08:41:03,498] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 171500, global step 2744503: learning rate 0.0000
[2019-04-04 08:41:03,779] A3C_AGENT_WORKER-Thread-11 INFO:Local step 171500, global step 2744623: loss 0.0097
[2019-04-04 08:41:03,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 171500, global step 2744623: learning rate 0.0000
[2019-04-04 08:41:04,666] A3C_AGENT_WORKER-Thread-6 INFO:Local step 171500, global step 2745025: loss 0.0077
[2019-04-04 08:41:04,668] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 171500, global step 2745026: learning rate 0.0000
[2019-04-04 08:41:05,281] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.0036421e-11 4.7421657e-11 1.6300580e-25 5.1284007e-12 4.5738218e-12
 4.8818748e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:05,285] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9032
[2019-04-04 08:41:05,300] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.333333333333333, 80.66666666666666, 0.0, 0.0, 26.0, 25.45073683447724, 0.426289201305636, 0.0, 1.0, 64559.7939299492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 960000.0000, 
sim time next is 960600.0000, 
raw observation next is [7.516666666666667, 80.33333333333334, 0.0, 0.0, 26.0, 25.40563315836883, 0.4247577318506484, 0.0, 1.0, 72880.853311213], 
processed observation next is [1.0, 0.08695652173913043, 0.6708217913204063, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.6171360965307358, 0.6415859106168827, 0.0, 1.0, 0.3470516824343476], 
reward next is 0.6529, 
noisyNet noise sample is [array([0.5034979], dtype=float32), -0.044997044]. 
=============================================
[2019-04-04 08:41:07,249] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4659814e-14 1.4343217e-13 7.1205843e-30 8.0857494e-15 1.2516808e-15
 3.0310801e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:07,256] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0885
[2019-04-04 08:41:07,263] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.38333333333333, 82.5, 74.0, 179.0, 26.0, 26.52330486311274, 0.7453522686550015, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1069800.0000, 
sim time next is 1070400.0000, 
raw observation next is [12.56666666666667, 82.0, 87.00000000000001, 206.5, 26.0, 26.56359825573097, 0.7688858622328221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8107109879963068, 0.82, 0.29000000000000004, 0.2281767955801105, 0.6666666666666666, 0.7136331879775808, 0.7562952874109407, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6326414], dtype=float32), 0.45122322]. 
=============================================
[2019-04-04 08:41:09,533] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172000, global step 2747745: loss 15.3027
[2019-04-04 08:41:09,535] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172000, global step 2747746: learning rate 0.0000
[2019-04-04 08:41:10,063] A3C_AGENT_WORKER-Thread-4 INFO:Local step 171500, global step 2748031: loss 0.0113
[2019-04-04 08:41:10,064] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 171500, global step 2748031: learning rate 0.0000
[2019-04-04 08:41:10,599] A3C_AGENT_WORKER-Thread-14 INFO:Local step 171500, global step 2748380: loss 0.0212
[2019-04-04 08:41:10,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 171500, global step 2748382: learning rate 0.0000
[2019-04-04 08:41:11,143] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172000, global step 2748720: loss 14.9203
[2019-04-04 08:41:11,145] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172000, global step 2748721: learning rate 0.0000
[2019-04-04 08:41:12,273] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172000, global step 2749437: loss 14.5823
[2019-04-04 08:41:12,275] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172000, global step 2749437: learning rate 0.0000
[2019-04-04 08:41:12,749] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.39253435e-11 7.04795423e-12 5.32369130e-26 1.06366999e-12
 1.54703464e-12 3.31820753e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:41:12,761] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5476
[2019-04-04 08:41:12,790] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.833333333333334, 96.0, 0.0, 0.0, 26.0, 24.71221966974382, 0.462886254669425, 0.0, 1.0, 198762.5005953572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1279200.0000, 
sim time next is 1279800.0000, 
raw observation next is [6.65, 96.0, 0.0, 0.0, 26.0, 24.72617691571089, 0.4936731095863097, 0.0, 1.0, 169270.513014879], 
processed observation next is [0.0, 0.8260869565217391, 0.6468144044321331, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5605147429759075, 0.6645577031954365, 0.0, 1.0, 0.8060500619756142], 
reward next is 0.1939, 
noisyNet noise sample is [array([0.13040134], dtype=float32), 0.817657]. 
=============================================
[2019-04-04 08:41:12,819] A3C_AGENT_WORKER-Thread-10 INFO:Local step 171500, global step 2749779: loss 0.0194
[2019-04-04 08:41:12,821] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 171500, global step 2749779: learning rate 0.0000
[2019-04-04 08:41:13,306] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172000, global step 2750093: loss 14.1378
[2019-04-04 08:41:13,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172000, global step 2750094: learning rate 0.0000
[2019-04-04 08:41:13,478] A3C_AGENT_WORKER-Thread-13 INFO:Local step 171500, global step 2750209: loss 0.0224
[2019-04-04 08:41:13,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 171500, global step 2750209: learning rate 0.0000
[2019-04-04 08:41:13,982] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172000, global step 2750513: loss 13.8181
[2019-04-04 08:41:13,984] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172000, global step 2750515: learning rate 0.0000
[2019-04-04 08:41:14,151] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.1491514e-11 1.0738979e-11 1.9293493e-26 6.2032705e-12 1.9742494e-13
 3.1737999e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:14,155] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6346
[2019-04-04 08:41:14,162] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.11666666666667, 65.33333333333334, 140.0, 0.0, 26.0, 25.2710793318689, 0.5200015568447952, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1162200.0000, 
sim time next is 1162800.0000, 
raw observation next is [18.3, 65.0, 145.0, 0.0, 26.0, 25.23942585825207, 0.514923305453996, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.9695290858725764, 0.65, 0.48333333333333334, 0.0, 0.6666666666666666, 0.6032854881876725, 0.6716411018179986, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.76877564], dtype=float32), 0.7987953]. 
=============================================
[2019-04-04 08:41:14,189] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172000, global step 2750643: loss 13.6615
[2019-04-04 08:41:14,191] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172000, global step 2750644: learning rate 0.0000
[2019-04-04 08:41:14,329] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3155033e-13 1.0585597e-12 1.5960667e-27 8.1898396e-14 1.8734710e-14
 1.6821806e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:14,332] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7348
[2019-04-04 08:41:14,404] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 89.33333333333334, 0.0, 0.0, 26.0, 24.76210533559027, 0.3955057766058336, 1.0, 1.0, 196667.2270737656], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1446000.0000, 
sim time next is 1446600.0000, 
raw observation next is [1.1, 88.66666666666667, 0.0, 0.0, 26.0, 24.70442229230803, 0.4586871058724658, 1.0, 1.0, 192503.4127005804], 
processed observation next is [1.0, 0.7391304347826086, 0.49307479224376743, 0.8866666666666667, 0.0, 0.0, 0.6666666666666666, 0.5587018576923359, 0.6528957019574886, 1.0, 1.0, 0.9166829176218114], 
reward next is 0.0833, 
noisyNet noise sample is [array([-1.2236805], dtype=float32), 1.0176735]. 
=============================================
[2019-04-04 08:41:14,463] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7465367e-13 2.1864705e-13 1.8671710e-29 2.3944149e-14 2.3697261e-14
 3.2223751e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:14,466] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9199
[2019-04-04 08:41:14,514] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 98.0, 95.0, 0.0, 26.0, 25.00574518388384, 0.4794721511719577, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [14.4, 97.33333333333334, 96.0, 0.0, 26.0, 25.04367020509613, 0.4824698167974957, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 0.9733333333333334, 0.32, 0.0, 0.6666666666666666, 0.5869725170913442, 0.6608232722658319, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.00825231], dtype=float32), 0.057858795]. 
=============================================
[2019-04-04 08:41:14,709] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172000, global step 2750959: loss 13.3589
[2019-04-04 08:41:14,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172000, global step 2750959: learning rate 0.0000
[2019-04-04 08:41:14,944] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172000, global step 2751086: loss 13.0980
[2019-04-04 08:41:14,947] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172000, global step 2751087: learning rate 0.0000
[2019-04-04 08:41:15,894] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172000, global step 2751612: loss 12.5701
[2019-04-04 08:41:15,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172000, global step 2751612: learning rate 0.0000
[2019-04-04 08:41:16,067] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172000, global step 2751704: loss 12.4360
[2019-04-04 08:41:16,090] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172000, global step 2751710: learning rate 0.0000
[2019-04-04 08:41:17,278] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172000, global step 2752343: loss 11.8635
[2019-04-04 08:41:17,280] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172000, global step 2752343: learning rate 0.0000
[2019-04-04 08:41:19,819] A3C_AGENT_WORKER-Thread-2 INFO:Local step 172500, global step 2753669: loss 0.0030
[2019-04-04 08:41:19,821] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 172500, global step 2753669: learning rate 0.0000
[2019-04-04 08:41:22,387] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172000, global step 2754941: loss 11.1430
[2019-04-04 08:41:22,387] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172000, global step 2754942: learning rate 0.0000
[2019-04-04 08:41:22,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0647443e-11 1.3981392e-11 1.6688865e-24 6.0609444e-13 5.3282064e-13
 1.3297720e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:22,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0463
[2019-04-04 08:41:22,536] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28457464495722, 0.4981197988760955, 0.0, 1.0, 40775.94494718486], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1382400.0000, 
sim time next is 1383000.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.27586230230212, 0.4931913466793625, 0.0, 1.0, 40592.40545103702], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6063218585251766, 0.6643971155597875, 0.0, 1.0, 0.193297168814462], 
reward next is 0.8067, 
noisyNet noise sample is [array([-1.4614222], dtype=float32), -1.2431316]. 
=============================================
[2019-04-04 08:41:22,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.55386 ]
 [84.54251 ]
 [84.574844]
 [84.615234]
 [84.57558 ]], R is [[84.49142456]
 [84.4523468 ]
 [84.41268158]
 [84.37245941]
 [84.33163452]].
[2019-04-04 08:41:23,094] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172000, global step 2755274: loss 11.0597
[2019-04-04 08:41:23,098] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172000, global step 2755275: learning rate 0.0000
[2019-04-04 08:41:25,360] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172000, global step 2756362: loss 10.5555
[2019-04-04 08:41:25,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172000, global step 2756362: learning rate 0.0000
[2019-04-04 08:41:26,641] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172000, global step 2756913: loss 10.2901
[2019-04-04 08:41:26,643] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172000, global step 2756914: learning rate 0.0000
[2019-04-04 08:41:27,439] A3C_AGENT_WORKER-Thread-12 INFO:Local step 172500, global step 2757230: loss 0.0085
[2019-04-04 08:41:27,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 172500, global step 2757230: learning rate 0.0000
[2019-04-04 08:41:28,038] A3C_AGENT_WORKER-Thread-5 INFO:Local step 172500, global step 2757525: loss 0.0113
[2019-04-04 08:41:28,042] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 172500, global step 2757525: learning rate 0.0000
[2019-04-04 08:41:28,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6848842e-13 1.6207496e-12 1.3749465e-26 1.5569119e-13 1.8070787e-14
 1.8414569e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:28,882] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5582
[2019-04-04 08:41:28,892] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.8, 51.33333333333334, 76.66666666666667, 508.8333333333334, 26.0, 26.4355068260707, 0.7178440016166582, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1524000.0000, 
sim time next is 1524600.0000, 
raw observation next is [11.9, 51.0, 77.0, 478.0, 26.0, 26.6460720176742, 0.7563460146234018, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.7922437673130196, 0.51, 0.25666666666666665, 0.5281767955801105, 0.6666666666666666, 0.72050600147285, 0.7521153382078006, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.56675446], dtype=float32), 0.2783203]. 
=============================================
[2019-04-04 08:41:29,868] A3C_AGENT_WORKER-Thread-17 INFO:Local step 172500, global step 2758330: loss 0.0066
[2019-04-04 08:41:29,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 172500, global step 2758330: learning rate 0.0000
[2019-04-04 08:41:30,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3394369e-12 1.0179958e-11 7.1939609e-26 3.6254008e-13 8.6432516e-14
 7.2025922e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:30,279] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6569
[2019-04-04 08:41:30,291] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.41666666666667, 58.33333333333334, 30.33333333333333, 13.33333333333333, 26.0, 26.76207434458045, 0.7309271535362023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1530600.0000, 
sim time next is 1531200.0000, 
raw observation next is [10.33333333333333, 58.66666666666667, 0.0, 0.0, 26.0, 26.91343028335547, 0.6978328815122944, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7488457987072946, 0.5866666666666667, 0.0, 0.0, 0.6666666666666666, 0.7427858569462892, 0.7326109605040981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1720951], dtype=float32), -2.0030801]. 
=============================================
[2019-04-04 08:41:30,941] A3C_AGENT_WORKER-Thread-16 INFO:Local step 172500, global step 2758882: loss 0.0025
[2019-04-04 08:41:30,943] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 172500, global step 2758882: learning rate 0.0000
[2019-04-04 08:41:31,265] A3C_AGENT_WORKER-Thread-15 INFO:Local step 172500, global step 2759053: loss 0.0034
[2019-04-04 08:41:31,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 172500, global step 2759053: learning rate 0.0000
[2019-04-04 08:41:31,754] A3C_AGENT_WORKER-Thread-20 INFO:Local step 172500, global step 2759284: loss 0.0051
[2019-04-04 08:41:31,755] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 172500, global step 2759284: learning rate 0.0000
[2019-04-04 08:41:32,444] A3C_AGENT_WORKER-Thread-3 INFO:Local step 172500, global step 2759640: loss 0.0095
[2019-04-04 08:41:32,445] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 172500, global step 2759640: learning rate 0.0000
[2019-04-04 08:41:32,582] A3C_AGENT_WORKER-Thread-18 INFO:Local step 172500, global step 2759709: loss 0.0075
[2019-04-04 08:41:32,583] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 172500, global step 2759710: learning rate 0.0000
[2019-04-04 08:41:33,429] A3C_AGENT_WORKER-Thread-11 INFO:Local step 172500, global step 2760124: loss 0.0077
[2019-04-04 08:41:33,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 172500, global step 2760125: learning rate 0.0000
[2019-04-04 08:41:33,979] A3C_AGENT_WORKER-Thread-19 INFO:Local step 172500, global step 2760397: loss 0.0169
[2019-04-04 08:41:33,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 172500, global step 2760398: learning rate 0.0000
[2019-04-04 08:41:34,849] A3C_AGENT_WORKER-Thread-6 INFO:Local step 172500, global step 2760804: loss 0.0075
[2019-04-04 08:41:34,854] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 172500, global step 2760805: learning rate 0.0000
[2019-04-04 08:41:34,983] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.3881969e-13 2.0922646e-11 2.3710149e-25 6.7548332e-13 1.3518379e-13
 1.0826116e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:34,986] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8172
[2019-04-04 08:41:35,068] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 87.33333333333334, 104.6666666666667, 0.0, 26.0, 24.88493441504368, 0.4216428668331084, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1687800.0000, 
sim time next is 1688400.0000, 
raw observation next is [1.1, 88.0, 103.5, 0.0, 26.0, 24.19128551497719, 0.4096227243802243, 1.0, 1.0, 197185.7549351849], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.88, 0.345, 0.0, 0.6666666666666666, 0.5159404595814324, 0.6365409081267415, 1.0, 1.0, 0.9389797854056424], 
reward next is 0.0610, 
noisyNet noise sample is [array([-0.19869427], dtype=float32), -0.37807807]. 
=============================================
[2019-04-04 08:41:38,775] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173000, global step 2762714: loss 6.6190
[2019-04-04 08:41:38,776] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173000, global step 2762714: learning rate 0.0000
[2019-04-04 08:41:39,879] A3C_AGENT_WORKER-Thread-4 INFO:Local step 172500, global step 2763174: loss 0.0396
[2019-04-04 08:41:39,880] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 172500, global step 2763174: learning rate 0.0000
[2019-04-04 08:41:41,129] A3C_AGENT_WORKER-Thread-14 INFO:Local step 172500, global step 2763628: loss 0.0615
[2019-04-04 08:41:41,136] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 172500, global step 2763628: learning rate 0.0000
[2019-04-04 08:41:43,054] A3C_AGENT_WORKER-Thread-10 INFO:Local step 172500, global step 2764344: loss 0.1202
[2019-04-04 08:41:43,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 172500, global step 2764344: learning rate 0.0000
[2019-04-04 08:41:44,179] A3C_AGENT_WORKER-Thread-13 INFO:Local step 172500, global step 2764809: loss 0.1375
[2019-04-04 08:41:44,182] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 172500, global step 2764811: learning rate 0.0000
[2019-04-04 08:41:46,578] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173000, global step 2765678: loss 6.6498
[2019-04-04 08:41:46,579] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173000, global step 2765678: learning rate 0.0000
[2019-04-04 08:41:47,351] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173000, global step 2765899: loss 6.6543
[2019-04-04 08:41:47,366] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173000, global step 2765899: learning rate 0.0000
[2019-04-04 08:41:49,609] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173000, global step 2766637: loss 6.5611
[2019-04-04 08:41:49,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173000, global step 2766637: learning rate 0.0000
[2019-04-04 08:41:50,496] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173000, global step 2766956: loss 6.5643
[2019-04-04 08:41:50,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173000, global step 2766956: learning rate 0.0000
[2019-04-04 08:41:51,405] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173000, global step 2767264: loss 6.5682
[2019-04-04 08:41:51,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173000, global step 2767265: learning rate 0.0000
[2019-04-04 08:41:51,571] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173000, global step 2767335: loss 6.5550
[2019-04-04 08:41:51,572] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173000, global step 2767335: learning rate 0.0000
[2019-04-04 08:41:51,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5521235e-09 4.8426054e-09 1.0413566e-21 2.8565555e-10 1.0553976e-10
 1.1947172e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:51,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5907
[2019-04-04 08:41:52,009] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 26.0, 24.1707305896439, 0.1630856528805252, 0.0, 1.0, 161283.0144685804], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1845600.0000, 
sim time next is 1846200.0000, 
raw observation next is [-6.7, 78.0, 67.33333333333333, 31.33333333333333, 26.0, 24.63826088208528, 0.2261688013602329, 0.0, 1.0, 84617.47002491], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.22444444444444442, 0.034622467771639034, 0.6666666666666666, 0.55318840684044, 0.575389600453411, 0.0, 1.0, 0.40294033345195235], 
reward next is 0.5971, 
noisyNet noise sample is [array([0.9093969], dtype=float32), 0.7732114]. 
=============================================
[2019-04-04 08:41:52,281] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173000, global step 2767572: loss 6.5113
[2019-04-04 08:41:52,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173000, global step 2767572: learning rate 0.0000
[2019-04-04 08:41:52,505] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173000, global step 2767646: loss 6.5049
[2019-04-04 08:41:52,506] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173000, global step 2767646: learning rate 0.0000
[2019-04-04 08:41:52,569] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173000, global step 2767666: loss 6.4789
[2019-04-04 08:41:52,570] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173000, global step 2767666: learning rate 0.0000
[2019-04-04 08:41:53,435] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173000, global step 2767910: loss 6.4209
[2019-04-04 08:41:53,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173000, global step 2767910: learning rate 0.0000
[2019-04-04 08:41:55,246] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173000, global step 2768338: loss 6.2562
[2019-04-04 08:41:55,255] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173000, global step 2768338: learning rate 0.0000
[2019-04-04 08:41:57,447] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9652291e-11 7.8537371e-11 1.9492078e-23 4.1675634e-12 1.7137509e-12
 6.2081057e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:41:57,447] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9749
[2019-04-04 08:41:57,532] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 75.0, 0.0, 0.0, 26.0, 25.13657335432288, 0.3018531392277857, 1.0, 1.0, 82043.39268657524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1967400.0000, 
sim time next is 1968000.0000, 
raw observation next is [-4.666666666666667, 73.66666666666666, 0.0, 0.0, 26.0, 24.98143490055637, 0.3233261801785467, 1.0, 1.0, 148403.9682616459], 
processed observation next is [1.0, 0.782608695652174, 0.3333333333333333, 0.7366666666666666, 0.0, 0.0, 0.6666666666666666, 0.5817862417130307, 0.6077753933928489, 1.0, 1.0, 0.7066855631506948], 
reward next is 0.2933, 
noisyNet noise sample is [array([-0.0435966], dtype=float32), 0.7155442]. 
=============================================
[2019-04-04 08:41:57,548] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.1792  ]
 [80.95985 ]
 [80.851906]
 [80.75773 ]
 [80.72045 ]], R is [[81.1397171 ]
 [80.93763733]
 [80.98812866]
 [81.02955627]
 [81.05848694]].
[2019-04-04 08:42:01,180] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173000, global step 2770140: loss 6.1988
[2019-04-04 08:42:01,213] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173000, global step 2770140: learning rate 0.0000
[2019-04-04 08:42:02,275] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173000, global step 2770443: loss 6.1763
[2019-04-04 08:42:02,276] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173000, global step 2770443: learning rate 0.0000
[2019-04-04 08:42:03,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.2980566e-11 1.5977314e-10 1.3878431e-23 6.4879031e-12 8.3005295e-12
 2.7247771e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:42:03,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0960
[2019-04-04 08:42:03,914] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.36584199868301, 0.1616941210419964, 0.0, 1.0, 42216.17268505218], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1989600.0000, 
sim time next is 1990200.0000, 
raw observation next is [-6.1, 86.33333333333333, 0.0, 0.0, 26.0, 24.34979759879714, 0.1584151576461038, 0.0, 1.0, 42109.4935449344], 
processed observation next is [1.0, 0.0, 0.29362880886426596, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5291497998997615, 0.5528050525487013, 0.0, 1.0, 0.20052139783302095], 
reward next is 0.7995, 
noisyNet noise sample is [array([-1.3688343], dtype=float32), 0.2650442]. 
=============================================
[2019-04-04 08:42:05,151] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173000, global step 2771223: loss 6.0777
[2019-04-04 08:42:05,156] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173000, global step 2771223: learning rate 0.0000
[2019-04-04 08:42:05,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.21041466e-11 1.82480225e-10 4.08375107e-24 1.24050804e-11
 4.54776841e-12 2.57120476e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:42:05,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8865
[2019-04-04 08:42:05,565] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 83.0, 0.0, 0.0, 26.0, 24.74841197852467, 0.2414485010030502, 0.0, 1.0, 42987.69677334322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1983000.0000, 
sim time next is 1983600.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.69257264886767, 0.2316151000703044, 0.0, 1.0, 43000.31830018537], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5577143874056393, 0.5772050333567681, 0.0, 1.0, 0.2047634204770732], 
reward next is 0.7952, 
noisyNet noise sample is [array([-0.9949319], dtype=float32), 1.6463174]. 
=============================================
[2019-04-04 08:42:06,497] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173000, global step 2771639: loss 5.9570
[2019-04-04 08:42:06,499] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173000, global step 2771639: learning rate 0.0000
[2019-04-04 08:42:07,620] A3C_AGENT_WORKER-Thread-2 INFO:Local step 173500, global step 2771965: loss 15.9390
[2019-04-04 08:42:07,621] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 173500, global step 2771965: learning rate 0.0000
[2019-04-04 08:42:14,025] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4909197e-11 4.2570485e-11 1.1746071e-24 1.3356790e-12 4.7106449e-13
 1.4640834e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:42:14,028] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0336
[2019-04-04 08:42:14,093] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 66.5, 86.0, 0.0, 26.0, 26.23126169473126, 0.483208446574009, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2129400.0000, 
sim time next is 2130000.0000, 
raw observation next is [-4.666666666666667, 66.0, 76.0, 0.0, 26.0, 26.19650846487978, 0.3698032426078819, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3333333333333333, 0.66, 0.25333333333333335, 0.0, 0.6666666666666666, 0.6830423720733151, 0.6232677475359606, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1444725], dtype=float32), 1.0472157]. 
=============================================
[2019-04-04 08:42:14,109] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[83.394806]
 [83.15485 ]
 [83.088806]
 [82.96926 ]
 [82.87604 ]], R is [[83.53466797]
 [83.69932556]
 [83.86233521]
 [84.02371216]
 [84.18347931]].
[2019-04-04 08:42:14,733] A3C_AGENT_WORKER-Thread-12 INFO:Local step 173500, global step 2773921: loss 15.1912
[2019-04-04 08:42:14,734] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 173500, global step 2773921: learning rate 0.0000
[2019-04-04 08:42:16,032] A3C_AGENT_WORKER-Thread-5 INFO:Local step 173500, global step 2774323: loss 15.2796
[2019-04-04 08:42:16,056] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 173500, global step 2774323: learning rate 0.0000
[2019-04-04 08:42:17,533] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.0793913e-12 4.7111169e-11 5.8829799e-24 2.8899105e-12 3.8560553e-13
 3.6624697e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:42:17,533] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6305
[2019-04-04 08:42:17,561] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 64.0, 222.0, 117.5, 26.0, 25.92678619073176, 0.43505104424295, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2116800.0000, 
sim time next is 2117400.0000, 
raw observation next is [-6.616666666666667, 64.0, 198.3333333333333, 123.0, 26.0, 25.90835749314255, 0.4272028735543922, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.2793167128347184, 0.64, 0.661111111111111, 0.13591160220994475, 0.6666666666666666, 0.6590297910952124, 0.6424009578514641, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.516151], dtype=float32), -0.022357466]. 
=============================================
[2019-04-04 08:42:18,189] A3C_AGENT_WORKER-Thread-17 INFO:Local step 173500, global step 2774958: loss 15.4651
[2019-04-04 08:42:18,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 173500, global step 2774958: learning rate 0.0000
[2019-04-04 08:42:19,617] A3C_AGENT_WORKER-Thread-16 INFO:Local step 173500, global step 2775332: loss 16.0209
[2019-04-04 08:42:19,619] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 173500, global step 2775332: learning rate 0.0000
[2019-04-04 08:42:20,392] A3C_AGENT_WORKER-Thread-15 INFO:Local step 173500, global step 2775543: loss 16.2689
[2019-04-04 08:42:20,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 173500, global step 2775543: learning rate 0.0000
[2019-04-04 08:42:20,996] A3C_AGENT_WORKER-Thread-11 INFO:Local step 173500, global step 2775694: loss 16.3735
[2019-04-04 08:42:21,001] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 173500, global step 2775695: learning rate 0.0000
[2019-04-04 08:42:21,085] A3C_AGENT_WORKER-Thread-20 INFO:Local step 173500, global step 2775716: loss 16.4954
[2019-04-04 08:42:21,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 173500, global step 2775716: learning rate 0.0000
[2019-04-04 08:42:21,241] A3C_AGENT_WORKER-Thread-18 INFO:Local step 173500, global step 2775757: loss 16.7007
[2019-04-04 08:42:21,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 173500, global step 2775757: learning rate 0.0000
[2019-04-04 08:42:21,965] A3C_AGENT_WORKER-Thread-3 INFO:Local step 173500, global step 2775980: loss 16.4394
[2019-04-04 08:42:21,966] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 173500, global step 2775980: learning rate 0.0000
[2019-04-04 08:42:22,631] A3C_AGENT_WORKER-Thread-19 INFO:Local step 173500, global step 2776186: loss 16.4317
[2019-04-04 08:42:22,641] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 173500, global step 2776186: learning rate 0.0000
[2019-04-04 08:42:23,528] A3C_AGENT_WORKER-Thread-6 INFO:Local step 173500, global step 2776473: loss 16.2771
[2019-04-04 08:42:23,528] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 173500, global step 2776474: learning rate 0.0000
[2019-04-04 08:42:30,357] A3C_AGENT_WORKER-Thread-4 INFO:Local step 173500, global step 2778411: loss 14.4957
[2019-04-04 08:42:30,358] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 173500, global step 2778411: learning rate 0.0000
[2019-04-04 08:42:31,062] A3C_AGENT_WORKER-Thread-14 INFO:Local step 173500, global step 2778634: loss 14.7628
[2019-04-04 08:42:31,063] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 173500, global step 2778634: learning rate 0.0000
[2019-04-04 08:42:33,812] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174000, global step 2779519: loss 6.9216
[2019-04-04 08:42:33,825] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174000, global step 2779519: learning rate 0.0000
[2019-04-04 08:42:34,181] A3C_AGENT_WORKER-Thread-10 INFO:Local step 173500, global step 2779631: loss 15.3870
[2019-04-04 08:42:34,182] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 173500, global step 2779631: learning rate 0.0000
[2019-04-04 08:42:35,524] A3C_AGENT_WORKER-Thread-13 INFO:Local step 173500, global step 2780015: loss 15.4739
[2019-04-04 08:42:35,538] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 173500, global step 2780016: learning rate 0.0000
[2019-04-04 08:42:37,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6350239e-10 1.6344051e-09 1.1404384e-22 7.7609356e-11 8.3069482e-11
 3.5561338e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:42:37,962] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3485
[2019-04-04 08:42:37,991] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.60546972680335, 0.2032493892234127, 0.0, 1.0, 39533.57893524713], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2343600.0000, 
sim time next is 2344200.0000, 
raw observation next is [-2.383333333333333, 62.5, 0.0, 0.0, 26.0, 24.56845591603492, 0.1962069693745433, 0.0, 1.0, 39645.55916907276], 
processed observation next is [0.0, 0.13043478260869565, 0.3965835641735919, 0.625, 0.0, 0.0, 0.6666666666666666, 0.5473713263362434, 0.5654023231248478, 0.0, 1.0, 0.18878837699558457], 
reward next is 0.8112, 
noisyNet noise sample is [array([-0.66043204], dtype=float32), 1.4521083]. 
=============================================
[2019-04-04 08:42:39,604] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174000, global step 2781510: loss 6.8845
[2019-04-04 08:42:39,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174000, global step 2781510: learning rate 0.0000
[2019-04-04 08:42:40,838] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174000, global step 2781844: loss 6.8811
[2019-04-04 08:42:40,840] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174000, global step 2781844: learning rate 0.0000
[2019-04-04 08:42:43,090] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174000, global step 2782544: loss 6.9005
[2019-04-04 08:42:43,091] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174000, global step 2782544: learning rate 0.0000
[2019-04-04 08:42:44,287] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174000, global step 2783020: loss 6.9117
[2019-04-04 08:42:44,288] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174000, global step 2783020: learning rate 0.0000
[2019-04-04 08:42:44,326] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174000, global step 2783036: loss 6.9050
[2019-04-04 08:42:44,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174000, global step 2783036: learning rate 0.0000
[2019-04-04 08:42:44,841] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174000, global step 2783234: loss 6.8913
[2019-04-04 08:42:44,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174000, global step 2783234: learning rate 0.0000
[2019-04-04 08:42:45,409] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174000, global step 2783468: loss 6.8683
[2019-04-04 08:42:45,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174000, global step 2783468: learning rate 0.0000
[2019-04-04 08:42:45,773] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174000, global step 2783605: loss 6.8453
[2019-04-04 08:42:45,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174000, global step 2783609: learning rate 0.0000
[2019-04-04 08:42:46,408] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174000, global step 2783821: loss 6.8453
[2019-04-04 08:42:46,416] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174000, global step 2783823: learning rate 0.0000
[2019-04-04 08:42:46,505] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174000, global step 2783851: loss 6.8512
[2019-04-04 08:42:46,508] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174000, global step 2783852: learning rate 0.0000
[2019-04-04 08:42:48,388] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174000, global step 2784516: loss 6.6024
[2019-04-04 08:42:48,390] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174000, global step 2784516: learning rate 0.0000
[2019-04-04 08:42:53,497] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174000, global step 2786628: loss 6.4958
[2019-04-04 08:42:53,508] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174000, global step 2786628: learning rate 0.0000
[2019-04-04 08:42:53,588] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174000, global step 2786665: loss 6.5289
[2019-04-04 08:42:53,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174000, global step 2786666: learning rate 0.0000
[2019-04-04 08:42:53,798] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7754491e-12 2.2138418e-11 2.5213821e-25 1.2630817e-12 1.1488754e-13
 7.5504120e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:42:53,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2002
[2019-04-04 08:42:53,812] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.966666666666667, 31.0, 17.5, 31.16666666666666, 26.0, 25.85920561468964, 0.3546824000799946, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2568000.0000, 
sim time next is 2568600.0000, 
raw observation next is [1.6, 32.0, 0.0, 0.0, 26.0, 25.66008080435066, 0.174769518270306, 1.0, 1.0, 9340.205835115268], 
processed observation next is [1.0, 0.7391304347826086, 0.5069252077562327, 0.32, 0.0, 0.0, 0.6666666666666666, 0.6383400670292216, 0.558256506090102, 1.0, 1.0, 0.04447717064340604], 
reward next is 0.9555, 
noisyNet noise sample is [array([1.6072402], dtype=float32), -0.39409146]. 
=============================================
[2019-04-04 08:42:55,408] A3C_AGENT_WORKER-Thread-2 INFO:Local step 174500, global step 2787376: loss 35.0702
[2019-04-04 08:42:55,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 174500, global step 2787376: learning rate 0.0000
[2019-04-04 08:42:55,769] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.5891149e-10 3.5288622e-10 1.3476046e-22 6.6914550e-11 1.5788724e-11
 2.0699704e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:42:55,770] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8020
[2019-04-04 08:42:55,825] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 81.33333333333333, 0.0, 0.0, 26.0, 24.38448523501291, 0.1263432343656406, 0.0, 1.0, 42791.72891428861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2608800.0000, 
sim time next is 2609400.0000, 
raw observation next is [-6.1, 82.16666666666667, 0.0, 0.0, 26.0, 24.35870407890724, 0.1147649715364045, 0.0, 1.0, 42884.56055438859], 
processed observation next is [1.0, 0.17391304347826086, 0.29362880886426596, 0.8216666666666668, 0.0, 0.0, 0.6666666666666666, 0.5298920065756034, 0.5382549905121349, 0.0, 1.0, 0.20421219311613614], 
reward next is 0.7958, 
noisyNet noise sample is [array([-0.6115304], dtype=float32), -0.28958097]. 
=============================================
[2019-04-04 08:42:56,035] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.7696352e-12 1.1798562e-10 1.7075575e-24 2.1222277e-11 5.6980869e-13
 4.6338615e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:42:56,038] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3147
[2019-04-04 08:42:56,084] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8666666666666667, 34.0, 0.0, 0.0, 26.0, 24.79874766844122, 0.3250461624317785, 1.0, 1.0, 196402.2554811637], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2569800.0000, 
sim time next is 2570400.0000, 
raw observation next is [0.5, 35.0, 0.0, 0.0, 26.0, 25.01429494280535, 0.3387048299137259, 1.0, 1.0, 66824.1888511687], 
processed observation next is [1.0, 0.782608695652174, 0.4764542936288089, 0.35, 0.0, 0.0, 0.6666666666666666, 0.5845245785671125, 0.612901609971242, 1.0, 1.0, 0.31821042310080333], 
reward next is 0.6818, 
noisyNet noise sample is [array([0.5657281], dtype=float32), -0.7432163]. 
=============================================
[2019-04-04 08:42:56,501] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174000, global step 2787797: loss 6.7106
[2019-04-04 08:42:56,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174000, global step 2787797: learning rate 0.0000
[2019-04-04 08:42:58,105] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174000, global step 2788397: loss 6.8376
[2019-04-04 08:42:58,108] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174000, global step 2788399: learning rate 0.0000
[2019-04-04 08:43:00,446] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7529619e-10 2.1118967e-09 1.4110074e-21 1.3197668e-10 8.0362883e-11
 8.3829894e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:00,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6663
[2019-04-04 08:43:00,477] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 88.33333333333334, 0.0, 0.0, 26.0, 23.42010681963442, -0.02414641470089889, 0.0, 1.0, 44413.45477428773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2694000.0000, 
sim time next is 2694600.0000, 
raw observation next is [-15.0, 87.0, 0.0, 0.0, 26.0, 23.40160163833825, -0.03562714537551242, 0.0, 1.0, 44318.26056192593], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.87, 0.0, 0.0, 0.6666666666666666, 0.45013346986152075, 0.48812428487482923, 0.0, 1.0, 0.21103933600917107], 
reward next is 0.7890, 
noisyNet noise sample is [array([-0.18954349], dtype=float32), 1.2705444]. 
=============================================
[2019-04-04 08:43:01,141] A3C_AGENT_WORKER-Thread-12 INFO:Local step 174500, global step 2789498: loss 34.1821
[2019-04-04 08:43:01,144] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 174500, global step 2789498: learning rate 0.0000
[2019-04-04 08:43:02,574] A3C_AGENT_WORKER-Thread-5 INFO:Local step 174500, global step 2790077: loss 33.1854
[2019-04-04 08:43:02,576] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 174500, global step 2790077: learning rate 0.0000
[2019-04-04 08:43:02,794] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3001220e-09 8.9921777e-09 6.7668891e-21 4.3256237e-10 1.3022108e-10
 1.7218995e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:02,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8845
[2019-04-04 08:43:02,906] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.65827632632871, -0.1114806781234396, 0.0, 1.0, 202379.7707628406], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2704800.0000, 
sim time next is 2705400.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 23.15334610897002, -0.00885021965184333, 1.0, 1.0, 203105.2441193045], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.42944550908083495, 0.49704992678271886, 1.0, 1.0, 0.9671678291395452], 
reward next is 0.0328, 
noisyNet noise sample is [array([0.48616248], dtype=float32), 0.76021296]. 
=============================================
[2019-04-04 08:43:04,455] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.2255341e-10 1.4568120e-09 6.6624577e-22 1.2416947e-10 6.0767974e-11
 1.3140654e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:04,456] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9634
[2019-04-04 08:43:04,473] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.49614079522166, 0.1591109708053124, 0.0, 1.0, 40748.17288898793], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2778600.0000, 
sim time next is 2779200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.46090830261587, 0.1527962030780163, 0.0, 1.0, 40753.45255464116], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5384090252179892, 0.5509320676926721, 0.0, 1.0, 0.19406405978400554], 
reward next is 0.8059, 
noisyNet noise sample is [array([-0.03747278], dtype=float32), 1.3788627]. 
=============================================
[2019-04-04 08:43:04,933] A3C_AGENT_WORKER-Thread-17 INFO:Local step 174500, global step 2790962: loss 34.1919
[2019-04-04 08:43:04,935] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 174500, global step 2790962: learning rate 0.0000
[2019-04-04 08:43:05,848] A3C_AGENT_WORKER-Thread-11 INFO:Local step 174500, global step 2791240: loss 34.7501
[2019-04-04 08:43:05,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 174500, global step 2791240: learning rate 0.0000
[2019-04-04 08:43:06,357] A3C_AGENT_WORKER-Thread-16 INFO:Local step 174500, global step 2791417: loss 34.6507
[2019-04-04 08:43:06,357] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 174500, global step 2791417: learning rate 0.0000
[2019-04-04 08:43:06,457] A3C_AGENT_WORKER-Thread-15 INFO:Local step 174500, global step 2791452: loss 34.9667
[2019-04-04 08:43:06,459] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 174500, global step 2791452: learning rate 0.0000
[2019-04-04 08:43:06,756] A3C_AGENT_WORKER-Thread-20 INFO:Local step 174500, global step 2791557: loss 34.3731
[2019-04-04 08:43:06,764] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 174500, global step 2791557: learning rate 0.0000
[2019-04-04 08:43:07,051] A3C_AGENT_WORKER-Thread-18 INFO:Local step 174500, global step 2791662: loss 34.3876
[2019-04-04 08:43:07,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 174500, global step 2791662: learning rate 0.0000
[2019-04-04 08:43:08,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3464692e-11 8.1421758e-11 8.4144196e-24 3.7861654e-12 1.3297715e-12
 2.0953325e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:08,034] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6946
[2019-04-04 08:43:08,049] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.99813204504273, 0.3655692366369291, 0.0, 1.0, 43464.76609509136], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2935800.0000, 
sim time next is 2936400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.96170169489411, 0.3695505450696226, 0.0, 1.0, 43424.40573680677], 
processed observation next is [1.0, 1.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5801418079078425, 0.6231835150232076, 0.0, 1.0, 0.20678288446098464], 
reward next is 0.7932, 
noisyNet noise sample is [array([-1.5011064], dtype=float32), 0.122901194]. 
=============================================
[2019-04-04 08:43:08,100] A3C_AGENT_WORKER-Thread-3 INFO:Local step 174500, global step 2792138: loss 33.9786
[2019-04-04 08:43:08,101] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 174500, global step 2792138: learning rate 0.0000
[2019-04-04 08:43:08,287] A3C_AGENT_WORKER-Thread-19 INFO:Local step 174500, global step 2792213: loss 33.7434
[2019-04-04 08:43:08,288] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 174500, global step 2792213: learning rate 0.0000
[2019-04-04 08:43:08,803] A3C_AGENT_WORKER-Thread-6 INFO:Local step 174500, global step 2792416: loss 33.7736
[2019-04-04 08:43:08,803] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 174500, global step 2792416: learning rate 0.0000
[2019-04-04 08:43:10,746] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5333117e-13 6.7586002e-12 1.0607193e-25 3.4120363e-13 6.0514229e-14
 1.6124397e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:10,747] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2165
[2019-04-04 08:43:10,768] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.666666666666667, 33.33333333333334, 237.1666666666667, 258.3333333333333, 26.0, 25.86512982116034, 0.4808116166700439, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2812800.0000, 
sim time next is 2813400.0000, 
raw observation next is [5.0, 32.5, 249.0, 173.0, 26.0, 25.97315907993863, 0.4756492130876544, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.6011080332409973, 0.325, 0.83, 0.19116022099447513, 0.6666666666666666, 0.664429923328219, 0.6585497376958848, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0951954], dtype=float32), -0.18012352]. 
=============================================
[2019-04-04 08:43:14,929] A3C_AGENT_WORKER-Thread-4 INFO:Local step 174500, global step 2794808: loss 32.0443
[2019-04-04 08:43:14,929] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 174500, global step 2794808: learning rate 0.0000
[2019-04-04 08:43:15,027] A3C_AGENT_WORKER-Thread-14 INFO:Local step 174500, global step 2794828: loss 31.8767
[2019-04-04 08:43:15,029] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 174500, global step 2794828: learning rate 0.0000
[2019-04-04 08:43:15,581] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.0666142e-12 2.0248546e-11 2.6209639e-24 1.5617963e-12 4.1678713e-13
 2.4594474e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:15,581] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4571
[2019-04-04 08:43:15,616] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666666, 98.83333333333334, 58.66666666666666, 0.0, 26.0, 25.46394604930493, 0.3076971907968034, 1.0, 1.0, 18682.08334758304], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2886600.0000, 
sim time next is 2887200.0000, 
raw observation next is [0.0, 100.0, 63.5, 0.0, 26.0, 25.43695890936713, 0.3047936991307217, 1.0, 1.0, 18680.85235552142], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 1.0, 0.21166666666666667, 0.0, 0.6666666666666666, 0.6197465757805943, 0.6015978997102406, 1.0, 1.0, 0.08895643978819724], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.28540683], dtype=float32), 0.20370327]. 
=============================================
[2019-04-04 08:43:15,688] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175000, global step 2794987: loss 0.0243
[2019-04-04 08:43:15,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175000, global step 2794987: learning rate 0.0000
[2019-04-04 08:43:19,244] A3C_AGENT_WORKER-Thread-10 INFO:Local step 174500, global step 2795770: loss 32.4315
[2019-04-04 08:43:19,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 174500, global step 2795770: learning rate 0.0000
[2019-04-04 08:43:22,957] A3C_AGENT_WORKER-Thread-13 INFO:Local step 174500, global step 2796443: loss 32.8192
[2019-04-04 08:43:22,985] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 174500, global step 2796445: learning rate 0.0000
[2019-04-04 08:43:27,292] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175000, global step 2797251: loss 0.0231
[2019-04-04 08:43:27,292] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175000, global step 2797251: learning rate 0.0000
[2019-04-04 08:43:29,885] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175000, global step 2797783: loss 0.0225
[2019-04-04 08:43:29,886] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175000, global step 2797783: learning rate 0.0000
[2019-04-04 08:43:32,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8276296e-10 2.4552446e-10 8.7305168e-22 6.9096874e-11 1.1312504e-11
 1.1026984e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:32,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8016
[2019-04-04 08:43:32,966] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.04670453195752, 0.3205895278681672, 0.0, 1.0, 47609.16342866742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3003000.0000, 
sim time next is 3003600.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.01585084798057, 0.3201143538857718, 0.0, 1.0, 55982.0596810974], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5846542373317142, 0.6067047846285906, 0.0, 1.0, 0.2665812365766543], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.5791916], dtype=float32), -0.51512307]. 
=============================================
[2019-04-04 08:43:34,518] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175000, global step 2798672: loss 0.0129
[2019-04-04 08:43:34,528] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175000, global step 2798672: learning rate 0.0000
[2019-04-04 08:43:35,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.04287260e-12 1.18622785e-11 1.28924598e-25 4.18037892e-13
 3.59297769e-13 1.16240788e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:43:35,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4702
[2019-04-04 08:43:35,815] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 88.33333333333334, 0.0, 26.0, 25.44156025167431, 0.4167284059185223, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2904600.0000, 
sim time next is 2905200.0000, 
raw observation next is [2.0, 100.0, 87.5, 0.0, 26.0, 25.66150895762087, 0.4382999572098387, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.2916666666666667, 0.0, 0.6666666666666666, 0.6384590798017392, 0.6460999857366129, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0279799], dtype=float32), -1.2448081]. 
=============================================
[2019-04-04 08:43:36,919] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175000, global step 2799192: loss 0.0088
[2019-04-04 08:43:36,920] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175000, global step 2799192: learning rate 0.0000
[2019-04-04 08:43:37,404] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175000, global step 2799284: loss 0.0097
[2019-04-04 08:43:37,404] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175000, global step 2799284: learning rate 0.0000
[2019-04-04 08:43:37,543] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175000, global step 2799311: loss 0.0102
[2019-04-04 08:43:37,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175000, global step 2799311: learning rate 0.0000
[2019-04-04 08:43:38,085] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175000, global step 2799436: loss 0.0090
[2019-04-04 08:43:38,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175000, global step 2799436: learning rate 0.0000
[2019-04-04 08:43:39,107] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.1517139e-11 3.2216141e-10 2.9799861e-23 8.9967105e-12 4.1645042e-12
 6.7790270e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:43:39,108] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1694
[2019-04-04 08:43:39,272] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00089616684421, 0.4417484461020406, 0.0, 1.0, 101817.6854686568], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2925000.0000, 
sim time next is 2925600.0000, 
raw observation next is [-1.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.09565862037598, 0.4686543638795432, 0.0, 1.0, 64672.91056831117], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5913048850313315, 0.6562181212931811, 0.0, 1.0, 0.30796624080148177], 
reward next is 0.6920, 
noisyNet noise sample is [array([0.73763156], dtype=float32), -0.15380092]. 
=============================================
[2019-04-04 08:43:39,311] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175000, global step 2799713: loss 0.0050
[2019-04-04 08:43:39,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175000, global step 2799713: learning rate 0.0000
[2019-04-04 08:43:40,530] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175000, global step 2799972: loss 0.0055
[2019-04-04 08:43:40,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175000, global step 2799972: learning rate 0.0000
[2019-04-04 08:43:40,650] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 08:43:40,651] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:43:40,651] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:43:40,653] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run29
[2019-04-04 08:43:40,691] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:43:40,692] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:43:40,697] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run29
[2019-04-04 08:43:40,762] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:43:40,762] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:43:40,764] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run29
[2019-04-04 08:44:07,565] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.22112092], dtype=float32), -0.1757759]
[2019-04-04 08:44:07,565] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-10.82773286666667, 35.60261324333334, 73.42322731666667, 285.64509035, 26.0, 26.25776753031389, 0.4674328057340818, 1.0, 1.0, 0.0]
[2019-04-04 08:44:07,565] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:44:07,566] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.5086725e-11 1.3621672e-10 3.3864564e-22 2.0424763e-11 2.2484462e-12
 5.6361372e-14 1.0000000e+00], sampled 0.008951949524145353
[2019-04-04 08:46:43,150] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:47:16,497] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:47:25,670] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:47:26,716] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2800000, evaluation results [2800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:47:27,956] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175000, global step 2800215: loss 0.0039
[2019-04-04 08:47:27,958] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175000, global step 2800215: learning rate 0.0000
[2019-04-04 08:47:28,484] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175000, global step 2800333: loss 0.0031
[2019-04-04 08:47:28,558] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175000, global step 2800333: learning rate 0.0000
[2019-04-04 08:47:39,709] A3C_AGENT_WORKER-Thread-2 INFO:Local step 175500, global step 2802493: loss 0.4632
[2019-04-04 08:47:39,711] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 175500, global step 2802493: learning rate 0.0000
[2019-04-04 08:47:39,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5698652e-11 1.2308932e-09 3.7462782e-22 2.5158852e-11 1.3287554e-11
 6.5678305e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:47:39,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8132
[2019-04-04 08:47:39,936] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.45967893653649, 0.5421454251278937, 1.0, 1.0, 46869.58616582953], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3348000.0000, 
sim time next is 3348600.0000, 
raw observation next is [-3.0, 55.0, 0.0, 0.0, 26.0, 25.69027408019912, 0.5686778852446234, 1.0, 1.0, 31976.5163561593], 
processed observation next is [1.0, 0.782608695652174, 0.3795013850415513, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6408561733499267, 0.6895592950815411, 1.0, 1.0, 0.15226912550552046], 
reward next is 0.8477, 
noisyNet noise sample is [array([-1.3077065], dtype=float32), -1.713719]. 
=============================================
[2019-04-04 08:47:41,433] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175000, global step 2802949: loss 0.0179
[2019-04-04 08:47:41,433] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175000, global step 2802949: learning rate 0.0000
[2019-04-04 08:47:41,518] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175000, global step 2802964: loss 0.0171
[2019-04-04 08:47:41,546] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175000, global step 2802973: learning rate 0.0000
[2019-04-04 08:47:46,593] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175000, global step 2804049: loss 0.0197
[2019-04-04 08:47:46,598] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175000, global step 2804049: learning rate 0.0000
[2019-04-04 08:47:47,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3946153e-12 1.0247107e-11 3.3511243e-25 3.7448809e-13 1.4775506e-13
 6.8407985e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:47:47,297] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3068
[2019-04-04 08:47:47,359] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 15.0, 130.0, 26.0, 25.66659569103756, 0.5158100379164908, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3224400.0000, 
sim time next is 3225000.0000, 
raw observation next is [-3.0, 92.0, 29.0, 178.0, 26.0, 25.57961655384193, 0.5142344332204015, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3795013850415513, 0.92, 0.09666666666666666, 0.19668508287292819, 0.6666666666666666, 0.6316347128201608, 0.6714114777401338, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20443365], dtype=float32), 2.1105788]. 
=============================================
[2019-04-04 08:47:47,475] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[86.190254]
 [84.39476 ]
 [81.15949 ]
 [76.34149 ]
 [76.397285]], R is [[87.14559174]
 [87.2741394 ]
 [87.40139771]
 [87.5273819 ]
 [87.45631409]].
[2019-04-04 08:47:49,232] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175000, global step 2804636: loss 0.0146
[2019-04-04 08:47:49,233] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175000, global step 2804636: learning rate 0.0000
[2019-04-04 08:47:49,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9405096e-11 1.3425945e-10 1.9446338e-22 8.7896426e-12 2.1528814e-12
 6.7690821e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:47:49,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9474
[2019-04-04 08:47:49,974] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.15009625866716, 0.4828489440101838, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.31243677102204, 0.4626895121560047, 1.0, 1.0, 196883.5665705758], 
processed observation next is [1.0, 0.782608695652174, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.52603639758517, 0.6542298373853349, 1.0, 1.0, 0.937540793193218], 
reward next is 0.0625, 
noisyNet noise sample is [array([-1.8165468], dtype=float32), -0.44787532]. 
=============================================
[2019-04-04 08:47:50,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5920760e-11 2.4252658e-10 2.4812162e-22 2.4234706e-11 5.1816823e-12
 4.8588117e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:47:50,732] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0759
[2019-04-04 08:47:50,784] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87489639686132, 0.5971717201518153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 26.0, 25.79176387535801, 0.5753833251803645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6493136562798343, 0.6917944417267882, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7629026], dtype=float32), 0.20970447]. 
=============================================
[2019-04-04 08:47:52,256] A3C_AGENT_WORKER-Thread-12 INFO:Local step 175500, global step 2805356: loss 0.1226
[2019-04-04 08:47:52,257] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 175500, global step 2805356: learning rate 0.0000
[2019-04-04 08:47:53,268] A3C_AGENT_WORKER-Thread-5 INFO:Local step 175500, global step 2805615: loss 0.1203
[2019-04-04 08:47:53,305] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 175500, global step 2805616: learning rate 0.0000
[2019-04-04 08:47:56,816] A3C_AGENT_WORKER-Thread-17 INFO:Local step 175500, global step 2806388: loss 0.0868
[2019-04-04 08:47:56,849] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 175500, global step 2806392: learning rate 0.0000
[2019-04-04 08:48:00,590] A3C_AGENT_WORKER-Thread-15 INFO:Local step 175500, global step 2807200: loss 0.1067
[2019-04-04 08:48:00,592] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 175500, global step 2807200: learning rate 0.0000
[2019-04-04 08:48:00,643] A3C_AGENT_WORKER-Thread-11 INFO:Local step 175500, global step 2807205: loss 0.1108
[2019-04-04 08:48:00,643] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 175500, global step 2807205: learning rate 0.0000
[2019-04-04 08:48:01,181] A3C_AGENT_WORKER-Thread-16 INFO:Local step 175500, global step 2807366: loss 0.1065
[2019-04-04 08:48:01,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9877090e-12 3.1021782e-12 4.1602769e-25 4.4041902e-13 2.4884410e-13
 5.5626332e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:01,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3540
[2019-04-04 08:48:01,254] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.8, 99.5, 84.0, 679.0, 26.0, 26.80268871379635, 0.8941478132285486, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3166200.0000, 
sim time next is 3166800.0000, 
raw observation next is [6.733333333333333, 99.33333333333334, 79.66666666666666, 649.0, 26.0, 27.14119391127963, 0.9364839226548725, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.649122807017544, 0.9933333333333334, 0.26555555555555554, 0.7171270718232045, 0.6666666666666666, 0.7617661592733024, 0.8121613075516242, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9175966], dtype=float32), -1.4453164]. 
=============================================
[2019-04-04 08:48:01,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 175500, global step 2807366: learning rate 0.0000
[2019-04-04 08:48:01,772] A3C_AGENT_WORKER-Thread-18 INFO:Local step 175500, global step 2807495: loss 0.0929
[2019-04-04 08:48:01,773] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 175500, global step 2807495: learning rate 0.0000
[2019-04-04 08:48:02,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8322054e-11 6.6661301e-11 1.2639983e-22 6.2878838e-12 1.1642180e-12
 2.3159178e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:02,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1005
[2019-04-04 08:48:02,413] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 48.0, 60.0, 501.0, 26.0, 26.65521768856553, 0.6754748042776946, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3342600.0000, 
sim time next is 3343200.0000, 
raw observation next is [-2.0, 48.66666666666666, 51.83333333333334, 439.6666666666667, 26.0, 26.6025763045729, 0.5200568680441534, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.4866666666666666, 0.1727777777777778, 0.48581952117863725, 0.6666666666666666, 0.7168813587144083, 0.6733522893480511, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7133883], dtype=float32), -0.5207107]. 
=============================================
[2019-04-04 08:48:02,705] A3C_AGENT_WORKER-Thread-20 INFO:Local step 175500, global step 2807793: loss 0.1001
[2019-04-04 08:48:02,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 175500, global step 2807794: learning rate 0.0000
[2019-04-04 08:48:03,116] A3C_AGENT_WORKER-Thread-3 INFO:Local step 175500, global step 2807995: loss 0.0906
[2019-04-04 08:48:03,116] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 175500, global step 2807995: learning rate 0.0000
[2019-04-04 08:48:03,912] A3C_AGENT_WORKER-Thread-19 INFO:Local step 175500, global step 2808363: loss 0.0824
[2019-04-04 08:48:03,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 175500, global step 2808364: learning rate 0.0000
[2019-04-04 08:48:04,347] A3C_AGENT_WORKER-Thread-6 INFO:Local step 175500, global step 2808540: loss 0.0778
[2019-04-04 08:48:04,348] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 175500, global step 2808540: learning rate 0.0000
[2019-04-04 08:48:06,353] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.89720764e-11 2.83961604e-11 1.60027124e-22 1.00085305e-11
 2.54205663e-12 1.13440327e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:48:06,353] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2606
[2019-04-04 08:48:06,386] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 36.5, 317.0, 26.0, 26.45307797277955, 0.4663895889570895, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3430800.0000, 
sim time next is 3431400.0000, 
raw observation next is [2.0, 67.0, 28.33333333333333, 251.6666666666666, 26.0, 26.38382372072548, 0.596409898305238, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.67, 0.09444444444444443, 0.27808471454880285, 0.6666666666666666, 0.6986519767271234, 0.6988032994350794, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2551653], dtype=float32), 0.9890433]. 
=============================================
[2019-04-04 08:48:08,427] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176000, global step 2810186: loss 0.1121
[2019-04-04 08:48:08,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176000, global step 2810186: learning rate 0.0000
[2019-04-04 08:48:09,718] A3C_AGENT_WORKER-Thread-14 INFO:Local step 175500, global step 2810801: loss 0.0644
[2019-04-04 08:48:09,718] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 175500, global step 2810801: learning rate 0.0000
[2019-04-04 08:48:10,058] A3C_AGENT_WORKER-Thread-4 INFO:Local step 175500, global step 2810962: loss 0.0529
[2019-04-04 08:48:10,059] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 175500, global step 2810962: learning rate 0.0000
[2019-04-04 08:48:12,771] A3C_AGENT_WORKER-Thread-10 INFO:Local step 175500, global step 2812164: loss 0.0534
[2019-04-04 08:48:12,773] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 175500, global step 2812164: learning rate 0.0000
[2019-04-04 08:48:13,978] A3C_AGENT_WORKER-Thread-13 INFO:Local step 175500, global step 2812695: loss 0.0522
[2019-04-04 08:48:13,980] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 175500, global step 2812695: learning rate 0.0000
[2019-04-04 08:48:15,237] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176000, global step 2813312: loss 0.0976
[2019-04-04 08:48:15,254] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176000, global step 2813314: learning rate 0.0000
[2019-04-04 08:48:15,870] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176000, global step 2813592: loss 0.0941
[2019-04-04 08:48:15,872] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176000, global step 2813592: learning rate 0.0000
[2019-04-04 08:48:17,709] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176000, global step 2814418: loss 0.0989
[2019-04-04 08:48:17,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176000, global step 2814418: learning rate 0.0000
[2019-04-04 08:48:18,946] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176000, global step 2814960: loss 0.0982
[2019-04-04 08:48:18,952] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176000, global step 2814961: learning rate 0.0000
[2019-04-04 08:48:19,337] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176000, global step 2815172: loss 0.0950
[2019-04-04 08:48:19,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176000, global step 2815172: learning rate 0.0000
[2019-04-04 08:48:19,886] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176000, global step 2815475: loss 0.0802
[2019-04-04 08:48:19,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176000, global step 2815475: learning rate 0.0000
[2019-04-04 08:48:20,284] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176000, global step 2815688: loss 0.0885
[2019-04-04 08:48:20,285] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176000, global step 2815688: learning rate 0.0000
[2019-04-04 08:48:20,498] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176000, global step 2815793: loss 0.0871
[2019-04-04 08:48:20,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176000, global step 2815793: learning rate 0.0000
[2019-04-04 08:48:21,302] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176000, global step 2816232: loss 0.0836
[2019-04-04 08:48:21,306] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176000, global step 2816234: learning rate 0.0000
[2019-04-04 08:48:21,776] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176000, global step 2816493: loss 0.0954
[2019-04-04 08:48:21,782] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176000, global step 2816493: learning rate 0.0000
[2019-04-04 08:48:22,082] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176000, global step 2816667: loss 0.0935
[2019-04-04 08:48:22,082] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176000, global step 2816667: learning rate 0.0000
[2019-04-04 08:48:25,172] A3C_AGENT_WORKER-Thread-2 INFO:Local step 176500, global step 2818154: loss 0.2923
[2019-04-04 08:48:25,173] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 176500, global step 2818154: learning rate 0.0000
[2019-04-04 08:48:27,467] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176000, global step 2819193: loss 0.1051
[2019-04-04 08:48:27,469] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176000, global step 2819193: learning rate 0.0000
[2019-04-04 08:48:27,702] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176000, global step 2819310: loss 0.1126
[2019-04-04 08:48:27,703] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176000, global step 2819310: learning rate 0.0000
[2019-04-04 08:48:30,610] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176000, global step 2820647: loss 0.0995
[2019-04-04 08:48:30,611] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176000, global step 2820647: learning rate 0.0000
[2019-04-04 08:48:31,416] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176000, global step 2821032: loss 0.0891
[2019-04-04 08:48:31,422] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176000, global step 2821032: learning rate 0.0000
[2019-04-04 08:48:32,045] A3C_AGENT_WORKER-Thread-12 INFO:Local step 176500, global step 2821315: loss 0.6044
[2019-04-04 08:48:32,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 176500, global step 2821315: learning rate 0.0000
[2019-04-04 08:48:32,573] A3C_AGENT_WORKER-Thread-5 INFO:Local step 176500, global step 2821564: loss 0.6421
[2019-04-04 08:48:32,578] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 176500, global step 2821566: learning rate 0.0000
[2019-04-04 08:48:34,578] A3C_AGENT_WORKER-Thread-17 INFO:Local step 176500, global step 2822536: loss 0.6590
[2019-04-04 08:48:34,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 176500, global step 2822536: learning rate 0.0000
[2019-04-04 08:48:35,490] A3C_AGENT_WORKER-Thread-15 INFO:Local step 176500, global step 2822895: loss 0.6652
[2019-04-04 08:48:35,492] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 176500, global step 2822896: learning rate 0.0000
[2019-04-04 08:48:36,221] A3C_AGENT_WORKER-Thread-11 INFO:Local step 176500, global step 2823195: loss 0.3327
[2019-04-04 08:48:36,221] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 176500, global step 2823195: learning rate 0.0000
[2019-04-04 08:48:36,678] A3C_AGENT_WORKER-Thread-16 INFO:Local step 176500, global step 2823351: loss 0.7126
[2019-04-04 08:48:36,679] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 176500, global step 2823351: learning rate 0.0000
[2019-04-04 08:48:37,270] A3C_AGENT_WORKER-Thread-18 INFO:Local step 176500, global step 2823564: loss 0.7506
[2019-04-04 08:48:37,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 176500, global step 2823564: learning rate 0.0000
[2019-04-04 08:48:37,460] A3C_AGENT_WORKER-Thread-20 INFO:Local step 176500, global step 2823640: loss 0.7684
[2019-04-04 08:48:37,474] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 176500, global step 2823640: learning rate 0.0000
[2019-04-04 08:48:38,485] A3C_AGENT_WORKER-Thread-6 INFO:Local step 176500, global step 2824069: loss 0.8652
[2019-04-04 08:48:38,486] A3C_AGENT_WORKER-Thread-3 INFO:Local step 176500, global step 2824069: loss 0.8512
[2019-04-04 08:48:38,487] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 176500, global step 2824069: learning rate 0.0000
[2019-04-04 08:48:38,487] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 176500, global step 2824069: learning rate 0.0000
[2019-04-04 08:48:38,825] A3C_AGENT_WORKER-Thread-19 INFO:Local step 176500, global step 2824227: loss 0.8519
[2019-04-04 08:48:38,826] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 176500, global step 2824227: learning rate 0.0000
[2019-04-04 08:48:39,615] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8979391e-11 4.2729950e-10 8.5865437e-22 4.7980828e-11 1.4379554e-11
 1.6748015e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:39,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7532
[2019-04-04 08:48:39,658] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.54023949470823, 0.5287702829215134, 0.0, 1.0, 57312.84665554328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3961800.0000, 
sim time next is 3962400.0000, 
raw observation next is [-7.0, 45.0, 0.0, 0.0, 26.0, 25.53454477867558, 0.5265055714499868, 0.0, 1.0, 46528.30073919384], 
processed observation next is [1.0, 0.8695652173913043, 0.2686980609418283, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6278787315562985, 0.6755018571499956, 0.0, 1.0, 0.22156333685330398], 
reward next is 0.7784, 
noisyNet noise sample is [array([0.79129666], dtype=float32), 0.25549358]. 
=============================================
[2019-04-04 08:48:43,289] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4818523e-13 1.5344071e-12 7.3711340e-27 1.5565911e-13 3.6011110e-15
 2.6538520e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:43,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6235
[2019-04-04 08:48:43,339] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 23.0, 104.0, 794.0, 26.0, 26.47675918500377, 0.6862569173780121, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4026600.0000, 
sim time next is 4027200.0000, 
raw observation next is [-2.333333333333333, 22.0, 101.5, 780.3333333333334, 26.0, 25.9022431366627, 0.6466485088312501, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3979686057248385, 0.22, 0.3383333333333333, 0.8622467771639043, 0.6666666666666666, 0.6585202613885585, 0.71554950294375, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7613419], dtype=float32), -0.9735786]. 
=============================================
[2019-04-04 08:48:44,237] A3C_AGENT_WORKER-Thread-4 INFO:Local step 176500, global step 2826392: loss 1.3934
[2019-04-04 08:48:44,237] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 176500, global step 2826392: learning rate 0.0000
[2019-04-04 08:48:44,472] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177000, global step 2826512: loss 0.0245
[2019-04-04 08:48:44,475] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177000, global step 2826512: learning rate 0.0000
[2019-04-04 08:48:44,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.2832929e-10 1.2303814e-09 9.1122989e-23 7.9379982e-11 2.8949859e-12
 2.1619279e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:44,903] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0272
[2019-04-04 08:48:44,938] A3C_AGENT_WORKER-Thread-14 INFO:Local step 176500, global step 2826729: loss 1.3238
[2019-04-04 08:48:44,938] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 176500, global step 2826729: learning rate 0.0000
[2019-04-04 08:48:44,982] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666667, 38.66666666666667, 0.0, 0.0, 26.0, 24.53075639866983, 0.2402681839364551, 0.0, 1.0, 202467.1525545342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4087200.0000, 
sim time next is 4087800.0000, 
raw observation next is [-4.5, 37.5, 0.0, 0.0, 26.0, 24.91582747105262, 0.2795580706476145, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.3379501385041552, 0.375, 0.0, 0.0, 0.6666666666666666, 0.5763189559210516, 0.5931860235492049, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5516734], dtype=float32), -0.56319654]. 
=============================================
[2019-04-04 08:48:46,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9843824e-13 1.4302710e-12 1.4863542e-26 1.4096009e-13 2.2159337e-14
 1.1036472e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:46,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6528
[2019-04-04 08:48:46,352] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 28.16666666666667, 120.3333333333333, 832.6666666666667, 26.0, 26.61868396526148, 0.6264745561622386, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4104600.0000, 
sim time next is 4105200.0000, 
raw observation next is [1.666666666666667, 28.33333333333334, 120.1666666666667, 836.8333333333334, 26.0, 26.18186702029078, 0.607830619796859, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5087719298245615, 0.2833333333333334, 0.40055555555555566, 0.9246777163904236, 0.6666666666666666, 0.6818222516908984, 0.702610206598953, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6760401], dtype=float32), -0.41598552]. 
=============================================
[2019-04-04 08:48:47,667] A3C_AGENT_WORKER-Thread-10 INFO:Local step 176500, global step 2827879: loss 1.5367
[2019-04-04 08:48:47,667] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 176500, global step 2827879: learning rate 0.0000
[2019-04-04 08:48:48,903] A3C_AGENT_WORKER-Thread-13 INFO:Local step 176500, global step 2828408: loss 1.0793
[2019-04-04 08:48:48,909] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 176500, global step 2828410: learning rate 0.0000
[2019-04-04 08:48:50,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8860171e-09 3.1252974e-09 2.6198071e-22 2.8584901e-10 3.6803758e-11
 9.4614334e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:50,004] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1864
[2019-04-04 08:48:50,019] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 51.5, 0.0, 0.0, 26.0, 24.69487281820997, 0.2098145453249042, 0.0, 1.0, 40243.73240846171], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4174200.0000, 
sim time next is 4174800.0000, 
raw observation next is [-5.0, 52.33333333333334, 15.33333333333333, 81.33333333333331, 26.0, 24.64942613646074, 0.2154504997965918, 0.0, 1.0, 40288.6637722446], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.5233333333333334, 0.0511111111111111, 0.0898710865561694, 0.6666666666666666, 0.5541188447050617, 0.5718168332655306, 0.0, 1.0, 0.19185077986783144], 
reward next is 0.8081, 
noisyNet noise sample is [array([1.2432767], dtype=float32), 2.0778162]. 
=============================================
[2019-04-04 08:48:50,886] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177000, global step 2829359: loss 0.0258
[2019-04-04 08:48:50,887] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177000, global step 2829359: learning rate 0.0000
[2019-04-04 08:48:51,050] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177000, global step 2829428: loss 0.0267
[2019-04-04 08:48:51,066] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177000, global step 2829429: learning rate 0.0000
[2019-04-04 08:48:53,438] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177000, global step 2830469: loss 0.0290
[2019-04-04 08:48:53,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177000, global step 2830470: learning rate 0.0000
[2019-04-04 08:48:54,437] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177000, global step 2830921: loss 0.0338
[2019-04-04 08:48:54,440] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177000, global step 2830921: learning rate 0.0000
[2019-04-04 08:48:54,792] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5806904e-13 6.0891392e-12 9.5355727e-27 8.7849907e-14 1.2512804e-14
 7.3284430e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:54,793] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3836
[2019-04-04 08:48:54,813] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 30.0, 116.0, 830.0, 26.0, 26.47188451440882, 0.5482515630635704, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4109400.0000, 
sim time next is 4110000.0000, 
raw observation next is [3.0, 30.33333333333333, 114.3333333333333, 824.0, 26.0, 26.18531474533951, 0.6231220760525352, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.3033333333333333, 0.381111111111111, 0.9104972375690608, 0.6666666666666666, 0.6821095621116259, 0.7077073586841784, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7987029], dtype=float32), -0.088067584]. 
=============================================
[2019-04-04 08:48:54,820] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[96.43419 ]
 [96.415085]
 [96.352165]
 [96.1812  ]
 [96.06131 ]], R is [[96.48210144]
 [96.51728058]
 [96.55210876]
 [96.586586  ]
 [96.62071991]].
[2019-04-04 08:48:55,239] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177000, global step 2831344: loss 0.0266
[2019-04-04 08:48:55,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177000, global step 2831344: learning rate 0.0000
[2019-04-04 08:48:55,445] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177000, global step 2831454: loss 0.0282
[2019-04-04 08:48:55,448] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177000, global step 2831455: learning rate 0.0000
[2019-04-04 08:48:55,975] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177000, global step 2831726: loss 0.0272
[2019-04-04 08:48:55,976] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177000, global step 2831726: learning rate 0.0000
[2019-04-04 08:48:56,114] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177000, global step 2831792: loss 0.0340
[2019-04-04 08:48:56,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177000, global step 2831792: learning rate 0.0000
[2019-04-04 08:48:56,758] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177000, global step 2832090: loss 0.0274
[2019-04-04 08:48:56,760] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177000, global step 2832090: learning rate 0.0000
[2019-04-04 08:48:56,822] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177000, global step 2832117: loss 0.0305
[2019-04-04 08:48:56,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177000, global step 2832117: learning rate 0.0000
[2019-04-04 08:48:56,968] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177000, global step 2832187: loss 0.0291
[2019-04-04 08:48:56,968] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177000, global step 2832187: learning rate 0.0000
[2019-04-04 08:48:56,978] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.9545147e-12 1.2846233e-11 1.1352325e-25 8.4137932e-13 3.4165428e-13
 2.8071917e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:56,979] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9673
[2019-04-04 08:48:56,992] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.483333333333333, 75.83333333333333, 0.0, 0.0, 26.0, 25.47443579499377, 0.3998818926722769, 0.0, 1.0, 66111.12301688216], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4319400.0000, 
sim time next is 4320000.0000, 
raw observation next is [4.5, 76.0, 0.0, 0.0, 26.0, 25.45326070069806, 0.4091276563107401, 0.0, 1.0, 59603.33556110971], 
processed observation next is [1.0, 0.0, 0.5872576177285319, 0.76, 0.0, 0.0, 0.6666666666666666, 0.621105058391505, 0.6363758854369134, 0.0, 1.0, 0.28382540743385576], 
reward next is 0.7162, 
noisyNet noise sample is [array([1.0787368], dtype=float32), -2.5695033]. 
=============================================
[2019-04-04 08:48:57,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[87.48437 ]
 [87.32728 ]
 [87.217766]
 [87.243675]
 [87.33666 ]], R is [[87.54585266]
 [87.35558319]
 [87.2035141 ]
 [87.16748047]
 [87.20658112]].
[2019-04-04 08:48:58,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.6120473e-12 8.1113571e-12 1.6549637e-25 1.2649742e-12 1.9578248e-13
 1.2433480e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:58,160] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5128
[2019-04-04 08:48:58,182] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.833333333333334, 59.5, 191.3333333333333, 475.0, 26.0, 25.48339695121662, 0.4572423279496459, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4287000.0000, 
sim time next is 4287600.0000, 
raw observation next is [6.8, 60.0, 172.5, 520.0, 26.0, 25.48511401226866, 0.4564173359832055, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6509695290858727, 0.6, 0.575, 0.574585635359116, 0.6666666666666666, 0.6237595010223883, 0.6521391119944019, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47104004], dtype=float32), 0.16995554]. 
=============================================
[2019-04-04 08:48:59,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6374408e-11 1.2663098e-11 9.8466466e-25 2.9409344e-12 6.1110096e-13
 6.2664192e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:48:59,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7533
[2019-04-04 08:48:59,646] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 75.66666666666667, 0.0, 0.0, 26.0, 25.53547244106747, 0.4099602093216294, 0.0, 1.0, 18747.56861575616], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4321200.0000, 
sim time next is 4321800.0000, 
raw observation next is [4.35, 75.5, 0.0, 0.0, 26.0, 25.53340598579315, 0.403251406668364, 0.0, 1.0, 18745.13104393443], 
processed observation next is [1.0, 0.0, 0.5831024930747922, 0.755, 0.0, 0.0, 0.6666666666666666, 0.627783832149429, 0.6344171355561213, 0.0, 1.0, 0.08926252878064014], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.2445247], dtype=float32), 0.25386658]. 
=============================================
[2019-04-04 08:49:00,526] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.7455677e-13 2.9594231e-13 3.2760407e-27 4.4465432e-14 4.8764585e-15
 4.9932953e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:00,528] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6504
[2019-04-04 08:49:00,534] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 85.33333333333333, 179.3333333333333, 50.16666666666666, 26.0, 26.3100074008005, 0.6246015988340452, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4441200.0000, 
sim time next is 4441800.0000, 
raw observation next is [1.05, 85.66666666666667, 193.6666666666667, 69.33333333333333, 26.0, 26.36185477865678, 0.6261638385267259, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.49168975069252085, 0.8566666666666667, 0.6455555555555557, 0.07661141804788213, 0.6666666666666666, 0.6968212315547317, 0.7087212795089086, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31797743], dtype=float32), -0.7943173]. 
=============================================
[2019-04-04 08:49:00,688] A3C_AGENT_WORKER-Thread-2 INFO:Local step 177500, global step 2834172: loss 5.1997
[2019-04-04 08:49:00,692] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 177500, global step 2834174: learning rate 0.0000
[2019-04-04 08:49:02,108] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177000, global step 2834891: loss 0.0099
[2019-04-04 08:49:02,109] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177000, global step 2834891: learning rate 0.0000
[2019-04-04 08:49:03,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.3881704e-12 6.8510662e-11 4.1635157e-25 8.1950358e-12 1.5427914e-12
 4.9857630e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:03,010] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5515
[2019-04-04 08:49:03,036] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.9, 62.66666666666667, 0.0, 0.0, 26.0, 25.62174092010765, 0.621365800175056, 0.0, 1.0, 143190.9370672884], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4405200.0000, 
sim time next is 4405800.0000, 
raw observation next is [7.749999999999999, 62.83333333333333, 0.0, 0.0, 26.0, 25.64366947500667, 0.6377453504389271, 0.0, 1.0, 60572.69993139317], 
processed observation next is [1.0, 1.0, 0.6772853185595569, 0.6283333333333333, 0.0, 0.0, 0.6666666666666666, 0.636972456250556, 0.7125817834796423, 0.0, 1.0, 0.28844142824472935], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.452083], dtype=float32), -1.7632111]. 
=============================================
[2019-04-04 08:49:03,193] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177000, global step 2835477: loss 0.0075
[2019-04-04 08:49:03,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177000, global step 2835477: learning rate 0.0000
[2019-04-04 08:49:05,622] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177000, global step 2836801: loss 0.0031
[2019-04-04 08:49:05,626] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177000, global step 2836801: learning rate 0.0000
[2019-04-04 08:49:06,737] A3C_AGENT_WORKER-Thread-5 INFO:Local step 177500, global step 2837352: loss 5.1968
[2019-04-04 08:49:06,738] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 177500, global step 2837352: learning rate 0.0000
[2019-04-04 08:49:06,884] A3C_AGENT_WORKER-Thread-12 INFO:Local step 177500, global step 2837425: loss 5.1594
[2019-04-04 08:49:06,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 177500, global step 2837427: learning rate 0.0000
[2019-04-04 08:49:06,942] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177000, global step 2837450: loss 0.0025
[2019-04-04 08:49:06,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177000, global step 2837451: learning rate 0.0000
[2019-04-04 08:49:09,118] A3C_AGENT_WORKER-Thread-17 INFO:Local step 177500, global step 2838462: loss 5.1143
[2019-04-04 08:49:09,121] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 177500, global step 2838464: learning rate 0.0000
[2019-04-04 08:49:09,226] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3760083e-10 1.1872651e-10 1.9056021e-23 7.4927269e-12 8.3732934e-12
 3.3487854e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:09,229] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0682
[2019-04-04 08:49:09,254] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8500000000000001, 72.0, 0.0, 0.0, 26.0, 25.21679536259799, 0.412574941342855, 0.0, 1.0, 42104.48246218143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4509000.0000, 
sim time next is 4509600.0000, 
raw observation next is [-0.8333333333333334, 71.66666666666666, 0.0, 0.0, 26.0, 25.25794941923004, 0.4210578720169367, 0.0, 1.0, 41762.21933511127], 
processed observation next is [1.0, 0.17391304347826086, 0.43951985226223456, 0.7166666666666666, 0.0, 0.0, 0.6666666666666666, 0.6048291182691701, 0.6403526240056455, 0.0, 1.0, 0.1988677111195775], 
reward next is 0.8011, 
noisyNet noise sample is [array([-1.4160802], dtype=float32), 0.9031871]. 
=============================================
[2019-04-04 08:49:09,715] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.2449778e-10 1.1265742e-10 3.7109030e-23 2.4126598e-11 5.1766443e-12
 7.7139345e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:09,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5194
[2019-04-04 08:49:09,726] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.133333333333334, 65.66666666666667, 0.0, 0.0, 26.0, 25.20137411506479, 0.3238580596932596, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4299600.0000, 
sim time next is 4300200.0000, 
raw observation next is [6.1, 66.5, 0.0, 0.0, 26.0, 25.13663145339499, 0.3109951346145273, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.782608695652174, 0.6315789473684211, 0.665, 0.0, 0.0, 0.6666666666666666, 0.5947192877829158, 0.6036650448715091, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2662945], dtype=float32), 0.40076894]. 
=============================================
[2019-04-04 08:49:10,428] A3C_AGENT_WORKER-Thread-15 INFO:Local step 177500, global step 2839057: loss 4.9644
[2019-04-04 08:49:10,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 177500, global step 2839057: learning rate 0.0000
[2019-04-04 08:49:10,788] A3C_AGENT_WORKER-Thread-16 INFO:Local step 177500, global step 2839235: loss 4.9341
[2019-04-04 08:49:10,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 177500, global step 2839235: learning rate 0.0000
[2019-04-04 08:49:11,237] A3C_AGENT_WORKER-Thread-18 INFO:Local step 177500, global step 2839446: loss 5.0807
[2019-04-04 08:49:11,242] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 177500, global step 2839447: learning rate 0.0000
[2019-04-04 08:49:11,638] A3C_AGENT_WORKER-Thread-20 INFO:Local step 177500, global step 2839649: loss 5.1443
[2019-04-04 08:49:11,640] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 177500, global step 2839649: learning rate 0.0000
[2019-04-04 08:49:11,646] A3C_AGENT_WORKER-Thread-11 INFO:Local step 177500, global step 2839652: loss 8.1642
[2019-04-04 08:49:11,647] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 177500, global step 2839652: learning rate 0.0000
[2019-04-04 08:49:11,827] A3C_AGENT_WORKER-Thread-6 INFO:Local step 177500, global step 2839747: loss 5.0697
[2019-04-04 08:49:11,828] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 177500, global step 2839747: learning rate 0.0000
[2019-04-04 08:49:11,954] A3C_AGENT_WORKER-Thread-3 INFO:Local step 177500, global step 2839808: loss 5.0309
[2019-04-04 08:49:11,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 177500, global step 2839809: learning rate 0.0000
[2019-04-04 08:49:12,859] A3C_AGENT_WORKER-Thread-19 INFO:Local step 177500, global step 2840267: loss 4.9617
[2019-04-04 08:49:12,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 177500, global step 2840268: learning rate 0.0000
[2019-04-04 08:49:17,531] A3C_AGENT_WORKER-Thread-2 INFO:Local step 178000, global step 2842578: loss 0.0112
[2019-04-04 08:49:17,533] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 178000, global step 2842578: learning rate 0.0000
[2019-04-04 08:49:18,017] A3C_AGENT_WORKER-Thread-4 INFO:Local step 177500, global step 2842809: loss 5.1296
[2019-04-04 08:49:18,019] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 177500, global step 2842812: learning rate 0.0000
[2019-04-04 08:49:18,525] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2920433e-13 3.8447769e-12 3.7515907e-26 2.3145765e-13 2.6495772e-14
 9.2147113e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:18,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1762
[2019-04-04 08:49:18,566] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.37389472609917, 0.5680392093541692, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4700400.0000, 
sim time next is 4701000.0000, 
raw observation next is [0.0, 92.0, 146.0, 2.0, 26.0, 26.40039673309152, 0.5757189163583821, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.4866666666666667, 0.0022099447513812156, 0.6666666666666666, 0.7000330610909599, 0.6919063054527941, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.85834914], dtype=float32), -0.2785643]. 
=============================================
[2019-04-04 08:49:18,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[91.65916]
 [91.49174]
 [91.38821]
 [91.30601]
 [91.17577]], R is [[92.05358887]
 [92.13305664]
 [92.21172333]
 [92.28960419]
 [92.36670685]].
[2019-04-04 08:49:18,865] A3C_AGENT_WORKER-Thread-14 INFO:Local step 177500, global step 2843238: loss 5.1010
[2019-04-04 08:49:18,865] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 177500, global step 2843238: learning rate 0.0000
[2019-04-04 08:49:19,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2663129e-10 4.6259496e-10 1.1154697e-23 2.4738064e-11 3.1301546e-12
 6.2741479e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:19,021] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2665
[2019-04-04 08:49:19,048] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.73913244433583, 0.2883634992373491, 0.0, 1.0, 40548.27411111189], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4762800.0000, 
sim time next is 4763400.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.67583451659301, 0.2766512762858221, 0.0, 1.0, 40588.72060253459], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5563195430494176, 0.592217092095274, 0.0, 1.0, 0.19327962191683137], 
reward next is 0.8067, 
noisyNet noise sample is [array([-0.8325128], dtype=float32), -0.31382492]. 
=============================================
[2019-04-04 08:49:21,759] A3C_AGENT_WORKER-Thread-10 INFO:Local step 177500, global step 2844461: loss 4.8959
[2019-04-04 08:49:21,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 177500, global step 2844461: learning rate 0.0000
[2019-04-04 08:49:23,451] A3C_AGENT_WORKER-Thread-13 INFO:Local step 177500, global step 2845225: loss 7.7064
[2019-04-04 08:49:23,452] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 177500, global step 2845226: learning rate 0.0000
[2019-04-04 08:49:24,269] A3C_AGENT_WORKER-Thread-12 INFO:Local step 178000, global step 2845641: loss 0.0048
[2019-04-04 08:49:24,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 178000, global step 2845643: learning rate 0.0000
[2019-04-04 08:49:24,399] A3C_AGENT_WORKER-Thread-5 INFO:Local step 178000, global step 2845699: loss 0.0052
[2019-04-04 08:49:24,400] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 178000, global step 2845701: learning rate 0.0000
[2019-04-04 08:49:26,543] A3C_AGENT_WORKER-Thread-17 INFO:Local step 178000, global step 2846601: loss 0.0093
[2019-04-04 08:49:26,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 178000, global step 2846601: learning rate 0.0000
[2019-04-04 08:49:27,201] A3C_AGENT_WORKER-Thread-15 INFO:Local step 178000, global step 2846900: loss 0.0047
[2019-04-04 08:49:27,234] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 178000, global step 2846900: learning rate 0.0000
[2019-04-04 08:49:28,110] A3C_AGENT_WORKER-Thread-18 INFO:Local step 178000, global step 2847355: loss 0.0104
[2019-04-04 08:49:28,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 178000, global step 2847355: learning rate 0.0000
[2019-04-04 08:49:28,359] A3C_AGENT_WORKER-Thread-16 INFO:Local step 178000, global step 2847467: loss 0.0068
[2019-04-04 08:49:28,364] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 178000, global step 2847468: learning rate 0.0000
[2019-04-04 08:49:28,625] A3C_AGENT_WORKER-Thread-20 INFO:Local step 178000, global step 2847586: loss 0.0172
[2019-04-04 08:49:28,627] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 178000, global step 2847586: learning rate 0.0000
[2019-04-04 08:49:28,722] A3C_AGENT_WORKER-Thread-6 INFO:Local step 178000, global step 2847627: loss 0.0156
[2019-04-04 08:49:28,726] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 178000, global step 2847631: learning rate 0.0000
[2019-04-04 08:49:28,773] A3C_AGENT_WORKER-Thread-11 INFO:Local step 178000, global step 2847649: loss 0.0178
[2019-04-04 08:49:28,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 178000, global step 2847651: learning rate 0.0000
[2019-04-04 08:49:28,839] A3C_AGENT_WORKER-Thread-3 INFO:Local step 178000, global step 2847682: loss 0.0143
[2019-04-04 08:49:28,840] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 178000, global step 2847682: learning rate 0.0000
[2019-04-04 08:49:28,906] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2473852e-09 1.5761270e-09 2.5519249e-22 3.2258443e-10 3.9780311e-11
 3.5982445e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:28,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3464
[2019-04-04 08:49:28,926] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.50301645422781, 0.1570457289357492, 0.0, 1.0, 39521.28121315775], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4863600.0000, 
sim time next is 4864200.0000, 
raw observation next is [-4.0, 66.0, 0.0, 0.0, 26.0, 24.47004942841824, 0.1518245117834945, 0.0, 1.0, 39552.63958229432], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5391707857015199, 0.5506081705944982, 0.0, 1.0, 0.18834590277283011], 
reward next is 0.8117, 
noisyNet noise sample is [array([-1.2602642], dtype=float32), -0.16290715]. 
=============================================
[2019-04-04 08:49:30,049] A3C_AGENT_WORKER-Thread-19 INFO:Local step 178000, global step 2848243: loss 0.0103
[2019-04-04 08:49:30,050] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 178000, global step 2848243: learning rate 0.0000
[2019-04-04 08:49:31,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:31,816] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:31,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run22
[2019-04-04 08:49:35,651] A3C_AGENT_WORKER-Thread-4 INFO:Local step 178000, global step 2850559: loss 0.0103
[2019-04-04 08:49:35,655] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 178000, global step 2850561: learning rate 0.0000
[2019-04-04 08:49:36,580] A3C_AGENT_WORKER-Thread-14 INFO:Local step 178000, global step 2850974: loss 0.0119
[2019-04-04 08:49:36,584] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 178000, global step 2850974: learning rate 0.0000
[2019-04-04 08:49:39,159] A3C_AGENT_WORKER-Thread-10 INFO:Local step 178000, global step 2852230: loss 0.0038
[2019-04-04 08:49:39,161] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 178000, global step 2852230: learning rate 0.0000
[2019-04-04 08:49:39,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:39,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:39,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run22
[2019-04-04 08:49:39,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:39,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:39,649] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run22
[2019-04-04 08:49:40,557] A3C_AGENT_WORKER-Thread-13 INFO:Local step 178000, global step 2852719: loss 0.0085
[2019-04-04 08:49:40,560] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 178000, global step 2852719: learning rate 0.0000
[2019-04-04 08:49:41,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3019929e-13 4.6996060e-13 1.1335263e-27 3.4372675e-14 2.0054596e-15
 5.4270782e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:41,196] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1813
[2019-04-04 08:49:41,210] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.66666666666667, 20.83333333333334, 115.6666666666667, 846.3333333333333, 26.0, 26.95937100429299, 0.8919099132647196, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5061000.0000, 
sim time next is 5061600.0000, 
raw observation next is [11.0, 20.0, 114.5, 839.5, 26.0, 27.52131170307823, 0.9527841666404541, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7673130193905818, 0.2, 0.38166666666666665, 0.9276243093922651, 0.6666666666666666, 0.7934426419231858, 0.8175947222134847, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.20040414], dtype=float32), 0.12641616]. 
=============================================
[2019-04-04 08:49:42,053] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:42,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:42,058] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run22
[2019-04-04 08:49:42,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:42,193] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:42,211] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run22
[2019-04-04 08:49:43,291] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:43,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:43,307] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run22
[2019-04-04 08:49:43,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:43,457] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:43,463] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run22
[2019-04-04 08:49:43,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:43,685] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:43,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run22
[2019-04-04 08:49:43,752] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:43,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:43,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:43,753] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:43,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run22
[2019-04-04 08:49:43,826] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run22
[2019-04-04 08:49:43,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:43,897] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:43,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run22
[2019-04-04 08:49:44,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:44,497] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:44,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run22
[2019-04-04 08:49:46,440] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2271686e-12 1.8411516e-12 1.7174396e-26 2.3784669e-13 5.1520823e-14
 1.2074588e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:49:46,441] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7646
[2019-04-04 08:49:46,478] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 37.5, 103.0, 664.6666666666667, 26.0, 25.9121708221129, 0.4324686590254591, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4957800.0000, 
sim time next is 4958400.0000, 
raw observation next is [-0.3333333333333334, 36.0, 105.5, 690.8333333333333, 26.0, 26.00615102632463, 0.4489912368381963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4533702677747, 0.36, 0.3516666666666667, 0.7633517495395947, 0.6666666666666666, 0.6671792521937192, 0.6496637456127321, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.7298944], dtype=float32), -0.7797456]. 
=============================================
[2019-04-04 08:49:53,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:53,278] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:53,298] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run22
[2019-04-04 08:49:53,546] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:53,546] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:53,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run22
[2019-04-04 08:49:56,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:56,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:56,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run22
[2019-04-04 08:49:58,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:49:58,221] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:49:58,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run22
[2019-04-04 08:50:07,320] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1144456e-10 3.7083944e-10 2.2982366e-22 2.8035161e-11 9.7217165e-12
 1.9651277e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:07,321] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7752
[2019-04-04 08:50:07,366] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.01513880113686, 0.3172113489269448, 1.0, 1.0, 52896.85902126878], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 159000.0000, 
sim time next is 159600.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.04744272226232, 0.3007494785770807, 0.0, 1.0, 23213.9378479856], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5872868935218601, 0.6002498261923602, 0.0, 1.0, 0.1105425611808838], 
reward next is 0.8895, 
noisyNet noise sample is [array([-0.0764946], dtype=float32), -0.2956935]. 
=============================================
[2019-04-04 08:50:13,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2903414e-09 1.0574992e-10 4.8255685e-23 2.8119722e-11 1.8444600e-11
 3.2634350e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:13,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5582
[2019-04-04 08:50:13,654] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 91.0, 0.0, 0.0, 26.0, 24.32998788235721, 0.1452704966806129, 0.0, 1.0, 41215.53058806681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 91200.0000, 
sim time next is 91800.0000, 
raw observation next is [-1.15, 91.0, 0.0, 0.0, 26.0, 24.35063198306668, 0.1370149550293152, 0.0, 1.0, 41418.3595285769], 
processed observation next is [1.0, 0.043478260869565216, 0.4307479224376732, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5292193319222234, 0.5456716516764384, 0.0, 1.0, 0.1972302834694138], 
reward next is 0.8028, 
noisyNet noise sample is [array([-0.52950925], dtype=float32), 1.9221475]. 
=============================================
[2019-04-04 08:50:15,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6128682e-11 7.5531248e-11 6.7673435e-24 2.6491706e-12 1.1685472e-12
 1.7040749e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:15,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2018
[2019-04-04 08:50:15,943] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.1, 60.5, 56.0, 0.0, 26.0, 25.68902698365761, 0.3682774906361383, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 228600.0000, 
sim time next is 229200.0000, 
raw observation next is [-3.2, 61.0, 49.66666666666667, 0.0, 26.0, 25.90998291930379, 0.3794848360184744, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.37396121883656513, 0.61, 0.16555555555555557, 0.0, 0.6666666666666666, 0.6591652432753158, 0.6264949453394915, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.60654217], dtype=float32), 1.0940776]. 
=============================================
[2019-04-04 08:50:34,208] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.6976702e-12 2.8014421e-11 1.6428508e-23 1.8432246e-12 6.8793165e-13
 6.6266444e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:34,208] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0482
[2019-04-04 08:50:34,287] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.0, 43.0, 48.0, 832.0, 26.0, 25.16343722392237, 0.4282634088666383, 1.0, 1.0, 183622.5172362765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 397800.0000, 
sim time next is 398400.0000, 
raw observation next is [-9.833333333333334, 42.0, 46.16666666666666, 811.1666666666666, 26.0, 25.70203958544609, 0.4677773563238912, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.1902123730378578, 0.42, 0.15388888888888885, 0.8963167587476979, 0.6666666666666666, 0.6418366321205076, 0.6559257854412971, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9542379], dtype=float32), 0.4002432]. 
=============================================
[2019-04-04 08:50:41,327] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1170071e-12 7.6755486e-12 1.7386465e-25 6.5068979e-13 3.7324283e-14
 1.1621064e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:41,328] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8368
[2019-04-04 08:50:41,374] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 30.0, 98.0, 0.0, 26.0, 25.47240956494794, 0.1736042507390298, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 469800.0000, 
sim time next is 470400.0000, 
raw observation next is [-3.033333333333333, 29.33333333333333, 102.0, 0.0, 26.0, 25.35822583698339, 0.1671124352577991, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.37857802400738694, 0.2933333333333333, 0.34, 0.0, 0.6666666666666666, 0.6131854864152825, 0.555704145085933, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4798946], dtype=float32), 0.37627873]. 
=============================================
[2019-04-04 08:50:43,644] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1910178e-12 1.0351990e-11 1.1015397e-25 1.4019733e-12 7.6618650e-14
 1.4686368e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:43,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8132
[2019-04-04 08:50:43,699] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 38.5, 20.0, 0.0, 26.0, 24.34788797131685, 0.1720396030547403, 1.0, 1.0, 197728.5200991297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 491400.0000, 
sim time next is 492000.0000, 
raw observation next is [1.1, 40.0, 16.66666666666667, 0.0, 26.0, 24.8683784027478, 0.2325869169961444, 1.0, 1.0, 58600.04206612171], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.4, 0.05555555555555557, 0.0, 0.6666666666666666, 0.5723648668956501, 0.5775289723320481, 1.0, 1.0, 0.2790478193624843], 
reward next is 0.7210, 
noisyNet noise sample is [array([1.5785352], dtype=float32), 1.2273737]. 
=============================================
[2019-04-04 08:50:43,703] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[87.507675]
 [86.768456]
 [86.67961 ]
 [86.6383  ]
 [86.58485 ]], R is [[87.63598633]
 [86.81806183]
 [86.784935  ]
 [86.91708374]
 [87.0479126 ]].
[2019-04-04 08:50:45,244] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0225053e-13 1.9848866e-12 2.3786040e-27 2.6585082e-13 2.1653738e-14
 2.3047130e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:45,244] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4972
[2019-04-04 08:50:45,288] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.74259195371506, 0.2149957148620994, 1.0, 1.0, 29895.83325642082], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 503400.0000, 
sim time next is 504000.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.82401837598023, 0.2205625526252598, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5686681979983526, 0.5735208508750866, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6615689], dtype=float32), 1.8694452]. 
=============================================
[2019-04-04 08:50:45,294] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[93.13673]
 [92.37445]
 [89.98727]
 [85.75595]
 [88.03698]], R is [[93.39044952]
 [93.31417847]
 [92.86305237]
 [92.44768524]
 [92.30784607]].
[2019-04-04 08:50:45,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7683183e-10 4.0100745e-10 1.1786839e-22 1.2532320e-10 1.1661410e-11
 3.9875286e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:45,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6082
[2019-04-04 08:50:45,951] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.60675253345813, -0.0625677641537301, 0.0, 1.0, 45416.12501326669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 435600.0000, 
sim time next is 436200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.57892224971059, -0.06679103742811132, 0.0, 1.0, 45445.56087529077], 
processed observation next is [1.0, 0.043478260869565216, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.46491018747588253, 0.4777363208572962, 0.0, 1.0, 0.21640743273947988], 
reward next is 0.7836, 
noisyNet noise sample is [array([0.37956604], dtype=float32), -1.55407]. 
=============================================
[2019-04-04 08:50:49,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9343938e-11 8.3283019e-11 1.0430844e-24 4.1835476e-12 2.5980748e-12
 1.1631182e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:49,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9851
[2019-04-04 08:50:49,590] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 83.66666666666667, 81.0, 139.0, 26.0, 24.88683738906877, 0.2741095462601091, 0.0, 1.0, 18739.44038428506], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 557400.0000, 
sim time next is 558000.0000, 
raw observation next is [-0.6, 83.0, 83.0, 138.0, 26.0, 24.869448751666, 0.2724417122137637, 0.0, 1.0, 30839.27728795656], 
processed observation next is [0.0, 0.4782608695652174, 0.44598337950138506, 0.83, 0.27666666666666667, 0.15248618784530388, 0.6666666666666666, 0.5724540626388332, 0.5908139040712546, 0.0, 1.0, 0.14685370137122172], 
reward next is 0.8531, 
noisyNet noise sample is [array([1.0881299], dtype=float32), -0.31177732]. 
=============================================
[2019-04-04 08:50:49,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.89774 ]
 [81.95292 ]
 [82.081055]
 [82.44699 ]
 [82.9557  ]], R is [[81.9249115 ]
 [82.01642609]
 [82.10701752]
 [82.14067841]
 [82.10868073]].
[2019-04-04 08:50:53,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6250849e-10 1.2750889e-10 1.6646338e-23 1.2077037e-11 1.2754531e-11
 2.3452292e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:53,695] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5259
[2019-04-04 08:50:53,732] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.42832901466321, 0.1458366793930325, 0.0, 1.0, 42138.93995359495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 609000.0000, 
sim time next is 609600.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.39486097840527, 0.1455057243631493, 0.0, 1.0, 42119.83469298471], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5329050815337725, 0.5485019081210497, 0.0, 1.0, 0.20057064139516528], 
reward next is 0.7994, 
noisyNet noise sample is [array([0.32627976], dtype=float32), -0.0418632]. 
=============================================
[2019-04-04 08:50:56,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9317853e-11 6.0665944e-11 1.3398733e-24 1.2758838e-11 1.2145965e-12
 2.1171748e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:56,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5132
[2019-04-04 08:50:56,166] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 71.0, 77.0, 25.5, 26.0, 25.13008609069737, 0.2543150253935838, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 637200.0000, 
sim time next is 637800.0000, 
raw observation next is [-3.9, 70.0, 96.33333333333334, 34.00000000000001, 26.0, 25.23761189391551, 0.2571782653964879, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3545706371191136, 0.7, 0.3211111111111111, 0.03756906077348067, 0.6666666666666666, 0.6031343244929591, 0.585726088465496, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7166981], dtype=float32), 0.2720289]. 
=============================================
[2019-04-04 08:50:56,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.5862164e-11 2.1745541e-10 4.8395583e-24 1.1855945e-11 1.7516796e-12
 2.3125368e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:50:56,955] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4194
[2019-04-04 08:50:56,985] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 75.0, 0.0, 0.0, 26.0, 24.29300867019631, 0.06332679966618288, 0.0, 1.0, 41551.88535919147], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 703200.0000, 
sim time next is 703800.0000, 
raw observation next is [-3.1, 75.0, 0.0, 0.0, 26.0, 24.36373022598906, 0.05828493686467379, 0.0, 1.0, 41572.65697519847], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5303108521657549, 0.5194283122882246, 0.0, 1.0, 0.19796503321523082], 
reward next is 0.8020, 
noisyNet noise sample is [array([0.30221182], dtype=float32), 0.88597965]. 
=============================================
[2019-04-04 08:51:07,533] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3403906e-11 3.2158859e-10 9.1729131e-23 2.6311342e-11 5.1959140e-12
 1.9738680e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:07,539] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6722
[2019-04-04 08:51:07,578] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 67.0, 0.0, 0.0, 26.0, 24.55979689713424, 0.2067274320862237, 0.0, 1.0, 42624.62173195268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 774000.0000, 
sim time next is 774600.0000, 
raw observation next is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.52974427632785, 0.1992595160311072, 0.0, 1.0, 42464.16096084129], 
processed observation next is [1.0, 1.0, 0.2742382271468144, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5441453563606542, 0.5664198386770357, 0.0, 1.0, 0.20221029028972043], 
reward next is 0.7978, 
noisyNet noise sample is [array([-1.3785985], dtype=float32), 0.28550625]. 
=============================================
[2019-04-04 08:51:18,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0302367e-12 1.6917574e-12 1.2231307e-27 5.6507011e-14 1.8165509e-14
 6.7631352e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:18,097] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4730
[2019-04-04 08:51:18,166] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.8, 92.33333333333333, 15.0, 0.0, 26.0, 25.22273504082424, 0.3138752501077078, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 924000.0000, 
sim time next is 924600.0000, 
raw observation next is [4.9, 92.16666666666667, 12.0, 0.0, 26.0, 24.51399361889501, 0.310557693130901, 1.0, 1.0, 196475.6107603647], 
processed observation next is [1.0, 0.6956521739130435, 0.5983379501385043, 0.9216666666666667, 0.04, 0.0, 0.6666666666666666, 0.5428328015745842, 0.6035192310436336, 1.0, 1.0, 0.9355981464779272], 
reward next is 0.0644, 
noisyNet noise sample is [array([0.03863242], dtype=float32), -0.19265048]. 
=============================================
[2019-04-04 08:51:23,222] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.99333139e-14 1.01509025e-13 7.51762379e-30 6.61053269e-15
 5.04131829e-16 5.80616932e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 08:51:23,236] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2288
[2019-04-04 08:51:23,261] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.86666666666667, 68.33333333333334, 230.6666666666667, 179.1666666666667, 26.0, 27.3641311709984, 0.9629047030957963, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1077600.0000, 
sim time next is 1078200.0000, 
raw observation next is [16.05, 67.5, 254.0, 215.0, 26.0, 27.48533988829138, 0.6458381962412766, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.9072022160664821, 0.675, 0.8466666666666667, 0.23756906077348067, 0.6666666666666666, 0.7904449906909484, 0.7152793987470921, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38016158], dtype=float32), -0.15423003]. 
=============================================
[2019-04-04 08:51:23,366] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0076670e-12 8.4979055e-12 1.0930628e-26 2.1268004e-13 5.2127339e-14
 5.2671981e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:23,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7976
[2019-04-04 08:51:23,405] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 77.0, 0.0, 0.0, 26.0, 24.83251530042941, 0.4730710096951243, 0.0, 1.0, 198382.711794217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1024800.0000, 
sim time next is 1025400.0000, 
raw observation next is [14.4, 77.0, 0.0, 0.0, 26.0, 24.93054604912188, 0.526469687447128, 0.0, 1.0, 167082.0007064135], 
processed observation next is [1.0, 0.8695652173913043, 0.8614958448753465, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5775455040934899, 0.6754898958157094, 0.0, 1.0, 0.7956285747924452], 
reward next is 0.2044, 
noisyNet noise sample is [array([1.9402053], dtype=float32), -0.001054282]. 
=============================================
[2019-04-04 08:51:24,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.4424703e-12 7.6466269e-13 1.5539622e-29 5.2791921e-14 7.7953831e-15
 4.3985879e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:24,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4845
[2019-04-04 08:51:24,292] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 83.0, 11.5, 38.0, 26.0, 25.92345422276947, 0.623993847899504, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1065600.0000, 
sim time next is 1066200.0000, 
raw observation next is [12.2, 83.0, 15.0, 48.33333333333334, 26.0, 25.9086541543485, 0.6242666349420898, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.8005540166204987, 0.83, 0.05, 0.05340699815837938, 0.6666666666666666, 0.659054512862375, 0.7080888783140299, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6125484], dtype=float32), -0.98929715]. 
=============================================
[2019-04-04 08:51:26,488] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8508370e-13 4.1546958e-13 9.7010401e-28 3.5232796e-14 2.6910131e-15
 1.7721071e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:26,494] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2152
[2019-04-04 08:51:26,501] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 82.5, 0.0, 26.0, 26.72357767076501, 0.9032164576194494, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1093200.0000, 
sim time next is 1093800.0000, 
raw observation next is [19.4, 49.0, 73.0, 0.0, 26.0, 27.21418040627201, 0.945011141919245, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.24333333333333335, 0.0, 0.6666666666666666, 0.7678483671893342, 0.8150037139730816, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.69692165], dtype=float32), -0.55905306]. 
=============================================
[2019-04-04 08:51:31,253] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8320952e-11 3.5973426e-11 9.4667863e-26 8.9248929e-12 1.3701210e-12
 3.6211384e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:31,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7890
[2019-04-04 08:51:31,265] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.52404718765425, 0.1524128900867561, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1233600.0000, 
sim time next is 1234200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.50285567266144, 0.148620532142258, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.2608695652173913, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.45857130605512, 0.5495401773807527, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14859597], dtype=float32), 1.0192723]. 
=============================================
[2019-04-04 08:51:31,541] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.9997010e-11 7.7635565e-11 4.0750656e-25 1.4846193e-12 1.1533901e-12
 2.8351078e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:31,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9613
[2019-04-04 08:51:31,563] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.13878020198759, 0.4145810887115342, 0.0, 1.0, 38508.24167545074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1405200.0000, 
sim time next is 1405800.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.08757107965064, 0.4078779578935505, 0.0, 1.0, 38556.09838312528], 
processed observation next is [1.0, 0.2608695652173913, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5906309233042201, 0.6359593192978502, 0.0, 1.0, 0.18360046849107278], 
reward next is 0.8164, 
noisyNet noise sample is [array([0.49304435], dtype=float32), 1.2239611]. 
=============================================
[2019-04-04 08:51:35,316] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2479979e-11 9.0430406e-12 5.8592114e-26 6.5556187e-13 1.5087459e-13
 4.9408692e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:35,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8036
[2019-04-04 08:51:35,392] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.0, 5.999999999999998, 0.0, 26.0, 24.38308974745081, 0.4368673853398315, 1.0, 1.0, 197326.727735802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1357800.0000, 
sim time next is 1358400.0000, 
raw observation next is [0.5, 96.0, 0.0, 0.0, 26.0, 25.02371666575545, 0.4430952439498727, 1.0, 1.0, 14032.82129157385], 
processed observation next is [1.0, 0.7391304347826086, 0.4764542936288089, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5853097221462876, 0.6476984146499576, 1.0, 1.0, 0.06682295853130404], 
reward next is 0.9332, 
noisyNet noise sample is [array([-0.31458515], dtype=float32), 1.3296244]. 
=============================================
[2019-04-04 08:51:36,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.5183124e-11 3.0476840e-11 1.6488646e-24 3.8039866e-12 1.3672728e-12
 8.3978802e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:51:36,024] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9435
[2019-04-04 08:51:36,043] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.716666666666666, 92.83333333333333, 0.0, 0.0, 26.0, 25.56207739344113, 0.5885318818971411, 0.0, 1.0, 18743.15084342029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1300200.0000, 
sim time next is 1300800.0000, 
raw observation next is [3.633333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.60140710580352, 0.5646036972630579, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.5632502308402586, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6334505921502934, 0.6882012324210193, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6120854], dtype=float32), 0.567014]. 
=============================================
[2019-04-04 08:52:05,634] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6606260e-11 1.4508652e-11 3.7469586e-25 1.1130106e-12 3.2400590e-13
 4.7370932e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:52:05,634] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5946
[2019-04-04 08:52:05,667] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.633333333333334, 84.66666666666667, 0.0, 0.0, 26.0, 25.58142817318393, 0.5132439903515031, 0.0, 1.0, 18739.72490329198], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1570800.0000, 
sim time next is 1571400.0000, 
raw observation next is [4.65, 84.5, 0.0, 0.0, 26.0, 25.5519897457547, 0.5013079458887385, 0.0, 1.0, 31267.34942411253], 
processed observation next is [1.0, 0.17391304347826086, 0.5914127423822716, 0.845, 0.0, 0.0, 0.6666666666666666, 0.6293324788128917, 0.6671026486295796, 0.0, 1.0, 0.14889214011482158], 
reward next is 0.8511, 
noisyNet noise sample is [array([-0.05141329], dtype=float32), 0.65659285]. 
=============================================
[2019-04-04 08:52:06,678] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 08:52:06,685] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 08:52:06,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:52:06,691] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run30
[2019-04-04 08:52:06,688] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 08:52:06,732] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:52:06,733] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 08:52:06,734] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:52:06,748] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run30
[2019-04-04 08:52:06,812] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run30
[2019-04-04 08:52:59,112] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.22878556], dtype=float32), -0.19018444]
[2019-04-04 08:52:59,113] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [15.1, 69.0, 0.0, 0.0, 26.0, 25.61890878340461, 0.5185832832598425, 0.0, 1.0, 26538.5259448723]
[2019-04-04 08:52:59,113] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:52:59,114] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.8979708e-11 1.6717107e-11 3.6385809e-26 9.6468656e-13 4.9885649e-13
 9.8141419e-16 1.0000000e+00], sampled 0.6787962822732219
[2019-04-04 08:54:40,787] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.22878556], dtype=float32), -0.19018444]
[2019-04-04 08:54:40,787] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.659265521666666, 48.03252994666666, 101.64255226, 776.7300332666667, 26.0, 25.06283626486699, 0.3937979028565914, 0.0, 1.0, 43730.81025892268]
[2019-04-04 08:54:40,788] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:54:40,789] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.3249573e-11 7.4219964e-11 1.2843099e-22 8.6247502e-12 3.4254446e-12
 2.7646786e-14 1.0000000e+00], sampled 0.39190790233564143
[2019-04-04 08:54:42,305] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.22878556], dtype=float32), -0.19018444]
[2019-04-04 08:54:42,306] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.642764606, 60.70583063666667, 0.0, 0.0, 26.0, 24.86168369094836, 0.2358960754833139, 0.0, 1.0, 37442.61555046946]
[2019-04-04 08:54:42,306] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 08:54:42,307] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.2758725e-10 2.0205501e-10 8.4410117e-23 2.5161925e-11 9.6535705e-12
 8.3770230e-14 1.0000000e+00], sampled 0.9848385763474805
[2019-04-04 08:55:06,533] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.22878556], dtype=float32), -0.19018444]
[2019-04-04 08:55:06,534] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.98333333333333, 71.5, 0.0, 0.0, 26.0, 26.12428175297861, 0.7938196625074468, 0.0, 1.0, 0.0]
[2019-04-04 08:55:06,534] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 08:55:06,535] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.7110017e-11 1.2299930e-11 5.4536407e-26 9.6466270e-13 4.6379578e-13
 1.1388035e-15 1.0000000e+00], sampled 0.26911702935304826
[2019-04-04 08:55:15,445] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 08:55:44,193] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 08:55:49,614] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 08:55:50,652] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2900000, evaluation results [2900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 08:55:51,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9371373e-13 9.5012193e-13 1.7294633e-27 6.3191131e-14 9.2725146e-15
 5.4720671e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 08:55:51,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3840
[2019-04-04 08:55:51,182] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.266666666666667, 65.66666666666667, 185.8333333333333, 96.0, 26.0, 26.72715417435157, 0.7213878221643358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1592400.0000, 
sim time next is 1593000.0000, 
raw observation next is [8.55, 64.5, 200.0, 88.0, 26.0, 26.78202466763614, 0.7272374585834583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6994459833795015, 0.645, 0.6666666666666666, 0.09723756906077348, 0.6666666666666666, 0.7318353889696784, 0.7424124861944862, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4803886], dtype=float32), 0.98708093]. 
=============================================
[2019-04-04 08:55:51,211] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[93.01594]
 [93.00987]
 [93.0818 ]
 [93.21628]
 [93.34177]], R is [[93.12744904]
 [93.19617462]
 [93.26421356]
 [93.33157349]
 [93.39826202]].
[2019-04-04 08:55:59,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.8617512e-12 1.8581818e-11 1.5390699e-24 1.8394002e-12 2.9176845e-13
 7.5588881e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:55:59,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1067
[2019-04-04 08:55:59,160] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.00000000000001, 86.66666666666667, 0.0, 26.0, 26.27952804149711, 0.6241519293494594, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1692600.0000, 
sim time next is 1693200.0000, 
raw observation next is [1.1, 88.0, 83.33333333333334, 0.0, 26.0, 26.30586506898782, 0.6246989675444394, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.88, 0.2777777777777778, 0.0, 0.6666666666666666, 0.6921554224156518, 0.7082329891814799, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8332275], dtype=float32), 1.8492042]. 
=============================================
[2019-04-04 08:56:23,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.0624230e-10 3.8410278e-10 1.5582713e-22 2.8383977e-11 8.4748562e-12
 1.1165695e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:56:23,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3628
[2019-04-04 08:56:23,823] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.633333333333333, 77.0, 0.0, 0.0, 26.0, 24.04182751803177, 0.02211545887414687, 0.0, 1.0, 45144.29041296341], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1906800.0000, 
sim time next is 1907400.0000, 
raw observation next is [-7.716666666666667, 77.5, 0.0, 0.0, 26.0, 24.05541235920311, 0.01605240647698288, 0.0, 1.0, 45072.13526853644], 
processed observation next is [1.0, 0.043478260869565216, 0.24884579870729456, 0.775, 0.0, 0.0, 0.6666666666666666, 0.5046176966002592, 0.5053508021589943, 0.0, 1.0, 0.21462921556445924], 
reward next is 0.7854, 
noisyNet noise sample is [array([-0.40355685], dtype=float32), 1.0241828]. 
=============================================
[2019-04-04 08:56:28,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.7462469e-11 2.6980249e-10 3.2565103e-23 2.5294249e-11 4.4994854e-12
 3.2457727e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:56:28,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5242
[2019-04-04 08:56:28,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.2, 87.66666666666667, 0.0, 0.0, 26.0, 24.46721326904312, 0.1702331428976488, 0.0, 1.0, 43302.18611546865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2085600.0000, 
sim time next is 2086200.0000, 
raw observation next is [-5.3, 88.5, 0.0, 0.0, 26.0, 24.45761373424872, 0.1578820276250919, 0.0, 1.0, 43376.3593797503], 
processed observation next is [1.0, 0.13043478260869565, 0.31578947368421056, 0.885, 0.0, 0.0, 0.6666666666666666, 0.5381344778540601, 0.5526273425416973, 0.0, 1.0, 0.20655409228452523], 
reward next is 0.7934, 
noisyNet noise sample is [array([0.5905853], dtype=float32), 0.1346227]. 
=============================================
[2019-04-04 08:56:40,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5563737e-10 4.8085663e-10 3.9463044e-23 1.5216977e-11 4.9567655e-12
 1.4345762e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:56:40,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9143
[2019-04-04 08:56:40,094] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 79.0, 0.0, 0.0, 26.0, 23.77279941898912, 0.007508376851335319, 0.0, 1.0, 41901.31589277943], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2181600.0000, 
sim time next is 2182200.0000, 
raw observation next is [-6.100000000000001, 78.33333333333334, 0.0, 0.0, 26.0, 23.75849940046304, -0.0003213479278438523, 0.0, 1.0, 41891.66792788614], 
processed observation next is [1.0, 0.2608695652173913, 0.2936288088642659, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.47987495003858677, 0.499892884024052, 0.0, 1.0, 0.199484132989934], 
reward next is 0.8005, 
noisyNet noise sample is [array([0.65544724], dtype=float32), -1.8366206]. 
=============================================
[2019-04-04 08:56:40,783] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7283102e-12 1.0286625e-11 2.8153706e-25 5.5100228e-13 6.5261978e-14
 2.8587284e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:56:40,784] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3919
[2019-04-04 08:56:40,888] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 69.0, 140.0, 0.0, 26.0, 25.00787926245235, 0.339117377854758, 1.0, 1.0, 136431.5469962826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2209200.0000, 
sim time next is 2209800.0000, 
raw observation next is [-3.816666666666666, 70.0, 136.0, 0.0, 26.0, 24.80562613833888, 0.3877665631830647, 1.0, 1.0, 193867.4535741204], 
processed observation next is [1.0, 0.5652173913043478, 0.3568790397045245, 0.7, 0.4533333333333333, 0.0, 0.6666666666666666, 0.56713551152824, 0.6292555210610216, 1.0, 1.0, 0.9231783503529543], 
reward next is 0.0768, 
noisyNet noise sample is [array([0.21647464], dtype=float32), 0.790707]. 
=============================================
[2019-04-04 08:56:45,045] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0391532e-10 6.2422068e-10 4.7874678e-23 2.3331640e-11 4.6037197e-12
 3.1029221e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:56:45,045] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6550
[2019-04-04 08:56:45,098] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.13660494440588, 0.4101295452474757, 0.0, 1.0, 73821.4702849791], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.1802260734843, 0.4149596043896009, 0.0, 1.0, 53245.41020231142], 
processed observation next is [1.0, 0.8695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5983521727903582, 0.638319868129867, 0.0, 1.0, 0.25354957239195913], 
reward next is 0.7465, 
noisyNet noise sample is [array([-0.13998516], dtype=float32), -0.8967804]. 
=============================================
[2019-04-04 08:56:45,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.225494]
 [78.1894  ]
 [79.01002 ]
 [81.197784]
 [84.71601 ]], R is [[78.55583191]
 [78.41873932]
 [78.11152649]
 [77.98754883]
 [77.61981201]].
[2019-04-04 08:56:48,914] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3166668e-12 2.2977892e-11 1.4026119e-24 1.0080984e-12 1.9455924e-13
 1.5324218e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:56:48,914] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6753
[2019-04-04 08:56:49,020] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.5, 0.0, 0.0, 26.0, 25.10354775028267, 0.4387595376696549, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2224200.0000, 
sim time next is 2224800.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 25.50258222980727, 0.4731886861764458, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3379501385041552, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6252151858172725, 0.6577295620588153, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0027096], dtype=float32), -1.6952128]. 
=============================================
[2019-04-04 08:57:08,042] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1307009e-10 1.1931767e-10 6.1809492e-23 2.4516588e-11 5.4197502e-12
 8.5932638e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:08,044] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1617
[2019-04-04 08:57:08,070] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.11756734812886, 0.2891689869141399, 0.0, 1.0, 43929.79471456485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2405400.0000, 
sim time next is 2406000.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15746206337576, 0.286818246293717, 0.0, 1.0, 43326.67371916811], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.59645517194798, 0.5956060820979057, 0.0, 1.0, 0.2063174939008005], 
reward next is 0.7937, 
noisyNet noise sample is [array([0.94851094], dtype=float32), -0.276288]. 
=============================================
[2019-04-04 08:57:08,077] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.9338  ]
 [80.00778 ]
 [80.06537 ]
 [80.137405]
 [80.101585]], R is [[79.88283539]
 [79.87481689]
 [79.86270142]
 [79.83594513]
 [79.76086426]].
[2019-04-04 08:57:26,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3292826e-12 7.1644114e-11 2.3832411e-23 3.8124196e-12 3.4204876e-13
 9.3711931e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:26,153] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6822
[2019-04-04 08:57:26,201] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 25.18997216704179, 0.4243753137864168, 1.0, 1.0, 120875.0045796179], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2742000.0000, 
sim time next is 2742600.0000, 
raw observation next is [-3.833333333333333, 53.33333333333333, 0.0, 0.0, 26.0, 25.29738060647928, 0.4364256338418928, 1.0, 1.0, 53142.26142346413], 
processed observation next is [1.0, 0.7391304347826086, 0.3564173591874424, 0.5333333333333333, 0.0, 0.0, 0.6666666666666666, 0.60811505053994, 0.6454752112806309, 1.0, 1.0, 0.25305838773078154], 
reward next is 0.7469, 
noisyNet noise sample is [array([-1.6307589], dtype=float32), 0.036298953]. 
=============================================
[2019-04-04 08:57:26,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2953688e-10 8.8733698e-10 1.4134093e-22 3.7064195e-11 2.7988199e-11
 1.6336662e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:26,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3909
[2019-04-04 08:57:26,834] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 70.33333333333333, 0.0, 0.0, 26.0, 25.20506202046645, 0.3257530326318893, 0.0, 1.0, 46611.55610209408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2850600.0000, 
sim time next is 2851200.0000, 
raw observation next is [1.0, 72.0, 0.0, 0.0, 26.0, 25.14584294954889, 0.3117778658472056, 0.0, 1.0, 52209.26564110586], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5954869124624075, 0.6039259552824019, 0.0, 1.0, 0.2486155506719327], 
reward next is 0.7514, 
noisyNet noise sample is [array([-1.1268033], dtype=float32), 1.376703]. 
=============================================
[2019-04-04 08:57:33,769] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0542850e-10 3.7050715e-10 1.3566716e-23 1.9485209e-11 5.2098868e-12
 2.0958202e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:33,769] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7888
[2019-04-04 08:57:33,787] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 24.65533785013277, 0.2799074046892829, 0.0, 1.0, 42921.64832973864], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2947200.0000, 
sim time next is 2947800.0000, 
raw observation next is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 24.66953346760217, 0.2749367843015045, 0.0, 1.0, 42836.4864539621], 
processed observation next is [0.0, 0.08695652173913043, 0.3841181902123731, 0.8416666666666666, 0.0, 0.0, 0.6666666666666666, 0.5557944556335143, 0.5916455947671682, 0.0, 1.0, 0.20398326882839093], 
reward next is 0.7960, 
noisyNet noise sample is [array([-0.7560605], dtype=float32), 1.4647895]. 
=============================================
[2019-04-04 08:57:37,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1294218e-10 3.9165837e-10 6.7113029e-23 1.5657283e-11 5.1843911e-12
 5.6520482e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:37,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0024
[2019-04-04 08:57:37,643] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.40783441040387, 0.2208823290117106, 0.0, 1.0, 42894.88668086967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2952600.0000, 
sim time next is 2953200.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.40186915621378, 0.215341902463565, 0.0, 1.0, 42941.48955224399], 
processed observation next is [0.0, 0.17391304347826086, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5334890963511484, 0.5717806341545216, 0.0, 1.0, 0.20448328358211423], 
reward next is 0.7955, 
noisyNet noise sample is [array([0.90454906], dtype=float32), -0.2872221]. 
=============================================
[2019-04-04 08:57:42,980] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0212842e-11 8.2673049e-11 7.3922117e-23 1.4911017e-11 1.9980022e-12
 3.2722404e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:42,992] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1752
[2019-04-04 08:57:43,061] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 54.16666666666667, 113.0, 813.0, 26.0, 25.04108436923014, 0.3459832359257873, 0.0, 1.0, 28535.60457817365], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3067800.0000, 
sim time next is 3068400.0000, 
raw observation next is [-2.666666666666667, 53.33333333333334, 113.5, 815.0, 26.0, 25.05885292883699, 0.3526875434052281, 0.0, 1.0, 18727.83527791791], 
processed observation next is [0.0, 0.5217391304347826, 0.38873499538319484, 0.5333333333333334, 0.37833333333333335, 0.9005524861878453, 0.6666666666666666, 0.5882377440697493, 0.6175625144684094, 0.0, 1.0, 0.0891801679900853], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.744014], dtype=float32), -0.769833]. 
=============================================
[2019-04-04 08:57:43,109] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6919560e-10 4.7360216e-10 6.5633460e-22 5.2176756e-11 3.0231765e-11
 2.3513803e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:43,109] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4821
[2019-04-04 08:57:43,168] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.04662013180931, 0.3206711947734541, 0.0, 1.0, 47630.42054228159], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3003000.0000, 
sim time next is 3003600.0000, 
raw observation next is [-2.0, 60.00000000000001, 0.0, 0.0, 26.0, 25.01596971078681, 0.3201920376597005, 0.0, 1.0, 55876.80334326326], 
processed observation next is [0.0, 0.782608695652174, 0.40720221606648205, 0.6000000000000001, 0.0, 0.0, 0.6666666666666666, 0.5846641425655674, 0.6067306792199002, 0.0, 1.0, 0.2660800159203012], 
reward next is 0.7339, 
noisyNet noise sample is [array([0.6344053], dtype=float32), 0.4598837]. 
=============================================
[2019-04-04 08:57:45,518] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.7286202e-11 1.0969789e-11 1.5340408e-24 3.5025591e-12 1.7176090e-12
 1.2811345e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:45,519] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9292
[2019-04-04 08:57:45,542] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 25.52922234817845, 0.3364204492951821, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3115200.0000, 
sim time next is 3115800.0000, 
raw observation next is [1.0, 100.0, 0.0, 0.0, 26.0, 25.47587103918539, 0.3179941037637085, 0.0, 1.0, 24853.5360551642], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6229892532654491, 0.6059980345879028, 0.0, 1.0, 0.1183501716912581], 
reward next is 0.8816, 
noisyNet noise sample is [array([-2.4076211], dtype=float32), 1.1501316]. 
=============================================
[2019-04-04 08:57:48,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2929444e-11 6.7544415e-11 4.6546726e-24 6.5656577e-12 4.2168802e-13
 1.5156213e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:57:48,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6890
[2019-04-04 08:57:48,201] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 54.33333333333334, 110.1666666666667, 797.3333333333334, 26.0, 25.16557538836452, 0.3435617218844387, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3064800.0000, 
sim time next is 3065400.0000, 
raw observation next is [-3.5, 54.5, 111.0, 805.0, 26.0, 25.15223174268277, 0.337292357226428, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.36565096952908593, 0.545, 0.37, 0.8895027624309392, 0.6666666666666666, 0.5960193118902307, 0.6124307857421426, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5176032], dtype=float32), -0.42333853]. 
=============================================
[2019-04-04 08:58:09,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6990463e-12 1.2750299e-11 2.9736597e-24 5.0592332e-13 1.8245384e-13
 1.5715174e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:09,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2344
[2019-04-04 08:58:09,754] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.833333333333333, 71.0, 76.33333333333333, 627.6666666666666, 26.0, 26.58954287743919, 0.8037845349363048, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3253800.0000, 
sim time next is 3254400.0000, 
raw observation next is [-3.0, 71.0, 72.0, 598.5, 26.0, 26.78275278271611, 0.8227565158837623, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3795013850415513, 0.71, 0.24, 0.6613259668508288, 0.6666666666666666, 0.7318960652263424, 0.7742521719612542, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7541463], dtype=float32), 0.8897328]. 
=============================================
[2019-04-04 08:58:20,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3413173e-10 1.6514258e-10 5.4281770e-25 6.7708447e-12 4.6060009e-13
 2.4102219e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:20,826] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0946
[2019-04-04 08:58:20,850] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 25.0, 0.0, 0.0, 26.0, 25.49358161850459, 0.3703319787103798, 0.0, 1.0, 35230.71284634083], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3634800.0000, 
sim time next is 3635400.0000, 
raw observation next is [9.0, 25.0, 0.0, 0.0, 26.0, 25.50142325610013, 0.3705087786623166, 0.0, 1.0, 27975.74533634291], 
processed observation next is [0.0, 0.043478260869565216, 0.7119113573407203, 0.25, 0.0, 0.0, 0.6666666666666666, 0.6251186046750107, 0.6235029262207722, 0.0, 1.0, 0.13321783493496622], 
reward next is 0.8668, 
noisyNet noise sample is [array([-1.2441088], dtype=float32), -0.6936157]. 
=============================================
[2019-04-04 08:58:22,929] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3580465e-12 4.6820455e-12 2.0969520e-25 3.1136649e-13 6.5946435e-14
 2.7441604e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:22,932] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9953
[2019-04-04 08:58:22,974] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 55.33333333333334, 115.8333333333333, 820.8333333333334, 26.0, 25.62551801954501, 0.5505783075491518, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3500400.0000, 
sim time next is 3501000.0000, 
raw observation next is [2.0, 54.5, 116.0, 823.0, 26.0, 25.82078561305109, 0.5748248955394444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.518005540166205, 0.545, 0.38666666666666666, 0.9093922651933701, 0.6666666666666666, 0.6517321344209241, 0.6916082985131481, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8279929], dtype=float32), 0.8265614]. 
=============================================
[2019-04-04 08:58:23,007] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[89.110466]
 [89.15954 ]
 [89.04386 ]
 [89.1627  ]
 [89.23693 ]], R is [[89.12949371]
 [89.14924622]
 [88.9444809 ]
 [89.05503845]
 [89.16448975]].
[2019-04-04 08:58:29,560] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6287982e-12 1.8561238e-11 1.2978920e-24 6.0533425e-13 4.1307303e-13
 4.6894298e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:29,563] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7639
[2019-04-04 08:58:29,585] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 69.0, 0.0, 0.0, 26.0, 25.43814747587798, 0.4772593069546264, 1.0, 1.0, 51257.83019201336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3784800.0000, 
sim time next is 3785400.0000, 
raw observation next is [-2.0, 68.0, 0.0, 0.0, 26.0, 25.33324295657602, 0.4632854297136327, 0.0, 1.0, 34868.42543807897], 
processed observation next is [1.0, 0.8260869565217391, 0.40720221606648205, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6111035797146682, 0.6544284765712108, 0.0, 1.0, 0.1660401211337094], 
reward next is 0.8340, 
noisyNet noise sample is [array([-0.15150025], dtype=float32), 0.4758233]. 
=============================================
[2019-04-04 08:58:33,736] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.5502395e-12 5.8691186e-12 3.2243679e-25 5.0279876e-13 9.3823935e-14
 2.0848469e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:33,739] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8643
[2019-04-04 08:58:33,751] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 40.5, 343.0, 26.0, 26.43610554846064, 0.6156232288423917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3776400.0000, 
sim time next is 3777000.0000, 
raw observation next is [-0.3333333333333333, 61.83333333333333, 32.66666666666666, 277.6666666666666, 26.0, 26.20108427603206, 0.5821953554052103, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4533702677747, 0.6183333333333333, 0.10888888888888885, 0.30681399631675865, 0.6666666666666666, 0.6834236896693383, 0.6940651184684034, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.558881], dtype=float32), 0.061654765]. 
=============================================
[2019-04-04 08:58:33,764] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[86.10912]
 [86.79973]
 [87.70173]
 [88.34912]
 [88.59575]], R is [[85.58934784]
 [85.73345184]
 [85.87612152]
 [86.0173645 ]
 [86.15718842]].
[2019-04-04 08:58:38,472] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.06134526e-10 1.86340013e-10 1.03002515e-22 1.70556364e-11
 1.28964868e-11 7.53945382e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 08:58:38,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9509
[2019-04-04 08:58:38,491] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.92304421986042, 0.2943063824581099, 0.0, 1.0, 41754.01769909492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3909600.0000, 
sim time next is 3910200.0000, 
raw observation next is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.90839451530307, 0.295918252242036, 0.0, 1.0, 41901.63964919668], 
processed observation next is [1.0, 0.2608695652173913, 0.29178208679593726, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.5756995429419224, 0.598639417414012, 0.0, 1.0, 0.19953161737712705], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.1570838], dtype=float32), -0.48883882]. 
=============================================
[2019-04-04 08:58:41,888] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.2013313e-10 2.2862971e-10 4.4763421e-22 6.7534624e-11 7.4592745e-12
 4.6704036e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:41,888] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0089
[2019-04-04 08:58:41,933] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.17201008482544, 0.3697923687557649, 0.0, 1.0, 40756.54499651513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4059600.0000, 
sim time next is 4060200.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.1496286089317, 0.3628116850914495, 0.0, 1.0, 40721.00678499127], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.37, 0.0, 0.0, 0.6666666666666666, 0.5958023840776416, 0.6209372283638165, 0.0, 1.0, 0.19390955611900604], 
reward next is 0.8061, 
noisyNet noise sample is [array([-0.6483059], dtype=float32), -0.025893563]. 
=============================================
[2019-04-04 08:58:43,975] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6663204e-12 7.2740941e-12 2.1120376e-24 8.0865750e-13 9.7675591e-14
 4.5403142e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:43,976] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8261
[2019-04-04 08:58:44,012] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 35.33333333333334, 69.66666666666666, 567.3333333333334, 26.0, 27.21722892827036, 0.4912751474781973, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3946800.0000, 
sim time next is 3947400.0000, 
raw observation next is [-4.5, 36.0, 66.0, 536.0, 26.0, 27.05341432342756, 0.7517978244821943, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.36, 0.22, 0.5922651933701657, 0.6666666666666666, 0.7544511936189634, 0.7505992748273981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.265885], dtype=float32), -0.23775661]. 
=============================================
[2019-04-04 08:58:44,209] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.6113377e-10 7.4809681e-10 6.5071291e-22 1.0497442e-10 3.4402901e-11
 1.7037638e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:44,212] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3630
[2019-04-04 08:58:44,240] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.66666666666667, 67.0, 0.0, 0.0, 26.0, 23.98358378727368, 0.06950128421111705, 0.0, 1.0, 43753.08724385611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3991200.0000, 
sim time next is 3991800.0000, 
raw observation next is [-12.83333333333333, 68.0, 0.0, 0.0, 26.0, 23.9206066475089, 0.0562749376262967, 0.0, 1.0, 43786.69438677255], 
processed observation next is [1.0, 0.17391304347826086, 0.10710987996306563, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4933838872924084, 0.5187583125420989, 0.0, 1.0, 0.20850806850844072], 
reward next is 0.7915, 
noisyNet noise sample is [array([-0.4526223], dtype=float32), -0.55251855]. 
=============================================
[2019-04-04 08:58:46,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.67791126e-12 1.01905455e-11 1.47851133e-25 8.31132960e-13
 8.50371748e-14 1.01890281e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 08:58:46,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3175
[2019-04-04 08:58:46,917] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 56.33333333333334, 76.83333333333334, 415.5000000000001, 26.0, 25.66757814434812, 0.4581106897049016, 1.0, 1.0, 9370.64502307263], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3918000.0000, 
sim time next is 3918600.0000, 
raw observation next is [-8.0, 55.5, 91.0, 466.0, 26.0, 25.77258075045681, 0.4679726537523053, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24099722991689754, 0.555, 0.30333333333333334, 0.5149171270718232, 0.6666666666666666, 0.6477150625380675, 0.6559908845841017, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.12170792], dtype=float32), 0.45167923]. 
=============================================
[2019-04-04 08:58:48,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.31334327e-11 1.13157626e-11 1.27588933e-25 5.88570945e-12
 3.81084026e-13 8.03967145e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 08:58:48,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0346
[2019-04-04 08:58:48,778] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666666, 60.0, 20.16666666666666, 213.5, 26.0, 25.44921049896939, 0.4333973043121939, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3915600.0000, 
sim time next is 3916200.0000, 
raw observation next is [-7.833333333333334, 59.0, 34.33333333333333, 264.0, 26.0, 25.65664911459719, 0.4368561592241547, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.2456140350877193, 0.59, 0.11444444444444443, 0.29171270718232045, 0.6666666666666666, 0.6380540928830992, 0.6456187197413848, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4678522], dtype=float32), -0.43173093]. 
=============================================
[2019-04-04 08:58:53,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.9646663e-12 1.9356412e-11 1.8408594e-24 1.2501748e-12 3.1796216e-13
 3.9243731e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:58:53,159] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3169
[2019-04-04 08:58:53,167] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 27.66666666666667, 0.0, 0.0, 26.0, 25.91552180482203, 0.5156275082306138, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4040400.0000, 
sim time next is 4041000.0000, 
raw observation next is [-3.5, 28.5, 0.0, 0.0, 26.0, 25.76516500908979, 0.4927298755243791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.36565096952908593, 0.285, 0.0, 0.0, 0.6666666666666666, 0.647097084090816, 0.6642432918414597, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1442301], dtype=float32), 1.4137217]. 
=============================================
[2019-04-04 08:58:53,201] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[84.460396]
 [84.32125 ]
 [84.627815]
 [86.008995]
 [86.277954]], R is [[84.76250458]
 [84.91487885]
 [85.06572723]
 [85.21507263]
 [85.36292267]].
[2019-04-04 08:59:03,896] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.5328524e-12 1.4391792e-11 3.6663928e-25 3.8129341e-13 2.2364349e-13
 2.0127559e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:03,896] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8780
[2019-04-04 08:59:03,923] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 80.33333333333334, 67.33333333333334, 0.0, 26.0, 26.21016329910996, 0.6161901060557948, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4462800.0000, 
sim time next is 4463400.0000, 
raw observation next is [0.0, 79.16666666666666, 63.66666666666666, 0.0, 26.0, 26.27854590195335, 0.6133649160351088, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.7916666666666665, 0.2122222222222222, 0.0, 0.6666666666666666, 0.6898788251627792, 0.7044549720117029, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34099], dtype=float32), -0.3338852]. 
=============================================
[2019-04-04 08:59:07,472] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5706777e-12 1.3886427e-12 1.6821923e-26 1.9521421e-13 2.9535063e-14
 1.7244596e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:07,472] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2595
[2019-04-04 08:59:07,486] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.533333333333333, 82.66666666666667, 127.5, 198.5, 26.0, 25.88770848022005, 0.5788145603027993, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4437600.0000, 
sim time next is 4438200.0000, 
raw observation next is [1.416666666666667, 83.33333333333333, 135.0, 165.0, 26.0, 26.03915394114394, 0.5998196624173474, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5018467220683288, 0.8333333333333333, 0.45, 0.18232044198895028, 0.6666666666666666, 0.6699294950953284, 0.6999398874724491, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04386025], dtype=float32), 1.2084349]. 
=============================================
[2019-04-04 08:59:07,783] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2702428e-12 2.0376387e-12 4.4926873e-26 2.1811390e-13 4.2887779e-14
 2.0013852e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:07,786] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4325
[2019-04-04 08:59:07,799] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 26.0, 26.48342636703084, 0.6621030946930381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [1.0, 86.0, 196.5, 73.0, 26.0, 26.50146284245206, 0.6608231045004946, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.655, 0.08066298342541436, 0.6666666666666666, 0.708455236871005, 0.7202743681668315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2725425], dtype=float32), -0.56359035]. 
=============================================
[2019-04-04 08:59:07,810] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.87423 ]
 [91.279915]
 [92.2648  ]
 [92.19212 ]
 [92.00993 ]], R is [[88.65605164]
 [88.7694931 ]
 [88.88179779]
 [88.99298096]
 [89.10305023]].
[2019-04-04 08:59:14,153] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6559489e-10 1.2976858e-10 1.4863053e-23 5.8077046e-12 2.4091775e-12
 2.2038198e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:14,155] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5238
[2019-04-04 08:59:14,178] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 62.66666666666666, 0.0, 0.0, 26.0, 25.47105579201749, 0.4512425031086456, 0.0, 1.0, 21943.95911458535], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4582200.0000, 
sim time next is 4582800.0000, 
raw observation next is [0.4, 63.0, 0.0, 0.0, 26.0, 25.42949719234343, 0.4466189168226791, 0.0, 1.0, 48480.70662007505], 
processed observation next is [1.0, 0.043478260869565216, 0.4736842105263158, 0.63, 0.0, 0.0, 0.6666666666666666, 0.6191247660286191, 0.6488729722742264, 0.0, 1.0, 0.2308605077146431], 
reward next is 0.7691, 
noisyNet noise sample is [array([1.8161533], dtype=float32), -0.33051494]. 
=============================================
[2019-04-04 08:59:14,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6200225e-11 1.3911122e-10 8.5561597e-24 1.0763505e-11 3.9320062e-12
 2.6909734e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:14,964] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3574
[2019-04-04 08:59:14,974] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.2, 62.33333333333334, 0.0, 0.0, 26.0, 25.69781373803192, 0.5829860494049286, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4404000.0000, 
sim time next is 4404600.0000, 
raw observation next is [8.05, 62.5, 0.0, 0.0, 26.0, 25.64159423997291, 0.5986162173265589, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 1.0, 0.6855955678670361, 0.625, 0.0, 0.0, 0.6666666666666666, 0.6367995199977425, 0.6995387391088529, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([0.16228521], dtype=float32), 1.3218361]. 
=============================================
[2019-04-04 08:59:26,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.6521321e-11 2.2998719e-11 7.3337263e-25 2.3253569e-12 8.7990314e-13
 7.4385764e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:26,957] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9911
[2019-04-04 08:59:27,019] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 79.5, 140.6666666666667, 419.6666666666667, 26.0, 25.47092022751614, 0.4561575605218042, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4783800.0000, 
sim time next is 4784400.0000, 
raw observation next is [-5.0, 77.0, 149.0, 420.0, 26.0, 25.63661838861924, 0.4661146066203857, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.32409972299168976, 0.77, 0.49666666666666665, 0.46408839779005523, 0.6666666666666666, 0.63638486571827, 0.6553715355401285, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99110734], dtype=float32), -1.2531013]. 
=============================================
[2019-04-04 08:59:33,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:33,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:33,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run23
[2019-04-04 08:59:39,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:39,623] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:39,644] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run23
[2019-04-04 08:59:40,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:40,018] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:40,032] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run23
[2019-04-04 08:59:42,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1376666e-10 2.6882396e-10 5.1972031e-23 1.2946815e-11 8.4275460e-12
 5.8401566e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:42,042] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5938
[2019-04-04 08:59:42,056] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 51.66666666666666, 0.0, 0.0, 26.0, 25.40701204364291, 0.3933680615606672, 0.0, 1.0, 28384.51467155455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5028000.0000, 
sim time next is 5028600.0000, 
raw observation next is [-1.0, 50.83333333333334, 0.0, 0.0, 26.0, 25.40576046550209, 0.3879413481717256, 0.0, 1.0, 33805.45743529902], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.5083333333333334, 0.0, 0.0, 0.6666666666666666, 0.6171467054585076, 0.6293137827239085, 0.0, 1.0, 0.16097836873951912], 
reward next is 0.8390, 
noisyNet noise sample is [array([-1.2095112], dtype=float32), 1.3279233]. 
=============================================
[2019-04-04 08:59:43,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:43,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:43,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run23
[2019-04-04 08:59:43,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:43,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:43,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run23
[2019-04-04 08:59:43,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:43,806] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:43,812] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run23
[2019-04-04 08:59:43,934] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:43,935] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:43,938] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run23
[2019-04-04 08:59:44,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:44,453] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:44,456] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run23
[2019-04-04 08:59:46,710] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:46,711] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:46,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run23
[2019-04-04 08:59:47,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:47,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:47,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run23
[2019-04-04 08:59:47,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:47,200] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:47,203] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run23
[2019-04-04 08:59:49,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:49,561] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:49,575] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run23
[2019-04-04 08:59:51,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:51,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:51,333] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run23
[2019-04-04 08:59:54,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:54,273] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:54,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run23
[2019-04-04 08:59:54,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:54,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:54,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run23
[2019-04-04 08:59:56,082] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 08:59:56,082] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 08:59:56,085] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run23
[2019-04-04 08:59:58,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0124128e-13 5.8625509e-12 1.6076364e-24 1.8492594e-13 8.0657906e-14
 3.1637445e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 08:59:58,515] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0716
[2019-04-04 08:59:58,576] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 61.0, 134.5, 543.5, 26.0, 25.58258430707023, 0.4206556198014363, 1.0, 1.0, 38982.82722510967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 133200.0000, 
sim time next is 133800.0000, 
raw observation next is [-7.616666666666667, 61.0, 136.0, 523.6666666666666, 26.0, 25.64353955327216, 0.4294092867374479, 1.0, 1.0, 34624.99955232429], 
processed observation next is [1.0, 0.5652173913043478, 0.2516158818097876, 0.61, 0.4533333333333333, 0.5786372007366483, 0.6666666666666666, 0.6369616294393466, 0.6431364289124827, 1.0, 1.0, 0.1648809502491633], 
reward next is 0.8351, 
noisyNet noise sample is [array([0.7411344], dtype=float32), -0.058149006]. 
=============================================
[2019-04-04 09:00:05,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5507615e-10 3.6216952e-10 1.1156106e-22 2.2908793e-11 2.5487530e-11
 1.6782422e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:00:05,685] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1075
[2019-04-04 09:00:05,755] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 68.0, 0.0, 0.0, 26.0, 24.1150240080375, 0.1411152908471363, 0.0, 1.0, 158974.5997729312], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 114000.0000, 
sim time next is 114600.0000, 
raw observation next is [-7.299999999999999, 68.0, 12.33333333333333, 2.999999999999999, 26.0, 24.51828861482019, 0.199469939182974, 1.0, 1.0, 109956.4547632211], 
processed observation next is [1.0, 0.30434782608695654, 0.2603878116343491, 0.68, 0.0411111111111111, 0.0033149171270718224, 0.6666666666666666, 0.5431907179016825, 0.566489979727658, 1.0, 1.0, 0.5236021655391481], 
reward next is 0.4764, 
noisyNet noise sample is [array([0.4373522], dtype=float32), -0.018316844]. 
=============================================
[2019-04-04 09:00:12,075] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2292607e-10 1.8941611e-09 5.3716644e-21 8.2600080e-11 5.8149676e-11
 4.5033728e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:00:12,075] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0448
[2019-04-04 09:00:12,111] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.816666666666666, 73.5, 0.0, 0.0, 26.0, 23.73807492215903, 0.002853394037237481, 0.0, 1.0, 44402.62783155789], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 175800.0000, 
sim time next is 176400.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.72503635443777, -0.007172114961741953, 0.0, 1.0, 44412.14353040721], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 0.6666666666666666, 0.4770863628698141, 0.4976092950127527, 0.0, 1.0, 0.21148639776384384], 
reward next is 0.7885, 
noisyNet noise sample is [array([-0.00280154], dtype=float32), 1.2250868]. 
=============================================
[2019-04-04 09:00:12,811] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6298746e-12 5.1938457e-11 3.2016825e-24 2.3244835e-12 3.6287008e-13
 6.7993283e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:00:12,829] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2085
[2019-04-04 09:00:12,913] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666666, 76.0, 86.5, 0.0, 26.0, 25.39277262474626, 0.2186697964266428, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 207600.0000, 
sim time next is 208200.0000, 
raw observation next is [-7.483333333333333, 75.5, 94.0, 0.0, 26.0, 25.39066981908058, 0.211145755858431, 1.0, 1.0, 18738.8136536837], 
processed observation next is [1.0, 0.391304347826087, 0.25530932594644506, 0.755, 0.31333333333333335, 0.0, 0.6666666666666666, 0.6158891515900482, 0.5703819186194771, 1.0, 1.0, 0.08923244596992239], 
reward next is 0.9108, 
noisyNet noise sample is [array([1.0791739], dtype=float32), 0.040385623]. 
=============================================
[2019-04-04 09:00:14,695] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5560384e-10 7.5072115e-10 1.0193312e-21 5.3461693e-11 1.8130922e-11
 2.4549880e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:00:14,696] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9549
[2019-04-04 09:00:14,710] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.7918903075572, -0.2372427111298757, 0.0, 1.0, 44943.37009409874], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 193800.0000, 
sim time next is 194400.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.74138015727413, -0.2438855594443943, 0.0, 1.0, 44958.22572610667], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3951150131061774, 0.4187048135185352, 0.0, 1.0, 0.21408678917193652], 
reward next is 0.7859, 
noisyNet noise sample is [array([0.98843837], dtype=float32), 0.300904]. 
=============================================
[2019-04-04 09:00:14,771] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.6100140e-09 6.2419483e-09 2.2154029e-20 2.1215246e-10 2.1785354e-10
 3.9726837e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 09:00:14,772] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5375
[2019-04-04 09:00:14,796] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.2, 67.5, 0.0, 0.0, 26.0, 22.49436981691802, -0.271494243364201, 0.0, 1.0, 47923.38660380243], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 283800.0000, 
sim time next is 284400.0000, 
raw observation next is [-12.3, 67.0, 0.0, 0.0, 26.0, 22.52900624515986, -0.2781489002776362, 0.0, 1.0, 47957.80718720323], 
processed observation next is [1.0, 0.30434782608695654, 0.12188365650969527, 0.67, 0.0, 0.0, 0.6666666666666666, 0.37741718709665495, 0.40728369990745455, 0.0, 1.0, 0.22837051041525347], 
reward next is 0.7716, 
noisyNet noise sample is [array([-0.28712562], dtype=float32), 1.595036]. 
=============================================
[2019-04-04 09:00:20,175] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8803107e-10 1.6155393e-09 2.4697818e-21 3.6503283e-11 3.6395109e-11
 6.9622536e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:00:20,175] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0369
[2019-04-04 09:00:20,285] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.683333333333334, 69.5, 0.0, 0.0, 26.0, 23.3323253809277, -0.1032875480473283, 0.0, 1.0, 46717.53386556815], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 274200.0000, 
sim time next is 274800.0000, 
raw observation next is [-9.866666666666667, 69.0, 0.0, 0.0, 26.0, 23.29248462940915, -0.1180826243502077, 0.0, 1.0, 46905.79287988741], 
processed observation next is [1.0, 0.17391304347826086, 0.18928901200369344, 0.69, 0.0, 0.0, 0.6666666666666666, 0.4410403857840957, 0.4606391252165974, 0.0, 1.0, 0.22336091847565434], 
reward next is 0.7766, 
noisyNet noise sample is [array([-1.8371431], dtype=float32), -0.45317498]. 
=============================================
[2019-04-04 09:00:31,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.16915047e-10 2.22907526e-10 7.42457406e-22 1.24282425e-11
 3.47459457e-11 1.28643393e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 09:00:31,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0611
[2019-04-04 09:00:31,268] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 24.08343087015335, 0.0535814927209981, 0.0, 1.0, 45188.94413755413], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 264000.0000, 
sim time next is 264600.0000, 
raw observation next is [-7.0, 69.0, 0.0, 0.0, 26.0, 24.01987610252099, 0.04883353517894815, 0.0, 1.0, 45326.65466958711], 
processed observation next is [1.0, 0.043478260869565216, 0.2686980609418283, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5016563418767491, 0.5162778450596494, 0.0, 1.0, 0.21584121271231957], 
reward next is 0.7842, 
noisyNet noise sample is [array([0.68332356], dtype=float32), -1.012366]. 
=============================================
[2019-04-04 09:00:32,510] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 09:00:32,511] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:00:32,511] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:32,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run31
[2019-04-04 09:00:32,577] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:00:32,578] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:32,583] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:00:32,583] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:00:32,585] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run31
[2019-04-04 09:00:32,711] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run31
[2019-04-04 09:01:30,342] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.22251114], dtype=float32), -0.20147961]
[2019-04-04 09:01:30,342] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.43333333333333, 52.33333333333334, 103.3333333333333, 648.8333333333334, 26.0, 27.0156342076539, 0.7793022697104376, 1.0, 1.0, 0.0]
[2019-04-04 09:01:30,342] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:01:30,343] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.5940078e-13 1.3259142e-12 3.6014067e-26 8.7324177e-14 4.0507863e-14
 2.0703424e-16 1.0000000e+00], sampled 0.24390078587671493
[2019-04-04 09:02:59,919] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.22251114], dtype=float32), -0.20147961]
[2019-04-04 09:02:59,919] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.0, 87.16666666666667, 0.0, 0.0, 26.0, 25.08633935751141, 0.3048864958962967, 0.0, 1.0, 52562.95092755607]
[2019-04-04 09:02:59,919] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:02:59,919] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.2181152e-11 1.2165986e-10 8.4419729e-24 7.8801271e-12 6.1570631e-12
 2.4896362e-14 1.0000000e+00], sampled 0.32330770082966886
[2019-04-04 09:03:03,212] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.22251114], dtype=float32), -0.20147961]
[2019-04-04 09:03:03,217] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.169231546333333, 91.85359535500001, 0.0, 0.0, 26.0, 24.24194737684743, 0.1111685234693107, 0.0, 1.0, 52463.24959073274]
[2019-04-04 09:03:03,217] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:03:03,218] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.3733880e-11 9.0234036e-11 8.7162322e-24 5.8778342e-12 5.1624850e-12
 2.0393123e-14 1.0000000e+00], sampled 0.1790194060024437
[2019-04-04 09:03:46,977] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 09:03:52,337] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.22251114], dtype=float32), -0.20147961]
[2019-04-04 09:03:52,338] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [2.0, 48.0, 0.0, 0.0, 26.0, 25.39813240381957, 0.3530482738635543, 0.0, 1.0, 48542.88099196129]
[2019-04-04 09:03:52,338] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:03:52,339] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.0343134e-10 1.1914755e-10 2.3071864e-23 1.2665514e-11 6.9087349e-12
 4.4058012e-14 1.0000000e+00], sampled 0.502239651068054
[2019-04-04 09:04:19,531] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:04:25,811] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:04:26,846] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 3000000, evaluation results [3000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:04:31,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5406968e-12 2.0334893e-11 2.0769878e-23 2.7363992e-12 3.3415580e-13
 5.8684827e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:04:31,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2992
[2019-04-04 09:04:31,933] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 44.0, 93.0, 652.1666666666666, 26.0, 25.97094155626662, 0.5032931614020371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 307200.0000, 
sim time next is 307800.0000, 
raw observation next is [-9.5, 44.0, 95.0, 631.0, 26.0, 26.15356354260403, 0.5193641246158939, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.31666666666666665, 0.6972375690607735, 0.6666666666666666, 0.6794636285503358, 0.6731213748719647, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04720912], dtype=float32), 0.87292635]. 
=============================================
[2019-04-04 09:04:38,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.2952292e-10 1.2132646e-09 9.2341468e-22 5.4812030e-11 3.9179562e-11
 7.0975403e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:04:38,396] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5525
[2019-04-04 09:04:38,422] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.42712149837105, -0.1163161861369142, 0.0, 1.0, 45688.52046904894], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 438600.0000, 
sim time next is 439200.0000, 
raw observation next is [-11.2, 55.0, 0.0, 0.0, 26.0, 23.34885006994265, -0.1232588294714476, 0.0, 1.0, 45747.35838805421], 
processed observation next is [1.0, 0.08695652173913043, 0.15235457063711913, 0.55, 0.0, 0.0, 0.6666666666666666, 0.44573750582855415, 0.4589137235095175, 0.0, 1.0, 0.21784456375263908], 
reward next is 0.7822, 
noisyNet noise sample is [array([1.8633661], dtype=float32), -0.14047302]. 
=============================================
[2019-04-04 09:05:00,083] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.7177169e-11 6.7190156e-11 1.5453716e-23 7.2349774e-12 5.6643895e-12
 9.5922534e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:05:00,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3182
[2019-04-04 09:05:00,132] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.88366717523854, 0.2161671909210693, 0.0, 1.0, 40500.7359990539], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 666600.0000, 
sim time next is 667200.0000, 
raw observation next is [-1.2, 57.00000000000001, 0.0, 0.0, 26.0, 24.92469601497108, 0.2141380510020773, 0.0, 1.0, 18736.21245953262], 
processed observation next is [0.0, 0.7391304347826086, 0.42936288088642666, 0.5700000000000001, 0.0, 0.0, 0.6666666666666666, 0.57705800124759, 0.5713793503340258, 0.0, 1.0, 0.08922005933110772], 
reward next is 0.9108, 
noisyNet noise sample is [array([-1.2430792], dtype=float32), 1.064577]. 
=============================================
[2019-04-04 09:05:32,177] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6673371e-12 6.5876853e-12 2.5479036e-25 6.8716712e-13 1.3823658e-13
 1.4780846e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:05:32,181] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8832
[2019-04-04 09:05:32,219] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 26.0, 25.9545985033458, 0.4248330232141264, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 834600.0000, 
sim time next is 835200.0000, 
raw observation next is [-3.9, 82.0, 39.0, 0.0, 26.0, 26.01863144467182, 0.4054041425763033, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3545706371191136, 0.82, 0.13, 0.0, 0.6666666666666666, 0.6682192870559849, 0.6351347141921011, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.991332], dtype=float32), -0.12729028]. 
=============================================
[2019-04-04 09:05:37,897] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7843079e-13 2.1456582e-12 2.3650685e-27 6.5662523e-14 4.3215082e-14
 2.4223784e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:05:37,900] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4512
[2019-04-04 09:05:37,973] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 97.33333333333333, 0.0, 0.0, 26.0, 24.86895971307145, 0.2533145789868047, 1.0, 1.0, 97064.194441834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 930000.0000, 
sim time next is 930600.0000, 
raw observation next is [4.4, 98.0, 0.0, 0.0, 26.0, 24.66757603789744, 0.2628617678631753, 1.0, 1.0, 177251.2187402372], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.98, 0.0, 0.0, 0.6666666666666666, 0.5556313364914534, 0.5876205892877251, 1.0, 1.0, 0.8440534225725581], 
reward next is 0.1559, 
noisyNet noise sample is [array([0.35809842], dtype=float32), 1.1619518]. 
=============================================
[2019-04-04 09:05:41,473] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.01411892e-14 5.15652842e-13 1.31222202e-28 8.76351468e-15
 1.00704425e-14 1.93985980e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 09:05:41,474] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1372
[2019-04-04 09:05:41,483] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 26.09443237362697, 0.5721203169892283, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1015200.0000, 
sim time next is 1015800.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.99844262222411, 0.5712475465055048, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6665368851853426, 0.6904158488351683, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3767275], dtype=float32), -0.45669475]. 
=============================================
[2019-04-04 09:05:44,726] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3644645e-12 1.5901050e-12 2.8989714e-26 8.7562011e-14 3.6075062e-14
 1.2559561e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:05:44,731] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8495
[2019-04-04 09:05:44,760] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23977803303761, 0.4092096379802662, 0.0, 1.0, 38549.2671142858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 948600.0000, 
sim time next is 949200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.24021052798343, 0.4145045783637066, 0.0, 1.0, 38454.95948723696], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6033508773319524, 0.6381681927879023, 0.0, 1.0, 0.1831188547011284], 
reward next is 0.8169, 
noisyNet noise sample is [array([-0.18431059], dtype=float32), -0.09394464]. 
=============================================
[2019-04-04 09:05:46,373] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4080721e-13 1.0382680e-11 4.3476898e-26 1.1237225e-13 3.6225116e-13
 1.7128757e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:05:46,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9952
[2019-04-04 09:05:46,382] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.08333333333333, 78.66666666666667, 0.0, 0.0, 26.0, 25.65863468838921, 0.6335016934821194, 0.0, 1.0, 24076.0528859765], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1129800.0000, 
sim time next is 1130400.0000, 
raw observation next is [10.0, 79.0, 0.0, 0.0, 26.0, 25.64847160358162, 0.6311922815301574, 0.0, 1.0, 29239.58828615332], 
processed observation next is [0.0, 0.08695652173913043, 0.739612188365651, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6373726336318016, 0.7103974271767192, 0.0, 1.0, 0.13923613469596818], 
reward next is 0.8608, 
noisyNet noise sample is [array([-1.1135608], dtype=float32), -0.9676705]. 
=============================================
[2019-04-04 09:05:49,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3282075e-12 2.4335070e-12 2.0938564e-27 1.1832687e-13 1.3387058e-13
 5.0204549e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:05:49,087] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9723
[2019-04-04 09:05:49,117] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.86774153011006, 0.6026986746054934, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1032600.0000, 
sim time next is 1033200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.85261723437053, 0.5938972745220825, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6543847695308775, 0.6979657581740275, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.89147013], dtype=float32), 1.6641467]. 
=============================================
[2019-04-04 09:05:55,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.1863034e-11 7.4769227e-12 7.1009062e-26 2.0150522e-12 5.5334350e-13
 2.4016026e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:05:55,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2155
[2019-04-04 09:05:55,830] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 78.66666666666667, 0.0, 0.0, 26.0, 24.13840733661447, 0.2807690691417994, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1210800.0000, 
sim time next is 1211400.0000, 
raw observation next is [16.1, 79.0, 0.0, 0.0, 26.0, 24.11465165540062, 0.2809293106884665, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5095543046167185, 0.5936431035628221, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.49140957], dtype=float32), -0.75857854]. 
=============================================
[2019-04-04 09:06:16,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5109522e-11 7.0048044e-11 8.9104821e-24 3.6471113e-12 4.5412758e-12
 2.6184390e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:16,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9985
[2019-04-04 09:06:16,958] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.3, 73.0, 184.0, 81.0, 26.0, 24.94890292938419, 0.2594565638724239, 0.0, 1.0, 53024.40544159035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1855800.0000, 
sim time next is 1856400.0000, 
raw observation next is [-5.199999999999999, 72.33333333333333, 173.3333333333333, 67.5, 26.0, 24.9533912057362, 0.2616909529673233, 0.0, 1.0, 46737.89656451304], 
processed observation next is [0.0, 0.4782608695652174, 0.31855955678670367, 0.7233333333333333, 0.5777777777777776, 0.07458563535911603, 0.6666666666666666, 0.5794492671446833, 0.5872303176557744, 0.0, 1.0, 0.22256141221196685], 
reward next is 0.7774, 
noisyNet noise sample is [array([1.4785739], dtype=float32), -1.2617898]. 
=============================================
[2019-04-04 09:06:20,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9505543e-12 8.4228895e-11 4.2484384e-24 1.5926091e-12 7.8880180e-13
 1.6308820e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:20,555] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2076
[2019-04-04 09:06:20,767] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.35457301732419, 0.4735240581696719, 0.0, 1.0, 43295.09364771321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1728000.0000, 
sim time next is 1728600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.35190556326903, 0.4716885264051141, 0.0, 1.0, 43196.18831257158], 
processed observation next is [0.0, 0.0, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.612658796939086, 0.6572295088017047, 0.0, 1.0, 0.20569613482176943], 
reward next is 0.7943, 
noisyNet noise sample is [array([-0.34886855], dtype=float32), -0.87084085]. 
=============================================
[2019-04-04 09:06:24,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.34358280e-10 7.86795518e-10 5.96975440e-21 1.05627763e-10
 1.81160573e-11 1.17036068e-12 1.00000000e+00], sum to 1.0000
[2019-04-04 09:06:24,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8793
[2019-04-04 09:06:24,203] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 129.0, 42.0, 26.0, 25.05681356324504, 0.2856416625221069, 0.0, 1.0, 27582.27758590728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1868400.0000, 
sim time next is 1869000.0000, 
raw observation next is [-4.5, 81.66666666666667, 110.0, 27.99999999999999, 26.0, 25.07948981906921, 0.2776306576923559, 0.0, 1.0, 21307.94226258704], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.8166666666666668, 0.36666666666666664, 0.030939226519337004, 0.6666666666666666, 0.5899574849224342, 0.5925435525641186, 0.0, 1.0, 0.10146639172660495], 
reward next is 0.8985, 
noisyNet noise sample is [array([0.0960537], dtype=float32), 1.504794]. 
=============================================
[2019-04-04 09:06:24,207] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[72.57429 ]
 [73.278   ]
 [73.95234 ]
 [74.449356]
 [74.3595  ]], R is [[72.10678101]
 [72.25436401]
 [72.34353638]
 [72.41464996]
 [72.47476196]].
[2019-04-04 09:06:24,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8514574e-12 1.4349280e-11 1.6461999e-24 1.0034445e-12 1.6157486e-13
 1.7077606e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:24,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5731
[2019-04-04 09:06:24,697] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 0.0, 0.0, 26.0, 25.20181073631178, 0.3301090335899205, 1.0, 1.0, 22333.31716755708], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1708200.0000, 
sim time next is 1708800.0000, 
raw observation next is [1.1, 88.0, 0.0, 0.0, 26.0, 24.3805760332082, 0.3326486565884603, 1.0, 1.0, 196524.2986362654], 
processed observation next is [1.0, 0.782608695652174, 0.49307479224376743, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5317146694340167, 0.6108828855294868, 1.0, 1.0, 0.9358299935060257], 
reward next is 0.0642, 
noisyNet noise sample is [array([0.73626727], dtype=float32), -0.013848956]. 
=============================================
[2019-04-04 09:06:28,330] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.0530014e-10 5.0446136e-10 5.4934729e-21 7.6258215e-11 3.1200469e-11
 5.3206609e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:28,332] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8845
[2019-04-04 09:06:28,463] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.00000000000001, 0.0, 0.0, 26.0, 23.10268740586633, -0.1985076971403363, 0.0, 1.0, 44502.11013177613], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1926600.0000, 
sim time next is 1927200.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.082246176207, -0.0971919606688255, 1.0, 1.0, 202359.7642434866], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4235205146839167, 0.4676026797770582, 1.0, 1.0, 0.9636179249689838], 
reward next is 0.0364, 
noisyNet noise sample is [array([-0.871642], dtype=float32), 0.7168035]. 
=============================================
[2019-04-04 09:06:29,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5909602e-10 1.1634739e-09 6.4838379e-22 5.5182636e-11 7.1908743e-11
 1.0879455e-12 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:29,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4614
[2019-04-04 09:06:29,569] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 14.0, 0.0, 26.0, 23.21336146354057, -0.108682403510682, 0.0, 1.0, 47097.94790349197], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1843200.0000, 
sim time next is 1843800.0000, 
raw observation next is [-6.700000000000001, 78.0, 18.33333333333334, 0.0, 26.0, 23.19151571695981, -0.1132462454846014, 0.0, 1.0, 47043.8616952466], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.06111111111111113, 0.0, 0.6666666666666666, 0.4326263097466508, 0.46225125150513285, 0.0, 1.0, 0.22401838902498383], 
reward next is 0.7760, 
noisyNet noise sample is [array([0.55656385], dtype=float32), -0.18918496]. 
=============================================
[2019-04-04 09:06:55,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1091601e-12 2.6791114e-11 2.7025122e-24 7.9044795e-13 2.4196023e-13
 1.9160334e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:55,026] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9640
[2019-04-04 09:06:55,130] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.2, 84.0, 71.0, 0.0, 26.0, 25.46191721817112, 0.3418933799540489, 1.0, 1.0, 66972.36192178019], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2043000.0000, 
sim time next is 2043600.0000, 
raw observation next is [-4.1, 83.33333333333334, 64.5, 0.0, 26.0, 24.69863374516758, 0.3274181905613234, 1.0, 1.0, 198319.4969898018], 
processed observation next is [1.0, 0.6521739130434783, 0.3490304709141275, 0.8333333333333335, 0.215, 0.0, 0.6666666666666666, 0.558219478763965, 0.6091393968537745, 1.0, 1.0, 0.9443785570942942], 
reward next is 0.0556, 
noisyNet noise sample is [array([0.39376202], dtype=float32), -1.2680566]. 
=============================================
[2019-04-04 09:06:56,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0359708e-12 5.6795029e-12 3.3991089e-25 4.5545018e-13 5.2016822e-13
 2.0276910e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:56,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4645
[2019-04-04 09:06:56,356] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 6.999999999999999, 0.0, 26.0, 25.59352812684543, 0.2497789076089486, 1.0, 1.0, 36179.08616501695], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2049600.0000, 
sim time next is 2050200.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.81972358013049, 0.2448726394124953, 1.0, 1.0, 198466.2585087998], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5683102983442074, 0.5816242131374985, 1.0, 1.0, 0.9450774214704752], 
reward next is 0.0549, 
noisyNet noise sample is [array([-0.84220773], dtype=float32), 0.051996555]. 
=============================================
[2019-04-04 09:06:57,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8835703e-10 4.4873458e-10 3.7822899e-23 1.9679954e-11 1.2141315e-11
 5.9288681e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:57,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1018
[2019-04-04 09:06:57,255] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.49888018357656, 0.1600654390824777, 0.0, 1.0, 42362.12942320872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2170800.0000, 
sim time next is 2171400.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.51320408492168, 0.1553648203399516, 0.0, 1.0, 42295.06904517108], 
processed observation next is [1.0, 0.13043478260869565, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5427670070768066, 0.5517882734466505, 0.0, 1.0, 0.20140509069129087], 
reward next is 0.7986, 
noisyNet noise sample is [array([0.737604], dtype=float32), 0.07531575]. 
=============================================
[2019-04-04 09:06:59,517] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5698935e-12 7.0906436e-12 8.6952390e-25 6.1750892e-13 1.9561563e-13
 1.6810982e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:06:59,519] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4023
[2019-04-04 09:06:59,558] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.1, 51.5, 0.0, 0.0, 26.0, 25.34288210029951, 0.4075151213840201, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2310600.0000, 
sim time next is 2311200.0000, 
raw observation next is [-1.2, 52.0, 0.0, 0.0, 26.0, 25.50686074055875, 0.4156808413630275, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.42936288088642666, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6255717283798958, 0.6385602804543425, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38659436], dtype=float32), 0.7078777]. 
=============================================
[2019-04-04 09:07:13,393] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6716174e-12 6.2210914e-12 1.8232966e-24 1.1589737e-12 9.5206971e-14
 4.6961069e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:13,394] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9706
[2019-04-04 09:07:13,485] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 26.0, 25.28085895697983, 0.2401469322942308, 1.0, 1.0, 7477.258160976296], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2294400.0000, 
sim time next is 2295000.0000, 
raw observation next is [-1.15, 48.0, 221.0, 69.0, 26.0, 25.02726676135889, 0.3269678040444552, 1.0, 1.0, 119782.7184146446], 
processed observation next is [1.0, 0.5652173913043478, 0.4307479224376732, 0.48, 0.7366666666666667, 0.07624309392265194, 0.6666666666666666, 0.5856055634465743, 0.6089892680148185, 1.0, 1.0, 0.5703938972125933], 
reward next is 0.4296, 
noisyNet noise sample is [array([-0.11279454], dtype=float32), -0.31311262]. 
=============================================
[2019-04-04 09:07:13,489] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[85.88399 ]
 [86.27577 ]
 [86.824585]
 [87.44758 ]
 [88.07878 ]], R is [[85.63708496]
 [85.74510956]
 [85.88765717]
 [85.93974304]
 [85.99131012]].
[2019-04-04 09:07:13,985] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3181101e-12 7.0796971e-12 2.1234769e-24 4.2757115e-13 2.4671598e-13
 9.1912646e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:13,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9413
[2019-04-04 09:07:14,030] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7833333333333332, 46.0, 187.6666666666667, 66.0, 26.0, 25.27813311453504, 0.4151256246957483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2296200.0000, 
sim time next is 2296800.0000, 
raw observation next is [-0.6, 45.0, 171.0, 64.5, 26.0, 25.7804699135174, 0.4593240235849318, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.44598337950138506, 0.45, 0.57, 0.0712707182320442, 0.6666666666666666, 0.6483724927931167, 0.6531080078616439, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7296807], dtype=float32), -0.98240596]. 
=============================================
[2019-04-04 09:07:16,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7309029e-11 6.9841931e-11 2.3879186e-23 2.5148284e-12 1.8622577e-12
 2.4614901e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:16,124] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0961
[2019-04-04 09:07:16,158] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 54.0, 0.0, 0.0, 26.0, 25.38781642744049, 0.4339933900514041, 0.0, 1.0, 33807.73325232392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2322000.0000, 
sim time next is 2322600.0000, 
raw observation next is [-1.7, 54.33333333333333, 0.0, 0.0, 26.0, 25.44985306641757, 0.4321295562847838, 0.0, 1.0, 18762.95486425245], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5433333333333333, 0.0, 0.0, 0.6666666666666666, 0.6208210888681309, 0.6440431854282612, 0.0, 1.0, 0.08934740411548786], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.14659145], dtype=float32), -1.0329454]. 
=============================================
[2019-04-04 09:07:18,824] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.3053280e-11 3.4624631e-10 3.7987179e-22 6.4070874e-11 1.5524240e-11
 1.1094282e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:18,824] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8765
[2019-04-04 09:07:18,920] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.566666666666667, 26.0, 13.0, 82.33333333333333, 26.0, 24.91197755766269, 0.2071651839425779, 0.0, 1.0, 75844.23410464385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2481600.0000, 
sim time next is 2482200.0000, 
raw observation next is [2.2, 26.5, 0.0, 0.0, 26.0, 24.85893252401149, 0.2064678281656796, 0.0, 1.0, 90358.52771250856], 
processed observation next is [0.0, 0.7391304347826086, 0.5235457063711911, 0.265, 0.0, 0.0, 0.6666666666666666, 0.5715777103342908, 0.5688226093885599, 0.0, 1.0, 0.43027870339289787], 
reward next is 0.5697, 
noisyNet noise sample is [array([0.13363728], dtype=float32), -0.50931937]. 
=============================================
[2019-04-04 09:07:20,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5850013e-10 3.7795456e-10 5.2616966e-23 4.5448135e-11 1.4104279e-11
 1.4327851e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:20,684] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1174
[2019-04-04 09:07:20,707] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 29.0, 0.0, 0.0, 26.0, 25.2399406698794, 0.2843621973500608, 0.0, 1.0, 42259.38276239834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.28946653986403, 0.2866569717933605, 0.0, 1.0, 40744.81278979175], 
processed observation next is [0.0, 0.8695652173913043, 0.44090489381348114, 0.3033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6074555449886692, 0.5955523239311201, 0.0, 1.0, 0.19402291804662739], 
reward next is 0.8060, 
noisyNet noise sample is [array([1.3350449], dtype=float32), -0.04061747]. 
=============================================
[2019-04-04 09:07:23,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9601332e-10 1.4725281e-10 2.3045926e-22 3.4001964e-11 6.1158053e-12
 1.3009205e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:23,706] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2814
[2019-04-04 09:07:23,724] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.566666666666666, 49.66666666666666, 0.0, 0.0, 26.0, 24.28841913613412, 0.08683666556689608, 0.0, 1.0, 43435.15021335985], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2424000.0000, 
sim time next is 2424600.0000, 
raw observation next is [-6.75, 50.5, 0.0, 0.0, 26.0, 24.28417937984926, 0.07833116900614592, 0.0, 1.0, 43461.36811603409], 
processed observation next is [0.0, 0.043478260869565216, 0.275623268698061, 0.505, 0.0, 0.0, 0.6666666666666666, 0.5236816149874383, 0.5261103896687153, 0.0, 1.0, 0.20695889579063853], 
reward next is 0.7930, 
noisyNet noise sample is [array([0.4530856], dtype=float32), 0.17996103]. 
=============================================
[2019-04-04 09:07:29,533] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1259834e-10 1.6110796e-10 1.5994587e-23 3.2831359e-11 2.2536511e-11
 9.4271980e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:29,536] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5062
[2019-04-04 09:07:29,564] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.95, 43.0, 0.0, 0.0, 26.0, 25.08302442326526, 0.2547449836129242, 0.0, 1.0, 43029.45516421563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2410200.0000, 
sim time next is 2410800.0000, 
raw observation next is [-4.133333333333334, 43.33333333333334, 0.0, 0.0, 26.0, 25.03559631373835, 0.2459451347242951, 0.0, 1.0, 43025.26883430732], 
processed observation next is [0.0, 0.9130434782608695, 0.34810710987996313, 0.4333333333333334, 0.0, 0.0, 0.6666666666666666, 0.586299692811529, 0.5819817115747651, 0.0, 1.0, 0.20488223254432056], 
reward next is 0.7951, 
noisyNet noise sample is [array([-1.0173074], dtype=float32), 1.5603734]. 
=============================================
[2019-04-04 09:07:43,330] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.9625976e-10 9.6460528e-10 2.9505791e-21 1.1384090e-10 4.8290968e-11
 6.5877696e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:43,330] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3201
[2019-04-04 09:07:43,344] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 89.66666666666667, 0.0, 0.0, 26.0, 23.43901901650056, -0.01876596742700646, 0.0, 1.0, 44479.33604797773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2693400.0000, 
sim time next is 2694000.0000, 
raw observation next is [-15.0, 88.33333333333334, 0.0, 0.0, 26.0, 23.42010681963442, -0.02414641470089889, 0.0, 1.0, 44413.45477428773], 
processed observation next is [1.0, 0.17391304347826086, 0.04709141274238226, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.4516755683028683, 0.49195119509970037, 0.0, 1.0, 0.21149264178232255], 
reward next is 0.7885, 
noisyNet noise sample is [array([0.7422829], dtype=float32), 0.52460265]. 
=============================================
[2019-04-04 09:07:43,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.019226]
 [72.1908  ]
 [72.40191 ]
 [72.605995]
 [72.81646 ]], R is [[71.95868683]
 [72.02729034]
 [72.09510803]
 [72.16217041]
 [72.22850037]].
[2019-04-04 09:07:43,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3679094e-12 1.2764972e-11 1.5746306e-24 1.8670198e-12 3.8238295e-13
 1.2083079e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:43,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0575
[2019-04-04 09:07:43,963] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 30.0, 183.5, 86.5, 26.0, 25.62846943455152, 0.417903513866839, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2815200.0000, 
sim time next is 2815800.0000, 
raw observation next is [6.166666666666666, 29.0, 161.6666666666667, 57.66666666666666, 26.0, 25.76749750147655, 0.4207619917830225, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6334256694367498, 0.29, 0.5388888888888891, 0.06372007366482503, 0.6666666666666666, 0.6472914584563793, 0.6402539972610075, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21099463], dtype=float32), 1.2980518]. 
=============================================
[2019-04-04 09:07:44,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.9860994e-11 6.3019978e-10 1.0182619e-22 1.8161448e-11 2.5653793e-11
 1.2629100e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:44,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8867
[2019-04-04 09:07:44,660] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.7504073783059, 0.2487083619728957, 0.0, 1.0, 42515.99514875948], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2768400.0000, 
sim time next is 2769000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.72982842196208, 0.252026227481899, 0.0, 1.0, 42375.70839434001], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5608190351635066, 0.5840087424939663, 0.0, 1.0, 0.20178908759209527], 
reward next is 0.7982, 
noisyNet noise sample is [array([1.0046691], dtype=float32), -1.068721]. 
=============================================
[2019-04-04 09:07:44,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.76282]
 [76.71869]
 [76.65861]
 [76.60472]
 [76.59537]], R is [[76.85429382]
 [76.88330078]
 [76.91116333]
 [76.93767548]
 [76.96286774]].
[2019-04-04 09:07:47,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4267688e-11 3.0342812e-10 9.5184020e-23 1.1316432e-11 9.9552597e-12
 1.2916062e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:47,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1585
[2019-04-04 09:07:47,049] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.66364262874959, 0.2486464830684262, 0.0, 1.0, 43363.88917734687], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2766000.0000, 
sim time next is 2766600.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.64120014719525, 0.2461485832070079, 0.0, 1.0, 43140.07923676342], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5534333455996041, 0.5820495277356693, 0.0, 1.0, 0.20542894874649248], 
reward next is 0.7946, 
noisyNet noise sample is [array([0.9953785], dtype=float32), -0.7817428]. 
=============================================
[2019-04-04 09:07:56,033] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6609510e-11 5.3466894e-11 1.3416692e-23 2.8779136e-12 2.6766738e-12
 2.9596080e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:07:56,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2005
[2019-04-04 09:07:56,058] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.8666326841388, 0.240690969325915, 0.0, 1.0, 55762.66483853722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2867400.0000, 
sim time next is 2868000.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.81176688098763, 0.2451438436352428, 0.0, 1.0, 55784.64407784038], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5676472400823025, 0.581714614545081, 0.0, 1.0, 0.2656411622754304], 
reward next is 0.7344, 
noisyNet noise sample is [array([-0.06029485], dtype=float32), 0.3286461]. 
=============================================
[2019-04-04 09:07:56,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[81.08968 ]
 [81.110634]
 [81.12011 ]
 [81.12144 ]
 [81.12344 ]], R is [[80.98571777]
 [80.91033173]
 [80.83583069]
 [80.7621994 ]
 [80.68978882]].
[2019-04-04 09:08:00,837] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6988509e-12 9.1883393e-12 2.2439427e-24 5.1595197e-13 4.8640904e-13
 2.0408444e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:00,838] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8751
[2019-04-04 09:08:00,886] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 25.12373996406784, 0.3967198235440612, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2916600.0000, 
sim time next is 2917200.0000, 
raw observation next is [0.3333333333333334, 92.66666666666667, 0.0, 0.0, 26.0, 25.1869947454226, 0.3700268441267016, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.4718374884579871, 0.9266666666666667, 0.0, 0.0, 0.6666666666666666, 0.5989162287852166, 0.6233422813755672, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0578333], dtype=float32), 0.0067891637]. 
=============================================
[2019-04-04 09:08:14,200] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0699018e-10 1.4577117e-10 4.0969381e-23 1.4668145e-11 1.2620780e-11
 2.6552433e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:14,200] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5373
[2019-04-04 09:08:14,225] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.30812124347784, 0.1228126126485738, 0.0, 1.0, 39015.22109962886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3033600.0000, 
sim time next is 3034200.0000, 
raw observation next is [-5.833333333333333, 76.0, 0.0, 0.0, 26.0, 24.28028956686658, 0.115446393878376, 0.0, 1.0, 39188.49241787539], 
processed observation next is [0.0, 0.08695652173913043, 0.30101569713758086, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5233574639055482, 0.538482131292792, 0.0, 1.0, 0.1866118686565495], 
reward next is 0.8134, 
noisyNet noise sample is [array([0.6233546], dtype=float32), 2.0785959]. 
=============================================
[2019-04-04 09:08:19,375] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5544718e-12 2.2125248e-11 6.0043458e-24 4.4284075e-13 2.5542266e-12
 5.0902085e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:19,375] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0830
[2019-04-04 09:08:19,387] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 95.33333333333334, 0.0, 0.0, 26.0, 25.47356727516603, 0.5882747036919711, 0.0, 1.0, 61749.93797262498], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3198000.0000, 
sim time next is 3198600.0000, 
raw observation next is [1.5, 96.5, 0.0, 0.0, 26.0, 25.45468231894926, 0.5992649334300142, 0.0, 1.0, 56185.37329824246], 
processed observation next is [1.0, 0.0, 0.5041551246537397, 0.965, 0.0, 0.0, 0.6666666666666666, 0.621223526579105, 0.6997549778100046, 0.0, 1.0, 0.26754939665829747], 
reward next is 0.7325, 
noisyNet noise sample is [array([-0.09891988], dtype=float32), -0.66388935]. 
=============================================
[2019-04-04 09:08:26,986] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2366384e-11 6.0958356e-11 5.7901633e-23 4.2277865e-12 7.2816975e-13
 6.3892122e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:26,989] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1589
[2019-04-04 09:08:27,012] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 47.33333333333334, 64.5, 529.8333333333333, 26.0, 26.5730525684216, 0.6803965637273612, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3342000.0000, 
sim time next is 3342600.0000, 
raw observation next is [-2.0, 48.0, 60.0, 501.0, 26.0, 26.65521768856553, 0.6754748042776946, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.40720221606648205, 0.48, 0.2, 0.5535911602209945, 0.6666666666666666, 0.7212681407137941, 0.7251582680925649, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5501648], dtype=float32), -0.36863756]. 
=============================================
[2019-04-04 09:08:36,946] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7383846e-10 3.3758360e-10 5.1955181e-23 4.0065250e-11 1.2392617e-11
 7.0138048e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:36,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6887
[2019-04-04 09:08:36,963] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.66288268345607, 0.2650494905173263, 0.0, 1.0, 40864.27800435283], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3564000.0000, 
sim time next is 3564600.0000, 
raw observation next is [-6.0, 70.0, 0.0, 0.0, 26.0, 24.63270668449049, 0.2559220515661171, 0.0, 1.0, 40872.00424855428], 
processed observation next is [0.0, 0.2608695652173913, 0.296398891966759, 0.7, 0.0, 0.0, 0.6666666666666666, 0.5527255570408741, 0.5853073505220391, 0.0, 1.0, 0.19462859165978227], 
reward next is 0.8054, 
noisyNet noise sample is [array([0.3885173], dtype=float32), -1.0384792]. 
=============================================
[2019-04-04 09:08:37,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3345204e-11 1.4959595e-10 7.0679697e-24 1.8850874e-11 1.0877967e-11
 3.7133555e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:37,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8125
[2019-04-04 09:08:37,570] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.34337364449395, 0.3432903160621018, 0.0, 1.0, 41025.704529839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3724800.0000, 
sim time next is 3725400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.30907238091333, 0.3380392414813653, 0.0, 1.0, 41044.95226634765], 
processed observation next is [1.0, 0.08695652173913043, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.609089365076111, 0.6126797471604551, 0.0, 1.0, 0.19545215364927454], 
reward next is 0.8045, 
noisyNet noise sample is [array([-0.661923], dtype=float32), 0.32018328]. 
=============================================
[2019-04-04 09:08:37,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8134690e-11 6.8358891e-10 3.9145487e-23 4.2229050e-11 3.2596295e-12
 5.6899167e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:37,985] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6173
[2019-04-04 09:08:38,005] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.74658514334127, 0.2482205678714617, 0.0, 1.0, 42285.4112472412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3387600.0000, 
sim time next is 3388200.0000, 
raw observation next is [-4.666666666666667, 69.16666666666667, 0.0, 0.0, 26.0, 24.80934858496982, 0.2319241356713299, 0.0, 1.0, 42730.73512258197], 
processed observation next is [1.0, 0.21739130434782608, 0.3333333333333333, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.5674457154141516, 0.5773080452237767, 0.0, 1.0, 0.20347969105991415], 
reward next is 0.7965, 
noisyNet noise sample is [array([-0.7773077], dtype=float32), -0.02611921]. 
=============================================
[2019-04-04 09:08:38,632] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0236873e-11 1.7387858e-10 1.9452635e-23 8.1980065e-12 6.4074388e-12
 3.5187534e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:38,633] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3347
[2019-04-04 09:08:38,644] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 67.33333333333333, 0.0, 0.0, 26.0, 25.2829990567475, 0.4282691602812966, 0.0, 1.0, 41632.60928495839], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3548400.0000, 
sim time next is 3549000.0000, 
raw observation next is [-2.833333333333333, 69.16666666666667, 0.0, 0.0, 26.0, 25.26345402006411, 0.422645839648508, 0.0, 1.0, 41341.53673530473], 
processed observation next is [0.0, 0.043478260869565216, 0.3841181902123731, 0.6916666666666668, 0.0, 0.0, 0.6666666666666666, 0.6052878350053424, 0.6408819465495027, 0.0, 1.0, 0.19686446064430826], 
reward next is 0.8031, 
noisyNet noise sample is [array([-0.8131838], dtype=float32), 0.8799227]. 
=============================================
[2019-04-04 09:08:38,649] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.73261 ]
 [80.675835]
 [80.64893 ]
 [80.69598 ]
 [80.64263 ]], R is [[80.70777893]
 [80.70245361]
 [80.69424438]
 [80.67835999]
 [80.6421051 ]].
[2019-04-04 09:08:39,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2502027e-13 6.7639872e-13 1.3264721e-25 1.2103244e-13 2.6405012e-14
 6.1028819e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:39,133] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-04 09:08:39,159] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 51.0, 115.1666666666667, 808.8333333333334, 26.0, 26.3512154345212, 0.5623961991325289, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3504000.0000, 
sim time next is 3504600.0000, 
raw observation next is [2.5, 50.5, 115.0, 806.0, 26.0, 26.01872094874965, 0.625899037072052, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5318559556786704, 0.505, 0.38333333333333336, 0.8906077348066298, 0.6666666666666666, 0.6682267457291374, 0.7086330123573507, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5568906], dtype=float32), -0.47060126]. 
=============================================
[2019-04-04 09:08:39,547] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7849045e-11 1.6749439e-11 7.7329431e-24 2.5464630e-12 7.6785317e-13
 4.6368854e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:39,549] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2271
[2019-04-04 09:08:39,559] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 42.33333333333334, 113.6666666666667, 819.8333333333334, 26.0, 25.29096786394497, 0.4540702993860884, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3676800.0000, 
sim time next is 3677400.0000, 
raw observation next is [5.5, 42.5, 113.0, 818.0, 26.0, 25.29794716278786, 0.4564897372560201, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6149584487534627, 0.425, 0.37666666666666665, 0.9038674033149171, 0.6666666666666666, 0.6081622635656551, 0.6521632457520067, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6070902], dtype=float32), -0.46535954]. 
=============================================
[2019-04-04 09:08:46,523] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6623167e-11 3.8678893e-11 5.9606657e-24 2.6426806e-12 1.6577960e-12
 5.8616586e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:46,525] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5434
[2019-04-04 09:08:46,540] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.43372729413926, 0.4669768913819838, 0.0, 1.0, 18763.23054729054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3544200.0000, 
sim time next is 3544800.0000, 
raw observation next is [-2.0, 60.0, 0.0, 0.0, 26.0, 25.42332000466836, 0.4558003444788273, 0.0, 1.0, 28013.63339986143], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6186100003890299, 0.6519334481596091, 0.0, 1.0, 0.13339825428505442], 
reward next is 0.8666, 
noisyNet noise sample is [array([-1.7335709], dtype=float32), -0.9573809]. 
=============================================
[2019-04-04 09:08:48,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9191070e-11 4.5584609e-11 1.0239250e-23 4.5964023e-12 9.3955432e-13
 1.0634641e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:08:48,728] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4139
[2019-04-04 09:08:48,743] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 23.16666666666666, 224.5, 26.0, 25.35893634458277, 0.4077383330533548, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3691200.0000, 
sim time next is 3691800.0000, 
raw observation next is [4.0, 59.0, 15.0, 165.0, 26.0, 25.30659093338998, 0.3902015901878302, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.05, 0.18232044198895028, 0.6666666666666666, 0.6088825777824983, 0.6300671967292767, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2379261], dtype=float32), -0.0984571]. 
=============================================
[2019-04-04 09:09:01,400] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.9910371e-11 1.1341719e-10 1.9353830e-22 3.8316891e-11 4.8645844e-12
 2.6196341e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:09:01,402] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0872
[2019-04-04 09:09:01,420] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.17022359151943, 0.3197100740166268, 0.0, 1.0, 40863.00327190929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [-5.833333333333333, 40.50000000000001, 0.0, 0.0, 26.0, 25.1396656943636, 0.322093349321145, 0.0, 1.0, 40871.39773825724], 
processed observation next is [1.0, 0.08695652173913043, 0.30101569713758086, 0.4050000000000001, 0.0, 0.0, 0.6666666666666666, 0.5949721411969667, 0.6073644497737151, 0.0, 1.0, 0.19462570351551067], 
reward next is 0.8054, 
noisyNet noise sample is [array([0.04997538], dtype=float32), -1.4912546]. 
=============================================
[2019-04-04 09:09:02,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8260285e-12 2.5739173e-11 1.1897912e-24 2.8568085e-12 1.1421982e-13
 6.0083372e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:09:02,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5830
[2019-04-04 09:09:02,534] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 31.66666666666666, 117.3333333333333, 839.1666666666667, 26.0, 26.22480925044406, 0.5598868924694124, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4020000.0000, 
sim time next is 4020600.0000, 
raw observation next is [-4.333333333333333, 30.33333333333333, 116.6666666666667, 837.3333333333334, 26.0, 25.91249707525905, 0.5290429030743188, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.342566943674977, 0.3033333333333333, 0.388888888888889, 0.9252302025782689, 0.6666666666666666, 0.6593747562715876, 0.6763476343581063, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.52529687], dtype=float32), -0.90405357]. 
=============================================
[2019-04-04 09:09:06,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8088122e-13 2.8693686e-12 1.4509814e-26 3.9128125e-14 2.6589936e-14
 1.2905755e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:09:06,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1092
[2019-04-04 09:09:06,198] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 27.5, 114.0, 830.0, 26.0, 26.30503091981505, 0.662308395504141, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4023000.0000, 
sim time next is 4023600.0000, 
raw observation next is [-3.333333333333333, 27.0, 112.3333333333333, 824.0, 26.0, 26.58744556081609, 0.7036133643341403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.37026777469990774, 0.27, 0.37444444444444436, 0.9104972375690608, 0.6666666666666666, 0.7156204634013408, 0.7345377881113802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.2200519], dtype=float32), -1.3079253]. 
=============================================
[2019-04-04 09:09:09,366] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 09:09:09,368] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:09:09,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:09:09,370] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:09:09,371] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:09:09,371] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:09:09,371] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:09:09,376] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run32
[2019-04-04 09:09:09,398] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run32
[2019-04-04 09:09:09,421] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run32
[2019-04-04 09:10:22,350] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20939264], dtype=float32), -0.21974613]
[2019-04-04 09:10:22,350] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.662796325, 87.61199604, 0.0, 0.0, 26.0, 25.15907904993814, 0.4039892165458966, 0.0, 1.0, 37250.78452130505]
[2019-04-04 09:10:22,351] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:10:22,352] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.7307644e-11 3.7983117e-11 1.0751000e-24 1.9056636e-12 2.5307315e-12
 6.5592143e-15 1.0000000e+00], sampled 0.2802794530061069
[2019-04-04 09:12:16,653] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 09:12:31,896] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20939264], dtype=float32), -0.21974613]
[2019-04-04 09:12:31,897] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [8.063422213833332, 69.53203278333334, 246.7206134666667, 405.4553769666667, 26.0, 25.19729750965005, 0.4026102100202902, 0.0, 1.0, 0.0]
[2019-04-04 09:12:31,897] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:12:31,897] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.7673487e-12 2.1511369e-12 2.4409366e-26 2.0825439e-13 6.3725873e-14
 2.5281837e-16 1.0000000e+00], sampled 0.9892895011884844
[2019-04-04 09:12:50,251] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:12:50,476] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20939264], dtype=float32), -0.21974613]
[2019-04-04 09:12:50,476] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [1.272961261333333, 42.48467439333334, 299.5552348166667, 314.5123660666666, 26.0, 25.07479509031092, 0.3675489276190886, 0.0, 1.0, 0.0]
[2019-04-04 09:12:50,476] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:12:50,477] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [7.0883447e-12 7.9915441e-12 1.7823446e-24 1.1288143e-12 3.0631826e-13
 2.6107180e-15 1.0000000e+00], sampled 0.38876053500507735
[2019-04-04 09:12:55,307] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:12:56,345] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 3100000, evaluation results [3100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:12:56,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3870919e-14 2.9556381e-13 1.0617019e-27 5.0265947e-14 5.0772569e-15
 2.6502200e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:12:56,382] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5989
[2019-04-04 09:12:56,425] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 31.33333333333334, 118.6666666666667, 800.3333333333334, 26.0, 26.7121346590459, 0.6381722488613221, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4101000.0000, 
sim time next is 4101600.0000, 
raw observation next is [-0.3333333333333334, 30.66666666666667, 119.8333333333333, 808.1666666666666, 26.0, 26.73894804780739, 0.646005992757353, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4533702677747, 0.3066666666666667, 0.3994444444444443, 0.8930018416206261, 0.6666666666666666, 0.7282456706506159, 0.7153353309191176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25096253], dtype=float32), 2.4436626]. 
=============================================
[2019-04-04 09:13:00,093] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1295972e-13 6.9120664e-13 3.7212295e-27 8.8260386e-14 2.1378485e-15
 1.3912724e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:00,094] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3486
[2019-04-04 09:13:00,127] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 30.66666666666667, 119.8333333333333, 808.1666666666666, 26.0, 26.73894804780739, 0.646005992757353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4101600.0000, 
sim time next is 4102200.0000, 
raw observation next is [0.0, 30.0, 121.0, 816.0, 26.0, 26.77385466836493, 0.4096484775126956, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.3, 0.4033333333333333, 0.901657458563536, 0.6666666666666666, 0.7311545556970774, 0.6365494925042319, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8032756], dtype=float32), -0.27231947]. 
=============================================
[2019-04-04 09:13:04,164] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3452617e-11 1.0367507e-10 1.6080412e-24 7.4382193e-12 1.7126497e-12
 1.4459656e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:04,168] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5142
[2019-04-04 09:13:04,183] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 46.5, 0.0, 0.0, 26.0, 25.3994941135691, 0.3476176788052936, 0.0, 1.0, 36757.02658301219], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4239000.0000, 
sim time next is 4239600.0000, 
raw observation next is [2.666666666666667, 46.0, 0.0, 0.0, 26.0, 25.40966319154165, 0.3474612814894543, 0.0, 1.0, 32081.60865117364], 
processed observation next is [0.0, 0.043478260869565216, 0.5364727608494922, 0.46, 0.0, 0.0, 0.6666666666666666, 0.6174719326284709, 0.6158204271631514, 0.0, 1.0, 0.15276956500558875], 
reward next is 0.8472, 
noisyNet noise sample is [array([0.99980444], dtype=float32), -0.41069254]. 
=============================================
[2019-04-04 09:13:07,448] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7931493e-14 1.4496854e-13 7.3065938e-30 5.8070199e-15 6.6801396e-16
 1.2493605e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:07,454] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7914
[2019-04-04 09:13:07,465] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.56666666666667, 29.66666666666667, 115.5, 843.8333333333334, 26.0, 27.49134794789448, 0.9959441493611122, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4368000.0000, 
sim time next is 4368600.0000, 
raw observation next is [14.55, 30.0, 115.0, 842.0, 26.0, 27.89835232581437, 1.048946488107494, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.865650969529086, 0.3, 0.38333333333333336, 0.9303867403314917, 0.6666666666666666, 0.824862693817864, 0.8496488293691646, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2279904], dtype=float32), -1.541991]. 
=============================================
[2019-04-04 09:13:08,401] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6417215e-10 1.9085829e-10 5.1072456e-24 1.7443588e-11 7.3065070e-12
 7.1583900e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:08,401] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0240
[2019-04-04 09:13:08,435] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 0.0, 0.0, 26.0, 25.33097823201356, 0.3152990720268202, 0.0, 1.0, 39170.06881807988], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4260000.0000, 
sim time next is 4260600.0000, 
raw observation next is [3.0, 49.0, 0.0, 0.0, 26.0, 25.3240356238631, 0.3154149071168832, 0.0, 1.0, 39154.45928068344], 
processed observation next is [0.0, 0.30434782608695654, 0.5457063711911359, 0.49, 0.0, 0.0, 0.6666666666666666, 0.6103363019885917, 0.6051383023722944, 0.0, 1.0, 0.18644980609849257], 
reward next is 0.8136, 
noisyNet noise sample is [array([1.9959275], dtype=float32), -0.066257775]. 
=============================================
[2019-04-04 09:13:09,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2333330e-11 6.0856362e-11 3.2080153e-24 3.4063613e-12 2.4803204e-12
 5.9543981e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:09,457] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3106
[2019-04-04 09:13:09,512] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 47.5, 0.0, 0.0, 26.0, 25.40053017654161, 0.3624491644393828, 0.0, 1.0, 40036.56166284086], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4231800.0000, 
sim time next is 4232400.0000, 
raw observation next is [1.666666666666667, 47.66666666666667, 0.0, 0.0, 26.0, 25.42313982323657, 0.3627926019422748, 0.0, 1.0, 27135.83236932779], 
processed observation next is [0.0, 1.0, 0.5087719298245615, 0.47666666666666674, 0.0, 0.0, 0.6666666666666666, 0.6185949852697142, 0.6209308673140916, 0.0, 1.0, 0.12921824937775137], 
reward next is 0.8708, 
noisyNet noise sample is [array([0.2102016], dtype=float32), -1.1466821]. 
=============================================
[2019-04-04 09:13:14,400] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.0398372e-13 2.0702756e-12 3.4977609e-26 1.6575042e-13 4.5094215e-14
 4.1212332e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:14,404] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9552
[2019-04-04 09:13:14,445] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 38.0, 24.0, 0.0, 26.0, 28.24661199379788, 0.8858466238949919, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4381200.0000, 
sim time next is 4381800.0000, 
raw observation next is [12.9, 39.0, 19.0, 0.0, 26.0, 28.21055797233625, 1.062926917209444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.8199445983379503, 0.39, 0.06333333333333334, 0.0, 0.6666666666666666, 0.8508798310280209, 0.854308972403148, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.82082945], dtype=float32), -0.6886386]. 
=============================================
[2019-04-04 09:13:19,594] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.8957139e-14 8.4572621e-14 5.3008044e-29 5.0497318e-15 1.2489097e-15
 2.2085030e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:19,595] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5487
[2019-04-04 09:13:19,610] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 42.0, 111.0, 728.5, 26.0, 27.232602144565, 0.7609039229163285, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4356000.0000, 
sim time next is 4356600.0000, 
raw observation next is [10.43333333333333, 40.66666666666666, 112.3333333333333, 745.6666666666667, 26.0, 27.33201617691212, 0.7863973937373115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7516158818097877, 0.40666666666666657, 0.37444444444444436, 0.8239410681399633, 0.6666666666666666, 0.7776680147426767, 0.7621324645791039, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.63347954], dtype=float32), 0.1547486]. 
=============================================
[2019-04-04 09:13:22,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.0902078e-12 7.0865772e-11 6.1272104e-24 9.2790749e-12 6.2463061e-13
 2.8080593e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:22,320] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0875
[2019-04-04 09:13:22,356] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3597652e-12 5.2879953e-12 2.9000773e-26 2.9556435e-13 3.2711377e-13
 9.6850880e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:22,356] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1188
[2019-04-04 09:13:22,374] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 60.0, 116.0, 26.0, 25.55903948527298, 0.4947649718951239, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4435200.0000, 
sim time next is 4435800.0000, 
raw observation next is [1.883333333333333, 80.66666666666667, 80.00000000000001, 154.6666666666667, 26.0, 25.49245132988116, 0.499160830564231, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.5147737765466298, 0.8066666666666668, 0.2666666666666667, 0.17090239410681404, 0.6666666666666666, 0.6243709441567633, 0.6663869435214104, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7748488], dtype=float32), 0.565392]. 
=============================================
[2019-04-04 09:13:22,386] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.02129069221632, 0.407355202263519, 1.0, 1.0, 49038.66773905886], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563000.0000, 
sim time next is 4563600.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.9835160789254, 0.4085711170836073, 1.0, 1.0, 56537.67331763697], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5819596732437834, 0.6361903723612025, 1.0, 1.0, 0.2692270157982713], 
reward next is 0.7308, 
noisyNet noise sample is [array([-0.26496682], dtype=float32), 0.45032048]. 
=============================================
[2019-04-04 09:13:29,593] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3367224e-12 2.8508007e-11 7.4614204e-24 1.5800044e-12 3.3020356e-13
 2.5583781e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:29,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1312
[2019-04-04 09:13:29,627] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 55.0, 41.33333333333333, 26.0, 26.02367656776397, 0.530022349507092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4554600.0000, 
sim time next is 4555200.0000, 
raw observation next is [2.0, 52.0, 41.5, 34.66666666666666, 26.0, 26.07325012575709, 0.5075865406684029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.13833333333333334, 0.03830570902394106, 0.6666666666666666, 0.6727708438130909, 0.6691955135561343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05467754], dtype=float32), 0.32851505]. 
=============================================
[2019-04-04 09:13:32,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.4483685e-11 1.5617085e-10 6.8393389e-23 7.9563960e-12 1.1548694e-12
 3.3454464e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:32,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8783
[2019-04-04 09:13:32,801] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 75.5, 0.0, 0.0, 26.0, 25.10962143196898, 0.3429162598277068, 0.0, 1.0, 36256.89028860336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4602600.0000, 
sim time next is 4603200.0000, 
raw observation next is [-2.866666666666667, 76.0, 0.0, 0.0, 26.0, 25.06705548204502, 0.3397441864518473, 0.0, 1.0, 36253.69521175508], 
processed observation next is [1.0, 0.2608695652173913, 0.3831948291782087, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5889212901704184, 0.6132480621506158, 0.0, 1.0, 0.1726366438655004], 
reward next is 0.8274, 
noisyNet noise sample is [array([-1.5665425], dtype=float32), 0.41120467]. 
=============================================
[2019-04-04 09:13:45,616] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3457327e-10 1.7558494e-10 9.2149998e-23 1.0811486e-11 2.0114667e-11
 1.0664566e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:45,618] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2329
[2019-04-04 09:13:45,643] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 69.66666666666667, 0.0, 0.0, 26.0, 25.30788319704585, 0.396023787840085, 0.0, 1.0, 50107.02255247395], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4594800.0000, 
sim time next is 4595400.0000, 
raw observation next is [-1.75, 70.0, 0.0, 0.0, 26.0, 25.26916054921151, 0.3992736704193492, 0.0, 1.0, 40973.00385203881], 
processed observation next is [1.0, 0.17391304347826086, 0.4141274238227147, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6057633791009591, 0.6330912234731164, 0.0, 1.0, 0.19510954215256576], 
reward next is 0.8049, 
noisyNet noise sample is [array([1.2250451], dtype=float32), -0.51320845]. 
=============================================
[2019-04-04 09:13:47,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8838042e-11 6.1051907e-11 1.3317609e-24 1.2723982e-12 1.0080119e-12
 1.6761684e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:47,020] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8980
[2019-04-04 09:13:47,087] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 97.33333333333334, 0.0, 0.0, 26.0, 25.3975707502239, 0.4645554250586275, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4692000.0000, 
sim time next is 4692600.0000, 
raw observation next is [-0.5, 96.0, 0.0, 0.0, 26.0, 25.69198043906281, 0.4748508865845062, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44875346260387816, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6409983699219008, 0.658283628861502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0516698], dtype=float32), -1.4097251]. 
=============================================
[2019-04-04 09:13:49,259] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.82533128e-10 1.23431557e-10 8.84104374e-23 1.47798614e-11
 5.69707909e-12 1.05075616e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 09:13:49,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6883
[2019-04-04 09:13:49,306] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 42.5, 0.0, 0.0, 26.0, 25.01017142553801, 0.3332252937592092, 0.0, 1.0, 31028.96719910811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4819800.0000, 
sim time next is 4820400.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 24.99440725981882, 0.3295306629265488, 0.0, 1.0, 40510.10504205435], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.5828672716515685, 0.6098435543088496, 0.0, 1.0, 0.1929052621050207], 
reward next is 0.8071, 
noisyNet noise sample is [array([0.19305171], dtype=float32), -1.9724057]. 
=============================================
[2019-04-04 09:13:50,332] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.99331261e-12 1.29388895e-11 6.68745552e-24 1.86263418e-12
 1.26454240e-12 1.19410766e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 09:13:50,332] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2924
[2019-04-04 09:13:50,342] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 105.5, 729.5, 26.0, 25.18554610781176, 0.4440344088789673, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4807200.0000, 
sim time next is 4807800.0000, 
raw observation next is [3.0, 37.0, 97.0, 727.0, 26.0, 25.18501721192654, 0.4432620315263363, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.5457063711911359, 0.37, 0.3233333333333333, 0.8033149171270718, 0.6666666666666666, 0.5987514343272116, 0.6477540105087788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3971678], dtype=float32), 0.5241365]. 
=============================================
[2019-04-04 09:13:53,242] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9358454e-10 7.8515208e-11 5.7809822e-23 2.1324324e-11 7.9977500e-12
 1.9929154e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:13:53,242] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5984
[2019-04-04 09:13:53,267] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8333333333333334, 40.5, 0.0, 0.0, 26.0, 25.38988816387315, 0.3417385555975074, 0.0, 1.0, 38545.67553263682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4925400.0000, 
sim time next is 4926000.0000, 
raw observation next is [0.6666666666666667, 41.0, 0.0, 0.0, 26.0, 25.37062772014415, 0.3382371363210934, 0.0, 1.0, 48175.60118729259], 
processed observation next is [1.0, 0.0, 0.4810710987996307, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6142189766786791, 0.6127457121070311, 0.0, 1.0, 0.22940762470139328], 
reward next is 0.7706, 
noisyNet noise sample is [array([0.5060227], dtype=float32), -0.25206476]. 
=============================================
[2019-04-04 09:13:53,291] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.454575]
 [79.30485 ]
 [79.15503 ]
 [79.11964 ]
 [79.14761 ]], R is [[79.45462036]
 [79.47652435]
 [79.53585815]
 [79.55992126]
 [79.62176514]].
[2019-04-04 09:13:55,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:13:55,839] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:13:55,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run24
[2019-04-04 09:13:59,194] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:13:59,195] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:13:59,198] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run24
[2019-04-04 09:13:59,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:13:59,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:13:59,528] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run24
[2019-04-04 09:14:00,291] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.6933062e-13 9.5359495e-13 9.1312437e-27 6.7618301e-14 7.6653687e-15
 5.6101096e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:00,292] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4907
[2019-04-04 09:14:00,323] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 26.0, 111.6666666666667, 832.6666666666667, 26.0, 27.35274114775165, 0.819647650715981, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4975800.0000, 
sim time next is 4976400.0000, 
raw observation next is [8.0, 26.0, 110.3333333333333, 825.8333333333333, 26.0, 27.45616628454879, 0.8384583496162615, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6842105263157896, 0.26, 0.36777777777777765, 0.9125230202578268, 0.6666666666666666, 0.7880138570457325, 0.7794861165387538, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.95962536], dtype=float32), -0.48434386]. 
=============================================
[2019-04-04 09:14:02,502] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:02,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:02,526] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run24
[2019-04-04 09:14:03,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:03,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:03,856] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run24
[2019-04-04 09:14:04,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:04,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:04,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run24
[2019-04-04 09:14:06,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:06,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:06,668] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run24
[2019-04-04 09:14:06,778] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:06,778] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:06,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run24
[2019-04-04 09:14:07,162] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:07,163] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:07,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run24
[2019-04-04 09:14:08,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:08,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:08,446] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run24
[2019-04-04 09:14:08,540] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:08,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:08,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run24
[2019-04-04 09:14:08,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:08,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:08,849] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run24
[2019-04-04 09:14:08,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:08,922] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:08,926] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run24
[2019-04-04 09:14:14,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:14,285] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:14,289] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run24
[2019-04-04 09:14:15,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:15,505] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:15,510] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run24
[2019-04-04 09:14:17,898] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3195154e-11 3.0309595e-11 7.9089893e-24 8.1139056e-13 3.3373048e-12
 2.1232031e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:17,899] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6280
[2019-04-04 09:14:17,963] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.233333333333333, 83.33333333333334, 23.33333333333333, 0.0, 26.0, 24.5220054204006, 0.1773502594089305, 0.0, 1.0, 50689.92145719513], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [6.05, 84.0, 18.0, 0.0, 26.0, 24.50422910401001, 0.1860511567913048, 0.0, 1.0, 58935.20283097521], 
processed observation next is [0.0, 0.6956521739130435, 0.6301939058171746, 0.84, 0.06, 0.0, 0.6666666666666666, 0.5420190920008343, 0.5620170522637683, 0.0, 1.0, 0.28064382300464386], 
reward next is 0.7194, 
noisyNet noise sample is [array([0.88806], dtype=float32), 0.48226532]. 
=============================================
[2019-04-04 09:14:19,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:14:19,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:14:19,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run24
[2019-04-04 09:14:21,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0733922e-11 3.7185068e-11 2.4704954e-23 4.2407813e-12 2.5682128e-12
 1.4241746e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:21,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9879
[2019-04-04 09:14:21,099] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 24.2940583202824, 0.1514382451003804, 0.0, 1.0, 40097.68339413843], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 86400.0000, 
sim time next is 87000.0000, 
raw observation next is [-0.09999999999999999, 94.33333333333334, 0.0, 0.0, 26.0, 24.29096053619881, 0.1470085875488506, 0.0, 1.0, 40125.4163927612], 
processed observation next is [1.0, 0.0, 0.4598337950138504, 0.9433333333333335, 0.0, 0.0, 0.6666666666666666, 0.5242467113499009, 0.5490028625162835, 0.0, 1.0, 0.19107341139410097], 
reward next is 0.8089, 
noisyNet noise sample is [array([0.3307226], dtype=float32), 0.21524428]. 
=============================================
[2019-04-04 09:14:21,183] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[78.114235]
 [78.005646]
 [78.00562 ]
 [78.00385 ]
 [78.00353 ]], R is [[78.22959137]
 [78.25635529]
 [78.28291321]
 [78.30926514]
 [78.33538818]].
[2019-04-04 09:14:33,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1463439e-13 9.3840135e-12 1.9517440e-24 3.2684684e-13 1.5342103e-13
 1.7185567e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:33,100] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9826
[2019-04-04 09:14:33,163] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.683333333333334, 44.83333333333334, 87.0, 715.6666666666666, 26.0, 25.06840703956814, 0.4037761711770327, 1.0, 1.0, 101953.4833047579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 305400.0000, 
sim time next is 306000.0000, 
raw observation next is [-9.5, 44.0, 89.0, 694.5, 26.0, 25.47955481038625, 0.4482820464085409, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1994459833795014, 0.44, 0.2966666666666667, 0.7674033149171271, 0.6666666666666666, 0.6232962341988543, 0.6494273488028469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4216776], dtype=float32), -1.0774751]. 
=============================================
[2019-04-04 09:14:33,165] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.72221 ]
 [82.77478 ]
 [82.50719 ]
 [82.329834]
 [82.71159 ]], R is [[82.45679474]
 [82.14672852]
 [81.36997223]
 [80.87649536]
 [81.06773376]].
[2019-04-04 09:14:35,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0597405e-10 6.5020944e-10 9.4898740e-22 3.9168582e-11 2.4901387e-11
 1.2305855e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:35,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3011
[2019-04-04 09:14:36,022] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.08932940744918, -0.1513679740907117, 0.0, 1.0, 44266.14068746554], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 187200.0000, 
sim time next is 187800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 23.08898998626671, -0.1589418719373603, 0.0, 1.0, 44279.72124114567], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.4240824988555592, 0.4470193760208799, 0.0, 1.0, 0.210855815434027], 
reward next is 0.7891, 
noisyNet noise sample is [array([0.54883933], dtype=float32), -0.96957374]. 
=============================================
[2019-04-04 09:14:43,681] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.9954237e-10 6.9550438e-10 3.8945343e-21 8.1822951e-11 4.8096277e-11
 8.0475698e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:43,682] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3943
[2019-04-04 09:14:43,704] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.3, 67.0, 0.0, 0.0, 26.0, 22.52900624515986, -0.2781489002776362, 0.0, 1.0, 47957.80718720323], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 284400.0000, 
sim time next is 285000.0000, 
raw observation next is [-12.38333333333333, 67.5, 0.0, 0.0, 26.0, 22.4967782019845, -0.2951474945723209, 0.0, 1.0, 48023.04654014605], 
processed observation next is [1.0, 0.30434782608695654, 0.1195752539242845, 0.675, 0.0, 0.0, 0.6666666666666666, 0.37473151683204176, 0.40161750180922634, 0.0, 1.0, 0.2286811740006955], 
reward next is 0.7713, 
noisyNet noise sample is [array([-1.4864393], dtype=float32), -0.53181064]. 
=============================================
[2019-04-04 09:14:43,712] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[69.48915]
 [69.56554]
 [69.66463]
 [69.78469]
 [69.91175]], R is [[69.49788666]
 [69.57453918]
 [69.65058899]
 [69.72607422]
 [69.80095673]].
[2019-04-04 09:14:47,666] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1576920e-12 9.6244540e-11 1.0669950e-22 4.8169793e-12 1.2750269e-12
 2.0111720e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:47,668] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0266
[2019-04-04 09:14:47,711] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.98263725e-10 1.52613189e-09 1.18690595e-20 5.24683630e-11
 5.71147643e-11 3.73651874e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 09:14:47,711] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6656
[2019-04-04 09:14:47,749] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.1, 79.5, 0.0, 0.0, 26.0, 24.43409989114882, 0.1530192062437922, 0.0, 1.0, 47550.24547310801], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [-13.2, 80.33333333333333, 0.0, 0.0, 26.0, 24.34969245520572, 0.1392303310622896, 0.0, 1.0, 47560.43384245523], 
processed observation next is [1.0, 0.9130434782608695, 0.09695290858725764, 0.8033333333333332, 0.0, 0.0, 0.6666666666666666, 0.52914103793381, 0.5464101103540965, 0.0, 1.0, 0.22647825639264396], 
reward next is 0.7735, 
noisyNet noise sample is [array([1.398996], dtype=float32), -1.4751107]. 
=============================================
[2019-04-04 09:14:47,769] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.5, 46.0, 51.5, 859.5, 26.0, 26.35091430048618, 0.5154388282080333, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 396000.0000, 
sim time next is 396600.0000, 
raw observation next is [-10.33333333333333, 45.0, 50.33333333333333, 850.3333333333334, 26.0, 26.38819600286435, 0.4128792571188107, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.17636195752539252, 0.45, 0.16777777777777778, 0.9395948434622469, 0.6666666666666666, 0.6990163335720293, 0.6376264190396036, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11555119], dtype=float32), -1.7927095]. 
=============================================
[2019-04-04 09:14:52,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1219783e-13 3.0580486e-11 2.3023860e-23 6.9309180e-13 3.4185832e-13
 5.3498406e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:52,495] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1696
[2019-04-04 09:14:52,565] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.866666666666667, 45.66666666666667, 85.0, 736.8333333333334, 26.0, 24.60864735638268, 0.3296979623693908, 1.0, 1.0, 200611.3222299295], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 304800.0000, 
sim time next is 305400.0000, 
raw observation next is [-9.683333333333334, 44.83333333333334, 87.0, 715.6666666666666, 26.0, 25.06840703956814, 0.4037761711770327, 1.0, 1.0, 101953.4833047579], 
processed observation next is [1.0, 0.5217391304347826, 0.19436749769159742, 0.4483333333333334, 0.29, 0.7907918968692449, 0.6666666666666666, 0.5890339199640117, 0.6345920570590109, 1.0, 1.0, 0.4854927776417043], 
reward next is 0.5145, 
noisyNet noise sample is [array([0.1197501], dtype=float32), 0.6267118]. 
=============================================
[2019-04-04 09:14:59,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.4796918e-12 1.7651665e-11 3.7328632e-25 1.2897415e-12 2.5374922e-13
 1.1158573e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:59,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0460
[2019-04-04 09:14:59,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2941401e-12 1.0555038e-11 3.5419661e-24 9.5669024e-13 2.8599529e-13
 6.7814326e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:14:59,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3967
[2019-04-04 09:15:00,013] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.1, 41.5, 0.0, 0.0, 26.0, 22.87396089058539, -0.1718418274959656, 1.0, 1.0, 203487.4415189996], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 459000.0000, 
sim time next is 459600.0000, 
raw observation next is [-8.0, 41.0, 0.0, 0.0, 26.0, 23.46338043244446, -0.05871066988511914, 1.0, 1.0, 159859.1910821923], 
processed observation next is [1.0, 0.30434782608695654, 0.24099722991689754, 0.41, 0.0, 0.0, 0.6666666666666666, 0.455281702703705, 0.4804297767049603, 1.0, 1.0, 0.7612342432485347], 
reward next is 0.2388, 
noisyNet noise sample is [array([0.95920825], dtype=float32), -0.7169502]. 
=============================================
[2019-04-04 09:15:00,016] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 33.83333333333334, 110.6666666666667, 0.0, 26.0, 25.51262260078344, 0.2560962266191513, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 481800.0000, 
sim time next is 482400.0000, 
raw observation next is [-0.6, 35.0, 106.5, 0.0, 26.0, 25.5754128531393, 0.2513228844463852, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.44598337950138506, 0.35, 0.355, 0.0, 0.6666666666666666, 0.6312844044282752, 0.5837742948154617, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.65652853], dtype=float32), -0.28169876]. 
=============================================
[2019-04-04 09:15:00,183] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.2838311e-12 2.5862940e-11 3.2439102e-25 3.0227180e-13 8.0398063e-13
 7.9951413e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:00,183] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8325
[2019-04-04 09:15:00,211] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.533333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.99946733032607, 0.2527773886542353, 0.0, 1.0, 39560.50337170452], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 524400.0000, 
sim time next is 525000.0000, 
raw observation next is [4.416666666666667, 88.16666666666667, 0.0, 0.0, 26.0, 24.96214719582699, 0.247756246464346, 0.0, 1.0, 39596.23827926137], 
processed observation next is [0.0, 0.043478260869565216, 0.584949215143121, 0.8816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5801789329855825, 0.5825854154881153, 0.0, 1.0, 0.18855351561553035], 
reward next is 0.8114, 
noisyNet noise sample is [array([0.6380779], dtype=float32), -0.24593978]. 
=============================================
[2019-04-04 09:15:00,218] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.70848 ]
 [85.74445 ]
 [85.776764]
 [85.82482 ]
 [85.71861 ]], R is [[85.47447968]
 [85.43135071]
 [85.38884735]
 [85.34703827]
 [85.30471802]].
[2019-04-04 09:15:02,255] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2470658e-12 9.3552414e-12 7.2045412e-24 3.3422592e-13 4.5525308e-13
 1.9967345e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:02,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4287
[2019-04-04 09:15:02,312] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.61666666666667, 51.0, 58.0, 858.0, 26.0, 25.99706710375354, 0.4182283478674062, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 389400.0000, 
sim time next is 390000.0000, 
raw observation next is [-12.43333333333333, 51.00000000000001, 58.0, 881.5, 26.0, 25.71912357033881, 0.3561643953946385, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.11819021237303795, 0.5100000000000001, 0.19333333333333333, 0.9740331491712707, 0.6666666666666666, 0.6432602975282341, 0.6187214651315461, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.23918414], dtype=float32), 0.77652407]. 
=============================================
[2019-04-04 09:15:02,343] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.621254]
 [83.75997 ]
 [83.89941 ]
 [83.15858 ]
 [82.371925]], R is [[83.68753815]
 [83.85066223]
 [84.01215363]
 [83.47806549]
 [82.69080353]].
[2019-04-04 09:15:08,982] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4954626e-12 1.7773491e-11 3.2537751e-25 1.4057622e-12 1.1168406e-12
 1.6250622e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:08,984] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9906
[2019-04-04 09:15:09,039] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 88.33333333333334, 132.8333333333333, 109.3333333333333, 26.0, 24.90647955130909, 0.2547004662284126, 0.0, 1.0, 18735.04534364745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 553200.0000, 
sim time next is 553800.0000, 
raw observation next is [-0.5, 87.66666666666666, 121.6666666666667, 115.6666666666667, 26.0, 24.83107855167048, 0.2521732707414668, 0.0, 1.0, 63808.80679601403], 
processed observation next is [0.0, 0.391304347826087, 0.44875346260387816, 0.8766666666666666, 0.40555555555555567, 0.12780847145488033, 0.6666666666666666, 0.5692565459725399, 0.5840577569138222, 0.0, 1.0, 0.30385146093340015], 
reward next is 0.6961, 
noisyNet noise sample is [array([1.1975603], dtype=float32), 0.7160242]. 
=============================================
[2019-04-04 09:15:15,088] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0495780e-10 3.1547483e-11 8.1153151e-24 3.3091303e-12 5.4284615e-12
 1.0329884e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:15,089] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6695
[2019-04-04 09:15:15,102] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 69.16666666666667, 0.0, 0.0, 26.0, 23.83102855405017, 0.01208945802038918, 0.0, 1.0, 44441.55344394018], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 622200.0000, 
sim time next is 622800.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 23.80190521137781, 0.006288519104980318, 0.0, 1.0, 44395.75759693926], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.68, 0.0, 0.0, 0.6666666666666666, 0.4834921009481509, 0.5020961730349934, 0.0, 1.0, 0.21140836950923456], 
reward next is 0.7886, 
noisyNet noise sample is [array([1.2237698], dtype=float32), 0.22145422]. 
=============================================
[2019-04-04 09:15:15,403] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1824956e-13 6.8704656e-13 2.7053254e-26 4.1055853e-14 1.5712964e-14
 4.3160650e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:15,403] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3657
[2019-04-04 09:15:15,421] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84235977200898, 0.3889771677855018, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 734400.0000, 
sim time next is 735000.0000, 
raw observation next is [-0.4166666666666667, 55.83333333333333, 115.3333333333333, 559.0, 26.0, 25.84071161178181, 0.3824118822356947, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.45106186518928904, 0.5583333333333332, 0.3844444444444443, 0.6176795580110497, 0.6666666666666666, 0.6533926343151508, 0.6274706274118983, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.01157076], dtype=float32), -0.2734659]. 
=============================================
[2019-04-04 09:15:15,432] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[90.08184 ]
 [91.23894 ]
 [92.623924]
 [93.64469 ]
 [93.44493 ]], R is [[89.36304474]
 [89.46941376]
 [89.57472229]
 [89.67897797]
 [89.78218842]].
[2019-04-04 09:15:17,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6549961e-11 5.7086665e-11 1.5565000e-23 6.2529730e-12 4.4548159e-12
 2.9898837e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:17,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8171
[2019-04-04 09:15:17,390] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 65.0, 0.0, 0.0, 26.0, 23.72189753451286, -0.01661821261597634, 0.0, 1.0, 43895.12979217836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 626400.0000, 
sim time next is 627000.0000, 
raw observation next is [-4.5, 65.5, 0.0, 0.0, 26.0, 23.70190448037962, -0.02337769539826946, 0.0, 1.0, 43824.39304675314], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.655, 0.0, 0.0, 0.6666666666666666, 0.4751587066983018, 0.49220743486724355, 0.0, 1.0, 0.20868758593691972], 
reward next is 0.7913, 
noisyNet noise sample is [array([0.11948843], dtype=float32), -2.0222433]. 
=============================================
[2019-04-04 09:15:17,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.13523 ]
 [78.14883 ]
 [78.16714 ]
 [78.194145]
 [78.22881 ]], R is [[78.13188934]
 [78.14154816]
 [78.15067291]
 [78.15932465]
 [78.16751099]].
[2019-04-04 09:15:19,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.5226893e-11 4.0133397e-11 1.4907572e-23 3.8508610e-12 1.1132951e-12
 1.8319720e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:19,731] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1984
[2019-04-04 09:15:19,828] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.933333333333334, 62.33333333333333, 92.83333333333334, 48.33333333333333, 26.0, 24.84607951113493, 0.2131142364524765, 0.0, 1.0, 52108.13162204412], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 646800.0000, 
sim time next is 647400.0000, 
raw observation next is [-2.816666666666667, 61.66666666666666, 96.66666666666667, 58.66666666666666, 26.0, 24.8547762071985, 0.2199825296758894, 0.0, 1.0, 42597.95612102791], 
processed observation next is [0.0, 0.4782608695652174, 0.38457987072945526, 0.6166666666666666, 0.32222222222222224, 0.06482504604051564, 0.6666666666666666, 0.5712313505998751, 0.5733275098919631, 0.0, 1.0, 0.2028474101001329], 
reward next is 0.7972, 
noisyNet noise sample is [array([0.48820573], dtype=float32), 0.50733566]. 
=============================================
[2019-04-04 09:15:25,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.96460987e-12 1.61142297e-11 2.23669558e-24 3.47989866e-12
 1.24127184e-12 1.32916656e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 09:15:25,762] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5046
[2019-04-04 09:15:25,779] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.3129330148201, 0.05782150104129884, 0.0, 1.0, 41267.06948638991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 700800.0000, 
sim time next is 701400.0000, 
raw observation next is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.27338460775152, 0.05418290486540006, 0.0, 1.0, 41347.44289499913], 
processed observation next is [1.0, 0.08695652173913043, 0.368421052631579, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5227820506459601, 0.5180609682884667, 0.0, 1.0, 0.19689258521428155], 
reward next is 0.8031, 
noisyNet noise sample is [array([0.15923956], dtype=float32), -0.5885843]. 
=============================================
[2019-04-04 09:15:34,819] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.7625417e-13 3.5371001e-12 3.0598006e-25 4.5243713e-13 1.4279126e-13
 2.0414284e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:34,819] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6753
[2019-04-04 09:15:34,858] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 72.33333333333334, 102.6666666666667, 0.0, 26.0, 25.61386364740836, 0.3119579178440714, 1.0, 1.0, 27607.86632318267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 821400.0000, 
sim time next is 822000.0000, 
raw observation next is [-4.5, 73.66666666666667, 100.8333333333333, 0.0, 26.0, 25.58438049535899, 0.3029497696853317, 1.0, 1.0, 27700.45550589011], 
processed observation next is [1.0, 0.5217391304347826, 0.3379501385041552, 0.7366666666666667, 0.336111111111111, 0.0, 0.6666666666666666, 0.6320317079465824, 0.6009832565617773, 1.0, 1.0, 0.1319069309804291], 
reward next is 0.8681, 
noisyNet noise sample is [array([0.8074759], dtype=float32), 0.8362126]. 
=============================================
[2019-04-04 09:15:34,884] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[87.455345]
 [87.60729 ]
 [87.76835 ]
 [87.93472 ]
 [88.04206 ]], R is [[87.34223175]
 [87.33734131]
 [87.33297729]
 [87.32939911]
 [87.32752991]].
[2019-04-04 09:15:36,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2347933e-13 1.0025817e-12 7.1443792e-26 2.0345840e-14 8.3409867e-15
 8.0069488e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:36,815] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2500
[2019-04-04 09:15:36,821] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 60.0, 91.83333333333333, 724.0, 26.0, 25.85742940678581, 0.391733178831867, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 733200.0000, 
sim time next is 733800.0000, 
raw observation next is [-0.6, 58.5, 99.66666666666666, 669.0, 26.0, 25.86301252543569, 0.3895616733027364, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.585, 0.3322222222222222, 0.7392265193370166, 0.6666666666666666, 0.6552510437863074, 0.6298538911009121, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.196977], dtype=float32), 0.45284072]. 
=============================================
[2019-04-04 09:15:38,484] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7881339e-12 5.2689372e-12 3.0052907e-26 5.7688072e-13 8.7708100e-14
 1.4359182e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:38,499] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9505
[2019-04-04 09:15:38,552] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.99289755714832, 0.2482116820843802, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 891000.0000, 
sim time next is 891600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.13529510005398, 0.2494024886136979, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5946079250044983, 0.5831341628712327, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.08348162], dtype=float32), -1.2176924]. 
=============================================
[2019-04-04 09:15:41,783] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1869531e-13 4.7045121e-13 5.6936028e-27 2.2252941e-14 2.4007074e-14
 1.6930217e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:41,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1970
[2019-04-04 09:15:41,792] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.63333333333333, 52.0, 0.0, 0.0, 26.0, 25.9110791344075, 0.7509257730530479, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1100400.0000, 
sim time next is 1101000.0000, 
raw observation next is [16.36666666666667, 52.5, 0.0, 0.0, 26.0, 26.20048767338358, 0.7719070387205882, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.9159741458910436, 0.525, 0.0, 0.0, 0.6666666666666666, 0.6833739727819651, 0.7573023462401961, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9325112], dtype=float32), 1.176982]. 
=============================================
[2019-04-04 09:15:41,805] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[97.05546]
 [97.44627]
 [97.6099 ]
 [97.76999]
 [98.08294]], R is [[96.93310547]
 [96.96377563]
 [96.99414062]
 [97.02420044]
 [97.05396271]].
[2019-04-04 09:15:43,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1456139e-11 3.3183456e-12 5.1303991e-26 6.0319973e-13 3.6701726e-13
 4.1405210e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:43,847] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3896
[2019-04-04 09:15:43,871] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.6, 78.0, 0.0, 0.0, 26.0, 25.66381537278685, 0.6221985357629286, 0.0, 1.0, 18725.29157426547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1141800.0000, 
sim time next is 1142400.0000, 
raw observation next is [11.6, 79.0, 0.0, 0.0, 26.0, 25.69791669807339, 0.6185969198922388, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.21739130434782608, 0.7839335180055402, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6414930581727823, 0.706198973297413, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.11417142], dtype=float32), -0.5152259]. 
=============================================
[2019-04-04 09:15:48,368] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1286876e-13 1.5764424e-13 1.3047747e-28 5.6160461e-15 4.9537049e-15
 4.6088014e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:48,370] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0778
[2019-04-04 09:15:48,378] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 0.0, 0.0, 26.0, 25.77511016562056, 0.5307559688764998, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1017000.0000, 
sim time next is 1017600.0000, 
raw observation next is [14.4, 81.0, 0.0, 0.0, 26.0, 25.65853643939625, 0.5225276721938997, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8614958448753465, 0.81, 0.0, 0.0, 0.6666666666666666, 0.6382113699496875, 0.6741758907312999, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.253816], dtype=float32), -0.7613081]. 
=============================================
[2019-04-04 09:15:51,139] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.1165817e-14 2.5317343e-13 2.2931252e-27 6.0094603e-15 2.4281805e-15
 9.5216587e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:51,144] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4249
[2019-04-04 09:15:51,154] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 26.0, 27.40726132940654, 0.9538065979128548, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1088400.0000, 
sim time next is 1089000.0000, 
raw observation next is [19.1, 51.5, 140.0, 0.0, 26.0, 27.61107967035245, 0.9769939635811035, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9916897506925209, 0.515, 0.4666666666666667, 0.0, 0.6666666666666666, 0.8009233058627041, 0.8256646545270345, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1761512], dtype=float32), 0.46863937]. 
=============================================
[2019-04-04 09:15:51,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[96.1405 ]
 [95.52016]
 [94.85322]
 [94.18508]
 [93.60583]], R is [[96.85392761]
 [96.88539124]
 [96.91654205]
 [96.94738007]
 [96.97790527]].
[2019-04-04 09:15:59,215] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.5015728e-14 3.2929787e-13 3.5002818e-27 3.1989765e-14 6.1724688e-15
 1.5026029e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:15:59,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6975
[2019-04-04 09:15:59,246] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 64.0, 0.0, 26.0, 26.09554245711539, 0.5845045098020911, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1331400.0000, 
sim time next is 1332000.0000, 
raw observation next is [0.5, 92.0, 73.5, 0.0, 26.0, 26.06933093980359, 0.5858146544161991, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4764542936288089, 0.92, 0.245, 0.0, 0.6666666666666666, 0.6724442449836324, 0.6952715514720663, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6283367], dtype=float32), -0.5227864]. 
=============================================
[2019-04-04 09:15:59,256] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[92.86332]
 [92.83449]
 [92.8849 ]
 [93.01045]
 [93.15553]], R is [[92.94213104]
 [93.01271057]
 [93.08258057]
 [93.15175629]
 [93.22023773]].
[2019-04-04 09:16:01,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1355410e-12 9.0124765e-12 4.3170167e-26 2.1262934e-13 9.6660397e-13
 1.8680312e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:01,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4544
[2019-04-04 09:16:01,306] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.25, 78.0, 0.0, 0.0, 26.0, 25.65906534410261, 0.6394094914384292, 0.0, 1.0, 20757.2012979367], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1128600.0000, 
sim time next is 1129200.0000, 
raw observation next is [10.16666666666667, 78.33333333333333, 0.0, 0.0, 26.0, 25.66444083731969, 0.6367836195615608, 0.0, 1.0, 19026.53144336948], 
processed observation next is [0.0, 0.043478260869565216, 0.7442289935364729, 0.7833333333333333, 0.0, 0.0, 0.6666666666666666, 0.6387034031099743, 0.7122612065205202, 0.0, 1.0, 0.0906025306827118], 
reward next is 0.9094, 
noisyNet noise sample is [array([0.15991686], dtype=float32), -0.2523793]. 
=============================================
[2019-04-04 09:16:10,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0994289e-13 2.2814246e-12 2.4514250e-26 1.9368913e-13 5.5212559e-14
 8.7309053e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:10,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2872
[2019-04-04 09:16:10,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5653568e-13 2.0114119e-12 7.4556082e-26 8.4210457e-14 2.5857227e-14
 3.4147720e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:10,197] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 92.0, 12.0, 0.0, 26.0, 25.62581039839307, 0.5300071317524419, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1325400.0000, 
sim time next is 1326000.0000, 
raw observation next is [0.9000000000000001, 92.0, 15.0, 0.0, 26.0, 25.54458331309984, 0.5550285712347339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.48753462603878117, 0.92, 0.05, 0.0, 0.6666666666666666, 0.6287152760916532, 0.6850095237449113, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.46124342], dtype=float32), -1.2546884]. 
=============================================
[2019-04-04 09:16:10,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0673
[2019-04-04 09:16:10,210] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.40221]
 [91.62423]
 [91.90243]
 [91.82572]
 [91.09059]], R is [[91.2529068 ]
 [91.34037781]
 [91.42697144]
 [91.51270294]
 [91.59757996]].
[2019-04-04 09:16:10,214] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 67.66666666666667, 0.0, 26.0, 25.99102095195037, 0.50674508010658, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1419600.0000, 
sim time next is 1420200.0000, 
raw observation next is [0.0, 95.0, 72.0, 0.0, 26.0, 25.9434746670807, 0.5004600256327009, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.24, 0.0, 0.6666666666666666, 0.661956222256725, 0.6668200085442336, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.113541], dtype=float32), -0.2768357]. 
=============================================
[2019-04-04 09:16:15,964] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4313199e-12 9.6053764e-12 2.4631625e-25 2.4437291e-13 7.0142232e-13
 2.5701934e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:15,968] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7791
[2019-04-04 09:16:15,982] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.9, 84.0, 0.0, 0.0, 26.0, 25.82975373054571, 0.6095682961377173, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1636200.0000, 
sim time next is 1636800.0000, 
raw observation next is [7.0, 83.33333333333334, 0.0, 0.0, 26.0, 25.71671463364167, 0.5972397830102607, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.6565096952908588, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6430595528034724, 0.6990799276700869, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5738845], dtype=float32), -1.2269834]. 
=============================================
[2019-04-04 09:16:21,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5721848e-11 6.8123007e-11 1.2785209e-24 5.3926698e-13 3.0321780e-12
 5.8723790e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:21,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1493
[2019-04-04 09:16:22,018] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.03333333333333333, 91.0, 0.0, 0.0, 26.0, 25.25733166671147, 0.4285050897529799, 0.0, 1.0, 42979.93972489836], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1738200.0000, 
sim time next is 1738800.0000, 
raw observation next is [0.0, 91.0, 0.0, 0.0, 26.0, 25.23739481876392, 0.4242200163001235, 0.0, 1.0, 43011.64622837646], 
processed observation next is [0.0, 0.13043478260869565, 0.46260387811634357, 0.91, 0.0, 0.0, 0.6666666666666666, 0.6031162348969934, 0.6414066721000412, 0.0, 1.0, 0.20481736299226888], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.53699136], dtype=float32), -1.5059769]. 
=============================================
[2019-04-04 09:16:22,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1208000e-12 2.7959385e-11 5.1499315e-25 6.0212154e-13 1.5141289e-12
 2.2525808e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:22,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0937
[2019-04-04 09:16:22,066] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 92.0, 0.0, 0.0, 26.0, 25.58664347650115, 0.5376818275066634, 0.0, 1.0, 28060.19429868904], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1666800.0000, 
sim time next is 1667400.0000, 
raw observation next is [4.716666666666667, 92.0, 0.0, 0.0, 26.0, 25.58343938406546, 0.534965568940391, 0.0, 1.0, 30748.8560770091], 
processed observation next is [1.0, 0.30434782608695654, 0.5932594644506002, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6319532820054551, 0.6783218563134636, 0.0, 1.0, 0.1464231241762338], 
reward next is 0.8536, 
noisyNet noise sample is [array([-0.92045647], dtype=float32), 0.18997696]. 
=============================================
[2019-04-04 09:16:32,077] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3581979e-10 6.7841538e-10 5.1409300e-22 3.8574789e-11 7.9953405e-12
 1.9111190e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:32,077] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5240
[2019-04-04 09:16:32,106] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.98629822425945, 0.2942398070869985, 0.0, 1.0, 45947.97256395272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1805400.0000, 
sim time next is 1806000.0000, 
raw observation next is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.99759629048355, 0.2927164744324882, 0.0, 1.0, 45901.50578423889], 
processed observation next is [0.0, 0.9130434782608695, 0.32409972299168976, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5831330242069624, 0.5975721581441628, 0.0, 1.0, 0.21857859897256615], 
reward next is 0.7814, 
noisyNet noise sample is [array([-1.4479009], dtype=float32), -1.4740921]. 
=============================================
[2019-04-04 09:16:32,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[75.97126 ]
 [76.017334]
 [76.06413 ]
 [76.101685]
 [76.15228 ]], R is [[75.9667511 ]
 [75.98828125]
 [76.00951385]
 [76.03044891]
 [76.05097961]].
[2019-04-04 09:16:41,047] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.62761304e-10 7.57333501e-11 2.62480882e-22 1.06789681e-11
 9.03368005e-12 1.02836325e-13 1.00000000e+00], sum to 1.0000
[2019-04-04 09:16:41,047] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4445
[2019-04-04 09:16:41,064] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.75, 77.0, 0.0, 0.0, 26.0, 24.53235242062739, 0.1326262750747133, 0.0, 1.0, 44883.7625001777], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1895400.0000, 
sim time next is 1896000.0000, 
raw observation next is [-6.933333333333334, 77.66666666666667, 0.0, 0.0, 26.0, 24.49183735239777, 0.1241383748052975, 0.0, 1.0, 44897.38838181293], 
processed observation next is [0.0, 0.9565217391304348, 0.270544783010157, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5409864460331475, 0.5413794582684325, 0.0, 1.0, 0.21379708753244253], 
reward next is 0.7862, 
noisyNet noise sample is [array([-0.7587674], dtype=float32), -0.36483872]. 
=============================================
[2019-04-04 09:16:41,104] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[75.64432 ]
 [75.78572 ]
 [75.91758 ]
 [76.06311 ]
 [76.215866]], R is [[75.60133362]
 [75.6315918 ]
 [75.66162872]
 [75.69148254]
 [75.72116852]].
[2019-04-04 09:16:48,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.8514868e-12 2.3296644e-11 1.5994100e-23 3.1714343e-12 1.4075168e-12
 1.8367654e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:48,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7263
[2019-04-04 09:16:48,198] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.15917574545531, 0.05870301665495665, 0.0, 1.0, 41111.63103439956], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2008200.0000, 
sim time next is 2008800.0000, 
raw observation next is [-6.2, 87.0, 0.0, 0.0, 26.0, 24.13805997751053, 0.05771806164269403, 0.0, 1.0, 41132.93142092844], 
processed observation next is [1.0, 0.2608695652173913, 0.2908587257617729, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5115049981258775, 0.519239353880898, 0.0, 1.0, 0.19587110200442115], 
reward next is 0.8041, 
noisyNet noise sample is [array([-1.4122878], dtype=float32), 0.31955034]. 
=============================================
[2019-04-04 09:16:58,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.3975674e-11 1.2623194e-10 2.7806741e-23 3.0246803e-12 2.5199181e-12
 9.4106343e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:16:58,219] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6634
[2019-04-04 09:16:58,248] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.45, 85.0, 0.0, 0.0, 26.0, 24.2189630706375, 0.09926074059078693, 0.0, 1.0, 43340.42548557898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2093400.0000, 
sim time next is 2094000.0000, 
raw observation next is [-6.533333333333333, 84.33333333333333, 0.0, 0.0, 26.0, 24.18206743030889, 0.08848944376720148, 0.0, 1.0, 43424.79155368249], 
processed observation next is [1.0, 0.21739130434782608, 0.2816251154201293, 0.8433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5151722858590743, 0.5294964812557338, 0.0, 1.0, 0.2067847216842023], 
reward next is 0.7932, 
noisyNet noise sample is [array([0.7846297], dtype=float32), -0.06438192]. 
=============================================
[2019-04-04 09:16:58,272] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[78.46255 ]
 [78.53749 ]
 [78.624886]
 [78.749825]
 [78.87708 ]], R is [[78.38457489]
 [78.39434814]
 [78.4029541 ]
 [78.4102478 ]
 [78.41912079]].
[2019-04-04 09:17:08,206] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.6093329e-12 3.5808103e-11 3.5586245e-24 6.1334417e-13 6.9246811e-13
 5.3854689e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:08,210] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3720
[2019-04-04 09:17:08,262] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.933333333333334, 70.83333333333334, 0.0, 0.0, 26.0, 25.0021967894088, 0.3785890143777702, 0.0, 1.0, 84415.33399310205], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2231400.0000, 
sim time next is 2232000.0000, 
raw observation next is [-5.0, 71.0, 0.0, 0.0, 26.0, 25.05332424933608, 0.3887733325942085, 1.0, 1.0, 33677.15949494702], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5877770207780065, 0.6295911108647362, 1.0, 1.0, 0.1603674261664144], 
reward next is 0.8396, 
noisyNet noise sample is [array([0.78046364], dtype=float32), -1.1602149]. 
=============================================
[2019-04-04 09:17:08,267] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[82.07557 ]
 [86.90912 ]
 [86.130394]
 [85.0358  ]
 [83.36681 ]], R is [[84.60142517]
 [84.3534317 ]
 [83.8636322 ]
 [83.49747467]
 [83.50814819]].
[2019-04-04 09:17:09,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.1402776e-13 3.7126227e-12 1.9837580e-24 2.4576397e-13 2.2741116e-13
 4.8672382e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:09,738] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2616
[2019-04-04 09:17:09,776] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 68.5, 132.3333333333333, 94.99999999999999, 26.0, 26.37663961691351, 0.5143729098261195, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2213400.0000, 
sim time next is 2214000.0000, 
raw observation next is [-3.9, 68.0, 138.5, 142.5, 26.0, 26.36612639064812, 0.51350136582106, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.68, 0.46166666666666667, 0.1574585635359116, 0.6666666666666666, 0.6971771992206767, 0.6711671219403533, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6951748], dtype=float32), -1.3828421]. 
=============================================
[2019-04-04 09:17:09,779] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[86.86491]
 [86.21766]
 [85.75501]
 [85.87406]
 [86.10893]], R is [[87.59192657]
 [87.71601105]
 [87.83885193]
 [87.96046448]
 [88.08086395]].
[2019-04-04 09:17:14,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.3811469e-11 2.8792493e-11 7.0191253e-23 3.4193282e-12 2.7386132e-12
 1.0083979e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:14,442] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1437
[2019-04-04 09:17:14,468] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 75.0, 0.0, 0.0, 26.0, 24.40465044762112, 0.1726363602592991, 0.0, 1.0, 44196.03735009712], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2250000.0000, 
sim time next is 2250600.0000, 
raw observation next is [-6.800000000000001, 76.16666666666667, 0.0, 0.0, 26.0, 24.36424655172617, 0.1604414111218261, 0.0, 1.0, 44138.60339131512], 
processed observation next is [1.0, 0.043478260869565216, 0.2742382271468144, 0.7616666666666667, 0.0, 0.0, 0.6666666666666666, 0.530353879310514, 0.5534804703739421, 0.0, 1.0, 0.21018382567292915], 
reward next is 0.7898, 
noisyNet noise sample is [array([1.8147651], dtype=float32), 0.8024165]. 
=============================================
[2019-04-04 09:17:14,899] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3509931e-12 4.5177078e-12 1.2926693e-24 4.4510256e-13 3.9468816e-13
 2.2383104e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:14,899] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6820
[2019-04-04 09:17:14,957] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 87.0, 73.0, 27.5, 26.0, 25.62786587257388, 0.3082229650650252, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2278800.0000, 
sim time next is 2279400.0000, 
raw observation next is [-8.116666666666667, 85.5, 82.33333333333334, 31.33333333333334, 26.0, 25.64342517002868, 0.3027720382361976, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.23776546629732226, 0.855, 0.2744444444444445, 0.03462246777163905, 0.6666666666666666, 0.6369520975023901, 0.6009240127453992, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.30991855], dtype=float32), 0.16313058]. 
=============================================
[2019-04-04 09:17:18,047] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8911196e-11 1.2038177e-10 2.2057611e-23 1.6658437e-12 8.5449261e-12
 1.1229977e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:18,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4537
[2019-04-04 09:17:18,062] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.35589019679706, 0.4260993855719952, 0.0, 1.0, 49044.21485467817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2325600.0000, 
sim time next is 2326200.0000, 
raw observation next is [-1.8, 56.5, 0.0, 0.0, 26.0, 25.35395717009008, 0.3807690539700845, 0.0, 1.0, 42824.15990832199], 
processed observation next is [1.0, 0.9565217391304348, 0.41274238227146814, 0.565, 0.0, 0.0, 0.6666666666666666, 0.6128297641741733, 0.6269230179900281, 0.0, 1.0, 0.20392457099200947], 
reward next is 0.7961, 
noisyNet noise sample is [array([0.38202283], dtype=float32), -0.08963001]. 
=============================================
[2019-04-04 09:17:26,920] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2002508e-13 5.3178937e-12 4.9076546e-25 2.0831795e-13 6.3086119e-14
 3.1213442e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:26,925] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3402
[2019-04-04 09:17:26,982] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8999999999999999, 50.5, 0.0, 0.0, 26.0, 24.89548908209278, 0.3161692003843071, 1.0, 1.0, 135165.556747875], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2309400.0000, 
sim time next is 2310000.0000, 
raw observation next is [-1.0, 51.0, 0.0, 0.0, 26.0, 25.14712304855039, 0.3949133395880278, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4349030470914128, 0.51, 0.0, 0.0, 0.6666666666666666, 0.5955935873791992, 0.6316377798626759, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7386862], dtype=float32), -0.21240395]. 
=============================================
[2019-04-04 09:17:27,003] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.80477 ]
 [86.528984]
 [85.986374]
 [85.77252 ]
 [85.59379 ]], R is [[86.92160034]
 [86.40873718]
 [85.60670471]
 [85.38051605]
 [85.52671051]].
[2019-04-04 09:17:49,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.6337758e-13 2.2425742e-12 5.0408148e-25 2.0467913e-13 7.1932227e-14
 6.0181977e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:49,356] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8012
[2019-04-04 09:17:49,411] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8833333333333332, 48.16666666666667, 218.0, 168.3333333333333, 26.0, 24.39378074295448, 0.2998119942407746, 1.0, 1.0, 197762.2771956119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2638200.0000, 
sim time next is 2638800.0000, 
raw observation next is [-0.6, 47.0, 204.5, 179.0, 26.0, 24.92923088517778, 0.3889045023061561, 1.0, 1.0, 65585.8610099359], 
processed observation next is [1.0, 0.5652173913043478, 0.44598337950138506, 0.47, 0.6816666666666666, 0.19779005524861878, 0.6666666666666666, 0.5774359070981484, 0.629634834102052, 1.0, 1.0, 0.31231362385683764], 
reward next is 0.6877, 
noisyNet noise sample is [array([1.1709253], dtype=float32), -0.3968148]. 
=============================================
[2019-04-04 09:17:58,828] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3242951e-13 4.4688918e-12 3.2294536e-24 2.8525221e-13 9.1863847e-14
 3.5048977e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:17:58,845] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8226
[2019-04-04 09:17:58,901] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.4, 55.0, 102.0, 733.0, 26.0, 26.71360305330706, 0.6604479534888882, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2730600.0000, 
sim time next is 2731200.0000, 
raw observation next is [-4.266666666666667, 54.66666666666667, 99.33333333333333, 713.1666666666667, 26.0, 26.75427156693235, 0.6644031453334301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3444136657433057, 0.5466666666666667, 0.3311111111111111, 0.7880294659300185, 0.6666666666666666, 0.7295226305776957, 0.7214677151111434, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3880122], dtype=float32), 2.3427947]. 
=============================================
[2019-04-04 09:18:02,377] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 09:18:02,378] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:18:02,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:18:02,380] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run33
[2019-04-04 09:18:02,417] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:18:02,417] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:18:02,419] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run33
[2019-04-04 09:18:02,484] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:18:02,488] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:18:02,492] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run33
[2019-04-04 09:19:42,365] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.20370737], dtype=float32), -0.22845076]
[2019-04-04 09:19:42,365] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.9, 75.0, 17.5, 1.0, 26.0, 24.60894456563421, 0.2902678855665915, 1.0, 1.0, 198659.8129634583]
[2019-04-04 09:19:42,365] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:19:42,366] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.5372586e-12 1.3978726e-11 3.0187025e-24 1.0950489e-12 3.8985487e-13
 4.7580111e-15 1.0000000e+00], sampled 0.18863453714876022
[2019-04-04 09:21:01,531] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20370737], dtype=float32), -0.22845076]
[2019-04-04 09:21:01,531] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-2.3, 58.0, 0.0, 0.0, 26.0, 25.41104671976352, 0.5382557016829398, 0.0, 1.0, 58821.22059222888]
[2019-04-04 09:21:01,531] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:21:01,531] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [7.8432733e-11 9.8288912e-11 1.6929132e-23 7.1693558e-12 6.9969091e-12
 3.2073343e-14 1.0000000e+00], sampled 0.5023113965310312
[2019-04-04 09:21:07,733] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 09:21:32,741] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20370737], dtype=float32), -0.22845076]
[2019-04-04 09:21:32,741] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.918852297833333, 45.733397145, 168.9504332, 710.0363193666667, 26.0, 26.64601192052792, 0.7635328434980244, 1.0, 1.0, 0.0]
[2019-04-04 09:21:32,741] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:21:32,742] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.6974961e-13 6.8978292e-13 7.2760962e-26 5.5983596e-14 2.4447255e-14
 1.5331286e-16 1.0000000e+00], sampled 0.0253856667720197
[2019-04-04 09:21:40,512] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:21:47,692] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:21:48,745] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 3200000, evaluation results [3200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:21:56,543] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0690598e-12 7.2119684e-12 1.9672082e-25 6.8104360e-13 9.0985162e-14
 1.3032363e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:21:56,544] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-04 09:21:56,633] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 54.16666666666667, 166.6666666666667, 416.3333333333333, 26.0, 25.96479666712283, 0.433458179134489, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2801400.0000, 
sim time next is 2802000.0000, 
raw observation next is [-2.333333333333333, 53.33333333333334, 170.3333333333333, 462.1666666666667, 26.0, 25.99853967419151, 0.4443956842148797, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3979686057248385, 0.5333333333333334, 0.5677777777777776, 0.5106813996316759, 0.6666666666666666, 0.6665449728492924, 0.6481318947382932, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8967227], dtype=float32), -0.7258104]. 
=============================================
[2019-04-04 09:21:56,681] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.75217 ]
 [87.45533 ]
 [86.93181 ]
 [86.38288 ]
 [86.001015]], R is [[88.04020691]
 [88.1598053 ]
 [88.27820587]
 [88.39542389]
 [88.51146698]].
[2019-04-04 09:22:02,452] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3316102e-12 1.3617579e-12 6.7459181e-26 2.8338402e-13 1.1705208e-13
 6.5942561e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:02,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5272
[2019-04-04 09:22:02,526] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 93.0, 70.00000000000001, 113.0, 26.0, 24.90473993582142, 0.2374963515969784, 1.0, 1.0, 18717.69450508669], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2880600.0000, 
sim time next is 2881200.0000, 
raw observation next is [1.666666666666667, 93.0, 87.5, 134.5, 26.0, 24.80324729708973, 0.297618491782389, 1.0, 1.0, 88820.89384951346], 
processed observation next is [1.0, 0.34782608695652173, 0.5087719298245615, 0.93, 0.2916666666666667, 0.14861878453038674, 0.6666666666666666, 0.5669372747574775, 0.599206163927463, 1.0, 1.0, 0.42295663737863554], 
reward next is 0.5770, 
noisyNet noise sample is [array([-1.6350752], dtype=float32), 0.4006117]. 
=============================================
[2019-04-04 09:22:08,433] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6647456e-11 8.6322755e-11 8.5566496e-24 3.1813035e-12 5.3150548e-12
 3.5313193e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:08,433] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0968
[2019-04-04 09:22:08,455] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 80.16666666666667, 0.0, 0.0, 26.0, 25.13094350878814, 0.2978905948497493, 0.0, 1.0, 56201.58923101906], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2859000.0000, 
sim time next is 2859600.0000, 
raw observation next is [1.0, 81.33333333333334, 0.0, 0.0, 26.0, 25.11922425207557, 0.2957752791113055, 0.0, 1.0, 56223.8237837162], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.8133333333333335, 0.0, 0.0, 0.6666666666666666, 0.593268687672964, 0.5985917597037685, 0.0, 1.0, 0.2677324942081724], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.6543787], dtype=float32), 0.51918274]. 
=============================================
[2019-04-04 09:22:20,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6788509e-11 3.1357313e-11 6.3369682e-24 5.6590227e-12 1.0967838e-12
 4.9664605e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:20,038] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1267
[2019-04-04 09:22:20,071] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 73.16666666666667, 0.0, 0.0, 26.0, 25.14015126385536, 0.3211881723256231, 0.0, 1.0, 50916.71805539967], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2855400.0000, 
sim time next is 2856000.0000, 
raw observation next is [1.0, 74.33333333333334, 0.0, 0.0, 26.0, 25.16021168982872, 0.3223504055827781, 0.0, 1.0, 50103.27869011114], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5966843074857268, 0.6074501351942594, 0.0, 1.0, 0.23858704138148162], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.43110055], dtype=float32), 1.2337284]. 
=============================================
[2019-04-04 09:22:20,083] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[80.322014]
 [80.29498 ]
 [80.303474]
 [80.29458 ]
 [80.25661 ]], R is [[80.2875061 ]
 [80.24217224]
 [80.19304657]
 [80.13975525]
 [80.08200836]].
[2019-04-04 09:22:20,147] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1696271e-11 1.5908217e-11 2.6555248e-23 1.4259972e-12 2.2435925e-12
 3.5586272e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:20,149] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4266
[2019-04-04 09:22:20,186] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 39.0, 91.5, 724.0, 26.0, 25.1194115537939, 0.3653356299190771, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3078000.0000, 
sim time next is 3078600.0000, 
raw observation next is [0.1666666666666667, 39.16666666666666, 89.0, 707.0, 26.0, 25.12496436920787, 0.3648297498295625, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4672206832871654, 0.39166666666666655, 0.2966666666666667, 0.7812154696132597, 0.6666666666666666, 0.5937470307673225, 0.6216099166098542, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.31870455], dtype=float32), 0.8108058]. 
=============================================
[2019-04-04 09:22:26,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.9163620e-11 1.1753191e-10 5.1504277e-23 9.8401790e-12 3.5191461e-12
 5.0688220e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:26,876] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0121
[2019-04-04 09:22:26,889] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.21406333171262, 0.1046082741597717, 0.0, 1.0, 39491.52241072408], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3035400.0000, 
sim time next is 3036000.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.2113661826549, 0.09871443371297679, 0.0, 1.0, 39621.6907900564], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5176138485545749, 0.532904811237659, 0.0, 1.0, 0.1886747180478876], 
reward next is 0.8113, 
noisyNet noise sample is [array([-1.2892293], dtype=float32), 0.6897443]. 
=============================================
[2019-04-04 09:22:26,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.851295]
 [77.8649  ]
 [77.9354  ]
 [78.01542 ]
 [78.10347 ]], R is [[77.85308075]
 [77.8864975 ]
 [77.92029572]
 [77.9544754 ]
 [77.989151  ]].
[2019-04-04 09:22:30,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8233279e-11 3.4156861e-11 1.4800243e-24 4.9521541e-12 2.3665982e-12
 5.4955019e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:30,101] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6134
[2019-04-04 09:22:30,168] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 60.66666666666666, 85.66666666666667, 405.0, 26.0, 24.93314892956775, 0.2891828606275618, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3055200.0000, 
sim time next is 3055800.0000, 
raw observation next is [-6.0, 59.83333333333334, 88.33333333333334, 451.0, 26.0, 25.29867911727391, 0.3167370158206188, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.5983333333333334, 0.29444444444444445, 0.4983425414364641, 0.6666666666666666, 0.6082232597728258, 0.6055790052735396, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4905603], dtype=float32), -1.1291587]. 
=============================================
[2019-04-04 09:22:33,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3902978e-13 1.2521984e-12 2.0254388e-25 1.2201144e-14 5.7938571e-14
 1.6212720e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:33,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0758
[2019-04-04 09:22:33,578] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 90.0, 111.6666666666667, 813.8333333333334, 26.0, 25.4844489970005, 0.6860126318584049, 1.0, 1.0, 65057.4560804026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3244800.0000, 
sim time next is 3245400.0000, 
raw observation next is [-3.0, 92.5, 111.0, 812.0, 26.0, 25.9806841958818, 0.73328795655805, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3795013850415513, 0.925, 0.37, 0.8972375690607735, 0.6666666666666666, 0.6650570163234834, 0.7444293188526833, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8077011], dtype=float32), -0.0996327]. 
=============================================
[2019-04-04 09:22:38,229] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.9755030e-10 1.3536339e-09 6.8508579e-22 6.2626085e-11 5.7717296e-11
 9.2951492e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:38,234] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2126
[2019-04-04 09:22:38,248] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 77.0, 0.0, 0.0, 26.0, 24.76195458024647, 0.2665391113849586, 0.0, 1.0, 44206.4252363448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3297600.0000, 
sim time next is 3298200.0000, 
raw observation next is [-9.083333333333334, 76.83333333333334, 0.0, 0.0, 26.0, 24.66028106469301, 0.2671806405980895, 0.0, 1.0, 43998.25219366558], 
processed observation next is [1.0, 0.17391304347826086, 0.21098799630655585, 0.7683333333333334, 0.0, 0.0, 0.6666666666666666, 0.5550234220577508, 0.5890602135326964, 0.0, 1.0, 0.20951548663650277], 
reward next is 0.7905, 
noisyNet noise sample is [array([-1.016506], dtype=float32), -0.26530182]. 
=============================================
[2019-04-04 09:22:43,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4835832e-12 4.5851417e-12 2.2706446e-24 1.3955961e-13 3.6145606e-13
 4.0786926e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:22:43,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5807
[2019-04-04 09:22:43,388] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.59432046310412, 0.6257093286045742, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3195000.0000, 
sim time next is 3195600.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.60528811520976, 0.6151634927576999, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6337740096008133, 0.7050544975859, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.17431648], dtype=float32), -0.1043675]. 
=============================================
[2019-04-04 09:23:09,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1482820e-11 2.7849387e-10 7.8963323e-23 8.6549570e-12 9.1773568e-12
 2.8046831e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:09,606] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6368
[2019-04-04 09:23:09,626] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.97928260234481, 0.309256991751385, 0.0, 1.0, 43818.65602690108], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3816600.0000, 
sim time next is 3817200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 24.96308828561965, 0.3005874384167624, 0.0, 1.0, 43882.08864293048], 
processed observation next is [1.0, 0.17391304347826086, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.580257357134971, 0.6001958128055874, 0.0, 1.0, 0.20896232687109753], 
reward next is 0.7910, 
noisyNet noise sample is [array([-1.3498826], dtype=float32), 0.70027965]. 
=============================================
[2019-04-04 09:23:25,571] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.5221025e-14 1.2356333e-12 2.1441172e-26 3.1852528e-14 9.2246741e-15
 1.2112833e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:25,571] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1183
[2019-04-04 09:23:25,610] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 40.0, 115.5, 798.5, 26.0, 26.56591163649881, 0.5814053621988381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4014000.0000, 
sim time next is 4014600.0000, 
raw observation next is [-7.666666666666667, 39.5, 116.6666666666667, 804.3333333333334, 26.0, 26.49550303229624, 0.5812792328548869, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.2502308402585411, 0.395, 0.388888888888889, 0.8887661141804789, 0.6666666666666666, 0.7079585860246868, 0.6937597442849622, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1919289], dtype=float32), 0.25660282]. 
=============================================
[2019-04-04 09:23:26,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3362873e-11 1.6224896e-10 6.4376730e-23 9.3509775e-12 2.7422719e-12
 5.2524357e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:26,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0512
[2019-04-04 09:23:26,240] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.07849301226314, 0.331954634632601, 0.0, 1.0, 40815.55491352845], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4065600.0000, 
sim time next is 4066200.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.12340783229716, 0.3374208911349341, 0.0, 1.0, 40818.53046298795], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5936173193580968, 0.6124736303783114, 0.0, 1.0, 0.1943739545856569], 
reward next is 0.8056, 
noisyNet noise sample is [array([1.0275532], dtype=float32), -0.96969444]. 
=============================================
[2019-04-04 09:23:32,296] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5116164e-11 2.9520296e-11 1.2353908e-23 2.8817257e-12 2.6121713e-12
 4.0416931e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:32,297] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4622
[2019-04-04 09:23:32,318] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.19654905970373, 0.3611528742591841, 0.0, 1.0, 39489.9949385464], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4159200.0000, 
sim time next is 4159800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.22491560366252, 0.3595544997718135, 0.0, 1.0, 39485.69589934436], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.60207630030521, 0.6198514999239378, 0.0, 1.0, 0.18802712333021124], 
reward next is 0.8120, 
noisyNet noise sample is [array([-0.6006279], dtype=float32), -0.18674155]. 
=============================================
[2019-04-04 09:23:41,843] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3224993e-11 3.8412603e-11 2.9288755e-24 1.9878075e-12 1.0074795e-12
 1.7660933e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:41,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6840
[2019-04-04 09:23:41,859] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.8, 70.33333333333334, 0.0, 0.0, 26.0, 24.8901424094672, 0.2952639058175484, 0.0, 1.0, 196752.8527706141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4303200.0000, 
sim time next is 4303800.0000, 
raw observation next is [5.7, 71.0, 0.0, 0.0, 26.0, 24.87557183568607, 0.3223028093602953, 0.0, 1.0, 198265.5258298861], 
processed observation next is [0.0, 0.8260869565217391, 0.6204986149584488, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5729643196405059, 0.6074342697867651, 0.0, 1.0, 0.9441215515708862], 
reward next is 0.0559, 
noisyNet noise sample is [array([-0.5462131], dtype=float32), 0.75319666]. 
=============================================
[2019-04-04 09:23:43,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.5500821e-12 2.1590878e-11 1.3091447e-24 4.8320218e-13 3.0773910e-12
 4.1802152e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:43,453] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5246
[2019-04-04 09:23:43,465] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.8, 60.0, 0.0, 0.0, 26.0, 26.70083944653212, 0.8124031992566279, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4397400.0000, 
sim time next is 4398000.0000, 
raw observation next is [9.666666666666668, 60.33333333333333, 0.0, 0.0, 26.0, 26.63987165946278, 0.8002309816854529, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.7303785780240075, 0.6033333333333333, 0.0, 0.0, 0.6666666666666666, 0.7199893049552317, 0.7667436605618176, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0076691], dtype=float32), 1.6149569]. 
=============================================
[2019-04-04 09:23:43,473] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[83.06687 ]
 [83.1611  ]
 [83.20584 ]
 [83.23178 ]
 [83.213196]], R is [[83.18079376]
 [83.34898376]
 [83.5154953 ]
 [83.68034363]
 [83.84354401]].
[2019-04-04 09:23:44,684] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.4431085e-13 3.6080584e-13 6.3962691e-26 2.6548989e-14 1.4512061e-14
 2.9771448e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:44,684] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5792
[2019-04-04 09:23:44,742] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 149.0, 3.0, 26.0, 25.11843319524032, 0.5019339673561586, 1.0, 1.0, 35574.85346711096], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4453200.0000, 
sim time next is 4453800.0000, 
raw observation next is [0.0, 92.0, 164.6666666666667, 4.0, 26.0, 25.44752109312567, 0.5431178613526367, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.46260387811634357, 0.92, 0.5488888888888891, 0.004419889502762431, 0.6666666666666666, 0.6206267577604724, 0.6810392871175456, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4102179], dtype=float32), 0.5254701]. 
=============================================
[2019-04-04 09:23:48,889] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1223632e-13 1.0440403e-12 2.5884702e-26 7.4042403e-14 2.2122007e-14
 4.0800131e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:48,889] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-04 09:23:48,907] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 222.3333333333333, 107.6666666666667, 26.0, 26.37033947509359, 0.6473909967388115, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4443000.0000, 
sim time next is 4443600.0000, 
raw observation next is [1.0, 86.0, 236.6666666666667, 126.8333333333333, 26.0, 26.41474904703605, 0.659479774064707, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.86, 0.7888888888888891, 0.14014732965009205, 0.6666666666666666, 0.7012290872530041, 0.7198265913549023, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6432306], dtype=float32), -0.08682064]. 
=============================================
[2019-04-04 09:23:51,476] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5423644e-13 5.3215743e-13 2.9162521e-26 8.2577893e-14 2.9709874e-14
 9.8024635e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:51,487] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0256
[2019-04-04 09:23:51,514] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 115.0, 0.0, 26.0, 26.29328395174531, 0.5615001346381708, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4699800.0000, 
sim time next is 4700400.0000, 
raw observation next is [0.0, 92.0, 130.5, 0.9999999999999998, 26.0, 26.37389472609917, 0.5680392093541692, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.46260387811634357, 0.92, 0.435, 0.0011049723756906074, 0.6666666666666666, 0.6978245605082641, 0.6893464031180564, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42130002], dtype=float32), 0.028779527]. 
=============================================
[2019-04-04 09:23:54,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6106786e-12 2.0664410e-12 1.9726328e-26 1.4606374e-13 1.5735885e-13
 8.6568163e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:54,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5516
[2019-04-04 09:23:54,478] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 20.0, 38.66666666666666, 26.0, 25.5879908537487, 0.5065500774878173, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4434000.0000, 
sim time next is 4434600.0000, 
raw observation next is [2.0, 80.0, 39.99999999999999, 77.33333333333331, 26.0, 25.60241067106702, 0.5019171330079456, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.1333333333333333, 0.08545119705340698, 0.6666666666666666, 0.6335342225889183, 0.6673057110026486, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.72860146], dtype=float32), 0.8321157]. 
=============================================
[2019-04-04 09:23:55,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3934278e-11 2.8498603e-11 1.2245145e-24 5.1949601e-13 1.7874826e-12
 2.7995241e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:55,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4059
[2019-04-04 09:23:55,387] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.1, 66.0, 0.0, 0.0, 26.0, 25.74056321939412, 0.5578633293864073, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4413600.0000, 
sim time next is 4414200.0000, 
raw observation next is [5.916666666666666, 66.16666666666667, 0.0, 0.0, 26.0, 25.66689542217449, 0.5615948010454662, 0.0, 1.0, 148432.7184830936], 
processed observation next is [1.0, 0.08695652173913043, 0.6265004616805172, 0.6616666666666667, 0.0, 0.0, 0.6666666666666666, 0.6389079518478743, 0.6871982670151554, 0.0, 1.0, 0.7068224689671124], 
reward next is 0.2932, 
noisyNet noise sample is [array([-0.8684577], dtype=float32), 0.08561985]. 
=============================================
[2019-04-04 09:23:59,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2658160e-12 3.6816678e-11 7.8437599e-24 2.3103346e-12 2.2825127e-12
 8.1736680e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:23:59,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4109
[2019-04-04 09:23:59,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 60.33333333333334, 0.0, 0.0, 26.0, 25.5609890260627, 0.4720204234668229, 0.0, 1.0, 57599.84395202609], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4672200.0000, 
sim time next is 4672800.0000, 
raw observation next is [2.0, 62.0, 0.0, 0.0, 26.0, 25.48371523541405, 0.468938866281323, 0.0, 1.0, 83765.7111360752], 
processed observation next is [1.0, 0.08695652173913043, 0.518005540166205, 0.62, 0.0, 0.0, 0.6666666666666666, 0.6236429362845041, 0.6563129554271077, 0.0, 1.0, 0.39888433874321527], 
reward next is 0.6011, 
noisyNet noise sample is [array([1.9174587], dtype=float32), -1.7855309]. 
=============================================
[2019-04-04 09:24:00,202] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5526172e-12 2.1147766e-11 8.7169966e-24 1.3697212e-12 6.5995289e-13
 3.2810634e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:00,203] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7559
[2019-04-04 09:24:00,225] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 26.0, 26.12868891041919, 0.6414891887360449, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4648200.0000, 
sim time next is 4648800.0000, 
raw observation next is [2.666666666666667, 52.66666666666667, 0.0, 0.0, 26.0, 26.05518100750793, 0.6233009045685032, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5364727608494922, 0.5266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6712650839589941, 0.707766968189501, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8314355], dtype=float32), 0.25776777]. 
=============================================
[2019-04-04 09:24:02,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2192533e-13 5.4776333e-13 1.0107101e-25 9.2012376e-14 4.5573933e-14
 9.5714141e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:02,390] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1188
[2019-04-04 09:24:02,399] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 112.5, 0.0, 26.0, 26.07259901300294, 0.4826875044145688, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4711200.0000, 
sim time next is 4711800.0000, 
raw observation next is [1.0, 86.0, 117.0, 0.0, 26.0, 25.93239575589477, 0.4584221030436155, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.39, 0.0, 0.6666666666666666, 0.6610329796578975, 0.6528073676812052, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03647755], dtype=float32), 0.77688676]. 
=============================================
[2019-04-04 09:24:02,606] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0023058e-12 2.5549004e-11 8.7370716e-24 7.0539489e-13 6.3494170e-13
 2.8176735e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:02,610] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9349
[2019-04-04 09:24:02,634] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.57942982625696, 0.4374596303206384, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4687800.0000, 
sim time next is 4688400.0000, 
raw observation next is [-1.0, 100.0, 0.0, 0.0, 26.0, 25.52046215637667, 0.4170738197729407, 0.0, 1.0, 21125.33578117482], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6267051796980558, 0.6390246065909803, 0.0, 1.0, 0.10059683705321343], 
reward next is 0.8994, 
noisyNet noise sample is [array([-0.75720245], dtype=float32), 0.43766195]. 
=============================================
[2019-04-04 09:24:02,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4043074e-13 1.5195449e-12 3.3290818e-25 2.3833531e-13 4.3496324e-14
 4.6282743e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:02,909] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8800
[2019-04-04 09:24:02,922] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 47.5, 138.3333333333333, 30.66666666666666, 26.0, 26.45319366664809, 0.6021003586075785, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4549800.0000, 
sim time next is 4550400.0000, 
raw observation next is [2.0, 48.0, 131.0, 40.0, 26.0, 26.47423399040414, 0.6046649151141085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.48, 0.43666666666666665, 0.04419889502762431, 0.6666666666666666, 0.7061861658670118, 0.7015549717047028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.19798267], dtype=float32), 0.29926103]. 
=============================================
[2019-04-04 09:24:04,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.1475747e-13 1.9939059e-12 1.7347378e-25 1.0207478e-13 9.5580329e-14
 1.4629166e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:04,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1281
[2019-04-04 09:24:04,845] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 160.5, 3.0, 26.0, 26.3042741523689, 0.5517976612613782, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4708800.0000, 
sim time next is 4709400.0000, 
raw observation next is [1.0, 86.0, 143.0, 2.0, 26.0, 26.27856235682959, 0.5452418471696461, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4903047091412743, 0.86, 0.4766666666666667, 0.0022099447513812156, 0.6666666666666666, 0.6898801964024658, 0.6817472823898821, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7199223], dtype=float32), 0.8155723]. 
=============================================
[2019-04-04 09:24:08,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7470111e-11 2.1944401e-11 6.4280041e-23 5.0168016e-12 2.2878130e-12
 1.0404827e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:08,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8753
[2019-04-04 09:24:09,001] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 112.1666666666667, 334.5, 26.0, 25.15587178299585, 0.3760279582760872, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4897200.0000, 
sim time next is 4897800.0000, 
raw observation next is [3.0, 45.0, 102.0, 317.0, 26.0, 25.21631520609998, 0.3720363539786365, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.45, 0.34, 0.35027624309392263, 0.6666666666666666, 0.6013596005083318, 0.6240121179928788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3509673], dtype=float32), -1.0869806]. 
=============================================
[2019-04-04 09:24:13,086] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1867278e-11 8.5148645e-11 1.8824386e-23 3.6953704e-12 1.7347436e-11
 2.0490909e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:13,092] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9910
[2019-04-04 09:24:13,104] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 70.0, 46.99999999999999, 104.3333333333333, 26.0, 24.37199317192231, 0.1502253678384365, 0.0, 1.0, 39465.42780529632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4866600.0000, 
sim time next is 4867200.0000, 
raw observation next is [-4.0, 71.0, 70.5, 156.5, 26.0, 24.3637369658134, 0.1585826907811038, 0.0, 1.0, 39300.42875272663], 
processed observation next is [0.0, 0.34782608695652173, 0.3518005540166205, 0.71, 0.235, 0.17292817679558012, 0.6666666666666666, 0.5303114138177832, 0.5528608969270347, 0.0, 1.0, 0.18714489882250776], 
reward next is 0.8129, 
noisyNet noise sample is [array([-1.3890451], dtype=float32), -0.6567806]. 
=============================================
[2019-04-04 09:24:14,647] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:14,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:14,688] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run25
[2019-04-04 09:24:14,990] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8601854e-11 8.5588772e-11 3.7928950e-23 6.0371799e-12 4.1215560e-12
 2.5009158e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:14,992] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5593
[2019-04-04 09:24:15,008] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.30367374545067, 0.3454453116383274, 0.0, 1.0, 37139.82202209342], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5036400.0000, 
sim time next is 5037000.0000, 
raw observation next is [-2.833333333333333, 65.0, 39.33333333333334, 67.33333333333334, 26.0, 25.24778613458386, 0.3381819646447988, 0.0, 1.0, 37077.06121315458], 
processed observation next is [1.0, 0.30434782608695654, 0.3841181902123731, 0.65, 0.13111111111111115, 0.07440147329650093, 0.6666666666666666, 0.6039821778819882, 0.6127273215482663, 0.0, 1.0, 0.17655743434835514], 
reward next is 0.8234, 
noisyNet noise sample is [array([0.11319599], dtype=float32), 1.4048078]. 
=============================================
[2019-04-04 09:24:15,017] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.34145 ]
 [78.48605 ]
 [78.643295]
 [78.778305]
 [78.902824]], R is [[79.69449615]
 [79.7206955 ]
 [79.74536896]
 [79.7668457 ]
 [79.77879333]].
[2019-04-04 09:24:18,585] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:18,586] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:18,602] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run25
[2019-04-04 09:24:19,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:19,296] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:19,303] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run25
[2019-04-04 09:24:20,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:20,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:20,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run25
[2019-04-04 09:24:20,339] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6502582e-13 4.5992236e-13 5.8170751e-26 2.8873896e-14 1.5478802e-14
 1.4750022e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:20,341] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5217
[2019-04-04 09:24:20,459] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.83333333333333, 17.0, 49.33333333333333, 389.6666666666666, 26.0, 28.8669398719788, 1.175852070523924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5073000.0000, 
sim time next is 5073600.0000, 
raw observation next is [11.66666666666667, 17.0, 42.66666666666666, 340.8333333333333, 26.0, 29.05819799478462, 0.9868948916428573, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.785780240073869, 0.17, 0.1422222222222222, 0.3766114180478821, 0.6666666666666666, 0.9215164995653851, 0.8289649638809524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7102911], dtype=float32), 0.41012114]. 
=============================================
[2019-04-04 09:24:22,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:22,708] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:22,713] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run25
[2019-04-04 09:24:24,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:24,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:24,320] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run25
[2019-04-04 09:24:25,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:25,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:25,095] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run25
[2019-04-04 09:24:25,376] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:25,377] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:25,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run25
[2019-04-04 09:24:26,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2447097e-13 5.1241962e-12 2.4338713e-25 3.6986405e-13 4.5111247e-14
 8.3915671e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:26,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3142
[2019-04-04 09:24:26,026] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.0, 17.0, 0.0, 0.0, 26.0, 28.59772189135163, 1.095942210790192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5077200.0000, 
sim time next is 5077800.0000, 
raw observation next is [11.0, 17.0, 0.0, 0.0, 26.0, 28.455429099856, 1.075028519524866, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.0, 0.0, 0.6666666666666666, 0.8712857583213335, 0.8583428398416221, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3087164], dtype=float32), -0.7122034]. 
=============================================
[2019-04-04 09:24:26,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:26,548] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:26,555] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run25
[2019-04-04 09:24:28,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:28,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:28,257] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run25
[2019-04-04 09:24:28,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:28,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:28,661] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run25
[2019-04-04 09:24:29,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:29,125] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:29,129] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run25
[2019-04-04 09:24:29,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:29,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:29,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run25
[2019-04-04 09:24:36,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:36,154] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:36,157] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run25
[2019-04-04 09:24:37,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:37,189] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:37,195] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run25
[2019-04-04 09:24:38,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:24:38,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:24:38,025] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run25
[2019-04-04 09:24:45,411] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5564531e-11 8.7255383e-11 1.1092707e-22 6.5988274e-12 6.0717082e-12
 1.6717049e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:24:45,411] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0100
[2019-04-04 09:24:45,555] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.4833243543006, -0.2223548512970458, 1.0, 1.0, 202343.6265148717], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 199200.0000, 
sim time next is 199800.0000, 
raw observation next is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.83857292428236, -0.1278052470119021, 0.0, 1.0, 203489.4693139135], 
processed observation next is [1.0, 0.30434782608695654, 0.21606648199445982, 0.78, 0.0, 0.0, 0.6666666666666666, 0.40321441035686334, 0.45739825099603265, 0.0, 1.0, 0.9689974729233976], 
reward next is 0.0310, 
noisyNet noise sample is [array([-1.398089], dtype=float32), -0.34665284]. 
=============================================
[2019-04-04 09:25:01,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.5176120e-12 1.0214072e-11 7.4907092e-24 7.6841582e-13 9.9230845e-13
 3.7103520e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:25:01,073] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-04 09:25:01,132] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.51666666666667, 62.5, 89.66666666666667, 469.0, 26.0, 25.81580267567035, 0.3576527219096133, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 295800.0000, 
sim time next is 296400.0000, 
raw observation next is [-11.33333333333333, 62.00000000000001, 88.33333333333333, 490.5, 26.0, 25.82299256368055, 0.3609920887555328, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.14866112650046176, 0.6200000000000001, 0.29444444444444445, 0.541988950276243, 0.6666666666666666, 0.6519160469733792, 0.6203306962518442, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3633533], dtype=float32), 1.2629225]. 
=============================================
[2019-04-04 09:25:39,161] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9197896e-12 1.9555046e-12 4.0460225e-25 7.9592108e-14 7.8032117e-14
 8.2685259e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:25:39,161] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3981
[2019-04-04 09:25:39,218] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 80.0, 138.0, 595.0, 26.0, 24.96688054580883, 0.3485901708508063, 0.0, 1.0, 19021.05110812647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 567000.0000, 
sim time next is 567600.0000, 
raw observation next is [-1.2, 80.0, 136.1666666666667, 573.6666666666667, 26.0, 24.98610578876296, 0.3498936000809588, 0.0, 1.0, 18727.72066235705], 
processed observation next is [0.0, 0.5652173913043478, 0.42936288088642666, 0.8, 0.45388888888888906, 0.6338858195211787, 0.6666666666666666, 0.5821754823969133, 0.6166312000269863, 0.0, 1.0, 0.08917962220170024], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.4253399], dtype=float32), -0.287241]. 
=============================================
[2019-04-04 09:25:47,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9370390e-13 1.2829943e-12 2.1384249e-25 3.3386512e-14 4.6943362e-14
 8.5502192e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:25:47,084] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0399
[2019-04-04 09:25:47,127] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 25.59395917188408, 0.3830305304670608, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 839400.0000, 
sim time next is 840000.0000, 
raw observation next is [-3.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.80338004543098, 0.3683918319895188, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3545706371191136, 0.8466666666666667, 0.0, 0.0, 0.6666666666666666, 0.6502816704525817, 0.6227972773298396, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32431465], dtype=float32), -0.00423999]. 
=============================================
[2019-04-04 09:25:47,137] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.209595]
 [86.55268 ]
 [86.68726 ]
 [86.11013 ]
 [86.17673 ]], R is [[86.28424072]
 [86.42140198]
 [86.55718994]
 [85.74702454]
 [85.66650391]].
[2019-04-04 09:25:51,743] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6127630e-13 4.7987921e-13 1.5911183e-27 1.0211835e-14 1.3161241e-14
 4.9974686e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:25:51,743] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7366
[2019-04-04 09:25:51,801] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.0, 92.0, 43.5, 0.0, 26.0, 26.14637123743889, 0.5740108565304533, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 982800.0000, 
sim time next is 983400.0000, 
raw observation next is [10.08333333333333, 92.16666666666667, 49.00000000000001, 0.0, 26.0, 26.24191036606288, 0.5790427284364678, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.7419205909510619, 0.9216666666666667, 0.16333333333333336, 0.0, 0.6666666666666666, 0.6868258638385732, 0.693014242812156, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1120121], dtype=float32), -0.6462747]. 
=============================================
[2019-04-04 09:25:57,736] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1796322e-12 1.3601019e-12 7.6396040e-27 1.3852374e-13 3.7468225e-14
 6.7547814e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:25:57,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5424
[2019-04-04 09:25:57,771] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.1, 66.83333333333333, 0.0, 0.0, 26.0, 25.67388359371163, 0.669381389590986, 0.0, 1.0, 18724.46257108603], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1120200.0000, 
sim time next is 1120800.0000, 
raw observation next is [12.0, 67.66666666666667, 0.0, 0.0, 26.0, 25.71078473151037, 0.669300627801188, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7950138504155125, 0.6766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6425653942925308, 0.7231002092670628, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02266889], dtype=float32), -0.4132179]. 
=============================================
[2019-04-04 09:25:59,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6053617e-13 1.3738134e-12 1.7606978e-26 2.9383241e-14 2.7016910e-14
 6.7947355e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:25:59,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6028
[2019-04-04 09:25:59,351] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 90.5, 97.0, 0.0, 26.0, 25.35464579227338, 0.2806896550437646, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 905400.0000, 
sim time next is 906000.0000, 
raw observation next is [2.166666666666667, 92.66666666666666, 98.16666666666667, 0.0, 26.0, 25.35283886433297, 0.2767645800862847, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5226223453370269, 0.9266666666666665, 0.32722222222222225, 0.0, 0.6666666666666666, 0.6127365720277475, 0.5922548600287616, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1472081], dtype=float32), 0.5450798]. 
=============================================
[2019-04-04 09:25:59,370] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[91.05665 ]
 [91.052574]
 [91.11297 ]
 [91.21578 ]
 [91.36414 ]], R is [[91.16131592]
 [91.24970245]
 [91.33720398]
 [91.42383575]
 [91.50959778]].
[2019-04-04 09:26:00,115] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9698364e-13 4.3270058e-12 6.9157525e-26 4.9878199e-14 2.5694019e-13
 1.4060891e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:26:00,115] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0105
[2019-04-04 09:26:00,132] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 97.33333333333334, 0.0, 0.0, 26.0, 25.14254202696572, 0.3930804697720537, 0.0, 1.0, 40157.11741611649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 942000.0000, 
sim time next is 942600.0000, 
raw observation next is [5.0, 96.66666666666666, 0.0, 0.0, 26.0, 25.16060280246975, 0.3958216665274363, 0.0, 1.0, 39957.68714271274], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5967169002058125, 0.6319405555091454, 0.0, 1.0, 0.1902747006795845], 
reward next is 0.8097, 
noisyNet noise sample is [array([0.12368602], dtype=float32), 0.92686695]. 
=============================================
[2019-04-04 09:26:02,575] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.5842160e-13 9.5507489e-13 1.5385284e-26 2.5109628e-14 4.1644124e-14
 4.5002725e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:26:02,577] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4647
[2019-04-04 09:26:02,587] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.92117634287681, 0.6065342207709152, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1032000.0000, 
sim time next is 1032600.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.86774153011006, 0.6026986746054934, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6556451275091716, 0.7008995582018311, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.47191337], dtype=float32), 1.445371]. 
=============================================
[2019-04-04 09:26:09,925] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6471584e-12 3.2576344e-12 3.8085336e-26 4.3475508e-14 2.1227680e-13
 1.2401642e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:26:09,927] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3357
[2019-04-04 09:26:09,952] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.60644914512243, 0.5862622304403459, 0.0, 1.0, 44611.7739729213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1062000.0000, 
sim time next is 1062600.0000, 
raw observation next is [13.11666666666667, 80.5, 0.0, 0.0, 26.0, 25.62216666231978, 0.5970340078864028, 0.0, 1.0, 24752.07907255277], 
processed observation next is [1.0, 0.30434782608695654, 0.8259464450600187, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6351805551933151, 0.6990113359621343, 0.0, 1.0, 0.11786704320263224], 
reward next is 0.8821, 
noisyNet noise sample is [array([1.8604474], dtype=float32), -0.053264722]. 
=============================================
[2019-04-04 09:26:10,948] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9562281e-12 1.5223182e-12 6.6816093e-26 1.8185903e-13 5.4116464e-14
 1.0934937e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:26:10,948] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5713
[2019-04-04 09:26:11,006] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.18333333333333, 56.33333333333333, 0.0, 0.0, 26.0, 26.36928040316712, 0.7386766719341473, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1104600.0000, 
sim time next is 1105200.0000, 
raw observation next is [15.0, 57.0, 0.0, 0.0, 26.0, 26.27744130050078, 0.7354246004738254, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8781163434903049, 0.57, 0.0, 0.0, 0.6666666666666666, 0.6897867750417316, 0.7451415334912751, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2335271], dtype=float32), 0.6592924]. 
=============================================
[2019-04-04 09:26:36,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.56005893e-12 1.06912985e-11 2.21695281e-25 2.85238060e-13
 8.37186819e-13 3.24435980e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 09:26:36,491] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1117
[2019-04-04 09:26:36,534] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 96.0, 0.0, 0.0, 26.0, 25.31714148992549, 0.4543666911725995, 0.0, 1.0, 47884.22954857634], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [2.2, 96.0, 0.0, 0.0, 26.0, 25.34876411467571, 0.4542443835074009, 0.0, 1.0, 40823.0403931985], 
processed observation next is [1.0, 0.17391304347826086, 0.5235457063711911, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6123970095563092, 0.651414794502467, 0.0, 1.0, 0.19439543044380236], 
reward next is 0.8056, 
noisyNet noise sample is [array([-0.43810815], dtype=float32), 1.5953692]. 
=============================================
[2019-04-04 09:26:38,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.4586544e-12 3.4405968e-12 1.9126708e-26 8.4530527e-14 1.3900227e-13
 5.0901866e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:26:38,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6997
[2019-04-04 09:26:38,550] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.7, 89.66666666666667, 0.0, 0.0, 26.0, 23.97651321997819, 0.2394109587653867, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1219200.0000, 
sim time next is 1219800.0000, 
raw observation next is [15.6, 91.33333333333333, 0.0, 0.0, 26.0, 23.94489350018761, 0.2355859088861035, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.08695652173913043, 0.8947368421052633, 0.9133333333333333, 0.0, 0.0, 0.6666666666666666, 0.4954077916823009, 0.5785286362953678, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6830232], dtype=float32), 0.20957729]. 
=============================================
[2019-04-04 09:26:40,908] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 09:26:40,925] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:26:40,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:26:40,927] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run34
[2019-04-04 09:26:41,002] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:26:41,003] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:26:41,005] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:26:41,006] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:26:41,007] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run34
[2019-04-04 09:26:41,006] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run34
[2019-04-04 09:27:58,851] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18659459], dtype=float32), -0.24847072]
[2019-04-04 09:27:58,851] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [7.187784075833333, 69.655535635, 0.0, 0.0, 26.0, 25.68334387049427, 0.6321603655038469, 0.0, 1.0, 0.0]
[2019-04-04 09:27:58,851] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:27:58,852] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.0525749e-12 8.7856875e-12 5.3999415e-26 3.0758756e-13 2.8954318e-13
 7.7346559e-16 1.0000000e+00], sampled 0.3643390251549685
[2019-04-04 09:28:05,980] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18659459], dtype=float32), -0.24847072]
[2019-04-04 09:28:05,980] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.645882254, 95.34063456, 0.0, 0.0, 26.0, 24.94291563570503, 0.3675471481355606, 0.0, 1.0, 43846.23535422939]
[2019-04-04 09:28:05,980] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:28:05,982] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [9.3022534e-12 1.7895303e-11 1.1311997e-24 6.4759819e-13 1.6593777e-12
 3.1884120e-15 1.0000000e+00], sampled 0.3256093842185126
[2019-04-04 09:29:45,738] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.18659459], dtype=float32), -0.24847072]
[2019-04-04 09:29:45,738] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.718374461333333, 55.59295916666667, 34.94192585, 710.2738761666667, 26.0, 26.49985911100237, 0.6442867957158581, 1.0, 1.0, 0.0]
[2019-04-04 09:29:45,738] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:29:45,739] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.9516493e-13 1.0662599e-12 2.7204361e-25 5.5156454e-14 4.2367589e-14
 2.1146019e-16 1.0000000e+00], sampled 0.03738852848500207
[2019-04-04 09:29:50,727] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.4172 239942393.0563 1605.0250
[2019-04-04 09:30:20,304] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:30:26,581] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:30:27,626] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 3300000, evaluation results [3300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.417175922201, 239942393.05634084, 1605.024971488376, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:30:51,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1961782e-13 4.0265545e-13 3.7538954e-26 1.5424221e-14 2.9150686e-14
 1.0833546e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:30:51,302] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5805
[2019-04-04 09:30:51,313] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 49.0, 111.5, 0.0, 26.0, 26.11157826484739, 0.7123247739705945, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [13.71666666666667, 49.33333333333334, 100.3333333333333, 0.0, 26.0, 26.48394611072443, 0.7601804954541151, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.8425669436749772, 0.4933333333333334, 0.3344444444444443, 0.0, 0.6666666666666666, 0.7069955092270357, 0.753393498484705, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7866261], dtype=float32), -0.49206942]. 
=============================================
[2019-04-04 09:31:10,094] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6519313e-12 9.0820884e-12 5.0875091e-24 3.0253653e-13 3.9556994e-13
 2.3684056e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:31:10,094] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8012
[2019-04-04 09:31:10,161] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.633333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.49605243267775, 0.348666740919426, 1.0, 1.0, 18715.60489475307], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1964400.0000, 
sim time next is 1965000.0000, 
raw observation next is [-4.816666666666666, 78.33333333333334, 0.0, 0.0, 26.0, 25.33151688927054, 0.3483495085572961, 1.0, 1.0, 24675.21412799677], 
processed observation next is [1.0, 0.7391304347826086, 0.32917820867959374, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.6109597407725449, 0.616116502852432, 1.0, 1.0, 0.11750101965712748], 
reward next is 0.8825, 
noisyNet noise sample is [array([-1.4011344], dtype=float32), 1.0682716]. 
=============================================
[2019-04-04 09:31:10,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.0671 ]
 [82.24176]
 [82.59513]
 [82.69204]
 [82.71722]], R is [[82.06118774]
 [82.15145874]
 [82.32994843]
 [82.50665283]
 [82.68158722]].
[2019-04-04 09:31:13,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0321313e-11 6.6110631e-11 1.7465721e-23 2.3139244e-12 2.2389202e-12
 1.1855565e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:31:13,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5511
[2019-04-04 09:31:13,294] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.8, 79.66666666666667, 0.0, 0.0, 26.0, 25.2124321304205, 0.3686245488348447, 0.0, 1.0, 43684.57236831055], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1977600.0000, 
sim time next is 1978200.0000, 
raw observation next is [-5.9, 80.5, 0.0, 0.0, 26.0, 25.17328501466623, 0.3625959104477088, 0.0, 1.0, 43570.79078625991], 
processed observation next is [1.0, 0.9130434782608695, 0.2991689750692521, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5977737512221859, 0.6208653034825696, 0.0, 1.0, 0.2074799561250472], 
reward next is 0.7925, 
noisyNet noise sample is [array([0.30369037], dtype=float32), 1.2017392]. 
=============================================
[2019-04-04 09:31:31,498] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.3186340e-12 5.8798778e-11 2.5124735e-24 8.0851715e-13 1.1009756e-12
 2.7371364e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:31:31,498] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4651
[2019-04-04 09:31:31,583] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.59915514780625, 0.09175488005604204, 1.0, 1.0, 202977.48468661], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2273400.0000, 
sim time next is 2274000.0000, 
raw observation next is [-9.5, 91.0, 9.999999999999998, 19.33333333333334, 26.0, 24.40465840529729, 0.1884000092838827, 1.0, 1.0, 152803.0860635954], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.033333333333333326, 0.021362799263351755, 0.6666666666666666, 0.5337215337747742, 0.5628000030946275, 1.0, 1.0, 0.7276337431599781], 
reward next is 0.2724, 
noisyNet noise sample is [array([0.57584274], dtype=float32), 0.0028071639]. 
=============================================
[2019-04-04 09:31:31,625] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[84.32174 ]
 [81.457466]
 [76.15766 ]
 [76.25098 ]
 [76.332085]], R is [[85.46593475]
 [84.64471436]
 [83.83450317]
 [83.79130554]
 [83.74842834]].
[2019-04-04 09:31:34,789] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.9687038e-11 1.7099880e-10 1.1712123e-22 5.7065949e-12 2.4492472e-11
 1.6914072e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:31:34,789] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0178
[2019-04-04 09:31:34,838] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.37081712083968, 0.1707564950661233, 0.0, 1.0, 42569.45129801419], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2167800.0000, 
sim time next is 2168400.0000, 
raw observation next is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49007990360808, 0.169993000209038, 0.0, 1.0, 42515.78503846233], 
processed observation next is [1.0, 0.08695652173913043, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.54083999196734, 0.5566643334030127, 0.0, 1.0, 0.202456119230773], 
reward next is 0.7975, 
noisyNet noise sample is [array([-0.8293117], dtype=float32), -0.5331264]. 
=============================================
[2019-04-04 09:31:42,566] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0148904e-11 1.1762476e-10 2.4826163e-23 6.7467382e-12 7.7712559e-12
 1.8117344e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:31:42,566] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0751
[2019-04-04 09:31:42,580] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926057366, 0.4261490673601802, 0.0, 1.0, 32336.74252528141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323800.0000, 
sim time next is 2324400.0000, 
raw observation next is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40218378438302, 0.4230242712954465, 0.0, 1.0, 51810.36194285156], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.616848648698585, 0.6410080904318155, 0.0, 1.0, 0.24671600925167408], 
reward next is 0.7533, 
noisyNet noise sample is [array([-0.7334841], dtype=float32), 0.48425058]. 
=============================================
[2019-04-04 09:31:44,463] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.4942171e-11 1.3606923e-10 2.7806422e-23 8.2490455e-12 8.5487077e-12
 1.9237096e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:31:44,463] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9673
[2019-04-04 09:31:44,484] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 65.0, 0.0, 0.0, 26.0, 24.12885982520534, 0.1025527686571428, 0.0, 1.0, 41193.98300876558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2354400.0000, 
sim time next is 2355000.0000, 
raw observation next is [-2.9, 65.66666666666667, 0.0, 0.0, 26.0, 24.13852170005622, 0.1116471860467236, 0.0, 1.0, 41189.56669257415], 
processed observation next is [0.0, 0.2608695652173913, 0.38227146814404434, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.511543475004685, 0.5372157286822412, 0.0, 1.0, 0.19614079377416263], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.3998509], dtype=float32), -0.072097845]. 
=============================================
[2019-04-04 09:31:44,508] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.550385]
 [77.51415 ]
 [77.49174 ]
 [77.48448 ]
 [77.49704 ]], R is [[77.61553955]
 [77.64322662]
 [77.67073059]
 [77.69812775]
 [77.7254715 ]].
[2019-04-04 09:31:49,886] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6583576e-11 2.0247899e-10 1.3726748e-22 7.7904306e-12 6.9346902e-12
 2.8810962e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:31:49,886] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3340
[2019-04-04 09:31:49,906] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15746206337576, 0.286818246293717, 0.0, 1.0, 43326.67371916811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406000.0000, 
sim time next is 2406600.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15863164123137, 0.2810747596082467, 0.0, 1.0, 43113.74704714974], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5965526367692808, 0.5936915865360822, 0.0, 1.0, 0.20530355736737974], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.454681], dtype=float32), -0.18943116]. 
=============================================
[2019-04-04 09:31:56,360] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.03004126e-10 2.94852531e-10 2.40848417e-23 9.61483497e-12
 9.46023399e-11 3.59213967e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 09:31:56,360] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8157
[2019-04-04 09:31:56,388] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 63.0, 0.0, 0.0, 26.0, 24.53399347836289, 0.1920901211838017, 0.0, 1.0, 39755.88521956332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2344800.0000, 
sim time next is 2345400.0000, 
raw observation next is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.52150073818959, 0.1860453205060347, 0.0, 1.0, 39862.47935494516], 
processed observation next is [0.0, 0.13043478260869565, 0.3919667590027701, 0.635, 0.0, 0.0, 0.6666666666666666, 0.5434583948491326, 0.5620151068353448, 0.0, 1.0, 0.1898213302616436], 
reward next is 0.8102, 
noisyNet noise sample is [array([-2.0502446], dtype=float32), -0.24244392]. 
=============================================
[2019-04-04 09:32:04,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2750414e-12 2.7174359e-12 3.4992174e-25 1.6708355e-13 1.2609290e-13
 1.0140171e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:32:04,335] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5546
[2019-04-04 09:32:04,378] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 25.17282556520242, 0.3556701130240427, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2571000.0000, 
sim time next is 2571600.0000, 
raw observation next is [0.1333333333333334, 35.33333333333334, 0.0, 0.0, 26.0, 25.22870236015867, 0.3476266848262554, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.46629732225300097, 0.35333333333333344, 0.0, 0.0, 0.6666666666666666, 0.6023918633465559, 0.6158755616087518, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.94672066], dtype=float32), -1.1191763]. 
=============================================
[2019-04-04 09:32:10,587] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.7134550e-11 5.9044702e-10 1.6834098e-21 2.9670395e-11 3.1806439e-11
 7.7227822e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:32:10,587] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8055
[2019-04-04 09:32:10,612] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.25, 84.33333333333334, 0.0, 0.0, 26.0, 23.81829279778278, 0.05585433590073271, 0.0, 1.0, 44515.67640616995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2689800.0000, 
sim time next is 2690400.0000, 
raw observation next is [-13.6, 85.66666666666667, 0.0, 0.0, 26.0, 23.75307257451133, 0.04367901029084734, 0.0, 1.0, 44524.70975901533], 
processed observation next is [1.0, 0.13043478260869565, 0.08587257617728532, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.4794227145426107, 0.5145596700969491, 0.0, 1.0, 0.21202242742388253], 
reward next is 0.7880, 
noisyNet noise sample is [array([-0.41944358], dtype=float32), 0.11431874]. 
=============================================
[2019-04-04 09:32:11,334] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9949646e-11 2.9569718e-11 3.5455349e-24 1.6008472e-12 9.4863342e-13
 2.2097029e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:32:11,337] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4971
[2019-04-04 09:32:11,407] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 54.00000000000001, 0.0, 0.0, 26.0, 24.72557190680874, 0.1760451754177989, 1.0, 1.0, 88502.44952902241], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2532000.0000, 
sim time next is 2532600.0000, 
raw observation next is [-2.8, 54.0, 0.0, 0.0, 26.0, 25.01462187021848, 0.2109959151764791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.38504155124653744, 0.54, 0.0, 0.0, 0.6666666666666666, 0.5845518225182067, 0.570331971725493, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2723877], dtype=float32), -0.32145694]. 
=============================================
[2019-04-04 09:32:14,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.17872476e-10 3.53488461e-10 1.22112508e-21 1.19508925e-11
 8.78441330e-12 8.83319568e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 09:32:14,285] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8180
[2019-04-04 09:32:14,425] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.69845057496094, -0.2064646904015345, 0.0, 1.0, 43299.03822464857], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2704200.0000, 
sim time next is 2704800.0000, 
raw observation next is [-15.0, 83.0, 0.0, 0.0, 26.0, 22.65827632632871, -0.1114806781234396, 0.0, 1.0, 202379.7707628406], 
processed observation next is [1.0, 0.30434782608695654, 0.04709141274238226, 0.83, 0.0, 0.0, 0.6666666666666666, 0.3881896938607259, 0.46283977395885345, 0.0, 1.0, 0.9637131941087648], 
reward next is 0.0363, 
noisyNet noise sample is [array([0.86014146], dtype=float32), -0.31581396]. 
=============================================
[2019-04-04 09:32:25,648] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3487371e-12 6.0963706e-11 1.4781349e-23 1.2909820e-12 1.5144899e-12
 1.0937410e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:32:25,648] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8053
[2019-04-04 09:32:25,671] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 67.0, 0.0, 0.0, 26.0, 25.22468865733192, 0.3498030718973957, 0.0, 1.0, 42724.76100815783], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2849400.0000, 
sim time next is 2850000.0000, 
raw observation next is [1.333333333333333, 68.66666666666667, 0.0, 0.0, 26.0, 25.21645676288252, 0.3464976608157298, 0.0, 1.0, 42421.50907439947], 
processed observation next is [1.0, 1.0, 0.4995383194829178, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6013713969068766, 0.61549922027191, 0.0, 1.0, 0.20200718606856893], 
reward next is 0.7980, 
noisyNet noise sample is [array([-0.8099148], dtype=float32), 0.18795997]. 
=============================================
[2019-04-04 09:32:25,675] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.43659 ]
 [80.54941 ]
 [80.584465]
 [80.57044 ]
 [80.505455]], R is [[80.42868042]
 [80.42094421]
 [80.41123199]
 [80.39788818]
 [80.37572479]].
[2019-04-04 09:32:28,326] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1713149e-11 1.5351515e-11 2.0842330e-24 5.8474829e-13 2.9643679e-12
 5.9218242e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:32:28,327] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9261
[2019-04-04 09:32:28,347] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 100.0, 0.0, 0.0, 26.0, 24.87903418373469, 0.2363941372649189, 0.0, 1.0, 55413.54071604238], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2872800.0000, 
sim time next is 2873400.0000, 
raw observation next is [1.166666666666667, 98.83333333333334, 0.0, 0.0, 26.0, 24.82343801853745, 0.2270150480086178, 0.0, 1.0, 55439.05687490878], 
processed observation next is [1.0, 0.2608695652173913, 0.49492151431209613, 0.9883333333333334, 0.0, 0.0, 0.6666666666666666, 0.5686198348781207, 0.5756716826695393, 0.0, 1.0, 0.26399550892813706], 
reward next is 0.7360, 
noisyNet noise sample is [array([0.21138127], dtype=float32), 0.19137128]. 
=============================================
[2019-04-04 09:32:45,258] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.23163586e-14 8.80406316e-14 4.84795779e-27 1.95346613e-15
 1.39682035e-14 2.05549982e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 09:32:45,261] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7198
[2019-04-04 09:32:45,285] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.833333333333334, 94.16666666666666, 113.6666666666667, 811.0, 26.0, 27.16585768630426, 0.8023743668491043, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3153000.0000, 
sim time next is 3153600.0000, 
raw observation next is [8.0, 93.0, 113.5, 814.0, 26.0, 27.22060185370187, 0.8141174447292379, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6842105263157896, 0.93, 0.37833333333333335, 0.8994475138121547, 0.6666666666666666, 0.7683834878084891, 0.7713724815764126, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29236218], dtype=float32), -0.13151133]. 
=============================================
[2019-04-04 09:32:49,426] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1025587e-14 2.5472585e-13 2.5165845e-27 4.0908019e-15 7.6883580e-15
 4.9249072e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:32:49,427] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6748
[2019-04-04 09:32:49,456] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.166666666666667, 98.83333333333334, 112.0, 785.3333333333334, 26.0, 27.07482250864806, 0.7569433004677717, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3150600.0000, 
sim time next is 3151200.0000, 
raw observation next is [7.333333333333334, 97.66666666666667, 113.0, 795.1666666666666, 26.0, 27.11696250937133, 0.7602889307106514, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.6657433056325024, 0.9766666666666667, 0.37666666666666665, 0.8786372007366482, 0.6666666666666666, 0.7597468757809441, 0.7534296435702171, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.38485897], dtype=float32), -0.7901332]. 
=============================================
[2019-04-04 09:32:55,529] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.8519903e-13 1.0977052e-11 3.5135983e-24 6.8734765e-14 1.1967121e-13
 2.5949593e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:32:55,529] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1675
[2019-04-04 09:32:55,556] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 57.33333333333334, 116.3333333333333, 800.1666666666666, 26.0, 26.36966099388933, 0.6036673320237477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3325200.0000, 
sim time next is 3325800.0000, 
raw observation next is [-6.166666666666666, 55.66666666666666, 116.6666666666667, 802.3333333333334, 26.0, 26.37188237478832, 0.6031385605387916, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.29178208679593726, 0.5566666666666665, 0.388888888888889, 0.8865561694290977, 0.6666666666666666, 0.6976568645656934, 0.701046186846264, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.75222397], dtype=float32), -2.33408]. 
=============================================
[2019-04-04 09:33:06,316] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3606692e-13 2.4414008e-12 2.9798696e-25 8.8670404e-14 6.3038482e-14
 1.1984234e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:06,342] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0807
[2019-04-04 09:33:06,385] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.166666666666667, 65.5, 76.33333333333334, 640.3333333333334, 26.0, 26.70273931924915, 0.5656510575798792, 1.0, 1.0, 3736.082334046107], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3426600.0000, 
sim time next is 3427200.0000, 
raw observation next is [2.0, 67.0, 72.5, 608.5, 26.0, 26.08035787447303, 0.5870011981383253, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.24166666666666667, 0.6723756906077348, 0.6666666666666666, 0.6733631562060859, 0.6956670660461084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.60418075], dtype=float32), 0.19458]. 
=============================================
[2019-04-04 09:33:13,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5827281e-12 2.5067460e-11 1.6861572e-25 4.1389640e-13 6.5151693e-13
 5.2565594e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:13,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1966
[2019-04-04 09:33:13,936] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 70.0, 90.0, 466.8333333333333, 26.0, 25.33831806462065, 0.4853902807997727, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3573600.0000, 
sim time next is 3574200.0000, 
raw observation next is [-6.166666666666666, 70.0, 92.0, 508.6666666666666, 26.0, 25.73467235693603, 0.5157287852537477, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.29178208679593726, 0.7, 0.30666666666666664, 0.5620626151012891, 0.6666666666666666, 0.6445560297446692, 0.6719095950845825, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.25328016], dtype=float32), -0.47462633]. 
=============================================
[2019-04-04 09:33:24,844] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.846768e-12 5.337407e-11 6.277779e-24 9.142881e-13 7.243272e-13
 4.637697e-15 1.000000e+00], sum to 1.0000
[2019-04-04 09:33:24,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1187
[2019-04-04 09:33:24,864] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.06428128864625, 0.3404368835370539, 0.0, 1.0, 43785.76177811476], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3813600.0000, 
sim time next is 3814200.0000, 
raw observation next is [-4.0, 71.0, 0.0, 0.0, 26.0, 25.09924300324538, 0.3343548208723637, 0.0, 1.0, 43622.84047238018], 
processed observation next is [1.0, 0.13043478260869565, 0.3518005540166205, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5916035836037817, 0.6114516069574546, 0.0, 1.0, 0.20772781177323896], 
reward next is 0.7923, 
noisyNet noise sample is [array([0.12550843], dtype=float32), 1.2750691]. 
=============================================
[2019-04-04 09:33:30,482] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.8165796e-14 7.9235441e-13 2.1668863e-25 5.8721806e-14 4.0772964e-14
 2.5842607e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:30,484] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2368
[2019-04-04 09:33:30,506] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 34.0, 77.0, 630.0, 26.0, 27.03684445193544, 0.7973685899329993, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3945600.0000, 
sim time next is 3946200.0000, 
raw observation next is [-4.166666666666667, 34.66666666666667, 73.33333333333334, 598.6666666666666, 26.0, 27.11364420785407, 0.8041768978943796, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3471837488457987, 0.34666666666666673, 0.24444444444444446, 0.6615101289134437, 0.6666666666666666, 0.7594703506545057, 0.7680589659647933, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5357184], dtype=float32), 1.3641351]. 
=============================================
[2019-04-04 09:33:34,918] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.44638355e-11 3.50944204e-11 2.33661910e-23 1.95866999e-12
 2.78131034e-12 1.53411880e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 09:33:34,922] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2041
[2019-04-04 09:33:34,952] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.27247470144449, 0.4007292073451531, 0.0, 1.0, 43960.12926067881], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3804600.0000, 
sim time next is 3805200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.24115768370186, 0.396162727143249, 0.0, 1.0, 44074.09710743227], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.603429806975155, 0.632054242381083, 0.0, 1.0, 0.20987665289253463], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.8280389], dtype=float32), -1.1956439]. 
=============================================
[2019-04-04 09:33:36,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.4820915e-11 3.1059350e-10 4.6628702e-22 4.7639115e-12 1.3863225e-11
 1.0091290e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:36,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2803
[2019-04-04 09:33:36,082] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.31538781434478, 0.1675479142193624, 0.0, 1.0, 43761.52558020704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3984000.0000, 
sim time next is 3984600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.26272556757615, 0.1657580356239948, 0.0, 1.0, 43748.98248055012], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5218937972980126, 0.5552526785413315, 0.0, 1.0, 0.2083284880026196], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.37306935], dtype=float32), -0.09789573]. 
=============================================
[2019-04-04 09:33:38,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3357750e-14 4.0546611e-13 7.2470071e-27 2.2984388e-14 4.4664822e-15
 1.3034885e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:38,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3860
[2019-04-04 09:33:38,989] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333333, 29.33333333333333, 120.8333333333333, 820.1666666666666, 26.0, 26.73685844683334, 0.6360009412425386, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [0.6666666666666667, 28.66666666666667, 120.6666666666667, 824.3333333333334, 26.0, 26.29944083894983, 0.621438890441193, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4810710987996307, 0.28666666666666674, 0.4022222222222223, 0.910865561694291, 0.6666666666666666, 0.6916200699124859, 0.7071462968137311, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6760701], dtype=float32), -0.7425904]. 
=============================================
[2019-04-04 09:33:40,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0106573e-11 9.7933925e-11 2.8815666e-23 6.3838466e-12 5.6930164e-12
 6.1528626e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:40,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0119
[2019-04-04 09:33:40,700] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 40.33333333333334, 0.0, 0.0, 26.0, 25.04164398116169, 0.3322170530138444, 0.0, 1.0, 40714.13537736724], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4063800.0000, 
sim time next is 4064400.0000, 
raw observation next is [-6.0, 41.0, 0.0, 0.0, 26.0, 25.05680472244678, 0.3277396060960582, 0.0, 1.0, 40752.43876395369], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5880670602038984, 0.6092465353653528, 0.0, 1.0, 0.19405923220930327], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.23595732], dtype=float32), -0.46307778]. 
=============================================
[2019-04-04 09:33:53,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4612612e-13 1.1990843e-12 2.1739808e-26 6.0429113e-14 3.7799852e-14
 9.2094756e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:53,406] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2535
[2019-04-04 09:33:53,430] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 33.0, 115.1666666666667, 776.8333333333334, 26.0, 26.61289020800783, 0.6103998119332888, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4099200.0000, 
sim time next is 4099800.0000, 
raw observation next is [-1.166666666666667, 32.5, 116.3333333333333, 784.6666666666667, 26.0, 26.64697701860292, 0.6214571435686085, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43028624192059095, 0.325, 0.38777777777777767, 0.8670349907918969, 0.6666666666666666, 0.7205814182169101, 0.7071523811895362, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.23249672], dtype=float32), 0.9414366]. 
=============================================
[2019-04-04 09:33:59,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1438398e-13 2.0503979e-13 2.3933228e-26 8.7343793e-15 1.8527184e-14
 3.6251309e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:59,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5337
[2019-04-04 09:33:59,185] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666666, 57.33333333333334, 134.8333333333333, 724.0, 26.0, 26.55359774650552, 0.6613902107120356, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4616400.0000, 
sim time next is 4617000.0000, 
raw observation next is [1.0, 56.0, 129.0, 767.0, 26.0, 26.64746032834829, 0.6701905047337519, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.56, 0.43, 0.8475138121546961, 0.6666666666666666, 0.7206216940290243, 0.7233968349112506, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.260695], dtype=float32), -0.87977344]. 
=============================================
[2019-04-04 09:33:59,202] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[95.14784 ]
 [95.019875]
 [95.00116 ]
 [94.98201 ]
 [95.030235]], R is [[95.35163116]
 [95.39811707]
 [95.44413757]
 [95.48970032]
 [95.5348053 ]].
[2019-04-04 09:33:59,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1829465e-12 9.4142038e-12 8.8255687e-25 2.0629508e-13 3.3618361e-13
 3.8607025e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:33:59,353] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3529
[2019-04-04 09:33:59,367] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 72.0, 0.0, 0.0, 26.0, 25.52828761628171, 0.516471499148414, 0.0, 1.0, 50350.55355044808], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4429200.0000, 
sim time next is 4429800.0000, 
raw observation next is [2.5, 74.0, 0.0, 0.0, 26.0, 25.50549092837258, 0.5215598352460594, 0.0, 1.0, 52245.96490971497], 
processed observation next is [1.0, 0.2608695652173913, 0.5318559556786704, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6254575773643817, 0.6738532784153531, 0.0, 1.0, 0.2487903090938808], 
reward next is 0.7512, 
noisyNet noise sample is [array([0.59577554], dtype=float32), 0.52816427]. 
=============================================
[2019-04-04 09:34:06,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5405292e-14 9.9216784e-14 5.0174042e-27 6.4873491e-15 9.1657452e-15
 6.8833024e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:06,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0502
[2019-04-04 09:34:06,960] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.1, 31.66666666666666, 179.6666666666667, 524.1666666666667, 26.0, 28.31254101219794, 1.111077629995501, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4372800.0000, 
sim time next is 4373400.0000, 
raw observation next is [14.0, 31.83333333333333, 164.3333333333334, 419.3333333333334, 26.0, 27.58791225454176, 1.054772045165219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8504155124653741, 0.3183333333333333, 0.547777777777778, 0.46335174953959496, 0.6666666666666666, 0.79899268787848, 0.8515906817217397, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4941214], dtype=float32), 1.9462984]. 
=============================================
[2019-04-04 09:34:09,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.6389519e-13 1.8456239e-12 4.5517038e-25 6.5981915e-14 3.6114717e-14
 3.4133787e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:09,857] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5951
[2019-04-04 09:34:09,881] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 140.5, 3.0, 26.0, 26.17012067416086, 0.6218168057309964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4456800.0000, 
sim time next is 4457400.0000, 
raw observation next is [0.0, 90.83333333333334, 122.0, 2.0, 26.0, 26.24447408160266, 0.6246954704201436, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46260387811634357, 0.9083333333333334, 0.4066666666666667, 0.0022099447513812156, 0.6666666666666666, 0.6870395068002217, 0.7082318234733812, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.13586093], dtype=float32), -0.03967158]. 
=============================================
[2019-04-04 09:34:10,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9433361e-11 1.2995882e-10 3.0216995e-23 2.9981213e-12 3.1287876e-12
 7.7082573e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:10,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9730
[2019-04-04 09:34:10,670] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 58.33333333333334, 0.0, 0.0, 26.0, 25.40676067366283, 0.3765715641736455, 0.0, 1.0, 40997.40730147939], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4837200.0000, 
sim time next is 4837800.0000, 
raw observation next is [-1.833333333333333, 59.16666666666666, 0.0, 0.0, 26.0, 25.38710764350263, 0.383683233206179, 0.0, 1.0, 50627.1191802789], 
processed observation next is [0.0, 1.0, 0.41181902123730385, 0.5916666666666666, 0.0, 0.0, 0.6666666666666666, 0.6155923036252192, 0.6278944110687263, 0.0, 1.0, 0.24108151990609003], 
reward next is 0.7589, 
noisyNet noise sample is [array([-0.49889016], dtype=float32), 1.3993073]. 
=============================================
[2019-04-04 09:34:11,587] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6745099e-12 2.5801104e-11 1.5188960e-23 4.3106604e-13 2.1511165e-12
 7.4808349e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:11,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7112
[2019-04-04 09:34:11,627] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.14371949618502, 0.423772834711716, 0.0, 1.0, 26466.84550367529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4737000.0000, 
sim time next is 4737600.0000, 
raw observation next is [-1.0, 78.0, 0.0, 0.0, 26.0, 25.04108275064648, 0.4173758454975736, 0.0, 1.0, 82735.61278444852], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5867568958872068, 0.6391252818325245, 0.0, 1.0, 0.3939791084973739], 
reward next is 0.6060, 
noisyNet noise sample is [array([-0.21924981], dtype=float32), -0.78274226]. 
=============================================
[2019-04-04 09:34:14,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1670267e-14 1.2296329e-13 1.9521133e-27 6.5879516e-15 3.6214165e-15
 1.3308972e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:14,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9325
[2019-04-04 09:34:14,743] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 50.5, 122.0, 833.0, 26.0, 26.54399543585851, 0.4739409851136653, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4620600.0000, 
sim time next is 4621200.0000, 
raw observation next is [2.666666666666667, 50.0, 121.6666666666667, 837.3333333333334, 26.0, 26.68503846367531, 0.7140385586662207, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5364727608494922, 0.5, 0.40555555555555567, 0.9252302025782689, 0.6666666666666666, 0.7237532053062757, 0.7380128528887403, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8410621], dtype=float32), -0.6149597]. 
=============================================
[2019-04-04 09:34:18,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4301389e-10 1.7697556e-10 8.1977416e-23 1.8890034e-11 6.9322076e-11
 4.4559330e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:18,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7030
[2019-04-04 09:34:18,941] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.39948879760225, 0.2106869786212927, 0.0, 1.0, 41192.67382498774], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4769400.0000, 
sim time next is 4770000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.35754877895628, 0.2002646779981427, 0.0, 1.0, 41246.05906660381], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5297957315796898, 0.5667548926660476, 0.0, 1.0, 0.19640980507906577], 
reward next is 0.8036, 
noisyNet noise sample is [array([-0.7470317], dtype=float32), -1.0327008]. 
=============================================
[2019-04-04 09:34:18,948] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.5484  ]
 [78.50569 ]
 [78.46826 ]
 [78.4698  ]
 [78.498184]], R is [[78.59694672]
 [78.61482239]
 [78.63269043]
 [78.65052032]
 [78.66842651]].
[2019-04-04 09:34:23,569] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8629943e-12 7.8550812e-12 6.3036208e-25 1.7872601e-13 4.3591110e-13
 8.0529312e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:23,573] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5724
[2019-04-04 09:34:23,650] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 97.33333333333334, 0.0, 0.0, 26.0, 25.3975707502239, 0.4645554250586275, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4692000.0000, 
sim time next is 4692600.0000, 
raw observation next is [-0.5, 96.0, 0.0, 0.0, 26.0, 25.69198043906281, 0.4748508865845062, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.44875346260387816, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6409983699219008, 0.658283628861502, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.38021177], dtype=float32), 1.404521]. 
=============================================
[2019-04-04 09:34:24,426] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4195231e-11 2.9443553e-10 2.1170155e-23 4.5442998e-12 5.6107979e-12
 2.3258869e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:24,426] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3150
[2019-04-04 09:34:24,442] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.63286602953966, 0.5740545590701878, 0.0, 1.0, 49659.37578964593], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5001600.0000, 
sim time next is 5002200.0000, 
raw observation next is [3.5, 33.0, 0.0, 0.0, 26.0, 25.69370977139023, 0.5792816883454509, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.5595567867036012, 0.33, 0.0, 0.0, 0.6666666666666666, 0.6411424809491857, 0.6930938961151503, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.47716543], dtype=float32), 0.1856294]. 
=============================================
[2019-04-04 09:34:26,302] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.6674689e-11 2.8525318e-10 1.3839475e-22 1.3183320e-11 1.6991995e-11
 1.1494562e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:26,302] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7936
[2019-04-04 09:34:26,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:26,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:26,342] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run26
[2019-04-04 09:34:26,386] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.02455404340505, 0.2279256812161097, 0.0, 1.0, 38654.98369697621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4948200.0000, 
sim time next is 4948800.0000, 
raw observation next is [-3.0, 50.0, 0.0, 0.0, 26.0, 25.03037806962126, 0.2197346755060496, 0.0, 1.0, 38671.46466260744], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.5, 0.0, 0.0, 0.6666666666666666, 0.585864839135105, 0.5732448918353499, 0.0, 1.0, 0.1841498317267021], 
reward next is 0.8159, 
noisyNet noise sample is [array([0.2062249], dtype=float32), -0.47605377]. 
=============================================
[2019-04-04 09:34:27,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:27,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:27,284] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run26
[2019-04-04 09:34:28,996] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.1613800e-14 4.3491372e-13 4.5314836e-26 1.9412396e-14 6.6660405e-15
 4.0790016e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:28,998] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2542
[2019-04-04 09:34:29,055] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.0, 26.0, 100.5, 796.5, 26.0, 27.76036828963077, 0.7525818316127547, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4978800.0000, 
sim time next is 4979400.0000, 
raw observation next is [8.166666666666668, 25.83333333333334, 97.66666666666667, 789.0, 26.0, 27.07099334339733, 0.8037840228149921, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.6888273314866113, 0.2583333333333334, 0.3255555555555556, 0.8718232044198895, 0.6666666666666666, 0.7559161119497775, 0.7679280076049974, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68907654], dtype=float32), 0.64504856]. 
=============================================
[2019-04-04 09:34:33,136] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:33,161] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:33,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run26
[2019-04-04 09:34:33,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0792795e-11 1.7537263e-11 5.4422165e-24 2.2075705e-12 5.9569033e-13
 8.0180215e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:33,401] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3564
[2019-04-04 09:34:33,517] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9333333333333333, 54.66666666666667, 297.1666666666666, 188.0, 26.0, 25.07815054317049, 0.3215591201562711, 0.0, 1.0, 18713.40524772888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4876800.0000, 
sim time next is 4877400.0000, 
raw observation next is [-0.6666666666666666, 53.33333333333334, 294.3333333333334, 212.0, 26.0, 25.02327432260908, 0.3223232449470932, 0.0, 1.0, 33690.04098392948], 
processed observation next is [0.0, 0.43478260869565216, 0.44413665743305636, 0.5333333333333334, 0.9811111111111114, 0.23425414364640884, 0.6666666666666666, 0.5852728602174233, 0.6074410816490311, 0.0, 1.0, 0.16042876659014038], 
reward next is 0.8396, 
noisyNet noise sample is [array([-0.26730752], dtype=float32), -0.39684433]. 
=============================================
[2019-04-04 09:34:36,507] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:36,508] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:36,514] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run26
[2019-04-04 09:34:39,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:39,013] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:39,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run26
[2019-04-04 09:34:39,992] A3C_AGENT_WORKER-Thread-12 INFO:Local step 212500, global step 3396719: loss 0.0981
[2019-04-04 09:34:39,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 212500, global step 3396719: learning rate 0.0000
[2019-04-04 09:34:40,826] A3C_AGENT_WORKER-Thread-2 INFO:Local step 212500, global step 3396947: loss 0.1018
[2019-04-04 09:34:40,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 212500, global step 3396947: learning rate 0.0000
[2019-04-04 09:34:43,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:43,011] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:43,016] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run26
[2019-04-04 09:34:43,346] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:43,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:43,367] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run26
[2019-04-04 09:34:43,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:43,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:43,589] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run26
[2019-04-04 09:34:43,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:43,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:43,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run26
[2019-04-04 09:34:44,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:44,916] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:44,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run26
[2019-04-04 09:34:45,484] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4685491e-11 1.3163527e-10 1.7667220e-23 3.4956649e-12 4.4343687e-12
 3.8202992e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:34:45,485] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4669
[2019-04-04 09:34:45,504] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 51.66666666666666, 0.0, 0.0, 26.0, 25.40701204364291, 0.3933680615606672, 0.0, 1.0, 28384.51467155455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5028000.0000, 
sim time next is 5028600.0000, 
raw observation next is [-1.0, 50.83333333333334, 0.0, 0.0, 26.0, 25.40576046550209, 0.3879413481717256, 0.0, 1.0, 33805.45743529902], 
processed observation next is [1.0, 0.17391304347826086, 0.4349030470914128, 0.5083333333333334, 0.0, 0.0, 0.6666666666666666, 0.6171467054585076, 0.6293137827239085, 0.0, 1.0, 0.16097836873951912], 
reward next is 0.8390, 
noisyNet noise sample is [array([-0.3460796], dtype=float32), 0.4577207]. 
=============================================
[2019-04-04 09:34:45,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:45,893] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:45,900] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run26
[2019-04-04 09:34:46,527] A3C_AGENT_WORKER-Thread-5 INFO:Local step 212500, global step 3398237: loss 0.0956
[2019-04-04 09:34:46,527] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 212500, global step 3398237: learning rate 0.0000
[2019-04-04 09:34:48,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:48,088] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:48,091] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run26
[2019-04-04 09:34:50,533] A3C_AGENT_WORKER-Thread-18 INFO:Local step 212500, global step 3398849: loss 0.0757
[2019-04-04 09:34:50,535] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 212500, global step 3398849: learning rate 0.0000
[2019-04-04 09:34:52,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:34:52,330] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:52,338] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run26
[2019-04-04 09:34:52,973] A3C_AGENT_WORKER-Thread-6 INFO:Local step 212500, global step 3399341: loss 0.0779
[2019-04-04 09:34:52,990] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 212500, global step 3399341: learning rate 0.0000
[2019-04-04 09:34:56,046] A3C_AGENT_WORKER-Thread-6 INFO:Evaluating...
[2019-04-04 09:34:56,061] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:34:56,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:56,070] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run35
[2019-04-04 09:34:56,105] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:34:56,109] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:56,110] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:34:56,110] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:34:56,113] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run35
[2019-04-04 09:34:56,148] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run35
[2019-04-04 09:35:39,683] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18330972], dtype=float32), -0.25358236]
[2019-04-04 09:35:39,683] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-2.8, 83.66666666666667, 0.0, 0.0, 26.0, 25.0987207397736, 0.2927237319419615, 0.0, 1.0, 42941.76721682448]
[2019-04-04 09:35:39,683] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:35:39,684] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [9.3481247e-12 1.5106508e-11 6.6609766e-24 8.4840381e-13 1.4738613e-12
 4.8565392e-15 1.0000000e+00], sampled 0.24167750542446687
[2019-04-04 09:36:18,532] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.18330972], dtype=float32), -0.25358236]
[2019-04-04 09:36:18,532] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [4.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 25.48154033150686, 0.4936896145746226, 0.0, 1.0, 67739.4122642359]
[2019-04-04 09:36:18,532] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:36:18,545] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.4522879e-12 9.3831548e-12 7.9839804e-25 4.0202387e-13 7.9360975e-13
 1.4800425e-15 1.0000000e+00], sampled 0.8653060727735379
[2019-04-04 09:38:10,167] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.5219 239920394.3441 1605.1240
[2019-04-04 09:38:41,889] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:38:46,715] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:38:47,752] A3C_AGENT_WORKER-Thread-6 INFO:Global step: 3400000, evaluation results [3400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.521931694581, 239920394.3441402, 1605.1240493772043, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:38:49,457] A3C_AGENT_WORKER-Thread-20 INFO:Local step 212500, global step 3400288: loss 0.0633
[2019-04-04 09:38:49,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 212500, global step 3400288: learning rate 0.0000
[2019-04-04 09:38:49,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.6449653e-11 2.2323204e-11 1.8234845e-24 2.4180395e-12 2.0675252e-12
 1.5231385e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:38:49,734] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9041
[2019-04-04 09:38:49,747] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 0.0, 0.0, 26.0, 20.88710572992931, -0.6666360283300962, 0.0, 1.0, 41240.80741395101], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 14400.0000, 
sim time next is 15000.0000, 
raw observation next is [7.7, 93.0, 0.0, 0.0, 26.0, 20.90241927339964, -0.654927190245909, 0.0, 1.0, 41131.68019074238], 
processed observation next is [0.0, 0.17391304347826086, 0.6759002770083103, 0.93, 0.0, 0.0, 0.6666666666666666, 0.2418682727833034, 0.281690936584697, 0.0, 1.0, 0.1958651437654399], 
reward next is 0.8041, 
noisyNet noise sample is [array([0.78248554], dtype=float32), -1.2139534]. 
=============================================
[2019-04-04 09:38:49,750] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.55382]
 [81.6701 ]
 [81.50544]
 [80.68985]
 [79.2033 ]], R is [[81.49642944]
 [81.4850769 ]
 [81.47331238]
 [81.46102142]
 [81.44811249]].
[2019-04-04 09:38:49,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:38:49,769] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:38:49,777] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run26
[2019-04-04 09:38:49,969] A3C_AGENT_WORKER-Thread-15 INFO:Local step 212500, global step 3400366: loss 0.0596
[2019-04-04 09:38:49,970] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 212500, global step 3400366: learning rate 0.0000
[2019-04-04 09:38:50,186] A3C_AGENT_WORKER-Thread-3 INFO:Local step 212500, global step 3400403: loss 0.0574
[2019-04-04 09:38:50,186] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 212500, global step 3400403: learning rate 0.0000
[2019-04-04 09:38:50,297] A3C_AGENT_WORKER-Thread-19 INFO:Local step 212500, global step 3400423: loss 0.0524
[2019-04-04 09:38:50,298] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 212500, global step 3400423: learning rate 0.0000
[2019-04-04 09:38:50,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:38:50,403] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:38:50,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run26
[2019-04-04 09:38:52,082] A3C_AGENT_WORKER-Thread-16 INFO:Local step 212500, global step 3400696: loss 0.0399
[2019-04-04 09:38:52,083] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 212500, global step 3400696: learning rate 0.0000
[2019-04-04 09:38:52,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:38:52,501] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:38:52,504] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run26
[2019-04-04 09:38:53,387] A3C_AGENT_WORKER-Thread-17 INFO:Local step 212500, global step 3400865: loss 0.0379
[2019-04-04 09:38:53,390] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 212500, global step 3400865: learning rate 0.0000
[2019-04-04 09:38:55,800] A3C_AGENT_WORKER-Thread-4 INFO:Local step 212500, global step 3401284: loss 0.0270
[2019-04-04 09:38:55,800] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 212500, global step 3401284: learning rate 0.0000
[2019-04-04 09:38:57,732] A3C_AGENT_WORKER-Thread-11 INFO:Local step 212500, global step 3401574: loss 0.0208
[2019-04-04 09:38:57,732] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 212500, global step 3401574: learning rate 0.0000
[2019-04-04 09:39:06,411] A3C_AGENT_WORKER-Thread-14 INFO:Local step 212500, global step 3403169: loss 0.0201
[2019-04-04 09:39:06,427] A3C_AGENT_WORKER-Thread-13 INFO:Local step 212500, global step 3403172: loss 0.0186
[2019-04-04 09:39:06,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 212500, global step 3403172: learning rate 0.0000
[2019-04-04 09:39:06,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 212500, global step 3403169: learning rate 0.0000
[2019-04-04 09:39:08,390] A3C_AGENT_WORKER-Thread-10 INFO:Local step 212500, global step 3403571: loss 0.0234
[2019-04-04 09:39:08,391] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 212500, global step 3403571: learning rate 0.0000
[2019-04-04 09:39:08,790] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213000, global step 3403640: loss 0.0036
[2019-04-04 09:39:08,792] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213000, global step 3403640: learning rate 0.0000
[2019-04-04 09:39:11,358] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213000, global step 3404162: loss 0.0044
[2019-04-04 09:39:11,359] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213000, global step 3404162: learning rate 0.0000
[2019-04-04 09:39:13,284] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7992180e-11 2.5922908e-10 1.7252045e-22 2.8968364e-12 7.7029598e-12
 2.1916687e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:13,284] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5985
[2019-04-04 09:39:13,341] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 78.16666666666667, 0.0, 0.0, 26.0, 23.89690736440135, 0.05375498451688789, 0.0, 1.0, 43664.32616177559], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 101400.0000, 
sim time next is 102000.0000, 
raw observation next is [-3.933333333333334, 77.33333333333334, 0.0, 0.0, 26.0, 23.94466671033408, 0.04859233642720131, 0.0, 1.0, 43700.06597198677], 
processed observation next is [1.0, 0.17391304347826086, 0.3536472760849492, 0.7733333333333334, 0.0, 0.0, 0.6666666666666666, 0.49538889252783996, 0.5161974454757338, 0.0, 1.0, 0.20809555224755605], 
reward next is 0.7919, 
noisyNet noise sample is [array([-0.6088384], dtype=float32), 0.94113725]. 
=============================================
[2019-04-04 09:39:13,364] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[76.15418 ]
 [76.262764]
 [76.35712 ]
 [76.46514 ]
 [76.5835  ]], R is [[76.08370972]
 [76.11495209]
 [76.14612579]
 [76.17731476]
 [76.20862579]].
[2019-04-04 09:39:17,295] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213000, global step 3405214: loss 0.0030
[2019-04-04 09:39:17,317] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213000, global step 3405214: learning rate 0.0000
[2019-04-04 09:39:21,157] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213000, global step 3405975: loss 0.0020
[2019-04-04 09:39:21,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213000, global step 3405975: learning rate 0.0000
[2019-04-04 09:39:25,051] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213000, global step 3406751: loss 0.0039
[2019-04-04 09:39:25,052] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213000, global step 3406751: learning rate 0.0000
[2019-04-04 09:39:29,959] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213000, global step 3407614: loss 0.0027
[2019-04-04 09:39:29,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213000, global step 3407614: learning rate 0.0000
[2019-04-04 09:39:31,388] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213000, global step 3407886: loss 0.0039
[2019-04-04 09:39:31,389] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213000, global step 3407886: learning rate 0.0000
[2019-04-04 09:39:32,478] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213000, global step 3408086: loss 0.0087
[2019-04-04 09:39:32,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213000, global step 3408086: learning rate 0.0000
[2019-04-04 09:39:32,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7149297e-11 1.0297741e-10 6.6136836e-23 2.0459493e-12 9.3917983e-13
 3.0592689e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:32,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2544
[2019-04-04 09:39:33,073] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.45, 79.5, 0.0, 0.0, 26.0, 21.92792010392076, -0.3418148870755788, 1.0, 1.0, 203358.3625419095], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 372600.0000, 
sim time next is 373200.0000, 
raw observation next is [-16.53333333333333, 80.0, 0.0, 0.0, 26.0, 22.68914621317499, -0.2505289343859654, 1.0, 1.0, 169295.3726917159], 
processed observation next is [1.0, 0.30434782608695654, 0.0046168051708218244, 0.8, 0.0, 0.0, 0.6666666666666666, 0.39076218443124916, 0.4164903552046782, 1.0, 1.0, 0.8061684413891232], 
reward next is 0.1938, 
noisyNet noise sample is [array([1.0328048], dtype=float32), 0.67474645]. 
=============================================
[2019-04-04 09:39:33,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3757964e-11 5.0740627e-11 9.1064954e-23 8.4820318e-13 3.2573672e-12
 4.0160675e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:33,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9871
[2019-04-04 09:39:33,872] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.733333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 24.58420384332527, 0.1866394411927706, 0.0, 1.0, 44318.92019326074], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 250800.0000, 
sim time next is 251400.0000, 
raw observation next is [-3.816666666666666, 73.33333333333333, 0.0, 0.0, 26.0, 24.54577392550435, 0.1784812613325746, 0.0, 1.0, 44155.55983606545], 
processed observation next is [1.0, 0.9130434782608695, 0.3568790397045245, 0.7333333333333333, 0.0, 0.0, 0.6666666666666666, 0.5454811604586958, 0.5594937537775249, 0.0, 1.0, 0.2102645706479307], 
reward next is 0.7897, 
noisyNet noise sample is [array([-1.873143], dtype=float32), 0.13790317]. 
=============================================
[2019-04-04 09:39:33,989] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213000, global step 3408373: loss 0.0074
[2019-04-04 09:39:33,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213000, global step 3408373: learning rate 0.0000
[2019-04-04 09:39:34,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.8146569e-10 4.3885581e-10 1.0578888e-21 2.0495477e-11 9.9920506e-12
 2.9327712e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:34,086] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7666
[2019-04-04 09:39:34,144] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-15.1, 69.66666666666667, 0.0, 0.0, 26.0, 22.6748325099077, -0.2577075683602514, 0.0, 1.0, 49309.35487379272], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 357000.0000, 
sim time next is 357600.0000, 
raw observation next is [-15.2, 70.33333333333334, 0.0, 0.0, 26.0, 22.63755588034809, -0.2684941272091597, 0.0, 1.0, 49318.87435764341], 
processed observation next is [1.0, 0.13043478260869565, 0.04155124653739613, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.38646299002900736, 0.4105019575969468, 0.0, 1.0, 0.23485178265544482], 
reward next is 0.7651, 
noisyNet noise sample is [array([-0.3141199], dtype=float32), -0.9088946]. 
=============================================
[2019-04-04 09:39:34,323] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213000, global step 3408439: loss 0.0083
[2019-04-04 09:39:34,338] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213000, global step 3408439: learning rate 0.0000
[2019-04-04 09:39:36,282] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213000, global step 3408862: loss 0.0073
[2019-04-04 09:39:36,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213000, global step 3408862: learning rate 0.0000
[2019-04-04 09:39:37,119] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213000, global step 3409041: loss 0.0065
[2019-04-04 09:39:37,121] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213000, global step 3409041: learning rate 0.0000
[2019-04-04 09:39:38,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4595187e-12 2.0343573e-12 1.2841916e-23 1.6124482e-13 2.8858055e-13
 6.2199276e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:38,197] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3508
[2019-04-04 09:39:38,250] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 80.5, 130.6666666666667, 509.6666666666666, 26.0, 24.999815484291, 0.3477009824114687, 0.0, 1.0, 25931.66447797677], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 569400.0000, 
sim time next is 570000.0000, 
raw observation next is [-1.2, 81.0, 128.8333333333333, 488.3333333333333, 26.0, 24.9946501321666, 0.3471977419198642, 0.0, 1.0, 32031.58099451198], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.81, 0.4294444444444443, 0.5395948434622467, 0.6666666666666666, 0.5828875110138835, 0.6157325806399547, 0.0, 1.0, 0.15253133806910465], 
reward next is 0.8475, 
noisyNet noise sample is [array([-1.4950562], dtype=float32), -1.157597]. 
=============================================
[2019-04-04 09:39:38,254] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.14681 ]
 [79.71643 ]
 [80.418045]
 [81.16355 ]
 [81.63896 ]], R is [[78.89209747]
 [78.97969818]
 [79.10073853]
 [79.22055817]
 [79.33917236]].
[2019-04-04 09:39:39,876] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213000, global step 3409513: loss 0.0089
[2019-04-04 09:39:39,877] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213000, global step 3409513: learning rate 0.0000
[2019-04-04 09:39:45,918] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4143261e-12 2.4884472e-12 1.2228900e-24 3.9282412e-13 1.6009135e-13
 7.7381680e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:45,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9510
[2019-04-04 09:39:45,995] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 88.33333333333334, 132.8333333333333, 109.3333333333333, 26.0, 24.90647955130909, 0.2547004662284126, 0.0, 1.0, 18735.04534364745], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 553200.0000, 
sim time next is 553800.0000, 
raw observation next is [-0.5, 87.66666666666666, 121.6666666666667, 115.6666666666667, 26.0, 24.83107855167048, 0.2521732707414668, 0.0, 1.0, 63808.80679601403], 
processed observation next is [0.0, 0.391304347826087, 0.44875346260387816, 0.8766666666666666, 0.40555555555555567, 0.12780847145488033, 0.6666666666666666, 0.5692565459725399, 0.5840577569138222, 0.0, 1.0, 0.30385146093340015], 
reward next is 0.6961, 
noisyNet noise sample is [array([0.9809952], dtype=float32), -1.3897337]. 
=============================================
[2019-04-04 09:39:46,107] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213000, global step 3411120: loss 0.0188
[2019-04-04 09:39:46,108] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213000, global step 3411120: learning rate 0.0000
[2019-04-04 09:39:46,579] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213000, global step 3411251: loss 0.0191
[2019-04-04 09:39:46,595] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213000, global step 3411251: learning rate 0.0000
[2019-04-04 09:39:46,788] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213000, global step 3411310: loss 0.0170
[2019-04-04 09:39:46,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213000, global step 3411310: learning rate 0.0000
[2019-04-04 09:39:48,217] A3C_AGENT_WORKER-Thread-12 INFO:Local step 213500, global step 3411656: loss 0.6160
[2019-04-04 09:39:48,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 213500, global step 3411656: learning rate 0.0000
[2019-04-04 09:39:49,845] A3C_AGENT_WORKER-Thread-2 INFO:Local step 213500, global step 3412032: loss 0.6142
[2019-04-04 09:39:49,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 213500, global step 3412032: learning rate 0.0000
[2019-04-04 09:39:51,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5106434e-13 1.1546492e-12 1.4847390e-26 2.1175261e-14 3.1039150e-14
 1.4506789e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:51,997] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9775
[2019-04-04 09:39:52,059] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.74259195371506, 0.2149957148620994, 1.0, 1.0, 29895.83325642082], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 503400.0000, 
sim time next is 504000.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.82401837598023, 0.2205625526252598, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5686681979983526, 0.5735208508750866, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09496649], dtype=float32), -0.892216]. 
=============================================
[2019-04-04 09:39:52,065] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[89.50353]
 [88.99127]
 [87.1624 ]
 [83.35922]
 [85.81084]], R is [[89.63575745]
 [89.59703827]
 [89.18308258]
 [88.80451202]
 [88.70110321]].
[2019-04-04 09:39:52,665] A3C_AGENT_WORKER-Thread-5 INFO:Local step 213500, global step 3412755: loss 0.6388
[2019-04-04 09:39:52,675] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 213500, global step 3412757: learning rate 0.0000
[2019-04-04 09:39:55,464] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.0137203e-12 3.2509804e-12 1.5370860e-25 2.8039626e-13 2.6969765e-13
 1.5913882e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:55,469] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6401
[2019-04-04 09:39:55,523] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 91.33333333333334, 52.33333333333333, 103.8333333333333, 26.0, 25.00134839822407, 0.2932608035857546, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 549600.0000, 
sim time next is 550200.0000, 
raw observation next is [0.08333333333333331, 91.16666666666667, 70.66666666666666, 103.6666666666667, 26.0, 25.1118829885312, 0.2989672247063788, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.4649122807017544, 0.9116666666666667, 0.23555555555555552, 0.11454880294659305, 0.6666666666666666, 0.5926569157109333, 0.599655741568793, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3709379], dtype=float32), 1.7329984]. 
=============================================
[2019-04-04 09:39:56,505] A3C_AGENT_WORKER-Thread-18 INFO:Local step 213500, global step 3414011: loss 0.6080
[2019-04-04 09:39:56,506] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 213500, global step 3414012: learning rate 0.0000
[2019-04-04 09:39:59,167] A3C_AGENT_WORKER-Thread-6 INFO:Local step 213500, global step 3414709: loss 0.6189
[2019-04-04 09:39:59,167] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 213500, global step 3414709: learning rate 0.0000
[2019-04-04 09:39:59,571] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.2239276e-12 1.3116028e-11 1.0989931e-23 4.9139788e-13 5.0168084e-13
 7.4549674e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:39:59,571] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8210
[2019-04-04 09:39:59,617] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8, 55.00000000000001, 36.33333333333333, 18.83333333333333, 26.0, 24.8986967784748, 0.2153159390080196, 0.0, 1.0, 37991.63021781467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 663600.0000, 
sim time next is 664200.0000, 
raw observation next is [-0.8999999999999999, 55.5, 27.0, 15.0, 26.0, 24.88801076382936, 0.2118458221844686, 0.0, 1.0, 47468.55708739007], 
processed observation next is [0.0, 0.6956521739130435, 0.43767313019390586, 0.555, 0.09, 0.016574585635359115, 0.6666666666666666, 0.57400089698578, 0.5706152740614895, 0.0, 1.0, 0.2260407480351908], 
reward next is 0.7740, 
noisyNet noise sample is [array([0.32217914], dtype=float32), 0.9059981]. 
=============================================
[2019-04-04 09:40:01,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6546757e-11 1.7066961e-11 3.8810769e-24 1.7484151e-12 3.9333562e-12
 9.8789783e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:01,213] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0400
[2019-04-04 09:40:01,231] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 83.0, 0.0, 0.0, 26.0, 24.82940202168966, 0.2322694345869911, 0.0, 1.0, 42700.68598723267], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 600600.0000, 
sim time next is 601200.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.79734092891119, 0.2271291276862409, 0.0, 1.0, 42622.43646306849], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5664450774092659, 0.575709709228747, 0.0, 1.0, 0.202963983157469], 
reward next is 0.7970, 
noisyNet noise sample is [array([-1.9651875], dtype=float32), 2.4201202]. 
=============================================
[2019-04-04 09:40:01,886] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2966982e-11 3.1015229e-11 6.5919434e-24 1.4510222e-12 1.4173310e-12
 9.6600025e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:01,886] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6626
[2019-04-04 09:40:01,951] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 66.0, 0.0, 0.0, 26.0, 23.67482476615485, -0.02631029118422627, 0.0, 1.0, 43780.07362082966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 627600.0000, 
sim time next is 628200.0000, 
raw observation next is [-4.5, 66.5, 0.0, 0.0, 26.0, 23.67203620058412, -0.02856394677403999, 0.0, 1.0, 43747.09377457052], 
processed observation next is [0.0, 0.2608695652173913, 0.3379501385041552, 0.665, 0.0, 0.0, 0.6666666666666666, 0.47266968338201004, 0.49047868440865333, 0.0, 1.0, 0.20831949416462153], 
reward next is 0.7917, 
noisyNet noise sample is [array([0.4445436], dtype=float32), -0.8979754]. 
=============================================
[2019-04-04 09:40:01,965] A3C_AGENT_WORKER-Thread-20 INFO:Local step 213500, global step 3415549: loss 0.6271
[2019-04-04 09:40:01,968] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 213500, global step 3415549: learning rate 0.0000
[2019-04-04 09:40:04,044] A3C_AGENT_WORKER-Thread-15 INFO:Local step 213500, global step 3416236: loss 0.6748
[2019-04-04 09:40:04,046] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 213500, global step 3416236: learning rate 0.0000
[2019-04-04 09:40:05,239] A3C_AGENT_WORKER-Thread-3 INFO:Local step 213500, global step 3416536: loss 0.7040
[2019-04-04 09:40:05,240] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 213500, global step 3416536: learning rate 0.0000
[2019-04-04 09:40:05,812] A3C_AGENT_WORKER-Thread-19 INFO:Local step 213500, global step 3416670: loss 0.6960
[2019-04-04 09:40:05,813] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 213500, global step 3416670: learning rate 0.0000
[2019-04-04 09:40:06,076] A3C_AGENT_WORKER-Thread-17 INFO:Local step 213500, global step 3416749: loss 0.7005
[2019-04-04 09:40:06,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 213500, global step 3416749: learning rate 0.0000
[2019-04-04 09:40:06,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0489149e-12 2.6410794e-11 1.5071333e-23 1.6247827e-12 1.6471427e-12
 8.5251173e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:06,983] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8430
[2019-04-04 09:40:07,017] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.36194490612996, 0.1583037248834525, 0.0, 1.0, 41881.33905127856], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 777600.0000, 
sim time next is 778200.0000, 
raw observation next is [-7.3, 71.0, 0.0, 0.0, 26.0, 24.32511007063034, 0.1499228247635609, 0.0, 1.0, 41810.71164456981], 
processed observation next is [1.0, 0.0, 0.26038781163434904, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5270925058858618, 0.549974274921187, 0.0, 1.0, 0.19909862687890387], 
reward next is 0.8009, 
noisyNet noise sample is [array([-1.8223802], dtype=float32), 0.3505385]. 
=============================================
[2019-04-04 09:40:07,064] A3C_AGENT_WORKER-Thread-16 INFO:Local step 213500, global step 3416996: loss 0.6822
[2019-04-04 09:40:07,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 213500, global step 3416996: learning rate 0.0000
[2019-04-04 09:40:08,176] A3C_AGENT_WORKER-Thread-4 INFO:Local step 213500, global step 3417318: loss 0.6702
[2019-04-04 09:40:08,179] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 213500, global step 3417319: learning rate 0.0000
[2019-04-04 09:40:09,886] A3C_AGENT_WORKER-Thread-11 INFO:Local step 213500, global step 3417914: loss 0.6701
[2019-04-04 09:40:09,887] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 213500, global step 3417914: learning rate 0.0000
[2019-04-04 09:40:12,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6397349e-13 3.6633275e-12 8.7978028e-25 3.6178973e-14 6.2997377e-14
 4.2808296e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:12,359] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2248
[2019-04-04 09:40:12,398] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 46.0, 80.0, 714.0, 26.0, 25.89073588865145, 0.4519577484267781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 743400.0000, 
sim time next is 744000.0000, 
raw observation next is [0.1666666666666667, 46.33333333333334, 80.83333333333334, 600.1666666666666, 26.0, 25.93577638798653, 0.4544810173413052, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4672206832871654, 0.46333333333333343, 0.2694444444444445, 0.6631675874769797, 0.6666666666666666, 0.6613146989988774, 0.6514936724471018, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.36438528], dtype=float32), 1.2442722]. 
=============================================
[2019-04-04 09:40:12,422] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[86.43244 ]
 [86.47479 ]
 [86.53764 ]
 [86.653656]
 [86.94653 ]], R is [[85.67488098]
 [85.81813049]
 [85.95995331]
 [86.10035706]
 [86.23935699]].
[2019-04-04 09:40:14,040] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214000, global step 3419245: loss 0.2112
[2019-04-04 09:40:14,041] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214000, global step 3419245: learning rate 0.0000
[2019-04-04 09:40:14,235] A3C_AGENT_WORKER-Thread-13 INFO:Local step 213500, global step 3419300: loss 0.6596
[2019-04-04 09:40:14,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 213500, global step 3419300: learning rate 0.0000
[2019-04-04 09:40:15,413] A3C_AGENT_WORKER-Thread-14 INFO:Local step 213500, global step 3419677: loss 0.6632
[2019-04-04 09:40:15,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 213500, global step 3419677: learning rate 0.0000
[2019-04-04 09:40:15,539] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214000, global step 3419714: loss 0.2175
[2019-04-04 09:40:15,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214000, global step 3419714: learning rate 0.0000
[2019-04-04 09:40:15,671] A3C_AGENT_WORKER-Thread-10 INFO:Local step 213500, global step 3419756: loss 0.6715
[2019-04-04 09:40:15,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 213500, global step 3419766: learning rate 0.0000
[2019-04-04 09:40:17,372] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214000, global step 3420335: loss 0.2351
[2019-04-04 09:40:17,375] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214000, global step 3420335: learning rate 0.0000
[2019-04-04 09:40:22,109] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214000, global step 3422020: loss 0.2393
[2019-04-04 09:40:22,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214000, global step 3422020: learning rate 0.0000
[2019-04-04 09:40:23,791] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214000, global step 3422667: loss 0.2631
[2019-04-04 09:40:23,794] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214000, global step 3422667: learning rate 0.0000
[2019-04-04 09:40:26,660] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214000, global step 3423938: loss 0.2197
[2019-04-04 09:40:26,665] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214000, global step 3423938: learning rate 0.0000
[2019-04-04 09:40:26,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [7.3842169e-13 6.3449771e-12 8.0938509e-26 2.9483352e-13 3.2768639e-13
 1.3176983e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:26,878] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5801
[2019-04-04 09:40:26,899] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.62256499194848, 0.1641023118524476, 0.0, 1.0, 38889.69918574175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 884400.0000, 
sim time next is 885000.0000, 
raw observation next is [-0.09999999999999998, 72.0, 0.0, 0.0, 26.0, 24.55566077534014, 0.1551483131757126, 0.0, 1.0, 38869.9045806], 
processed observation next is [1.0, 0.21739130434782608, 0.4598337950138504, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5463050646116784, 0.5517161043919042, 0.0, 1.0, 0.18509478371714286], 
reward next is 0.8149, 
noisyNet noise sample is [array([0.89475006], dtype=float32), 0.5804581]. 
=============================================
[2019-04-04 09:40:26,910] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.87509 ]
 [85.82274 ]
 [85.75173 ]
 [85.67672 ]
 [85.624794]], R is [[85.867836  ]
 [85.82396698]
 [85.7804718 ]
 [85.73725891]
 [85.69425201]].
[2019-04-04 09:40:27,187] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8894774e-13 9.7137782e-13 3.0473452e-27 2.8038327e-14 3.1373060e-14
 1.1836393e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:27,189] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3921
[2019-04-04 09:40:27,200] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.7, 82.0, 0.0, 0.0, 26.0, 25.67807029932417, 0.6162648320547074, 0.0, 1.0, 18724.41652088541], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1150200.0000, 
sim time next is 1150800.0000, 
raw observation next is [12.7, 82.66666666666667, 0.0, 0.0, 26.0, 25.70565306531518, 0.6152337300261607, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8144044321329641, 0.8266666666666667, 0.0, 0.0, 0.6666666666666666, 0.6421377554429316, 0.7050779100087202, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1484487], dtype=float32), 0.06772011]. 
=============================================
[2019-04-04 09:40:27,942] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214000, global step 3424531: loss 0.1713
[2019-04-04 09:40:27,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214000, global step 3424531: learning rate 0.0000
[2019-04-04 09:40:28,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.08421972e-13 2.85568617e-12 1.10257388e-25 8.99121137e-14
 1.09316805e-13 7.22975840e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 09:40:28,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3644
[2019-04-04 09:40:28,089] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.083333333333334, 94.83333333333333, 0.0, 0.0, 26.0, 25.26859548288974, 0.4137020424580526, 0.0, 1.0, 38220.3784454995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 951000.0000, 
sim time next is 951600.0000, 
raw observation next is [5.166666666666667, 93.66666666666666, 0.0, 0.0, 26.0, 25.26637100401493, 0.4251574959975011, 0.0, 1.0, 38115.60745423881], 
processed observation next is [1.0, 0.0, 0.6057248384118191, 0.9366666666666665, 0.0, 0.0, 0.6666666666666666, 0.6055309170012441, 0.6417191653325004, 0.0, 1.0, 0.18150289263923242], 
reward next is 0.8185, 
noisyNet noise sample is [array([-0.4666319], dtype=float32), -1.8354648]. 
=============================================
[2019-04-04 09:40:28,509] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214000, global step 3424765: loss 0.1696
[2019-04-04 09:40:28,510] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214000, global step 3424765: learning rate 0.0000
[2019-04-04 09:40:29,039] A3C_AGENT_WORKER-Thread-12 INFO:Local step 214500, global step 3425019: loss 0.0932
[2019-04-04 09:40:29,040] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 214500, global step 3425019: learning rate 0.0000
[2019-04-04 09:40:29,131] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214000, global step 3425057: loss 0.1908
[2019-04-04 09:40:29,140] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214000, global step 3425057: learning rate 0.0000
[2019-04-04 09:40:29,745] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214000, global step 3425349: loss 0.1856
[2019-04-04 09:40:29,749] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214000, global step 3425349: learning rate 0.0000
[2019-04-04 09:40:29,968] A3C_AGENT_WORKER-Thread-2 INFO:Local step 214500, global step 3425457: loss 0.0913
[2019-04-04 09:40:29,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 214500, global step 3425460: learning rate 0.0000
[2019-04-04 09:40:30,330] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214000, global step 3425640: loss 0.1950
[2019-04-04 09:40:30,330] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214000, global step 3425640: learning rate 0.0000
[2019-04-04 09:40:30,411] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4365593e-12 7.3340266e-12 1.1326544e-25 7.0733594e-14 4.3708751e-13
 6.3629369e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:30,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2176
[2019-04-04 09:40:30,443] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.500000000000001, 100.0, 0.0, 0.0, 26.0, 25.50893181133063, 0.5888945490678317, 0.0, 1.0, 25460.42596165899], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1285800.0000, 
sim time next is 1286400.0000, 
raw observation next is [5.5, 100.0, 0.0, 0.0, 26.0, 25.48054083601781, 0.5860758732853425, 0.0, 1.0, 46095.32163259199], 
processed observation next is [0.0, 0.9130434782608695, 0.6149584487534627, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6233784030014841, 0.6953586244284474, 0.0, 1.0, 0.2195015315837714], 
reward next is 0.7805, 
noisyNet noise sample is [array([0.9969945], dtype=float32), 1.4190238]. 
=============================================
[2019-04-04 09:40:31,589] A3C_AGENT_WORKER-Thread-5 INFO:Local step 214500, global step 3426278: loss 0.0930
[2019-04-04 09:40:31,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 214500, global step 3426278: learning rate 0.0000
[2019-04-04 09:40:32,100] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214000, global step 3426568: loss 0.2136
[2019-04-04 09:40:32,101] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214000, global step 3426568: learning rate 0.0000
[2019-04-04 09:40:32,977] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214000, global step 3427072: loss 0.2660
[2019-04-04 09:40:32,979] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214000, global step 3427073: learning rate 0.0000
[2019-04-04 09:40:35,113] A3C_AGENT_WORKER-Thread-18 INFO:Local step 214500, global step 3428307: loss 0.0890
[2019-04-04 09:40:35,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 214500, global step 3428307: learning rate 0.0000
[2019-04-04 09:40:35,755] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.1461824e-13 1.1689547e-13 5.8583310e-29 3.4647269e-15 5.3268089e-15
 1.3364114e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:35,755] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5579
[2019-04-04 09:40:35,797] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.5, 100.0, 83.66666666666667, 0.0, 26.0, 24.45726140577369, 0.4259845501270266, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1248600.0000, 
sim time next is 1249200.0000, 
raw observation next is [14.4, 100.0, 86.5, 0.0, 26.0, 24.70938401756385, 0.4496120801522759, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.8614958448753465, 1.0, 0.28833333333333333, 0.0, 0.6666666666666666, 0.5591153347969874, 0.649870693384092, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1339508], dtype=float32), -0.014350663]. 
=============================================
[2019-04-04 09:40:36,417] A3C_AGENT_WORKER-Thread-6 INFO:Local step 214500, global step 3429023: loss 0.0775
[2019-04-04 09:40:36,420] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 214500, global step 3429023: learning rate 0.0000
[2019-04-04 09:40:36,904] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214000, global step 3429306: loss 0.3162
[2019-04-04 09:40:36,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214000, global step 3429307: learning rate 0.0000
[2019-04-04 09:40:37,714] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214000, global step 3429784: loss 0.3511
[2019-04-04 09:40:37,716] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214000, global step 3429785: learning rate 0.0000
[2019-04-04 09:40:37,734] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214000, global step 3429800: loss 0.3521
[2019-04-04 09:40:37,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214000, global step 3429802: learning rate 0.0000
[2019-04-04 09:40:39,389] A3C_AGENT_WORKER-Thread-20 INFO:Local step 214500, global step 3430816: loss 0.0969
[2019-04-04 09:40:39,391] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 214500, global step 3430816: learning rate 0.0000
[2019-04-04 09:40:39,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1489406e-14 2.7821299e-14 4.6575226e-28 2.3611346e-15 2.3230217e-15
 3.7643215e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:39,571] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3080
[2019-04-04 09:40:39,579] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.5, 76.5, 25.0, 0.0, 26.0, 25.94885173989908, 0.6332677426625071, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1009800.0000, 
sim time next is 1010400.0000, 
raw observation next is [15.5, 77.0, 20.83333333333334, 0.0, 26.0, 26.45133463895038, 0.6776089465763605, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8919667590027703, 0.77, 0.06944444444444446, 0.0, 0.6666666666666666, 0.7042778865791984, 0.7258696488587869, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5203083], dtype=float32), -0.655992]. 
=============================================
[2019-04-04 09:40:40,284] A3C_AGENT_WORKER-Thread-15 INFO:Local step 214500, global step 3431309: loss 0.1024
[2019-04-04 09:40:40,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 214500, global step 3431309: learning rate 0.0000
[2019-04-04 09:40:41,298] A3C_AGENT_WORKER-Thread-19 INFO:Local step 214500, global step 3431864: loss 0.1037
[2019-04-04 09:40:41,304] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 214500, global step 3431867: learning rate 0.0000
[2019-04-04 09:40:41,573] A3C_AGENT_WORKER-Thread-3 INFO:Local step 214500, global step 3432024: loss 0.0963
[2019-04-04 09:40:41,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 214500, global step 3432024: learning rate 0.0000
[2019-04-04 09:40:42,057] A3C_AGENT_WORKER-Thread-17 INFO:Local step 214500, global step 3432302: loss 0.0876
[2019-04-04 09:40:42,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 214500, global step 3432305: learning rate 0.0000
[2019-04-04 09:40:43,172] A3C_AGENT_WORKER-Thread-16 INFO:Local step 214500, global step 3432930: loss 0.0699
[2019-04-04 09:40:43,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 214500, global step 3432930: learning rate 0.0000
[2019-04-04 09:40:44,373] A3C_AGENT_WORKER-Thread-4 INFO:Local step 214500, global step 3433608: loss 0.0561
[2019-04-04 09:40:44,373] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 214500, global step 3433608: learning rate 0.0000
[2019-04-04 09:40:45,227] A3C_AGENT_WORKER-Thread-11 INFO:Local step 214500, global step 3434026: loss 0.0552
[2019-04-04 09:40:45,228] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 214500, global step 3434026: learning rate 0.0000
[2019-04-04 09:40:46,187] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215000, global step 3434525: loss 0.0943
[2019-04-04 09:40:46,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215000, global step 3434525: learning rate 0.0000
[2019-04-04 09:40:46,635] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215000, global step 3434755: loss 0.0957
[2019-04-04 09:40:46,637] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215000, global step 3434756: learning rate 0.0000
[2019-04-04 09:40:48,391] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215000, global step 3435715: loss 0.0401
[2019-04-04 09:40:48,392] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215000, global step 3435715: learning rate 0.0000
[2019-04-04 09:40:49,525] A3C_AGENT_WORKER-Thread-13 INFO:Local step 214500, global step 3436285: loss 0.0736
[2019-04-04 09:40:49,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 214500, global step 3436286: learning rate 0.0000
[2019-04-04 09:40:50,349] A3C_AGENT_WORKER-Thread-10 INFO:Local step 214500, global step 3436632: loss 0.0762
[2019-04-04 09:40:50,350] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 214500, global step 3436632: learning rate 0.0000
[2019-04-04 09:40:50,432] A3C_AGENT_WORKER-Thread-14 INFO:Local step 214500, global step 3436668: loss 0.0682
[2019-04-04 09:40:50,436] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 214500, global step 3436669: learning rate 0.0000
[2019-04-04 09:40:52,390] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215000, global step 3437576: loss 0.1049
[2019-04-04 09:40:52,392] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215000, global step 3437576: learning rate 0.0000
[2019-04-04 09:40:53,288] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215000, global step 3437976: loss 0.1297
[2019-04-04 09:40:53,291] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215000, global step 3437977: learning rate 0.0000
[2019-04-04 09:40:54,534] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5630604e-15 4.8036523e-14 2.9490398e-27 5.7303499e-16 4.0629172e-15
 4.8875877e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:40:54,540] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9615
[2019-04-04 09:40:54,548] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 93.0, 94.0, 704.0, 26.0, 26.07877424620583, 0.6293275422409025, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1512000.0000, 
sim time next is 1512600.0000, 
raw observation next is [4.866666666666667, 89.66666666666667, 96.0, 702.6666666666667, 26.0, 26.25175262185644, 0.6348138135018059, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.5974145891043399, 0.8966666666666667, 0.32, 0.7764272559852671, 0.6666666666666666, 0.68764605182137, 0.7116046045006019, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.197347], dtype=float32), 0.81129414]. 
=============================================
[2019-04-04 09:40:56,796] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215000, global step 3439476: loss 0.0525
[2019-04-04 09:40:56,802] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215000, global step 3439476: learning rate 0.0000
[2019-04-04 09:40:57,644] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215000, global step 3439926: loss 0.0676
[2019-04-04 09:40:57,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215000, global step 3439926: learning rate 0.0000
[2019-04-04 09:40:58,518] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215000, global step 3440346: loss 0.1007
[2019-04-04 09:40:58,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215000, global step 3440346: learning rate 0.0000
[2019-04-04 09:40:58,571] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215000, global step 3440375: loss 0.1053
[2019-04-04 09:40:58,572] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215000, global step 3440375: learning rate 0.0000
[2019-04-04 09:40:59,164] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.79727952e-14 3.07940929e-13 1.10303185e-26 3.96471432e-15
 4.45809026e-15 1.76052456e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 09:40:59,164] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0668
[2019-04-04 09:40:59,191] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.383333333333333, 99.33333333333334, 64.33333333333333, 0.0, 26.0, 26.07092028861077, 0.5225919103896128, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1505400.0000, 
sim time next is 1506000.0000, 
raw observation next is [2.566666666666667, 98.66666666666667, 68.66666666666667, 0.0, 26.0, 26.07536368925812, 0.5281367598658216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5337026777469991, 0.9866666666666667, 0.2288888888888889, 0.0, 0.6666666666666666, 0.6729469741048432, 0.6760455866219406, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9192605], dtype=float32), 0.9846965]. 
=============================================
[2019-04-04 09:40:59,198] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[90.16969]
 [90.20326]
 [90.26972]
 [90.24098]
 [90.25777]], R is [[90.26852417]
 [90.3658371 ]
 [90.46218109]
 [90.55756378]
 [90.6519928 ]].
[2019-04-04 09:40:59,535] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215000, global step 3440775: loss 0.0904
[2019-04-04 09:40:59,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215000, global step 3440776: learning rate 0.0000
[2019-04-04 09:41:00,570] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215000, global step 3441190: loss 0.1001
[2019-04-04 09:41:00,572] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215000, global step 3441190: learning rate 0.0000
[2019-04-04 09:41:01,980] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215000, global step 3441804: loss 0.0987
[2019-04-04 09:41:01,984] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215000, global step 3441806: learning rate 0.0000
[2019-04-04 09:41:02,633] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215000, global step 3442119: loss 0.1036
[2019-04-04 09:41:02,633] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215000, global step 3442119: learning rate 0.0000
[2019-04-04 09:41:05,988] A3C_AGENT_WORKER-Thread-12 INFO:Local step 215500, global step 3443586: loss 0.5523
[2019-04-04 09:41:05,991] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 215500, global step 3443586: learning rate 0.0000
[2019-04-04 09:41:06,931] A3C_AGENT_WORKER-Thread-2 INFO:Local step 215500, global step 3443925: loss 0.5252
[2019-04-04 09:41:06,932] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 215500, global step 3443925: learning rate 0.0000
[2019-04-04 09:41:07,904] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215000, global step 3444300: loss 0.1993
[2019-04-04 09:41:07,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215000, global step 3444300: learning rate 0.0000
[2019-04-04 09:41:08,288] A3C_AGENT_WORKER-Thread-5 INFO:Local step 215500, global step 3444437: loss 0.5032
[2019-04-04 09:41:08,289] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 215500, global step 3444437: learning rate 0.0000
[2019-04-04 09:41:08,493] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215000, global step 3444516: loss 0.2024
[2019-04-04 09:41:08,494] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215000, global step 3444516: learning rate 0.0000
[2019-04-04 09:41:08,714] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215000, global step 3444611: loss 0.1865
[2019-04-04 09:41:08,717] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215000, global step 3444612: learning rate 0.0000
[2019-04-04 09:41:12,362] A3C_AGENT_WORKER-Thread-18 INFO:Local step 215500, global step 3445893: loss 0.4416
[2019-04-04 09:41:12,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 215500, global step 3445894: learning rate 0.0000
[2019-04-04 09:41:13,325] A3C_AGENT_WORKER-Thread-6 INFO:Local step 215500, global step 3446223: loss 0.4517
[2019-04-04 09:41:13,330] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 215500, global step 3446223: learning rate 0.0000
[2019-04-04 09:41:17,456] A3C_AGENT_WORKER-Thread-20 INFO:Local step 215500, global step 3447511: loss 0.3900
[2019-04-04 09:41:17,456] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 215500, global step 3447511: learning rate 0.0000
[2019-04-04 09:41:18,567] A3C_AGENT_WORKER-Thread-15 INFO:Local step 215500, global step 3447847: loss 0.3889
[2019-04-04 09:41:18,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 215500, global step 3447847: learning rate 0.0000
[2019-04-04 09:41:18,757] A3C_AGENT_WORKER-Thread-3 INFO:Local step 215500, global step 3447899: loss 0.3927
[2019-04-04 09:41:18,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 215500, global step 3447899: learning rate 0.0000
[2019-04-04 09:41:19,159] A3C_AGENT_WORKER-Thread-19 INFO:Local step 215500, global step 3448021: loss 0.3861
[2019-04-04 09:41:19,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 215500, global step 3448021: learning rate 0.0000
[2019-04-04 09:41:21,578] A3C_AGENT_WORKER-Thread-17 INFO:Local step 215500, global step 3448729: loss 0.3806
[2019-04-04 09:41:21,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 215500, global step 3448729: learning rate 0.0000
[2019-04-04 09:41:21,909] A3C_AGENT_WORKER-Thread-16 INFO:Local step 215500, global step 3448815: loss 0.3736
[2019-04-04 09:41:21,909] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 215500, global step 3448815: learning rate 0.0000
[2019-04-04 09:41:24,456] A3C_AGENT_WORKER-Thread-4 INFO:Local step 215500, global step 3449471: loss 0.3639
[2019-04-04 09:41:24,457] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 215500, global step 3449471: learning rate 0.0000
[2019-04-04 09:41:24,555] A3C_AGENT_WORKER-Thread-11 INFO:Local step 215500, global step 3449496: loss 0.3556
[2019-04-04 09:41:24,556] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 215500, global step 3449496: learning rate 0.0000
[2019-04-04 09:41:30,172] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6599244e-13 3.0283172e-12 7.9701386e-24 5.4572120e-14 7.3605334e-14
 1.8352095e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:41:30,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9173
[2019-04-04 09:41:30,263] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.916666666666667, 67.5, 99.0, 0.0, 26.0, 26.30315047605412, 0.5023603862678175, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2128200.0000, 
sim time next is 2128800.0000, 
raw observation next is [-4.833333333333334, 67.0, 92.5, 0.0, 26.0, 26.28267522230804, 0.4917460546124376, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.32871652816251157, 0.67, 0.30833333333333335, 0.0, 0.6666666666666666, 0.6902229351923367, 0.6639153515374793, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8860573], dtype=float32), -0.19931184]. 
=============================================
[2019-04-04 09:41:30,901] A3C_AGENT_WORKER-Thread-13 INFO:Local step 215500, global step 3451327: loss 0.3557
[2019-04-04 09:41:30,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 215500, global step 3451327: learning rate 0.0000
[2019-04-04 09:41:31,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4914131e-13 2.4348798e-13 3.3298184e-25 1.2046766e-14 7.8418217e-15
 2.0592522e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:41:31,295] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8591
[2019-04-04 09:41:31,355] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 82.0, 185.3333333333333, 98.66666666666667, 26.0, 25.79682429116654, 0.3973865403668149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2109000.0000, 
sim time next is 2109600.0000, 
raw observation next is [-7.8, 82.0, 191.0, 89.0, 26.0, 25.84290754779257, 0.4027765732773818, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.82, 0.6366666666666667, 0.09834254143646409, 0.6666666666666666, 0.6535756289827143, 0.6342588577591273, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01749179], dtype=float32), 0.4185849]. 
=============================================
[2019-04-04 09:41:31,365] A3C_AGENT_WORKER-Thread-10 INFO:Local step 215500, global step 3451445: loss 0.3744
[2019-04-04 09:41:31,365] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 215500, global step 3451445: learning rate 0.0000
[2019-04-04 09:41:31,840] A3C_AGENT_WORKER-Thread-14 INFO:Local step 215500, global step 3451550: loss 0.3799
[2019-04-04 09:41:31,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 215500, global step 3451550: learning rate 0.0000
[2019-04-04 09:41:33,313] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216000, global step 3451935: loss 0.0076
[2019-04-04 09:41:33,335] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216000, global step 3451935: learning rate 0.0000
[2019-04-04 09:41:34,671] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216000, global step 3452373: loss 0.0128
[2019-04-04 09:41:34,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216000, global step 3452374: learning rate 0.0000
[2019-04-04 09:41:36,624] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216000, global step 3452954: loss 0.0118
[2019-04-04 09:41:36,640] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216000, global step 3452954: learning rate 0.0000
[2019-04-04 09:41:41,341] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216000, global step 3454168: loss 0.0098
[2019-04-04 09:41:41,352] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216000, global step 3454168: learning rate 0.0000
[2019-04-04 09:41:41,936] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216000, global step 3454339: loss 0.0042
[2019-04-04 09:41:41,952] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216000, global step 3454339: learning rate 0.0000
[2019-04-04 09:41:46,253] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216000, global step 3455604: loss 0.0126
[2019-04-04 09:41:46,258] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216000, global step 3455604: learning rate 0.0000
[2019-04-04 09:41:47,740] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216000, global step 3455968: loss 0.0132
[2019-04-04 09:41:47,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216000, global step 3455968: learning rate 0.0000
[2019-04-04 09:41:47,985] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216000, global step 3456033: loss 0.0100
[2019-04-04 09:41:47,989] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216000, global step 3456034: learning rate 0.0000
[2019-04-04 09:41:48,812] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216000, global step 3456242: loss 0.0099
[2019-04-04 09:41:48,813] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216000, global step 3456242: learning rate 0.0000
[2019-04-04 09:41:49,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1961736e-13 2.6546067e-13 6.1181621e-27 4.2120868e-14 1.1600076e-14
 2.9367956e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:41:49,070] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3306
[2019-04-04 09:41:49,151] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 41.0, 262.0, 26.0, 25.66817722051523, 0.3803621114827708, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2190600.0000, 
sim time next is 2191200.0000, 
raw observation next is [-5.6, 75.0, 51.16666666666667, 293.5, 26.0, 25.82565141103336, 0.3919443157969573, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.30747922437673136, 0.75, 0.17055555555555557, 0.3243093922651934, 0.6666666666666666, 0.6521376175861132, 0.6306481052656524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05864124], dtype=float32), -0.049050238]. 
=============================================
[2019-04-04 09:41:50,232] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216000, global step 3456671: loss 0.0077
[2019-04-04 09:41:50,249] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216000, global step 3456671: learning rate 0.0000
[2019-04-04 09:41:50,603] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216000, global step 3456786: loss 0.0049
[2019-04-04 09:41:50,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216000, global step 3456786: learning rate 0.0000
[2019-04-04 09:41:51,377] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2850329e-11 1.6111534e-10 6.3208199e-23 6.1220842e-12 6.2140115e-12
 8.5723753e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:41:51,378] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8926
[2019-04-04 09:41:51,408] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.466666666666667, 63.0, 0.0, 0.0, 26.0, 24.53399347836289, 0.1920901211838017, 0.0, 1.0, 39755.88521956332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2344800.0000, 
sim time next is 2345400.0000, 
raw observation next is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.52150073818959, 0.1860453205060347, 0.0, 1.0, 39862.47935494516], 
processed observation next is [0.0, 0.13043478260869565, 0.3919667590027701, 0.635, 0.0, 0.0, 0.6666666666666666, 0.5434583948491326, 0.5620151068353448, 0.0, 1.0, 0.1898213302616436], 
reward next is 0.8102, 
noisyNet noise sample is [array([0.34450257], dtype=float32), -0.84086573]. 
=============================================
[2019-04-04 09:41:53,271] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216000, global step 3457577: loss 0.0058
[2019-04-04 09:41:53,272] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216000, global step 3457577: learning rate 0.0000
[2019-04-04 09:41:53,414] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216000, global step 3457621: loss 0.0030
[2019-04-04 09:41:53,416] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216000, global step 3457621: learning rate 0.0000
[2019-04-04 09:41:56,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6301496e-12 3.2235915e-11 1.0710030e-23 5.0169520e-13 7.7955692e-13
 1.1938298e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:41:56,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5569
[2019-04-04 09:41:56,148] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 73.66666666666667, 0.0, 0.0, 26.0, 24.93001285260176, 0.2926861873627243, 0.0, 1.0, 44254.91274637901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2241600.0000, 
sim time next is 2242200.0000, 
raw observation next is [-6.1, 74.33333333333333, 0.0, 0.0, 26.0, 24.8546303209086, 0.2794598560025186, 0.0, 1.0, 44247.95537157904], 
processed observation next is [1.0, 0.9565217391304348, 0.29362880886426596, 0.7433333333333333, 0.0, 0.0, 0.6666666666666666, 0.5712191934090501, 0.5931532853341729, 0.0, 1.0, 0.2107045493884716], 
reward next is 0.7893, 
noisyNet noise sample is [array([0.589563], dtype=float32), -2.090671]. 
=============================================
[2019-04-04 09:41:59,805] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216000, global step 3459450: loss 0.0027
[2019-04-04 09:41:59,806] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216000, global step 3459450: learning rate 0.0000
[2019-04-04 09:42:00,353] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216000, global step 3459615: loss 0.0029
[2019-04-04 09:42:00,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216000, global step 3459615: learning rate 0.0000
[2019-04-04 09:42:00,618] A3C_AGENT_WORKER-Thread-12 INFO:Local step 216500, global step 3459694: loss 0.1426
[2019-04-04 09:42:00,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 216500, global step 3459694: learning rate 0.0000
[2019-04-04 09:42:00,984] A3C_AGENT_WORKER-Thread-2 INFO:Local step 216500, global step 3459807: loss 0.1473
[2019-04-04 09:42:00,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 216500, global step 3459807: learning rate 0.0000
[2019-04-04 09:42:01,197] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216000, global step 3459877: loss 0.0032
[2019-04-04 09:42:01,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216000, global step 3459877: learning rate 0.0000
[2019-04-04 09:42:03,305] A3C_AGENT_WORKER-Thread-5 INFO:Local step 216500, global step 3460531: loss 0.1823
[2019-04-04 09:42:03,306] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 216500, global step 3460531: learning rate 0.0000
[2019-04-04 09:42:07,588] A3C_AGENT_WORKER-Thread-18 INFO:Local step 216500, global step 3461910: loss 0.2369
[2019-04-04 09:42:07,590] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 216500, global step 3461911: learning rate 0.0000
[2019-04-04 09:42:09,269] A3C_AGENT_WORKER-Thread-6 INFO:Local step 216500, global step 3462449: loss 0.2700
[2019-04-04 09:42:09,271] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 216500, global step 3462450: learning rate 0.0000
[2019-04-04 09:42:13,275] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [9.0078197e-12 2.5385404e-11 2.2254452e-23 1.9434942e-12 1.1556935e-12
 1.7042083e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:13,275] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4805
[2019-04-04 09:42:13,323] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 45.66666666666667, 66.83333333333334, 51.0, 26.0, 24.94797735317678, 0.2844648081251537, 0.0, 1.0, 45660.27675235552], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2392800.0000, 
sim time next is 2393400.0000, 
raw observation next is [-0.5, 45.33333333333333, 54.66666666666667, 44.0, 26.0, 24.95061022506581, 0.2839013695858044, 0.0, 1.0, 39577.8053111454], 
processed observation next is [0.0, 0.6956521739130435, 0.44875346260387816, 0.4533333333333333, 0.18222222222222223, 0.04861878453038674, 0.6666666666666666, 0.5792175187554841, 0.5946337898619348, 0.0, 1.0, 0.18846573957688284], 
reward next is 0.8115, 
noisyNet noise sample is [array([-0.980013], dtype=float32), 0.5308835]. 
=============================================
[2019-04-04 09:42:13,381] A3C_AGENT_WORKER-Thread-20 INFO:Local step 216500, global step 3463827: loss 0.3615
[2019-04-04 09:42:13,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 216500, global step 3463827: learning rate 0.0000
[2019-04-04 09:42:14,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1341773e-12 7.6368504e-12 6.9171316e-24 8.0982589e-13 3.4040086e-13
 2.2845042e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:14,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9171
[2019-04-04 09:42:14,243] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 56.0, 0.0, 0.0, 26.0, 25.00152839940588, 0.3570679732289589, 0.0, 1.0, 18730.56746857888], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2318400.0000, 
sim time next is 2319000.0000, 
raw observation next is [-1.7, 55.66666666666667, 0.0, 0.0, 26.0, 25.03416955601566, 0.3545413170480947, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4155124653739613, 0.5566666666666668, 0.0, 0.0, 0.6666666666666666, 0.5861807963346383, 0.6181804390160316, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6052508], dtype=float32), 0.096855216]. 
=============================================
[2019-04-04 09:42:14,276] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[81.24131 ]
 [83.71281 ]
 [79.05798 ]
 [79.36892 ]
 [80.886505]], R is [[84.65934753]
 [84.72355652]
 [84.66854858]
 [84.42629242]
 [84.02429199]].
[2019-04-04 09:42:14,482] A3C_AGENT_WORKER-Thread-19 INFO:Local step 216500, global step 3464178: loss 0.3768
[2019-04-04 09:42:14,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 216500, global step 3464178: learning rate 0.0000
[2019-04-04 09:42:14,491] A3C_AGENT_WORKER-Thread-3 INFO:Local step 216500, global step 3464182: loss 0.3701
[2019-04-04 09:42:14,492] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 216500, global step 3464182: learning rate 0.0000
[2019-04-04 09:42:15,707] A3C_AGENT_WORKER-Thread-15 INFO:Local step 216500, global step 3464597: loss 0.3723
[2019-04-04 09:42:15,711] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 216500, global step 3464597: learning rate 0.0000
[2019-04-04 09:42:15,780] A3C_AGENT_WORKER-Thread-16 INFO:Local step 216500, global step 3464619: loss 0.3698
[2019-04-04 09:42:15,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 216500, global step 3464620: learning rate 0.0000
[2019-04-04 09:42:16,774] A3C_AGENT_WORKER-Thread-17 INFO:Local step 216500, global step 3465000: loss 0.3934
[2019-04-04 09:42:16,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 216500, global step 3465001: learning rate 0.0000
[2019-04-04 09:42:17,026] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.44357088e-13 6.77687507e-13 1.23774815e-25 4.12152036e-14
 1.38948115e-14 8.11878102e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 09:42:17,027] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7702
[2019-04-04 09:42:17,042] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 46.33333333333334, 195.6666666666667, 155.3333333333333, 26.0, 25.26126112212711, 0.406638695169429, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2645400.0000, 
sim time next is 2646000.0000, 
raw observation next is [0.5, 47.0, 185.5, 168.0, 26.0, 25.58253824014778, 0.4385059889822889, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.47, 0.6183333333333333, 0.1856353591160221, 0.6666666666666666, 0.6318781866789817, 0.6461686629940963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8932326], dtype=float32), -0.58412206]. 
=============================================
[2019-04-04 09:42:17,049] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[86.29084 ]
 [86.89028 ]
 [87.266365]
 [87.348946]
 [87.36425 ]], R is [[86.04897308]
 [86.18848419]
 [86.32659912]
 [86.46333313]
 [86.59870148]].
[2019-04-04 09:42:17,672] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.8032533e-12 6.4986815e-11 1.1202543e-23 9.8408120e-13 1.6181365e-12
 2.1271847e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:17,672] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0128
[2019-04-04 09:42:17,721] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.25, 50.0, 0.0, 0.0, 26.0, 25.02255241911284, 0.3822758262256021, 0.0, 1.0, 98800.88632481228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2579400.0000, 
sim time next is 2580000.0000, 
raw observation next is [-2.433333333333333, 52.0, 0.0, 0.0, 26.0, 25.11085527817831, 0.4046251918533503, 0.0, 1.0, 61779.19744075568], 
processed observation next is [1.0, 0.8695652173913043, 0.3951985226223454, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5925712731815258, 0.6348750639511168, 0.0, 1.0, 0.2941866544797889], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.6085015], dtype=float32), 0.026837533]. 
=============================================
[2019-04-04 09:42:17,730] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[79.09842 ]
 [78.64542 ]
 [78.062836]
 [77.825584]
 [77.44806 ]], R is [[79.26422119]
 [79.00109863]
 [78.45700836]
 [78.41411591]
 [78.26438904]].
[2019-04-04 09:42:18,208] A3C_AGENT_WORKER-Thread-4 INFO:Local step 216500, global step 3465581: loss 0.4134
[2019-04-04 09:42:18,209] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 216500, global step 3465581: learning rate 0.0000
[2019-04-04 09:42:19,771] A3C_AGENT_WORKER-Thread-11 INFO:Local step 216500, global step 3466064: loss 0.4526
[2019-04-04 09:42:19,772] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 216500, global step 3466064: learning rate 0.0000
[2019-04-04 09:42:22,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6283928e-13 1.3174365e-12 9.1516487e-26 2.6310949e-14 2.5966700e-14
 1.5979336e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:22,681] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9222
[2019-04-04 09:42:22,687] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.466666666666667, 28.0, 151.5, 287.6666666666667, 26.0, 25.84612039378364, 0.3817021212109628, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2558400.0000, 
sim time next is 2559000.0000, 
raw observation next is [3.383333333333333, 28.5, 144.0, 300.3333333333333, 26.0, 25.82298679259759, 0.378137008079257, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5563250230840259, 0.285, 0.48, 0.3318600368324125, 0.6666666666666666, 0.6519155660497992, 0.6260456693597524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1510706], dtype=float32), -0.93548536]. 
=============================================
[2019-04-04 09:42:22,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[87.66456]
 [87.45627]
 [87.50426]
 [87.60192]
 [87.91958]], R is [[87.9779129 ]
 [88.0981369 ]
 [88.21715546]
 [88.33498383]
 [88.45163727]].
[2019-04-04 09:42:22,965] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217000, global step 3467233: loss 0.0078
[2019-04-04 09:42:22,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217000, global step 3467234: learning rate 0.0000
[2019-04-04 09:42:23,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5245949e-10 3.5164927e-10 3.3087790e-22 1.4907974e-11 1.6529439e-11
 1.5056355e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:23,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7482
[2019-04-04 09:42:23,937] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 58.5, 15.33333333333333, 165.3333333333333, 26.0, 22.8201147340376, -0.2236800170369394, 0.0, 1.0, 44180.10964521149], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2447400.0000, 
sim time next is 2448000.0000, 
raw observation next is [-9.5, 58.0, 21.5, 228.0, 26.0, 22.8314615183268, -0.2198925687987227, 0.0, 1.0, 44076.79577135408], 
processed observation next is [0.0, 0.34782608695652173, 0.1994459833795014, 0.58, 0.07166666666666667, 0.25193370165745854, 0.6666666666666666, 0.40262179319390007, 0.4267024770670924, 0.0, 1.0, 0.20988950367311468], 
reward next is 0.7901, 
noisyNet noise sample is [array([-1.0425088], dtype=float32), 0.36092678]. 
=============================================
[2019-04-04 09:42:23,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[76.82167]
 [75.88227]
 [74.85412]
 [74.91876]
 [74.99599]], R is [[77.48297119]
 [77.49776459]
 [77.51216888]
 [77.526474  ]
 [77.5408783 ]].
[2019-04-04 09:42:23,975] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217000, global step 3467670: loss 0.0055
[2019-04-04 09:42:23,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217000, global step 3467670: learning rate 0.0000
[2019-04-04 09:42:25,580] A3C_AGENT_WORKER-Thread-10 INFO:Local step 216500, global step 3468199: loss 0.4500
[2019-04-04 09:42:25,582] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 216500, global step 3468200: learning rate 0.0000
[2019-04-04 09:42:25,605] A3C_AGENT_WORKER-Thread-13 INFO:Local step 216500, global step 3468204: loss 0.4512
[2019-04-04 09:42:25,606] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 216500, global step 3468204: learning rate 0.0000
[2019-04-04 09:42:26,108] A3C_AGENT_WORKER-Thread-14 INFO:Local step 216500, global step 3468371: loss 0.4329
[2019-04-04 09:42:26,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 216500, global step 3468372: learning rate 0.0000
[2019-04-04 09:42:26,221] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217000, global step 3468407: loss 0.0070
[2019-04-04 09:42:26,229] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217000, global step 3468409: learning rate 0.0000
[2019-04-04 09:42:26,858] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.5566846e-11 1.2559120e-10 3.9951381e-23 1.1361064e-12 7.0571960e-12
 2.7711035e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:26,858] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0803
[2019-04-04 09:42:26,905] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.80873738397342, 0.237053059124621, 0.0, 1.0, 41810.85104890822], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2771400.0000, 
sim time next is 2772000.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.76105796464427, 0.2224831998535401, 0.0, 1.0, 41709.76996661215], 
processed observation next is [1.0, 0.08695652173913043, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5634214970536892, 0.5741610666178467, 0.0, 1.0, 0.1986179522219626], 
reward next is 0.8014, 
noisyNet noise sample is [array([-0.16682035], dtype=float32), -1.0246887]. 
=============================================
[2019-04-04 09:42:26,907] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.35012 ]
 [79.34819 ]
 [79.318565]
 [79.28201 ]
 [79.26384 ]], R is [[79.34986877]
 [79.35727692]
 [79.36399078]
 [79.37000275]
 [79.3752594 ]].
[2019-04-04 09:42:27,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7058998e-13 1.9746383e-12 4.5835027e-25 1.1996625e-13 7.7609387e-14
 2.9944576e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:27,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8137
[2019-04-04 09:42:27,565] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8833333333333332, 48.16666666666667, 218.0, 168.3333333333333, 26.0, 24.39378074295448, 0.2998119942407746, 1.0, 1.0, 197762.2771956119], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2638200.0000, 
sim time next is 2638800.0000, 
raw observation next is [-0.6, 47.0, 204.5, 179.0, 26.0, 24.92923088517778, 0.3889045023061561, 1.0, 1.0, 65585.8610099359], 
processed observation next is [1.0, 0.5652173913043478, 0.44598337950138506, 0.47, 0.6816666666666666, 0.19779005524861878, 0.6666666666666666, 0.5774359070981484, 0.629634834102052, 1.0, 1.0, 0.31231362385683764], 
reward next is 0.6877, 
noisyNet noise sample is [array([-0.84029806], dtype=float32), -0.03890768]. 
=============================================
[2019-04-04 09:42:28,363] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1890007e-12 4.2885553e-11 1.6714230e-23 6.2904955e-13 6.9783534e-13
 4.1159720e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:28,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8060
[2019-04-04 09:42:28,400] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.1243201200029, 0.334301217318226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2836800.0000, 
sim time next is 2837400.0000, 
raw observation next is [2.0, 44.00000000000001, 0.0, 0.0, 26.0, 25.02018448515511, 0.3170445803483048, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.44000000000000006, 0.0, 0.0, 0.6666666666666666, 0.5850153737629258, 0.6056815267827683, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37524834], dtype=float32), 0.28066933]. 
=============================================
[2019-04-04 09:42:30,254] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217000, global step 3469855: loss 0.0055
[2019-04-04 09:42:30,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217000, global step 3469855: learning rate 0.0000
[2019-04-04 09:42:32,209] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9427564e-13 6.1345417e-13 1.0225621e-24 4.1004201e-14 4.4960238e-14
 1.8480436e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:32,210] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0435
[2019-04-04 09:42:32,231] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.266666666666667, 27.33333333333334, 169.0, 447.5, 26.0, 25.63608393242848, 0.3692025396751665, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2554800.0000, 
sim time next is 2555400.0000, 
raw observation next is [3.533333333333333, 26.66666666666667, 167.0, 413.0, 26.0, 25.78976682835471, 0.3643415257364966, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5604801477377656, 0.2666666666666667, 0.5566666666666666, 0.456353591160221, 0.6666666666666666, 0.6491472356962259, 0.6214471752454989, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2993348], dtype=float32), -2.0827546]. 
=============================================
[2019-04-04 09:42:32,242] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217000, global step 3470566: loss 0.0099
[2019-04-04 09:42:32,246] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217000, global step 3470566: learning rate 0.0000
[2019-04-04 09:42:35,740] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217000, global step 3471779: loss 0.0043
[2019-04-04 09:42:35,741] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217000, global step 3471779: learning rate 0.0000
[2019-04-04 09:42:36,405] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6216393e-13 4.9651277e-13 8.7977022e-25 1.5921731e-14 1.4463262e-14
 3.8238090e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:36,407] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5611
[2019-04-04 09:42:36,410] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217000, global step 3472029: loss 0.0050
[2019-04-04 09:42:36,412] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217000, global step 3472029: learning rate 0.0000
[2019-04-04 09:42:36,452] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 52.0, 86.0, 614.0, 26.0, 26.32949896040012, 0.6311462570544477, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2734200.0000, 
sim time next is 2734800.0000, 
raw observation next is [-3.333333333333333, 51.33333333333333, 80.66666666666667, 587.3333333333334, 26.0, 26.56466115468712, 0.6480115876086808, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.37026777469990774, 0.5133333333333333, 0.2688888888888889, 0.648987108655617, 0.6666666666666666, 0.7137217628905933, 0.7160038625362269, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4310852], dtype=float32), 0.37194237]. 
=============================================
[2019-04-04 09:42:37,395] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217000, global step 3472372: loss 0.0040
[2019-04-04 09:42:37,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217000, global step 3472372: learning rate 0.0000
[2019-04-04 09:42:38,543] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217000, global step 3472729: loss 0.0050
[2019-04-04 09:42:38,543] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217000, global step 3472729: learning rate 0.0000
[2019-04-04 09:42:38,811] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217000, global step 3472818: loss 0.0052
[2019-04-04 09:42:38,812] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217000, global step 3472818: learning rate 0.0000
[2019-04-04 09:42:39,428] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217000, global step 3473028: loss 0.0050
[2019-04-04 09:42:39,436] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217000, global step 3473031: learning rate 0.0000
[2019-04-04 09:42:40,054] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217000, global step 3473281: loss 0.0038
[2019-04-04 09:42:40,054] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217000, global step 3473281: learning rate 0.0000
[2019-04-04 09:42:42,527] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217000, global step 3474177: loss 0.0087
[2019-04-04 09:42:42,527] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217000, global step 3474177: learning rate 0.0000
[2019-04-04 09:42:43,636] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9552331e-13 1.9336145e-12 3.5561394e-25 2.7999329e-13 4.5536918e-14
 3.6960116e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:43,636] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8553
[2019-04-04 09:42:43,718] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.0, 91.0, 88.5, 471.0, 26.0, 26.08015618050031, 0.4238655192419358, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2710800.0000, 
sim time next is 2711400.0000, 
raw observation next is [-13.66666666666667, 88.50000000000001, 91.33333333333333, 518.0, 26.0, 26.01044182238433, 0.4162407229499998, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.08402585410895651, 0.8850000000000001, 0.3044444444444444, 0.5723756906077349, 0.6666666666666666, 0.6675368185320275, 0.63874690765, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.19714698], dtype=float32), -0.18120283]. 
=============================================
[2019-04-04 09:42:44,394] A3C_AGENT_WORKER-Thread-12 INFO:Local step 217500, global step 3474895: loss 0.2197
[2019-04-04 09:42:44,394] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 217500, global step 3474895: learning rate 0.0000
[2019-04-04 09:42:46,008] A3C_AGENT_WORKER-Thread-2 INFO:Local step 217500, global step 3475485: loss 0.2638
[2019-04-04 09:42:46,008] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 217500, global step 3475485: learning rate 0.0000
[2019-04-04 09:42:47,705] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217000, global step 3476032: loss 0.0088
[2019-04-04 09:42:47,716] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217000, global step 3476034: learning rate 0.0000
[2019-04-04 09:42:47,957] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217000, global step 3476118: loss 0.0057
[2019-04-04 09:42:47,959] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217000, global step 3476118: learning rate 0.0000
[2019-04-04 09:42:48,162] A3C_AGENT_WORKER-Thread-5 INFO:Local step 217500, global step 3476201: loss 0.2998
[2019-04-04 09:42:48,163] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 217500, global step 3476201: learning rate 0.0000
[2019-04-04 09:42:48,356] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0572527e-11 5.4905049e-11 6.1744452e-23 8.0121288e-13 5.2570014e-12
 2.1861734e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:48,364] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8530
[2019-04-04 09:42:48,432] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.7855760160398, 0.2505037022713388, 0.0, 1.0, 42218.74772118029], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2769600.0000, 
sim time next is 2770200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.78303818629344, 0.2471352488818172, 0.0, 1.0, 42073.26572856863], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5652531821911199, 0.582378416293939, 0.0, 1.0, 0.20034888442175539], 
reward next is 0.7997, 
noisyNet noise sample is [array([0.8160182], dtype=float32), -0.050550267]. 
=============================================
[2019-04-04 09:42:48,695] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217000, global step 3476391: loss 0.0054
[2019-04-04 09:42:48,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217000, global step 3476391: learning rate 0.0000
[2019-04-04 09:42:52,363] A3C_AGENT_WORKER-Thread-18 INFO:Local step 217500, global step 3477841: loss 0.2826
[2019-04-04 09:42:52,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 217500, global step 3477842: learning rate 0.0000
[2019-04-04 09:42:53,566] A3C_AGENT_WORKER-Thread-6 INFO:Local step 217500, global step 3478297: loss 0.2918
[2019-04-04 09:42:53,567] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 217500, global step 3478298: learning rate 0.0000
[2019-04-04 09:42:57,135] A3C_AGENT_WORKER-Thread-20 INFO:Local step 217500, global step 3479773: loss 0.3623
[2019-04-04 09:42:57,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 217500, global step 3479773: learning rate 0.0000
[2019-04-04 09:42:58,280] A3C_AGENT_WORKER-Thread-3 INFO:Local step 217500, global step 3480211: loss 0.3309
[2019-04-04 09:42:58,281] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 217500, global step 3480211: learning rate 0.0000
[2019-04-04 09:42:58,542] A3C_AGENT_WORKER-Thread-19 INFO:Local step 217500, global step 3480320: loss 0.3429
[2019-04-04 09:42:58,543] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 217500, global step 3480320: learning rate 0.0000
[2019-04-04 09:42:58,792] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6689161e-11 4.3500044e-11 2.9922658e-23 4.3029655e-12 3.7726493e-12
 2.4320937e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:42:58,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8957
[2019-04-04 09:42:58,821] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.24906032958831, 0.1079555900073748, 0.0, 1.0, 39340.94100900415], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3034800.0000, 
sim time next is 3035400.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.21406333171262, 0.1046082741597717, 0.0, 1.0, 39491.52241072408], 
processed observation next is [0.0, 0.13043478260869565, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5178386109760517, 0.5348694247199238, 0.0, 1.0, 0.18805486862249562], 
reward next is 0.8119, 
noisyNet noise sample is [array([-0.41906518], dtype=float32), -0.19073646]. 
=============================================
[2019-04-04 09:42:59,779] A3C_AGENT_WORKER-Thread-16 INFO:Local step 217500, global step 3480834: loss 0.3421
[2019-04-04 09:42:59,779] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 217500, global step 3480834: learning rate 0.0000
[2019-04-04 09:42:59,941] A3C_AGENT_WORKER-Thread-15 INFO:Local step 217500, global step 3480903: loss 0.3296
[2019-04-04 09:42:59,941] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 217500, global step 3480903: learning rate 0.0000
[2019-04-04 09:43:00,422] A3C_AGENT_WORKER-Thread-17 INFO:Local step 217500, global step 3481112: loss 0.3218
[2019-04-04 09:43:00,423] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 217500, global step 3481113: learning rate 0.0000
[2019-04-04 09:43:01,073] A3C_AGENT_WORKER-Thread-4 INFO:Local step 217500, global step 3481372: loss 0.3177
[2019-04-04 09:43:01,075] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 217500, global step 3481372: learning rate 0.0000
[2019-04-04 09:43:03,308] A3C_AGENT_WORKER-Thread-11 INFO:Local step 217500, global step 3482302: loss 0.4040
[2019-04-04 09:43:03,310] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 217500, global step 3482303: learning rate 0.0000
[2019-04-04 09:43:03,315] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218000, global step 3482309: loss 0.1257
[2019-04-04 09:43:03,316] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218000, global step 3482309: learning rate 0.0000
[2019-04-04 09:43:04,133] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218000, global step 3482658: loss 0.1681
[2019-04-04 09:43:04,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218000, global step 3482658: learning rate 0.0000
[2019-04-04 09:43:06,370] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218000, global step 3483765: loss 0.2045
[2019-04-04 09:43:06,371] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218000, global step 3483765: learning rate 0.0000
[2019-04-04 09:43:06,917] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7032924e-14 1.0691028e-12 7.6781484e-25 2.2314614e-14 8.3016927e-14
 3.8269025e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:43:06,918] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3187
[2019-04-04 09:43:06,928] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 0.0, 0.0, 26.0, 26.71841129377468, 0.8654679145053209, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3174600.0000, 
sim time next is 3175200.0000, 
raw observation next is [6.0, 100.0, 0.0, 0.0, 26.0, 26.92412069699157, 0.8689484737004403, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.6288088642659281, 1.0, 0.0, 0.0, 0.6666666666666666, 0.7436767247492974, 0.78964949123348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8550677], dtype=float32), -2.0946574]. 
=============================================
[2019-04-04 09:43:08,412] A3C_AGENT_WORKER-Thread-10 INFO:Local step 217500, global step 3484774: loss 0.2789
[2019-04-04 09:43:08,413] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 217500, global step 3484774: learning rate 0.0000
[2019-04-04 09:43:08,532] A3C_AGENT_WORKER-Thread-13 INFO:Local step 217500, global step 3484831: loss 0.2839
[2019-04-04 09:43:08,533] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 217500, global step 3484831: learning rate 0.0000
[2019-04-04 09:43:09,007] A3C_AGENT_WORKER-Thread-14 INFO:Local step 217500, global step 3485026: loss 0.2651
[2019-04-04 09:43:09,008] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 217500, global step 3485026: learning rate 0.0000
[2019-04-04 09:43:10,256] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218000, global step 3485607: loss 0.2623
[2019-04-04 09:43:10,257] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218000, global step 3485607: learning rate 0.0000
[2019-04-04 09:43:10,266] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.6634108e-13 2.2436011e-12 8.4196495e-25 4.4472469e-14 4.6440723e-14
 7.8901876e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:43:10,269] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9822
[2019-04-04 09:43:10,281] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 61.0, 513.0, 26.0, 26.13147025168256, 0.6157556568732404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3429000.0000, 
sim time next is 3429600.0000, 
raw observation next is [2.0, 67.0, 52.83333333333334, 447.6666666666667, 26.0, 26.33051869446398, 0.6259984155822239, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.17611111111111113, 0.4946593001841621, 0.6666666666666666, 0.6942098912053316, 0.7086661385274079, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7484204], dtype=float32), -2.0736775]. 
=============================================
[2019-04-04 09:43:10,357] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218000, global step 3485653: loss 0.2774
[2019-04-04 09:43:10,358] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218000, global step 3485653: learning rate 0.0000
[2019-04-04 09:43:14,937] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218000, global step 3487813: loss 0.3090
[2019-04-04 09:43:14,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218000, global step 3487813: learning rate 0.0000
[2019-04-04 09:43:15,302] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218000, global step 3487978: loss 0.3055
[2019-04-04 09:43:15,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218000, global step 3487978: learning rate 0.0000
[2019-04-04 09:43:15,707] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9031790e-12 3.5637424e-11 4.6974853e-23 1.1273168e-12 3.2407406e-12
 2.8660895e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:43:15,710] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3663
[2019-04-04 09:43:15,726] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.666666666666667, 75.0, 0.0, 0.0, 26.0, 25.08904066718254, 0.3781800287084036, 0.0, 1.0, 41630.78543731099], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3368400.0000, 
sim time next is 3369000.0000, 
raw observation next is [-5.833333333333333, 76.0, 0.0, 0.0, 26.0, 25.06010969907096, 0.3687588391246023, 0.0, 1.0, 41565.88720247196], 
processed observation next is [1.0, 1.0, 0.30101569713758086, 0.76, 0.0, 0.0, 0.6666666666666666, 0.58834247492258, 0.6229196130415341, 0.0, 1.0, 0.19793279620224744], 
reward next is 0.8021, 
noisyNet noise sample is [array([-0.7735904], dtype=float32), -1.6420698]. 
=============================================
[2019-04-04 09:43:15,733] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.164635]
 [79.29957 ]
 [79.44634 ]
 [79.41946 ]
 [79.40951 ]], R is [[79.02674103]
 [79.0382309 ]
 [79.04901123]
 [79.0582428 ]
 [79.0630722 ]].
[2019-04-04 09:43:15,939] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218000, global step 3488284: loss 0.2854
[2019-04-04 09:43:15,940] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218000, global step 3488285: learning rate 0.0000
[2019-04-04 09:43:16,675] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218000, global step 3488665: loss 0.2733
[2019-04-04 09:43:16,677] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218000, global step 3488666: learning rate 0.0000
[2019-04-04 09:43:17,489] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218000, global step 3489013: loss 0.2779
[2019-04-04 09:43:17,507] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218000, global step 3489013: learning rate 0.0000
[2019-04-04 09:43:17,597] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218000, global step 3489056: loss 0.2609
[2019-04-04 09:43:17,601] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218000, global step 3489057: learning rate 0.0000
[2019-04-04 09:43:18,001] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218000, global step 3489223: loss 0.2259
[2019-04-04 09:43:18,002] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218000, global step 3489225: learning rate 0.0000
[2019-04-04 09:43:18,238] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7009455e-13 7.9504605e-13 9.8971796e-26 3.6609069e-14 9.9921739e-14
 5.2286583e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:43:18,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1988
[2019-04-04 09:43:18,291] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 92.0, 87.66666666666667, 417.1666666666667, 26.0, 25.78166205702883, 0.5760984425395571, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3228000.0000, 
sim time next is 3228600.0000, 
raw observation next is [-3.0, 92.0, 90.33333333333333, 464.3333333333333, 26.0, 25.90785620746978, 0.5948292413311294, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3795013850415513, 0.92, 0.3011111111111111, 0.5130755064456721, 0.6666666666666666, 0.6589880172891484, 0.6982764137770431, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.40833095], dtype=float32), 1.6200823]. 
=============================================
[2019-04-04 09:43:20,067] A3C_AGENT_WORKER-Thread-12 INFO:Local step 218500, global step 3490227: loss 0.7062
[2019-04-04 09:43:20,067] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 218500, global step 3490227: learning rate 0.0000
[2019-04-04 09:43:20,293] A3C_AGENT_WORKER-Thread-2 INFO:Local step 218500, global step 3490327: loss 0.7094
[2019-04-04 09:43:20,295] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 218500, global step 3490328: learning rate 0.0000
[2019-04-04 09:43:20,446] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218000, global step 3490404: loss 0.1873
[2019-04-04 09:43:20,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218000, global step 3490407: learning rate 0.0000
[2019-04-04 09:43:21,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6229870e-11 3.2714904e-11 1.6554074e-23 2.2061773e-12 2.3599137e-12
 3.9535835e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:43:21,426] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2685
[2019-04-04 09:43:21,440] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.166666666666667, 65.83333333333334, 0.0, 0.0, 26.0, 24.85047182274906, 0.3041705778640142, 0.0, 1.0, 40876.26200981422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3561000.0000, 
sim time next is 3561600.0000, 
raw observation next is [-5.333333333333334, 66.66666666666667, 0.0, 0.0, 26.0, 24.79758124861742, 0.2939922341627513, 0.0, 1.0, 40881.9426082577], 
processed observation next is [0.0, 0.21739130434782608, 0.31486611265004616, 0.6666666666666667, 0.0, 0.0, 0.6666666666666666, 0.5664651040514516, 0.5979974113875838, 0.0, 1.0, 0.19467591718217953], 
reward next is 0.8053, 
noisyNet noise sample is [array([-0.27093655], dtype=float32), 1.194004]. 
=============================================
[2019-04-04 09:43:22,672] A3C_AGENT_WORKER-Thread-5 INFO:Local step 218500, global step 3491556: loss 0.7726
[2019-04-04 09:43:22,673] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 218500, global step 3491556: learning rate 0.0000
[2019-04-04 09:43:25,538] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218000, global step 3493073: loss 0.1434
[2019-04-04 09:43:25,539] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218000, global step 3493073: learning rate 0.0000
[2019-04-04 09:43:25,599] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218000, global step 3493111: loss 0.1527
[2019-04-04 09:43:25,601] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218000, global step 3493112: learning rate 0.0000
[2019-04-04 09:43:25,844] A3C_AGENT_WORKER-Thread-6 INFO:Local step 218500, global step 3493252: loss 0.8157
[2019-04-04 09:43:25,847] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 218500, global step 3493255: learning rate 0.0000
[2019-04-04 09:43:26,108] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218000, global step 3493392: loss 0.1658
[2019-04-04 09:43:26,109] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218000, global step 3493393: learning rate 0.0000
[2019-04-04 09:43:26,633] A3C_AGENT_WORKER-Thread-18 INFO:Local step 218500, global step 3493686: loss 0.8464
[2019-04-04 09:43:26,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 218500, global step 3493686: learning rate 0.0000
[2019-04-04 09:43:27,164] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6605111e-12 4.6684644e-12 2.4554040e-25 1.8090626e-13 3.4455335e-13
 1.7021144e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:43:27,164] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1346
[2019-04-04 09:43:27,213] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 59.5, 111.0, 758.0, 26.0, 25.57117083339504, 0.4816104547708062, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3580200.0000, 
sim time next is 3580800.0000, 
raw observation next is [-4.333333333333334, 57.66666666666667, 111.5, 767.6666666666667, 26.0, 25.50010391073022, 0.471965352702187, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.3425669436749769, 0.5766666666666667, 0.37166666666666665, 0.8482504604051566, 0.6666666666666666, 0.6250086592275185, 0.6573217842340623, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.97253376], dtype=float32), -0.25120148]. 
=============================================
[2019-04-04 09:43:34,455] A3C_AGENT_WORKER-Thread-19 INFO:Local step 218500, global step 3496076: loss 0.9069
[2019-04-04 09:43:34,492] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 218500, global step 3496076: learning rate 0.0000
[2019-04-04 09:43:34,532] A3C_AGENT_WORKER-Thread-3 INFO:Local step 218500, global step 3496091: loss 0.8855
[2019-04-04 09:43:34,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 218500, global step 3496091: learning rate 0.0000
[2019-04-04 09:43:34,797] A3C_AGENT_WORKER-Thread-20 INFO:Local step 218500, global step 3496158: loss 0.8943
[2019-04-04 09:43:34,804] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 218500, global step 3496159: learning rate 0.0000
[2019-04-04 09:43:36,586] A3C_AGENT_WORKER-Thread-16 INFO:Local step 218500, global step 3496607: loss 0.9273
[2019-04-04 09:43:36,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 218500, global step 3496607: learning rate 0.0000
[2019-04-04 09:43:38,303] A3C_AGENT_WORKER-Thread-15 INFO:Local step 218500, global step 3497144: loss 0.9228
[2019-04-04 09:43:38,304] A3C_AGENT_WORKER-Thread-17 INFO:Local step 218500, global step 3497144: loss 0.9442
[2019-04-04 09:43:38,357] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 218500, global step 3497153: learning rate 0.0000
[2019-04-04 09:43:38,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 218500, global step 3497159: learning rate 0.0000
[2019-04-04 09:43:38,589] A3C_AGENT_WORKER-Thread-4 INFO:Local step 218500, global step 3497228: loss 0.9360
[2019-04-04 09:43:38,625] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 218500, global step 3497228: learning rate 0.0000
[2019-04-04 09:43:41,672] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219000, global step 3498156: loss 35.5569
[2019-04-04 09:43:41,672] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219000, global step 3498156: learning rate 0.0000
[2019-04-04 09:43:42,330] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219000, global step 3498328: loss 35.5213
[2019-04-04 09:43:42,330] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219000, global step 3498328: learning rate 0.0000
[2019-04-04 09:43:42,964] A3C_AGENT_WORKER-Thread-11 INFO:Local step 218500, global step 3498521: loss 1.0613
[2019-04-04 09:43:42,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 218500, global step 3498521: learning rate 0.0000
[2019-04-04 09:43:46,390] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219000, global step 3499495: loss 35.9099
[2019-04-04 09:43:46,392] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219000, global step 3499495: learning rate 0.0000
[2019-04-04 09:43:48,297] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-04 09:43:48,299] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:43:48,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:43:48,309] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:43:48,310] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:43:48,311] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:43:48,320] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:43:48,354] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run36
[2019-04-04 09:43:48,425] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run36
[2019-04-04 09:43:48,562] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run36
[2019-04-04 09:46:18,810] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.17151088], dtype=float32), -0.25374347]
[2019-04-04 09:46:18,810] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [8.783333333333333, 72.16666666666667, 179.0, 197.0, 26.0, 24.95508026526808, 0.4268732975990654, 0.0, 1.0, 0.0]
[2019-04-04 09:46:18,810] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 09:46:18,811] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.48240369e-13 1.05476066e-12 9.23885493e-25 7.06287107e-14
 1.03625686e-13 4.73786820e-16 1.00000000e+00], sampled 0.2848895462077208
[2019-04-04 09:46:57,102] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 09:47:28,126] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 09:47:33,615] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:47:34,658] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 3500000, evaluation results [3500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:47:37,208] A3C_AGENT_WORKER-Thread-10 INFO:Local step 218500, global step 3500800: loss 1.2206
[2019-04-04 09:47:37,208] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 218500, global step 3500800: learning rate 0.0000
[2019-04-04 09:47:38,349] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219000, global step 3501199: loss 36.7283
[2019-04-04 09:47:38,352] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219000, global step 3501199: learning rate 0.0000
[2019-04-04 09:47:38,774] A3C_AGENT_WORKER-Thread-13 INFO:Local step 218500, global step 3501348: loss 1.2503
[2019-04-04 09:47:38,774] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 218500, global step 3501348: learning rate 0.0000
[2019-04-04 09:47:38,857] A3C_AGENT_WORKER-Thread-14 INFO:Local step 218500, global step 3501370: loss 1.2500
[2019-04-04 09:47:38,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 218500, global step 3501370: learning rate 0.0000
[2019-04-04 09:47:38,963] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219000, global step 3501407: loss 36.6019
[2019-04-04 09:47:38,989] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219000, global step 3501407: learning rate 0.0000
[2019-04-04 09:47:41,603] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5287932e-11 3.7581223e-11 6.5083884e-24 1.4065990e-12 1.1062254e-12
 1.1935157e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:47:41,610] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4817
[2019-04-04 09:47:41,628] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.29114024272175, 0.4664981747624774, 0.0, 1.0, 46428.91918482632], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3884400.0000, 
sim time next is 3885000.0000, 
raw observation next is [-1.166666666666667, 60.83333333333333, 0.0, 0.0, 26.0, 25.3719793983514, 0.4702319291697075, 0.0, 1.0, 42181.64105696402], 
processed observation next is [1.0, 1.0, 0.43028624192059095, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.6143316165292833, 0.6567439763899025, 0.0, 1.0, 0.2008649574141144], 
reward next is 0.7991, 
noisyNet noise sample is [array([2.0097191], dtype=float32), 0.6514701]. 
=============================================
[2019-04-04 09:47:41,705] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.60363 ]
 [81.556694]
 [81.51774 ]
 [81.21494 ]
 [80.71772 ]], R is [[81.66458893]
 [81.62685394]
 [81.5316391 ]
 [81.27925873]
 [80.75084686]].
[2019-04-04 09:47:43,973] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3526131e-11 7.5300953e-11 3.0860851e-23 4.4624018e-12 7.3935502e-12
 1.2918822e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:47:43,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3376
[2019-04-04 09:47:44,005] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39282942398051, 0.4108803480924597, 0.0, 1.0, 60800.41158737372], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3894000.0000, 
sim time next is 3894600.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.31862039561346, 0.4047185117648353, 0.0, 1.0, 61982.47077916178], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6098850329677884, 0.6349061705882785, 0.0, 1.0, 0.2951546227579132], 
reward next is 0.7048, 
noisyNet noise sample is [array([-0.67009485], dtype=float32), -0.34418452]. 
=============================================
[2019-04-04 09:47:46,117] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219000, global step 3503751: loss 36.8536
[2019-04-04 09:47:46,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219000, global step 3503751: learning rate 0.0000
[2019-04-04 09:47:46,692] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219000, global step 3503942: loss 37.9152
[2019-04-04 09:47:46,694] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219000, global step 3503943: learning rate 0.0000
[2019-04-04 09:47:46,843] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8394403e-14 1.2675723e-13 8.4286383e-27 5.8991423e-15 4.9782752e-15
 1.2319779e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:47:46,844] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1306
[2019-04-04 09:47:46,875] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 100.0, 744.5, 26.0, 27.09305948784415, 0.7676522397180808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4114800.0000, 
sim time next is 4115400.0000, 
raw observation next is [4.0, 35.0, 98.0, 728.0, 26.0, 27.15546292257693, 0.7817550602941606, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.32666666666666666, 0.8044198895027624, 0.6666666666666666, 0.7629552435480775, 0.7605850200980536, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.45103213], dtype=float32), -0.24119468]. 
=============================================
[2019-04-04 09:47:48,168] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219000, global step 3504426: loss 37.2993
[2019-04-04 09:47:48,173] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219000, global step 3504426: learning rate 0.0000
[2019-04-04 09:47:48,437] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8169819e-12 1.2539558e-11 1.0463050e-23 4.2160278e-13 5.5527197e-13
 3.0604582e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:47:48,438] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5933
[2019-04-04 09:47:48,449] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.60529104457443, 0.5238181528440676, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3792600.0000, 
sim time next is 3793200.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.55353811799841, 0.5128233472634022, 0.0, 1.0, 44010.22838853925], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6294615098332009, 0.6709411157544674, 0.0, 1.0, 0.2095725161359012], 
reward next is 0.7904, 
noisyNet noise sample is [array([0.31878555], dtype=float32), -0.09438342]. 
=============================================
[2019-04-04 09:47:48,515] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219000, global step 3504540: loss 37.0264
[2019-04-04 09:47:48,516] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219000, global step 3504540: learning rate 0.0000
[2019-04-04 09:47:49,152] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219000, global step 3504742: loss 37.3051
[2019-04-04 09:47:49,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219000, global step 3504742: learning rate 0.0000
[2019-04-04 09:47:50,801] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219000, global step 3505221: loss 36.8916
[2019-04-04 09:47:50,814] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219000, global step 3505221: learning rate 0.0000
[2019-04-04 09:47:51,269] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219000, global step 3505371: loss 37.2238
[2019-04-04 09:47:51,274] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219000, global step 3505375: learning rate 0.0000
[2019-04-04 09:47:52,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9940609e-14 5.2604693e-13 4.2905353e-26 6.5766022e-15 4.6849773e-15
 5.1690636e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:47:52,592] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7796
[2019-04-04 09:47:52,612] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 46.0, 83.16666666666666, 689.3333333333333, 26.0, 26.6443361617717, 0.7220095177900901, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3858000.0000, 
sim time next is 3858600.0000, 
raw observation next is [2.833333333333333, 45.5, 79.33333333333334, 661.6666666666667, 26.0, 26.78212967799563, 0.7386493810719074, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.541089566020314, 0.455, 0.2644444444444445, 0.7311233885819522, 0.6666666666666666, 0.7318441398329693, 0.7462164603573025, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.51825833], dtype=float32), 0.4782582]. 
=============================================
[2019-04-04 09:47:53,191] A3C_AGENT_WORKER-Thread-12 INFO:Local step 219500, global step 3505986: loss 0.0412
[2019-04-04 09:47:53,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 219500, global step 3505989: learning rate 0.0000
[2019-04-04 09:47:53,804] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219000, global step 3506182: loss 38.2252
[2019-04-04 09:47:53,808] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219000, global step 3506182: learning rate 0.0000
[2019-04-04 09:47:54,109] A3C_AGENT_WORKER-Thread-2 INFO:Local step 219500, global step 3506272: loss 0.0353
[2019-04-04 09:47:54,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 219500, global step 3506273: learning rate 0.0000
[2019-04-04 09:47:58,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.7944604e-12 1.0346345e-11 2.7855869e-25 2.3287690e-13 2.8308418e-13
 3.1018050e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:47:58,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6362
[2019-04-04 09:47:58,287] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 40.0, 108.0, 660.0, 26.0, 25.53769251314317, 0.4222099034391429, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4181400.0000, 
sim time next is 4182000.0000, 
raw observation next is [-2.666666666666667, 38.33333333333334, 109.0, 679.0, 26.0, 25.50051825714764, 0.4161444791017324, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.38873499538319484, 0.3833333333333334, 0.36333333333333334, 0.7502762430939226, 0.6666666666666666, 0.6250431880956366, 0.6387148263672441, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0275425], dtype=float32), -0.5387776]. 
=============================================
[2019-04-04 09:47:58,321] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.06059]
 [86.66819]
 [86.27885]
 [86.22906]
 [86.37519]], R is [[87.54026794]
 [87.66486359]
 [87.78821564]
 [87.91033173]
 [88.03122711]].
[2019-04-04 09:47:58,413] A3C_AGENT_WORKER-Thread-5 INFO:Local step 219500, global step 3507634: loss 0.0341
[2019-04-04 09:47:58,414] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 219500, global step 3507634: learning rate 0.0000
[2019-04-04 09:47:58,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4515170e-13 2.9283165e-13 2.2508490e-27 1.0788852e-14 3.1737653e-15
 7.4138385e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:47:58,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5919
[2019-04-04 09:47:58,750] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 35.5, 107.3333333333333, 709.0, 26.0, 26.35210976064098, 0.5425145574333192, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4096200.0000, 
sim time next is 4096800.0000, 
raw observation next is [-2.0, 35.0, 109.0, 724.0, 26.0, 26.44083596781659, 0.5639495359215754, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.40720221606648205, 0.35, 0.36333333333333334, 0.8, 0.6666666666666666, 0.7034029973180491, 0.6879831786405252, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.42094904], dtype=float32), 0.03994834]. 
=============================================
[2019-04-04 09:48:01,073] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219000, global step 3508673: loss 38.1147
[2019-04-04 09:48:01,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219000, global step 3508673: learning rate 0.0000
[2019-04-04 09:48:01,687] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219000, global step 3508946: loss 38.3284
[2019-04-04 09:48:01,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219000, global step 3508948: learning rate 0.0000
[2019-04-04 09:48:01,746] A3C_AGENT_WORKER-Thread-6 INFO:Local step 219500, global step 3508965: loss 0.0379
[2019-04-04 09:48:01,747] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 219500, global step 3508965: learning rate 0.0000
[2019-04-04 09:48:02,296] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219000, global step 3509203: loss 38.4665
[2019-04-04 09:48:02,300] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219000, global step 3509205: learning rate 0.0000
[2019-04-04 09:48:02,740] A3C_AGENT_WORKER-Thread-18 INFO:Local step 219500, global step 3509431: loss 0.0342
[2019-04-04 09:48:02,742] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 219500, global step 3509431: learning rate 0.0000
[2019-04-04 09:48:03,047] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.0939732e-14 2.9810757e-13 3.0824330e-26 3.1599693e-15 5.3099903e-15
 4.3168882e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:03,048] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9107
[2019-04-04 09:48:03,063] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 31.0, 111.0, 812.0, 26.0, 26.29804224187465, 0.6588967370323069, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4111200.0000, 
sim time next is 4111800.0000, 
raw observation next is [3.166666666666667, 31.66666666666667, 109.3333333333333, 806.0, 26.0, 26.63457215665083, 0.6937527007454812, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5503231763619576, 0.3166666666666667, 0.36444444444444435, 0.8906077348066298, 0.6666666666666666, 0.7195476797209025, 0.7312509002484937, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21781343], dtype=float32), -0.021697182]. 
=============================================
[2019-04-04 09:48:05,808] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8258825e-13 2.0041664e-12 1.5576271e-25 1.7572395e-14 1.7093885e-13
 5.1667993e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:05,809] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4431
[2019-04-04 09:48:05,845] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 35.0, 114.8333333333333, 782.0, 26.0, 25.213339138749, 0.3892989717825463, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4185600.0000, 
sim time next is 4186200.0000, 
raw observation next is [-1.166666666666667, 35.0, 115.6666666666667, 790.0, 26.0, 25.17468684356895, 0.3834087449722499, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.43478260869565216, 0.43028624192059095, 0.35, 0.38555555555555565, 0.8729281767955801, 0.6666666666666666, 0.5978905702974124, 0.6278029149907499, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7540915], dtype=float32), 0.8920363]. 
=============================================
[2019-04-04 09:48:07,594] A3C_AGENT_WORKER-Thread-20 INFO:Local step 219500, global step 3511620: loss 0.0277
[2019-04-04 09:48:07,595] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 219500, global step 3511620: learning rate 0.0000
[2019-04-04 09:48:08,123] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8422826e-12 6.3462027e-11 2.6257275e-24 9.6969882e-13 4.1369040e-13
 6.3612899e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:08,126] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5933
[2019-04-04 09:48:08,141] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 31.0, 0.0, 0.0, 26.0, 25.46098558328369, 0.4853110379540038, 0.0, 1.0, 64010.81647600716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4053600.0000, 
sim time next is 4054200.0000, 
raw observation next is [-5.166666666666667, 32.0, 0.0, 0.0, 26.0, 25.45984417683852, 0.4394693407686881, 0.0, 1.0, 48412.21203549852], 
processed observation next is [1.0, 0.9565217391304348, 0.31948291782086796, 0.32, 0.0, 0.0, 0.6666666666666666, 0.62165368140321, 0.6464897802562294, 0.0, 1.0, 0.23053434302618345], 
reward next is 0.7695, 
noisyNet noise sample is [array([0.07961363], dtype=float32), -0.57664555]. 
=============================================
[2019-04-04 09:48:08,483] A3C_AGENT_WORKER-Thread-19 INFO:Local step 219500, global step 3512066: loss 0.0311
[2019-04-04 09:48:08,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 219500, global step 3512066: learning rate 0.0000
[2019-04-04 09:48:09,670] A3C_AGENT_WORKER-Thread-3 INFO:Local step 219500, global step 3512660: loss 0.0384
[2019-04-04 09:48:09,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 219500, global step 3512662: learning rate 0.0000
[2019-04-04 09:48:09,872] A3C_AGENT_WORKER-Thread-16 INFO:Local step 219500, global step 3512752: loss 0.0344
[2019-04-04 09:48:09,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 219500, global step 3512752: learning rate 0.0000
[2019-04-04 09:48:10,113] A3C_AGENT_WORKER-Thread-17 INFO:Local step 219500, global step 3512863: loss 0.0348
[2019-04-04 09:48:10,114] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 219500, global step 3512865: learning rate 0.0000
[2019-04-04 09:48:10,875] A3C_AGENT_WORKER-Thread-15 INFO:Local step 219500, global step 3513219: loss 0.0264
[2019-04-04 09:48:10,878] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 219500, global step 3513220: learning rate 0.0000
[2019-04-04 09:48:11,364] A3C_AGENT_WORKER-Thread-4 INFO:Local step 219500, global step 3513485: loss 0.0259
[2019-04-04 09:48:11,366] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 219500, global step 3513486: learning rate 0.0000
[2019-04-04 09:48:11,779] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220000, global step 3513707: loss 0.8360
[2019-04-04 09:48:11,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220000, global step 3513707: learning rate 0.0000
[2019-04-04 09:48:12,488] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220000, global step 3514060: loss 0.8918
[2019-04-04 09:48:12,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220000, global step 3514060: learning rate 0.0000
[2019-04-04 09:48:13,812] A3C_AGENT_WORKER-Thread-11 INFO:Local step 219500, global step 3514755: loss 0.0141
[2019-04-04 09:48:13,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 219500, global step 3514756: learning rate 0.0000
[2019-04-04 09:48:14,743] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220000, global step 3515211: loss 0.9902
[2019-04-04 09:48:14,744] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220000, global step 3515211: learning rate 0.0000
[2019-04-04 09:48:17,500] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220000, global step 3516654: loss 0.9015
[2019-04-04 09:48:17,500] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220000, global step 3516654: learning rate 0.0000
[2019-04-04 09:48:18,347] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220000, global step 3517102: loss 0.8441
[2019-04-04 09:48:18,348] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220000, global step 3517102: learning rate 0.0000
[2019-04-04 09:48:18,622] A3C_AGENT_WORKER-Thread-10 INFO:Local step 219500, global step 3517237: loss 0.0195
[2019-04-04 09:48:18,624] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 219500, global step 3517238: learning rate 0.0000
[2019-04-04 09:48:19,531] A3C_AGENT_WORKER-Thread-13 INFO:Local step 219500, global step 3517707: loss 0.0170
[2019-04-04 09:48:19,532] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 219500, global step 3517707: learning rate 0.0000
[2019-04-04 09:48:20,093] A3C_AGENT_WORKER-Thread-14 INFO:Local step 219500, global step 3517976: loss 0.0118
[2019-04-04 09:48:20,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 219500, global step 3517976: learning rate 0.0000
[2019-04-04 09:48:23,228] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220000, global step 3519458: loss 1.0311
[2019-04-04 09:48:23,230] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220000, global step 3519458: learning rate 0.0000
[2019-04-04 09:48:23,987] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220000, global step 3519827: loss 0.9520
[2019-04-04 09:48:23,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220000, global step 3519828: learning rate 0.0000
[2019-04-04 09:48:25,172] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220000, global step 3520345: loss 0.8909
[2019-04-04 09:48:25,190] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220000, global step 3520347: learning rate 0.0000
[2019-04-04 09:48:25,576] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220000, global step 3520508: loss 0.8549
[2019-04-04 09:48:25,576] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220000, global step 3520508: learning rate 0.0000
[2019-04-04 09:48:25,668] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220000, global step 3520556: loss 0.8840
[2019-04-04 09:48:25,693] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220000, global step 3520558: learning rate 0.0000
[2019-04-04 09:48:26,534] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220000, global step 3520993: loss 0.8868
[2019-04-04 09:48:26,534] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220000, global step 3520993: learning rate 0.0000
[2019-04-04 09:48:27,491] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220000, global step 3521489: loss 0.9105
[2019-04-04 09:48:27,493] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220000, global step 3521489: learning rate 0.0000
[2019-04-04 09:48:28,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.7402820e-12 3.8655145e-11 3.2885932e-23 6.5743765e-13 3.8232989e-12
 1.5031498e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:28,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2536
[2019-04-04 09:48:28,608] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.44254784187171, 0.611778048243195, 0.0, 1.0, 179316.3948221749], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4653600.0000, 
sim time next is 4654200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.54556532408463, 0.6436978210696807, 0.0, 1.0, 40148.28127009138], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6287971103403859, 0.7145659403565602, 0.0, 1.0, 0.1911822917623399], 
reward next is 0.8088, 
noisyNet noise sample is [array([2.3267934], dtype=float32), -1.0528511]. 
=============================================
[2019-04-04 09:48:28,788] A3C_AGENT_WORKER-Thread-12 INFO:Local step 220500, global step 3522092: loss 0.0583
[2019-04-04 09:48:28,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 220500, global step 3522092: learning rate 0.0000
[2019-04-04 09:48:29,640] A3C_AGENT_WORKER-Thread-2 INFO:Local step 220500, global step 3522502: loss 0.0483
[2019-04-04 09:48:29,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 220500, global step 3522506: learning rate 0.0000
[2019-04-04 09:48:29,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4683484e-11 1.1429316e-11 1.1971991e-23 7.1725937e-13 1.4754476e-12
 3.3723539e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:29,892] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8280
[2019-04-04 09:48:29,915] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.4772184056705, 0.507690522076037, 0.0, 1.0, 45586.71376037322], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4667400.0000, 
sim time next is 4668000.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.48425309024827, 0.5123761932454156, 0.0, 1.0, 33612.84438551131], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6236877575206892, 0.6707920644151386, 0.0, 1.0, 0.16006116374053003], 
reward next is 0.8399, 
noisyNet noise sample is [array([1.9621061], dtype=float32), -1.2466714]. 
=============================================
[2019-04-04 09:48:29,942] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[82.45397 ]
 [82.39547 ]
 [82.267845]
 [82.18777 ]
 [82.16024 ]], R is [[82.47874451]
 [82.43688202]
 [82.31182098]
 [82.19361115]
 [82.16605377]].
[2019-04-04 09:48:30,161] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220000, global step 3522741: loss 0.8501
[2019-04-04 09:48:30,164] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220000, global step 3522742: learning rate 0.0000
[2019-04-04 09:48:31,610] A3C_AGENT_WORKER-Thread-5 INFO:Local step 220500, global step 3523364: loss 0.0474
[2019-04-04 09:48:31,611] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 220500, global step 3523364: learning rate 0.0000
[2019-04-04 09:48:32,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1803614e-12 8.8906747e-12 1.3934870e-24 8.0681135e-14 6.1526104e-13
 5.8792123e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:32,107] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5268
[2019-04-04 09:48:32,118] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.55211903163443, 0.5445879975879083, 0.0, 1.0, 28443.3709684987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4485000.0000, 
sim time next is 4485600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.50678810841445, 0.5399563255586105, 0.0, 1.0, 57598.37979708827], 
processed observation next is [1.0, 0.9565217391304348, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6255656757012042, 0.6799854418528701, 0.0, 1.0, 0.27427799903375366], 
reward next is 0.7257, 
noisyNet noise sample is [array([-1.4454137], dtype=float32), 0.41698116]. 
=============================================
[2019-04-04 09:48:34,695] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220000, global step 3524797: loss 1.3609
[2019-04-04 09:48:34,696] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220000, global step 3524798: learning rate 0.0000
[2019-04-04 09:48:34,954] A3C_AGENT_WORKER-Thread-6 INFO:Local step 220500, global step 3524914: loss 0.0633
[2019-04-04 09:48:34,959] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 220500, global step 3524914: learning rate 0.0000
[2019-04-04 09:48:35,235] A3C_AGENT_WORKER-Thread-18 INFO:Local step 220500, global step 3525052: loss 0.0608
[2019-04-04 09:48:35,235] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 220500, global step 3525052: learning rate 0.0000
[2019-04-04 09:48:35,478] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.5470450e-12 2.3307578e-11 2.3628730e-23 1.5823657e-12 1.2725220e-12
 1.0729258e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:35,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3797
[2019-04-04 09:48:35,502] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.38841005547334, 0.2203201275771762, 0.0, 1.0, 41072.98989092105], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4767600.0000, 
sim time next is 4768200.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 24.35731626691095, 0.2236887321930442, 0.0, 1.0, 41127.7293927049], 
processed observation next is [0.0, 0.17391304347826086, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5297763555759124, 0.5745629107310147, 0.0, 1.0, 0.19584633044145192], 
reward next is 0.8042, 
noisyNet noise sample is [array([0.8850066], dtype=float32), 2.7115564]. 
=============================================
[2019-04-04 09:48:35,794] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220000, global step 3525302: loss 0.9968
[2019-04-04 09:48:35,795] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220000, global step 3525302: learning rate 0.0000
[2019-04-04 09:48:36,175] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220000, global step 3525480: loss 1.0631
[2019-04-04 09:48:36,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220000, global step 3525481: learning rate 0.0000
[2019-04-04 09:48:40,828] A3C_AGENT_WORKER-Thread-20 INFO:Local step 220500, global step 3527582: loss 0.0710
[2019-04-04 09:48:40,828] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 220500, global step 3527582: learning rate 0.0000
[2019-04-04 09:48:40,836] A3C_AGENT_WORKER-Thread-19 INFO:Local step 220500, global step 3527584: loss 0.0720
[2019-04-04 09:48:40,836] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 220500, global step 3527584: learning rate 0.0000
[2019-04-04 09:48:42,689] A3C_AGENT_WORKER-Thread-16 INFO:Local step 220500, global step 3528510: loss 0.0531
[2019-04-04 09:48:42,695] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 220500, global step 3528515: learning rate 0.0000
[2019-04-04 09:48:42,733] A3C_AGENT_WORKER-Thread-3 INFO:Local step 220500, global step 3528529: loss 0.0524
[2019-04-04 09:48:42,734] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 220500, global step 3528529: learning rate 0.0000
[2019-04-04 09:48:42,890] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.0259996e-11 1.1170995e-10 2.9212892e-23 2.1260430e-12 3.0936458e-12
 2.8419634e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:42,890] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7845
[2019-04-04 09:48:42,895] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 26.0, 0.0, 0.0, 26.0, 25.84312514588501, 0.5395104206158676, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4998600.0000, 
sim time next is 4999200.0000, 
raw observation next is [4.666666666666666, 27.0, 0.0, 0.0, 26.0, 25.7654162115691, 0.5229933635874905, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5918744228993538, 0.27, 0.0, 0.0, 0.6666666666666666, 0.6471180176307584, 0.6743311211958302, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49568978], dtype=float32), 0.11094106]. 
=============================================
[2019-04-04 09:48:43,166] A3C_AGENT_WORKER-Thread-17 INFO:Local step 220500, global step 3528742: loss 0.0544
[2019-04-04 09:48:43,168] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 220500, global step 3528742: learning rate 0.0000
[2019-04-04 09:48:43,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:43,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:43,345] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run27
[2019-04-04 09:48:43,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:43,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:43,523] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run27
[2019-04-04 09:48:44,158] A3C_AGENT_WORKER-Thread-15 INFO:Local step 220500, global step 3529083: loss 0.0628
[2019-04-04 09:48:44,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 220500, global step 3529083: learning rate 0.0000
[2019-04-04 09:48:45,053] A3C_AGENT_WORKER-Thread-4 INFO:Local step 220500, global step 3529427: loss 0.0626
[2019-04-04 09:48:45,054] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 220500, global step 3529428: learning rate 0.0000
[2019-04-04 09:48:45,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:45,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:45,760] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run27
[2019-04-04 09:48:47,974] A3C_AGENT_WORKER-Thread-11 INFO:Local step 220500, global step 3530438: loss 0.0723
[2019-04-04 09:48:47,982] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 220500, global step 3530438: learning rate 0.0000
[2019-04-04 09:48:48,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2835138e-11 2.9189512e-11 1.6449328e-23 2.2568503e-12 7.8922318e-13
 1.9140619e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:48,504] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7691
[2019-04-04 09:48:48,537] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.5, 0.0, 0.0, 26.0, 25.12713939175858, 0.3479835325957756, 0.0, 1.0, 152652.7670043305], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4908600.0000, 
sim time next is 4909200.0000, 
raw observation next is [1.0, 42.33333333333333, 0.0, 0.0, 26.0, 25.17728557210885, 0.3679739124701962, 0.0, 1.0, 82808.14351107727], 
processed observation next is [0.0, 0.8260869565217391, 0.4903047091412743, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5981071310090709, 0.6226579708233987, 0.0, 1.0, 0.3943244929098918], 
reward next is 0.6057, 
noisyNet noise sample is [array([-0.12162887], dtype=float32), -1.1233183]. 
=============================================
[2019-04-04 09:48:49,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:49,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:49,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run27
[2019-04-04 09:48:50,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:50,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:50,301] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run27
[2019-04-04 09:48:53,003] A3C_AGENT_WORKER-Thread-10 INFO:Local step 220500, global step 3532129: loss 0.0568
[2019-04-04 09:48:53,018] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 220500, global step 3532129: learning rate 0.0000
[2019-04-04 09:48:53,617] A3C_AGENT_WORKER-Thread-13 INFO:Local step 220500, global step 3532365: loss 0.0653
[2019-04-04 09:48:53,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 220500, global step 3532365: learning rate 0.0000
[2019-04-04 09:48:53,649] A3C_AGENT_WORKER-Thread-14 INFO:Local step 220500, global step 3532378: loss 0.0701
[2019-04-04 09:48:53,650] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 220500, global step 3532378: learning rate 0.0000
[2019-04-04 09:48:55,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:55,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:55,993] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run27
[2019-04-04 09:48:56,613] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.1208131e-12 5.8515024e-12 1.4657475e-24 4.1398086e-13 6.8192099e-13
 2.5852778e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:48:56,614] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1898
[2019-04-04 09:48:56,659] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 26.0, 24.45655319440051, 0.1588914843269057, 0.0, 1.0, 20857.99212652868], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [7.45, 86.0, 79.0, 0.0, 26.0, 24.45891312537651, 0.1662790722608276, 0.0, 1.0, 29345.69595567067], 
processed observation next is [0.0, 0.6086956521739131, 0.6689750692520776, 0.86, 0.2633333333333333, 0.0, 0.6666666666666666, 0.5382427604480426, 0.5554263574202759, 0.0, 1.0, 0.13974140931271747], 
reward next is 0.8603, 
noisyNet noise sample is [array([-0.5920235], dtype=float32), 1.753204]. 
=============================================
[2019-04-04 09:48:56,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:56,770] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:56,775] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run27
[2019-04-04 09:48:57,878] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:57,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:57,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run27
[2019-04-04 09:48:58,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:58,402] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:58,410] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run27
[2019-04-04 09:48:58,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:58,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:58,670] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run27
[2019-04-04 09:48:59,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:48:59,073] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:48:59,099] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run27
[2019-04-04 09:49:00,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:49:00,381] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:49:00,386] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run27
[2019-04-04 09:49:04,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:49:04,068] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:49:04,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run27
[2019-04-04 09:49:10,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:49:10,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:49:10,300] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run27
[2019-04-04 09:49:10,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:49:10,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:49:10,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run27
[2019-04-04 09:49:11,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:49:11,077] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:49:11,081] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run27
[2019-04-04 09:49:20,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4463384e-13 4.1383108e-12 1.5344922e-23 2.0229879e-13 1.0551806e-13
 3.3436458e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:49:20,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1912
[2019-04-04 09:49:20,907] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.33333333333333, 78.0, 574.3333333333334, 26.0, 24.89936855297147, 0.3611095228450151, 1.0, 1.0, 198442.407540736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 312600.0000, 
sim time next is 313200.0000, 
raw observation next is [-9.5, 42.0, 76.0, 550.0, 26.0, 25.24815336145667, 0.4525637075971103, 1.0, 1.0, 112482.3136055023], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.25333333333333335, 0.6077348066298343, 0.6666666666666666, 0.6040127801213891, 0.6508545691990367, 1.0, 1.0, 0.5356300647881063], 
reward next is 0.4644, 
noisyNet noise sample is [array([1.9322759], dtype=float32), 1.3332249]. 
=============================================
[2019-04-04 09:49:21,242] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1318152e-12 1.5794657e-11 1.7100483e-23 1.3951649e-13 2.9022547e-13
 4.3082887e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:49:21,243] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7650
[2019-04-04 09:49:21,306] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.17715781973392, 0.2446127436050002, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 243000.0000, 
sim time next is 243600.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 24.92601119421957, 0.2229077440310776, 1.0, 1.0, 90566.42659379065], 
processed observation next is [1.0, 0.8260869565217391, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5771675995182974, 0.5743025813436925, 1.0, 1.0, 0.43126869806566975], 
reward next is 0.5687, 
noisyNet noise sample is [array([-0.1812545], dtype=float32), 0.8308629]. 
=============================================
[2019-04-04 09:49:30,260] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3311357e-11 1.4288137e-10 4.8375879e-22 4.7239816e-12 8.6494778e-12
 5.4116671e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:49:30,260] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7152
[2019-04-04 09:49:30,428] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.54738901152137, -0.2980058871274366, 0.0, 1.0, 45023.61995836028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 198600.0000, 
sim time next is 199200.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.4833243543006, -0.2223548512970458, 1.0, 1.0, 202343.6265148717], 
processed observation next is [1.0, 0.30434782608695654, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.3736103628583833, 0.42588171623431803, 1.0, 1.0, 0.9635410786422461], 
reward next is 0.0365, 
noisyNet noise sample is [array([-0.31161466], dtype=float32), -0.09314087]. 
=============================================
[2019-04-04 09:49:30,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4902760e-13 1.5030345e-11 7.5871356e-24 5.2788926e-13 2.4276596e-13
 1.7673696e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:49:30,473] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8768
[2019-04-04 09:49:30,555] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.09727457012023, 0.3075527364799971, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 235200.0000, 
sim time next is 235800.0000, 
raw observation next is [-3.4, 65.0, 0.0, 0.0, 26.0, 25.46750723716804, 0.3284270612536845, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.368421052631579, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6222922697640033, 0.6094756870845615, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.20763205], dtype=float32), 0.18750086]. 
=============================================
[2019-04-04 09:49:34,721] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.7363964e-13 8.9762616e-12 1.4197852e-23 3.1083361e-13 1.7550364e-13
 2.1490505e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:49:34,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3500
[2019-04-04 09:49:34,778] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 63.5, 18.0, 0.0, 26.0, 25.29329031513466, 0.2734227891036996, 1.0, 1.0, 35719.92651684987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 232200.0000, 
sim time next is 232800.0000, 
raw observation next is [-3.4, 64.0, 15.0, 0.0, 26.0, 25.33306431882052, 0.2775880722185964, 1.0, 1.0, 32756.95021756213], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.64, 0.05, 0.0, 0.6666666666666666, 0.6110886932350432, 0.5925293574061988, 1.0, 1.0, 0.15598547722648634], 
reward next is 0.8440, 
noisyNet noise sample is [array([1.3416038], dtype=float32), 0.03971823]. 
=============================================
[2019-04-04 09:49:35,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0506492e-13 6.0338644e-12 6.4878110e-25 2.8221837e-13 4.4884923e-14
 1.3523472e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:49:35,702] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0023
[2019-04-04 09:49:35,755] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.033333333333333, 29.33333333333333, 102.0, 0.0, 26.0, 25.35822583698339, 0.1671124352577991, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 470400.0000, 
sim time next is 471000.0000, 
raw observation next is [-2.666666666666666, 28.66666666666667, 106.0, 0.0, 26.0, 25.32470230965109, 0.1637346999828527, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3887349953831949, 0.28666666666666674, 0.35333333333333333, 0.0, 0.6666666666666666, 0.6103918591375909, 0.5545782333276176, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5435262], dtype=float32), 0.7620325]. 
=============================================
[2019-04-04 09:49:35,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[83.84392 ]
 [83.85858 ]
 [83.956345]
 [84.03594 ]
 [84.07456 ]], R is [[84.05026245]
 [84.20976257]
 [84.36766815]
 [84.52399445]
 [84.67875671]].
[2019-04-04 09:49:56,922] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0765259e-13 1.5022599e-12 1.8057430e-24 1.6497601e-14 1.9451795e-14
 4.0545699e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:49:56,922] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1224
[2019-04-04 09:49:56,981] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.43333333333333, 51.00000000000001, 58.0, 881.5, 26.0, 25.71912357033881, 0.3561643953946385, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 390000.0000, 
sim time next is 390600.0000, 
raw observation next is [-12.25, 51.0, 58.0, 905.0, 26.0, 25.66217699126357, 0.3735093030081728, 1.0, 1.0, 131486.9546792351], 
processed observation next is [1.0, 0.5217391304347826, 0.12326869806094183, 0.51, 0.19333333333333333, 1.0, 0.6666666666666666, 0.6385147492719643, 0.6245031010027243, 1.0, 1.0, 0.6261283556154051], 
reward next is 0.3739, 
noisyNet noise sample is [array([-1.2827568], dtype=float32), -1.1049356]. 
=============================================
[2019-04-04 09:50:05,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1759834e-12 2.6605489e-12 2.0511374e-24 1.8691853e-13 3.3753545e-13
 1.4614385e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:05,824] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3897
[2019-04-04 09:50:05,875] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.94051795388465, 0.2840133886212677, 0.0, 1.0, 45983.22836862898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [-2.8, 87.0, 0.0, 0.0, 26.0, 24.96836698320608, 0.2918630260362654, 0.0, 1.0, 125930.8805881882], 
processed observation next is [0.0, 0.8260869565217391, 0.38504155124653744, 0.87, 0.0, 0.0, 0.6666666666666666, 0.5806972486005068, 0.5972876753454218, 0.0, 1.0, 0.5996708599437534], 
reward next is 0.4003, 
noisyNet noise sample is [array([0.7392961], dtype=float32), -0.0045810803]. 
=============================================
[2019-04-04 09:50:05,893] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[82.4107  ]
 [82.24522 ]
 [82.04883 ]
 [81.84529 ]
 [81.718475]], R is [[82.50354767]
 [82.45954132]
 [82.37102509]
 [82.27125549]
 [82.2074585 ]].
[2019-04-04 09:50:07,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6752144e-13 2.9209378e-12 4.4570301e-25 1.6905211e-13 1.4955827e-14
 6.0528220e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:07,440] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0462
[2019-04-04 09:50:07,499] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 36.0, 94.0, 0.0, 26.0, 24.44096525628457, 0.1806223074810993, 1.0, 1.0, 198395.930798331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 484200.0000, 
sim time next is 484800.0000, 
raw observation next is [-0.2, 36.33333333333333, 87.66666666666667, 0.0, 26.0, 24.9395545108863, 0.2284579148952023, 1.0, 1.0, 23914.7471645275], 
processed observation next is [1.0, 0.6086956521739131, 0.4570637119113574, 0.3633333333333333, 0.2922222222222222, 0.0, 0.6666666666666666, 0.5782962092405249, 0.5761526382984008, 1.0, 1.0, 0.1138797484025119], 
reward next is 0.8861, 
noisyNet noise sample is [array([-0.5172322], dtype=float32), -1.9170609]. 
=============================================
[2019-04-04 09:50:20,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6720916e-13 2.7768048e-12 1.1946249e-25 5.6384276e-14 7.4811237e-14
 2.4645647e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:20,216] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8599
[2019-04-04 09:50:20,243] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 97.33333333333334, 0.0, 0.0, 26.0, 25.14254202696572, 0.3930804697720537, 0.0, 1.0, 40157.11741611649], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 942000.0000, 
sim time next is 942600.0000, 
raw observation next is [5.0, 96.66666666666666, 0.0, 0.0, 26.0, 25.16060280246975, 0.3958216665274363, 0.0, 1.0, 39957.68714271274], 
processed observation next is [1.0, 0.9130434782608695, 0.6011080332409973, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5967169002058125, 0.6319405555091454, 0.0, 1.0, 0.1902747006795845], 
reward next is 0.8097, 
noisyNet noise sample is [array([1.0458797], dtype=float32), 0.13817196]. 
=============================================
[2019-04-04 09:50:27,652] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.55904933e-12 7.95345213e-12 1.87674466e-24 1.92053567e-13
 1.03843326e-13 8.11225318e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 09:50:27,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2136
[2019-04-04 09:50:27,705] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 72.0, 0.0, 0.0, 26.0, 24.62256499194848, 0.1641023118524476, 0.0, 1.0, 38889.69918574175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 884400.0000, 
sim time next is 885000.0000, 
raw observation next is [-0.09999999999999998, 72.0, 0.0, 0.0, 26.0, 24.55566077534014, 0.1551483131757126, 0.0, 1.0, 38869.9045806], 
processed observation next is [1.0, 0.21739130434782608, 0.4598337950138504, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5463050646116784, 0.5517161043919042, 0.0, 1.0, 0.18509478371714286], 
reward next is 0.8149, 
noisyNet noise sample is [array([1.3565944], dtype=float32), 0.36753184]. 
=============================================
[2019-04-04 09:50:27,716] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[84.48627 ]
 [84.43202 ]
 [84.35827 ]
 [84.279854]
 [84.22471 ]], R is [[84.49468231]
 [84.4645462 ]
 [84.43464661]
 [84.40489197]
 [84.37520599]].
[2019-04-04 09:50:28,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7052609e-13 3.0870501e-12 2.9850802e-25 2.4749307e-14 7.6065327e-14
 1.3994857e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:28,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8131
[2019-04-04 09:50:28,258] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.716666666666667, 79.16666666666667, 0.0, 0.0, 26.0, 24.76246307863278, 0.2359219821789847, 0.0, 1.0, 40916.21507934703], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 861000.0000, 
sim time next is 861600.0000, 
raw observation next is [-2.633333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 24.7303943294307, 0.2302659608049119, 0.0, 1.0, 40793.0507959004], 
processed observation next is [1.0, 1.0, 0.38965835641735924, 0.7933333333333334, 0.0, 0.0, 0.6666666666666666, 0.5608661941192249, 0.5767553202683039, 0.0, 1.0, 0.19425262283762096], 
reward next is 0.8057, 
noisyNet noise sample is [array([-0.9629801], dtype=float32), -0.30157587]. 
=============================================
[2019-04-04 09:50:30,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.90652791e-13 7.54930027e-13 2.30319944e-26 2.96270305e-14
 1.00745540e-14 1.19932834e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 09:50:30,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5666
[2019-04-04 09:50:30,682] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666667, 74.66666666666667, 24.16666666666667, 0.0, 26.0, 24.90941937703329, 0.2508761708785909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 894000.0000, 
sim time next is 894600.0000, 
raw observation next is [0.55, 76.0, 29.0, 0.0, 26.0, 25.11626504081585, 0.2711700017541115, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4778393351800555, 0.76, 0.09666666666666666, 0.0, 0.6666666666666666, 0.5930220867346542, 0.5903900005847038, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0588495], dtype=float32), 1.9122181]. 
=============================================
[2019-04-04 09:50:32,224] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.5357770e-13 1.7950776e-12 5.2287544e-26 7.5533004e-14 1.2502027e-13
 9.0412065e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:32,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4823
[2019-04-04 09:50:32,241] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.46666666666667, 64.33333333333333, 24.16666666666667, 0.0, 26.0, 24.97709401130443, 0.4660789281488449, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1183200.0000, 
sim time next is 1183800.0000, 
raw observation next is [18.38333333333333, 64.66666666666667, 19.33333333333334, 0.0, 26.0, 24.95630396905947, 0.4613817278594322, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.9718374884579871, 0.6466666666666667, 0.06444444444444447, 0.0, 0.6666666666666666, 0.5796919974216225, 0.6537939092864774, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35301754], dtype=float32), 0.6374463]. 
=============================================
[2019-04-04 09:50:44,941] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7861313e-14 2.1864621e-13 9.2698406e-27 2.7255917e-15 1.8364492e-15
 9.0632674e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:44,941] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9182
[2019-04-04 09:50:44,952] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.16666666666667, 53.0, 0.0, 26.0, 25.70682445484368, 0.5131441649732681, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1350600.0000, 
sim time next is 1351200.0000, 
raw observation next is [1.1, 92.33333333333334, 48.5, 0.0, 26.0, 25.70665467015125, 0.5107668824383351, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.9233333333333335, 0.16166666666666665, 0.0, 0.6666666666666666, 0.642221222512604, 0.670255627479445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3714509], dtype=float32), 1.0457317]. 
=============================================
[2019-04-04 09:50:49,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2768939e-13 1.7153533e-12 1.1067245e-25 4.4127476e-14 5.9487476e-14
 1.4788735e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:49,491] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8871
[2019-04-04 09:50:49,544] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 24.69507344116894, 0.4478227964968755, 0.0, 1.0, 30260.43785789407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1278000.0000, 
sim time next is 1278600.0000, 
raw observation next is [7.016666666666667, 96.0, 0.0, 0.0, 26.0, 24.70398737735966, 0.4487052616607377, 0.0, 1.0, 27244.39166156606], 
processed observation next is [0.0, 0.8260869565217391, 0.656971375807941, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5586656147799717, 0.6495684205535792, 0.0, 1.0, 0.1297351983884098], 
reward next is 0.8703, 
noisyNet noise sample is [array([-1.830177], dtype=float32), -1.1610006]. 
=============================================
[2019-04-04 09:50:52,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6106809e-13 2.0774905e-12 2.0981112e-26 3.1357746e-14 1.1487088e-13
 6.0376945e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:52,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7826
[2019-04-04 09:50:52,820] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.41666666666667, 77.33333333333334, 0.0, 0.0, 26.0, 25.65296513231596, 0.6412616542253575, 0.0, 1.0, 23345.77936636044], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1127400.0000, 
sim time next is 1128000.0000, 
raw observation next is [10.33333333333333, 77.66666666666667, 0.0, 0.0, 26.0, 25.65489203047144, 0.6403226067851164, 0.0, 1.0, 22279.31023488069], 
processed observation next is [0.0, 0.043478260869565216, 0.7488457987072946, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.6379076692059534, 0.7134408689283722, 0.0, 1.0, 0.10609195349943186], 
reward next is 0.8939, 
noisyNet noise sample is [array([-0.99138355], dtype=float32), -0.26205945]. 
=============================================
[2019-04-04 09:50:52,832] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[88.16264]
 [88.22446]
 [88.31425]
 [88.27725]
 [88.19447]], R is [[88.02873993]
 [88.03728485]
 [88.02805328]
 [87.98356628]
 [87.89679718]].
[2019-04-04 09:50:52,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.1135569e-14 9.8238204e-13 1.7461963e-26 2.2145395e-14 1.6497822e-14
 2.6702841e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:50:52,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5023
[2019-04-04 09:50:53,013] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.44074470147399, 0.4816385913224221, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1451400.0000, 
sim time next is 1452000.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.3658865339988, 0.4621588172209133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6138238778332333, 0.6540529390736377, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3876903], dtype=float32), 1.0191526]. 
=============================================
[2019-04-04 09:50:53,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.646324]
 [88.75577 ]
 [88.8126  ]
 [88.8491  ]
 [88.928925]], R is [[88.73549652]
 [88.84814453]
 [88.95966339]
 [89.07006836]
 [89.17936707]].
[2019-04-04 09:50:58,569] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.20925544e-13 2.18499589e-12 9.81675570e-25 1.04972854e-13
 1.44299828e-13 5.47467993e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 09:50:58,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1620
[2019-04-04 09:50:58,600] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.00000000000001, 0.0, 0.0, 26.0, 25.45988316201126, 0.5550283084019634, 0.0, 1.0, 53152.93446353033], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1555800.0000, 
sim time next is 1556400.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.45909044762929, 0.5589669481036421, 0.0, 1.0, 42457.21795152664], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6215908706357741, 0.6863223160345474, 0.0, 1.0, 0.20217722834060306], 
reward next is 0.7978, 
noisyNet noise sample is [array([-0.39410815], dtype=float32), 0.19833457]. 
=============================================
[2019-04-04 09:51:01,107] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.1532410e-14 1.0410635e-12 1.6638067e-25 7.4089260e-15 1.4687102e-14
 4.4839097e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:01,107] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0584
[2019-04-04 09:51:01,116] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 90.0, 0.0, 26.0, 25.66673588108202, 0.4652817405328531, 1.0, 1.0, 31445.55675472944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1423800.0000, 
sim time next is 1424400.0000, 
raw observation next is [0.0, 95.0, 91.0, 0.0, 26.0, 25.65528769916033, 0.4649061098812584, 1.0, 1.0, 23591.69602927901], 
processed observation next is [1.0, 0.4782608695652174, 0.46260387811634357, 0.95, 0.30333333333333334, 0.0, 0.6666666666666666, 0.6379406415966941, 0.6549687032937528, 1.0, 1.0, 0.11234140966323337], 
reward next is 0.8877, 
noisyNet noise sample is [array([-0.6375774], dtype=float32), 0.08945097]. 
=============================================
[2019-04-04 09:51:02,125] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5381963e-15 1.4500469e-14 9.3864030e-29 1.3884625e-16 5.2572815e-16
 1.1257190e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:02,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7377
[2019-04-04 09:51:02,142] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.85, 94.5, 88.0, 708.0, 26.0, 26.27395049979543, 0.6351597503977434, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1510200.0000, 
sim time next is 1510800.0000, 
raw observation next is [4.033333333333333, 94.0, 90.0, 706.6666666666666, 26.0, 26.34011041492293, 0.487551534612791, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5743305632502309, 0.94, 0.3, 0.7808471454880295, 0.6666666666666666, 0.6950092012435775, 0.6625171782042637, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.141027], dtype=float32), 0.9095263]. 
=============================================
[2019-04-04 09:51:03,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6266925e-14 5.2963984e-14 8.0390442e-27 1.9552925e-15 6.2771613e-15
 1.9979668e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:03,501] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3457
[2019-04-04 09:51:03,531] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.2, 100.0, 60.0, 0.0, 26.0, 26.129759101085, 0.5233572421458085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1504800.0000, 
sim time next is 1505400.0000, 
raw observation next is [2.383333333333333, 99.33333333333334, 64.33333333333333, 0.0, 26.0, 26.07092028861077, 0.5225919103896128, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5286241920590952, 0.9933333333333334, 0.21444444444444444, 0.0, 0.6666666666666666, 0.672576690717564, 0.6741973034632043, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.81408703], dtype=float32), 1.3318298]. 
=============================================
[2019-04-04 09:51:07,971] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9787525e-14 8.3173236e-13 7.5832834e-25 2.1314085e-14 5.0590378e-14
 5.1362802e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:07,971] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9030
[2019-04-04 09:51:08,012] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7000000000000001, 92.0, 91.0, 0.0, 26.0, 25.09172560961306, 0.508493067218053, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1430400.0000, 
sim time next is 1431000.0000, 
raw observation next is [0.8, 92.0, 90.0, 0.0, 26.0, 25.6364969381319, 0.5490290721711143, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.4847645429362882, 0.92, 0.3, 0.0, 0.6666666666666666, 0.6363747448443249, 0.6830096907237048, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.09218749], dtype=float32), -0.54596865]. 
=============================================
[2019-04-04 09:51:08,032] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[87.922134]
 [87.99533 ]
 [87.53561 ]
 [87.25448 ]
 [87.49609 ]], R is [[88.01325989]
 [88.13312531]
 [87.35025024]
 [86.66777802]
 [86.72203064]].
[2019-04-04 09:51:16,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2521578e-13 3.1726080e-12 6.6332336e-25 1.5411847e-13 1.4996884e-13
 1.3902440e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:16,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1469
[2019-04-04 09:51:16,593] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 17.5, 11.0, 26.0, 24.98534946820245, 0.18130498475133, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1929600.0000, 
sim time next is 1930200.0000, 
raw observation next is [-9.4, 90.16666666666667, 22.66666666666667, 14.33333333333334, 26.0, 25.13459414734267, 0.17998686515558, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.20221606648199447, 0.9016666666666667, 0.07555555555555557, 0.015837937384898717, 0.6666666666666666, 0.5945495122785559, 0.5599956217185267, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.30645344], dtype=float32), -1.1410382]. 
=============================================
[2019-04-04 09:51:17,095] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8061035e-14 1.6674895e-13 1.9603304e-26 4.7925853e-15 6.2864592e-15
 4.3028807e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:17,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3939
[2019-04-04 09:51:17,128] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.35, 57.5, 0.0, 0.0, 26.0, 26.8092742177253, 0.7372736782181959, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1618200.0000, 
sim time next is 1618800.0000, 
raw observation next is [11.06666666666667, 58.66666666666667, 0.0, 0.0, 26.0, 26.77745517967717, 0.7527409961793613, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7691597414589106, 0.5866666666666667, 0.0, 0.0, 0.6666666666666666, 0.7314545983064308, 0.7509136653931204, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5379352], dtype=float32), 1.0294799]. 
=============================================
[2019-04-04 09:51:23,200] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1274348e-11 5.7278730e-11 8.0463623e-23 2.8262575e-12 2.0103494e-12
 1.0685145e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:23,206] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9171
[2019-04-04 09:51:23,265] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 152.0, 66.0, 26.0, 24.97494860511811, 0.2485409067456974, 0.0, 1.0, 27542.61801436358], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1854000.0000, 
sim time next is 1854600.0000, 
raw observation next is [-5.5, 74.33333333333333, 162.6666666666667, 71.0, 26.0, 24.97006703325474, 0.2491906919688205, 0.0, 1.0, 40513.2663979592], 
processed observation next is [0.0, 0.4782608695652174, 0.3102493074792244, 0.7433333333333333, 0.5422222222222224, 0.07845303867403315, 0.6666666666666666, 0.580838919437895, 0.5830635639896068, 0.0, 1.0, 0.1929203161807581], 
reward next is 0.8071, 
noisyNet noise sample is [array([0.23461208], dtype=float32), 0.72803444]. 
=============================================
[2019-04-04 09:51:31,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.74100878e-12 1.01828171e-10 6.26675205e-23 4.08863958e-12
 7.61972516e-12 1.29615455e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 09:51:31,244] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4714
[2019-04-04 09:51:31,263] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.51514084383789, -0.1256645801738149, 0.0, 1.0, 44997.6479346148], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1921200.0000, 
sim time next is 1921800.0000, 
raw observation next is [-8.9, 82.0, 0.0, 0.0, 26.0, 23.43377019517114, -0.1391846257816527, 0.0, 1.0, 44929.38587972718], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.82, 0.0, 0.0, 0.6666666666666666, 0.45281418293092823, 0.4536051247394491, 0.0, 1.0, 0.21394945657012943], 
reward next is 0.7861, 
noisyNet noise sample is [array([-0.43204004], dtype=float32), 0.70486]. 
=============================================
[2019-04-04 09:51:35,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1465360e-12 1.9356745e-11 3.8083337e-24 3.1875213e-13 2.9944051e-13
 3.0401766e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:35,999] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6113
[2019-04-04 09:51:36,048] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.35721537142524, 0.1337888760391078, 0.0, 1.0, 41581.57025222336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1996800.0000, 
sim time next is 1997400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38253089767597, 0.1325626564301368, 0.0, 1.0, 41526.03944299657], 
processed observation next is [1.0, 0.08695652173913043, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5318775748063308, 0.5441875521433789, 0.0, 1.0, 0.19774304496665032], 
reward next is 0.8023, 
noisyNet noise sample is [array([0.3953033], dtype=float32), 0.26742387]. 
=============================================
[2019-04-04 09:51:56,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3535621e-12 1.7796898e-11 4.8417001e-24 2.5108442e-13 9.8094037e-13
 1.0172134e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:51:56,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4814
[2019-04-04 09:51:56,995] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 73.0, 0.0, 0.0, 26.0, 25.03924028569166, 0.3090821555550189, 0.0, 1.0, 44492.89274786998], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2241000.0000, 
sim time next is 2241600.0000, 
raw observation next is [-6.0, 73.66666666666667, 0.0, 0.0, 26.0, 24.93001285260176, 0.2926861873627243, 0.0, 1.0, 44254.91274637901], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.7366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5775010710501466, 0.5975620624542414, 0.0, 1.0, 0.21073767974466193], 
reward next is 0.7893, 
noisyNet noise sample is [array([0.21521875], dtype=float32), 0.606579]. 
=============================================
[2019-04-04 09:52:05,599] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7934517e-13 8.6865281e-12 7.1228886e-24 2.2825551e-13 6.0613840e-13
 7.6651349e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:52:05,609] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6873
[2019-04-04 09:52:05,705] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.22659278604367, 0.4172574119596196, 0.0, 1.0, 46124.55371933618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2148600.0000, 
sim time next is 2149200.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.25893521544404, 0.4173768206839886, 0.0, 1.0, 43960.34000687009], 
processed observation next is [1.0, 0.9130434782608695, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.60491126795367, 0.6391256068946629, 0.0, 1.0, 0.2093349524136671], 
reward next is 0.7907, 
noisyNet noise sample is [array([0.02020671], dtype=float32), -0.06920229]. 
=============================================
[2019-04-04 09:52:18,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5457462e-12 1.7988453e-11 1.9577401e-23 1.3927329e-12 9.2193711e-13
 2.7166646e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:52:18,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8388
[2019-04-04 09:52:18,834] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.58457900610213, -0.03019884183710195, 0.0, 1.0, 43230.28110860802], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2267400.0000, 
sim time next is 2268000.0000, 
raw observation next is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.60038679203332, -0.03514249512217459, 0.0, 1.0, 43205.08063489963], 
processed observation next is [1.0, 0.2608695652173913, 0.21606648199445982, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4666988993361099, 0.48828583495927513, 0.0, 1.0, 0.20573847921380775], 
reward next is 0.7943, 
noisyNet noise sample is [array([0.2800907], dtype=float32), -0.08695919]. 
=============================================
[2019-04-04 09:52:18,850] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.195564]
 [77.210304]
 [77.24302 ]
 [77.28744 ]
 [77.35053 ]], R is [[77.21916199]
 [77.24111176]
 [77.26279449]
 [77.28411102]
 [77.30497742]].
[2019-04-04 09:52:23,811] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 09:52:23,821] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 09:52:23,823] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:52:23,842] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run37
[2019-04-04 09:52:23,881] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 09:52:23,883] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 09:52:23,884] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:52:23,884] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:52:23,905] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run37
[2019-04-04 09:52:23,977] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run37
[2019-04-04 09:53:34,537] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16578539], dtype=float32), -0.2437177]
[2019-04-04 09:53:34,537] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [15.16666666666667, 95.0, 0.0, 0.0, 26.0, 23.71701123910007, 0.1914932266385729, 0.0, 0.0, 0.0]
[2019-04-04 09:53:34,537] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:53:34,538] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.5692464e-12 7.3828508e-13 1.9047198e-26 3.9284412e-14 6.3242739e-14
 3.3684915e-17 1.0000000e+00], sampled 0.8874251678992123
[2019-04-04 09:54:52,159] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16578539], dtype=float32), -0.2437177]
[2019-04-04 09:54:52,159] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [4.078625875666667, 41.72970797666667, 0.0, 0.0, 26.0, 24.96261285156163, 0.2785952355018534, 0.0, 1.0, 0.0]
[2019-04-04 09:54:52,159] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 09:54:52,160] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.37298722e-12 1.17988935e-11 1.23966312e-23 8.29802373e-13
 4.16068303e-13 6.07030947e-15 1.00000000e+00], sampled 0.5083047560890167
[2019-04-04 09:55:20,348] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16578539], dtype=float32), -0.2437177]
[2019-04-04 09:55:20,348] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.333333333333333, 65.33333333333334, 0.0, 0.0, 26.0, 25.59552816877539, 0.4087070886937862, 0.0, 1.0, 47878.66089091025]
[2019-04-04 09:55:20,349] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 09:55:20,349] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.0561560e-12 7.3271892e-12 3.4472299e-24 3.3380482e-13 6.5563440e-13
 2.2883069e-15 1.0000000e+00], sampled 0.7674181155038273
[2019-04-04 09:55:30,667] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 09:56:02,426] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.7639 263389580.4542 1552.0485
[2019-04-04 09:56:07,159] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 09:56:08,198] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 3600000, evaluation results [3600000.0, 7241.763902599205, 263389580.45416775, 1552.0485098418105, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 09:56:24,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6617188e-13 2.6624627e-12 1.0254059e-24 5.9431904e-14 5.7279736e-14
 8.1555336e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:56:24,594] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8575
[2019-04-04 09:56:24,688] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.05547765824042, 0.4105037993891509, 1.0, 1.0, 87055.91005170991], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [-5.0, 59.0, 0.0, 0.0, 26.0, 25.11392675087005, 0.4170734548615414, 0.0, 1.0, 37701.46491945855], 
processed observation next is [1.0, 0.8695652173913043, 0.32409972299168976, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5928272292391709, 0.6390244849538471, 0.0, 1.0, 0.179530785330755], 
reward next is 0.8205, 
noisyNet noise sample is [array([-2.1989257], dtype=float32), 0.2728405]. 
=============================================
[2019-04-04 09:56:38,636] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4118153e-13 4.1790475e-13 4.1053332e-26 6.4006776e-14 8.6611194e-15
 5.6688176e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:56:38,636] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9157
[2019-04-04 09:56:38,712] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 64.0, 108.0, 207.0, 26.0, 25.78978380306109, 0.3891604267470986, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2795400.0000, 
sim time next is 2796000.0000, 
raw observation next is [-6.0, 64.0, 115.3333333333333, 211.3333333333333, 26.0, 25.89527253990332, 0.3998276623086316, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.296398891966759, 0.64, 0.3844444444444443, 0.23351749539594838, 0.6666666666666666, 0.6579393783252767, 0.6332758874362105, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.1217742], dtype=float32), -1.1460559]. 
=============================================
[2019-04-04 09:56:38,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[87.436806]
 [87.46835 ]
 [87.30855 ]
 [86.972984]
 [86.47828 ]], R is [[87.04154205]
 [87.17112732]
 [87.29941559]
 [87.42642212]
 [87.55216217]].
[2019-04-04 09:56:48,092] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.04646759e-12 1.95413720e-11 1.02287476e-23 3.56228447e-13
 7.54725601e-13 2.20302025e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 09:56:48,092] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5385
[2019-04-04 09:56:48,105] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.78894727288464, 0.3110963980636554, 0.0, 1.0, 43279.2777978849], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2943000.0000, 
sim time next is 2943600.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.77573354971461, 0.312352951438599, 0.0, 1.0, 43241.2838328237], 
processed observation next is [0.0, 0.043478260869565216, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5646444624762174, 0.604117650479533, 0.0, 1.0, 0.20591087539439856], 
reward next is 0.7941, 
noisyNet noise sample is [array([-0.55119765], dtype=float32), 0.98517066]. 
=============================================
[2019-04-04 09:56:54,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3055581e-13 9.1991367e-13 4.7001694e-25 3.0589016e-14 3.9195575e-14
 1.1383431e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:56:54,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1575
[2019-04-04 09:56:54,342] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 100.0, 87.5, 0.0, 26.0, 25.66150895762087, 0.4382999572098387, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2905200.0000, 
sim time next is 2905800.0000, 
raw observation next is [2.0, 100.0, 86.66666666666666, 0.0, 26.0, 25.82550263267517, 0.4466750237731732, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.518005540166205, 1.0, 0.28888888888888886, 0.0, 0.6666666666666666, 0.6521252193895976, 0.6488916745910577, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5508748], dtype=float32), 0.244064]. 
=============================================
[2019-04-04 09:56:58,524] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.02091140e-13 1.73804135e-12 1.17487145e-24 1.36260615e-14
 6.74467264e-14 1.50337263e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 09:56:58,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6889
[2019-04-04 09:56:58,533] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.82135291521322, 0.6701807175269593, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3182400.0000, 
sim time next is 3183000.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.75898767244954, 0.6537104408804584, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6465823060374616, 0.7179034802934862, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.41293344], dtype=float32), 1.1322718]. 
=============================================
[2019-04-04 09:56:58,554] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[83.099754]
 [78.80321 ]
 [79.35764 ]
 [81.36066 ]
 [85.817535]], R is [[80.80101776]
 [80.99301147]
 [81.18308258]
 [81.37125397]
 [81.55754089]].
[2019-04-04 09:57:02,669] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.4126655e-14 2.1232336e-13 9.8317360e-27 6.8530780e-15 3.8372981e-15
 3.2390143e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:02,673] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9786
[2019-04-04 09:57:02,763] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 100.0, 28.33333333333333, 185.3333333333333, 26.0, 25.46780815855699, 0.3296990852095083, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3138600.0000, 
sim time next is 3139200.0000, 
raw observation next is [6.0, 100.0, 42.0, 237.0, 26.0, 25.37600581538433, 0.3723072969966096, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6288088642659281, 1.0, 0.14, 0.261878453038674, 0.6666666666666666, 0.6146671512820276, 0.6241024323322032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.896152], dtype=float32), 0.67474097]. 
=============================================
[2019-04-04 09:57:12,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2839497e-13 3.6648792e-12 6.9987449e-25 1.1859709e-13 2.6969147e-13
 4.9053001e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:12,040] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5204
[2019-04-04 09:57:12,051] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.8, 100.0, 0.0, 0.0, 26.0, 25.24630562746479, 0.2965724311340123, 0.0, 1.0, 53902.00188430025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3126600.0000, 
sim time next is 3127200.0000, 
raw observation next is [2.866666666666667, 100.0, 0.0, 0.0, 26.0, 25.26377895063481, 0.2957462034563267, 0.0, 1.0, 53836.05666584253], 
processed observation next is [1.0, 0.17391304347826086, 0.5420129270544783, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6053149125529007, 0.5985820678187755, 0.0, 1.0, 0.25636217459925015], 
reward next is 0.7436, 
noisyNet noise sample is [array([0.06354158], dtype=float32), 0.53772074]. 
=============================================
[2019-04-04 09:57:12,157] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9423538e-12 1.6795761e-11 7.4204895e-24 2.0371596e-13 1.9373605e-13
 6.5149481e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:12,162] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6101
[2019-04-04 09:57:12,171] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.5, 0.0, 0.0, 26.0, 24.65540965378807, 0.2429058055935412, 0.0, 1.0, 42892.47305937942], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3389400.0000, 
sim time next is 3390000.0000, 
raw observation next is [-3.666666666666667, 63.66666666666667, 0.0, 0.0, 26.0, 24.79544601669897, 0.2434178373407299, 0.0, 1.0, 42747.84890546789], 
processed observation next is [1.0, 0.21739130434782608, 0.3610341643582641, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5662871680582476, 0.5811392791135767, 0.0, 1.0, 0.2035611852641328], 
reward next is 0.7964, 
noisyNet noise sample is [array([1.4931198], dtype=float32), -0.569862]. 
=============================================
[2019-04-04 09:57:12,189] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.10278]
 [78.96396]
 [78.85058]
 [78.73285]
 [78.64678]], R is [[79.29466248]
 [79.29747009]
 [79.30133057]
 [79.30484009]
 [79.31043243]].
[2019-04-04 09:57:14,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6056736e-11 4.2276113e-11 1.1192376e-23 2.7994323e-12 2.0112393e-12
 4.7339680e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:14,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6417
[2019-04-04 09:57:14,014] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93079208640976, 0.2752204650914976, 0.0, 1.0, 38150.93490819046], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3021600.0000, 
sim time next is 3022200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 24.93795231173934, 0.2698984712044595, 0.0, 1.0, 38075.94099414045], 
processed observation next is [0.0, 1.0, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.5781626926449451, 0.5899661570681531, 0.0, 1.0, 0.18131400473400217], 
reward next is 0.8187, 
noisyNet noise sample is [array([1.3691194], dtype=float32), -0.5682018]. 
=============================================
[2019-04-04 09:57:19,847] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5419306e-12 3.0023044e-12 1.2337748e-24 8.9463755e-14 1.6808980e-13
 9.8017583e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:19,847] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2408
[2019-04-04 09:57:19,877] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.31075626221571, 0.324578950628729, 0.0, 1.0, 41212.454857276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3109200.0000, 
sim time next is 3109800.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.30526344330958, 0.3231263991870789, 0.0, 1.0, 40246.05916105882], 
processed observation next is [0.0, 1.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6087719536091317, 0.6077087997290264, 0.0, 1.0, 0.19164790076694677], 
reward next is 0.8084, 
noisyNet noise sample is [array([1.728556], dtype=float32), -0.7133794]. 
=============================================
[2019-04-04 09:57:21,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5526482e-12 1.2050806e-11 1.9103548e-23 1.7947446e-13 2.8448608e-13
 1.8141193e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:21,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9772
[2019-04-04 09:57:21,252] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.20243645694078, 0.5529562488359516, 0.0, 1.0, 99478.85531621941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3530400.0000, 
sim time next is 3531000.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 25.36805573151265, 0.5902443239171239, 0.0, 1.0, 62022.36599482335], 
processed observation next is [1.0, 0.8695652173913043, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.614004644292721, 0.6967481079723746, 0.0, 1.0, 0.2953445999753493], 
reward next is 0.7047, 
noisyNet noise sample is [array([0.4820282], dtype=float32), -0.75507337]. 
=============================================
[2019-04-04 09:57:21,265] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[79.83263 ]
 [79.445755]
 [78.73809 ]
 [77.84056 ]
 [77.58026 ]], R is [[79.81207275]
 [79.54024506]
 [78.9059906 ]
 [78.17103577]
 [78.08392334]].
[2019-04-04 09:57:28,492] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7891863e-14 1.1113067e-13 5.9713304e-27 2.4207446e-15 1.4142234e-15
 3.5655926e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:28,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4440
[2019-04-04 09:57:28,523] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 104.0, 711.0, 26.0, 26.53790666784383, 0.6002541237615617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3405600.0000, 
sim time next is 3406200.0000, 
raw observation next is [2.166666666666667, 48.16666666666666, 105.6666666666667, 728.6666666666666, 26.0, 26.57625097970191, 0.6127545354310061, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5226223453370269, 0.4816666666666666, 0.3522222222222223, 0.8051565377532228, 0.6666666666666666, 0.7146875816418259, 0.7042515118103353, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.13004178], dtype=float32), -0.28584462]. 
=============================================
[2019-04-04 09:57:29,111] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2084676e-13 5.3409252e-13 2.0872155e-25 2.7747583e-14 3.5892770e-14
 2.0015531e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:29,111] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3850
[2019-04-04 09:57:29,173] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 43.0, 108.5, 797.0, 26.0, 25.3234273494535, 0.4680431926033817, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3679200.0000, 
sim time next is 3679800.0000, 
raw observation next is [6.0, 43.66666666666667, 107.0, 790.0, 26.0, 25.37336021540277, 0.4694607535598136, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.4366666666666667, 0.3566666666666667, 0.8729281767955801, 0.6666666666666666, 0.6144466846168974, 0.6564869178532712, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4625747], dtype=float32), -0.5235151]. 
=============================================
[2019-04-04 09:57:32,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0890468e-12 1.0810455e-11 3.5113071e-24 9.5354903e-14 3.4995322e-13
 1.2659474e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:32,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6150
[2019-04-04 09:57:32,685] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.21988633372742, 0.350710277837764, 0.0, 1.0, 51733.86978371377], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3481200.0000, 
sim time next is 3481800.0000, 
raw observation next is [-0.1666666666666667, 71.83333333333333, 0.0, 0.0, 26.0, 25.16503487779442, 0.3460333026552462, 0.0, 1.0, 55226.20184439694], 
processed observation next is [1.0, 0.30434782608695654, 0.4579870729455217, 0.7183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5970862398162016, 0.6153444342184154, 0.0, 1.0, 0.26298191354474737], 
reward next is 0.7370, 
noisyNet noise sample is [array([-0.7185717], dtype=float32), -1.1315116]. 
=============================================
[2019-04-04 09:57:47,110] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1047487e-11 1.3060764e-10 3.0559207e-22 7.1312513e-12 8.3207235e-12
 1.2853836e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:47,120] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8290
[2019-04-04 09:57:47,143] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.33333333333333, 65.0, 0.0, 0.0, 26.0, 24.07974829941636, 0.09828364358355646, 0.0, 1.0, 43705.91617426986], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3990000.0000, 
sim time next is 3990600.0000, 
raw observation next is [-12.5, 66.0, 0.0, 0.0, 26.0, 24.05354233622288, 0.08391329806530168, 0.0, 1.0, 43720.17626365295], 
processed observation next is [1.0, 0.17391304347826086, 0.11634349030470914, 0.66, 0.0, 0.0, 0.6666666666666666, 0.5044618613519066, 0.5279710993551006, 0.0, 1.0, 0.20819131554120454], 
reward next is 0.7918, 
noisyNet noise sample is [array([-0.08185044], dtype=float32), 0.32734686]. 
=============================================
[2019-04-04 09:57:50,764] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.5771806e-13 2.9826358e-12 8.6572061e-26 2.1411152e-13 1.2739031e-13
 7.9183197e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:50,774] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8666
[2019-04-04 09:57:50,800] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.6, 26.0, 0.0, 0.0, 26.0, 25.50364497276636, 0.3627464312145487, 0.0, 1.0, 33564.61652506736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3637800.0000, 
sim time next is 3638400.0000, 
raw observation next is [8.466666666666667, 26.33333333333334, 0.0, 0.0, 26.0, 25.49453823969293, 0.3614135146873612, 0.0, 1.0, 36583.16495336448], 
processed observation next is [0.0, 0.08695652173913043, 0.6971375807940905, 0.2633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6245448533077441, 0.6204711715624537, 0.0, 1.0, 0.17420554739697372], 
reward next is 0.8258, 
noisyNet noise sample is [array([-1.6000742], dtype=float32), 0.87471324]. 
=============================================
[2019-04-04 09:57:55,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5249773e-12 8.1706100e-12 1.8802632e-24 2.9484084e-13 4.0439795e-13
 4.8506676e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:55,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9790
[2019-04-04 09:57:55,599] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.69352433985244, 0.4247576118361267, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3897600.0000, 
sim time next is 3898200.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.55003712823476, 0.3982022252120083, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.62916976068623, 0.6327340750706695, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.97574776], dtype=float32), 0.16181305]. 
=============================================
[2019-04-04 09:57:56,476] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.64757139e-12 5.36127184e-11 1.23115676e-23 1.38808127e-12
 1.08592009e-12 2.22338932e-14 1.00000000e+00], sum to 1.0000
[2019-04-04 09:57:56,489] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5369
[2019-04-04 09:57:56,501] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.16666666666666, 0.0, 0.0, 26.0, 25.47298311021645, 0.5703616190214101, 0.0, 1.0, 44474.78444768094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4135800.0000, 
sim time next is 4136400.0000, 
raw observation next is [1.0, 36.0, 0.0, 0.0, 26.0, 25.71966180432339, 0.5897693970202159, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.36, 0.0, 0.0, 0.6666666666666666, 0.6433051503602826, 0.6965897990067386, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0386597], dtype=float32), -0.46654946]. 
=============================================
[2019-04-04 09:57:57,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6857035e-14 1.0796412e-13 2.0678771e-26 1.4025006e-15 3.1776541e-15
 4.0809452e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 09:57:57,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7922
[2019-04-04 09:57:57,926] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 64.16666666666667, 117.6666666666667, 826.6666666666667, 26.0, 26.49093471372817, 0.6073378761821365, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3759000.0000, 
sim time next is 3759600.0000, 
raw observation next is [-1.666666666666667, 63.33333333333334, 118.3333333333333, 827.8333333333334, 26.0, 26.52228346874753, 0.6115753345103265, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.4164358264081256, 0.6333333333333334, 0.3944444444444443, 0.9147329650092082, 0.6666666666666666, 0.7101902890622943, 0.7038584448367754, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7777754], dtype=float32), 0.3335369]. 
=============================================
[2019-04-04 09:58:13,281] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8121672e-12 1.4442967e-11 1.0989973e-23 2.8564092e-13 6.1953453e-13
 5.6693826e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:13,281] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7172
[2019-04-04 09:58:13,309] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 49.83333333333334, 0.0, 0.0, 26.0, 24.5081599524001, 0.2147816153822746, 0.0, 1.0, 40599.80212903266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4173000.0000, 
sim time next is 4173600.0000, 
raw observation next is [-5.0, 50.66666666666667, 0.0, 0.0, 26.0, 24.60805456345191, 0.2119983211198185, 0.0, 1.0, 40426.65788787013], 
processed observation next is [0.0, 0.30434782608695654, 0.32409972299168976, 0.5066666666666667, 0.0, 0.0, 0.6666666666666666, 0.5506712136209924, 0.5706661070399395, 0.0, 1.0, 0.19250789470414348], 
reward next is 0.8075, 
noisyNet noise sample is [array([0.89912236], dtype=float32), 0.52988786]. 
=============================================
[2019-04-04 09:58:15,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3797191e-12 3.7605943e-12 1.4654346e-24 2.7091645e-13 2.0585367e-13
 1.5783242e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:15,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8314
[2019-04-04 09:58:15,845] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 42.16666666666666, 0.0, 0.0, 26.0, 25.00313998094597, 0.2939366407841337, 0.0, 1.0, 29825.46038514846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4216200.0000, 
sim time next is 4216800.0000, 
raw observation next is [1.266666666666667, 42.33333333333334, 0.0, 0.0, 26.0, 24.98459631677888, 0.3094680506145677, 0.0, 1.0, 198502.8380438001], 
processed observation next is [0.0, 0.8260869565217391, 0.4976915974145891, 0.42333333333333345, 0.0, 0.0, 0.6666666666666666, 0.5820496930649067, 0.6031560168715225, 0.0, 1.0, 0.9452516097323814], 
reward next is 0.0547, 
noisyNet noise sample is [array([-0.01225592], dtype=float32), 0.09199903]. 
=============================================
[2019-04-04 09:58:19,370] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1537854e-13 3.8726764e-13 6.2707841e-26 3.5047072e-14 2.2205286e-14
 2.5516992e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:19,371] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0176
[2019-04-04 09:58:19,391] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 54.5, 148.6666666666667, 749.3333333333334, 26.0, 25.25425645486551, 0.3972033807040876, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4273800.0000, 
sim time next is 4274400.0000, 
raw observation next is [5.666666666666667, 54.0, 134.8333333333333, 785.6666666666666, 26.0, 25.23198840453338, 0.3955656863944099, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.6195752539242845, 0.54, 0.4494444444444443, 0.8681399631675875, 0.6666666666666666, 0.6026657003777816, 0.6318552287981366, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.7537317], dtype=float32), -0.4006575]. 
=============================================
[2019-04-04 09:58:22,813] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.4619070e-12 2.9361548e-11 1.3810727e-23 5.4258085e-13 1.2028594e-12
 1.4593661e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:22,816] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1815
[2019-04-04 09:58:22,833] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3666666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 25.35184470260153, 0.4843000423497375, 0.0, 1.0, 43912.08322973729], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4490400.0000, 
sim time next is 4491000.0000, 
raw observation next is [-0.4, 72.5, 0.0, 0.0, 26.0, 25.3550323021227, 0.4812392703157455, 0.0, 1.0, 43665.2809005191], 
processed observation next is [1.0, 1.0, 0.45152354570637127, 0.725, 0.0, 0.0, 0.6666666666666666, 0.6129193585102252, 0.6604130901052485, 0.0, 1.0, 0.20792990905009096], 
reward next is 0.7921, 
noisyNet noise sample is [array([-0.00412851], dtype=float32), 1.509937]. 
=============================================
[2019-04-04 09:58:22,852] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[80.16423 ]
 [80.15172 ]
 [80.22818 ]
 [80.24621 ]
 [80.230225]], R is [[80.08144379]
 [80.07152557]
 [80.05841064]
 [80.03533936]
 [79.98317719]].
[2019-04-04 09:58:23,548] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.8342956e-13 6.2262669e-12 2.0453554e-24 1.6266732e-13 4.2212981e-13
 4.2204484e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:23,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9124
[2019-04-04 09:58:23,570] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.09999999999999999, 72.0, 0.0, 0.0, 26.0, 25.43742408732419, 0.4883267883283871, 0.0, 1.0, 66505.28292957248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4486800.0000, 
sim time next is 4487400.0000, 
raw observation next is [-0.15, 72.0, 0.0, 0.0, 26.0, 25.38788823057259, 0.4855666871971596, 0.0, 1.0, 77587.74379240825], 
processed observation next is [1.0, 0.9565217391304348, 0.458448753462604, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6156573525477157, 0.6618555623990532, 0.0, 1.0, 0.36946544663051545], 
reward next is 0.6305, 
noisyNet noise sample is [array([0.18995057], dtype=float32), -0.5385084]. 
=============================================
[2019-04-04 09:58:37,140] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3010725e-12 2.3261124e-11 1.0231401e-23 6.3654594e-13 5.7590667e-13
 3.4103738e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:37,141] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8696
[2019-04-04 09:58:37,158] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 83.83333333333334, 0.0, 0.0, 26.0, 25.27128243403869, 0.523841366176364, 0.0, 1.0, 52557.40115099897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4740600.0000, 
sim time next is 4741200.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 25.42322830199795, 0.5330339721515504, 0.0, 1.0, 18763.41100743162], 
processed observation next is [1.0, 0.9130434782608695, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.618602358499829, 0.6776779907171835, 0.0, 1.0, 0.08934957622586485], 
reward next is 0.9107, 
noisyNet noise sample is [array([2.044217], dtype=float32), -1.1728356]. 
=============================================
[2019-04-04 09:58:39,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.8339011e-12 4.0649255e-11 8.5966953e-24 2.2479819e-13 7.2789480e-13
 3.8289781e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:39,804] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3966
[2019-04-04 09:58:39,838] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 73.0, 0.0, 0.0, 26.0, 25.28925414934671, 0.4422032187008544, 0.0, 1.0, 42637.09595340549], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4502400.0000, 
sim time next is 4503000.0000, 
raw observation next is [-0.9333333333333333, 73.0, 0.0, 0.0, 26.0, 25.34747231279461, 0.4472415114791808, 0.0, 1.0, 42507.43949570083], 
processed observation next is [1.0, 0.08695652173913043, 0.4367497691597415, 0.73, 0.0, 0.0, 0.6666666666666666, 0.612289359399551, 0.6490805038263936, 0.0, 1.0, 0.20241637855095634], 
reward next is 0.7976, 
noisyNet noise sample is [array([-0.08102376], dtype=float32), -0.28732705]. 
=============================================
[2019-04-04 09:58:39,849] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.76514 ]
 [81.82795 ]
 [81.89353 ]
 [81.96365 ]
 [81.969055]], R is [[81.71672821]
 [81.69652557]
 [81.67581177]
 [81.65350342]
 [81.62574005]].
[2019-04-04 09:58:42,182] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.6771124e-12 2.6799037e-11 1.7953723e-23 1.3918751e-12 1.2609633e-12
 1.8600691e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:42,182] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4170
[2019-04-04 09:58:42,222] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 47.33333333333334, 0.0, 0.0, 26.0, 25.1752115277003, 0.2605342064087108, 0.0, 1.0, 38529.36155711316], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4944000.0000, 
sim time next is 4944600.0000, 
raw observation next is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.17309587889672, 0.250772466391443, 0.0, 1.0, 38555.57746059151], 
processed observation next is [1.0, 0.21739130434782608, 0.39335180055401664, 0.48, 0.0, 0.0, 0.6666666666666666, 0.5977579899080601, 0.583590822130481, 0.0, 1.0, 0.1835979879075786], 
reward next is 0.8164, 
noisyNet noise sample is [array([1.2960998], dtype=float32), 0.107650064]. 
=============================================
[2019-04-04 09:58:44,959] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7929950e-12 1.0644885e-11 1.3392353e-23 6.9099175e-13 1.5385804e-12
 2.7265173e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:44,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3698
[2019-04-04 09:58:44,990] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.2, 65.0, 0.0, 0.0, 26.0, 25.44633201592603, 0.4448406568764362, 0.0, 1.0, 23878.77530376792], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4586400.0000, 
sim time next is 4587000.0000, 
raw observation next is [-0.35, 65.33333333333334, 0.0, 0.0, 26.0, 25.47292750336202, 0.4393524933925459, 0.0, 1.0, 18755.97308541553], 
processed observation next is [1.0, 0.08695652173913043, 0.45290858725761773, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6227439586135016, 0.6464508311308487, 0.0, 1.0, 0.08931415754959778], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.1354052], dtype=float32), -1.0176747]. 
=============================================
[2019-04-04 09:58:45,009] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.81756 ]
 [79.9368  ]
 [80.030975]
 [80.02916 ]
 [80.10496 ]], R is [[79.81207275]
 [79.90024567]
 [80.01190948]
 [79.98226166]
 [80.05849457]].
[2019-04-04 09:58:51,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:58:51,229] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:58:51,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run28
[2019-04-04 09:58:51,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:58:51,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:58:51,441] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run28
[2019-04-04 09:58:51,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:58:51,541] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:58:51,544] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run28
[2019-04-04 09:58:56,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6360868e-12 2.3536832e-12 2.8211668e-24 1.2269293e-13 3.2343662e-13
 2.3512656e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:58:56,736] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3338
[2019-04-04 09:58:56,763] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.066666666666666, 92.33333333333333, 20.66666666666666, 69.83333333333331, 26.0, 23.82486478741811, 0.09276322967466975, 0.0, 1.0, 41895.04283221092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4779600.0000, 
sim time next is 4780200.0000, 
raw observation next is [-6.033333333333333, 92.16666666666667, 41.33333333333332, 139.6666666666666, 26.0, 23.80477625513047, 0.09882538547008764, 0.0, 1.0, 41792.64482487012], 
processed observation next is [0.0, 0.30434782608695654, 0.29547553093259465, 0.9216666666666667, 0.13777777777777775, 0.1543278084714548, 0.6666666666666666, 0.48373135459420585, 0.5329417951566958, 0.0, 1.0, 0.19901259440414343], 
reward next is 0.8010, 
noisyNet noise sample is [array([-0.31368127], dtype=float32), 0.39350665]. 
=============================================
[2019-04-04 09:58:59,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:58:59,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:58:59,187] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run28
[2019-04-04 09:58:59,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:58:59,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:58:59,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run28
[2019-04-04 09:59:03,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:03,969] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:03,989] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run28
[2019-04-04 09:59:04,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9552337e-12 3.1602514e-12 3.9753123e-24 3.7814725e-13 5.0303186e-13
 3.7881101e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:59:04,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8691
[2019-04-04 09:59:04,231] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3, 95.0, 0.0, 0.0, 26.0, 24.41986300824521, 0.1744534435126807, 0.0, 1.0, 40084.88789819977], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 82800.0000, 
sim time next is 83400.0000, 
raw observation next is [0.25, 95.0, 0.0, 0.0, 26.0, 24.39685148527897, 0.1700303505498292, 0.0, 1.0, 40074.08927067763], 
processed observation next is [0.0, 1.0, 0.46952908587257625, 0.95, 0.0, 0.0, 0.6666666666666666, 0.5330709571065807, 0.5566767835166098, 0.0, 1.0, 0.19082899652703633], 
reward next is 0.8092, 
noisyNet noise sample is [array([-0.4290944], dtype=float32), -0.49319017]. 
=============================================
[2019-04-04 09:59:05,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:05,407] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:05,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run28
[2019-04-04 09:59:05,447] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:05,449] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:05,459] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run28
[2019-04-04 09:59:05,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:05,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:05,638] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run28
[2019-04-04 09:59:05,923] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:05,925] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:05,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run28
[2019-04-04 09:59:06,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:06,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:06,636] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run28
[2019-04-04 09:59:07,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:07,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:07,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run28
[2019-04-04 09:59:07,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:07,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:07,732] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run28
[2019-04-04 09:59:14,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:14,182] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:14,184] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run28
[2019-04-04 09:59:14,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.7735440e-12 7.3845027e-12 2.4554626e-23 2.3901955e-13 4.4407482e-13
 3.3382803e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:59:14,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9604
[2019-04-04 09:59:14,416] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.733333333333334, 73.0, 0.0, 0.0, 26.0, 23.77890500024309, 0.009321873032790676, 0.0, 1.0, 44402.24777808877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 175200.0000, 
sim time next is 175800.0000, 
raw observation next is [-8.816666666666666, 73.5, 0.0, 0.0, 26.0, 23.73807492215903, 0.002853394037237481, 0.0, 1.0, 44402.62783155789], 
processed observation next is [1.0, 0.0, 0.21837488457987075, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4781729101799191, 0.5009511313457459, 0.0, 1.0, 0.21144108491218042], 
reward next is 0.7886, 
noisyNet noise sample is [array([0.8720505], dtype=float32), 0.3715055]. 
=============================================
[2019-04-04 09:59:17,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5136580e-13 1.3726687e-12 1.0336887e-24 5.9118285e-14 1.6836222e-13
 1.1203998e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:59:17,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0778
[2019-04-04 09:59:17,838] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 86.0, 83.0, 0.0, 26.0, 24.41561794947653, 0.1504969573772645, 0.0, 1.0, 29194.17229717485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 50400.0000, 
sim time next is 51000.0000, 
raw observation next is [7.616666666666667, 86.0, 81.66666666666666, 0.0, 26.0, 24.42717168991418, 0.1561718751607261, 0.0, 1.0, 29863.67495325939], 
processed observation next is [0.0, 0.6086956521739131, 0.6735918744228995, 0.86, 0.2722222222222222, 0.0, 0.6666666666666666, 0.5355976408261816, 0.5520572917202421, 0.0, 1.0, 0.14220797596790186], 
reward next is 0.8578, 
noisyNet noise sample is [array([0.8795557], dtype=float32), -0.7335483]. 
=============================================
[2019-04-04 09:59:17,846] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[82.381195]
 [82.50013 ]
 [82.57517 ]
 [82.60908 ]
 [82.62617 ]], R is [[82.26907349]
 [82.30735779]
 [82.32653809]
 [82.30709076]
 [82.26512146]].
[2019-04-04 09:59:18,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:18,061] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:18,065] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run28
[2019-04-04 09:59:18,102] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 09:59:18,104] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 09:59:18,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run28
[2019-04-04 09:59:29,644] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.5566892e-13 2.5588798e-11 1.7336357e-22 3.9082423e-13 4.0000536e-13
 1.0998370e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:59:29,645] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0362
[2019-04-04 09:59:29,701] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.98698591364652, 0.3143169160407694, 0.0, 1.0, 83795.70213747746], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 158400.0000, 
sim time next is 159000.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 25.01513880113686, 0.3172113489269448, 1.0, 1.0, 52896.85902126878], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5845949000947384, 0.6057371163089816, 1.0, 1.0, 0.25188980486318463], 
reward next is 0.7481, 
noisyNet noise sample is [array([0.13350631], dtype=float32), -1.5429536]. 
=============================================
[2019-04-04 09:59:29,748] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.53976 ]
 [78.874695]
 [78.717636]
 [78.58873 ]
 [78.693245]], R is [[77.15470886]
 [76.98413849]
 [76.68858337]
 [76.45811462]
 [76.52872467]].
[2019-04-04 09:59:36,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3725036e-13 3.2575349e-12 1.6492806e-23 7.5074678e-14 1.1890899e-13
 2.6739856e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 09:59:36,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3313
[2019-04-04 09:59:36,900] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 61.0, 49.66666666666667, 0.0, 26.0, 25.90998291930379, 0.3794848360184744, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 229200.0000, 
sim time next is 229800.0000, 
raw observation next is [-3.3, 61.5, 43.33333333333334, 0.0, 26.0, 25.9265472076893, 0.3730608492451919, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.37119113573407203, 0.615, 0.1444444444444445, 0.0, 0.6666666666666666, 0.6605456006407749, 0.6243536164150639, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4237296], dtype=float32), 1.5680466]. 
=============================================
[2019-04-04 09:59:42,730] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3507723e-11 6.7354268e-11 1.2562260e-21 2.3825818e-12 1.5246108e-12
 9.7247480e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 09:59:42,731] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3288
[2019-04-04 09:59:42,760] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.0, 76.33333333333334, 0.0, 0.0, 26.0, 22.0801167179802, -0.4147324806918082, 0.0, 1.0, 48610.76102454025], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 366000.0000, 
sim time next is 366600.0000, 
raw observation next is [-16.1, 77.16666666666666, 0.0, 0.0, 26.0, 21.9875273075273, -0.4363105681234463, 0.0, 1.0, 48660.89965763258], 
processed observation next is [1.0, 0.21739130434782608, 0.01662049861495839, 0.7716666666666666, 0.0, 0.0, 0.6666666666666666, 0.33229394229394177, 0.35456314395885125, 0.0, 1.0, 0.2317185697982504], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.7575383], dtype=float32), -1.2353158]. 
=============================================
[2019-04-04 09:59:52,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1222347e-12 3.3993648e-12 1.1828942e-24 2.6385643e-13 5.1890058e-14
 4.3517607e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 09:59:52,311] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9587
[2019-04-04 09:59:52,410] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.61666666666667, 80.5, 8.333333333333332, 192.3333333333333, 26.0, 23.0292743874449, -0.1737174884420432, 1.0, 1.0, 117085.9982290762], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 373800.0000, 
sim time next is 374400.0000, 
raw observation next is [-16.7, 81.0, 12.5, 263.5, 26.0, 23.34937087830939, -0.1171762221104732, 0.0, 1.0, 96285.22724192735], 
processed observation next is [1.0, 0.34782608695652173, 0.0, 0.81, 0.041666666666666664, 0.29116022099447514, 0.6666666666666666, 0.4457809065257825, 0.4609412592965089, 0.0, 1.0, 0.458501082104416], 
reward next is 0.5415, 
noisyNet noise sample is [array([-0.23447189], dtype=float32), -0.4954183]. 
=============================================
[2019-04-04 10:00:01,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.6699774e-13 1.4481302e-12 1.6372443e-24 2.9041358e-14 7.7419848e-14
 1.0643477e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:01,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9118
[2019-04-04 10:00:01,943] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 83.0, 38.0, 26.0, 24.89313104816942, 0.222057482162332, 0.0, 1.0, 41586.89701356961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 660600.0000, 
sim time next is 661200.0000, 
raw observation next is [-0.6, 54.0, 73.66666666666667, 34.16666666666666, 26.0, 24.8821419930352, 0.220668537675879, 0.0, 1.0, 46427.79326338567], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.24555555555555558, 0.03775322283609575, 0.6666666666666666, 0.5735118327529335, 0.573556179225293, 0.0, 1.0, 0.22108472982564603], 
reward next is 0.7789, 
noisyNet noise sample is [array([-0.01272568], dtype=float32), -0.24439646]. 
=============================================
[2019-04-04 10:00:07,480] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1672353e-12 7.2704048e-12 1.8991787e-24 9.9125799e-14 2.3497079e-13
 1.5052548e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:07,483] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3766
[2019-04-04 10:00:07,498] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.06897386127001, 0.2861725372592945, 0.0, 1.0, 42981.79043260738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 594000.0000, 
sim time next is 594600.0000, 
raw observation next is [-2.8, 83.0, 0.0, 0.0, 26.0, 25.0371190713614, 0.2803696009345246, 0.0, 1.0, 43005.45613726814], 
processed observation next is [0.0, 0.9130434782608695, 0.38504155124653744, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5864265892801166, 0.5934565336448415, 0.0, 1.0, 0.20478788636794354], 
reward next is 0.7952, 
noisyNet noise sample is [array([0.33049318], dtype=float32), 0.056799993]. 
=============================================
[2019-04-04 10:00:10,558] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0896589e-12 1.9188561e-12 6.6093143e-25 6.6653443e-14 1.0997893e-13
 3.4538780e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:10,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6169
[2019-04-04 10:00:10,602] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 85.66666666666667, 0.0, 0.0, 26.0, 24.67178567700094, 0.2007848500020278, 0.0, 1.0, 42380.73982967357], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 603600.0000, 
sim time next is 604200.0000, 
raw observation next is [-3.4, 86.33333333333333, 0.0, 0.0, 26.0, 24.65282174625434, 0.1945032939196761, 0.0, 1.0, 42341.66524791199], 
processed observation next is [0.0, 1.0, 0.368421052631579, 0.8633333333333333, 0.0, 0.0, 0.6666666666666666, 0.5544018121878617, 0.5648344313065586, 0.0, 1.0, 0.20162697737100946], 
reward next is 0.7984, 
noisyNet noise sample is [array([-0.15298088], dtype=float32), 0.07218536]. 
=============================================
[2019-04-04 10:00:11,880] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3585756e-12 8.4043024e-12 2.6914011e-24 2.8756625e-13 3.9444283e-13
 4.3039523e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:11,886] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5200
[2019-04-04 10:00:11,929] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.39486097840527, 0.1455057243631493, 0.0, 1.0, 42119.83469298471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 609600.0000, 
sim time next is 610200.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.40788801318707, 0.1406210216017837, 0.0, 1.0, 42102.3437923145], 
processed observation next is [0.0, 0.043478260869565216, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5339906677655891, 0.5468736738672613, 0.0, 1.0, 0.20048735139197382], 
reward next is 0.7995, 
noisyNet noise sample is [array([-0.6557864], dtype=float32), -1.8882551]. 
=============================================
[2019-04-04 10:00:17,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.8076925e-13 5.8099428e-12 7.3745730e-25 5.7099318e-14 1.1470580e-13
 4.6604838e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:17,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0005
[2019-04-04 10:00:17,672] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.90778369893003, 0.2753594970344416, 0.0, 1.0, 42407.1798798204], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 854400.0000, 
sim time next is 855000.0000, 
raw observation next is [-3.4, 83.0, 0.0, 0.0, 26.0, 24.87166616670159, 0.2683075853839145, 0.0, 1.0, 42276.96126628766], 
processed observation next is [1.0, 0.9130434782608695, 0.368421052631579, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5726388472251326, 0.5894358617946381, 0.0, 1.0, 0.20131886317279837], 
reward next is 0.7987, 
noisyNet noise sample is [array([-1.4540324], dtype=float32), 1.7055888]. 
=============================================
[2019-04-04 10:00:17,683] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.57053 ]
 [81.4761  ]
 [81.35758 ]
 [81.160164]
 [81.01806 ]], R is [[81.59658813]
 [81.57868195]
 [81.55982971]
 [81.53824615]
 [81.50778198]].
[2019-04-04 10:00:18,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0878400e-14 2.5393999e-13 3.9963515e-26 3.9332005e-15 9.9715116e-15
 1.5858424e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:18,126] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0394
[2019-04-04 10:00:18,147] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 64.5, 102.3333333333333, 542.0, 26.0, 25.82353591546933, 0.3681357159940455, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 731400.0000, 
sim time next is 732000.0000, 
raw observation next is [-0.6, 63.00000000000001, 93.16666666666666, 660.5000000000001, 26.0, 25.82442271861845, 0.3752387010553447, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.6300000000000001, 0.31055555555555553, 0.7298342541436466, 0.6666666666666666, 0.6520352265515376, 0.6250795670184482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.63429064], dtype=float32), -1.0164874]. 
=============================================
[2019-04-04 10:00:18,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[89.69461 ]
 [89.28448 ]
 [88.6291  ]
 [87.712425]
 [86.95367 ]], R is [[89.98741913]
 [90.0875473 ]
 [90.18667603]
 [90.28481293]
 [90.38196564]].
[2019-04-04 10:00:25,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4529329e-14 1.3576397e-12 6.3911262e-25 1.1108378e-14 5.4313531e-15
 4.8005423e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:25,558] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1670
[2019-04-04 10:00:25,582] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 45.33333333333333, 79.33333333333333, 21.66666666666667, 26.0, 25.59122307874308, 0.3852151382067355, 1.0, 1.0, 24271.85937881611], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 748200.0000, 
sim time next is 748800.0000, 
raw observation next is [-0.6, 45.0, 76.5, 17.0, 26.0, 25.67063217322353, 0.2942881602218132, 1.0, 1.0, 23438.78694443085], 
processed observation next is [1.0, 0.6956521739130435, 0.44598337950138506, 0.45, 0.255, 0.01878453038674033, 0.6666666666666666, 0.6392193477686275, 0.5980960534072711, 1.0, 1.0, 0.11161327116395642], 
reward next is 0.8884, 
noisyNet noise sample is [array([1.0263073], dtype=float32), 0.3904376]. 
=============================================
[2019-04-04 10:00:44,126] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4293432e-13 1.4098570e-12 2.6117493e-26 2.2559615e-14 2.3494334e-14
 1.2190897e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:44,126] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0546
[2019-04-04 10:00:44,184] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.8, 83.0, 0.0, 0.0, 26.0, 25.48069166941104, 0.4610635938767084, 0.0, 1.0, 18757.31018437667], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 969000.0000, 
sim time next is 969600.0000, 
raw observation next is [8.8, 83.0, 0.0, 0.0, 26.0, 25.53902361684786, 0.4560312125260388, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.21739130434782608, 0.7063711911357342, 0.83, 0.0, 0.0, 0.6666666666666666, 0.628251968070655, 0.6520104041753463, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3181652], dtype=float32), -0.1711673]. 
=============================================
[2019-04-04 10:00:49,424] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5271546e-14 1.0714825e-13 3.0059658e-27 1.3632709e-15 2.1534161e-15
 2.5901219e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:49,424] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4679
[2019-04-04 10:00:49,585] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 100.0, 0.0, 0.0, 26.0, 24.86154588581087, 0.3362083880392246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 932400.0000, 
sim time next is 933000.0000, 
raw observation next is [4.5, 100.0, 0.0, 0.0, 26.0, 25.01040611665172, 0.343445083538449, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5872576177285319, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5842005097209766, 0.6144816945128163, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8304664], dtype=float32), -2.090129]. 
=============================================
[2019-04-04 10:00:49,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[91.93323]
 [92.07064]
 [92.08931]
 [91.63444]
 [90.96843]], R is [[91.97180939]
 [92.05209351]
 [91.93907166]
 [91.40485382]
 [90.6467514 ]].
[2019-04-04 10:00:52,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.0360225e-14 2.3809451e-13 1.3665331e-26 2.2584833e-15 1.7705826e-14
 9.7089745e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:00:52,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4946
[2019-04-04 10:00:52,716] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 81.5, 0.0, 0.0, 26.0, 25.47810020453428, 0.4507749146842999, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 963000.0000, 
sim time next is 963600.0000, 
raw observation next is [7.699999999999999, 82.0, 0.0, 0.0, 26.0, 25.46361490029802, 0.4467761501924883, 0.0, 1.0, 18759.29653333307], 
processed observation next is [1.0, 0.13043478260869565, 0.6759002770083103, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6219679083581683, 0.6489253833974961, 0.0, 1.0, 0.08932998349206223], 
reward next is 0.9107, 
noisyNet noise sample is [array([2.5201492], dtype=float32), -0.8170769]. 
=============================================
[2019-04-04 10:01:05,340] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0868157e-15 5.3326848e-15 4.1927059e-29 3.1479723e-16 1.1780941e-16
 3.7319065e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:01:05,355] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4641
[2019-04-04 10:01:05,418] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.38333333333333, 82.5, 74.0, 179.0, 26.0, 26.52330486311274, 0.7453522686550015, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1069800.0000, 
sim time next is 1070400.0000, 
raw observation next is [12.56666666666667, 82.0, 87.00000000000001, 206.5, 26.0, 26.56359825573097, 0.7688858622328221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.8107109879963068, 0.82, 0.29000000000000004, 0.2281767955801105, 0.6666666666666666, 0.7136331879775808, 0.7562952874109407, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.96861887], dtype=float32), -0.045518514]. 
=============================================
[2019-04-04 10:01:08,397] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 10:01:08,397] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:01:08,398] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:01:08,400] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:01:08,400] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:01:08,439] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run38
[2019-04-04 10:01:08,429] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:01:08,440] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:01:08,441] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run38
[2019-04-04 10:01:08,401] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run38
[2019-04-04 10:02:16,985] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1528517], dtype=float32), -0.25802505]
[2019-04-04 10:02:16,985] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [3.250123494, 81.79993729333333, 40.38538568166667, 0.0, 26.0, 24.84280775146939, 0.4200654479432537, 0.0, 1.0, 18718.34338255145]
[2019-04-04 10:02:16,985] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:02:16,986] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.4878003e-13 6.9564656e-13 4.9050878e-26 1.8630303e-14 2.8998295e-14
 8.3586879e-17 1.0000000e+00], sampled 0.8739695813817223
[2019-04-04 10:02:28,380] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1528517], dtype=float32), -0.25802505]
[2019-04-04 10:02:28,381] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [10.53333333333333, 86.0, 197.5, 355.3333333333333, 26.0, 25.69858964292271, 0.5376656794057856, 1.0, 1.0, 0.0]
[2019-04-04 10:02:28,381] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:02:28,382] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [3.2264760e-15 1.6325315e-14 1.0479966e-27 4.1019100e-16 4.5950780e-16
 2.2093709e-18 1.0000000e+00], sampled 0.8342042497445221
[2019-04-04 10:02:45,920] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1528517], dtype=float32), -0.25802505]
[2019-04-04 10:02:45,920] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.454981016666667, 72.68743447, 0.0, 0.0, 26.0, 24.93231684527302, 0.2693432276716683, 0.0, 1.0, 98635.55541780229]
[2019-04-04 10:02:45,920] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:02:45,922] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.5981974e-13 2.7332904e-12 1.3706955e-24 8.6986719e-14 6.7576783e-14
 6.9449216e-16 1.0000000e+00], sampled 0.16008455681010125
[2019-04-04 10:03:37,370] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1528517], dtype=float32), -0.25802505]
[2019-04-04 10:03:37,370] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-8.3, 61.0, 0.0, 0.0, 26.0, 24.52996560074185, 0.2576952982073826, 0.0, 1.0, 63733.67980211734]
[2019-04-04 10:03:37,370] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:03:37,371] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.5899211e-13 3.5732149e-12 1.3165266e-24 1.3329424e-13 1.3363892e-13
 7.0937405e-16 1.0000000e+00], sampled 0.36150667608401954
[2019-04-04 10:04:21,218] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.1528517], dtype=float32), -0.25802505]
[2019-04-04 10:04:21,219] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.333333333333333, 33.0, 115.1666666666667, 776.8333333333334, 26.0, 26.61289020800783, 0.6103998119332888, 1.0, 1.0, 0.0]
[2019-04-04 10:04:21,219] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:04:21,219] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.2734730e-14 1.6000710e-13 1.0543408e-26 4.3720131e-15 2.6528324e-15
 1.3500933e-17 1.0000000e+00], sampled 0.8703783418268134
[2019-04-04 10:04:21,651] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:04:49,563] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:04:53,051] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:04:54,089] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 3700000, evaluation results [3700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:05:02,816] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.11108415e-13 6.79194724e-14 3.16780694e-27 5.30219755e-15
 4.08860220e-15 4.91914301e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 10:05:02,816] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2920
[2019-04-04 10:05:02,821] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35590633610353, 0.3284045748231311, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1204800.0000, 
sim time next is 1205400.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35457843481877, 0.324250079432387, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5295482029015641, 0.6080833598107956, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33610857], dtype=float32), 0.7273119]. 
=============================================
[2019-04-04 10:05:23,504] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.3600069e-14 2.3587604e-13 1.7465428e-26 5.7763763e-15 5.8388285e-15
 1.6555577e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:05:23,505] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5198
[2019-04-04 10:05:23,521] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.933333333333333, 97.33333333333334, 75.5, 118.0, 26.0, 26.08557421801837, 0.5320640654010781, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1507200.0000, 
sim time next is 1507800.0000, 
raw observation next is [3.116666666666667, 96.66666666666666, 78.0, 235.9999999999999, 26.0, 26.04645600683811, 0.5472631076609404, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5489381348107111, 0.9666666666666666, 0.26, 0.2607734806629833, 0.6666666666666666, 0.6705380005698425, 0.6824210358869802, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.998915], dtype=float32), 1.1806363]. 
=============================================
[2019-04-04 10:05:25,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7846318e-13 1.7116112e-12 1.2052720e-25 2.8182877e-14 6.8469536e-14
 6.9639389e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:05:25,706] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6141
[2019-04-04 10:05:25,737] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.05, 79.0, 0.0, 0.0, 26.0, 25.85445954048084, 0.5830336300500613, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1549800.0000, 
sim time next is 1550400.0000, 
raw observation next is [5.866666666666667, 80.0, 0.0, 0.0, 26.0, 25.69904672548848, 0.5628863582583202, 0.0, 1.0, 33349.12372654081], 
processed observation next is [1.0, 0.9565217391304348, 0.6251154201292707, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6415872271240399, 0.6876287860861067, 0.0, 1.0, 0.15880535107876578], 
reward next is 0.8412, 
noisyNet noise sample is [array([0.36635715], dtype=float32), -1.1327186]. 
=============================================
[2019-04-04 10:05:36,271] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5570353e-12 5.6564788e-11 2.4326022e-23 4.4210566e-13 2.9499287e-12
 3.8344885e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:05:36,272] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9348
[2019-04-04 10:05:36,307] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.12298599387675, 0.3341018301897815, 0.0, 1.0, 49784.77058361328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1799400.0000, 
sim time next is 1800000.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.12513724839566, 0.330110763564654, 0.0, 1.0, 47430.06844452791], 
processed observation next is [0.0, 0.8695652173913043, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.593761437366305, 0.6100369211882181, 0.0, 1.0, 0.22585746878346624], 
reward next is 0.7741, 
noisyNet noise sample is [array([0.68951356], dtype=float32), -2.384875]. 
=============================================
[2019-04-04 10:05:36,313] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[79.51543 ]
 [79.485725]
 [79.394936]
 [79.181595]
 [78.87152 ]], R is [[79.49115753]
 [79.45917511]
 [79.39051819]
 [79.21977997]
 [78.85054779]].
[2019-04-04 10:05:39,844] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7760906e-12 1.6128283e-11 1.9255216e-23 1.2820665e-13 6.8324112e-13
 3.4851143e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:05:39,844] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5905
[2019-04-04 10:05:39,896] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 83.66666666666667, 121.3333333333333, 0.0, 26.0, 24.94728024030015, 0.3460635989648097, 0.0, 1.0, 34110.19580089922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1770600.0000, 
sim time next is 1771200.0000, 
raw observation next is [-2.3, 83.0, 122.5, 0.0, 26.0, 24.94534258944141, 0.345227818404622, 0.0, 1.0, 39828.05144401979], 
processed observation next is [0.0, 0.5217391304347826, 0.3988919667590028, 0.83, 0.4083333333333333, 0.0, 0.6666666666666666, 0.5787785491201175, 0.6150759394682074, 0.0, 1.0, 0.18965738782866565], 
reward next is 0.8103, 
noisyNet noise sample is [array([-1.0773996], dtype=float32), 0.4185627]. 
=============================================
[2019-04-04 10:05:42,071] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6893474e-12 8.3976680e-12 1.5160999e-23 4.4740581e-13 5.0684955e-13
 2.6970560e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:05:42,071] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5501
[2019-04-04 10:05:42,123] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 83.66666666666667, 39.0, 0.0, 26.0, 25.21908165048433, 0.3722468277059399, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1759800.0000, 
sim time next is 1760400.0000, 
raw observation next is [-1.7, 83.0, 45.5, 0.0, 26.0, 25.18859009105246, 0.3570922791250357, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.4155124653739613, 0.83, 0.15166666666666667, 0.0, 0.6666666666666666, 0.5990491742543718, 0.6190307597083452, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4303732], dtype=float32), 0.18489791]. 
=============================================
[2019-04-04 10:05:46,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.4497197e-12 8.5246629e-11 1.3707858e-22 2.0306278e-12 2.1838208e-12
 1.6815096e-14 1.0000000e+00], sum to 1.0000
[2019-04-04 10:05:46,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2761
[2019-04-04 10:05:46,095] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.483333333333334, 78.66666666666667, 0.0, 0.0, 26.0, 23.72035844973043, -0.06884632696630637, 0.0, 1.0, 45157.22825599468], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1915800.0000, 
sim time next is 1916400.0000, 
raw observation next is [-8.566666666666666, 79.33333333333334, 0.0, 0.0, 26.0, 23.69221791320683, -0.07373859808041104, 0.0, 1.0, 45241.56699042575], 
processed observation next is [1.0, 0.17391304347826086, 0.22530009233610343, 0.7933333333333334, 0.0, 0.0, 0.6666666666666666, 0.4743514927672357, 0.47542046730652965, 0.0, 1.0, 0.21543603328774166], 
reward next is 0.7846, 
noisyNet noise sample is [array([-0.13675216], dtype=float32), -0.61220765]. 
=============================================
[2019-04-04 10:05:54,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2852356e-14 4.1325586e-13 7.9329771e-25 3.2233530e-14 2.3790374e-14
 1.3395057e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:05:54,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6944
[2019-04-04 10:05:54,839] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 62.0, 80.33333333333334, 0.0, 26.0, 25.61369509099515, 0.3267668062394954, 1.0, 1.0, 53521.89651961753], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1956000.0000, 
sim time next is 1956600.0000, 
raw observation next is [-2.8, 62.0, 74.0, 0.0, 26.0, 25.63432904141165, 0.3237720360911077, 1.0, 1.0, 36291.59539592021], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.24666666666666667, 0.0, 0.6666666666666666, 0.6361940867843042, 0.6079240120303693, 1.0, 1.0, 0.17281712093295337], 
reward next is 0.8272, 
noisyNet noise sample is [array([0.16251251], dtype=float32), 0.1835464]. 
=============================================
[2019-04-04 10:06:03,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.8454991e-14 5.9730812e-13 2.8069633e-25 1.5129525e-14 1.1964878e-14
 2.9423143e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:03,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6588
[2019-04-04 10:06:03,191] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 79.0, 126.0, 0.0, 26.0, 26.29660625242786, 0.4902889879887155, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2037600.0000, 
sim time next is 2038200.0000, 
raw observation next is [-4.0, 80.16666666666667, 118.6666666666667, 0.0, 26.0, 26.35890798986649, 0.4908797682661111, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3518005540166205, 0.8016666666666667, 0.39555555555555566, 0.0, 0.6666666666666666, 0.6965756658222076, 0.6636265894220371, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16620216], dtype=float32), -0.9987567]. 
=============================================
[2019-04-04 10:06:03,336] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.6046757e-14 1.2154237e-12 5.5126912e-25 1.2252449e-14 1.8047262e-14
 3.2589079e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:03,336] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7772
[2019-04-04 10:06:03,416] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 78.33333333333334, 153.3333333333333, 0.0, 26.0, 25.32275660983174, 0.3100822329044441, 1.0, 1.0, 35237.52014046651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2033400.0000, 
sim time next is 2034000.0000, 
raw observation next is [-4.5, 79.0, 152.0, 0.0, 26.0, 25.36736097517946, 0.2169405854829753, 1.0, 1.0, 32062.39217552042], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.79, 0.5066666666666667, 0.0, 0.6666666666666666, 0.6139467479316215, 0.5723135284943252, 1.0, 1.0, 0.15267805797866868], 
reward next is 0.8473, 
noisyNet noise sample is [array([-0.96385175], dtype=float32), -0.1672997]. 
=============================================
[2019-04-04 10:06:03,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.972305]
 [84.08242 ]
 [84.16256 ]
 [84.17579 ]
 [84.322975]], R is [[83.80725098]
 [83.80137634]
 [83.75696564]
 [83.66905212]
 [83.83235931]].
[2019-04-04 10:06:05,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5268462e-14 1.5907745e-13 3.3774592e-25 4.5971671e-15 7.7605497e-15
 7.2253743e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:05,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8624
[2019-04-04 10:06:05,156] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.3, 62.0, 120.3333333333333, 0.0, 26.0, 25.74158055138917, 0.3446914087737483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1951800.0000, 
sim time next is 1952400.0000, 
raw observation next is [-3.2, 62.0, 116.1666666666667, 0.0, 26.0, 25.76057828336438, 0.3445323225852374, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37396121883656513, 0.62, 0.38722222222222236, 0.0, 0.6666666666666666, 0.6467148569470318, 0.6148441075284125, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8733805], dtype=float32), -1.1027566]. 
=============================================
[2019-04-04 10:06:16,171] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.1464642e-13 3.0812674e-12 7.8931655e-24 6.6247741e-14 1.2654272e-13
 4.4467080e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:16,180] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0822
[2019-04-04 10:06:16,197] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.75020727658442, 0.2614209294493259, 0.0, 1.0, 42669.79333486462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2073000.0000, 
sim time next is 2073600.0000, 
raw observation next is [-4.5, 91.0, 0.0, 0.0, 26.0, 24.71242524942515, 0.2535448426576029, 0.0, 1.0, 42695.21773464028], 
processed observation next is [1.0, 0.0, 0.3379501385041552, 0.91, 0.0, 0.0, 0.6666666666666666, 0.559368770785429, 0.5845149475525343, 0.0, 1.0, 0.2033105606411442], 
reward next is 0.7967, 
noisyNet noise sample is [array([0.9051395], dtype=float32), 0.8719081]. 
=============================================
[2019-04-04 10:06:18,764] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4170850e-12 1.1866940e-11 1.9303682e-23 2.7043632e-13 5.5477015e-13
 8.5007252e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:18,765] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7504
[2019-04-04 10:06:18,775] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.64670618500334, -0.02123990416427912, 0.0, 1.0, 41927.15078724897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2185200.0000, 
sim time next is 2185800.0000, 
raw observation next is [-5.600000000000001, 75.0, 0.0, 0.0, 26.0, 23.65141884938635, -0.02152890688398877, 0.0, 1.0, 41938.48604799178], 
processed observation next is [1.0, 0.30434782608695654, 0.3074792243767313, 0.75, 0.0, 0.0, 0.6666666666666666, 0.4709515707821958, 0.49282369770533707, 0.0, 1.0, 0.19970707641900845], 
reward next is 0.8003, 
noisyNet noise sample is [array([-0.2126318], dtype=float32), -0.07227013]. 
=============================================
[2019-04-04 10:06:20,601] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.3961627e-12 9.9531720e-12 1.1361520e-23 1.1285941e-13 1.1138176e-12
 2.2824048e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:20,601] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5610
[2019-04-04 10:06:20,629] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.4, 91.0, 0.0, 0.0, 26.0, 23.47436865908049, -0.07519735986041629, 0.0, 1.0, 43071.84505978855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2271000.0000, 
sim time next is 2271600.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.40948940203087, -0.09018449569682807, 0.0, 1.0, 43041.82098807226], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.6666666666666666, 0.4507907835025726, 0.46993850143439064, 0.0, 1.0, 0.2049610523241536], 
reward next is 0.7950, 
noisyNet noise sample is [array([-1.7106072], dtype=float32), -0.18361843]. 
=============================================
[2019-04-04 10:06:24,506] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9164017e-12 7.1519149e-12 6.5511325e-24 1.3861942e-13 2.9209030e-13
 2.3620265e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:24,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7244
[2019-04-04 10:06:24,534] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.2283776002959, 0.09755834960926636, 0.0, 1.0, 42168.90449558542], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2173800.0000, 
sim time next is 2174400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 24.15727722824665, 0.09611187842708387, 0.0, 1.0, 42155.03236378913], 
processed observation next is [1.0, 0.17391304347826086, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5131064356872207, 0.5320372928090279, 0.0, 1.0, 0.2007382493513768], 
reward next is 0.7993, 
noisyNet noise sample is [array([-2.1302595], dtype=float32), -0.52549237]. 
=============================================
[2019-04-04 10:06:25,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2547979e-12 1.3982673e-11 4.7848753e-23 2.4697353e-13 1.0397339e-12
 4.3880364e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:25,121] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6436
[2019-04-04 10:06:25,148] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 87.0, 0.0, 0.0, 26.0, 24.05992721602802, 0.06347761018118757, 0.0, 1.0, 43610.77796549167], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2260800.0000, 
sim time next is 2261400.0000, 
raw observation next is [-8.483333333333334, 87.66666666666667, 0.0, 0.0, 26.0, 24.00926770653047, 0.05559269467764027, 0.0, 1.0, 43598.73583005442], 
processed observation next is [1.0, 0.17391304347826086, 0.2276084949215143, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5007723088775391, 0.51853089822588, 0.0, 1.0, 0.20761302776216392], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.8926653], dtype=float32), 0.20843491]. 
=============================================
[2019-04-04 10:06:31,883] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.0874563e-12 7.3326284e-12 1.7264084e-23 1.0366765e-12 5.8638565e-13
 4.0200429e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:06:31,885] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7567
[2019-04-04 10:06:31,902] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.28946653986403, 0.2866569717933605, 0.0, 1.0, 40744.81278979175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491800.0000, 
sim time next is 2492400.0000, 
raw observation next is [-0.8666666666666667, 31.66666666666667, 0.0, 0.0, 26.0, 25.30695946438672, 0.2863798082109396, 0.0, 1.0, 40354.34948562337], 
processed observation next is [0.0, 0.8695652173913043, 0.4385964912280702, 0.3166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6089132886988932, 0.5954599360703132, 0.0, 1.0, 0.1921635689791589], 
reward next is 0.8078, 
noisyNet noise sample is [array([0.46099228], dtype=float32), 0.24887252]. 
=============================================
[2019-04-04 10:07:04,657] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.45853610e-14 3.87586854e-13 3.42322098e-25 5.55879644e-15
 6.13563520e-15 1.01755444e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:07:04,658] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2528
[2019-04-04 10:07:04,709] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 57.5, 109.0, 788.0, 26.0, 26.12161876058272, 0.5801679936030221, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2727000.0000, 
sim time next is 2727600.0000, 
raw observation next is [-5.199999999999999, 57.0, 107.8333333333333, 778.8333333333334, 26.0, 26.28336113735246, 0.5975653108273895, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.31855955678670367, 0.57, 0.35944444444444434, 0.8605893186003684, 0.6666666666666666, 0.6902800947793718, 0.6991884369424631, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04169755], dtype=float32), 0.074118]. 
=============================================
[2019-04-04 10:07:06,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.8015451e-14 1.0702331e-12 1.6562268e-25 5.7738130e-14 3.4896456e-14
 4.1879678e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:06,193] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0262
[2019-04-04 10:07:06,277] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 64.0, 18.0, 34.49999999999999, 26.0, 24.35962596452802, 0.2150456069623052, 1.0, 1.0, 192370.6314754373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2792400.0000, 
sim time next is 2793000.0000, 
raw observation next is [-6.166666666666666, 64.0, 35.99999999999999, 68.99999999999999, 26.0, 24.92644148372333, 0.2996700987276067, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.29178208679593726, 0.64, 0.11999999999999998, 0.07624309392265191, 0.6666666666666666, 0.5772034569769442, 0.5998900329092022, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44320574], dtype=float32), 1.7805399]. 
=============================================
[2019-04-04 10:07:06,284] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[86.61614 ]
 [84.565605]
 [82.01216 ]
 [77.11807 ]
 [77.121574]], R is [[87.65697479]
 [86.86434937]
 [86.02915192]
 [85.20703888]
 [85.05329132]].
[2019-04-04 10:07:14,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.2812956e-13 9.3438642e-12 4.4589881e-24 2.3262032e-13 3.0855356e-13
 5.5187788e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:14,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1132
[2019-04-04 10:07:14,967] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9216746468656, 0.2578790663546431, 0.0, 1.0, 55707.94438553952], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2866200.0000, 
sim time next is 2866800.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.9091503965053, 0.2503120424096703, 0.0, 1.0, 55732.5156238617], 
processed observation next is [1.0, 0.17391304347826086, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5757625330421083, 0.5834373474698901, 0.0, 1.0, 0.26539293154219856], 
reward next is 0.7346, 
noisyNet noise sample is [array([0.72234255], dtype=float32), 0.6580412]. 
=============================================
[2019-04-04 10:07:20,493] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1647327e-15 4.9967134e-14 7.8196384e-27 2.1025125e-16 7.0916838e-16
 2.5223917e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:20,493] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3078
[2019-04-04 10:07:20,529] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.333333333333333, 97.66666666666666, 112.8333333333333, 820.1666666666667, 26.0, 27.30905027318259, 0.835899487561572, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3156000.0000, 
sim time next is 3156600.0000, 
raw observation next is [7.166666666666667, 98.83333333333334, 112.6666666666667, 817.3333333333334, 26.0, 27.28227412314642, 0.8385597388096517, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6611265004616806, 0.9883333333333334, 0.37555555555555564, 0.9031307550644567, 0.6666666666666666, 0.7735228435955349, 0.7795199129365505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83096045], dtype=float32), 1.7710472]. 
=============================================
[2019-04-04 10:07:27,204] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1995533e-12 8.6606703e-12 3.6137548e-24 2.0095587e-13 2.5386686e-13
 7.8739510e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:27,204] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-04 10:07:27,217] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 24.65533785013277, 0.2799074046892829, 0.0, 1.0, 42921.64832973864], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2947200.0000, 
sim time next is 2947800.0000, 
raw observation next is [-2.833333333333333, 84.16666666666666, 0.0, 0.0, 26.0, 24.66953346760217, 0.2749367843015045, 0.0, 1.0, 42836.4864539621], 
processed observation next is [0.0, 0.08695652173913043, 0.3841181902123731, 0.8416666666666666, 0.0, 0.0, 0.6666666666666666, 0.5557944556335143, 0.5916455947671682, 0.0, 1.0, 0.20398326882839093], 
reward next is 0.7960, 
noisyNet noise sample is [array([-1.8924415], dtype=float32), 1.0518335]. 
=============================================
[2019-04-04 10:07:35,291] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5438592e-13 4.2676964e-12 3.5438988e-24 6.0384979e-14 6.3020695e-14
 5.7883511e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:35,293] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0573
[2019-04-04 10:07:35,348] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.25051926610011, 0.6073758929516184, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3264600.0000, 
sim time next is 3265200.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.65605556096883, 0.6190375233076216, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6380046300807359, 0.7063458411025406, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5049135], dtype=float32), -0.9097095]. 
=============================================
[2019-04-04 10:07:37,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.7953730e-14 8.0458047e-13 1.4323884e-25 3.0327748e-15 1.4758318e-14
 5.7454940e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:37,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2667
[2019-04-04 10:07:37,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1799577e-13 1.6000626e-12 4.4017303e-24 1.4061576e-14 9.7548800e-14
 4.3015536e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:37,021] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0220
[2019-04-04 10:07:37,023] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.7, 86.0, 109.0, 752.0, 26.0, 26.41783753803377, 0.6985344871061603, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3234600.0000, 
sim time next is 3235200.0000, 
raw observation next is [-2.6, 84.0, 109.6666666666667, 761.8333333333334, 26.0, 26.41052045180007, 0.7062095070588583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3905817174515236, 0.84, 0.3655555555555557, 0.841804788213628, 0.6666666666666666, 0.7008767043166726, 0.7354031690196194, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.347347], dtype=float32), 0.836261]. 
=============================================
[2019-04-04 10:07:37,069] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 100.0, 0.0, 0.0, 26.0, 25.30830175422471, 0.4896551301587411, 0.0, 1.0, 40654.48466676121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3218400.0000, 
sim time next is 3219000.0000, 
raw observation next is [-3.0, 98.66666666666667, 0.0, 0.0, 26.0, 25.34078315742936, 0.4829566565846545, 0.0, 1.0, 40700.53279956403], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.9866666666666667, 0.0, 0.0, 0.6666666666666666, 0.6117319297857801, 0.6609855521948848, 0.0, 1.0, 0.19381206095030493], 
reward next is 0.8062, 
noisyNet noise sample is [array([-0.28152788], dtype=float32), -0.17851533]. 
=============================================
[2019-04-04 10:07:37,088] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.29411 ]
 [79.436554]
 [79.58888 ]
 [79.729805]
 [79.832405]], R is [[79.19688416]
 [79.21131897]
 [79.22580719]
 [79.24028015]
 [79.2546463 ]].
[2019-04-04 10:07:37,898] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.6164615e-15 1.7634216e-13 1.2669748e-25 3.1875364e-15 5.7611480e-15
 5.9339839e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:37,899] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8519
[2019-04-04 10:07:37,916] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.266666666666667, 77.0, 112.3333333333333, 801.1666666666666, 26.0, 26.54874746733879, 0.7419567698580893, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3237600.0000, 
sim time next is 3238200.0000, 
raw observation next is [-2.2, 75.5, 113.0, 811.0, 26.0, 26.59173082707487, 0.7463774498329189, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4016620498614959, 0.755, 0.37666666666666665, 0.8961325966850828, 0.6666666666666666, 0.7159775689229058, 0.7487924832776396, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.25503746], dtype=float32), -0.8536829]. 
=============================================
[2019-04-04 10:07:46,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8720289e-13 2.8500040e-13 3.2341737e-25 7.6542073e-15 6.8992697e-15
 5.9741862e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:46,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2415
[2019-04-04 10:07:46,458] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.166666666666667, 43.33333333333333, 110.0, 804.0, 26.0, 25.2044554729403, 0.449187788383442, 0.0, 1.0, 18694.90121731825], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3592200.0000, 
sim time next is 3592800.0000, 
raw observation next is [-1.0, 42.0, 108.0, 800.0, 26.0, 25.19510810293284, 0.4507573974873288, 0.0, 1.0, 18695.07061288806], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.36, 0.8839779005524862, 0.6666666666666666, 0.5995923419110699, 0.6502524658291096, 0.0, 1.0, 0.08902414577565743], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.06053442], dtype=float32), 0.24131851]. 
=============================================
[2019-04-04 10:07:48,454] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5191479e-12 5.6376700e-12 1.8124992e-24 8.2405450e-14 3.3410037e-13
 1.3568117e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:48,454] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0149
[2019-04-04 10:07:48,473] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.0681121950617, 0.3552229490603145, 0.0, 1.0, 41167.81821213078], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3373200.0000, 
sim time next is 3373800.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.08963429420478, 0.3454514509156194, 0.0, 1.0, 41189.3994244978], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5908028578503984, 0.6151504836385399, 0.0, 1.0, 0.19613999725951334], 
reward next is 0.8039, 
noisyNet noise sample is [array([0.8043939], dtype=float32), 0.95131063]. 
=============================================
[2019-04-04 10:07:50,135] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2105128e-13 1.6716747e-12 2.3158708e-24 3.0692164e-14 4.9541046e-14
 5.5486217e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:50,136] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1774
[2019-04-04 10:07:50,250] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 78.66666666666667, 0.0, 0.0, 26.0, 23.79699428012074, 0.149508528095387, 1.0, 1.0, 202374.3300019872], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3309600.0000, 
sim time next is 3310200.0000, 
raw observation next is [-11.0, 80.0, 2.0, 94.0, 26.0, 24.15888290098042, 0.2604417523251015, 1.0, 1.0, 203137.0028249161], 
processed observation next is [1.0, 0.30434782608695654, 0.15789473684210528, 0.8, 0.006666666666666667, 0.10386740331491713, 0.6666666666666666, 0.5132402417483682, 0.5868139174417005, 1.0, 1.0, 0.967319061071029], 
reward next is 0.0327, 
noisyNet noise sample is [array([-1.273433], dtype=float32), -0.8925836]. 
=============================================
[2019-04-04 10:07:51,695] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.12590005e-13 2.31007813e-13 1.29636730e-25 1.10319850e-14
 1.32998572e-14 5.43373058e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 10:07:51,696] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0179
[2019-04-04 10:07:51,707] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.5, 54.5, 64.0, 522.0, 26.0, 25.49433936147977, 0.4675428400757573, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3688200.0000, 
sim time next is 3688800.0000, 
raw observation next is [4.333333333333334, 56.0, 55.83333333333334, 462.5, 26.0, 25.47945002848898, 0.4568034243897802, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.58264081255771, 0.56, 0.18611111111111114, 0.511049723756906, 0.6666666666666666, 0.6232875023740817, 0.6522678081299268, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5775775], dtype=float32), -1.2835525]. 
=============================================
[2019-04-04 10:07:53,479] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [8.4056443e-13 1.9373431e-12 5.3721250e-25 6.2937807e-14 8.9606863e-14
 2.1212995e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:53,479] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3839
[2019-04-04 10:07:53,494] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.42104113657188, 0.4438307277997307, 0.0, 1.0, 49728.5877836527], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3460200.0000, 
sim time next is 3460800.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.38480998594849, 0.4631099262268282, 0.0, 1.0, 63628.44806702762], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6154008321623742, 0.6543699754089427, 0.0, 1.0, 0.30299260984298865], 
reward next is 0.6970, 
noisyNet noise sample is [array([1.9192337], dtype=float32), 0.66342133]. 
=============================================
[2019-04-04 10:07:57,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3696926e-12 6.5928764e-12 1.6978405e-24 2.1335979e-13 2.9346290e-13
 6.5798361e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:57,161] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2348
[2019-04-04 10:07:57,174] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.17656483835031, 0.3825522573968878, 0.0, 1.0, 41110.61505693721], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3554400.0000, 
sim time next is 3555000.0000, 
raw observation next is [-3.5, 68.0, 0.0, 0.0, 26.0, 25.13533107948117, 0.3741872422875139, 0.0, 1.0, 41174.83124452666], 
processed observation next is [0.0, 0.13043478260869565, 0.36565096952908593, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5946109232900975, 0.6247290807625047, 0.0, 1.0, 0.19607062497393646], 
reward next is 0.8039, 
noisyNet noise sample is [array([2.3590543], dtype=float32), 0.4603522]. 
=============================================
[2019-04-04 10:07:57,178] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.67951]
 [82.76321]
 [82.65208]
 [82.52863]
 [82.49003]], R is [[82.56642914]
 [82.5450058 ]
 [82.52420807]
 [82.50401306]
 [82.48434448]].
[2019-04-04 10:07:57,328] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3027288e-12 7.4088244e-12 3.5630256e-24 7.2453687e-14 1.7494549e-13
 3.4322196e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:07:57,328] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9192
[2019-04-04 10:07:57,343] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 74.0, 0.0, 0.0, 26.0, 25.39492007453062, 0.3764395423191373, 0.0, 1.0, 55891.53488406067], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3810600.0000, 
sim time next is 3811200.0000, 
raw observation next is [-4.0, 73.0, 0.0, 0.0, 26.0, 25.29383173739667, 0.3611147029277801, 0.0, 1.0, 61469.29573365241], 
processed observation next is [1.0, 0.08695652173913043, 0.3518005540166205, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6078193114497225, 0.6203715676425934, 0.0, 1.0, 0.2927109320650115], 
reward next is 0.7073, 
noisyNet noise sample is [array([1.1745416], dtype=float32), 0.028198905]. 
=============================================
[2019-04-04 10:08:01,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2183854e-12 2.1891423e-12 9.0615713e-26 7.2210047e-14 1.7263250e-13
 2.2802715e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:01,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4594
[2019-04-04 10:08:01,390] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.833333333333334, 25.33333333333334, 0.0, 0.0, 26.0, 25.53835378274871, 0.3650164302570875, 0.0, 1.0, 18746.78486180462], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3651000.0000, 
sim time next is 3651600.0000, 
raw observation next is [9.666666666666668, 25.66666666666667, 0.0, 0.0, 26.0, 25.5513507154063, 0.3612629711000901, 0.0, 1.0, 18745.13807559275], 
processed observation next is [0.0, 0.2608695652173913, 0.7303785780240075, 0.2566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6292792262838584, 0.6204209903666967, 0.0, 1.0, 0.08926256226472738], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.66410893], dtype=float32), 0.8971212]. 
=============================================
[2019-04-04 10:08:06,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.8835230e-15 6.5901162e-14 2.2436321e-26 9.7040224e-16 2.7190389e-15
 4.4444650e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:06,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5354
[2019-04-04 10:08:06,626] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 68.0, 115.0, 822.0, 26.0, 26.42946085427081, 0.5863988178781246, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3756600.0000, 
sim time next is 3757200.0000, 
raw observation next is [-2.333333333333333, 67.0, 115.6666666666667, 823.1666666666666, 26.0, 26.46184196489986, 0.5943232283631309, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.3979686057248385, 0.67, 0.38555555555555565, 0.9095764272559852, 0.6666666666666666, 0.7051534970749884, 0.6981077427877103, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3218872], dtype=float32), -0.3427201]. 
=============================================
[2019-04-04 10:08:07,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5722829e-13 4.7353302e-12 2.9214088e-25 1.3408907e-13 1.6067563e-13
 3.7896257e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:07,204] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7662
[2019-04-04 10:08:07,238] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.5, 62.5, 0.0, 0.0, 26.0, 25.74935884001472, 0.4686151033708792, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3702600.0000, 
sim time next is 3703200.0000, 
raw observation next is [2.333333333333333, 62.33333333333333, 0.0, 0.0, 26.0, 25.75024121367722, 0.4579943019977479, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.5272391505078486, 0.6233333333333333, 0.0, 0.0, 0.6666666666666666, 0.6458534344731017, 0.6526647673325826, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.356736], dtype=float32), -0.14951856]. 
=============================================
[2019-04-04 10:08:13,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1895654e-12 8.3418897e-12 1.4424460e-23 3.4540808e-13 1.4633395e-13
 2.6266511e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:13,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3843
[2019-04-04 10:08:13,613] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 36.33333333333333, 0.0, 0.0, 26.0, 24.84516892751611, 0.2286926485071905, 0.0, 1.0, 40387.37840132401], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [-4.166666666666667, 35.16666666666667, 0.0, 0.0, 26.0, 24.78929686555816, 0.2317953975124562, 0.0, 1.0, 40362.70866115568], 
processed observation next is [1.0, 0.17391304347826086, 0.3471837488457987, 0.35166666666666674, 0.0, 0.0, 0.6666666666666666, 0.5657747387965134, 0.5772651325041521, 0.0, 1.0, 0.19220337457693182], 
reward next is 0.8078, 
noisyNet noise sample is [array([1.0304978], dtype=float32), -0.7965086]. 
=============================================
[2019-04-04 10:08:14,176] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [8.73864495e-15 2.22072455e-13 4.01741640e-25 1.17891295e-15
 6.29846119e-15 1.13205854e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 10:08:14,190] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.5306
[2019-04-04 10:08:14,206] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333334, 38.0, 108.6666666666667, 800.0, 26.0, 26.89281480347653, 0.7668123781565811, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3939000.0000, 
sim time next is 3939600.0000, 
raw observation next is [-4.666666666666667, 38.0, 106.8333333333333, 794.0, 26.0, 26.96964222180745, 0.7699698840369908, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3333333333333333, 0.38, 0.356111111111111, 0.8773480662983425, 0.6666666666666666, 0.7474701851506209, 0.7566566280123302, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1034226], dtype=float32), 0.6339898]. 
=============================================
[2019-04-04 10:08:15,666] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4315551e-14 1.2203942e-13 1.9375213e-26 6.6385291e-16 1.3704195e-15
 1.6665069e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:15,672] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2341
[2019-04-04 10:08:15,694] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 43.0, 64.0, 551.0, 26.0, 26.40236943220677, 0.678832453068794, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3861000.0000, 
sim time next is 3861600.0000, 
raw observation next is [3.0, 42.33333333333334, 56.33333333333334, 489.0, 26.0, 25.80386758894072, 0.627994469535578, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.42333333333333345, 0.18777777777777782, 0.5403314917127072, 0.6666666666666666, 0.6503222990783932, 0.7093314898451927, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3304372], dtype=float32), 0.5484816]. 
=============================================
[2019-04-04 10:08:16,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3662965e-14 2.2609250e-13 5.5506071e-26 1.5955841e-14 2.3192054e-15
 1.8507023e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:16,480] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9627
[2019-04-04 10:08:16,500] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.833333333333334, 49.0, 108.6666666666667, 747.3333333333334, 26.0, 26.31455525900564, 0.5769094090833565, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3924600.0000, 
sim time next is 3925200.0000, 
raw observation next is [-6.666666666666667, 49.0, 110.8333333333333, 761.1666666666667, 26.0, 26.34016524656331, 0.5861658492813148, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.27793167128347185, 0.49, 0.36944444444444435, 0.8410681399631676, 0.6666666666666666, 0.6950137705469425, 0.6953886164271049, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6371343], dtype=float32), -0.16771209]. 
=============================================
[2019-04-04 10:08:40,367] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5627952e-15 1.0748880e-14 4.7242132e-28 2.0393492e-16 1.7501465e-16
 1.1617938e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:40,367] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1703
[2019-04-04 10:08:40,383] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.73333333333333, 28.66666666666666, 117.5, 851.1666666666667, 26.0, 27.53326319815614, 0.9393590298643448, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4365600.0000, 
sim time next is 4366200.0000, 
raw observation next is [14.66666666666667, 28.83333333333334, 117.0, 849.3333333333334, 26.0, 26.65927911508724, 0.8709415447719969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8688827331486613, 0.2883333333333334, 0.39, 0.9384898710865562, 0.6666666666666666, 0.7216065929239367, 0.7903138482573323, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6533557], dtype=float32), -0.37586296]. 
=============================================
[2019-04-04 10:08:43,042] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.3321340e-12 9.8086973e-12 1.4326896e-24 2.5726925e-13 1.4035458e-13
 2.6758107e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:08:43,044] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9306
[2019-04-04 10:08:43,064] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.34046830198195, 0.3288955655182493, 0.0, 1.0, 41405.95315915846], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4251600.0000, 
sim time next is 4252200.0000, 
raw observation next is [3.0, 45.66666666666666, 0.0, 0.0, 26.0, 25.38172546124664, 0.3301112130326635, 0.0, 1.0, 34996.03634585295], 
processed observation next is [0.0, 0.21739130434782608, 0.5457063711911359, 0.45666666666666655, 0.0, 0.0, 0.6666666666666666, 0.6151437884372198, 0.6100370710108879, 0.0, 1.0, 0.16664779212310926], 
reward next is 0.8334, 
noisyNet noise sample is [array([-0.34143072], dtype=float32), -0.61421895]. 
=============================================
[2019-04-04 10:09:09,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.9939626e-13 1.4114122e-12 5.8777087e-24 2.9418684e-14 1.2723782e-13
 1.4335920e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:09:09,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7780
[2019-04-04 10:09:09,102] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 36.5, 87.0, 608.3333333333334, 26.0, 25.19631805640931, 0.432807375894622, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4810200.0000, 
sim time next is 4810800.0000, 
raw observation next is [3.0, 36.0, 84.5, 578.6666666666666, 26.0, 25.1962139766116, 0.4320022157655599, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.36, 0.2816666666666667, 0.6394106813996316, 0.6666666666666666, 0.5996844980509666, 0.6440007385885199, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18611914], dtype=float32), -0.7433037]. 
=============================================
[2019-04-04 10:09:13,174] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-04 10:09:13,185] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:09:13,186] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:09:13,186] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:09:13,186] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:09:13,186] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:09:13,187] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:09:13,191] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run39
[2019-04-04 10:09:13,226] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run39
[2019-04-04 10:09:13,258] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run39
[2019-04-04 10:09:20,403] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14895257], dtype=float32), -0.25580916]
[2019-04-04 10:09:20,403] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [7.7, 93.0, 32.33333333333334, 0.0, 26.0, 22.94383014226748, -0.1925630636117169, 0.0, 1.0, 59349.60713792682]
[2019-04-04 10:09:20,403] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:09:20,404] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.51823833e-12 2.98548165e-12 4.77655426e-25 1.00178024e-13
 1.39886339e-13 5.40293624e-16 1.00000000e+00], sampled 0.8259439952283408
[2019-04-04 10:10:20,859] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14895257], dtype=float32), -0.25580916]
[2019-04-04 10:10:20,859] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [1.7, 92.0, 0.0, 0.0, 26.0, 25.4269419483258, 0.5425751728150076, 0.0, 1.0, 56002.25267370819]
[2019-04-04 10:10:20,859] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:10:20,860] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2906767e-12 3.5015773e-12 1.5655626e-24 8.9790778e-14 2.4055855e-13
 7.7677135e-16 1.0000000e+00], sampled 0.5999405530890219
[2019-04-04 10:10:54,561] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14895257], dtype=float32), -0.25580916]
[2019-04-04 10:10:54,562] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-1.1, 63.33333333333333, 147.3333333333333, 177.0, 26.0, 25.64933544441816, 0.49048840209023, 1.0, 1.0, 24845.80256859938]
[2019-04-04 10:10:54,562] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:10:54,563] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [5.7200474e-14 3.1533795e-13 5.4790211e-25 1.2501545e-14 8.1178909e-15
 1.0787441e-16 1.0000000e+00], sampled 0.7602940142746241
[2019-04-04 10:11:15,999] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14895257], dtype=float32), -0.25580916]
[2019-04-04 10:11:15,999] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [0.2, 84.0, 208.0, 207.5, 26.0, 25.40641305667433, 0.3553749618885828, 1.0, 1.0, 18681.06352385264]
[2019-04-04 10:11:15,999] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:11:16,000] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.9459674e-14 1.9302160e-13 2.9685563e-26 5.0231231e-15 4.4373570e-15
 3.7213761e-17 1.0000000e+00], sampled 0.3674679441540476
[2019-04-04 10:12:23,359] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:12:53,415] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.14895257], dtype=float32), -0.25580916]
[2019-04-04 10:12:53,415] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-4.704413466333333, 56.68288522333333, 0.0, 0.0, 26.0, 25.07427705393145, 0.2933604260558821, 0.0, 1.0, 38888.81959629931]
[2019-04-04 10:12:53,415] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:12:53,416] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.1996333e-12 1.3247192e-11 7.0544746e-24 4.1635250e-13 7.0145577e-13
 3.1857496e-15 1.0000000e+00], sampled 0.4666227437553234
[2019-04-04 10:12:56,160] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:13:00,873] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:13:01,908] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 3800000, evaluation results [3800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:13:06,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:06,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:06,343] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run29
[2019-04-04 10:13:07,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:07,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:07,498] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run29
[2019-04-04 10:13:09,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:09,033] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:09,036] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run29
[2019-04-04 10:13:20,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:20,890] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:20,898] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run29
[2019-04-04 10:13:23,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6927487e-15 2.0910167e-14 1.0036014e-27 3.5785411e-16 1.8446277e-16
 9.1548235e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:13:23,879] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1981
[2019-04-04 10:13:23,959] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.5, 25.66666666666667, 122.3333333333333, 851.6666666666666, 26.0, 26.98873903577919, 0.68900807678361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4967400.0000, 
sim time next is 4968000.0000, 
raw observation next is [6.0, 25.0, 122.5, 855.0, 26.0, 27.00235161463256, 0.7073898442344232, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6288088642659281, 0.25, 0.4083333333333333, 0.9447513812154696, 0.6666666666666666, 0.7501959678860466, 0.7357966147448077, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.463246], dtype=float32), -0.9097281]. 
=============================================
[2019-04-04 10:13:23,994] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[93.57235 ]
 [93.389595]
 [93.27611 ]
 [93.15385 ]
 [93.08878 ]], R is [[93.82187653]
 [93.88365936]
 [93.94482422]
 [94.00537872]
 [94.06532288]].
[2019-04-04 10:13:25,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:25,620] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:25,631] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run29
[2019-04-04 10:13:29,127] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:29,140] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:29,143] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run29
[2019-04-04 10:13:29,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:29,965] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:29,968] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run29
[2019-04-04 10:13:30,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:30,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:30,120] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run29
[2019-04-04 10:13:32,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:32,334] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:32,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:32,349] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:32,438] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run29
[2019-04-04 10:13:32,616] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run29
[2019-04-04 10:13:33,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:33,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:33,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run29
[2019-04-04 10:13:34,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:34,389] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:34,393] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run29
[2019-04-04 10:13:36,740] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:36,741] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:36,744] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run29
[2019-04-04 10:13:37,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:37,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:37,408] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run29
[2019-04-04 10:13:47,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:47,392] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:47,395] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run29
[2019-04-04 10:13:47,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:13:47,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:13:47,892] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run29
[2019-04-04 10:13:59,421] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4148461e-12 1.4037373e-11 1.3778265e-22 3.0170387e-13 5.1308149e-13
 3.8032554e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:13:59,422] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3650
[2019-04-04 10:13:59,452] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.95617247812843, 0.05982449196822043, 0.0, 1.0, 44755.5227471297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 171000.0000, 
sim time next is 171600.0000, 
raw observation next is [-8.4, 71.0, 0.0, 0.0, 26.0, 23.91270954973291, 0.05040381727173127, 0.0, 1.0, 44700.31617772171], 
processed observation next is [1.0, 1.0, 0.2299168975069252, 0.71, 0.0, 0.0, 0.6666666666666666, 0.4927257958110758, 0.5168012724239105, 0.0, 1.0, 0.21285864846534147], 
reward next is 0.7871, 
noisyNet noise sample is [array([0.19410643], dtype=float32), 0.4594813]. 
=============================================
[2019-04-04 10:14:29,942] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.2038706e-13 1.4034555e-12 3.4237060e-24 4.0030357e-14 4.6508800e-14
 6.8714770e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:14:29,943] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8907
[2019-04-04 10:14:30,021] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.86734112436041, 0.2034636400330516, 0.0, 1.0, 55076.37726380645], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 669600.0000, 
sim time next is 670200.0000, 
raw observation next is [-1.383333333333333, 57.83333333333333, 0.0, 0.0, 26.0, 24.86993841729884, 0.2055641925384671, 0.0, 1.0, 51417.03919983044], 
processed observation next is [0.0, 0.782608695652174, 0.4242843951985227, 0.5783333333333333, 0.0, 0.0, 0.6666666666666666, 0.5724948681082367, 0.5685213975128224, 0.0, 1.0, 0.24484304380871638], 
reward next is 0.7552, 
noisyNet noise sample is [array([-0.8315509], dtype=float32), -0.08367371]. 
=============================================
[2019-04-04 10:14:30,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1175646e-12 2.0951678e-11 5.9239693e-23 3.1465965e-13 4.6834276e-13
 8.7597047e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:14:30,996] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7391
[2019-04-04 10:14:31,018] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.6, 48.33333333333333, 0.0, 0.0, 26.0, 24.45264075790766, 0.1410772028426763, 0.0, 1.0, 45160.04931476731], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 423600.0000, 
sim time next is 424200.0000, 
raw observation next is [-10.6, 48.66666666666666, 0.0, 0.0, 26.0, 24.39861733054365, 0.1286303989315176, 0.0, 1.0, 44914.29303314945], 
processed observation next is [1.0, 0.9130434782608695, 0.1689750692520776, 0.4866666666666666, 0.0, 0.0, 0.6666666666666666, 0.5332181108786376, 0.5428767996438392, 0.0, 1.0, 0.21387758587214023], 
reward next is 0.7861, 
noisyNet noise sample is [array([-2.7340374], dtype=float32), 0.1403012]. 
=============================================
[2019-04-04 10:14:34,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3762981e-13 2.0857370e-12 4.0167594e-24 8.2713147e-14 7.3893135e-14
 1.3354452e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:14:34,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5011
[2019-04-04 10:14:34,550] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 71.0, 0.0, 0.0, 26.0, 24.49943217045107, 0.1139857434458505, 0.0, 1.0, 41302.88315288259], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 690000.0000, 
sim time next is 690600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.46910440084979, 0.1084123731199622, 0.0, 1.0, 41223.7609573981], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5390920334041492, 0.5361374577066541, 0.0, 1.0, 0.19630362360665762], 
reward next is 0.8037, 
noisyNet noise sample is [array([1.101539], dtype=float32), 1.4682205]. 
=============================================
[2019-04-04 10:14:34,740] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5215460e-12 1.1290641e-12 9.9656049e-25 4.9838639e-14 7.4320710e-14
 1.6979575e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:14:34,741] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7040
[2019-04-04 10:14:34,787] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 24.58862794337909, 0.1688898611986422, 0.0, 1.0, 40863.73275968203], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 543600.0000, 
sim time next is 544200.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 24.51839323486541, 0.1588533401385972, 0.0, 1.0, 40949.57928230616], 
processed observation next is [0.0, 0.30434782608695654, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.5431994362387842, 0.5529511133795324, 0.0, 1.0, 0.19499799658241027], 
reward next is 0.8050, 
noisyNet noise sample is [array([-0.28857535], dtype=float32), -2.062476]. 
=============================================
[2019-04-04 10:14:50,542] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1195849e-13 2.3020696e-12 7.6909585e-25 4.5973974e-14 7.8058613e-14
 3.8695618e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:14:50,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0913
[2019-04-04 10:14:50,566] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.77947227194988, 0.2107184263930509, 0.0, 1.0, 39351.17227100507], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 872400.0000, 
sim time next is 873000.0000, 
raw observation next is [-1.7, 79.0, 0.0, 0.0, 26.0, 24.73799185799896, 0.1989886465477234, 0.0, 1.0, 39356.49253875094], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5614993214999133, 0.5663295488492411, 0.0, 1.0, 0.18741186923214734], 
reward next is 0.8126, 
noisyNet noise sample is [array([1.697091], dtype=float32), 0.08216122]. 
=============================================
[2019-04-04 10:14:50,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[81.94227 ]
 [81.9091  ]
 [81.88003 ]
 [81.899925]
 [81.935326]], R is [[81.94444275]
 [81.93761444]
 [81.93067169]
 [81.92366791]
 [81.91683197]].
[2019-04-04 10:15:01,898] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.1924061e-16 4.4361720e-15 2.5512358e-28 1.8522429e-16 1.0268858e-16
 3.1706125e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:01,898] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0359
[2019-04-04 10:15:01,904] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.83333333333333, 49.33333333333334, 44.5, 0.0, 26.0, 27.80882912825913, 1.00730801300034, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1095600.0000, 
sim time next is 1096200.0000, 
raw observation next is [18.55, 49.5, 35.0, 0.0, 26.0, 27.85850073034734, 1.014135601800239, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.976454293628809, 0.495, 0.11666666666666667, 0.0, 0.6666666666666666, 0.821541727528945, 0.8380452006000797, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.75091004], dtype=float32), 1.5671303]. 
=============================================
[2019-04-04 10:15:04,136] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3240099e-15 5.3960386e-15 1.2942315e-27 4.9702649e-16 8.3933919e-16
 1.7513936e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:04,137] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8882
[2019-04-04 10:15:04,141] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.0, 52.33333333333334, 145.1666666666667, 0.0, 26.0, 27.40726109730817, 0.9538065445659057, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1088400.0000, 
sim time next is 1089000.0000, 
raw observation next is [19.1, 51.5, 140.0, 0.0, 26.0, 27.61107943383156, 0.9769939063410774, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.9916897506925209, 0.515, 0.4666666666666667, 0.0, 0.6666666666666666, 0.80092328615263, 0.8256646354470258, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.3275924], dtype=float32), -1.2431855]. 
=============================================
[2019-04-04 10:15:04,144] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[94.22822]
 [93.53997]
 [92.79743]
 [92.0407 ]
 [91.30981]], R is [[94.9908371 ]
 [95.0409317 ]
 [95.09052277]
 [95.13961792]
 [95.18822479]].
[2019-04-04 10:15:15,869] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.5713289e-15 1.6620589e-14 5.5700311e-28 2.9468859e-16 1.1117359e-15
 1.1528368e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:15,872] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7174
[2019-04-04 10:15:15,881] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.23333333333333, 88.33333333333333, 100.0, 0.0, 26.0, 26.62812961089266, 0.6615781517978238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 988800.0000, 
sim time next is 989400.0000, 
raw observation next is [11.41666666666667, 87.16666666666667, 104.0, 0.0, 26.0, 26.65191340074291, 0.6680436545040082, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.7788550323176363, 0.8716666666666667, 0.3466666666666667, 0.0, 0.6666666666666666, 0.7209927833952424, 0.7226812181680028, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5763359], dtype=float32), 0.23371035]. 
=============================================
[2019-04-04 10:15:21,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8300445e-15 4.0973477e-14 4.6189709e-27 3.2850317e-16 1.4965122e-15
 7.1447899e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:21,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9840
[2019-04-04 10:15:21,959] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 41.33333333333334, 0.0, 26.0, 25.91991583185257, 0.4057190285351571, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1438800.0000, 
sim time next is 1439400.0000, 
raw observation next is [1.1, 92.0, 36.66666666666667, 0.0, 26.0, 25.39024002057411, 0.4281415773321284, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.49307479224376743, 0.92, 0.12222222222222223, 0.0, 0.6666666666666666, 0.6158533350478427, 0.6427138591107094, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.327972], dtype=float32), -0.19272508]. 
=============================================
[2019-04-04 10:15:23,718] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.03633532e-14 1.18221905e-14 2.87566233e-28 4.66507260e-16
 3.35504086e-16 3.20725246e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 10:15:23,720] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8654
[2019-04-04 10:15:23,727] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.18333333333333, 77.5, 0.0, 0.0, 26.0, 24.21966720505802, 0.2954127333902203, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1209000.0000, 
sim time next is 1209600.0000, 
raw observation next is [16.1, 78.0, 0.0, 0.0, 26.0, 24.19162865986968, 0.2903190208762574, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.0, 0.9085872576177286, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5159690549891399, 0.5967730069587525, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8434296], dtype=float32), 0.89630514]. 
=============================================
[2019-04-04 10:15:24,549] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.3708759e-14 6.7737055e-14 2.2558175e-27 1.8885241e-15 7.6205276e-15
 5.1671682e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:24,550] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1614
[2019-04-04 10:15:24,562] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.96666666666667, 82.0, 0.0, 0.0, 26.0, 25.6230904345656, 0.6105089025052972, 0.0, 1.0, 32283.6665585505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1146000.0000, 
sim time next is 1146600.0000, 
raw observation next is [12.15, 81.5, 0.0, 0.0, 26.0, 25.62829114679203, 0.6127366466710383, 0.0, 1.0, 25008.24241707429], 
processed observation next is [0.0, 0.2608695652173913, 0.7991689750692522, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6356909288993359, 0.7042455488903462, 0.0, 1.0, 0.11908686865273471], 
reward next is 0.8809, 
noisyNet noise sample is [array([1.4895382], dtype=float32), -0.2941235]. 
=============================================
[2019-04-04 10:15:25,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.4670080e-14 2.9444170e-14 2.1473818e-27 1.2007899e-15 2.3674480e-15
 1.3245813e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:25,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2137
[2019-04-04 10:15:25,669] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 100.0, 96.0, 0.0, 26.0, 24.84909131709885, 0.4571268188895259, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1257600.0000, 
sim time next is 1258200.0000, 
raw observation next is [13.8, 100.0, 95.0, 0.0, 26.0, 24.817401372178, 0.4563202398397088, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.844875346260388, 1.0, 0.31666666666666665, 0.0, 0.6666666666666666, 0.5681167810148334, 0.6521067466132363, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.35878482], dtype=float32), 1.9409304]. 
=============================================
[2019-04-04 10:15:27,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3988766e-14 2.6693410e-13 4.6550913e-27 2.9077260e-15 2.4164359e-15
 3.0410667e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:27,932] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6480
[2019-04-04 10:15:27,941] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 67.66666666666667, 0.0, 0.0, 26.0, 25.71078473151037, 0.669300627801188, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1120800.0000, 
sim time next is 1121400.0000, 
raw observation next is [11.9, 68.5, 0.0, 0.0, 26.0, 25.73737980849132, 0.6660460143941404, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.7922437673130196, 0.685, 0.0, 0.0, 0.6666666666666666, 0.64478165070761, 0.7220153381313801, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.1563155], dtype=float32), 0.29661053]. 
=============================================
[2019-04-04 10:15:43,632] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5653474e-13 1.3214631e-12 2.6981947e-25 6.6665745e-15 3.1819131e-14
 2.7004003e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:43,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8917
[2019-04-04 10:15:43,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.20020881970086, 0.4388543285841586, 0.0, 1.0, 38601.127304995], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1401000.0000, 
sim time next is 1401600.0000, 
raw observation next is [-0.6, 100.0, 0.0, 0.0, 26.0, 25.19241018278611, 0.4465236264244321, 0.0, 1.0, 38548.75284554109], 
processed observation next is [1.0, 0.21739130434782608, 0.44598337950138506, 1.0, 0.0, 0.0, 0.6666666666666666, 0.5993675152321757, 0.648841208808144, 0.0, 1.0, 0.18356548974067183], 
reward next is 0.8164, 
noisyNet noise sample is [array([2.4982386], dtype=float32), 0.86194277]. 
=============================================
[2019-04-04 10:15:47,988] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8612925e-14 4.9245254e-13 6.7582299e-26 4.8218714e-15 5.4432821e-14
 8.2849126e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:47,988] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6178
[2019-04-04 10:15:48,010] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.9, 94.5, 0.0, 0.0, 26.0, 25.61634398898589, 0.5887075946301761, 0.0, 1.0, 33647.67002965443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1647000.0000, 
sim time next is 1647600.0000, 
raw observation next is [7.0, 95.0, 0.0, 0.0, 26.0, 25.69235463489474, 0.599020800087236, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6565096952908588, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6410295529078951, 0.6996736000290786, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.32919356], dtype=float32), -0.842522]. 
=============================================
[2019-04-04 10:15:58,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.92974932e-12 1.02216135e-11 4.10425901e-23 1.84486270e-13
 4.62356986e-13 1.73551439e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 10:15:58,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9754
[2019-04-04 10:15:58,615] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 78.0, 0.0, 0.0, 26.0, 24.36013967770334, 0.1528062961196614, 0.0, 1.0, 45927.68327435266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1818000.0000, 
sim time next is 1818600.0000, 
raw observation next is [-5.666666666666667, 78.83333333333333, 0.0, 0.0, 26.0, 24.32822604086009, 0.1448793961964281, 0.0, 1.0, 46007.64803701664], 
processed observation next is [0.0, 0.043478260869565216, 0.30563250230840255, 0.7883333333333333, 0.0, 0.0, 0.6666666666666666, 0.5273521700716742, 0.548293132065476, 0.0, 1.0, 0.2190840382715078], 
reward next is 0.7809, 
noisyNet noise sample is [array([-0.6830262], dtype=float32), 0.55460393]. 
=============================================
[2019-04-04 10:15:58,778] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3857650e-13 6.6789048e-12 3.5318893e-23 1.2769585e-13 3.0104166e-13
 1.4937802e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:15:58,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4393
[2019-04-04 10:15:58,799] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.55879681494132, 0.2055965407761513, 0.0, 1.0, 42907.51386197519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1985400.0000, 
sim time next is 1986000.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.52134156251967, 0.1978180376921181, 0.0, 1.0, 42839.33155015157], 
processed observation next is [1.0, 1.0, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5434451302099724, 0.5659393458973727, 0.0, 1.0, 0.20399681690548366], 
reward next is 0.7960, 
noisyNet noise sample is [array([-0.10770561], dtype=float32), 0.5258676]. 
=============================================
[2019-04-04 10:15:58,825] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[76.01733]
 [76.06717]
 [76.04853]
 [75.91648]
 [75.78733]], R is [[76.10320282]
 [76.1378479 ]
 [76.17191315]
 [76.20549011]
 [76.23867035]].
[2019-04-04 10:16:01,366] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8180158e-12 1.5263516e-11 2.9735137e-23 2.9716464e-13 7.9869385e-13
 5.3804583e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:16:01,367] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5165
[2019-04-04 10:16:01,401] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.033333333333333, 83.66666666666667, 0.0, 0.0, 26.0, 24.14645339153761, 0.1072433408938425, 0.0, 1.0, 46442.90404994266], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1822200.0000, 
sim time next is 1822800.0000, 
raw observation next is [-6.066666666666666, 84.33333333333334, 0.0, 0.0, 26.0, 24.13478545455906, 0.1005023963919319, 0.0, 1.0, 46498.7296253738], 
processed observation next is [0.0, 0.08695652173913043, 0.2945521698984303, 0.8433333333333334, 0.0, 0.0, 0.6666666666666666, 0.5112321212132551, 0.5335007987973106, 0.0, 1.0, 0.22142252202558954], 
reward next is 0.7786, 
noisyNet noise sample is [array([-0.2233457], dtype=float32), -0.55724096]. 
=============================================
[2019-04-04 10:16:12,519] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1178742e-12 5.1779379e-12 9.6607322e-24 1.3470351e-13 3.1641459e-13
 2.6275731e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:16:12,522] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2663
[2019-04-04 10:16:12,548] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.38120711482748, 0.1178963331155424, 0.0, 1.0, 41283.30960516907], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1999800.0000, 
sim time next is 2000400.0000, 
raw observation next is [-5.6, 83.0, 0.0, 0.0, 26.0, 24.33841238473986, 0.109205251045296, 0.0, 1.0, 41260.30711373613], 
processed observation next is [1.0, 0.13043478260869565, 0.30747922437673136, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5282010320616551, 0.5364017503484321, 0.0, 1.0, 0.196477652922553], 
reward next is 0.8035, 
noisyNet noise sample is [array([0.30254045], dtype=float32), -0.75156546]. 
=============================================
[2019-04-04 10:16:14,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.6510069e-13 8.2681370e-12 1.7730346e-23 3.5602719e-14 2.1085718e-13
 1.3457241e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:16:14,627] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4189
[2019-04-04 10:16:14,669] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333334, 83.0, 0.0, 0.0, 26.0, 25.2418347339725, 0.4065072865468594, 0.0, 1.0, 42514.28222096964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2151600.0000, 
sim time next is 2152200.0000, 
raw observation next is [-6.516666666666667, 83.0, 0.0, 0.0, 26.0, 25.22880137705562, 0.402811064269501, 0.0, 1.0, 42264.40818521218], 
processed observation next is [1.0, 0.9130434782608695, 0.2820867959372115, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6024001147546351, 0.6342703547565004, 0.0, 1.0, 0.20125908659624847], 
reward next is 0.7987, 
noisyNet noise sample is [array([-0.6228223], dtype=float32), -0.6533027]. 
=============================================
[2019-04-04 10:16:19,848] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9118369e-13 1.4578243e-12 1.2799898e-24 3.9959721e-14 6.1697412e-14
 2.3076788e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:16:19,848] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0589
[2019-04-04 10:16:19,871] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 82.66666666666667, 0.0, 0.0, 26.0, 25.3359497100631, 0.3735434289213937, 0.0, 1.0, 42711.16487068625], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2067000.0000, 
sim time next is 2067600.0000, 
raw observation next is [-4.1, 83.33333333333334, 0.0, 0.0, 26.0, 25.31404358159216, 0.3578993590106412, 0.0, 1.0, 42625.69274403653], 
processed observation next is [1.0, 0.9565217391304348, 0.3490304709141275, 0.8333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6095036317993466, 0.6192997863368804, 0.0, 1.0, 0.20297948925731682], 
reward next is 0.7970, 
noisyNet noise sample is [array([-0.17281562], dtype=float32), -0.25172636]. 
=============================================
[2019-04-04 10:16:30,122] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.0146040e-13 3.5149670e-12 1.3762657e-23 9.8541794e-14 4.3060258e-13
 5.0809381e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:16:30,126] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4460
[2019-04-04 10:16:30,137] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 78.5, 0.0, 0.0, 26.0, 24.31499365597586, 0.1562797242551757, 0.0, 1.0, 42585.33867906682], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2165400.0000, 
sim time next is 2166000.0000, 
raw observation next is [-6.9, 78.33333333333334, 0.0, 0.0, 26.0, 24.30471993289186, 0.1492211344562627, 0.0, 1.0, 42594.85071370676], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.7833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5253933277409883, 0.5497403781520875, 0.0, 1.0, 0.20283262244622266], 
reward next is 0.7972, 
noisyNet noise sample is [array([2.4141986], dtype=float32), 0.29753032]. 
=============================================
[2019-04-04 10:16:30,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[79.07056 ]
 [78.99764 ]
 [78.950615]
 [78.8742  ]
 [78.81396 ]], R is [[79.15174103]
 [79.15743256]
 [79.16316223]
 [79.16902924]
 [79.17503357]].
[2019-04-04 10:16:51,352] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.6038676e-13 5.5878041e-12 6.6282791e-24 1.3042223e-13 4.1642717e-13
 2.0468555e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:16:51,363] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1345
[2019-04-04 10:16:51,426] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.25, 50.0, 0.0, 0.0, 26.0, 25.02255241911284, 0.3822758262256021, 0.0, 1.0, 98800.88632481228], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2579400.0000, 
sim time next is 2580000.0000, 
raw observation next is [-2.433333333333333, 52.0, 0.0, 0.0, 26.0, 25.11085527817831, 0.4046251918533503, 0.0, 1.0, 61779.19744075568], 
processed observation next is [1.0, 0.8695652173913043, 0.3951985226223454, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5925712731815258, 0.6348750639511168, 0.0, 1.0, 0.2941866544797889], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.1784213], dtype=float32), -0.5121604]. 
=============================================
[2019-04-04 10:16:51,429] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.2439  ]
 [78.75615 ]
 [78.19209 ]
 [77.992325]
 [77.720764]], R is [[79.38167572]
 [79.11737823]
 [78.57212067]
 [78.52807617]
 [78.37721252]].
[2019-04-04 10:16:57,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9663758e-12 1.3266030e-11 1.7146993e-23 2.3892883e-13 4.6451997e-13
 3.0928506e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:16:57,527] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1280
[2019-04-04 10:16:57,542] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.65, 60.5, 0.0, 0.0, 26.0, 23.29353983916405, -0.1357208393790042, 0.0, 1.0, 44255.1847029104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [-8.733333333333334, 60.33333333333334, 0.0, 0.0, 26.0, 23.25901658017526, -0.1443374975862114, 0.0, 1.0, 44214.77096530844], 
processed observation next is [0.0, 0.21739130434782608, 0.22068328716528163, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.43825138168127165, 0.45188750080459616, 0.0, 1.0, 0.21054652840623067], 
reward next is 0.7895, 
noisyNet noise sample is [array([-0.28976116], dtype=float32), 1.0698075]. 
=============================================
[2019-04-04 10:17:03,392] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6411703e-13 1.8099090e-12 2.7193479e-24 1.8493653e-13 3.9328871e-14
 6.0998559e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:03,397] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1083
[2019-04-04 10:17:03,465] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 28.0, 0.0, 0.0, 26.0, 24.90493339989402, 0.2225962914263923, 0.0, 1.0, 25416.2293456449], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2484000.0000, 
sim time next is 2484600.0000, 
raw observation next is [0.9166666666666667, 28.33333333333334, 0.0, 0.0, 26.0, 24.92738762874274, 0.2196868400637076, 0.0, 1.0, 18719.20271423861], 
processed observation next is [0.0, 0.782608695652174, 0.48799630655586346, 0.2833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5772823023952283, 0.5732289466879025, 0.0, 1.0, 0.08913906054399338], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.61637175], dtype=float32), 1.3537372]. 
=============================================
[2019-04-04 10:17:10,135] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.8148506e-13 2.7362232e-11 2.6800612e-23 3.8412126e-13 4.1990863e-13
 4.6353798e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:10,135] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0019
[2019-04-04 10:17:10,198] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 44.0, 0.0, 0.0, 26.0, 24.95026502417611, 0.3355338535142305, 0.0, 1.0, 76772.56585334317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2577600.0000, 
sim time next is 2578200.0000, 
raw observation next is [-1.883333333333333, 46.0, 0.0, 0.0, 26.0, 24.95971637046695, 0.3438015891527702, 0.0, 1.0, 54248.03499981008], 
processed observation next is [1.0, 0.8695652173913043, 0.4104339796860573, 0.46, 0.0, 0.0, 0.6666666666666666, 0.5799763642055792, 0.61460052971759, 0.0, 1.0, 0.2583239761895718], 
reward next is 0.7417, 
noisyNet noise sample is [array([1.3226753], dtype=float32), -1.2042469]. 
=============================================
[2019-04-04 10:17:18,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2997531e-12 3.7832203e-12 7.2007500e-24 1.2977731e-13 1.9082269e-13
 1.1274139e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:18,906] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1738
[2019-04-04 10:17:18,927] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.81172341021518, -0.01905509387178941, 0.0, 1.0, 62316.76203858463], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2789400.0000, 
sim time next is 2790000.0000, 
raw observation next is [-7.0, 64.0, 0.0, 0.0, 26.0, 23.68075511768425, -0.04155376038367106, 0.0, 1.0, 63098.70465357052], 
processed observation next is [1.0, 0.30434782608695654, 0.2686980609418283, 0.64, 0.0, 0.0, 0.6666666666666666, 0.47339625980702077, 0.4861487465387763, 0.0, 1.0, 0.30047002215985963], 
reward next is 0.6995, 
noisyNet noise sample is [array([-0.77154297], dtype=float32), 0.070875086]. 
=============================================
[2019-04-04 10:17:18,958] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.75768 ]
 [77.75436 ]
 [77.747246]
 [77.7404  ]
 [77.75304 ]], R is [[77.65007782]
 [77.57683563]
 [77.51258087]
 [77.46976471]
 [77.46711731]].
[2019-04-04 10:17:30,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0656901e-13 8.4173564e-13 2.2911613e-24 3.3289096e-14 9.8927099e-14
 1.2430350e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:30,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9937
[2019-04-04 10:17:30,252] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 100.0, 0.0, 0.0, 26.0, 25.30526344330958, 0.3231263991870789, 0.0, 1.0, 40246.05916105882], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3109800.0000, 
sim time next is 3110400.0000, 
raw observation next is [0.0, 100.0, 0.0, 0.0, 26.0, 25.29962031211526, 0.3234255689822552, 0.0, 1.0, 39888.30023972627], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6083016926762715, 0.607808522994085, 0.0, 1.0, 0.18994428685583936], 
reward next is 0.8101, 
noisyNet noise sample is [array([0.25761318], dtype=float32), 1.3595043]. 
=============================================
[2019-04-04 10:17:35,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.8092118e-14 6.3798142e-13 5.0070682e-24 1.7224430e-14 6.4050009e-14
 1.1833143e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:35,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1678
[2019-04-04 10:17:35,643] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 101.0, 751.0, 26.0, 25.18733908315681, 0.4166717158481388, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2989800.0000, 
sim time next is 2990400.0000, 
raw observation next is [-2.0, 60.0, 98.0, 737.0, 26.0, 25.17617565376543, 0.4081533761388808, 0.0, 1.0, 18706.76182187096], 
processed observation next is [0.0, 0.6086956521739131, 0.40720221606648205, 0.6, 0.32666666666666666, 0.8143646408839779, 0.6666666666666666, 0.5980146378137858, 0.6360511253796269, 0.0, 1.0, 0.08907981819938553], 
reward next is 0.9109, 
noisyNet noise sample is [array([1.4872054], dtype=float32), -0.78686464]. 
=============================================
[2019-04-04 10:17:38,244] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0631517e-14 6.7050125e-14 2.2010099e-25 5.5204816e-16 5.0110846e-15
 1.7018149e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:38,244] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1837
[2019-04-04 10:17:38,264] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 75.83333333333334, 95.66666666666667, 741.3333333333334, 26.0, 26.93563551531232, 0.8402794966661351, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3250200.0000, 
sim time next is 3250800.0000, 
raw observation next is [-2.0, 71.0, 93.0, 727.5, 26.0, 26.9768530107386, 0.8500744717974863, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.71, 0.31, 0.8038674033149171, 0.6666666666666666, 0.7480710842282168, 0.7833581572658287, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5475451], dtype=float32), -0.3634599]. 
=============================================
[2019-04-04 10:17:40,649] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0122546e-13 4.2345012e-12 3.8759872e-24 1.0286830e-13 2.2305774e-13
 1.2258702e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:40,652] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6134
[2019-04-04 10:17:40,669] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 93.0, 0.0, 0.0, 26.0, 24.85382388648476, 0.239914428866904, 0.0, 1.0, 55773.86891216321], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2868600.0000, 
sim time next is 2869200.0000, 
raw observation next is [1.0, 93.0, 0.0, 0.0, 26.0, 24.8096240416827, 0.2404126181495265, 0.0, 1.0, 55777.89868559438], 
processed observation next is [1.0, 0.21739130434782608, 0.4903047091412743, 0.93, 0.0, 0.0, 0.6666666666666666, 0.567468670140225, 0.5801375393831755, 0.0, 1.0, 0.26560904135997326], 
reward next is 0.7344, 
noisyNet noise sample is [array([0.19354807], dtype=float32), 0.046871617]. 
=============================================
[2019-04-04 10:17:49,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1519583e-13 3.7422675e-13 1.0295213e-24 7.8425696e-15 1.9594601e-14
 9.2698174e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:49,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6837
[2019-04-04 10:17:49,116] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.58491608623974, 0.6035299880582771, 0.0, 1.0, 18738.23346040317], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3196200.0000, 
sim time next is 3196800.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.54785934847195, 0.5944918580184151, 0.0, 1.0, 38908.19079582892], 
processed observation next is [1.0, 0.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6289882790393291, 0.6981639526728051, 0.0, 1.0, 0.18527709902775677], 
reward next is 0.8147, 
noisyNet noise sample is [array([-0.6959872], dtype=float32), -0.71437603]. 
=============================================
[2019-04-04 10:17:51,817] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2973833e-14 7.3884296e-13 2.7890227e-24 2.3043296e-14 1.0995271e-13
 1.1825071e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:51,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0626
[2019-04-04 10:17:51,829] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.05818267898679, 0.7036096481318066, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.93355090874136, 0.6830744519518244, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5503231763619576, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6611292423951133, 0.7276914839839415, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2752011], dtype=float32), 2.2170603]. 
=============================================
[2019-04-04 10:17:57,453] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2125189e-15 2.0710170e-13 1.9213521e-25 1.1108965e-15 1.9451525e-15
 7.6186560e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:17:57,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3172
[2019-04-04 10:17:57,523] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 50.0, 108.6666666666667, 768.0, 26.0, 25.09000921893377, 0.5248307315196606, 1.0, 1.0, 196283.6229633117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3334200.0000, 
sim time next is 3334800.0000, 
raw observation next is [-3.666666666666667, 50.0, 107.3333333333333, 760.0, 26.0, 25.63472722284086, 0.6048344637402776, 1.0, 1.0, 70333.60784504382], 
processed observation next is [1.0, 0.6086956521739131, 0.3610341643582641, 0.5, 0.3577777777777777, 0.8397790055248618, 0.6666666666666666, 0.6362272685700715, 0.7016114879134259, 1.0, 1.0, 0.3349219421192563], 
reward next is 0.6651, 
noisyNet noise sample is [array([-2.494401], dtype=float32), 0.100123346]. 
=============================================
[2019-04-04 10:18:13,733] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.0496530e-12 2.4300744e-12 1.4484280e-24 1.8591194e-13 6.3279778e-14
 7.6793844e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:18:13,733] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5248
[2019-04-04 10:18:13,774] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.11186532655789, 0.3391489445647, 0.0, 1.0, 18708.18457163546], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3611400.0000, 
sim time next is 3612000.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.09086308240111, 0.3453747824370487, 0.0, 1.0, 198880.9153780533], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5909052568667592, 0.6151249274790163, 0.0, 1.0, 0.9470519779907299], 
reward next is 0.0529, 
noisyNet noise sample is [array([-0.5790085], dtype=float32), 0.34327206]. 
=============================================
[2019-04-04 10:18:13,835] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.70866 ]
 [81.68151 ]
 [81.723595]
 [81.67238 ]
 [81.590324]], R is [[81.72220612]
 [81.81589508]
 [81.90864563]
 [81.88965607]
 [81.89244843]].
[2019-04-04 10:18:16,240] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5143258e-15 1.6432979e-13 1.6579778e-25 5.8753117e-16 9.3402450e-16
 1.6977429e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:18:16,241] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5287
[2019-04-04 10:18:16,304] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.5, 74.0, 59.0, 511.0, 26.0, 26.92284775677743, 0.7845859481276225, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3256200.0000, 
sim time next is 3256800.0000, 
raw observation next is [-3.666666666666667, 75.0, 50.66666666666667, 443.1666666666667, 26.0, 26.27142790388259, 0.7275690891285969, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3610341643582641, 0.75, 0.1688888888888889, 0.48968692449355433, 0.6666666666666666, 0.6892856586568824, 0.7425230297095323, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9269968], dtype=float32), -0.6974018]. 
=============================================
[2019-04-04 10:18:24,989] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-04 10:18:24,990] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:18:24,994] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:18:24,997] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run40
[2019-04-04 10:18:24,991] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:18:25,032] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:18:25,036] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:18:25,036] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:18:25,066] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run40
[2019-04-04 10:18:25,113] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run40
[2019-04-04 10:20:09,381] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14625613], dtype=float32), -0.25560948]
[2019-04-04 10:20:09,382] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-3.9, 54.0, 208.0, 217.5, 26.0, 25.4288807218085, 0.3492797603981625, 1.0, 1.0, 31849.31828248085]
[2019-04-04 10:20:09,382] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:20:09,382] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.9334076e-14 1.8239399e-13 4.9833798e-25 6.6885321e-15 4.4798942e-15
 6.5273577e-17 1.0000000e+00], sampled 0.543969939281809
[2019-04-04 10:20:38,662] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.14625613], dtype=float32), -0.25560948]
[2019-04-04 10:20:38,662] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.333333333333334, 64.0, 18.0, 34.49999999999999, 26.0, 24.35962596452802, 0.2150456069623052, 1.0, 1.0, 192370.6314754373]
[2019-04-04 10:20:38,662] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:20:38,663] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.2342148e-13 4.8052214e-13 2.1413053e-26 1.3932230e-14 9.2190456e-15
 3.0204871e-17 1.0000000e+00], sampled 0.10556558253482917
[2019-04-04 10:21:10,698] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.14625613], dtype=float32), -0.25560948]
[2019-04-04 10:21:10,698] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [12.51666666666667, 45.66666666666667, 0.0, 0.0, 26.0, 25.78672729452121, 0.5638981725905038, 0.0, 1.0, 0.0]
[2019-04-04 10:21:10,698] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:21:10,700] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.2686656e-13 4.0217496e-13 2.9736565e-26 1.2548942e-14 1.3903083e-14
 5.4829743e-17 1.0000000e+00], sampled 0.9578747003802661
[2019-04-04 10:21:29,206] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:22:03,061] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:22:08,995] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:22:10,040] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 3900000, evaluation results [3900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:22:15,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.6247402e-16 2.5052173e-14 1.8203690e-26 5.7759325e-16 3.3435164e-16
 3.7059892e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:15,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7465
[2019-04-04 10:22:15,103] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 90.33333333333334, 702.6666666666666, 26.0, 26.2472917296294, 0.6691519747389106, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3511200.0000, 
sim time next is 3511800.0000, 
raw observation next is [3.0, 49.0, 88.0, 687.0, 26.0, 25.6495859911362, 0.631724327522059, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.6521739130434783, 0.5457063711911359, 0.49, 0.29333333333333333, 0.7591160220994475, 0.6666666666666666, 0.6374654992613499, 0.7105747758406863, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.6208084], dtype=float32), -1.4366608]. 
=============================================
[2019-04-04 10:22:20,813] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5064240e-15 2.6045270e-14 3.0390697e-26 1.1556609e-15 6.4721936e-16
 9.4148572e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:20,814] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1483
[2019-04-04 10:22:20,830] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 99.66666666666666, 754.3333333333333, 26.0, 26.67866088023206, 0.6741252954594135, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3768600.0000, 
sim time next is 3769200.0000, 
raw observation next is [0.0, 60.0, 96.5, 743.5, 26.0, 26.72314536089698, 0.6799097403920027, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46260387811634357, 0.6, 0.32166666666666666, 0.8215469613259668, 0.6666666666666666, 0.7269287800747483, 0.7266365801306676, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68424773], dtype=float32), 1.3445406]. 
=============================================
[2019-04-04 10:22:22,494] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5690271e-15 2.9486094e-14 1.0883612e-26 9.2941114e-16 4.2768959e-16
 1.7906643e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:22,498] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5850
[2019-04-04 10:22:22,522] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 26.0, 26.2374795602468, 0.6118321854371301, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3496800.0000, 
sim time next is 3497400.0000, 
raw observation next is [1.5, 59.0, 115.0, 810.0, 26.0, 26.31439222117447, 0.6162053536616964, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5041551246537397, 0.59, 0.38333333333333336, 0.8950276243093923, 0.6666666666666666, 0.6928660184312058, 0.7054017845538988, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.21959053], dtype=float32), 0.22350554]. 
=============================================
[2019-04-04 10:22:30,330] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.9084265e-14 1.7743868e-13 2.9770276e-26 4.2133223e-15 1.5090034e-15
 3.8076354e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:30,331] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5074
[2019-04-04 10:22:30,366] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 75.0, 90.83333333333334, 470.0, 26.0, 25.50322822388705, 0.3723232335622808, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3746400.0000, 
sim time next is 3747000.0000, 
raw observation next is [-4.0, 76.0, 92.66666666666667, 511.0, 26.0, 25.56690661733962, 0.3727352889262565, 1.0, 1.0, 18710.48846261287], 
processed observation next is [1.0, 0.34782608695652173, 0.3518005540166205, 0.76, 0.3088888888888889, 0.5646408839779006, 0.6666666666666666, 0.6305755514449682, 0.6242450963087521, 1.0, 1.0, 0.08909756410768033], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.5865524], dtype=float32), -0.5489206]. 
=============================================
[2019-04-04 10:22:30,369] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[89.180145]
 [89.00539 ]
 [88.35417 ]
 [87.334145]
 [84.952774]], R is [[89.22499084]
 [89.33274078]
 [89.43941498]
 [89.50045013]
 [89.60544586]].
[2019-04-04 10:22:30,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.1941930e-15 7.6882003e-14 4.9427294e-26 2.0244604e-15 6.4409381e-16
 2.0347090e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:30,941] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1110
[2019-04-04 10:22:30,958] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 21.0, 99.0, 766.6666666666666, 26.0, 26.59516251025335, 0.6680206690085023, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4027800.0000, 
sim time next is 4028400.0000, 
raw observation next is [-2.0, 20.0, 96.5, 753.0, 26.0, 26.64156291579861, 0.6758491225321196, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.40720221606648205, 0.2, 0.32166666666666666, 0.8320441988950277, 0.6666666666666666, 0.7201302429832174, 0.7252830408440398, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27481687], dtype=float32), -2.168855]. 
=============================================
[2019-04-04 10:22:31,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4430370e-15 4.2344888e-14 5.6149652e-26 9.2727926e-16 1.3040320e-15
 2.2032605e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:31,865] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3937
[2019-04-04 10:22:31,918] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 43.0, 64.0, 551.0, 26.0, 26.40236943220677, 0.678832453068794, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3861000.0000, 
sim time next is 3861600.0000, 
raw observation next is [3.0, 42.33333333333334, 56.33333333333334, 489.0, 26.0, 25.80386758894072, 0.627994469535578, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.42333333333333345, 0.18777777777777782, 0.5403314917127072, 0.6666666666666666, 0.6503222990783932, 0.7093314898451927, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.32303527], dtype=float32), 0.093348704]. 
=============================================
[2019-04-04 10:22:34,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6744638e-12 5.4158856e-12 7.6804373e-24 1.1177201e-13 6.8965661e-13
 2.3511132e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:34,665] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8024
[2019-04-04 10:22:34,702] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.46512751628713, 0.4225176580688575, 0.0, 1.0, 18758.00321998502], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3893400.0000, 
sim time next is 3894000.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.39282942398051, 0.4108803480924597, 0.0, 1.0, 60800.41158737372], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6160691186650423, 0.6369601160308199, 0.0, 1.0, 0.28952576946368436], 
reward next is 0.7105, 
noisyNet noise sample is [array([0.25456572], dtype=float32), -0.16749948]. 
=============================================
[2019-04-04 10:22:34,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[80.416016]
 [80.47044 ]
 [80.61821 ]
 [80.66431 ]
 [80.71947 ]], R is [[80.46624756]
 [80.572258  ]
 [80.7665329 ]
 [80.84090424]
 [80.84318542]].
[2019-04-04 10:22:41,114] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0286873e-13 5.4382237e-12 3.4525069e-24 6.6928742e-14 2.2838922e-13
 4.7481994e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:41,115] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7166
[2019-04-04 10:22:41,132] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.92304421986042, 0.2943063824581099, 0.0, 1.0, 41754.01769909492], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3909600.0000, 
sim time next is 3910200.0000, 
raw observation next is [-6.166666666666666, 59.83333333333334, 0.0, 0.0, 26.0, 24.90839451530307, 0.295918252242036, 0.0, 1.0, 41901.63964919668], 
processed observation next is [1.0, 0.2608695652173913, 0.29178208679593726, 0.5983333333333334, 0.0, 0.0, 0.6666666666666666, 0.5756995429419224, 0.598639417414012, 0.0, 1.0, 0.19953161737712705], 
reward next is 0.8005, 
noisyNet noise sample is [array([-0.00039666], dtype=float32), -0.07069646]. 
=============================================
[2019-04-04 10:22:50,008] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.2153263e-12 2.4899345e-11 8.6140937e-24 2.1271534e-13 9.1089693e-14
 1.2765969e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:50,009] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5099
[2019-04-04 10:22:50,025] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 36.0, 0.0, 0.0, 26.0, 25.25142782779456, 0.4008508247047926, 0.0, 1.0, 48135.01224689217], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4056600.0000, 
sim time next is 4057200.0000, 
raw observation next is [-6.0, 37.0, 0.0, 0.0, 26.0, 25.24435168521387, 0.3956008505116836, 0.0, 1.0, 43088.89372162735], 
processed observation next is [1.0, 1.0, 0.296398891966759, 0.37, 0.0, 0.0, 0.6666666666666666, 0.6036959737678226, 0.6318669501705613, 0.0, 1.0, 0.20518520819822547], 
reward next is 0.7948, 
noisyNet noise sample is [array([-0.3403429], dtype=float32), 0.19238397]. 
=============================================
[2019-04-04 10:22:50,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.7469477e-13 2.3417362e-12 1.0008350e-24 3.0339698e-13 1.7085076e-14
 1.2390624e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:50,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9330
[2019-04-04 10:22:50,150] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 43.0, 0.0, 0.0, 26.0, 25.57976119363457, 0.4045296673174064, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4221000.0000, 
sim time next is 4221600.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.57213524669004, 0.3944156496743926, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6310112705575032, 0.6314718832247975, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8697063], dtype=float32), 0.13824825]. 
=============================================
[2019-04-04 10:22:50,276] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8892599e-14 2.9654634e-13 4.8376545e-25 1.1371588e-14 1.6982369e-15
 5.0475312e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:50,300] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3982
[2019-04-04 10:22:50,311] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 24.33333333333333, 35.66666666666666, 311.3333333333333, 26.0, 26.97992270867027, 0.7215309166776723, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4036200.0000, 
sim time next is 4036800.0000, 
raw observation next is [-2.333333333333333, 24.66666666666666, 27.83333333333333, 252.1666666666667, 26.0, 27.05480839998741, 0.6880881729440512, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3979686057248385, 0.24666666666666662, 0.09277777777777776, 0.2786372007366483, 0.6666666666666666, 0.7545673666656176, 0.7293627243146837, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0004317], dtype=float32), 2.2424648]. 
=============================================
[2019-04-04 10:22:58,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6999513e-13 1.4339067e-12 2.9870867e-24 1.1660681e-14 1.4656601e-14
 3.9477490e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:22:58,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5024
[2019-04-04 10:22:58,247] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8166666666666667, 71.33333333333334, 0.0, 0.0, 26.0, 25.3308655795141, 0.4147176994850297, 0.0, 1.0, 41622.95837867328], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4510200.0000, 
sim time next is 4510800.0000, 
raw observation next is [-0.8, 71.0, 0.0, 0.0, 26.0, 25.31326864750758, 0.4176834596537792, 0.0, 1.0, 41506.89354888102], 
processed observation next is [1.0, 0.21739130434782608, 0.4404432132963989, 0.71, 0.0, 0.0, 0.6666666666666666, 0.609439053958965, 0.6392278198845931, 0.0, 1.0, 0.19765187404229057], 
reward next is 0.8023, 
noisyNet noise sample is [array([0.5758121], dtype=float32), 1.260175]. 
=============================================
[2019-04-04 10:23:00,123] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.05058231e-14 5.54914079e-13 1.06309765e-24 5.17375686e-14
 8.66392817e-15 1.90393763e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:23:00,127] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9884
[2019-04-04 10:23:00,146] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 42.66666666666667, 182.5, 163.8333333333333, 26.0, 25.03453346227603, 0.3564041972271348, 0.0, 1.0, 53891.783088126], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4200000.0000, 
sim time next is 4200600.0000, 
raw observation next is [2.0, 43.33333333333334, 178.0, 238.6666666666666, 26.0, 24.9923256409709, 0.3662001056674151, 0.0, 1.0, 56458.97562234469], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.4333333333333334, 0.5933333333333334, 0.26372007366482497, 0.6666666666666666, 0.5826938034142417, 0.6220667018891384, 0.0, 1.0, 0.26885226486830804], 
reward next is 0.7311, 
noisyNet noise sample is [array([0.35033688], dtype=float32), 1.0565321]. 
=============================================
[2019-04-04 10:23:10,520] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.9762206e-15 5.2845115e-14 3.3985625e-26 3.2748987e-15 1.0023794e-15
 1.3433634e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:23:10,525] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3832
[2019-04-04 10:23:10,549] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.4, 44.0, 0.0, 0.0, 26.0, 27.72252575428364, 1.01161323777679, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [12.33333333333333, 45.0, 0.0, 0.0, 26.0, 27.78138014171072, 1.036306388478157, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.8042474607571561, 0.45, 0.0, 0.0, 0.6666666666666666, 0.8151150118092266, 0.8454354628260523, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35392612], dtype=float32), -0.39829212]. 
=============================================
[2019-04-04 10:23:13,922] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2455136e-13 1.7966533e-12 3.1485600e-24 6.8270151e-14 1.2870991e-13
 4.5035376e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:23:13,926] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9836
[2019-04-04 10:23:13,952] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.25893128980739, 0.4393315596976273, 0.0, 1.0, 86720.69880549329], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4498800.0000, 
sim time next is 4499400.0000, 
raw observation next is [-0.6, 73.0, 0.0, 0.0, 26.0, 25.29177047844758, 0.4406595535139126, 0.0, 1.0, 59451.28494261286], 
processed observation next is [1.0, 0.043478260869565216, 0.44598337950138506, 0.73, 0.0, 0.0, 0.6666666666666666, 0.6076475398706318, 0.6468865178379709, 0.0, 1.0, 0.28310135686958504], 
reward next is 0.7169, 
noisyNet noise sample is [array([0.63948166], dtype=float32), 0.47948948]. 
=============================================
[2019-04-04 10:23:18,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2386957e-14 2.6058702e-13 4.1948103e-25 7.2683694e-15 6.6143668e-15
 5.4057166e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:23:18,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0843
[2019-04-04 10:23:18,670] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 52.33333333333333, 0.0, 0.0, 26.0, 25.84969026607586, 0.5881862731587294, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4650000.0000, 
sim time next is 4650600.0000, 
raw observation next is [2.166666666666667, 52.16666666666667, 0.0, 0.0, 26.0, 25.76085066529712, 0.5702382797891798, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5226223453370269, 0.5216666666666667, 0.0, 0.0, 0.6666666666666666, 0.6467375554414266, 0.6900794265963933, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.33140785], dtype=float32), 0.42502153]. 
=============================================
[2019-04-04 10:23:22,294] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5913024e-15 1.9855093e-14 3.5046391e-26 4.3695597e-16 5.6311026e-16
 5.5906656e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:23:22,295] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3292
[2019-04-04 10:23:22,327] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 72.5, 196.0, 6.0, 26.0, 26.46285092168569, 0.6171217899096698, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4717800.0000, 
sim time next is 4718400.0000, 
raw observation next is [1.333333333333333, 72.33333333333334, 187.8333333333333, 5.0, 26.0, 26.48480094428829, 0.5088509406060135, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.4995383194829178, 0.7233333333333334, 0.626111111111111, 0.0055248618784530384, 0.6666666666666666, 0.7070667453573574, 0.6696169802020044, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.8717731], dtype=float32), -0.232655]. 
=============================================
[2019-04-04 10:23:32,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:32,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:32,242] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run30
[2019-04-04 10:23:32,274] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:32,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:32,297] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run30
[2019-04-04 10:23:33,014] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:33,015] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:33,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run30
[2019-04-04 10:23:37,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.6417054e-13 1.8690258e-12 2.8557709e-25 4.1305629e-14 6.2836096e-14
 3.0422284e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:23:37,016] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8132
[2019-04-04 10:23:37,116] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 69.0, 117.5, 260.8333333333334, 26.0, 24.40235324840997, 0.2654942244284128, 0.0, 1.0, 202487.9849510309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4868400.0000, 
sim time next is 4869000.0000, 
raw observation next is [-3.5, 68.0, 141.0, 313.0, 26.0, 24.80509418951284, 0.3384438834282639, 0.0, 1.0, 8346.298220997533], 
processed observation next is [0.0, 0.34782608695652173, 0.36565096952908593, 0.68, 0.47, 0.34585635359116024, 0.6666666666666666, 0.5670911824594033, 0.6128146278094213, 0.0, 1.0, 0.03974427724284539], 
reward next is 0.9603, 
noisyNet noise sample is [array([-0.03294116], dtype=float32), -0.26257515]. 
=============================================
[2019-04-04 10:23:37,119] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[85.39983 ]
 [84.192535]
 [83.3094  ]
 [82.18685 ]
 [80.921165]], R is [[86.16404724]
 [85.33818054]
 [85.29871368]
 [85.25858307]
 [85.21807098]].
[2019-04-04 10:23:40,809] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:40,810] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:40,814] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run30
[2019-04-04 10:23:44,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:44,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:44,987] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run30
[2019-04-04 10:23:45,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:45,326] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:45,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run30
[2019-04-04 10:23:45,902] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:45,903] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:45,908] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run30
[2019-04-04 10:23:46,275] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:46,276] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:46,280] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run30
[2019-04-04 10:23:49,324] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.6582094e-16 2.1781030e-14 4.5983315e-27 1.7808295e-16 2.9485839e-16
 7.6904868e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:23:49,333] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9615
[2019-04-04 10:23:49,360] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.0, 18.66666666666667, 82.66666666666666, 638.3333333333334, 26.0, 28.78448590871885, 1.165199511427877, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5069400.0000, 
sim time next is 5070000.0000, 
raw observation next is [12.0, 18.33333333333334, 79.33333333333333, 611.6666666666666, 26.0, 28.90820763771088, 1.186252323885489, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.7950138504155125, 0.1833333333333334, 0.2644444444444444, 0.6758747697974217, 0.6666666666666666, 0.9090173031425733, 0.8954174412951629, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.33780125], dtype=float32), -1.644239]. 
=============================================
[2019-04-04 10:23:49,424] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[92.96835 ]
 [93.206024]
 [93.420395]
 [93.829155]
 [94.2921  ]], R is [[92.84666443]
 [92.91819763]
 [92.98901367]
 [93.05912781]
 [93.12854004]].
[2019-04-04 10:23:49,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:49,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:49,953] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run30
[2019-04-04 10:23:50,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:50,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:50,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run30
[2019-04-04 10:23:50,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.4221776e-13 1.9486537e-12 1.0949544e-24 3.2505306e-14 5.3454271e-14
 3.0005637e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:23:50,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8018
[2019-04-04 10:23:50,973] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.15, 32.5, 0.0, 0.0, 26.0, 25.82291266057922, 0.5294697539613323, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5097000.0000, 
sim time next is 5097600.0000, 
raw observation next is [8.1, 35.0, 0.0, 0.0, 26.0, 25.75518798652967, 0.5119665944646544, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6869806094182825, 0.35, 0.0, 0.0, 0.6666666666666666, 0.6462656655441391, 0.6706555314882182, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48801827], dtype=float32), -1.202418]. 
=============================================
[2019-04-04 10:23:51,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:51,153] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:51,156] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run30
[2019-04-04 10:23:51,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:51,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:51,931] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run30
[2019-04-04 10:23:52,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:52,037] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:52,042] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run30
[2019-04-04 10:23:53,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:53,834] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:53,838] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run30
[2019-04-04 10:23:57,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:23:57,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:23:57,801] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run30
[2019-04-04 10:23:59,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.60852]], R is [[1.]].
[2019-04-04 10:24:05,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:24:05,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:24:05,796] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run30
[2019-04-04 10:24:41,516] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6791260e-13 3.0358932e-12 1.3231423e-24 1.9321672e-14 1.0966242e-13
 1.5386770e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:24:41,517] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9914
[2019-04-04 10:24:41,542] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.8, 92.66666666666667, 0.0, 0.0, 26.0, 24.85396003806601, 0.2335230979849157, 0.0, 1.0, 41010.82785850688], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 511800.0000, 
sim time next is 512400.0000, 
raw observation next is [2.9, 93.33333333333334, 0.0, 0.0, 26.0, 24.84885717990676, 0.2331518028813951, 0.0, 1.0, 40907.63460564463], 
processed observation next is [1.0, 0.9565217391304348, 0.5429362880886427, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.5707380983255632, 0.5777172676271317, 0.0, 1.0, 0.19479826002687917], 
reward next is 0.8052, 
noisyNet noise sample is [array([-1.5350215], dtype=float32), 0.35932803]. 
=============================================
[2019-04-04 10:24:48,056] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7161992e-14 6.7403933e-13 2.1715864e-25 2.5244135e-14 4.1934712e-15
 8.4187842e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:24:48,066] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5466
[2019-04-04 10:24:48,146] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.9, 26.0, 123.1666666666667, 0.0, 26.0, 25.11230423522714, 0.1482084469275736, 1.0, 1.0, 54202.99367186529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 474000.0000, 
sim time next is 474600.0000, 
raw observation next is [-1.8, 25.5, 124.3333333333333, 0.0, 26.0, 25.11429146993532, 0.1581820857759641, 1.0, 1.0, 35884.200516308], 
processed observation next is [1.0, 0.4782608695652174, 0.41274238227146814, 0.255, 0.41444444444444434, 0.0, 0.6666666666666666, 0.59285762249461, 0.5527273619253213, 1.0, 1.0, 0.1708771453157524], 
reward next is 0.8291, 
noisyNet noise sample is [array([-1.6709716], dtype=float32), 0.056027826]. 
=============================================
[2019-04-04 10:24:54,892] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7266353e-12 2.0905961e-12 2.0986256e-24 5.9660190e-14 8.6516135e-14
 2.6713029e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:24:54,892] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6925
[2019-04-04 10:24:54,907] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 75.0, 0.0, 0.0, 26.0, 24.03307346456412, 0.04851413633639745, 0.0, 1.0, 43963.65879555471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 619200.0000, 
sim time next is 619800.0000, 
raw observation next is [-4.5, 73.83333333333333, 0.0, 0.0, 26.0, 23.98367002589843, 0.03999155261563465, 0.0, 1.0, 44156.54556964591], 
processed observation next is [0.0, 0.17391304347826086, 0.3379501385041552, 0.7383333333333333, 0.0, 0.0, 0.6666666666666666, 0.4986391688248692, 0.5133305175385449, 0.0, 1.0, 0.21026926461736148], 
reward next is 0.7897, 
noisyNet noise sample is [array([-0.5541724], dtype=float32), -0.009347784]. 
=============================================
[2019-04-04 10:24:57,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.82616509e-12 1.15443835e-11 1.33899010e-23 2.55826230e-13
 1.18736984e-13 9.15124126e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:24:57,337] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4937
[2019-04-04 10:24:57,388] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.0, 51.0, 0.0, 0.0, 26.0, 23.02708203501658, -0.210894639354243, 0.0, 1.0, 46193.30568350979], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 445200.0000, 
sim time next is 445800.0000, 
raw observation next is [-11.1, 51.5, 0.0, 0.0, 26.0, 22.97454222382104, -0.224726834044513, 0.0, 1.0, 46294.59259444171], 
processed observation next is [1.0, 0.13043478260869565, 0.1551246537396122, 0.515, 0.0, 0.0, 0.6666666666666666, 0.41454518531841994, 0.42509105531849567, 0.0, 1.0, 0.2204504409259129], 
reward next is 0.7795, 
noisyNet noise sample is [array([1.6771722], dtype=float32), -0.82451946]. 
=============================================
[2019-04-04 10:24:57,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7370925e-13 1.4118214e-12 6.9661960e-25 3.0084405e-14 5.5065426e-14
 1.7165117e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:24:57,411] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8771
[2019-04-04 10:24:57,461] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 85.66666666666667, 88.16666666666666, 134.6666666666667, 26.0, 24.79973332181223, 0.269471391471996, 0.0, 1.0, 44224.46355040994], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 555600.0000, 
sim time next is 556200.0000, 
raw observation next is [-0.6, 85.0, 77.0, 141.0, 26.0, 24.82087438287883, 0.2764568168466149, 0.0, 1.0, 30507.55761369906], 
processed observation next is [0.0, 0.43478260869565216, 0.44598337950138506, 0.85, 0.25666666666666665, 0.1558011049723757, 0.6666666666666666, 0.5684061985732359, 0.5921522722822049, 0.0, 1.0, 0.14527408387475743], 
reward next is 0.8547, 
noisyNet noise sample is [array([-0.9843775], dtype=float32), -1.3014457]. 
=============================================
[2019-04-04 10:25:04,362] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3591629e-15 3.6744642e-14 8.4351356e-27 2.8161355e-16 3.9419405e-16
 5.3989338e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:04,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9075
[2019-04-04 10:25:04,384] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 63.00000000000001, 93.16666666666666, 660.5000000000001, 26.0, 25.8244224602784, 0.3752385835693854, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 732000.0000, 
sim time next is 732600.0000, 
raw observation next is [-0.6, 61.5, 84.0, 779.0, 26.0, 25.8343337012198, 0.389460024775109, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.44598337950138506, 0.615, 0.28, 0.8607734806629834, 0.6666666666666666, 0.6528611417683168, 0.6298200082583697, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7670315], dtype=float32), -0.43704325]. 
=============================================
[2019-04-04 10:25:04,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9501462e-15 5.0841011e-14 6.4815739e-26 4.6764403e-16 8.1650318e-16
 1.2080203e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:04,502] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7403
[2019-04-04 10:25:04,522] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 57.0, 107.5, 614.0, 26.0, 25.84235945136904, 0.3889770371379295, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 734400.0000, 
sim time next is 735000.0000, 
raw observation next is [-0.4166666666666667, 55.83333333333333, 115.3333333333333, 559.0, 26.0, 25.840711271506, 0.382411749769155, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.45106186518928904, 0.5583333333333332, 0.3844444444444443, 0.6176795580110497, 0.6666666666666666, 0.6533926059588332, 0.627470583256385, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7670315], dtype=float32), -0.43704325]. 
=============================================
[2019-04-04 10:25:04,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.06959 ]
 [87.96532 ]
 [89.256744]
 [90.36005 ]
 [90.29785 ]], R is [[86.71121979]
 [86.84410858]
 [86.97566986]
 [87.10591125]
 [87.23485565]].
[2019-04-04 10:25:04,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4522924e-13 1.4520605e-12 8.9986754e-25 2.1122620e-14 1.4665661e-14
 9.3692174e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:04,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3300
[2019-04-04 10:25:04,957] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 82.0, 47.0, 26.0, 24.88112943336666, 0.2292775556857284, 0.0, 1.0, 35054.52766636142], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 658800.0000, 
sim time next is 659400.0000, 
raw observation next is [-0.6, 54.0, 82.33333333333334, 44.0, 26.0, 24.90537562417295, 0.2280504114303763, 0.0, 1.0, 24212.01118608043], 
processed observation next is [0.0, 0.6521739130434783, 0.44598337950138506, 0.54, 0.2744444444444445, 0.04861878453038674, 0.6666666666666666, 0.5754479686810793, 0.5760168038101254, 0.0, 1.0, 0.11529529136228778], 
reward next is 0.8847, 
noisyNet noise sample is [array([0.30672422], dtype=float32), -1.1777173]. 
=============================================
[2019-04-04 10:25:12,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.45525849e-14 2.96226937e-13 1.60999762e-26 2.54528403e-15
 1.02253224e-14 4.40977719e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 10:25:12,914] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9071
[2019-04-04 10:25:12,956] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.43887766905723, 0.4590517533259002, 0.0, 1.0, 49411.88880657491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 976200.0000, 
sim time next is 976800.0000, 
raw observation next is [9.8, 86.33333333333334, 0.0, 0.0, 26.0, 25.48746379996748, 0.4579219987709814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.7340720221606649, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6239553166639565, 0.6526406662569938, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0656581], dtype=float32), -0.44017404]. 
=============================================
[2019-04-04 10:25:19,147] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0416221e-14 3.2206936e-13 6.7591063e-26 1.2348493e-15 6.8002397e-15
 2.9855367e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:19,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6466
[2019-04-04 10:25:19,203] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.683333333333334, 87.83333333333334, 0.0, 0.0, 26.0, 25.331639479186, 0.429877334563315, 0.0, 1.0, 38000.63453372124], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 954600.0000, 
sim time next is 955200.0000, 
raw observation next is [5.866666666666667, 86.66666666666667, 0.0, 0.0, 26.0, 25.35837869582365, 0.4345743049761091, 0.0, 1.0, 37969.7580060867], 
processed observation next is [1.0, 0.043478260869565216, 0.6251154201292707, 0.8666666666666667, 0.0, 0.0, 0.6666666666666666, 0.6131982246519708, 0.644858101658703, 0.0, 1.0, 0.1808083714575557], 
reward next is 0.8192, 
noisyNet noise sample is [array([0.04392628], dtype=float32), -0.26571873]. 
=============================================
[2019-04-04 10:25:28,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.2715164e-14 3.7435311e-13 2.2759409e-26 1.3700745e-15 3.1067447e-15
 5.4433543e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:28,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9661
[2019-04-04 10:25:28,071] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 82.0, 0.0, 0.0, 26.0, 25.55978519492045, 0.4567112556129091, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 957600.0000, 
sim time next is 958200.0000, 
raw observation next is [6.783333333333333, 81.66666666666667, 0.0, 0.0, 26.0, 25.56930492550499, 0.4482436101148818, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.08695652173913043, 0.6505078485687905, 0.8166666666666668, 0.0, 0.0, 0.6666666666666666, 0.6307754104587492, 0.6494145367049606, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14100127], dtype=float32), -2.4873133]. 
=============================================
[2019-04-04 10:25:29,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7981788e-15 1.6885610e-14 1.2104820e-27 2.9405188e-16 1.5362372e-16
 4.9683239e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:29,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0382
[2019-04-04 10:25:29,065] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.36666666666667, 52.5, 0.0, 0.0, 26.0, 26.20048742633513, 0.7719069770843706, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1101000.0000, 
sim time next is 1101600.0000, 
raw observation next is [16.1, 53.0, 0.0, 0.0, 26.0, 26.42372488059192, 0.7749382981584149, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.9085872576177286, 0.53, 0.0, 0.0, 0.6666666666666666, 0.7019770733826599, 0.758312766052805, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.4778178], dtype=float32), -2.1021423]. 
=============================================
[2019-04-04 10:25:33,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8844801e-16 1.1516831e-14 1.1860185e-28 6.4491957e-17 1.5252942e-16
 1.1137038e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:33,333] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0872
[2019-04-04 10:25:33,338] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.03333333333333, 76.66666666666667, 111.6666666666667, 38.99999999999999, 26.0, 27.12968058150722, 0.8574915747213345, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1074000.0000, 
sim time next is 1074600.0000, 
raw observation next is [14.4, 75.0, 114.0, 0.0, 26.0, 27.17288658777657, 0.8638857986559252, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.8614958448753465, 0.75, 0.38, 0.0, 0.6666666666666666, 0.7644072156480476, 0.7879619328853084, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1401113], dtype=float32), -1.0842118]. 
=============================================
[2019-04-04 10:25:41,664] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8390508e-14 6.7914546e-14 3.0204718e-27 9.8523654e-16 2.6287460e-15
 1.8005414e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:41,681] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6039
[2019-04-04 10:25:41,693] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.35, 77.0, 0.0, 0.0, 26.0, 25.64875965830743, 0.61679821126075, 0.0, 1.0, 24859.51971190475], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1139400.0000, 
sim time next is 1140000.0000, 
raw observation next is [11.43333333333333, 77.0, 0.0, 0.0, 26.0, 25.64298032249437, 0.6159655029321879, 0.0, 1.0, 27562.53279142772], 
processed observation next is [0.0, 0.17391304347826086, 0.7793167128347184, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6369150268745308, 0.7053218343107294, 0.0, 1.0, 0.1312501561496558], 
reward next is 0.8687, 
noisyNet noise sample is [array([-0.7958514], dtype=float32), -0.44666374]. 
=============================================
[2019-04-04 10:25:41,708] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[91.59719]
 [91.5629 ]
 [91.53796]
 [91.51088]
 [91.43846]], R is [[91.58711243]
 [91.55286407]
 [91.54138947]
 [91.53681183]
 [91.49003601]].
[2019-04-04 10:25:42,116] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3147761e-15 2.7782057e-14 7.5642077e-27 1.4589757e-16 3.7944861e-16
 5.8529803e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:42,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9214
[2019-04-04 10:25:42,140] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 93.0, 31.0, 0.0, 26.0, 25.70597884838442, 0.5190710957447235, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1353600.0000, 
sim time next is 1354200.0000, 
raw observation next is [1.0, 93.5, 26.66666666666666, 0.0, 26.0, 25.71549967433145, 0.5142478123898219, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4903047091412743, 0.935, 0.08888888888888886, 0.0, 0.6666666666666666, 0.6429583061942875, 0.6714159374632739, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10914907], dtype=float32), -0.9759502]. 
=============================================
[2019-04-04 10:25:46,764] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1076782e-15 6.2135782e-14 2.5113784e-26 8.4735297e-16 9.4382764e-16
 5.7862918e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:46,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7938
[2019-04-04 10:25:46,803] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 72.0, 0.0, 26.0, 25.53236003774968, 0.4925055935508838, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1434600.0000, 
sim time next is 1435200.0000, 
raw observation next is [1.1, 92.0, 67.66666666666667, 0.0, 26.0, 25.8153467485185, 0.5192126955759034, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22555555555555556, 0.0, 0.6666666666666666, 0.6512788957098751, 0.6730708985253012, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.74905944], dtype=float32), -2.0086422]. 
=============================================
[2019-04-04 10:25:57,377] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.2124031e-14 2.6178709e-13 1.3287409e-25 2.7559322e-15 5.5230118e-15
 5.7814406e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:25:57,377] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7948
[2019-04-04 10:25:57,390] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.083333333333334, 92.83333333333334, 0.0, 0.0, 26.0, 25.61223283323752, 0.5405305574771848, 0.0, 1.0, 18734.67350470922], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1666200.0000, 
sim time next is 1666800.0000, 
raw observation next is [5.0, 92.0, 0.0, 0.0, 26.0, 25.58664347650115, 0.5376818275066634, 0.0, 1.0, 28060.19429868904], 
processed observation next is [1.0, 0.30434782608695654, 0.6011080332409973, 0.92, 0.0, 0.0, 0.6666666666666666, 0.632220289708429, 0.6792272758355544, 0.0, 1.0, 0.1336199728509002], 
reward next is 0.8664, 
noisyNet noise sample is [array([-0.09960018], dtype=float32), 0.47701323]. 
=============================================
[2019-04-04 10:26:02,338] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.7736763e-13 3.8959840e-13 2.0540502e-24 2.4101679e-14 5.5323953e-15
 6.7977948e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:26:02,341] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3792
[2019-04-04 10:26:02,398] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 75.0, 183.3333333333333, 76.66666666666667, 26.0, 25.02868317847993, 0.2816130712064742, 0.0, 1.0, 41838.01953091958], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1866000.0000, 
sim time next is 1866600.0000, 
raw observation next is [-4.5, 77.0, 186.0, 84.0, 26.0, 25.02389678478425, 0.2848436564665893, 0.0, 1.0, 45306.65095097603], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.77, 0.62, 0.09281767955801105, 0.6666666666666666, 0.5853247320653541, 0.5949478854888631, 0.0, 1.0, 0.21574595690940968], 
reward next is 0.7843, 
noisyNet noise sample is [array([-0.36413208], dtype=float32), 1.0435094]. 
=============================================
[2019-04-04 10:26:06,506] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.52696811e-12 3.01246311e-12 1.13843404e-23 5.27420006e-14
 5.17912048e-13 2.38640754e-15 1.00000000e+00], sum to 1.0000
[2019-04-04 10:26:06,506] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7272
[2019-04-04 10:26:06,563] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.00880322946809, 0.3151774601432937, 0.0, 1.0, 48147.30965864663], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1796400.0000, 
sim time next is 1797000.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01150164596977, 0.3143140389017838, 0.0, 1.0, 45585.51167848399], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5842918038308141, 0.6047713463005946, 0.0, 1.0, 0.21707386513563806], 
reward next is 0.7829, 
noisyNet noise sample is [array([-1.6257577], dtype=float32), -0.63714236]. 
=============================================
[2019-04-04 10:26:06,569] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[77.91252]
 [77.84852]
 [77.78329]
 [77.71316]
 [77.64036]], R is [[77.99951935]
 [77.99025726]
 [77.9793396 ]
 [77.96590424]
 [77.95121765]].
[2019-04-04 10:26:12,453] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7549804e-12 1.1849480e-11 4.8540006e-23 8.1128802e-14 3.0713263e-13
 4.3961624e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:26:12,455] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3956
[2019-04-04 10:26:12,509] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01150164596977, 0.3143140389017838, 0.0, 1.0, 45585.51167848399], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1797000.0000, 
sim time next is 1797600.0000, 
raw observation next is [-4.5, 83.0, 0.0, 0.0, 26.0, 25.01340676877817, 0.3192166867906842, 0.0, 1.0, 121177.5370829508], 
processed observation next is [0.0, 0.8260869565217391, 0.3379501385041552, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5844505640648476, 0.6064055622635615, 0.0, 1.0, 0.5770358908711943], 
reward next is 0.4230, 
noisyNet noise sample is [array([0.7037394], dtype=float32), 0.12533985]. 
=============================================
[2019-04-04 10:26:29,315] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4509745e-13 3.7454683e-12 3.6349220e-24 3.0113570e-14 4.2043175e-14
 1.2924484e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:26:29,315] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6255
[2019-04-04 10:26:29,355] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 83.0, 0.0, 0.0, 26.0, 25.25893521544404, 0.4173768206839886, 0.0, 1.0, 43960.34000687009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2149200.0000, 
sim time next is 2149800.0000, 
raw observation next is [-5.783333333333333, 83.0, 0.0, 0.0, 26.0, 25.268028025828, 0.4175395602829695, 0.0, 1.0, 43317.65508273181], 
processed observation next is [1.0, 0.9130434782608695, 0.3024007386888274, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6056690021523332, 0.6391798534276565, 0.0, 1.0, 0.20627454801300862], 
reward next is 0.7937, 
noisyNet noise sample is [array([0.41054016], dtype=float32), -0.19566718]. 
=============================================
[2019-04-04 10:26:33,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6748215e-12 3.3858665e-12 8.6343595e-24 6.4282055e-14 2.4355393e-13
 6.0209304e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:26:33,226] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3663
[2019-04-04 10:26:33,261] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 86.0, 0.0, 0.0, 26.0, 24.95225502935209, 0.2753514089930717, 0.0, 1.0, 45836.41650810099], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1807200.0000, 
sim time next is 1807800.0000, 
raw observation next is [-5.0, 85.33333333333334, 0.0, 0.0, 26.0, 24.90529758003559, 0.2669736949136082, 0.0, 1.0, 45817.08405802683], 
processed observation next is [0.0, 0.9565217391304348, 0.32409972299168976, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5754414650029659, 0.5889912316378694, 0.0, 1.0, 0.2181765907525087], 
reward next is 0.7818, 
noisyNet noise sample is [array([0.21464925], dtype=float32), 0.16492735]. 
=============================================
[2019-04-04 10:26:52,377] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 10:26:52,406] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:26:52,406] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:26:52,408] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run41
[2019-04-04 10:26:52,467] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:26:52,467] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:26:52,470] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run41
[2019-04-04 10:26:52,501] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:26:52,502] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:26:52,504] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run41
[2019-04-04 10:29:58,327] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:30:27,775] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:30:34,691] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:30:35,742] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 4000000, evaluation results [4000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:30:39,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8012407e-15 7.3454989e-14 2.3711416e-25 1.9475433e-15 1.3563149e-15
 4.2684162e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:39,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8903
[2019-04-04 10:30:39,617] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 68.0, 144.0, 0.0, 26.0, 25.16956206714185, 0.224555725082591, 1.0, 1.0, 54609.20889526853], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2208600.0000, 
sim time next is 2209200.0000, 
raw observation next is [-3.733333333333333, 69.0, 140.0, 0.0, 26.0, 25.00787926245235, 0.339117377854758, 1.0, 1.0, 136431.5469962826], 
processed observation next is [1.0, 0.5652173913043478, 0.35918744228993543, 0.69, 0.4666666666666667, 0.0, 0.6666666666666666, 0.5839899385376958, 0.613039125951586, 1.0, 1.0, 0.6496740333156313], 
reward next is 0.3503, 
noisyNet noise sample is [array([-0.30435103], dtype=float32), -0.5764585]. 
=============================================
[2019-04-04 10:30:40,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2414322e-13 2.5820734e-12 1.7590279e-24 2.2693400e-14 2.7089047e-14
 2.3095105e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:40,392] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9906
[2019-04-04 10:30:40,407] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.9, 85.0, 0.0, 0.0, 26.0, 24.20775087106731, 0.08949037669847333, 0.0, 1.0, 41328.10787200006], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2003400.0000, 
sim time next is 2004000.0000, 
raw observation next is [-6.0, 85.66666666666667, 0.0, 0.0, 26.0, 24.2384020077733, 0.08508005896669135, 0.0, 1.0, 41093.73673040397], 
processed observation next is [1.0, 0.17391304347826086, 0.296398891966759, 0.8566666666666667, 0.0, 0.0, 0.6666666666666666, 0.5198668339811082, 0.5283600196555638, 0.0, 1.0, 0.1956844606209713], 
reward next is 0.8043, 
noisyNet noise sample is [array([1.0303968], dtype=float32), -0.11068679]. 
=============================================
[2019-04-04 10:30:40,412] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.718056]
 [79.76494 ]
 [79.84016 ]
 [79.91948 ]
 [79.95826 ]], R is [[79.67636108]
 [79.68280029]
 [79.68787384]
 [79.69441223]
 [79.70098114]].
[2019-04-04 10:30:43,267] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6607764e-14 2.6277361e-13 1.5552462e-25 1.2148801e-14 2.6725069e-15
 4.1083599e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:43,268] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2251
[2019-04-04 10:30:43,338] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.633333333333333, 81.0, 89.0, 50.5, 26.0, 25.54012966288465, 0.3148201078480236, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2104800.0000, 
sim time next is 2105400.0000, 
raw observation next is [-7.716666666666667, 81.5, 106.0, 63.99999999999999, 26.0, 25.45550190737459, 0.3225826986265306, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.24884579870729456, 0.815, 0.35333333333333333, 0.07071823204419889, 0.6666666666666666, 0.6212918256145491, 0.6075275662088435, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7916777], dtype=float32), 2.1182668]. 
=============================================
[2019-04-04 10:30:43,853] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9664704e-13 2.7274524e-12 1.3961800e-23 1.1797464e-13 8.2695325e-14
 6.5505341e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:43,853] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7995
[2019-04-04 10:30:44,028] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.600000000000001, 75.0, 0.0, 0.0, 26.0, 23.65141884938635, -0.02152890688398877, 0.0, 1.0, 41938.48604799178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2185800.0000, 
sim time next is 2186400.0000, 
raw observation next is [-5.6, 75.0, 0.0, 0.0, 26.0, 23.65786992108811, 0.04094790434246442, 1.0, 1.0, 202416.1885477095], 
processed observation next is [1.0, 0.30434782608695654, 0.30747922437673136, 0.75, 0.0, 0.0, 0.6666666666666666, 0.4714891600906759, 0.5136493014474881, 1.0, 1.0, 0.9638866121319499], 
reward next is 0.0361, 
noisyNet noise sample is [array([-0.6309417], dtype=float32), -0.8086199]. 
=============================================
[2019-04-04 10:30:45,473] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9040998e-13 3.1512466e-12 6.4111526e-24 4.8514679e-14 1.9070226e-13
 1.2710671e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:45,473] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2448
[2019-04-04 10:30:45,519] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 86.33333333333334, 0.0, 0.0, 26.0, 24.15635758961614, 0.09977588330870964, 0.0, 1.0, 43682.06076511289], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2258400.0000, 
sim time next is 2259000.0000, 
raw observation next is [-8.1, 86.5, 0.0, 0.0, 26.0, 24.1736016960221, 0.0845978233380756, 0.0, 1.0, 43643.5409863981], 
processed observation next is [1.0, 0.13043478260869565, 0.23822714681440446, 0.865, 0.0, 0.0, 0.6666666666666666, 0.5144668080018416, 0.5281992744460252, 0.0, 1.0, 0.20782638564951475], 
reward next is 0.7922, 
noisyNet noise sample is [array([-0.60250294], dtype=float32), -0.22294205]. 
=============================================
[2019-04-04 10:30:45,558] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.789604]
 [78.831215]
 [78.8823  ]
 [78.95671 ]
 [79.02966 ]], R is [[78.73445129]
 [78.7390976 ]
 [78.74343109]
 [78.74767303]
 [78.7509079 ]].
[2019-04-04 10:30:47,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2944376e-12 1.1109051e-11 2.4302648e-23 1.1442479e-13 3.4706301e-13
 1.7889260e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:47,987] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2993
[2019-04-04 10:30:48,047] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.483333333333334, 60.83333333333334, 0.0, 0.0, 26.0, 23.34730422195621, -0.1210125515594287, 0.0, 1.0, 44337.34298091124], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2437800.0000, 
sim time next is 2438400.0000, 
raw observation next is [-8.566666666666666, 60.66666666666667, 0.0, 0.0, 26.0, 23.31678559223303, -0.1279216809487761, 0.0, 1.0, 44295.88722860877], 
processed observation next is [0.0, 0.21739130434782608, 0.22530009233610343, 0.6066666666666667, 0.0, 0.0, 0.6666666666666666, 0.44306546601941904, 0.45735943968374126, 0.0, 1.0, 0.21093279632670844], 
reward next is 0.7891, 
noisyNet noise sample is [array([0.8821393], dtype=float32), 3.0304809]. 
=============================================
[2019-04-04 10:30:50,599] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.4680098e-13 6.4327376e-12 8.1008710e-24 4.3487613e-14 1.4153041e-13
 2.0302055e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:50,600] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4124
[2019-04-04 10:30:50,624] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926054262, 0.4261490673521577, 0.0, 1.0, 32336.74252940987], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323800.0000, 
sim time next is 2324400.0000, 
raw observation next is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40218378435724, 0.4230242712883694, 0.0, 1.0, 51810.36194303472], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.6168486486964367, 0.6410080904294565, 0.0, 1.0, 0.24671600925254628], 
reward next is 0.7533, 
noisyNet noise sample is [array([1.2150726], dtype=float32), 0.5456099]. 
=============================================
[2019-04-04 10:30:54,864] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5888008e-15 3.0359826e-14 1.0250651e-25 4.2186757e-16 9.4359724e-16
 5.0368922e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:54,864] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0107
[2019-04-04 10:30:54,909] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.566666666666666, 66.0, 130.6666666666667, 0.0, 26.0, 25.51469048115779, 0.3166449214087577, 1.0, 1.0, 24442.96306548701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2205600.0000, 
sim time next is 2206200.0000, 
raw observation next is [-3.483333333333333, 65.5, 133.3333333333333, 0.0, 26.0, 25.35679650160794, 0.3138160385129386, 1.0, 1.0, 24404.39120296582], 
processed observation next is [1.0, 0.5217391304347826, 0.3661126500461681, 0.655, 0.4444444444444443, 0.0, 0.6666666666666666, 0.6130663751339949, 0.6046053461709796, 1.0, 1.0, 0.11621138668078962], 
reward next is 0.8838, 
noisyNet noise sample is [array([-0.16191952], dtype=float32), -0.04395229]. 
=============================================
[2019-04-04 10:30:58,136] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.6251598e-13 1.6658692e-12 1.9832821e-23 5.8652627e-14 7.7504653e-14
 3.9085494e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:30:58,137] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1150
[2019-04-04 10:30:58,184] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 47.00000000000001, 110.6666666666667, 227.3333333333334, 26.0, 24.94912489195336, 0.3080602168246775, 0.0, 1.0, 28169.63016020905], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2387400.0000, 
sim time next is 2388000.0000, 
raw observation next is [0.0, 47.0, 98.33333333333333, 284.1666666666667, 26.0, 24.96511345834705, 0.3130466557968632, 0.0, 1.0, 20412.17705888427], 
processed observation next is [0.0, 0.6521739130434783, 0.46260387811634357, 0.47, 0.3277777777777778, 0.31399631675874773, 0.6666666666666666, 0.5804261215289209, 0.604348885265621, 0.0, 1.0, 0.09720084313754414], 
reward next is 0.9028, 
noisyNet noise sample is [array([0.84916514], dtype=float32), -1.2631149]. 
=============================================
[2019-04-04 10:30:58,199] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.04896]
 [79.72487]
 [79.29311]
 [78.81305]
 [78.59202]], R is [[80.22647858]
 [80.29007721]
 [80.30460358]
 [80.2951355 ]
 [80.30872345]].
[2019-04-04 10:31:03,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2100407e-13 1.9991040e-12 1.1548799e-24 1.3758318e-13 3.3347562e-14
 4.0874459e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:03,756] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3829
[2019-04-04 10:31:03,781] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15746206337576, 0.286818246293717, 0.0, 1.0, 43326.67371916811], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2406000.0000, 
sim time next is 2406600.0000, 
raw observation next is [-3.4, 42.0, 0.0, 0.0, 26.0, 25.15863164123137, 0.2810747596082467, 0.0, 1.0, 43113.74704714974], 
processed observation next is [0.0, 0.8695652173913043, 0.368421052631579, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5965526367692808, 0.5936915865360822, 0.0, 1.0, 0.20530355736737974], 
reward next is 0.7947, 
noisyNet noise sample is [array([1.3101232], dtype=float32), 0.3400984]. 
=============================================
[2019-04-04 10:31:03,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.7022698e-13 2.0338102e-12 1.9261157e-24 5.2205148e-14 5.5629675e-14
 2.9712224e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:03,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8928
[2019-04-04 10:31:03,826] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 79.66666666666667, 0.0, 0.0, 26.0, 23.98950775389666, 0.04690790268967274, 0.0, 1.0, 43602.18021903429], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2612400.0000, 
sim time next is 2613000.0000, 
raw observation next is [-6.616666666666667, 78.83333333333333, 0.0, 0.0, 26.0, 23.93241796139642, 0.03548468542256034, 0.0, 1.0, 43792.99852735725], 
processed observation next is [1.0, 0.21739130434782608, 0.2793167128347184, 0.7883333333333333, 0.0, 0.0, 0.6666666666666666, 0.49436816344970175, 0.5118282284741867, 0.0, 1.0, 0.20853808822551073], 
reward next is 0.7915, 
noisyNet noise sample is [array([0.7234094], dtype=float32), 1.7837844]. 
=============================================
[2019-04-04 10:31:03,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[78.69777 ]
 [78.81316 ]
 [78.9194  ]
 [79.017746]
 [79.11739 ]], R is [[78.59368134]
 [78.60012054]
 [78.60732269]
 [78.61521149]
 [78.62371063]].
[2019-04-04 10:31:10,305] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.4802283e-12 9.1789154e-12 3.9376710e-24 2.5840281e-13 1.5550243e-13
 3.4488897e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:10,305] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8089
[2019-04-04 10:31:10,337] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7, 29.0, 0.0, 0.0, 26.0, 25.2399406698794, 0.2843621973500608, 0.0, 1.0, 42259.38276239834], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [-0.7833333333333333, 30.33333333333334, 0.0, 0.0, 26.0, 25.28946653986403, 0.2866569717933605, 0.0, 1.0, 40744.81278979175], 
processed observation next is [0.0, 0.8695652173913043, 0.44090489381348114, 0.3033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6074555449886692, 0.5955523239311201, 0.0, 1.0, 0.19402291804662739], 
reward next is 0.8060, 
noisyNet noise sample is [array([-0.87935704], dtype=float32), 0.36822054]. 
=============================================
[2019-04-04 10:31:35,989] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.00319465e-14 1.40417503e-13 9.80354523e-25 3.58703037e-15
 1.83757742e-15 4.48084697e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 10:31:35,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2935
[2019-04-04 10:31:36,013] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.166666666666666, 29.0, 161.6666666666667, 57.66666666666666, 26.0, 25.76749750147655, 0.4207619917830225, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2815800.0000, 
sim time next is 2816400.0000, 
raw observation next is [6.333333333333333, 28.0, 139.8333333333333, 28.83333333333333, 26.0, 25.8754464797689, 0.4280916104874987, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6380424746075716, 0.28, 0.466111111111111, 0.031860036832412515, 0.6666666666666666, 0.6562872066474084, 0.6426972034958329, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29356542], dtype=float32), -1.3097218]. 
=============================================
[2019-04-04 10:31:45,456] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9201203e-15 2.3209546e-14 2.5033059e-26 5.4979522e-16 1.4792296e-15
 1.0605051e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:45,464] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9646
[2019-04-04 10:31:45,483] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 106.5, 729.5, 26.0, 26.80680909786748, 0.6775445537679293, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3147600.0000, 
sim time next is 3148200.0000, 
raw observation next is [7.0, 100.0, 108.0, 746.0, 26.0, 26.89040466452144, 0.6987240120290669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.6565096952908588, 1.0, 0.36, 0.8243093922651934, 0.6666666666666666, 0.7408670553767868, 0.732908004009689, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2560273], dtype=float32), 1.0034318]. 
=============================================
[2019-04-04 10:31:49,816] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2841296e-13 3.8818536e-13 4.2148444e-25 7.3505689e-15 3.4978090e-14
 3.9628503e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:49,816] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0251
[2019-04-04 10:31:49,886] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1666666666666667, 100.0, 0.0, 0.0, 26.0, 25.30764709337674, 0.3225186723125005, 0.0, 1.0, 39707.41281503543], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3111000.0000, 
sim time next is 3111600.0000, 
raw observation next is [0.3333333333333333, 100.0, 0.0, 0.0, 26.0, 25.3031440480109, 0.3219674233989786, 0.0, 1.0, 39579.48690476564], 
processed observation next is [1.0, 0.0, 0.4718374884579871, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6085953373342416, 0.6073224744663263, 0.0, 1.0, 0.18847374716555065], 
reward next is 0.8115, 
noisyNet noise sample is [array([0.11474654], dtype=float32), 0.84315807]. 
=============================================
[2019-04-04 10:31:50,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.5337463e-15 7.4951208e-14 2.9568267e-25 1.7551350e-15 6.3478606e-15
 2.2549514e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:50,344] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6253
[2019-04-04 10:31:50,381] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 93.0, 27.66666666666666, 45.33333333333333, 26.0, 25.92636791419971, 0.4723808361697787, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2913000.0000, 
sim time next is 2913600.0000, 
raw observation next is [1.666666666666667, 93.0, 16.83333333333333, 43.16666666666667, 26.0, 25.94523261137447, 0.4380448540274688, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5087719298245615, 0.93, 0.0561111111111111, 0.04769797421731124, 0.6666666666666666, 0.662102717614539, 0.6460149513424897, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5556356], dtype=float32), 0.73996663]. 
=============================================
[2019-04-04 10:31:53,850] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0423474e-13 5.3953062e-12 2.2632423e-24 1.8963577e-14 2.0764715e-13
 3.0101134e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:53,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0078
[2019-04-04 10:31:53,875] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 44.0, 0.0, 0.0, 26.0, 25.47761000987146, 0.4339419604715138, 0.0, 1.0, 18757.33950888826], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2842800.0000, 
sim time next is 2843400.0000, 
raw observation next is [2.0, 44.0, 0.0, 0.0, 26.0, 25.49151246997674, 0.4322263511537432, 0.0, 1.0, 18753.25437179721], 
processed observation next is [1.0, 0.9130434782608695, 0.518005540166205, 0.44, 0.0, 0.0, 0.6666666666666666, 0.6242927058313951, 0.644075450384581, 0.0, 1.0, 0.08930121129427243], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.3370612], dtype=float32), 0.8384524]. 
=============================================
[2019-04-04 10:31:59,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0712936e-13 2.5743236e-12 8.1020149e-24 2.4710157e-14 9.8763079e-14
 3.1118434e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:31:59,755] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8191
[2019-04-04 10:31:59,782] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.42866328382127, 0.5417853259457495, 0.0, 1.0, 83301.59557899495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3276000.0000, 
sim time next is 3276600.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.41351859206494, 0.4990034255490309, 0.0, 1.0, 67274.73179093328], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6177932160054116, 0.6663344751830103, 0.0, 1.0, 0.32035586567111085], 
reward next is 0.6796, 
noisyNet noise sample is [array([-1.0560198], dtype=float32), 0.74914825]. 
=============================================
[2019-04-04 10:32:08,126] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.1315261e-13 1.9108612e-12 4.0634564e-24 3.0608799e-14 1.2970059e-13
 7.0505752e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:08,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6314
[2019-04-04 10:32:08,143] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333334, 84.66666666666667, 0.0, 0.0, 26.0, 25.68133546645275, 0.5683533791373392, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3273600.0000, 
sim time next is 3274200.0000, 
raw observation next is [-5.5, 86.5, 0.0, 0.0, 26.0, 25.61774625450629, 0.5522432455349203, 0.0, 1.0, 18732.31365390505], 
processed observation next is [1.0, 0.9130434782608695, 0.3102493074792244, 0.865, 0.0, 0.0, 0.6666666666666666, 0.6348121878755242, 0.6840810818449734, 0.0, 1.0, 0.08920149359002405], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.985468], dtype=float32), 0.636977]. 
=============================================
[2019-04-04 10:32:09,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.2329470e-15 6.0744358e-14 1.5479593e-26 6.6583619e-16 1.0284780e-15
 1.3827216e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:09,478] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2104
[2019-04-04 10:32:09,516] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.833333333333334, 69.0, 108.6666666666667, 698.3333333333333, 26.0, 26.36082997064682, 0.5659410292567368, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3319800.0000, 
sim time next is 3320400.0000, 
raw observation next is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31550079037065, 0.5702048490235775, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2502308402585411, 0.68, 0.366111111111111, 0.7946593001841622, 0.6666666666666666, 0.6929583991975541, 0.6900682830078592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0776956], dtype=float32), 0.020196136]. 
=============================================
[2019-04-04 10:32:12,839] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9748052e-13 1.7588440e-12 3.6292134e-24 1.9607686e-14 9.3600508e-14
 1.7105437e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:12,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4857
[2019-04-04 10:32:12,872] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.94264865084124, 0.3213587915673476, 0.0, 1.0, 41197.8117180035], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3375600.0000, 
sim time next is 3376200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 24.93775212899168, 0.3120916143822904, 0.0, 1.0, 41177.1884560047], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5781460107493066, 0.6040305381274301, 0.0, 1.0, 0.19608184979049856], 
reward next is 0.8039, 
noisyNet noise sample is [array([-0.25078773], dtype=float32), -0.35859245]. 
=============================================
[2019-04-04 10:32:16,224] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.9279651e-14 4.5147596e-13 3.0732280e-26 5.4956904e-15 8.3338301e-15
 7.7459277e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:16,229] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.1607
[2019-04-04 10:32:16,242] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.0, 30.0, 74.83333333333334, 356.0000000000001, 26.0, 25.55889386973467, 0.4153065923730512, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3658800.0000, 
sim time next is 3659400.0000, 
raw observation next is [9.5, 29.0, 89.0, 403.0, 26.0, 25.57836968608911, 0.4251244586619747, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.7257617728531857, 0.29, 0.2966666666666667, 0.4453038674033149, 0.6666666666666666, 0.6315308071740926, 0.6417081528873249, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28752217], dtype=float32), 0.3984136]. 
=============================================
[2019-04-04 10:32:21,441] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3810416e-16 1.8278464e-14 6.1798524e-27 8.2202546e-16 1.1973038e-16
 2.6735532e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:21,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8323
[2019-04-04 10:32:21,477] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.85387625279866, 0.5189949064705648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490800.0000, 
sim time next is 3491400.0000, 
raw observation next is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 26.0, 26.01452750198677, 0.5421911438660643, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4579870729455217, 0.6183333333333333, 0.341111111111111, 0.7771639042357275, 0.6666666666666666, 0.6678772918322308, 0.6807303812886881, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.7094775], dtype=float32), 0.25338125]. 
=============================================
[2019-04-04 10:32:21,867] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.3908420e-15 4.8321207e-14 3.7805838e-25 8.8503884e-16 9.3599692e-16
 1.9411479e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:21,867] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7753
[2019-04-04 10:32:21,889] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 60.0, 40.5, 343.0, 26.0, 26.43610554846064, 0.6156232288423917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3776400.0000, 
sim time next is 3777000.0000, 
raw observation next is [-0.3333333333333333, 61.83333333333333, 32.66666666666666, 277.6666666666666, 26.0, 26.20108427603206, 0.5821953554052103, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4533702677747, 0.6183333333333333, 0.10888888888888885, 0.30681399631675865, 0.6666666666666666, 0.6834236896693383, 0.6940651184684034, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3005016], dtype=float32), -0.019247182]. 
=============================================
[2019-04-04 10:32:21,913] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[85.45565 ]
 [86.013176]
 [86.905594]
 [87.56169 ]
 [87.782814]], R is [[85.10169983]
 [85.25068665]
 [85.3981781 ]
 [85.54419708]
 [85.68875885]].
[2019-04-04 10:32:29,918] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3401767e-13 7.4750796e-13 3.0374138e-25 5.8468889e-14 4.1595067e-14
 1.6943471e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:29,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6209
[2019-04-04 10:32:29,946] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 54.16666666666666, 0.0, 0.0, 26.0, 25.31789215335619, 0.367592918719642, 0.0, 1.0, 38574.90154910761], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3625800.0000, 
sim time next is 3626400.0000, 
raw observation next is [1.0, 48.33333333333334, 0.0, 0.0, 26.0, 25.31639464522011, 0.3707691383639962, 0.0, 1.0, 38478.60943869619], 
processed observation next is [0.0, 1.0, 0.4903047091412743, 0.48333333333333345, 0.0, 0.0, 0.6666666666666666, 0.6096995537683426, 0.6235897127879987, 0.0, 1.0, 0.1832314735176009], 
reward next is 0.8168, 
noisyNet noise sample is [array([-1.3306555], dtype=float32), -0.47721425]. 
=============================================
[2019-04-04 10:32:32,623] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3170514e-13 3.0396013e-12 3.7415756e-24 3.7040182e-14 3.8734353e-14
 3.6445030e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:32,626] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6242
[2019-04-04 10:32:32,681] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 67.0, 0.0, 0.0, 26.0, 25.11960373549391, 0.4638115643804243, 0.0, 1.0, 198787.7383368569], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3788400.0000, 
sim time next is 3789000.0000, 
raw observation next is [-2.5, 68.0, 0.0, 0.0, 26.0, 25.11585148557431, 0.5037857409494001, 0.0, 1.0, 170408.7271059874], 
processed observation next is [1.0, 0.8695652173913043, 0.39335180055401664, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5929876237978592, 0.6679285803164667, 0.0, 1.0, 0.8114701290761305], 
reward next is 0.1885, 
noisyNet noise sample is [array([-0.40248156], dtype=float32), -0.8059363]. 
=============================================
[2019-04-04 10:32:32,685] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[81.154526]
 [82.037346]
 [85.50048 ]
 [84.80713 ]
 [83.58662 ]], R is [[80.56978607]
 [79.81748199]
 [79.73510742]
 [79.63253021]
 [79.55078125]].
[2019-04-04 10:32:34,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.8951071e-13 3.4703473e-12 9.9214200e-24 1.3240712e-13 9.4445888e-14
 1.0980128e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:34,374] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0191
[2019-04-04 10:32:34,389] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.31661434358644, 0.1700516007650759, 0.0, 1.0, 43626.75988136935], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3985800.0000, 
sim time next is 3986400.0000, 
raw observation next is [-12.0, 63.00000000000001, 0.0, 0.0, 26.0, 24.34582878521737, 0.1574518595533868, 0.0, 1.0, 43588.42509113], 
processed observation next is [1.0, 0.13043478260869565, 0.13019390581717452, 0.6300000000000001, 0.0, 0.0, 0.6666666666666666, 0.528819065434781, 0.5524839531844622, 0.0, 1.0, 0.20756392900538093], 
reward next is 0.7924, 
noisyNet noise sample is [array([0.09240744], dtype=float32), -1.5222923]. 
=============================================
[2019-04-04 10:32:37,925] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5210316e-13 5.0979923e-12 1.7882560e-24 1.7977930e-14 1.8432203e-13
 9.5156401e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:37,925] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9182
[2019-04-04 10:32:37,942] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666667, 58.5, 0.0, 0.0, 26.0, 25.21627409588835, 0.5447488504835446, 0.0, 1.0, 87126.31150101422], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3876600.0000, 
sim time next is 3877200.0000, 
raw observation next is [-1.0, 60.0, 0.0, 0.0, 26.0, 25.46565745605039, 0.5660325337742216, 0.0, 1.0, 18759.25425434499], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.6, 0.0, 0.0, 0.6666666666666666, 0.6221381213375325, 0.6886775112580739, 0.0, 1.0, 0.08932978216354756], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.19011286], dtype=float32), 2.1802874]. 
=============================================
[2019-04-04 10:32:43,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4450120e-14 9.7976987e-13 5.3689727e-24 1.7134648e-14 1.3651581e-14
 3.1994735e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:43,524] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7704
[2019-04-04 10:32:43,533] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.46419016304972, 0.4885594839193566, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3871800.0000, 
sim time next is 3872400.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.3778992497437, 0.4720413714756738, 0.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6148249374786415, 0.6573471238252245, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.0783122], dtype=float32), -0.95844346]. 
=============================================
[2019-04-04 10:32:44,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5838999e-13 1.1177463e-12 1.6924539e-24 3.0392040e-14 7.5939648e-14
 5.4182924e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:44,469] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0854
[2019-04-04 10:32:44,485] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.33954015869021, 0.4373681577692241, 0.0, 1.0, 40645.56966058978], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3889800.0000, 
sim time next is 3890400.0000, 
raw observation next is [-2.0, 65.0, 0.0, 0.0, 26.0, 25.34017362068409, 0.4366501780860071, 0.0, 1.0, 40068.40743408317], 
processed observation next is [1.0, 0.0, 0.40720221606648205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6116811350570076, 0.6455500593620024, 0.0, 1.0, 0.19080194016230081], 
reward next is 0.8092, 
noisyNet noise sample is [array([0.37556], dtype=float32), -0.47077376]. 
=============================================
[2019-04-04 10:32:46,725] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2706428e-15 5.5022274e-14 7.6943844e-27 1.2573086e-15 2.2004406e-16
 6.4849885e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:32:46,725] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9888
[2019-04-04 10:32:46,785] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.33333333333333, 50.0, 99.66666666666667, 655.6666666666667, 26.0, 26.42914941968986, 0.533546210770827, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4008000.0000, 
sim time next is 4008600.0000, 
raw observation next is [-10.0, 48.5, 101.0, 698.0, 26.0, 26.49120392159066, 0.5455615340710921, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.18559556786703602, 0.485, 0.33666666666666667, 0.7712707182320442, 0.6666666666666666, 0.7076003267992217, 0.681853844690364, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.85962], dtype=float32), -1.2351631]. 
=============================================
[2019-04-04 10:33:06,351] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.8559148e-15 5.3564903e-14 1.2602114e-26 1.4381006e-15 5.1516644e-16
 2.3262372e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:06,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7669
[2019-04-04 10:33:06,441] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.0, 44.0, 106.5, 739.5, 26.0, 26.55905216694713, 0.5656536824750701, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4010400.0000, 
sim time next is 4011000.0000, 
raw observation next is [-8.833333333333334, 43.33333333333334, 108.3333333333333, 753.3333333333334, 26.0, 26.57530991271009, 0.571596795528431, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.21791320406278855, 0.4333333333333334, 0.361111111111111, 0.8324125230202579, 0.6666666666666666, 0.7146091593925075, 0.6905322651761437, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44082487], dtype=float32), 1.3939525]. 
=============================================
[2019-04-04 10:33:06,443] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[91.16786]
 [91.34522]
 [91.47531]
 [91.42476]
 [91.09188]], R is [[91.03385162]
 [91.12351227]
 [91.21228027]
 [91.30015564]
 [91.38715363]].
[2019-04-04 10:33:07,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.7479863e-13 5.7141184e-13 4.7545939e-25 1.0202529e-14 5.8449373e-14
 6.7248804e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:07,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6363
[2019-04-04 10:33:07,573] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 47.0, 0.0, 0.0, 26.0, 25.38952141633659, 0.3462614696184534, 0.0, 1.0, 46967.06205429121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4238400.0000, 
sim time next is 4239000.0000, 
raw observation next is [2.5, 46.5, 0.0, 0.0, 26.0, 25.3994941135691, 0.3476176788052936, 0.0, 1.0, 36757.02658301219], 
processed observation next is [0.0, 0.043478260869565216, 0.5318559556786704, 0.465, 0.0, 0.0, 0.6666666666666666, 0.6166245094640918, 0.6158725596017646, 0.0, 1.0, 0.17503345991910568], 
reward next is 0.8250, 
noisyNet noise sample is [array([0.38343734], dtype=float32), 0.67915314]. 
=============================================
[2019-04-04 10:33:07,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[85.15291]
 [85.11364]
 [85.0361 ]
 [85.04153]
 [85.05223]], R is [[85.14107513]
 [85.06600952]
 [84.94419861]
 [84.90795135]
 [84.90487671]].
[2019-04-04 10:33:08,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.42098708e-13 8.19370342e-13 2.10907485e-24 2.73855289e-14
 1.29146825e-14 1.96060956e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:33:08,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1569
[2019-04-04 10:33:08,617] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.41849401195853, 0.4717227623571529, 0.0, 1.0, 26134.01109501012], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4578000.0000, 
sim time next is 4578600.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.43947153842422, 0.4785340826447799, 0.0, 1.0, 19859.63130565336], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6199559615353515, 0.6595113608815933, 0.0, 1.0, 0.09456967288406362], 
reward next is 0.9054, 
noisyNet noise sample is [array([-0.07558968], dtype=float32), -0.66302145]. 
=============================================
[2019-04-04 10:33:14,745] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2893095e-15 5.2672937e-14 2.0778484e-26 1.0133985e-15 4.1732980e-16
 7.0028027e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:14,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1975
[2019-04-04 10:33:14,769] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 214.6666666666667, 97.33333333333334, 26.0, 26.48342636703084, 0.6621030946930381, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4445400.0000, 
sim time next is 4446000.0000, 
raw observation next is [1.0, 86.0, 196.5, 73.0, 26.0, 26.50146284245206, 0.6608231045004946, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.86, 0.655, 0.08066298342541436, 0.6666666666666666, 0.708455236871005, 0.7202743681668315, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.39948356], dtype=float32), -0.660889]. 
=============================================
[2019-04-04 10:33:14,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[88.36735 ]
 [89.62655 ]
 [90.67476 ]
 [90.75198 ]
 [90.853355]], R is [[87.5080719 ]
 [87.63299561]
 [87.75666809]
 [87.87910461]
 [88.00031281]].
[2019-04-04 10:33:20,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7792069e-12 5.5813600e-12 8.8456275e-24 7.4976660e-14 2.0162852e-13
 9.2147158e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:20,475] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4833
[2019-04-04 10:33:20,491] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8500000000000001, 72.0, 0.0, 0.0, 26.0, 25.21679536259799, 0.412574941342855, 0.0, 1.0, 42104.48246218143], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4509000.0000, 
sim time next is 4509600.0000, 
raw observation next is [-0.8333333333333334, 71.66666666666666, 0.0, 0.0, 26.0, 25.25794941923004, 0.4210578720169367, 0.0, 1.0, 41762.21933511127], 
processed observation next is [1.0, 0.17391304347826086, 0.43951985226223456, 0.7166666666666666, 0.0, 0.0, 0.6666666666666666, 0.6048291182691701, 0.6403526240056455, 0.0, 1.0, 0.1988677111195775], 
reward next is 0.8011, 
noisyNet noise sample is [array([-1.149486], dtype=float32), 0.83808637]. 
=============================================
[2019-04-04 10:33:23,328] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.9812180e-13 1.3498959e-12 1.2211046e-24 4.8502096e-14 4.8380137e-14
 8.8565308e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:23,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5256
[2019-04-04 10:33:23,355] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.42012509121562, 0.3987867968370672, 0.0, 1.0, 26292.3432459196], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4831800.0000, 
sim time next is 4832400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.40204192955769, 0.3929131087102047, 0.0, 1.0, 40894.49970326685], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6168368274631408, 0.6309710362367349, 0.0, 1.0, 0.19473571287269928], 
reward next is 0.8053, 
noisyNet noise sample is [array([1.5155499], dtype=float32), 1.2763867]. 
=============================================
[2019-04-04 10:33:24,656] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.1655533e-12 4.8163452e-12 4.9185836e-24 1.5730424e-13 1.4948192e-13
 6.8347470e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:24,657] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2258
[2019-04-04 10:33:24,673] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 61.83333333333333, 0.0, 0.0, 26.0, 24.67987780416417, 0.1980413185090211, 0.0, 1.0, 39561.34959837276], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4859400.0000, 
sim time next is 4860000.0000, 
raw observation next is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.651321954251, 0.1928671791374717, 0.0, 1.0, 39539.55233482625], 
processed observation next is [0.0, 0.2608695652173913, 0.3795013850415513, 0.6, 0.0, 0.0, 0.6666666666666666, 0.5542768295209166, 0.5642890597124905, 0.0, 1.0, 0.18828358254679167], 
reward next is 0.8117, 
noisyNet noise sample is [array([-0.09087516], dtype=float32), 0.2951874]. 
=============================================
[2019-04-04 10:33:24,702] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[80.41791 ]
 [80.3436  ]
 [80.2723  ]
 [80.204895]
 [80.13985 ]], R is [[80.50928497]
 [80.51580811]
 [80.52225494]
 [80.52876282]
 [80.53540039]].
[2019-04-04 10:33:27,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3427623e-12 2.6687519e-12 6.0291799e-24 3.5587103e-14 1.3411592e-13
 2.3818318e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:27,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5424
[2019-04-04 10:33:27,275] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666667, 92.83333333333333, 0.0, 0.0, 26.0, 23.97861443771411, 0.1173401802885154, 0.0, 1.0, 41797.742106038], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4776600.0000, 
sim time next is 4777200.0000, 
raw observation next is [-6.2, 93.0, 0.0, 0.0, 26.0, 23.94451579214939, 0.1098643347689528, 0.0, 1.0, 41838.28672744806], 
processed observation next is [0.0, 0.30434782608695654, 0.2908587257617729, 0.93, 0.0, 0.0, 0.6666666666666666, 0.4953763160124491, 0.5366214449229842, 0.0, 1.0, 0.19922993679737172], 
reward next is 0.8008, 
noisyNet noise sample is [array([-0.20967615], dtype=float32), 1.1413977]. 
=============================================
[2019-04-04 10:33:29,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.6920830e-14 1.6075603e-12 3.6062337e-25 1.7875387e-14 3.0285799e-14
 2.5430311e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:29,097] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0704
[2019-04-04 10:33:29,117] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 68.0, 0.0, 0.0, 26.0, 25.55630572915067, 0.53268996589476, 0.0, 1.0, 66074.86926284489], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4424400.0000, 
sim time next is 4425000.0000, 
raw observation next is [3.666666666666667, 68.0, 0.0, 0.0, 26.0, 25.53889477077428, 0.5599692892413267, 0.0, 1.0, 56638.58018907436], 
processed observation next is [1.0, 0.21739130434782608, 0.564173591874423, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6282412308978568, 0.6866564297471088, 0.0, 1.0, 0.2697075247098779], 
reward next is 0.7303, 
noisyNet noise sample is [array([1.2054341], dtype=float32), 1.5891283]. 
=============================================
[2019-04-04 10:33:29,121] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.89944 ]
 [81.78748 ]
 [81.66424 ]
 [81.50585 ]
 [81.464066]], R is [[81.91941833]
 [81.7855835 ]
 [81.65454865]
 [81.50714874]
 [81.43367004]].
[2019-04-04 10:33:32,317] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.6975515e-15 2.7779794e-13 3.1962139e-25 1.6553952e-15 1.7038238e-15
 4.5489799e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:32,318] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5928
[2019-04-04 10:33:32,327] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.32320555404392, 0.6622999484455909, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4993200.0000, 
sim time next is 4993800.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.34907858172992, 0.6538112925658339, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.6957565484774934, 0.7179370975219447, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.680985], dtype=float32), -1.6968993]. 
=============================================
[2019-04-04 10:33:36,940] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.3690179e-14 2.3042869e-13 1.3703400e-24 8.1244283e-15 1.8723994e-14
 3.3976447e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:36,940] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6555
[2019-04-04 10:33:36,949] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 37.0, 92.0, 667.6666666666667, 26.0, 25.1912201711568, 0.4396580218399413, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4809000.0000, 
sim time next is 4809600.0000, 
raw observation next is [3.0, 37.0, 89.5, 638.0, 26.0, 25.19429263062671, 0.4364635043255369, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6956521739130435, 0.5457063711911359, 0.37, 0.29833333333333334, 0.7049723756906078, 0.6666666666666666, 0.5995243858855591, 0.645487834775179, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7570051], dtype=float32), 1.6053865]. 
=============================================
[2019-04-04 10:33:37,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5861829e-13 2.9791278e-12 1.7943034e-24 1.9545963e-14 2.3754961e-14
 1.7238808e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:37,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4288
[2019-04-04 10:33:37,867] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.2, 30.0, 0.0, 0.0, 26.0, 25.85852294824385, 0.5442836926187081, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5096400.0000, 
sim time next is 5097000.0000, 
raw observation next is [8.15, 32.5, 0.0, 0.0, 26.0, 25.82281933610484, 0.5294451444161364, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.6883656509695293, 0.325, 0.0, 0.0, 0.6666666666666666, 0.6519016113420699, 0.6764817148053788, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.49281514], dtype=float32), 1.6910254]. 
=============================================
[2019-04-04 10:33:37,898] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[82.449646]
 [82.42665 ]
 [82.422165]
 [82.42145 ]
 [82.44458 ]], R is [[82.66941071]
 [82.84272003]
 [83.01428986]
 [83.1841507 ]
 [83.35231018]].
[2019-04-04 10:33:38,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:38,486] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:38,494] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run31
[2019-04-04 10:33:38,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:38,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:38,633] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run31
[2019-04-04 10:33:38,980] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:38,981] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:38,984] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run31
[2019-04-04 10:33:46,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:46,177] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:46,188] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run31
[2019-04-04 10:33:47,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:47,930] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:47,951] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run31
[2019-04-04 10:33:48,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:48,917] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:48,924] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run31
[2019-04-04 10:33:49,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:49,729] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:49,736] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run31
[2019-04-04 10:33:50,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3888821e-13 1.1482538e-12 4.6326539e-24 9.9587773e-15 9.3946776e-14
 3.0680428e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:33:50,581] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3356
[2019-04-04 10:33:50,637] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 86.0, 0.0, 0.0, 26.0, 24.61461558917743, 0.2099684358843824, 0.0, 1.0, 41072.27521447837], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 68400.0000, 
sim time next is 69000.0000, 
raw observation next is [3.616666666666667, 86.5, 0.0, 0.0, 26.0, 24.62008675215614, 0.2135457428914421, 0.0, 1.0, 42494.15841481954], 
processed observation next is [0.0, 0.8260869565217391, 0.5627885503231764, 0.865, 0.0, 0.0, 0.6666666666666666, 0.5516738960130118, 0.5711819142971474, 0.0, 1.0, 0.20235313530866447], 
reward next is 0.7976, 
noisyNet noise sample is [array([0.07742475], dtype=float32), 2.0612342]. 
=============================================
[2019-04-04 10:33:50,644] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[80.787  ]
 [80.73852]
 [80.68541]
 [80.65332]
 [80.65764]], R is [[80.81252289]
 [80.80882263]
 [80.80390167]
 [80.80239868]
 [80.80922699]].
[2019-04-04 10:33:50,948] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255000, global step 4077104: loss 0.0802
[2019-04-04 10:33:50,965] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255000, global step 4077105: learning rate 0.0000
[2019-04-04 10:33:51,199] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255000, global step 4077196: loss 0.0822
[2019-04-04 10:33:51,201] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255000, global step 4077196: learning rate 0.0000
[2019-04-04 10:33:51,637] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255000, global step 4077357: loss 0.0753
[2019-04-04 10:33:51,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255000, global step 4077357: learning rate 0.0000
[2019-04-04 10:33:52,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:52,313] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:52,325] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run31
[2019-04-04 10:33:53,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:53,017] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:53,020] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run31
[2019-04-04 10:33:53,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:53,545] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:53,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run31
[2019-04-04 10:33:53,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:53,974] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:53,977] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run31
[2019-04-04 10:33:54,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:54,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:54,256] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run31
[2019-04-04 10:33:55,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:55,833] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:55,836] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run31
[2019-04-04 10:33:56,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:33:56,465] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:33:56,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run31
[2019-04-04 10:33:59,613] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255000, global step 4078886: loss 0.0620
[2019-04-04 10:33:59,614] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255000, global step 4078886: learning rate 0.0000
[2019-04-04 10:34:02,032] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255000, global step 4079502: loss 0.0472
[2019-04-04 10:34:02,033] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255000, global step 4079502: learning rate 0.0000
[2019-04-04 10:34:02,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:34:02,415] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:34:02,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run31
[2019-04-04 10:34:03,092] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255000, global step 4079768: loss 0.0476
[2019-04-04 10:34:03,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255000, global step 4079768: learning rate 0.0000
[2019-04-04 10:34:03,830] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255000, global step 4079923: loss 0.0495
[2019-04-04 10:34:03,840] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255000, global step 4079923: learning rate 0.0000
[2019-04-04 10:34:06,682] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255000, global step 4080635: loss 0.0244
[2019-04-04 10:34:06,697] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255000, global step 4080640: learning rate 0.0000
[2019-04-04 10:34:07,007] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255000, global step 4080700: loss 0.0240
[2019-04-04 10:34:07,038] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255000, global step 4080705: learning rate 0.0000
[2019-04-04 10:34:07,089] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0424349e-14 7.7777325e-13 4.4546526e-24 8.1112358e-15 8.0672324e-15
 1.4717935e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:34:07,089] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8945
[2019-04-04 10:34:07,131] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 62.0, 37.0, 0.0, 26.0, 25.95274416476157, 0.3623554894456101, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 230400.0000, 
sim time next is 231000.0000, 
raw observation next is [-3.4, 62.5, 30.66666666666666, 0.0, 26.0, 25.88841318559674, 0.2099215181730404, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.368421052631579, 0.625, 0.1022222222222222, 0.0, 0.6666666666666666, 0.657367765466395, 0.5699738393910135, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.0182102], dtype=float32), -0.40020138]. 
=============================================
[2019-04-04 10:34:07,145] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.970184]
 [79.240685]
 [79.35222 ]
 [79.489784]
 [79.79893 ]], R is [[79.01329803]
 [79.22316742]
 [79.43093872]
 [79.6366272 ]
 [79.84026337]].
[2019-04-04 10:34:07,601] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255000, global step 4080834: loss 0.0248
[2019-04-04 10:34:07,602] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255000, global step 4080834: learning rate 0.0000
[2019-04-04 10:34:08,039] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255000, global step 4080946: loss 0.0274
[2019-04-04 10:34:08,040] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255000, global step 4080946: learning rate 0.0000
[2019-04-04 10:34:08,066] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255000, global step 4080952: loss 0.0245
[2019-04-04 10:34:08,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255000, global step 4080952: learning rate 0.0000
[2019-04-04 10:34:10,323] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255000, global step 4081775: loss 0.0206
[2019-04-04 10:34:10,323] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255000, global step 4081775: learning rate 0.0000
[2019-04-04 10:34:10,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:34:10,749] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:34:10,754] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run31
[2019-04-04 10:34:11,251] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255000, global step 4081997: loss 0.0263
[2019-04-04 10:34:11,254] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255000, global step 4081997: learning rate 0.0000
[2019-04-04 10:34:16,085] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255000, global step 4083057: loss 0.0261
[2019-04-04 10:34:16,085] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255000, global step 4083057: learning rate 0.0000
[2019-04-04 10:34:19,588] A3C_AGENT_WORKER-Thread-5 INFO:Local step 255500, global step 4084121: loss 14.9670
[2019-04-04 10:34:19,589] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 255500, global step 4084121: learning rate 0.0000
[2019-04-04 10:34:19,971] A3C_AGENT_WORKER-Thread-2 INFO:Local step 255500, global step 4084224: loss 14.8497
[2019-04-04 10:34:19,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 255500, global step 4084224: learning rate 0.0000
[2019-04-04 10:34:20,668] A3C_AGENT_WORKER-Thread-12 INFO:Local step 255500, global step 4084411: loss 14.9349
[2019-04-04 10:34:20,669] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 255500, global step 4084411: learning rate 0.0000
[2019-04-04 10:34:24,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.8092270e-14 7.1801359e-13 4.3538519e-24 2.1556363e-14 2.4442638e-14
 4.5035551e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:34:24,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5776
[2019-04-04 10:34:24,766] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.916666666666667, 65.0, 137.0, 0.0, 26.0, 25.18426835585715, 0.2227937729355375, 1.0, 1.0, 53779.6325222601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 216600.0000, 
sim time next is 217200.0000, 
raw observation next is [-4.833333333333334, 65.0, 133.0, 0.0, 26.0, 25.216118102689, 0.2221807179058446, 1.0, 1.0, 26442.79685041578], 
processed observation next is [1.0, 0.5217391304347826, 0.32871652816251157, 0.65, 0.44333333333333336, 0.0, 0.6666666666666666, 0.6013431752240832, 0.5740602393019482, 1.0, 1.0, 0.12591808024007514], 
reward next is 0.8741, 
noisyNet noise sample is [array([-0.75460786], dtype=float32), -0.7685502]. 
=============================================
[2019-04-04 10:34:24,941] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255000, global step 4085540: loss 0.0270
[2019-04-04 10:34:24,942] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255000, global step 4085540: learning rate 0.0000
[2019-04-04 10:34:26,384] A3C_AGENT_WORKER-Thread-18 INFO:Local step 255500, global step 4086077: loss 14.8561
[2019-04-04 10:34:26,387] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 255500, global step 4086078: learning rate 0.0000
[2019-04-04 10:34:30,143] A3C_AGENT_WORKER-Thread-20 INFO:Local step 255500, global step 4087057: loss 14.6249
[2019-04-04 10:34:30,143] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 255500, global step 4087057: learning rate 0.0000
[2019-04-04 10:34:33,004] A3C_AGENT_WORKER-Thread-3 INFO:Local step 255500, global step 4087564: loss 14.7254
[2019-04-04 10:34:33,004] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 255500, global step 4087564: learning rate 0.0000
[2019-04-04 10:34:33,291] A3C_AGENT_WORKER-Thread-6 INFO:Local step 255500, global step 4087607: loss 14.6742
[2019-04-04 10:34:33,292] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 255500, global step 4087607: learning rate 0.0000
[2019-04-04 10:34:36,637] A3C_AGENT_WORKER-Thread-13 INFO:Local step 255500, global step 4088302: loss 14.4706
[2019-04-04 10:34:36,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 255500, global step 4088302: learning rate 0.0000
[2019-04-04 10:34:36,917] A3C_AGENT_WORKER-Thread-16 INFO:Local step 255500, global step 4088373: loss 14.4099
[2019-04-04 10:34:36,918] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 255500, global step 4088373: learning rate 0.0000
[2019-04-04 10:34:37,715] A3C_AGENT_WORKER-Thread-17 INFO:Local step 255500, global step 4088538: loss 14.4186
[2019-04-04 10:34:37,763] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 255500, global step 4088538: learning rate 0.0000
[2019-04-04 10:34:38,137] A3C_AGENT_WORKER-Thread-4 INFO:Local step 255500, global step 4088615: loss 14.3787
[2019-04-04 10:34:38,154] A3C_AGENT_WORKER-Thread-15 INFO:Local step 255500, global step 4088619: loss 14.4119
[2019-04-04 10:34:38,156] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 255500, global step 4088619: learning rate 0.0000
[2019-04-04 10:34:38,166] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 255500, global step 4088615: learning rate 0.0000
[2019-04-04 10:34:40,924] A3C_AGENT_WORKER-Thread-11 INFO:Local step 255500, global step 4089165: loss 14.3641
[2019-04-04 10:34:40,924] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 255500, global step 4089165: learning rate 0.0000
[2019-04-04 10:34:42,204] A3C_AGENT_WORKER-Thread-19 INFO:Local step 255500, global step 4089408: loss 14.3366
[2019-04-04 10:34:42,233] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 255500, global step 4089417: learning rate 0.0000
[2019-04-04 10:34:48,473] A3C_AGENT_WORKER-Thread-10 INFO:Local step 255500, global step 4090786: loss 14.1854
[2019-04-04 10:34:48,473] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 255500, global step 4090786: learning rate 0.0000
[2019-04-04 10:34:51,246] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7957356e-15 1.6706634e-13 5.1291105e-25 1.1827418e-15 1.2769135e-15
 1.0187035e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:34:51,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0943
[2019-04-04 10:34:51,323] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.06666666666667, 51.0, 57.5, 902.0, 26.0, 25.64163013859327, 0.41293519694321, 1.0, 1.0, 91278.61558868072], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 391200.0000, 
sim time next is 391800.0000, 
raw observation next is [-11.88333333333333, 51.0, 57.0, 899.0, 26.0, 25.76993970437299, 0.2908747732070613, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.13342566943674988, 0.51, 0.19, 0.9933701657458563, 0.6666666666666666, 0.6474949753644159, 0.5969582577356871, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.567839], dtype=float32), 1.3093345]. 
=============================================
[2019-04-04 10:34:54,367] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256000, global step 4091922: loss 0.0215
[2019-04-04 10:34:54,368] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256000, global step 4091922: learning rate 0.0000
[2019-04-04 10:34:55,664] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6286012e-13 5.8801076e-13 4.3569567e-25 3.1396645e-14 2.6407480e-14
 2.6550076e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:34:55,664] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0238
[2019-04-04 10:34:55,770] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.266666666666667, 87.0, 0.0, 0.0, 26.0, 24.46073662125342, 0.1745799943887899, 0.0, 1.0, 40766.81706955573], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 538800.0000, 
sim time next is 539400.0000, 
raw observation next is [1.183333333333333, 87.5, 0.0, 0.0, 26.0, 24.50708343904559, 0.1729124686910905, 0.0, 1.0, 40767.4282110385], 
processed observation next is [0.0, 0.21739130434782608, 0.49538319482917825, 0.875, 0.0, 0.0, 0.6666666666666666, 0.5422569532537992, 0.5576374895636969, 0.0, 1.0, 0.19413061052875477], 
reward next is 0.8059, 
noisyNet noise sample is [array([0.04027919], dtype=float32), -0.23965044]. 
=============================================
[2019-04-04 10:34:56,483] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256000, global step 4092319: loss 0.0169
[2019-04-04 10:34:56,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256000, global step 4092319: learning rate 0.0000
[2019-04-04 10:34:56,561] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256000, global step 4092333: loss 0.0198
[2019-04-04 10:34:56,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256000, global step 4092333: learning rate 0.0000
[2019-04-04 10:35:04,065] A3C_AGENT_WORKER-Thread-14 INFO:Local step 255500, global step 4093551: loss 13.8284
[2019-04-04 10:35:04,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 255500, global step 4093551: learning rate 0.0000
[2019-04-04 10:35:07,144] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4567299e-15 2.2384748e-13 1.2550985e-24 2.9678370e-15 1.7737455e-15
 3.1744280e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:35:07,144] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9896
[2019-04-04 10:35:07,326] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.1, 55.5, 58.0, 764.0, 26.0, 25.72599114830975, 0.3223382364971495, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 387000.0000, 
sim time next is 387600.0000, 
raw observation next is [-13.0, 54.0, 58.0, 787.5, 26.0, 25.68199481463215, 0.3393456177013119, 1.0, 1.0, 200021.6284135398], 
processed observation next is [1.0, 0.4782608695652174, 0.10249307479224376, 0.54, 0.19333333333333333, 0.8701657458563536, 0.6666666666666666, 0.6401662345526793, 0.6131152059004373, 1.0, 1.0, 0.95248394482638], 
reward next is 0.0475, 
noisyNet noise sample is [array([-1.2160931], dtype=float32), -1.4554596]. 
=============================================
[2019-04-04 10:35:08,225] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256000, global step 4094243: loss 0.0180
[2019-04-04 10:35:08,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256000, global step 4094243: learning rate 0.0000
[2019-04-04 10:35:12,464] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256000, global step 4094912: loss 0.0068
[2019-04-04 10:35:12,464] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256000, global step 4094912: learning rate 0.0000
[2019-04-04 10:35:15,509] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256000, global step 4095565: loss 0.0138
[2019-04-04 10:35:15,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256000, global step 4095565: learning rate 0.0000
[2019-04-04 10:35:16,120] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256000, global step 4095666: loss 0.0134
[2019-04-04 10:35:16,122] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256000, global step 4095667: learning rate 0.0000
[2019-04-04 10:35:21,869] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256000, global step 4096691: loss 0.0099
[2019-04-04 10:35:21,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256000, global step 4096691: learning rate 0.0000
[2019-04-04 10:35:22,149] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256000, global step 4096732: loss 0.0111
[2019-04-04 10:35:22,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256000, global step 4096732: learning rate 0.0000
[2019-04-04 10:35:23,303] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256000, global step 4096943: loss 0.0124
[2019-04-04 10:35:23,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256000, global step 4096943: learning rate 0.0000
[2019-04-04 10:35:23,564] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256000, global step 4097002: loss 0.0108
[2019-04-04 10:35:23,564] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256000, global step 4097002: learning rate 0.0000
[2019-04-04 10:35:23,943] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256000, global step 4097087: loss 0.0071
[2019-04-04 10:35:23,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256000, global step 4097087: learning rate 0.0000
[2019-04-04 10:35:24,123] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256000, global step 4097120: loss 0.0081
[2019-04-04 10:35:24,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256000, global step 4097120: learning rate 0.0000
[2019-04-04 10:35:25,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6003996e-14 1.1623631e-12 9.7178335e-25 6.0681796e-15 1.7775938e-14
 1.2954834e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:35:25,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9307
[2019-04-04 10:35:25,229] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 79.66666666666667, 0.0, 0.0, 26.0, 24.790060548924, 0.248015909947488, 0.0, 1.0, 41164.91678702298], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 859800.0000, 
sim time next is 860400.0000, 
raw observation next is [-2.8, 79.0, 0.0, 0.0, 26.0, 24.79141964176652, 0.241999522460024, 0.0, 1.0, 41034.75110482295], 
processed observation next is [1.0, 1.0, 0.38504155124653744, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5659516368138767, 0.5806665074866747, 0.0, 1.0, 0.1954035766896331], 
reward next is 0.8046, 
noisyNet noise sample is [array([0.9784412], dtype=float32), 0.5143218]. 
=============================================
[2019-04-04 10:35:26,619] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256000, global step 4097676: loss 0.0104
[2019-04-04 10:35:26,619] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256000, global step 4097676: learning rate 0.0000
[2019-04-04 10:35:34,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7316631e-15 4.7056136e-14 2.8869644e-26 1.1320381e-15 1.2149675e-15
 1.8886076e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:35:34,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8232
[2019-04-04 10:35:34,526] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 100.0, 0.0, 0.0, 26.0, 24.63065127276424, 0.287322960705285, 1.0, 1.0, 87177.60496059034], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 936000.0000, 
sim time next is 936600.0000, 
raw observation next is [5.0, 100.0, 0.0, 0.0, 26.0, 24.64845458987838, 0.2953339551923961, 0.0, 1.0, 47947.87873243982], 
processed observation next is [1.0, 0.8695652173913043, 0.6011080332409973, 1.0, 0.0, 0.0, 0.6666666666666666, 0.554037882489865, 0.5984446517307987, 0.0, 1.0, 0.22832323205923724], 
reward next is 0.7717, 
noisyNet noise sample is [array([0.79527885], dtype=float32), -1.466526]. 
=============================================
[2019-04-04 10:35:35,606] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256000, global step 4099489: loss 0.0055
[2019-04-04 10:35:35,606] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256000, global step 4099489: learning rate 0.0000
[2019-04-04 10:35:37,417] A3C_AGENT_WORKER-Thread-5 INFO:Local step 256500, global step 4099915: loss 0.0045
[2019-04-04 10:35:37,419] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 256500, global step 4099915: learning rate 0.0000
[2019-04-04 10:35:37,790] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 10:35:37,792] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:35:37,797] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:37,801] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:35:37,801] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:37,802] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:35:37,803] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:35:37,805] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run42
[2019-04-04 10:35:37,884] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run42
[2019-04-04 10:35:37,922] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run42
[2019-04-04 10:36:44,174] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16092536], dtype=float32), -0.27474254]
[2019-04-04 10:36:44,175] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [15.29973714166667, 81.62660324666666, 0.0, 0.0, 26.0, 24.97920854268764, 0.3869958192991089, 0.0, 0.0, 0.0]
[2019-04-04 10:36:44,175] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:36:44,176] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.0858801e-13 1.2488539e-13 5.1951933e-26 3.7692272e-15 7.4296724e-15
 1.0826104e-17 1.0000000e+00], sampled 0.42465008463979825
[2019-04-04 10:37:02,703] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16092536], dtype=float32), -0.27474254]
[2019-04-04 10:37:02,703] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [14.15, 67.83333333333334, 0.0, 0.0, 26.0, 26.18229770820121, 0.6968131106212595, 0.0, 1.0, 0.0]
[2019-04-04 10:37:02,703] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:37:02,704] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.8015066e-15 4.1372098e-14 2.5626004e-27 4.0911898e-16 1.0722092e-15
 1.6203614e-18 1.0000000e+00], sampled 0.6654674177986115
[2019-04-04 10:37:29,118] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16092536], dtype=float32), -0.27474254]
[2019-04-04 10:37:29,118] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 24.49793332647902, 0.1320579434254893, 0.0, 1.0, 42595.02810495335]
[2019-04-04 10:37:29,118] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:37:29,119] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.0984172e-13 7.6828534e-13 7.6458509e-25 1.6328398e-14 2.2338163e-14
 1.0455866e-16 1.0000000e+00], sampled 0.0508173154787227
[2019-04-04 10:38:50,285] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:39:19,964] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16092536], dtype=float32), -0.27474254]
[2019-04-04 10:39:19,964] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.310021675666666, 44.90769166, 0.0, 0.0, 26.0, 24.79498673651188, 0.2094404487589777, 0.0, 1.0, 38091.5756546464]
[2019-04-04 10:39:19,965] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:39:19,965] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [4.2988697e-13 1.8368302e-12 2.1813846e-24 4.4941891e-14 4.5734331e-14
 2.8277832e-16 1.0000000e+00], sampled 0.8878367187480432
[2019-04-04 10:39:21,205] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:39:23,791] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:39:24,826] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4100000, evaluation results [4100000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:39:25,207] A3C_AGENT_WORKER-Thread-12 INFO:Local step 256500, global step 4100096: loss 0.0032
[2019-04-04 10:39:25,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 256500, global step 4100096: learning rate 0.0000
[2019-04-04 10:39:25,486] A3C_AGENT_WORKER-Thread-2 INFO:Local step 256500, global step 4100181: loss 0.0017
[2019-04-04 10:39:25,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 256500, global step 4100181: learning rate 0.0000
[2019-04-04 10:39:32,184] A3C_AGENT_WORKER-Thread-18 INFO:Local step 256500, global step 4101878: loss 0.0057
[2019-04-04 10:39:32,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 256500, global step 4101878: learning rate 0.0000
[2019-04-04 10:39:33,292] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256000, global step 4102151: loss 0.0066
[2019-04-04 10:39:33,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256000, global step 4102151: learning rate 0.0000
[2019-04-04 10:39:34,835] A3C_AGENT_WORKER-Thread-20 INFO:Local step 256500, global step 4102637: loss 0.0094
[2019-04-04 10:39:34,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 256500, global step 4102637: learning rate 0.0000
[2019-04-04 10:39:37,295] A3C_AGENT_WORKER-Thread-6 INFO:Local step 256500, global step 4103424: loss 0.0115
[2019-04-04 10:39:37,296] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 256500, global step 4103424: learning rate 0.0000
[2019-04-04 10:39:37,658] A3C_AGENT_WORKER-Thread-3 INFO:Local step 256500, global step 4103519: loss 0.0116
[2019-04-04 10:39:37,706] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 256500, global step 4103519: learning rate 0.0000
[2019-04-04 10:39:42,131] A3C_AGENT_WORKER-Thread-16 INFO:Local step 256500, global step 4104849: loss 0.0362
[2019-04-04 10:39:42,152] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 256500, global step 4104851: learning rate 0.0000
[2019-04-04 10:39:42,607] A3C_AGENT_WORKER-Thread-13 INFO:Local step 256500, global step 4104996: loss 0.0346
[2019-04-04 10:39:42,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 256500, global step 4105000: learning rate 0.0000
[2019-04-04 10:39:44,900] A3C_AGENT_WORKER-Thread-17 INFO:Local step 256500, global step 4105784: loss 0.0319
[2019-04-04 10:39:44,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 256500, global step 4105784: learning rate 0.0000
[2019-04-04 10:39:44,909] A3C_AGENT_WORKER-Thread-4 INFO:Local step 256500, global step 4105787: loss 0.0284
[2019-04-04 10:39:44,909] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 256500, global step 4105787: learning rate 0.0000
[2019-04-04 10:39:45,079] A3C_AGENT_WORKER-Thread-11 INFO:Local step 256500, global step 4105860: loss 0.0252
[2019-04-04 10:39:45,080] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 256500, global step 4105860: learning rate 0.0000
[2019-04-04 10:39:45,635] A3C_AGENT_WORKER-Thread-15 INFO:Local step 256500, global step 4106053: loss 0.0254
[2019-04-04 10:39:45,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 256500, global step 4106053: learning rate 0.0000
[2019-04-04 10:39:45,867] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257000, global step 4106110: loss 0.5714
[2019-04-04 10:39:45,878] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257000, global step 4106111: learning rate 0.0000
[2019-04-04 10:39:46,021] A3C_AGENT_WORKER-Thread-19 INFO:Local step 256500, global step 4106154: loss 0.0166
[2019-04-04 10:39:46,024] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 256500, global step 4106154: learning rate 0.0000
[2019-04-04 10:39:46,264] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257000, global step 4106232: loss 0.5773
[2019-04-04 10:39:46,267] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257000, global step 4106233: learning rate 0.0000
[2019-04-04 10:39:47,106] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257000, global step 4106524: loss 0.6441
[2019-04-04 10:39:47,107] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257000, global step 4106524: learning rate 0.0000
[2019-04-04 10:39:47,711] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.96033270e-14 1.06099286e-13 3.51378215e-26 1.25977146e-15
 5.91577339e-15 2.18635764e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 10:39:47,731] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3438
[2019-04-04 10:39:47,753] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.833333333333334, 96.0, 0.0, 0.0, 26.0, 24.71221966974382, 0.462886254669425, 0.0, 1.0, 198762.5005953572], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1279200.0000, 
sim time next is 1279800.0000, 
raw observation next is [6.65, 96.0, 0.0, 0.0, 26.0, 24.72617691571089, 0.4936731095863097, 0.0, 1.0, 169270.513014879], 
processed observation next is [0.0, 0.8260869565217391, 0.6468144044321331, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5605147429759075, 0.6645577031954365, 0.0, 1.0, 0.8060500619756142], 
reward next is 0.1939, 
noisyNet noise sample is [array([0.2102251], dtype=float32), 0.00997246]. 
=============================================
[2019-04-04 10:39:50,981] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257000, global step 4108724: loss 0.6331
[2019-04-04 10:39:50,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257000, global step 4108724: learning rate 0.0000
[2019-04-04 10:39:51,519] A3C_AGENT_WORKER-Thread-10 INFO:Local step 256500, global step 4109030: loss 0.0217
[2019-04-04 10:39:51,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 256500, global step 4109031: learning rate 0.0000
[2019-04-04 10:39:52,596] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257000, global step 4109641: loss 0.6639
[2019-04-04 10:39:52,597] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257000, global step 4109642: learning rate 0.0000
[2019-04-04 10:39:53,934] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.9465614e-15 2.1410489e-14 9.9321536e-28 2.3398830e-16 1.2439547e-16
 5.6433084e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:39:53,936] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0790
[2019-04-04 10:39:53,971] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.8, 92.0, 18.0, 0.0, 26.0, 25.72109666919557, 0.5770021191048572, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1326600.0000, 
sim time next is 1327200.0000, 
raw observation next is [0.7000000000000001, 92.0, 22.5, 0.0, 26.0, 25.95731973512394, 0.5913353779505082, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4819944598337951, 0.92, 0.075, 0.0, 0.6666666666666666, 0.6631099779269949, 0.6971117926501694, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.37792578], dtype=float32), 0.298664]. 
=============================================
[2019-04-04 10:39:54,172] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257000, global step 4110588: loss 0.6933
[2019-04-04 10:39:54,173] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257000, global step 4110588: learning rate 0.0000
[2019-04-04 10:39:54,374] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257000, global step 4110700: loss 0.7090
[2019-04-04 10:39:54,376] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257000, global step 4110700: learning rate 0.0000
[2019-04-04 10:39:56,336] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257000, global step 4111850: loss 0.8022
[2019-04-04 10:39:56,339] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257000, global step 4111852: learning rate 0.0000
[2019-04-04 10:39:57,066] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257000, global step 4112246: loss 0.8414
[2019-04-04 10:39:57,069] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257000, global step 4112246: learning rate 0.0000
[2019-04-04 10:39:58,173] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257000, global step 4112764: loss 0.7679
[2019-04-04 10:39:58,178] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257000, global step 4112764: learning rate 0.0000
[2019-04-04 10:39:58,297] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257000, global step 4112813: loss 0.7597
[2019-04-04 10:39:58,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257000, global step 4112814: learning rate 0.0000
[2019-04-04 10:39:58,570] A3C_AGENT_WORKER-Thread-14 INFO:Local step 256500, global step 4112960: loss 0.0080
[2019-04-04 10:39:58,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 256500, global step 4112960: learning rate 0.0000
[2019-04-04 10:39:58,836] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257000, global step 4113100: loss 0.6611
[2019-04-04 10:39:58,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257000, global step 4113101: learning rate 0.0000
[2019-04-04 10:39:59,174] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257000, global step 4113259: loss 0.6736
[2019-04-04 10:39:59,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257000, global step 4113259: learning rate 0.0000
[2019-04-04 10:39:59,282] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257000, global step 4113305: loss 0.6758
[2019-04-04 10:39:59,284] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257000, global step 4113306: learning rate 0.0000
[2019-04-04 10:40:01,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1487734e-15 3.8248342e-14 2.6208212e-26 8.3686146e-16 2.4634085e-16
 7.3397142e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:01,326] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4537
[2019-04-04 10:40:01,334] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 109.5, 0.0, 26.0, 25.73038716062585, 0.5138884770530555, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [1.1, 92.0, 108.3333333333333, 0.0, 26.0, 25.65655686682818, 0.5199617862255391, 1.0, 1.0, 70812.03318335957], 
processed observation next is [1.0, 0.5652173913043478, 0.49307479224376743, 0.92, 0.361111111111111, 0.0, 0.6666666666666666, 0.6380464055690149, 0.6733205954085131, 1.0, 1.0, 0.33720015801599795], 
reward next is 0.6628, 
noisyNet noise sample is [array([0.31026664], dtype=float32), -0.0539757]. 
=============================================
[2019-04-04 10:40:03,412] A3C_AGENT_WORKER-Thread-5 INFO:Local step 257500, global step 4115253: loss 0.0191
[2019-04-04 10:40:03,413] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 257500, global step 4115253: learning rate 0.0000
[2019-04-04 10:40:04,084] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257000, global step 4115575: loss 0.6718
[2019-04-04 10:40:04,104] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257000, global step 4115581: learning rate 0.0000
[2019-04-04 10:40:04,482] A3C_AGENT_WORKER-Thread-2 INFO:Local step 257500, global step 4115723: loss 0.0121
[2019-04-04 10:40:04,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 257500, global step 4115723: learning rate 0.0000
[2019-04-04 10:40:04,509] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4147103e-14 7.7772681e-14 4.7581477e-26 2.2383019e-15 2.8517685e-15
 8.8255324e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:04,509] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5088
[2019-04-04 10:40:04,532] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.666666666666667, 84.33333333333334, 0.0, 0.0, 26.0, 25.48154033150686, 0.4936896145746226, 0.0, 1.0, 67739.4122642359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1572000.0000, 
sim time next is 1572600.0000, 
raw observation next is [4.683333333333334, 84.16666666666666, 0.0, 0.0, 26.0, 25.41137051689944, 0.4989810685417327, 0.0, 1.0, 87013.8486810967], 
processed observation next is [1.0, 0.17391304347826086, 0.5923361034164359, 0.8416666666666666, 0.0, 0.0, 0.6666666666666666, 0.6176142097416198, 0.6663270228472442, 0.0, 1.0, 0.41435166038617477], 
reward next is 0.5856, 
noisyNet noise sample is [array([-0.84421206], dtype=float32), 2.6035597]. 
=============================================
[2019-04-04 10:40:04,570] A3C_AGENT_WORKER-Thread-12 INFO:Local step 257500, global step 4115755: loss 0.0121
[2019-04-04 10:40:04,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 257500, global step 4115755: learning rate 0.0000
[2019-04-04 10:40:08,366] A3C_AGENT_WORKER-Thread-18 INFO:Local step 257500, global step 4117497: loss 0.0125
[2019-04-04 10:40:08,370] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 257500, global step 4117499: learning rate 0.0000
[2019-04-04 10:40:09,938] A3C_AGENT_WORKER-Thread-20 INFO:Local step 257500, global step 4118221: loss 0.0116
[2019-04-04 10:40:09,942] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 257500, global step 4118223: learning rate 0.0000
[2019-04-04 10:40:11,802] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257000, global step 4119029: loss 0.6470
[2019-04-04 10:40:11,803] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257000, global step 4119029: learning rate 0.0000
[2019-04-04 10:40:11,927] A3C_AGENT_WORKER-Thread-6 INFO:Local step 257500, global step 4119091: loss 0.0105
[2019-04-04 10:40:11,927] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 257500, global step 4119091: learning rate 0.0000
[2019-04-04 10:40:11,965] A3C_AGENT_WORKER-Thread-3 INFO:Local step 257500, global step 4119113: loss 0.0115
[2019-04-04 10:40:11,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 257500, global step 4119114: learning rate 0.0000
[2019-04-04 10:40:14,498] A3C_AGENT_WORKER-Thread-16 INFO:Local step 257500, global step 4120330: loss 0.0060
[2019-04-04 10:40:14,498] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 257500, global step 4120330: learning rate 0.0000
[2019-04-04 10:40:14,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2434666e-15 2.6733751e-14 1.7598516e-26 2.4184911e-16 5.7168133e-16
 5.3441418e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:14,544] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3786
[2019-04-04 10:40:14,572] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 72.0, 0.0, 26.0, 25.53236003774968, 0.4925055935508838, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1434600.0000, 
sim time next is 1435200.0000, 
raw observation next is [1.1, 92.0, 67.66666666666667, 0.0, 26.0, 25.8153467485185, 0.5192126955759034, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22555555555555556, 0.0, 0.6666666666666666, 0.6512788957098751, 0.6730708985253012, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27409902], dtype=float32), 0.48158005]. 
=============================================
[2019-04-04 10:40:15,225] A3C_AGENT_WORKER-Thread-13 INFO:Local step 257500, global step 4120673: loss 0.0049
[2019-04-04 10:40:15,227] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 257500, global step 4120673: learning rate 0.0000
[2019-04-04 10:40:16,237] A3C_AGENT_WORKER-Thread-17 INFO:Local step 257500, global step 4121119: loss 0.0060
[2019-04-04 10:40:16,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 257500, global step 4121120: learning rate 0.0000
[2019-04-04 10:40:16,281] A3C_AGENT_WORKER-Thread-4 INFO:Local step 257500, global step 4121141: loss 0.0051
[2019-04-04 10:40:16,282] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 257500, global step 4121141: learning rate 0.0000
[2019-04-04 10:40:16,932] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7871246e-14 5.4048823e-13 7.8332267e-26 1.9350281e-15 1.0793874e-14
 1.0043884e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:16,936] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0974
[2019-04-04 10:40:16,959] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 79.0, 0.0, 0.0, 26.0, 25.43431573142675, 0.5186322872518464, 0.0, 1.0, 127073.1683618299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1562400.0000, 
sim time next is 1563000.0000, 
raw observation next is [4.9, 80.16666666666667, 0.0, 0.0, 26.0, 25.42491536928962, 0.5362061970372428, 0.0, 1.0, 82056.18959140136], 
processed observation next is [1.0, 0.08695652173913043, 0.5983379501385043, 0.8016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6187429474408018, 0.6787353990124143, 0.0, 1.0, 0.3907437599590541], 
reward next is 0.6093, 
noisyNet noise sample is [array([-0.26827183], dtype=float32), -1.0202982]. 
=============================================
[2019-04-04 10:40:16,992] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[85.458405]
 [85.05651 ]
 [84.86182 ]
 [84.79588 ]
 [84.756195]], R is [[85.51313782]
 [85.05289459]
 [84.85929871]
 [84.96612549]
 [85.11646271]].
[2019-04-04 10:40:17,139] A3C_AGENT_WORKER-Thread-11 INFO:Local step 257500, global step 4121507: loss 0.0062
[2019-04-04 10:40:17,141] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 257500, global step 4121508: learning rate 0.0000
[2019-04-04 10:40:17,494] A3C_AGENT_WORKER-Thread-15 INFO:Local step 257500, global step 4121656: loss 0.0088
[2019-04-04 10:40:17,502] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 257500, global step 4121656: learning rate 0.0000
[2019-04-04 10:40:17,744] A3C_AGENT_WORKER-Thread-19 INFO:Local step 257500, global step 4121764: loss 0.0086
[2019-04-04 10:40:17,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 257500, global step 4121765: learning rate 0.0000
[2019-04-04 10:40:20,988] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.3136717e-14 1.7147897e-13 2.1412493e-25 2.9143668e-15 1.0120275e-14
 7.6977659e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:20,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1792
[2019-04-04 10:40:21,034] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.9, 84.33333333333334, 58.5, 0.0, 26.0, 25.06563496839059, 0.3261598340402398, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1761600.0000, 
sim time next is 1762200.0000, 
raw observation next is [-2.0, 85.0, 65.0, 0.0, 26.0, 24.97069762139525, 0.3100581199270108, 0.0, 1.0, 50268.82598940083], 
processed observation next is [0.0, 0.391304347826087, 0.40720221606648205, 0.85, 0.21666666666666667, 0.0, 0.6666666666666666, 0.5808914684496042, 0.6033527066423369, 0.0, 1.0, 0.23937536185428968], 
reward next is 0.7606, 
noisyNet noise sample is [array([-1.9085225], dtype=float32), 0.3369342]. 
=============================================
[2019-04-04 10:40:22,547] A3C_AGENT_WORKER-Thread-10 INFO:Local step 257500, global step 4123853: loss 0.0043
[2019-04-04 10:40:22,548] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 257500, global step 4123853: learning rate 0.0000
[2019-04-04 10:40:23,663] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258000, global step 4124165: loss 0.1950
[2019-04-04 10:40:23,664] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258000, global step 4124165: learning rate 0.0000
[2019-04-04 10:40:23,979] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [4.4643659e-13 1.0201528e-12 2.1342291e-24 1.3360623e-14 5.7046957e-14
 1.2211375e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:23,980] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9690
[2019-04-04 10:40:24,068] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 84.33333333333333, 120.1666666666667, 0.0, 26.0, 24.94177527990449, 0.3475148469689016, 0.0, 1.0, 30472.78054717026], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1770000.0000, 
sim time next is 1770600.0000, 
raw observation next is [-2.3, 83.66666666666667, 121.3333333333333, 0.0, 26.0, 24.94728024030015, 0.3460635989648097, 0.0, 1.0, 34110.19580089922], 
processed observation next is [0.0, 0.4782608695652174, 0.3988919667590028, 0.8366666666666667, 0.40444444444444433, 0.0, 0.6666666666666666, 0.5789400200250124, 0.6153545329882699, 0.0, 1.0, 0.16242950381380583], 
reward next is 0.8376, 
noisyNet noise sample is [array([-0.6937312], dtype=float32), 1.5949731]. 
=============================================
[2019-04-04 10:40:25,250] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258000, global step 4124674: loss 0.2221
[2019-04-04 10:40:25,251] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258000, global step 4124674: learning rate 0.0000
[2019-04-04 10:40:25,739] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258000, global step 4124836: loss 0.2209
[2019-04-04 10:40:25,741] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258000, global step 4124836: learning rate 0.0000
[2019-04-04 10:40:29,719] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258000, global step 4126201: loss 0.2946
[2019-04-04 10:40:29,719] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258000, global step 4126201: learning rate 0.0000
[2019-04-04 10:40:30,133] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.56666275e-13 9.20703509e-13 1.12204625e-23 3.81271383e-14
 4.42733280e-14 5.22136573e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:40:30,134] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0849
[2019-04-04 10:40:30,196] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 83.0, 119.0, 0.0, 26.0, 24.96439456273545, 0.3463579267881324, 0.0, 1.0, 54828.90168155248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1776600.0000, 
sim time next is 1777200.0000, 
raw observation next is [-2.8, 83.0, 115.6666666666667, 0.0, 26.0, 24.96167245328473, 0.3575402757344004, 0.0, 1.0, 49751.14215381105], 
processed observation next is [0.0, 0.5652173913043478, 0.38504155124653744, 0.83, 0.38555555555555565, 0.0, 0.6666666666666666, 0.5801393711070609, 0.6191800919114668, 0.0, 1.0, 0.23691020073243357], 
reward next is 0.7631, 
noisyNet noise sample is [array([-1.4762447], dtype=float32), -0.7135267]. 
=============================================
[2019-04-04 10:40:30,292] A3C_AGENT_WORKER-Thread-14 INFO:Local step 257500, global step 4126329: loss 0.0077
[2019-04-04 10:40:30,293] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 257500, global step 4126329: learning rate 0.0000
[2019-04-04 10:40:31,802] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258000, global step 4126673: loss 0.3199
[2019-04-04 10:40:31,803] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258000, global step 4126673: learning rate 0.0000
[2019-04-04 10:40:32,870] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.1026783e-13 1.0530298e-12 3.1435810e-23 5.6055187e-14 2.1699549e-14
 3.1778101e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:32,871] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9569
[2019-04-04 10:40:32,984] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 71.0, 172.6666666666667, 47.33333333333333, 26.0, 24.99952571132514, 0.2743328760988967, 0.0, 1.0, 39411.09071324031], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1863600.0000, 
sim time next is 1864200.0000, 
raw observation next is [-4.5, 71.0, 175.3333333333333, 54.66666666666666, 26.0, 25.00368421020111, 0.2798865341552835, 0.0, 1.0, 40262.60520950586], 
processed observation next is [0.0, 0.5652173913043478, 0.3379501385041552, 0.71, 0.5844444444444443, 0.06040515653775321, 0.6666666666666666, 0.5836403508500926, 0.5932955113850945, 0.0, 1.0, 0.19172669147383745], 
reward next is 0.8083, 
noisyNet noise sample is [array([-0.8602666], dtype=float32), 0.6810286]. 
=============================================
[2019-04-04 10:40:33,578] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258000, global step 4127219: loss 0.2988
[2019-04-04 10:40:33,579] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258000, global step 4127219: learning rate 0.0000
[2019-04-04 10:40:33,832] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258000, global step 4127308: loss 0.2977
[2019-04-04 10:40:33,834] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258000, global step 4127309: learning rate 0.0000
[2019-04-04 10:40:36,874] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258000, global step 4128345: loss 0.3115
[2019-04-04 10:40:36,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258000, global step 4128345: learning rate 0.0000
[2019-04-04 10:40:37,370] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2709605e-13 6.3957748e-13 3.9696601e-24 5.3393030e-14 1.8960394e-14
 1.4657703e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:37,370] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6886
[2019-04-04 10:40:37,462] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 73.66666666666667, 173.3333333333333, 76.0, 26.0, 24.95655110288769, 0.2530117806990611, 0.0, 1.0, 51316.92368706244], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1855200.0000, 
sim time next is 1855800.0000, 
raw observation next is [-5.3, 73.0, 184.0, 81.0, 26.0, 24.94890292938419, 0.2594565638724239, 0.0, 1.0, 53024.40544159035], 
processed observation next is [0.0, 0.4782608695652174, 0.31578947368421056, 0.73, 0.6133333333333333, 0.08950276243093923, 0.6666666666666666, 0.5790752441153492, 0.586485521290808, 0.0, 1.0, 0.25249716876947786], 
reward next is 0.7475, 
noisyNet noise sample is [array([-0.3152087], dtype=float32), 0.3938134]. 
=============================================
[2019-04-04 10:40:38,384] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258000, global step 4128705: loss 0.3155
[2019-04-04 10:40:38,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258000, global step 4128705: learning rate 0.0000
[2019-04-04 10:40:39,247] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258000, global step 4128889: loss 0.2813
[2019-04-04 10:40:39,247] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258000, global step 4128889: learning rate 0.0000
[2019-04-04 10:40:39,749] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258000, global step 4129002: loss 0.2823
[2019-04-04 10:40:39,749] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258000, global step 4129002: learning rate 0.0000
[2019-04-04 10:40:40,131] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258000, global step 4129097: loss 0.2718
[2019-04-04 10:40:40,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258000, global step 4129097: learning rate 0.0000
[2019-04-04 10:40:40,265] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258000, global step 4129129: loss 0.2786
[2019-04-04 10:40:40,268] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258000, global step 4129130: learning rate 0.0000
[2019-04-04 10:40:40,897] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258000, global step 4129306: loss 0.2878
[2019-04-04 10:40:40,899] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258000, global step 4129306: learning rate 0.0000
[2019-04-04 10:40:43,092] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6117280e-14 1.5759051e-12 7.0963639e-24 1.0057390e-14 1.5185466e-14
 3.2559878e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:43,098] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5737
[2019-04-04 10:40:43,165] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.91251592992491, 0.3660793151746995, 0.0, 1.0, 152091.1017079971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2059200.0000, 
sim time next is 2059800.0000, 
raw observation next is [-3.9, 86.0, 0.0, 0.0, 26.0, 24.98741234552878, 0.3865238957319475, 0.0, 1.0, 47404.32376442265], 
processed observation next is [1.0, 0.8695652173913043, 0.3545706371191136, 0.86, 0.0, 0.0, 0.6666666666666666, 0.5822843621273984, 0.6288412985773159, 0.0, 1.0, 0.22573487506867926], 
reward next is 0.7743, 
noisyNet noise sample is [array([0.42160696], dtype=float32), 0.044071205]. 
=============================================
[2019-04-04 10:40:46,349] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258000, global step 4130970: loss 0.3141
[2019-04-04 10:40:46,352] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258000, global step 4130970: learning rate 0.0000
[2019-04-04 10:40:47,662] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1910466e-13 7.8189186e-13 1.6709888e-24 9.6709165e-15 1.9644220e-14
 5.1835769e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:47,662] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4406
[2019-04-04 10:40:47,719] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.00497737990092, 0.3234504061825144, 0.0, 1.0, 46568.77504752864], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1790400.0000, 
sim time next is 1791000.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.0017997077469, 0.3222421425672485, 0.0, 1.0, 47178.55048034742], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5834833089789084, 0.6074140475224161, 0.0, 1.0, 0.22465976419213057], 
reward next is 0.7753, 
noisyNet noise sample is [array([-0.02727478], dtype=float32), -1.5768002]. 
=============================================
[2019-04-04 10:40:47,745] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.4877  ]
 [77.6746  ]
 [77.73343 ]
 [77.78004 ]
 [77.830246]], R is [[77.52841949]
 [77.5313797 ]
 [77.54108429]
 [77.56047058]
 [77.5969162 ]].
[2019-04-04 10:40:50,864] A3C_AGENT_WORKER-Thread-5 INFO:Local step 258500, global step 4132229: loss 0.0019
[2019-04-04 10:40:50,864] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 258500, global step 4132229: learning rate 0.0000
[2019-04-04 10:40:53,166] A3C_AGENT_WORKER-Thread-12 INFO:Local step 258500, global step 4132931: loss 0.0009
[2019-04-04 10:40:53,167] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 258500, global step 4132931: learning rate 0.0000
[2019-04-04 10:40:53,659] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5284428e-15 8.9998931e-14 6.8499327e-25 2.0315778e-15 8.7337414e-16
 5.3706101e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:40:53,661] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0890
[2019-04-04 10:40:53,724] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 62.0, 66.66666666666667, 0.0, 26.0, 25.58891997272728, 0.3250823317922544, 1.0, 1.0, 29363.60110550172], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1957200.0000, 
sim time next is 1957800.0000, 
raw observation next is [-2.8, 62.0, 59.33333333333334, 0.0, 26.0, 25.60666529554123, 0.3199389932160164, 1.0, 1.0, 27034.54183478788], 
processed observation next is [1.0, 0.6521739130434783, 0.38504155124653744, 0.62, 0.1977777777777778, 0.0, 0.6666666666666666, 0.6338887746284358, 0.6066463310720055, 1.0, 1.0, 0.1287359134989899], 
reward next is 0.8713, 
noisyNet noise sample is [array([-0.7004905], dtype=float32), -0.82797515]. 
=============================================
[2019-04-04 10:40:54,157] A3C_AGENT_WORKER-Thread-2 INFO:Local step 258500, global step 4133190: loss 0.0006
[2019-04-04 10:40:54,158] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 258500, global step 4133190: learning rate 0.0000
[2019-04-04 10:40:54,765] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258000, global step 4133329: loss 0.2979
[2019-04-04 10:40:54,766] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258000, global step 4133329: learning rate 0.0000
[2019-04-04 10:40:57,713] A3C_AGENT_WORKER-Thread-18 INFO:Local step 258500, global step 4134064: loss 0.0003
[2019-04-04 10:40:57,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 258500, global step 4134064: learning rate 0.0000
[2019-04-04 10:40:59,179] A3C_AGENT_WORKER-Thread-20 INFO:Local step 258500, global step 4134554: loss 0.0003
[2019-04-04 10:40:59,180] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 258500, global step 4134554: learning rate 0.0000
[2019-04-04 10:41:00,032] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.9119500e-15 5.1120668e-14 1.1705182e-25 7.9128844e-16 7.4807165e-16
 7.3726302e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:00,033] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7195
[2019-04-04 10:41:00,125] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 82.0, 191.0, 89.0, 26.0, 25.84290754779257, 0.4027765732773818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2109600.0000, 
sim time next is 2110200.0000, 
raw observation next is [-7.716666666666667, 80.83333333333334, 196.6666666666667, 79.33333333333333, 26.0, 25.87500907578895, 0.3994172016371669, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.24884579870729456, 0.8083333333333335, 0.6555555555555557, 0.0876611418047882, 0.6666666666666666, 0.6562507563157457, 0.633139067212389, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.4522855], dtype=float32), 0.52818555]. 
=============================================
[2019-04-04 10:41:00,481] A3C_AGENT_WORKER-Thread-6 INFO:Local step 258500, global step 4134956: loss 0.0012
[2019-04-04 10:41:00,482] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 258500, global step 4134956: learning rate 0.0000
[2019-04-04 10:41:02,823] A3C_AGENT_WORKER-Thread-3 INFO:Local step 258500, global step 4135566: loss 0.0008
[2019-04-04 10:41:02,825] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 258500, global step 4135566: learning rate 0.0000
[2019-04-04 10:41:05,411] A3C_AGENT_WORKER-Thread-16 INFO:Local step 258500, global step 4136185: loss 0.0003
[2019-04-04 10:41:05,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 258500, global step 4136185: learning rate 0.0000
[2019-04-04 10:41:06,171] A3C_AGENT_WORKER-Thread-13 INFO:Local step 258500, global step 4136436: loss 0.0003
[2019-04-04 10:41:06,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 258500, global step 4136436: learning rate 0.0000
[2019-04-04 10:41:07,240] A3C_AGENT_WORKER-Thread-17 INFO:Local step 258500, global step 4136843: loss 0.0039
[2019-04-04 10:41:07,240] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 258500, global step 4136843: learning rate 0.0000
[2019-04-04 10:41:07,425] A3C_AGENT_WORKER-Thread-15 INFO:Local step 258500, global step 4136904: loss 0.0034
[2019-04-04 10:41:07,427] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 258500, global step 4136904: learning rate 0.0000
[2019-04-04 10:41:07,721] A3C_AGENT_WORKER-Thread-4 INFO:Local step 258500, global step 4137003: loss 0.0046
[2019-04-04 10:41:07,724] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 258500, global step 4137003: learning rate 0.0000
[2019-04-04 10:41:08,351] A3C_AGENT_WORKER-Thread-11 INFO:Local step 258500, global step 4137187: loss 0.0017
[2019-04-04 10:41:08,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 258500, global step 4137187: learning rate 0.0000
[2019-04-04 10:41:08,534] A3C_AGENT_WORKER-Thread-19 INFO:Local step 258500, global step 4137240: loss 0.0010
[2019-04-04 10:41:08,535] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 258500, global step 4137240: learning rate 0.0000
[2019-04-04 10:41:11,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1791429e-14 2.4223961e-13 1.4017822e-25 2.3605042e-15 1.7721223e-15
 1.4305594e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:11,518] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7939
[2019-04-04 10:41:11,563] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 66.0, 36.0, 0.0, 26.0, 25.83404012440165, 0.453138989876085, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2132400.0000, 
sim time next is 2133000.0000, 
raw observation next is [-4.5, 66.5, 26.0, 0.0, 26.0, 25.98545423765922, 0.458471065821533, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.3379501385041552, 0.665, 0.08666666666666667, 0.0, 0.6666666666666666, 0.665454519804935, 0.6528236886071777, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5868294], dtype=float32), 0.40338263]. 
=============================================
[2019-04-04 10:41:11,580] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[83.30069]
 [83.45816]
 [83.55566]
 [83.07647]
 [83.10574]], R is [[83.36965942]
 [83.53596497]
 [83.7006073 ]
 [82.91866302]
 [82.72923279]].
[2019-04-04 10:41:12,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.1719288e-15 1.6908170e-14 4.0495564e-26 1.9995309e-16 4.1606290e-16
 2.5111971e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:12,388] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2290
[2019-04-04 10:41:12,463] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.2, 53.33333333333334, 255.1666666666667, 73.16666666666667, 26.0, 25.57908054714893, 0.3275436333564022, 1.0, 1.0, 9349.550104313854], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2292000.0000, 
sim time next is 2292600.0000, 
raw observation next is [-1.95, 52.16666666666666, 248.3333333333333, 72.33333333333333, 26.0, 25.32673803455797, 0.3167552644156448, 1.0, 1.0, 18697.59342053906], 
processed observation next is [1.0, 0.5217391304347826, 0.40858725761772857, 0.5216666666666666, 0.8277777777777776, 0.07992633517495396, 0.6666666666666666, 0.6105615028798308, 0.6055850881385483, 1.0, 1.0, 0.08903615914542409], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.47027013], dtype=float32), -0.70757324]. 
=============================================
[2019-04-04 10:41:13,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4534688e-15 4.2225924e-14 7.3088382e-26 8.9162987e-16 3.7345609e-16
 3.4364871e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:13,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8059
[2019-04-04 10:41:13,513] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.283333333333333, 69.66666666666667, 172.3333333333333, 70.0, 26.0, 25.74319499846047, 0.3495057331877049, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2285400.0000, 
sim time next is 2286000.0000, 
raw observation next is [-5.0, 68.0, 169.5, 80.0, 26.0, 25.74628130103325, 0.3495152671845472, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.32409972299168976, 0.68, 0.565, 0.08839779005524862, 0.6666666666666666, 0.6455234417527708, 0.6165050890615157, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.5599036], dtype=float32), -0.030913476]. 
=============================================
[2019-04-04 10:41:13,543] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.74795 ]
 [85.98126 ]
 [86.11725 ]
 [86.177444]
 [86.11208 ]], R is [[85.71922302]
 [85.86203003]
 [86.00341034]
 [86.14337921]
 [86.28194427]].
[2019-04-04 10:41:14,782] A3C_AGENT_WORKER-Thread-10 INFO:Local step 258500, global step 4138980: loss 0.0191
[2019-04-04 10:41:14,783] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 258500, global step 4138980: learning rate 0.0000
[2019-04-04 10:41:15,932] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.0065492e-14 8.0271347e-13 3.2410792e-24 1.1736277e-14 1.5667475e-14
 1.2775754e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:15,934] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3642
[2019-04-04 10:41:15,968] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 63.0, 0.0, 0.0, 26.0, 24.92131215792089, 0.2976721071974973, 0.0, 1.0, 38520.7256513087], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2331600.0000, 
sim time next is 2332200.0000, 
raw observation next is [-2.3, 64.0, 0.0, 0.0, 26.0, 24.88333670793936, 0.2946870969455991, 0.0, 1.0, 38538.31011026005], 
processed observation next is [1.0, 1.0, 0.3988919667590028, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5736113923282801, 0.5982290323151996, 0.0, 1.0, 0.18351576242980974], 
reward next is 0.8165, 
noisyNet noise sample is [array([-0.50172573], dtype=float32), 1.6459576]. 
=============================================
[2019-04-04 10:41:17,202] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259000, global step 4139766: loss 0.0313
[2019-04-04 10:41:17,204] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259000, global step 4139766: learning rate 0.0000
[2019-04-04 10:41:18,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7829335e-12 5.1852715e-12 3.4116480e-23 3.6554524e-13 3.0231850e-13
 1.1616322e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:18,421] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6248
[2019-04-04 10:41:18,451] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.633333333333333, 64.0, 0.0, 0.0, 26.0, 24.49227781970483, 0.1783296771472555, 0.0, 1.0, 39977.1636319941], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2346000.0000, 
sim time next is 2346600.0000, 
raw observation next is [-2.716666666666667, 64.5, 0.0, 0.0, 26.0, 24.45850954288382, 0.1716241660986596, 0.0, 1.0, 40099.56517510542], 
processed observation next is [0.0, 0.13043478260869565, 0.3873499538319483, 0.645, 0.0, 0.0, 0.6666666666666666, 0.5382091285736518, 0.5572080553662199, 0.0, 1.0, 0.19095031035764484], 
reward next is 0.8090, 
noisyNet noise sample is [array([0.02334099], dtype=float32), -0.7793523]. 
=============================================
[2019-04-04 10:41:21,135] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259000, global step 4140879: loss 0.0199
[2019-04-04 10:41:21,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259000, global step 4140879: learning rate 0.0000
[2019-04-04 10:41:21,641] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259000, global step 4141060: loss 0.0193
[2019-04-04 10:41:21,641] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259000, global step 4141060: learning rate 0.0000
[2019-04-04 10:41:23,133] A3C_AGENT_WORKER-Thread-14 INFO:Local step 258500, global step 4141604: loss 0.0102
[2019-04-04 10:41:23,135] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 258500, global step 4141604: learning rate 0.0000
[2019-04-04 10:41:24,449] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259000, global step 4142108: loss 0.0125
[2019-04-04 10:41:24,450] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259000, global step 4142108: learning rate 0.0000
[2019-04-04 10:41:25,664] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259000, global step 4142452: loss 0.0131
[2019-04-04 10:41:25,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259000, global step 4142452: learning rate 0.0000
[2019-04-04 10:41:28,097] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259000, global step 4143175: loss 0.0067
[2019-04-04 10:41:28,097] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259000, global step 4143175: learning rate 0.0000
[2019-04-04 10:41:28,656] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7383792e-15 5.0393239e-14 2.2152114e-25 8.7550570e-16 6.9819507e-16
 2.2186116e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:28,657] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6545
[2019-04-04 10:41:28,702] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.55, 69.0, 0.0, 0.0, 26.0, 25.85488005648553, 0.4684737004294818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2226600.0000, 
sim time next is 2227200.0000, 
raw observation next is [-4.566666666666666, 69.33333333333333, 0.0, 0.0, 26.0, 25.78792758507323, 0.4492900643521952, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3361034164358265, 0.6933333333333332, 0.0, 0.0, 0.6666666666666666, 0.6489939654227692, 0.649763354784065, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.10245685], dtype=float32), -0.6881719]. 
=============================================
[2019-04-04 10:41:29,267] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259000, global step 4143524: loss 0.0046
[2019-04-04 10:41:29,272] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259000, global step 4143527: learning rate 0.0000
[2019-04-04 10:41:31,551] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259000, global step 4144481: loss 0.0126
[2019-04-04 10:41:31,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259000, global step 4144481: learning rate 0.0000
[2019-04-04 10:41:32,685] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259000, global step 4144793: loss 0.0070
[2019-04-04 10:41:32,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259000, global step 4144793: learning rate 0.0000
[2019-04-04 10:41:33,656] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259000, global step 4145083: loss 0.0046
[2019-04-04 10:41:33,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259000, global step 4145083: learning rate 0.0000
[2019-04-04 10:41:33,838] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6947100e-15 5.8630035e-14 5.5086127e-25 1.9777978e-15 6.6520150e-16
 5.5515619e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:33,855] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1199
[2019-04-04 10:41:33,888] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.516666666666667, 50.0, 234.6666666666667, 70.66666666666667, 26.0, 25.24706514382228, 0.3251476190431353, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2293800.0000, 
sim time next is 2294400.0000, 
raw observation next is [-1.333333333333333, 49.00000000000001, 227.8333333333333, 69.83333333333333, 26.0, 25.28085895697983, 0.2401469322942308, 1.0, 1.0, 7477.258160976296], 
processed observation next is [1.0, 0.5652173913043478, 0.42566943674976926, 0.49000000000000005, 0.7594444444444443, 0.07716390423572743, 0.6666666666666666, 0.6067382464149859, 0.5800489774314103, 1.0, 1.0, 0.035605991242744266], 
reward next is 0.9644, 
noisyNet noise sample is [array([-0.2861825], dtype=float32), -0.2834578]. 
=============================================
[2019-04-04 10:41:34,357] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259000, global step 4145336: loss 0.0023
[2019-04-04 10:41:34,360] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259000, global step 4145339: learning rate 0.0000
[2019-04-04 10:41:34,392] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259000, global step 4145351: loss 0.0030
[2019-04-04 10:41:34,393] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259000, global step 4145351: learning rate 0.0000
[2019-04-04 10:41:34,932] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259000, global step 4145557: loss 0.0027
[2019-04-04 10:41:34,933] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259000, global step 4145557: learning rate 0.0000
[2019-04-04 10:41:35,318] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259000, global step 4145680: loss 0.0038
[2019-04-04 10:41:35,325] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259000, global step 4145680: learning rate 0.0000
[2019-04-04 10:41:37,930] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8066282e-15 6.5864218e-14 3.9654201e-25 1.3299546e-15 1.2068976e-15
 1.3124446e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:37,941] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4583
[2019-04-04 10:41:37,983] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.1333333333333334, 51.33333333333334, 18.33333333333333, 89.0, 26.0, 26.12073813495166, 0.4262957961360306, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2654400.0000, 
sim time next is 2655000.0000, 
raw observation next is [-0.04999999999999999, 52.0, 7.0, 82.0, 26.0, 25.83168798086335, 0.4159473848624478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.461218836565097, 0.52, 0.023333333333333334, 0.09060773480662983, 0.6666666666666666, 0.6526406650719458, 0.6386491282874825, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1911945], dtype=float32), 0.18042888]. 
=============================================
[2019-04-04 10:41:38,008] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[83.28119 ]
 [83.42534 ]
 [83.56653 ]
 [83.71789 ]
 [83.707436]], R is [[83.22685242]
 [83.39458466]
 [83.56063843]
 [83.72503662]
 [83.88778687]].
[2019-04-04 10:41:40,353] A3C_AGENT_WORKER-Thread-5 INFO:Local step 259500, global step 4147465: loss 0.0005
[2019-04-04 10:41:40,358] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 259500, global step 4147465: learning rate 0.0000
[2019-04-04 10:41:41,544] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259000, global step 4147940: loss 0.0038
[2019-04-04 10:41:41,548] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259000, global step 4147940: learning rate 0.0000
[2019-04-04 10:41:43,651] A3C_AGENT_WORKER-Thread-12 INFO:Local step 259500, global step 4148599: loss 0.0040
[2019-04-04 10:41:43,653] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 259500, global step 4148600: learning rate 0.0000
[2019-04-04 10:41:44,410] A3C_AGENT_WORKER-Thread-2 INFO:Local step 259500, global step 4148924: loss 0.0076
[2019-04-04 10:41:44,413] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 259500, global step 4148925: learning rate 0.0000
[2019-04-04 10:41:47,241] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0585914e-13 1.8531019e-12 8.2447166e-24 4.4765823e-14 4.3333115e-14
 4.1608196e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:47,241] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1751
[2019-04-04 10:41:47,256] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.31916490018376, 0.3721843052808311, 0.0, 1.0, 47779.57686888866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2758800.0000, 
sim time next is 2759400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 25.21104102090755, 0.3561822022425916, 0.0, 1.0, 46845.99201985052], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.6009200850756292, 0.6187274007475305, 0.0, 1.0, 0.22307615247547866], 
reward next is 0.7769, 
noisyNet noise sample is [array([0.87388283], dtype=float32), 0.7285689]. 
=============================================
[2019-04-04 10:41:47,749] A3C_AGENT_WORKER-Thread-18 INFO:Local step 259500, global step 4149974: loss 0.0153
[2019-04-04 10:41:47,749] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 259500, global step 4149974: learning rate 0.0000
[2019-04-04 10:41:47,856] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8873328e-15 2.3363280e-13 3.3822829e-24 4.5703090e-15 1.7539704e-15
 4.6083641e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:47,862] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8549
[2019-04-04 10:41:47,893] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 47.5, 175.3333333333333, 180.6666666666667, 26.0, 25.81295823724233, 0.4621066480545952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2646600.0000, 
sim time next is 2647200.0000, 
raw observation next is [0.5, 48.0, 165.1666666666667, 193.3333333333333, 26.0, 25.95793603825287, 0.4724719205521996, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4764542936288089, 0.48, 0.5505555555555557, 0.21362799263351745, 0.6666666666666666, 0.6631613365210726, 0.6574906401840666, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.02184512], dtype=float32), -0.12445931]. 
=============================================
[2019-04-04 10:41:47,915] A3C_AGENT_WORKER-Thread-20 INFO:Local step 259500, global step 4150045: loss 0.0118
[2019-04-04 10:41:47,916] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 259500, global step 4150045: learning rate 0.0000
[2019-04-04 10:41:48,131] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259000, global step 4150139: loss 0.0093
[2019-04-04 10:41:48,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259000, global step 4150139: learning rate 0.0000
[2019-04-04 10:41:50,949] A3C_AGENT_WORKER-Thread-6 INFO:Local step 259500, global step 4151148: loss 0.0407
[2019-04-04 10:41:50,949] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 259500, global step 4151148: learning rate 0.0000
[2019-04-04 10:41:51,908] A3C_AGENT_WORKER-Thread-3 INFO:Local step 259500, global step 4151498: loss 0.0319
[2019-04-04 10:41:51,909] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 259500, global step 4151498: learning rate 0.0000
[2019-04-04 10:41:54,847] A3C_AGENT_WORKER-Thread-16 INFO:Local step 259500, global step 4152404: loss 0.0364
[2019-04-04 10:41:54,848] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 259500, global step 4152404: learning rate 0.0000
[2019-04-04 10:41:55,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7746590e-13 4.5071820e-12 1.0246126e-23 6.2078096e-14 4.3201072e-14
 6.4164392e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:41:55,104] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2092
[2019-04-04 10:41:55,144] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.58893506862823, 0.1876427689905971, 0.0, 1.0, 41051.58476002089], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.56578971081239, 0.1800486860797594, 0.0, 1.0, 40962.29084073725], 
processed observation next is [1.0, 0.13043478260869565, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5471491425676991, 0.5600162286932532, 0.0, 1.0, 0.19505852781303454], 
reward next is 0.8049, 
noisyNet noise sample is [array([0.05822614], dtype=float32), 0.024725258]. 
=============================================
[2019-04-04 10:41:55,841] A3C_AGENT_WORKER-Thread-13 INFO:Local step 259500, global step 4152724: loss 0.0424
[2019-04-04 10:41:55,843] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 259500, global step 4152725: learning rate 0.0000
[2019-04-04 10:41:56,655] A3C_AGENT_WORKER-Thread-17 INFO:Local step 259500, global step 4153000: loss 0.0199
[2019-04-04 10:41:56,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 259500, global step 4153000: learning rate 0.0000
[2019-04-04 10:41:56,752] A3C_AGENT_WORKER-Thread-4 INFO:Local step 259500, global step 4153044: loss 0.0167
[2019-04-04 10:41:56,752] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 259500, global step 4153044: learning rate 0.0000
[2019-04-04 10:41:57,048] A3C_AGENT_WORKER-Thread-15 INFO:Local step 259500, global step 4153169: loss 0.0168
[2019-04-04 10:41:57,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 259500, global step 4153169: learning rate 0.0000
[2019-04-04 10:41:57,809] A3C_AGENT_WORKER-Thread-19 INFO:Local step 259500, global step 4153499: loss 0.0196
[2019-04-04 10:41:57,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 259500, global step 4153500: learning rate 0.0000
[2019-04-04 10:41:57,943] A3C_AGENT_WORKER-Thread-11 INFO:Local step 259500, global step 4153544: loss 0.0204
[2019-04-04 10:41:57,943] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 259500, global step 4153544: learning rate 0.0000
[2019-04-04 10:42:01,925] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0177086e-15 2.3202344e-13 1.4449567e-24 5.5727246e-15 3.1720344e-15
 2.0910734e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:01,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1612
[2019-04-04 10:42:01,956] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 0.0, 0.0, 26.0, 25.12940257575779, 0.3861925235868759, 1.0, 1.0, 83456.2701213297], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2656800.0000, 
sim time next is 2657400.0000, 
raw observation next is [-0.7, 55.0, 0.0, 0.0, 26.0, 25.27725748582148, 0.3903401368759905, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.443213296398892, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6064381238184566, 0.6301133789586635, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.44596434], dtype=float32), -2.0779192]. 
=============================================
[2019-04-04 10:42:03,528] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260000, global step 4155386: loss 0.2130
[2019-04-04 10:42:03,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260000, global step 4155386: learning rate 0.0000
[2019-04-04 10:42:05,049] A3C_AGENT_WORKER-Thread-10 INFO:Local step 259500, global step 4156001: loss 0.0389
[2019-04-04 10:42:05,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 259500, global step 4156001: learning rate 0.0000
[2019-04-04 10:42:06,822] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260000, global step 4156573: loss 0.2022
[2019-04-04 10:42:06,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260000, global step 4156573: learning rate 0.0000
[2019-04-04 10:42:07,549] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3952451e-15 5.4003660e-14 1.4052623e-25 5.5279837e-16 7.8891341e-16
 5.4146916e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:07,549] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8896
[2019-04-04 10:42:07,583] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 54.16666666666667, 166.6666666666667, 416.3333333333333, 26.0, 25.96479666712283, 0.433458179134489, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2801400.0000, 
sim time next is 2802000.0000, 
raw observation next is [-2.333333333333333, 53.33333333333334, 170.3333333333333, 462.1666666666667, 26.0, 25.99853967419151, 0.4443956842148797, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3979686057248385, 0.5333333333333334, 0.5677777777777776, 0.5106813996316759, 0.6666666666666666, 0.6665449728492924, 0.6481318947382932, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1495849], dtype=float32), 1.2003651]. 
=============================================
[2019-04-04 10:42:07,588] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[86.09091 ]
 [86.06421 ]
 [85.779816]
 [85.33985 ]
 [84.90349 ]], R is [[86.22994995]
 [86.36765289]
 [86.50397491]
 [86.6389389 ]
 [86.77255249]].
[2019-04-04 10:42:07,718] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260000, global step 4156878: loss 0.1964
[2019-04-04 10:42:07,719] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260000, global step 4156878: learning rate 0.0000
[2019-04-04 10:42:10,835] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260000, global step 4158037: loss 0.1771
[2019-04-04 10:42:10,835] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260000, global step 4158037: learning rate 0.0000
[2019-04-04 10:42:10,852] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260000, global step 4158043: loss 0.1612
[2019-04-04 10:42:10,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260000, global step 4158043: learning rate 0.0000
[2019-04-04 10:42:11,083] A3C_AGENT_WORKER-Thread-14 INFO:Local step 259500, global step 4158139: loss 0.0004
[2019-04-04 10:42:11,088] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 259500, global step 4158140: learning rate 0.0000
[2019-04-04 10:42:14,513] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260000, global step 4159345: loss 0.1317
[2019-04-04 10:42:14,520] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260000, global step 4159345: learning rate 0.0000
[2019-04-04 10:42:15,416] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260000, global step 4159688: loss 0.1167
[2019-04-04 10:42:15,418] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260000, global step 4159688: learning rate 0.0000
[2019-04-04 10:42:17,766] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260000, global step 4160704: loss 0.1143
[2019-04-04 10:42:17,767] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260000, global step 4160704: learning rate 0.0000
[2019-04-04 10:42:18,960] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260000, global step 4161149: loss 0.1136
[2019-04-04 10:42:18,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260000, global step 4161149: learning rate 0.0000
[2019-04-04 10:42:19,307] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260000, global step 4161251: loss 0.0957
[2019-04-04 10:42:19,308] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260000, global step 4161251: learning rate 0.0000
[2019-04-04 10:42:19,639] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260000, global step 4161366: loss 0.1014
[2019-04-04 10:42:19,641] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260000, global step 4161366: learning rate 0.0000
[2019-04-04 10:42:20,019] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1975734e-16 3.6048354e-15 4.5459034e-27 3.1274432e-17 1.0797528e-16
 3.5010438e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:20,020] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.7719
[2019-04-04 10:42:20,028] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.333333333333333, 97.66666666666666, 112.8333333333333, 820.1666666666667, 26.0, 27.30905027318259, 0.835899487561572, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3156000.0000, 
sim time next is 3156600.0000, 
raw observation next is [7.166666666666667, 98.83333333333334, 112.6666666666667, 817.3333333333334, 26.0, 27.28227412314642, 0.8385597388096517, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.6611265004616806, 0.9883333333333334, 0.37555555555555564, 0.9031307550644567, 0.6666666666666666, 0.7735228435955349, 0.7795199129365505, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.12170997], dtype=float32), 0.90940887]. 
=============================================
[2019-04-04 10:42:20,120] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260000, global step 4161545: loss 0.1020
[2019-04-04 10:42:20,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260000, global step 4161545: learning rate 0.0000
[2019-04-04 10:42:20,757] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260000, global step 4161780: loss 0.0922
[2019-04-04 10:42:20,757] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260000, global step 4161780: learning rate 0.0000
[2019-04-04 10:42:20,790] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.0285649e-14 3.0976946e-13 7.9082615e-25 5.2390258e-15 5.9766539e-15
 2.5294378e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:20,791] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2682
[2019-04-04 10:42:20,843] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.5, 80.33333333333334, 0.0, 0.0, 26.0, 25.00562467940301, 0.2831062008675246, 0.0, 1.0, 53396.36910306997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3088200.0000, 
sim time next is 3088800.0000, 
raw observation next is [-0.6, 82.0, 0.0, 0.0, 26.0, 24.99176819956359, 0.2838153664125689, 0.0, 1.0, 50345.09180038461], 
processed observation next is [0.0, 0.782608695652174, 0.44598337950138506, 0.82, 0.0, 0.0, 0.6666666666666666, 0.5826473499636325, 0.5946051221375229, 0.0, 1.0, 0.23973853238278386], 
reward next is 0.7603, 
noisyNet noise sample is [array([-1.464205], dtype=float32), -1.636627]. 
=============================================
[2019-04-04 10:42:21,145] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260000, global step 4161948: loss 0.0910
[2019-04-04 10:42:21,147] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260000, global step 4161949: learning rate 0.0000
[2019-04-04 10:42:22,806] A3C_AGENT_WORKER-Thread-5 INFO:Local step 260500, global step 4162660: loss 0.0829
[2019-04-04 10:42:22,809] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 260500, global step 4162660: learning rate 0.0000
[2019-04-04 10:42:25,529] A3C_AGENT_WORKER-Thread-12 INFO:Local step 260500, global step 4163935: loss 0.0745
[2019-04-04 10:42:25,530] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 260500, global step 4163935: learning rate 0.0000
[2019-04-04 10:42:26,224] A3C_AGENT_WORKER-Thread-2 INFO:Local step 260500, global step 4164260: loss 0.0693
[2019-04-04 10:42:26,227] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 260500, global step 4164262: learning rate 0.0000
[2019-04-04 10:42:27,140] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260000, global step 4164699: loss 0.0899
[2019-04-04 10:42:27,141] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260000, global step 4164699: learning rate 0.0000
[2019-04-04 10:42:27,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7999271e-14 2.1310194e-13 3.0546122e-24 5.2454650e-15 7.8050537e-15
 9.1252666e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:27,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1525
[2019-04-04 10:42:27,596] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.666666666666666, 86.66666666666667, 0.0, 0.0, 26.0, 25.21507265107335, 0.4375896762040517, 0.0, 1.0, 43287.80854042621], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3282000.0000, 
sim time next is 3282600.0000, 
raw observation next is [-6.833333333333334, 85.33333333333333, 0.0, 0.0, 26.0, 25.17845957153692, 0.4275291659556724, 0.0, 1.0, 43338.48789613928], 
processed observation next is [1.0, 1.0, 0.27331486611265005, 0.8533333333333333, 0.0, 0.0, 0.6666666666666666, 0.5982049642947432, 0.6425097219852242, 0.0, 1.0, 0.20637375188637755], 
reward next is 0.7936, 
noisyNet noise sample is [array([1.9671513], dtype=float32), -0.010984208]. 
=============================================
[2019-04-04 10:42:29,359] A3C_AGENT_WORKER-Thread-18 INFO:Local step 260500, global step 4165664: loss 0.0720
[2019-04-04 10:42:29,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 260500, global step 4165664: learning rate 0.0000
[2019-04-04 10:42:29,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.0431827e-16 3.6885222e-14 1.3329293e-25 2.5003981e-16 2.9289544e-16
 3.6618355e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:29,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6843
[2019-04-04 10:42:29,484] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.833333333333333, 59.5, 90.33333333333334, 727.6666666666666, 26.0, 26.42931698521799, 0.6504888801029303, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3424200.0000, 
sim time next is 3424800.0000, 
raw observation next is [2.666666666666667, 61.0, 87.16666666666666, 715.8333333333334, 26.0, 26.51806901351702, 0.6613466718121495, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5364727608494922, 0.61, 0.2905555555555555, 0.7909760589318601, 0.6666666666666666, 0.7098390844597517, 0.7204488906040498, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.36137354], dtype=float32), 1.0309782]. 
=============================================
[2019-04-04 10:42:29,906] A3C_AGENT_WORKER-Thread-20 INFO:Local step 260500, global step 4165899: loss 0.0941
[2019-04-04 10:42:29,907] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 260500, global step 4165899: learning rate 0.0000
[2019-04-04 10:42:32,303] A3C_AGENT_WORKER-Thread-6 INFO:Local step 260500, global step 4166905: loss 0.1140
[2019-04-04 10:42:32,309] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 260500, global step 4166905: learning rate 0.0000
[2019-04-04 10:42:32,389] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260000, global step 4166941: loss 0.0715
[2019-04-04 10:42:32,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260000, global step 4166942: learning rate 0.0000
[2019-04-04 10:42:33,505] A3C_AGENT_WORKER-Thread-3 INFO:Local step 260500, global step 4167439: loss 0.1050
[2019-04-04 10:42:33,520] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 260500, global step 4167440: learning rate 0.0000
[2019-04-04 10:42:36,398] A3C_AGENT_WORKER-Thread-16 INFO:Local step 260500, global step 4168631: loss 0.1464
[2019-04-04 10:42:36,406] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 260500, global step 4168633: learning rate 0.0000
[2019-04-04 10:42:37,021] A3C_AGENT_WORKER-Thread-13 INFO:Local step 260500, global step 4168929: loss 0.1523
[2019-04-04 10:42:37,022] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 260500, global step 4168929: learning rate 0.0000
[2019-04-04 10:42:37,261] A3C_AGENT_WORKER-Thread-4 INFO:Local step 260500, global step 4169030: loss 0.1820
[2019-04-04 10:42:37,278] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 260500, global step 4169030: learning rate 0.0000
[2019-04-04 10:42:37,474] A3C_AGENT_WORKER-Thread-15 INFO:Local step 260500, global step 4169113: loss 0.1476
[2019-04-04 10:42:37,475] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 260500, global step 4169114: learning rate 0.0000
[2019-04-04 10:42:37,508] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [7.8769897e-15 6.2915005e-14 1.7872390e-25 1.1666191e-15 2.1334818e-15
 1.5199439e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:37,511] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0655
[2019-04-04 10:42:37,535] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.3815881931991, 0.4622837915980054, 0.0, 1.0, 58552.1911969145], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3456600.0000, 
sim time next is 3457200.0000, 
raw observation next is [1.0, 83.66666666666667, 0.0, 0.0, 26.0, 25.41945720718427, 0.4686307181259802, 0.0, 1.0, 27458.72141847509], 
processed observation next is [1.0, 0.0, 0.4903047091412743, 0.8366666666666667, 0.0, 0.0, 0.6666666666666666, 0.6182881005986891, 0.6562102393753267, 0.0, 1.0, 0.1307558162784528], 
reward next is 0.8692, 
noisyNet noise sample is [array([-0.9372352], dtype=float32), -0.83314663]. 
=============================================
[2019-04-04 10:42:38,004] A3C_AGENT_WORKER-Thread-17 INFO:Local step 260500, global step 4169367: loss 0.1624
[2019-04-04 10:42:38,005] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 260500, global step 4169367: learning rate 0.0000
[2019-04-04 10:42:38,454] A3C_AGENT_WORKER-Thread-11 INFO:Local step 260500, global step 4169564: loss 0.1670
[2019-04-04 10:42:38,456] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 260500, global step 4169566: learning rate 0.0000
[2019-04-04 10:42:39,096] A3C_AGENT_WORKER-Thread-19 INFO:Local step 260500, global step 4169836: loss 0.1791
[2019-04-04 10:42:39,097] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 260500, global step 4169837: learning rate 0.0000
[2019-04-04 10:42:40,142] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1814971e-15 3.0553488e-15 1.6225057e-28 9.5524412e-17 9.7830380e-17
 9.2173692e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:40,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5365
[2019-04-04 10:42:40,204] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 60.0, 44.5, 264.5, 26.0, 25.68057900825761, 0.3822752058633924, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3398400.0000, 
sim time next is 3399000.0000, 
raw observation next is [-1.833333333333333, 60.0, 58.66666666666667, 317.0, 26.0, 25.55692882846579, 0.3807228681440273, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.41181902123730385, 0.6, 0.19555555555555557, 0.35027624309392263, 0.6666666666666666, 0.6297440690388157, 0.6269076227146758, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01002586], dtype=float32), -0.25849557]. 
=============================================
[2019-04-04 10:42:40,214] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[93.664055]
 [93.24768 ]
 [92.49491 ]
 [91.066826]
 [88.51928 ]], R is [[93.72454834]
 [93.78730011]
 [93.84942627]
 [93.91093445]
 [93.97182465]].
[2019-04-04 10:42:40,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4882861e-16 8.0141692e-15 1.7515935e-26 1.8702059e-16 1.1821863e-16
 1.6836051e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:40,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4636
[2019-04-04 10:42:40,435] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 67.0, 61.0, 513.0, 26.0, 26.13147025168256, 0.6157556568732404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3429000.0000, 
sim time next is 3429600.0000, 
raw observation next is [2.0, 67.0, 52.83333333333334, 447.6666666666667, 26.0, 26.33051869446398, 0.6259984155822239, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.67, 0.17611111111111113, 0.4946593001841621, 0.6666666666666666, 0.6942098912053316, 0.7086661385274079, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3331073], dtype=float32), 0.7932732]. 
=============================================
[2019-04-04 10:42:40,576] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261000, global step 4170527: loss 9.0340
[2019-04-04 10:42:40,578] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261000, global step 4170527: learning rate 0.0000
[2019-04-04 10:42:42,750] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261000, global step 4171527: loss 9.4246
[2019-04-04 10:42:42,753] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261000, global step 4171529: learning rate 0.0000
[2019-04-04 10:42:43,599] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1580312e-14 6.3835997e-13 7.8257339e-25 4.4134190e-15 7.1578680e-15
 7.5468088e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:43,600] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6322
[2019-04-04 10:42:43,628] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 75.5, 0.0, 0.0, 26.0, 25.40353631335341, 0.429566570499933, 0.0, 1.0, 53843.8722686788], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3465000.0000, 
sim time next is 3465600.0000, 
raw observation next is [1.0, 74.33333333333334, 0.0, 0.0, 26.0, 25.38768939122411, 0.4250506837539163, 0.0, 1.0, 52335.73696617677], 
processed observation next is [1.0, 0.08695652173913043, 0.4903047091412743, 0.7433333333333334, 0.0, 0.0, 0.6666666666666666, 0.6156407826020093, 0.6416835612513054, 0.0, 1.0, 0.24921779507703226], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.40625948], dtype=float32), 1.0677936]. 
=============================================
[2019-04-04 10:42:44,152] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261000, global step 4172166: loss 9.2696
[2019-04-04 10:42:44,154] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261000, global step 4172167: learning rate 0.0000
[2019-04-04 10:42:44,951] A3C_AGENT_WORKER-Thread-10 INFO:Local step 260500, global step 4172526: loss 0.1224
[2019-04-04 10:42:44,954] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 260500, global step 4172527: learning rate 0.0000
[2019-04-04 10:42:46,813] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261000, global step 4173420: loss 9.1985
[2019-04-04 10:42:46,815] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261000, global step 4173420: learning rate 0.0000
[2019-04-04 10:42:47,707] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261000, global step 4173879: loss 9.1474
[2019-04-04 10:42:47,709] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261000, global step 4173879: learning rate 0.0000
[2019-04-04 10:42:49,851] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261000, global step 4174856: loss 9.2208
[2019-04-04 10:42:49,854] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261000, global step 4174859: learning rate 0.0000
[2019-04-04 10:42:50,107] A3C_AGENT_WORKER-Thread-14 INFO:Local step 260500, global step 4174965: loss 0.1036
[2019-04-04 10:42:50,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 260500, global step 4174967: learning rate 0.0000
[2019-04-04 10:42:50,659] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261000, global step 4175209: loss 9.1712
[2019-04-04 10:42:50,670] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261000, global step 4175209: learning rate 0.0000
[2019-04-04 10:42:54,018] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261000, global step 4176855: loss 9.5330
[2019-04-04 10:42:54,023] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261000, global step 4176857: learning rate 0.0000
[2019-04-04 10:42:54,208] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261000, global step 4176947: loss 9.4162
[2019-04-04 10:42:54,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261000, global step 4176947: learning rate 0.0000
[2019-04-04 10:42:54,600] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261000, global step 4177150: loss 9.5883
[2019-04-04 10:42:54,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261000, global step 4177151: learning rate 0.0000
[2019-04-04 10:42:54,743] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261000, global step 4177233: loss 9.3194
[2019-04-04 10:42:54,746] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261000, global step 4177234: learning rate 0.0000
[2019-04-04 10:42:55,018] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261000, global step 4177407: loss 9.6559
[2019-04-04 10:42:55,018] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261000, global step 4177407: learning rate 0.0000
[2019-04-04 10:42:55,626] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261000, global step 4177748: loss 9.3944
[2019-04-04 10:42:55,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261000, global step 4177748: learning rate 0.0000
[2019-04-04 10:42:56,223] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261000, global step 4178061: loss 9.1126
[2019-04-04 10:42:56,229] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261000, global step 4178065: learning rate 0.0000
[2019-04-04 10:42:57,773] A3C_AGENT_WORKER-Thread-5 INFO:Local step 261500, global step 4178822: loss 0.2133
[2019-04-04 10:42:57,774] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 261500, global step 4178822: learning rate 0.0000
[2019-04-04 10:42:58,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5502019e-15 2.5748463e-14 3.0259284e-26 2.8020334e-16 2.7166424e-16
 4.2737090e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:42:58,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0843
[2019-04-04 10:42:58,181] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 76.0, 107.3333333333333, 737.6666666666667, 26.0, 26.26441726543885, 0.5274549292492238, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3751800.0000, 
sim time next is 3752400.0000, 
raw observation next is [-3.0, 75.0, 109.1666666666667, 753.3333333333334, 26.0, 26.30292183188306, 0.5387125836135936, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.75, 0.363888888888889, 0.8324125230202579, 0.6666666666666666, 0.6919101526569218, 0.6795708612045313, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.508726], dtype=float32), -0.40784043]. 
=============================================
[2019-04-04 10:42:59,194] A3C_AGENT_WORKER-Thread-12 INFO:Local step 261500, global step 4179522: loss 0.2325
[2019-04-04 10:42:59,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 261500, global step 4179526: learning rate 0.0000
[2019-04-04 10:43:00,918] A3C_AGENT_WORKER-Thread-2 INFO:Local step 261500, global step 4180361: loss 0.2355
[2019-04-04 10:43:00,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 261500, global step 4180361: learning rate 0.0000
[2019-04-04 10:43:01,857] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261000, global step 4180821: loss 9.1559
[2019-04-04 10:43:01,857] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261000, global step 4180821: learning rate 0.0000
[2019-04-04 10:43:03,298] A3C_AGENT_WORKER-Thread-18 INFO:Local step 261500, global step 4181409: loss 0.2317
[2019-04-04 10:43:03,302] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 261500, global step 4181409: learning rate 0.0000
[2019-04-04 10:43:03,371] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0101603e-15 3.2334144e-14 9.5677143e-26 5.1057664e-16 4.2677530e-16
 1.1470163e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:03,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7538
[2019-04-04 10:43:03,397] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.666666666666667, 23.33333333333334, 59.16666666666667, 488.8333333333334, 26.0, 26.31194668630805, 0.6763719325845875, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4034400.0000, 
sim time next is 4035000.0000, 
raw observation next is [-1.833333333333333, 23.66666666666666, 51.33333333333334, 429.6666666666667, 26.0, 26.62693941118911, 0.7014454184629756, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.41181902123730385, 0.2366666666666666, 0.17111111111111113, 0.47476979742173114, 0.6666666666666666, 0.7189116175990925, 0.7338151394876585, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22952417], dtype=float32), 0.31298372]. 
=============================================
[2019-04-04 10:43:03,409] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[86.8755 ]
 [87.66492]
 [88.12559]
 [88.42382]
 [88.64017]], R is [[86.35787201]
 [86.49429321]
 [86.62934875]
 [86.76305389]
 [86.89542389]].
[2019-04-04 10:43:04,597] A3C_AGENT_WORKER-Thread-20 INFO:Local step 261500, global step 4182034: loss 0.2169
[2019-04-04 10:43:04,599] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 261500, global step 4182034: learning rate 0.0000
[2019-04-04 10:43:06,143] A3C_AGENT_WORKER-Thread-6 INFO:Local step 261500, global step 4182787: loss 0.2022
[2019-04-04 10:43:06,143] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 261500, global step 4182787: learning rate 0.0000
[2019-04-04 10:43:06,604] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261000, global step 4183016: loss 9.1947
[2019-04-04 10:43:06,606] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261000, global step 4183016: learning rate 0.0000
[2019-04-04 10:43:06,947] A3C_AGENT_WORKER-Thread-3 INFO:Local step 261500, global step 4183181: loss 0.2037
[2019-04-04 10:43:06,948] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 261500, global step 4183181: learning rate 0.0000
[2019-04-04 10:43:08,562] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3904773e-15 3.1170264e-14 3.3548336e-25 4.7850199e-16 3.2645942e-16
 5.8348117e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:08,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6001
[2019-04-04 10:43:08,589] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.5, 23.0, 57.0, 26.0, 26.636558246562, 0.6274873714718255, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4123800.0000, 
sim time next is 4124400.0000, 
raw observation next is [3.0, 35.0, 19.16666666666667, 47.5, 26.0, 26.50335725151315, 0.6804277224355384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.5457063711911359, 0.35, 0.0638888888888889, 0.052486187845303865, 0.6666666666666666, 0.7086131042927626, 0.7268092408118462, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3364267], dtype=float32), -0.4888224]. 
=============================================
[2019-04-04 10:43:10,492] A3C_AGENT_WORKER-Thread-13 INFO:Local step 261500, global step 4184701: loss 0.2460
[2019-04-04 10:43:10,493] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 261500, global step 4184701: learning rate 0.0000
[2019-04-04 10:43:10,618] A3C_AGENT_WORKER-Thread-16 INFO:Local step 261500, global step 4184751: loss 0.2541
[2019-04-04 10:43:10,619] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 261500, global step 4184751: learning rate 0.0000
[2019-04-04 10:43:11,203] A3C_AGENT_WORKER-Thread-4 INFO:Local step 261500, global step 4185041: loss 0.2362
[2019-04-04 10:43:11,205] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 261500, global step 4185041: learning rate 0.0000
[2019-04-04 10:43:11,266] A3C_AGENT_WORKER-Thread-15 INFO:Local step 261500, global step 4185071: loss 0.2289
[2019-04-04 10:43:11,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 261500, global step 4185071: learning rate 0.0000
[2019-04-04 10:43:11,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2057392e-13 1.1002157e-12 8.9651368e-24 1.8443516e-14 3.5081919e-14
 4.1275108e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:11,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4472
[2019-04-04 10:43:11,304] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.31538781434478, 0.1675479142193624, 0.0, 1.0, 43761.52558020704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3984000.0000, 
sim time next is 3984600.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.26272556757615, 0.1657580356239948, 0.0, 1.0, 43748.98248055012], 
processed observation next is [1.0, 0.08695652173913043, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5218937972980126, 0.5552526785413315, 0.0, 1.0, 0.2083284880026196], 
reward next is 0.7917, 
noisyNet noise sample is [array([0.44970608], dtype=float32), -1.6622806]. 
=============================================
[2019-04-04 10:43:11,623] A3C_AGENT_WORKER-Thread-17 INFO:Local step 261500, global step 4185244: loss 0.2213
[2019-04-04 10:43:11,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 261500, global step 4185244: learning rate 0.0000
[2019-04-04 10:43:12,506] A3C_AGENT_WORKER-Thread-11 INFO:Local step 261500, global step 4185639: loss 0.2278
[2019-04-04 10:43:12,508] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 261500, global step 4185639: learning rate 0.0000
[2019-04-04 10:43:12,885] A3C_AGENT_WORKER-Thread-19 INFO:Local step 261500, global step 4185803: loss 0.2125
[2019-04-04 10:43:12,887] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 261500, global step 4185805: learning rate 0.0000
[2019-04-04 10:43:14,123] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.3396137e-15 2.0603622e-14 7.3735285e-26 5.6726924e-16 2.2931469e-16
 1.1108313e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:14,124] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4668
[2019-04-04 10:43:14,148] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.0, 38.5, 119.0, 816.0, 26.0, 26.45866945388077, 0.5827799361896371, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4015800.0000, 
sim time next is 4016400.0000, 
raw observation next is [-6.666666666666667, 38.0, 118.8333333333333, 820.1666666666666, 26.0, 26.44166647824474, 0.5833882971345098, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.27793167128347185, 0.38, 0.396111111111111, 0.9062615101289134, 0.6666666666666666, 0.7034722065203951, 0.6944627657115032, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.013195], dtype=float32), 1.2211384]. 
=============================================
[2019-04-04 10:43:14,926] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262000, global step 4186674: loss 0.6489
[2019-04-04 10:43:14,926] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262000, global step 4186674: learning rate 0.0000
[2019-04-04 10:43:15,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4479718e-15 3.2726399e-14 2.7599991e-26 6.3913750e-16 8.0217196e-16
 1.4757950e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:15,322] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.3595
[2019-04-04 10:43:15,346] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 55.33333333333334, 194.8333333333333, 661.6666666666666, 26.0, 25.36605031871756, 0.4404771906580298, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4282800.0000, 
sim time next is 4283400.0000, 
raw observation next is [7.0, 56.16666666666666, 201.6666666666667, 606.3333333333334, 26.0, 25.37189141583549, 0.4429090001489131, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.6565096952908588, 0.5616666666666665, 0.6722222222222224, 0.6699815837937385, 0.6666666666666666, 0.6143242846529574, 0.647636333382971, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5330022], dtype=float32), -0.27558094]. 
=============================================
[2019-04-04 10:43:16,769] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262000, global step 4187564: loss 0.6253
[2019-04-04 10:43:16,773] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262000, global step 4187564: learning rate 0.0000
[2019-04-04 10:43:18,146] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262000, global step 4188272: loss 0.6740
[2019-04-04 10:43:18,149] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262000, global step 4188274: learning rate 0.0000
[2019-04-04 10:43:18,717] A3C_AGENT_WORKER-Thread-10 INFO:Local step 261500, global step 4188536: loss 0.2130
[2019-04-04 10:43:18,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 261500, global step 4188536: learning rate 0.0000
[2019-04-04 10:43:19,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0533095e-15 1.1100372e-14 8.1298698e-26 1.7178874e-16 2.9720840e-16
 9.0912460e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:19,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9075
[2019-04-04 10:43:19,414] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 35.0, 93.83333333333334, 652.0, 26.0, 25.90378369617365, 0.6708065689847188, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4117200.0000, 
sim time next is 4117800.0000, 
raw observation next is [4.0, 35.0, 93.66666666666666, 609.0, 26.0, 26.49511937282342, 0.7181115541718291, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5734072022160666, 0.35, 0.3122222222222222, 0.6729281767955801, 0.6666666666666666, 0.7079266144019517, 0.7393705180572764, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18283549], dtype=float32), 1.6609868]. 
=============================================
[2019-04-04 10:43:20,938] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262000, global step 4189645: loss 0.7253
[2019-04-04 10:43:20,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262000, global step 4189645: learning rate 0.0000
[2019-04-04 10:43:21,606] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262000, global step 4190000: loss 0.7054
[2019-04-04 10:43:21,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262000, global step 4190000: learning rate 0.0000
[2019-04-04 10:43:22,663] A3C_AGENT_WORKER-Thread-14 INFO:Local step 261500, global step 4190617: loss 0.1881
[2019-04-04 10:43:22,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 261500, global step 4190617: learning rate 0.0000
[2019-04-04 10:43:23,404] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262000, global step 4190986: loss 0.7894
[2019-04-04 10:43:23,405] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262000, global step 4190986: learning rate 0.0000
[2019-04-04 10:43:23,792] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2029709e-14 1.7754499e-13 3.5717738e-25 2.7456696e-15 2.4585938e-15
 1.9529053e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:23,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7310
[2019-04-04 10:43:23,814] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 43.33333333333334, 178.0, 238.6666666666666, 26.0, 24.9923256409709, 0.3662001056674151, 0.0, 1.0, 56458.97562234469], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4200600.0000, 
sim time next is 4201200.0000, 
raw observation next is [2.0, 44.0, 173.5, 313.5, 26.0, 25.00997361016111, 0.3874280953327778, 0.0, 1.0, 26651.53363485365], 
processed observation next is [0.0, 0.6521739130434783, 0.518005540166205, 0.44, 0.5783333333333334, 0.34640883977900555, 0.6666666666666666, 0.584164467513426, 0.6291426984442593, 0.0, 1.0, 0.12691206492787452], 
reward next is 0.8731, 
noisyNet noise sample is [array([0.06069012], dtype=float32), 1.5397553]. 
=============================================
[2019-04-04 10:43:23,864] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262000, global step 4191238: loss 0.8320
[2019-04-04 10:43:23,865] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262000, global step 4191238: learning rate 0.0000
[2019-04-04 10:43:24,224] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.4749375e-13 3.8579239e-13 2.4754656e-24 1.5874548e-14 2.4449027e-14
 1.3014521e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:24,224] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8738
[2019-04-04 10:43:24,247] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 41.33333333333334, 0.0, 0.0, 26.0, 25.0258223982799, 0.3120890383238644, 0.0, 1.0, 30407.70915767929], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4213200.0000, 
sim time next is 4213800.0000, 
raw observation next is [1.55, 41.5, 0.0, 0.0, 26.0, 25.03316562767169, 0.3072938291436379, 0.0, 1.0, 23345.10764439996], 
processed observation next is [0.0, 0.782608695652174, 0.5055401662049862, 0.415, 0.0, 0.0, 0.6666666666666666, 0.5860971356393074, 0.6024312763812126, 0.0, 1.0, 0.11116717925904743], 
reward next is 0.8888, 
noisyNet noise sample is [array([2.0823088], dtype=float32), 0.76361215]. 
=============================================
[2019-04-04 10:43:24,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.5721917e-14 2.3246830e-12 1.3934392e-24 2.0461775e-14 2.1723236e-14
 2.1935587e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:24,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1193
[2019-04-04 10:43:24,762] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 40.0, 0.0, 0.0, 26.0, 25.00163406006261, 0.2686195909009406, 0.0, 1.0, 40572.11548356387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4074000.0000, 
sim time next is 4074600.0000, 
raw observation next is [-5.0, 40.5, 0.0, 0.0, 26.0, 24.96676701052392, 0.2644378552075386, 0.0, 1.0, 40536.31624381836], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.405, 0.0, 0.0, 0.6666666666666666, 0.58056391754366, 0.5881459517358462, 0.0, 1.0, 0.19303007735151598], 
reward next is 0.8070, 
noisyNet noise sample is [array([-0.57998127], dtype=float32), -0.6891602]. 
=============================================
[2019-04-04 10:43:27,244] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262000, global step 4193112: loss 0.9505
[2019-04-04 10:43:27,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262000, global step 4193112: learning rate 0.0000
[2019-04-04 10:43:27,536] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262000, global step 4193269: loss 0.9404
[2019-04-04 10:43:27,543] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262000, global step 4193276: learning rate 0.0000
[2019-04-04 10:43:27,745] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262000, global step 4193385: loss 0.9376
[2019-04-04 10:43:27,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262000, global step 4193386: learning rate 0.0000
[2019-04-04 10:43:28,081] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262000, global step 4193591: loss 0.9703
[2019-04-04 10:43:28,082] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262000, global step 4193591: learning rate 0.0000
[2019-04-04 10:43:28,261] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262000, global step 4193692: loss 0.9484
[2019-04-04 10:43:28,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262000, global step 4193695: learning rate 0.0000
[2019-04-04 10:43:29,082] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262000, global step 4194181: loss 0.9916
[2019-04-04 10:43:29,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262000, global step 4194181: learning rate 0.0000
[2019-04-04 10:43:29,220] A3C_AGENT_WORKER-Thread-5 INFO:Local step 262500, global step 4194257: loss 0.0126
[2019-04-04 10:43:29,231] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 262500, global step 4194257: learning rate 0.0000
[2019-04-04 10:43:29,624] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262000, global step 4194481: loss 1.0036
[2019-04-04 10:43:29,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262000, global step 4194481: learning rate 0.0000
[2019-04-04 10:43:31,602] A3C_AGENT_WORKER-Thread-12 INFO:Local step 262500, global step 4195173: loss 0.0215
[2019-04-04 10:43:31,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 262500, global step 4195176: learning rate 0.0000
[2019-04-04 10:43:33,866] A3C_AGENT_WORKER-Thread-2 INFO:Local step 262500, global step 4195896: loss 0.0249
[2019-04-04 10:43:33,867] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 262500, global step 4195896: learning rate 0.0000
[2019-04-04 10:43:38,036] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262000, global step 4197287: loss 1.1035
[2019-04-04 10:43:38,037] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262000, global step 4197287: learning rate 0.0000
[2019-04-04 10:43:38,363] A3C_AGENT_WORKER-Thread-18 INFO:Local step 262500, global step 4197405: loss 0.0093
[2019-04-04 10:43:38,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 262500, global step 4197405: learning rate 0.0000
[2019-04-04 10:43:39,425] A3C_AGENT_WORKER-Thread-20 INFO:Local step 262500, global step 4197788: loss 0.0039
[2019-04-04 10:43:39,430] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 262500, global step 4197790: learning rate 0.0000
[2019-04-04 10:43:41,992] A3C_AGENT_WORKER-Thread-6 INFO:Local step 262500, global step 4198589: loss 0.0041
[2019-04-04 10:43:41,994] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 262500, global step 4198589: learning rate 0.0000
[2019-04-04 10:43:42,479] A3C_AGENT_WORKER-Thread-3 INFO:Local step 262500, global step 4198751: loss 0.0048
[2019-04-04 10:43:42,485] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 262500, global step 4198751: learning rate 0.0000
[2019-04-04 10:43:44,110] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262000, global step 4199247: loss 1.2282
[2019-04-04 10:43:44,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262000, global step 4199247: learning rate 0.0000
[2019-04-04 10:43:45,272] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5448565e-15 5.8401350e-14 1.4299479e-25 7.6055385e-16 7.8746721e-16
 1.0021872e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:43:45,272] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5252
[2019-04-04 10:43:45,329] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 72.0, 76.33333333333334, 16.66666666666666, 26.0, 25.00057114212286, 0.443161358988682, 1.0, 1.0, 170719.191767522], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4726200.0000, 
sim time next is 4726800.0000, 
raw observation next is [1.0, 72.0, 64.5, 19.5, 26.0, 25.42567177108764, 0.4967021836291559, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.4903047091412743, 0.72, 0.215, 0.02154696132596685, 0.6666666666666666, 0.61880598092397, 0.6655673945430519, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.601001], dtype=float32), 0.17462912]. 
=============================================
[2019-04-04 10:43:46,522] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-04 10:43:46,529] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:43:46,529] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:43:46,531] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:43:46,531] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:43:46,532] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:43:46,532] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:43:46,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run43
[2019-04-04 10:43:46,553] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run43
[2019-04-04 10:43:46,659] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run43
[2019-04-04 10:44:22,601] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15490328], dtype=float32), -0.2804892]
[2019-04-04 10:44:22,602] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-2.048825488666667, 93.754247435, 0.0, 0.0, 26.0, 25.14741976984266, 0.1994984104925459, 1.0, 1.0, 0.0]
[2019-04-04 10:44:22,602] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:44:22,602] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.7298155e-15 4.9798731e-14 2.9307277e-26 1.4155133e-15 9.1158776e-16
 9.4792202e-18 1.0000000e+00], sampled 0.9578476934955514
[2019-04-04 10:46:17,733] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15490328], dtype=float32), -0.2804892]
[2019-04-04 10:46:17,733] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [-4.916666666666667, 58.5, 183.0, 318.6666666666666, 26.0, 25.90882931529535, 0.5323991172627611, 1.0, 1.0, 0.0]
[2019-04-04 10:46:17,733] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 10:46:17,734] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.52205743e-15 1.15844664e-14 3.09568987e-26 3.55455523e-16
 1.65325505e-16 2.82481538e-18 1.00000000e+00], sampled 0.7038269570755433
[2019-04-04 10:46:54,855] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:46:56,300] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15490328], dtype=float32), -0.2804892]
[2019-04-04 10:46:56,301] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [3.0, 34.5, 0.0, 0.0, 26.0, 26.55642312310427, 0.6744547851609412, 1.0, 1.0, 0.0]
[2019-04-04 10:46:56,301] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:46:56,301] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [7.5738400e-15 7.7749547e-14 1.1780797e-25 1.7647287e-15 1.1341560e-15
 1.5568315e-17 1.0000000e+00], sampled 0.9726046973484455
[2019-04-04 10:47:02,954] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15490328], dtype=float32), -0.2804892]
[2019-04-04 10:47:02,955] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [13.9, 32.0, 149.0, 314.5, 26.0, 28.05282531527876, 1.08504975634544, 1.0, 1.0, 0.0]
[2019-04-04 10:47:02,955] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 10:47:02,956] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.22586263e-16 7.45070340e-15 4.60278922e-27 1.26167053e-16
 1.10250325e-16 1.06371343e-18 1.00000000e+00], sampled 0.06363125487517096
[2019-04-04 10:47:24,135] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:47:25,443] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15490328], dtype=float32), -0.2804892]
[2019-04-04 10:47:25,443] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-1.735664450333333, 68.29389347, 0.0, 0.0, 26.0, 24.64757687717289, 0.1944939156001158, 0.0, 1.0, 39177.88669668899]
[2019-04-04 10:47:25,443] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 10:47:25,444] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [2.1167923e-13 6.4971504e-13 5.3397822e-25 1.7034542e-14 1.9716969e-14
 9.6300433e-17 1.0000000e+00], sampled 0.9576219778822133
[2019-04-04 10:47:32,239] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:47:33,272] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 4200000, evaluation results [4200000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:47:36,076] A3C_AGENT_WORKER-Thread-16 INFO:Local step 262500, global step 4200665: loss 0.0016
[2019-04-04 10:47:36,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 262500, global step 4200665: learning rate 0.0000
[2019-04-04 10:47:36,520] A3C_AGENT_WORKER-Thread-13 INFO:Local step 262500, global step 4200784: loss 0.0071
[2019-04-04 10:47:36,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 262500, global step 4200784: learning rate 0.0000
[2019-04-04 10:47:38,172] A3C_AGENT_WORKER-Thread-15 INFO:Local step 262500, global step 4201172: loss 0.0060
[2019-04-04 10:47:38,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 262500, global step 4201172: learning rate 0.0000
[2019-04-04 10:47:38,618] A3C_AGENT_WORKER-Thread-17 INFO:Local step 262500, global step 4201261: loss 0.0035
[2019-04-04 10:47:38,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 262500, global step 4201261: learning rate 0.0000
[2019-04-04 10:47:38,784] A3C_AGENT_WORKER-Thread-4 INFO:Local step 262500, global step 4201303: loss 0.0030
[2019-04-04 10:47:38,785] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 262500, global step 4201303: learning rate 0.0000
[2019-04-04 10:47:41,170] A3C_AGENT_WORKER-Thread-11 INFO:Local step 262500, global step 4201965: loss 0.0098
[2019-04-04 10:47:41,192] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 262500, global step 4201965: learning rate 0.0000
[2019-04-04 10:47:41,764] A3C_AGENT_WORKER-Thread-19 INFO:Local step 262500, global step 4202123: loss 0.0118
[2019-04-04 10:47:41,764] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 262500, global step 4202123: learning rate 0.0000
[2019-04-04 10:47:42,598] A3C_AGENT_WORKER-Thread-5 INFO:Local step 263000, global step 4202330: loss 0.2719
[2019-04-04 10:47:42,600] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 263000, global step 4202330: learning rate 0.0000
[2019-04-04 10:47:45,798] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9703374e-14 1.2717212e-12 1.0279319e-24 1.6460356e-14 4.4625843e-14
 5.0888495e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:47:45,798] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6026
[2019-04-04 10:47:45,812] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.8666666666666667, 71.0, 0.0, 0.0, 26.0, 25.29157411224604, 0.4083494460672032, 0.0, 1.0, 41284.79990031576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4512000.0000, 
sim time next is 4512600.0000, 
raw observation next is [-0.9, 71.0, 0.0, 0.0, 26.0, 25.27730626566164, 0.4033186285134325, 0.0, 1.0, 41206.8657760791], 
processed observation next is [1.0, 0.21739130434782608, 0.43767313019390586, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6064421888051367, 0.6344395428378108, 0.0, 1.0, 0.19622317036228143], 
reward next is 0.8038, 
noisyNet noise sample is [array([-0.16815838], dtype=float32), 0.6062674]. 
=============================================
[2019-04-04 10:47:48,227] A3C_AGENT_WORKER-Thread-12 INFO:Local step 263000, global step 4203788: loss 0.2664
[2019-04-04 10:47:48,232] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 263000, global step 4203788: learning rate 0.0000
[2019-04-04 10:47:50,190] A3C_AGENT_WORKER-Thread-2 INFO:Local step 263000, global step 4204377: loss 0.2685
[2019-04-04 10:47:50,191] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 263000, global step 4204377: learning rate 0.0000
[2019-04-04 10:47:51,608] A3C_AGENT_WORKER-Thread-10 INFO:Local step 262500, global step 4204765: loss 0.0038
[2019-04-04 10:47:51,613] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 262500, global step 4204765: learning rate 0.0000
[2019-04-04 10:47:54,979] A3C_AGENT_WORKER-Thread-18 INFO:Local step 263000, global step 4205627: loss 0.2652
[2019-04-04 10:47:54,979] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 263000, global step 4205627: learning rate 0.0000
[2019-04-04 10:47:56,510] A3C_AGENT_WORKER-Thread-20 INFO:Local step 263000, global step 4205977: loss 0.2605
[2019-04-04 10:47:56,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 263000, global step 4205977: learning rate 0.0000
[2019-04-04 10:47:58,748] A3C_AGENT_WORKER-Thread-6 INFO:Local step 263000, global step 4206557: loss 0.2535
[2019-04-04 10:47:58,750] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 263000, global step 4206557: learning rate 0.0000
[2019-04-04 10:47:59,655] A3C_AGENT_WORKER-Thread-14 INFO:Local step 262500, global step 4206758: loss 0.0371
[2019-04-04 10:47:59,658] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 262500, global step 4206758: learning rate 0.0000
[2019-04-04 10:48:00,164] A3C_AGENT_WORKER-Thread-3 INFO:Local step 263000, global step 4206870: loss 0.2502
[2019-04-04 10:48:00,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 263000, global step 4206870: learning rate 0.0000
[2019-04-04 10:48:05,624] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:05,625] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:05,641] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run32
[2019-04-04 10:48:07,506] A3C_AGENT_WORKER-Thread-16 INFO:Local step 263000, global step 4208745: loss 0.2389
[2019-04-04 10:48:07,507] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 263000, global step 4208745: learning rate 0.0000
[2019-04-04 10:48:07,980] A3C_AGENT_WORKER-Thread-13 INFO:Local step 263000, global step 4208838: loss 0.2307
[2019-04-04 10:48:07,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 263000, global step 4208838: learning rate 0.0000
[2019-04-04 10:48:08,502] A3C_AGENT_WORKER-Thread-17 INFO:Local step 263000, global step 4208968: loss 0.2270
[2019-04-04 10:48:08,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 263000, global step 4208968: learning rate 0.0000
[2019-04-04 10:48:09,438] A3C_AGENT_WORKER-Thread-15 INFO:Local step 263000, global step 4209140: loss 0.2346
[2019-04-04 10:48:09,441] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 263000, global step 4209140: learning rate 0.0000
[2019-04-04 10:48:10,185] A3C_AGENT_WORKER-Thread-4 INFO:Local step 263000, global step 4209485: loss 0.2308
[2019-04-04 10:48:10,188] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 263000, global step 4209485: learning rate 0.0000
[2019-04-04 10:48:10,827] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:10,827] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:10,831] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run32
[2019-04-04 10:48:11,073] A3C_AGENT_WORKER-Thread-11 INFO:Local step 263000, global step 4209809: loss 0.2285
[2019-04-04 10:48:11,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 263000, global step 4209811: learning rate 0.0000
[2019-04-04 10:48:11,434] A3C_AGENT_WORKER-Thread-19 INFO:Local step 263000, global step 4209927: loss 0.2216
[2019-04-04 10:48:11,436] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 263000, global step 4209928: learning rate 0.0000
[2019-04-04 10:48:12,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:12,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:12,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run32
[2019-04-04 10:48:16,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:16,347] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:16,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run32
[2019-04-04 10:48:16,791] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:16,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:16,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run32
[2019-04-04 10:48:17,347] A3C_AGENT_WORKER-Thread-10 INFO:Local step 263000, global step 4211905: loss 0.2176
[2019-04-04 10:48:17,357] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 263000, global step 4211907: learning rate 0.0000
[2019-04-04 10:48:18,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:18,277] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:18,287] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run32
[2019-04-04 10:48:19,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:19,049] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:19,063] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run32
[2019-04-04 10:48:23,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:23,369] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:23,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run32
[2019-04-04 10:48:23,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:23,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:23,619] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run32
[2019-04-04 10:48:23,851] A3C_AGENT_WORKER-Thread-14 INFO:Local step 263000, global step 4213695: loss 0.2002
[2019-04-04 10:48:23,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 263000, global step 4213695: learning rate 0.0000
[2019-04-04 10:48:24,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:24,689] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:24,699] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run32
[2019-04-04 10:48:25,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:25,181] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:25,185] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run32
[2019-04-04 10:48:25,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:25,260] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:25,267] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run32
[2019-04-04 10:48:26,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:26,865] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:26,868] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run32
[2019-04-04 10:48:27,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:27,149] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:27,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run32
[2019-04-04 10:48:34,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:34,694] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:34,700] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run32
[2019-04-04 10:48:39,941] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.63061665e-14 4.03493428e-13 2.26642164e-24 4.76086134e-15
 1.31591075e-14 1.12402093e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:48:39,942] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6929
[2019-04-04 10:48:40,003] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.366666666666667, 86.0, 74.16666666666667, 0.0, 26.0, 24.50632140861189, 0.1667588362405483, 0.0, 1.0, 18738.43220984866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 52800.0000, 
sim time next is 53400.0000, 
raw observation next is [7.283333333333333, 86.0, 69.33333333333334, 0.0, 26.0, 24.49826226561425, 0.1622515556051287, 0.0, 1.0, 24364.11875460199], 
processed observation next is [0.0, 0.6086956521739131, 0.6643582640812559, 0.86, 0.23111111111111116, 0.0, 0.6666666666666666, 0.5415218554678543, 0.5540838518683763, 0.0, 1.0, 0.11601961311715234], 
reward next is 0.8840, 
noisyNet noise sample is [array([-0.4597735], dtype=float32), 0.2087596]. 
=============================================
[2019-04-04 10:48:41,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:48:41,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:48:41,795] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run32
[2019-04-04 10:48:51,731] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.4048412e-14 3.5308564e-13 5.6562037e-24 7.2422977e-15 8.5185511e-15
 2.9339964e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:48:51,733] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0009
[2019-04-04 10:48:51,784] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 0.0, 0.0, 26.0, 24.38571045113125, 0.1410487780067769, 0.0, 1.0, 44220.51236973788], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 255600.0000, 
sim time next is 256200.0000, 
raw observation next is [-4.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.35661873669568, 0.1320930022999356, 0.0, 1.0, 44256.90502000415], 
processed observation next is [1.0, 1.0, 0.3518005540166205, 0.8150000000000002, 0.0, 0.0, 0.6666666666666666, 0.5297182280579733, 0.5440310007666452, 0.0, 1.0, 0.21074716676192454], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.479741], dtype=float32), 0.6313139]. 
=============================================
[2019-04-04 10:48:55,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.1480356e-14 1.7466609e-13 2.2138767e-25 7.9326541e-15 2.1950008e-15
 3.3358199e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:48:55,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9424
[2019-04-04 10:48:55,403] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 70.0, 15.0, 205.5, 26.0, 24.41710840335185, 0.1191061538683925, 1.0, 1.0, 93580.93430375599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 288000.0000, 
sim time next is 288600.0000, 
raw observation next is [-12.71666666666667, 69.5, 20.0, 265.6666666666667, 26.0, 24.66669876104517, 0.1788278527283342, 1.0, 1.0, 86625.87730279981], 
processed observation next is [1.0, 0.34782608695652173, 0.1103416435826407, 0.695, 0.06666666666666667, 0.29355432780847146, 0.6666666666666666, 0.5555582300870974, 0.5596092842427781, 1.0, 1.0, 0.41250417763238006], 
reward next is 0.5875, 
noisyNet noise sample is [array([0.14559492], dtype=float32), 0.65371054]. 
=============================================
[2019-04-04 10:49:06,631] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.8253307e-15 2.2222487e-14 4.4323264e-25 6.1184763e-16 6.5610378e-16
 2.1867070e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:06,632] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8138
[2019-04-04 10:49:06,708] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.66558872402265, 0.2037345146213128, 1.0, 1.0, 108776.8538690467], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 502800.0000, 
sim time next is 503400.0000, 
raw observation next is [1.1, 96.0, 0.0, 0.0, 26.0, 24.74259195371506, 0.2149957148620994, 1.0, 1.0, 29895.83325642082], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.96, 0.0, 0.0, 0.6666666666666666, 0.5618826628095883, 0.5716652382873665, 1.0, 1.0, 0.14236111074486105], 
reward next is 0.8576, 
noisyNet noise sample is [array([-1.0301892], dtype=float32), -1.4162996]. 
=============================================
[2019-04-04 10:49:08,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3735428e-15 8.3230799e-14 2.3965491e-25 1.2745191e-15 5.2016251e-16
 3.7454305e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:08,576] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4457
[2019-04-04 10:49:08,623] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.8, 25.5, 124.3333333333333, 0.0, 26.0, 25.11429146993532, 0.1581820857759641, 1.0, 1.0, 35884.200516308], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 474600.0000, 
sim time next is 475200.0000, 
raw observation next is [-1.7, 25.0, 125.5, 0.0, 26.0, 25.14699155037999, 0.1672624198964032, 1.0, 1.0, 18707.68748912897], 
processed observation next is [1.0, 0.5217391304347826, 0.4155124653739613, 0.25, 0.41833333333333333, 0.0, 0.6666666666666666, 0.5955826291983325, 0.5557541399654677, 1.0, 1.0, 0.08908422613870938], 
reward next is 0.9109, 
noisyNet noise sample is [array([-0.9193867], dtype=float32), 0.54394054]. 
=============================================
[2019-04-04 10:49:09,952] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7499338e-15 6.5320475e-15 4.3334998e-26 3.4491380e-16 2.9622479e-16
 3.9543464e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:09,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8331
[2019-04-04 10:49:10,021] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-16.15, 85.5, 25.0, 477.0, 26.0, 24.29385977118951, 0.07869889706107712, 1.0, 1.0, 84926.35681455064], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 376200.0000, 
sim time next is 376800.0000, 
raw observation next is [-15.96666666666667, 87.0, 27.33333333333334, 520.5, 26.0, 24.70494592415706, 0.1476465443764829, 1.0, 1.0, 83906.18360625481], 
processed observation next is [1.0, 0.34782608695652173, 0.020313942751615764, 0.87, 0.09111111111111113, 0.5751381215469613, 0.6666666666666666, 0.5587454936797549, 0.549215514792161, 1.0, 1.0, 0.39955325526788005], 
reward next is 0.6004, 
noisyNet noise sample is [array([-0.46061394], dtype=float32), 0.9671551]. 
=============================================
[2019-04-04 10:49:13,383] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.3060341e-15 3.3503137e-13 5.8725539e-24 6.7666002e-15 3.8115051e-15
 4.0432618e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:13,388] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1562
[2019-04-04 10:49:13,445] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 37.66666666666667, 26.33333333333333, 504.3333333333333, 26.0, 25.48937205171736, 0.390125224903618, 1.0, 1.0, 188965.6461299974], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 403800.0000, 
sim time next is 404400.0000, 
raw observation next is [-8.900000000000002, 37.33333333333334, 23.66666666666666, 453.6666666666667, 26.0, 25.60099869448844, 0.448602468048809, 1.0, 1.0, 116910.4217151053], 
processed observation next is [1.0, 0.6956521739130435, 0.2160664819944598, 0.3733333333333334, 0.07888888888888887, 0.5012891344383057, 0.6666666666666666, 0.6334165578740366, 0.6495341560162696, 1.0, 1.0, 0.5567162938814538], 
reward next is 0.4433, 
noisyNet noise sample is [array([1.2410885], dtype=float32), -0.57754433]. 
=============================================
[2019-04-04 10:49:25,371] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0812235e-15 2.1800981e-14 2.2557584e-26 1.4509722e-16 8.2450648e-16
 3.0616678e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:25,371] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6783
[2019-04-04 10:49:25,438] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 96.0, 0.0, 0.0, 26.0, 24.82401837598023, 0.2205625526252598, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 504000.0000, 
sim time next is 504600.0000, 
raw observation next is [1.183333333333333, 96.0, 0.0, 0.0, 26.0, 24.86724553064154, 0.2133454683968596, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.96, 0.0, 0.0, 0.6666666666666666, 0.572270460886795, 0.5711151561322866, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44206166], dtype=float32), 0.07127348]. 
=============================================
[2019-04-04 10:49:27,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1086032e-13 3.5825214e-13 6.8117064e-25 2.5117375e-15 1.0271062e-14
 7.6801424e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:27,658] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1194
[2019-04-04 10:49:27,681] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.7000000000000001, 90.66666666666666, 0.0, 0.0, 26.0, 24.60730797059144, 0.1871438655123977, 0.0, 1.0, 40691.00647434547], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 542400.0000, 
sim time next is 543000.0000, 
raw observation next is [0.6, 91.33333333333334, 0.0, 0.0, 26.0, 24.64464358232878, 0.1802858441909396, 0.0, 1.0, 40778.80787523805], 
processed observation next is [0.0, 0.2608695652173913, 0.479224376731302, 0.9133333333333334, 0.0, 0.0, 0.6666666666666666, 0.5537202985273982, 0.5600952813969798, 0.0, 1.0, 0.1941847994058955], 
reward next is 0.8058, 
noisyNet noise sample is [array([1.2218101], dtype=float32), 0.30863762]. 
=============================================
[2019-04-04 10:49:27,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.65421]
 [80.56286]
 [80.45306]
 [80.37198]
 [80.31707]], R is [[80.73106384]
 [80.7299881 ]
 [80.72810364]
 [80.72507477]
 [80.723526  ]].
[2019-04-04 10:49:34,098] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3301222e-14 1.2379853e-12 4.3327601e-24 6.7580359e-15 1.1375710e-14
 8.6836441e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:34,099] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3661
[2019-04-04 10:49:34,134] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 60.0, 0.0, 0.0, 26.0, 25.01415549662105, 0.3138983219119877, 0.0, 1.0, 48549.39408051515], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 765600.0000, 
sim time next is 766200.0000, 
raw observation next is [-5.5, 60.5, 0.0, 0.0, 26.0, 24.99096304383018, 0.3062672255715344, 0.0, 1.0, 45529.57231842505], 
processed observation next is [1.0, 0.8695652173913043, 0.3102493074792244, 0.605, 0.0, 0.0, 0.6666666666666666, 0.5825802536525151, 0.6020890751905115, 0.0, 1.0, 0.21680748723059548], 
reward next is 0.7832, 
noisyNet noise sample is [array([-0.6185494], dtype=float32), 0.36255938]. 
=============================================
[2019-04-04 10:49:45,479] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0349783e-14 3.4047079e-14 3.4189143e-25 8.6823196e-16 1.3323768e-15
 8.0845647e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:45,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1485
[2019-04-04 10:49:45,549] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.7666666666666667, 81.33333333333333, 102.6666666666667, 222.0, 26.0, 24.82400684198906, 0.2864745626891941, 0.0, 1.0, 58377.82488479392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 561000.0000, 
sim time next is 561600.0000, 
raw observation next is [-0.8, 81.0, 109.5, 265.5, 26.0, 24.83448818837244, 0.3077090229455779, 0.0, 1.0, 44099.4407143953], 
processed observation next is [0.0, 0.5217391304347826, 0.4404432132963989, 0.81, 0.365, 0.29337016574585634, 0.6666666666666666, 0.5695406823643699, 0.6025696743151926, 0.0, 1.0, 0.2099973367352157], 
reward next is 0.7900, 
noisyNet noise sample is [array([-0.44022354], dtype=float32), 1.3530933]. 
=============================================
[2019-04-04 10:49:51,503] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [8.7231535e-16 2.2226473e-14 1.1339378e-26 9.0211563e-17 3.6544151e-16
 2.2403145e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:51,503] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5877
[2019-04-04 10:49:51,530] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 80.33333333333333, 0.0, 0.0, 26.0, 25.60738410258111, 0.4975163769737332, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1019400.0000, 
sim time next is 1020000.0000, 
raw observation next is [14.4, 79.66666666666667, 0.0, 0.0, 26.0, 25.49085621194359, 0.4781583396824387, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6242380176619658, 0.6593861132274795, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.005549], dtype=float32), 1.3887062]. 
=============================================
[2019-04-04 10:49:51,537] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[88.0561  ]
 [85.932236]
 [89.06112 ]
 [89.007965]
 [89.149345]], R is [[88.93516541]
 [89.04581451]
 [89.15535736]
 [89.26380157]
 [89.37116241]].
[2019-04-04 10:49:56,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0630179e-14 1.2103152e-13 7.8956878e-26 1.4709579e-15 6.0291157e-15
 2.2961382e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:49:56,340] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4722
[2019-04-04 10:49:56,355] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 24.43789615405709, 0.1560711570502064, 0.0, 1.0, 38573.80798759028], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 888000.0000, 
sim time next is 888600.0000, 
raw observation next is [0.0, 72.0, 0.0, 0.0, 26.0, 24.52043131390391, 0.1619086637764659, 0.0, 1.0, 38492.44582292824], 
processed observation next is [1.0, 0.2608695652173913, 0.46260387811634357, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5433692761586592, 0.5539695545921554, 0.0, 1.0, 0.18329736106156305], 
reward next is 0.8167, 
noisyNet noise sample is [array([0.13471068], dtype=float32), 0.9176059]. 
=============================================
[2019-04-04 10:50:01,252] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5411920e-15 1.1050259e-15 5.3914560e-28 3.6195068e-17 1.8761510e-16
 1.6892010e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:01,256] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0508
[2019-04-04 10:50:01,262] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 153.8333333333333, 0.0, 26.0, 25.04139162581322, 0.4976438229751518, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1172400.0000, 
sim time next is 1173000.0000, 
raw observation next is [18.3, 65.0, 148.6666666666667, 0.0, 26.0, 25.04541450978305, 0.4970647284405432, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.5652173913043478, 0.9695290858725764, 0.65, 0.4955555555555557, 0.0, 0.6666666666666666, 0.5871178758152542, 0.6656882428135144, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6240788], dtype=float32), -0.5639832]. 
=============================================
[2019-04-04 10:50:01,275] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[94.16562]
 [94.36352]
 [94.51696]
 [94.70198]
 [94.93093]], R is [[94.04163361]
 [94.10121918]
 [94.16020966]
 [94.21860504]
 [94.27642059]].
[2019-04-04 10:50:02,261] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0607935e-15 3.6540963e-15 5.6400762e-28 1.8181218e-17 2.0462613e-16
 3.3816206e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:02,269] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9362
[2019-04-04 10:50:02,281] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.0, 77.66666666666667, 0.0, 0.0, 26.0, 25.83368416428377, 0.5897466584698301, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1053600.0000, 
sim time next is 1054200.0000, 
raw observation next is [13.9, 77.83333333333333, 0.0, 0.0, 26.0, 25.76176403031337, 0.5770837577119172, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.847645429362881, 0.7783333333333333, 0.0, 0.0, 0.6666666666666666, 0.6468136691927807, 0.6923612525706391, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.9099708], dtype=float32), -0.2998444]. 
=============================================
[2019-04-04 10:50:02,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3936456e-14 5.8979722e-15 5.3858645e-28 1.6425684e-16 3.2883542e-16
 6.6081462e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:02,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0294
[2019-04-04 10:50:02,492] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.5, 93.0, 0.0, 0.0, 26.0, 23.89170657287748, 0.2291829165525779, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1221000.0000, 
sim time next is 1221600.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 26.0, 23.89113261981763, 0.2252464918099679, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.6666666666666666, 0.49092771831813575, 0.575082163936656, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.04109556], dtype=float32), 0.22809283]. 
=============================================
[2019-04-04 10:50:06,422] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.7352267e-16 8.1200288e-15 5.5467071e-28 4.3835815e-17 1.3312381e-16
 2.0285379e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:06,424] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5345
[2019-04-04 10:50:06,435] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 75.0, 0.0, 0.0, 26.0, 25.8947498897097, 0.6299980311467492, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1029600.0000, 
sim time next is 1030200.0000, 
raw observation next is [14.4, 75.0, 0.0, 0.0, 26.0, 25.97011176552465, 0.6272501117463283, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.8614958448753465, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6641759804603874, 0.7090833705821095, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.24868637], dtype=float32), -0.5151254]. 
=============================================
[2019-04-04 10:50:19,856] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0541537e-15 1.0801741e-14 3.4385843e-26 1.8983751e-16 2.7868750e-16
 1.5364818e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:19,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9242
[2019-04-04 10:50:19,873] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 75.0, 0.0, 26.0, 25.90213424848336, 0.4968286442406799, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1420800.0000, 
sim time next is 1421400.0000, 
raw observation next is [0.0, 95.0, 78.0, 0.0, 26.0, 25.87647040517693, 0.496283329161858, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.26, 0.0, 0.6666666666666666, 0.6563725337647442, 0.665427776387286, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.68808573], dtype=float32), 0.23623078]. 
=============================================
[2019-04-04 10:50:30,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.4492762e-15 2.2579503e-14 3.2244770e-26 3.2005111e-16 2.2029362e-15
 4.7578221e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:30,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6003
[2019-04-04 10:50:30,945] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 94.0, 0.0, 0.0, 26.0, 25.5197794881064, 0.5890640326965645, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1298400.0000, 
sim time next is 1299000.0000, 
raw observation next is [3.9, 93.5, 0.0, 0.0, 26.0, 25.52391706347978, 0.5859163759186031, 0.0, 1.0, 18748.8218485078], 
processed observation next is [1.0, 0.0, 0.5706371191135734, 0.935, 0.0, 0.0, 0.6666666666666666, 0.6269930886233149, 0.6953054586395343, 0.0, 1.0, 0.08928010404051333], 
reward next is 0.9107, 
noisyNet noise sample is [array([-0.4927301], dtype=float32), 0.71088237]. 
=============================================
[2019-04-04 10:50:30,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.22861 ]
 [85.53714 ]
 [85.847336]
 [86.099724]
 [86.339806]], R is [[85.03351593]
 [85.18318176]
 [85.33135223]
 [85.28528595]
 [85.22135925]].
[2019-04-04 10:50:39,660] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.4800135e-14 7.4189420e-14 5.2446408e-24 6.4667920e-15 4.9059013e-15
 1.6066007e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:39,660] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8836
[2019-04-04 10:50:39,715] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 83.0, 129.0, 42.0, 26.0, 25.05681356324504, 0.2856416625221069, 0.0, 1.0, 27582.27758590728], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1868400.0000, 
sim time next is 1869000.0000, 
raw observation next is [-4.5, 81.66666666666667, 110.0, 27.99999999999999, 26.0, 25.07948981906921, 0.2776306576923559, 0.0, 1.0, 21307.94226258704], 
processed observation next is [0.0, 0.6521739130434783, 0.3379501385041552, 0.8166666666666668, 0.36666666666666664, 0.030939226519337004, 0.6666666666666666, 0.5899574849224342, 0.5925435525641186, 0.0, 1.0, 0.10146639172660495], 
reward next is 0.8985, 
noisyNet noise sample is [array([1.2005491], dtype=float32), 0.9622835]. 
=============================================
[2019-04-04 10:50:39,719] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[79.00567 ]
 [79.66015 ]
 [80.39422 ]
 [80.990456]
 [80.87759 ]], R is [[78.60559082]
 [78.68818665]
 [78.71302032]
 [78.7204361 ]
 [78.71749115]].
[2019-04-04 10:50:47,069] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2552539e-13 8.2110534e-13 4.2724269e-24 1.0298801e-14 2.7886857e-14
 1.0574113e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:47,070] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6657
[2019-04-04 10:50:47,091] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 91.0, 0.0, 0.0, 26.0, 25.23739481876392, 0.4242200163001235, 0.0, 1.0, 43011.64622837646], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1738800.0000, 
sim time next is 1739400.0000, 
raw observation next is [-0.09999999999999999, 90.33333333333334, 0.0, 0.0, 26.0, 25.21365418309197, 0.4197741953802738, 0.0, 1.0, 43043.18319270705], 
processed observation next is [0.0, 0.13043478260869565, 0.4598337950138504, 0.9033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6011378485909974, 0.6399247317934246, 0.0, 1.0, 0.20496753901289072], 
reward next is 0.7950, 
noisyNet noise sample is [array([-0.24327555], dtype=float32), -1.6160464]. 
=============================================
[2019-04-04 10:50:48,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4268193e-15 2.7021137e-14 9.5667341e-25 1.1969575e-15 4.0581134e-16
 3.0080572e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:48,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0187
[2019-04-04 10:50:48,121] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 88.0, 90.0, 0.0, 26.0, 26.27104727888874, 0.6224099630330994, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1692000.0000, 
sim time next is 1692600.0000, 
raw observation next is [1.1, 88.00000000000001, 86.66666666666667, 0.0, 26.0, 26.27952804149711, 0.6241519293494594, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.8800000000000001, 0.2888888888888889, 0.0, 0.6666666666666666, 0.689960670124759, 0.7080506431164865, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2952033], dtype=float32), -0.87968165]. 
=============================================
[2019-04-04 10:50:55,586] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3870127e-14 4.3806400e-13 7.2425130e-24 5.1311520e-15 1.8714495e-14
 1.9479511e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:55,586] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2872
[2019-04-04 10:50:55,645] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 152.0, 40.5, 26.0, 24.98434372201326, 0.2632273082413294, 0.0, 1.0, 30976.70338443068], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1857600.0000, 
sim time next is 1858200.0000, 
raw observation next is [-4.916666666666667, 71.0, 141.3333333333333, 26.99999999999999, 26.0, 24.99251926093111, 0.2586646918282045, 0.0, 1.0, 32995.61813767938], 
processed observation next is [0.0, 0.5217391304347826, 0.32640812557710064, 0.71, 0.471111111111111, 0.029834254143646398, 0.6666666666666666, 0.5827099384109259, 0.5862215639427348, 0.0, 1.0, 0.15712199113180658], 
reward next is 0.8429, 
noisyNet noise sample is [array([-0.8377606], dtype=float32), -1.144251]. 
=============================================
[2019-04-04 10:50:56,452] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5418287e-13 4.6670019e-13 2.3722286e-24 7.9201367e-15 1.9872445e-14
 7.8295285e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:50:56,452] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5551
[2019-04-04 10:50:56,588] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.75, 84.5, 0.0, 0.0, 26.0, 25.01824448817991, 0.2478340842463581, 0.0, 1.0, 38676.10905627751], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1881000.0000, 
sim time next is 1881600.0000, 
raw observation next is [-4.666666666666667, 84.0, 0.0, 0.0, 26.0, 25.03393117210714, 0.2448641321437929, 0.0, 1.0, 30835.66562559727], 
processed observation next is [0.0, 0.782608695652174, 0.3333333333333333, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5861609310089282, 0.5816213773812643, 0.0, 1.0, 0.1468365029790346], 
reward next is 0.8532, 
noisyNet noise sample is [array([1.5948911], dtype=float32), -0.7065793]. 
=============================================
[2019-04-04 10:51:08,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.97831931e-14 5.30543003e-13 1.44382717e-24 2.31169776e-15
 1.08283515e-14 1.82275468e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:51:08,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0491
[2019-04-04 10:51:08,363] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 78.83333333333333, 0.0, 0.0, 26.0, 25.2283603001295, 0.3738151513143783, 0.0, 1.0, 43894.48435728387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1977000.0000, 
sim time next is 1977600.0000, 
raw observation next is [-5.8, 79.66666666666667, 0.0, 0.0, 26.0, 25.2124321304205, 0.3686245488348447, 0.0, 1.0, 43684.57236831055], 
processed observation next is [1.0, 0.9130434782608695, 0.30193905817174516, 0.7966666666666667, 0.0, 0.0, 0.6666666666666666, 0.6010360108683749, 0.6228748496116149, 0.0, 1.0, 0.2080217731824312], 
reward next is 0.7920, 
noisyNet noise sample is [array([-0.4715831], dtype=float32), -1.2065111]. 
=============================================
[2019-04-04 10:51:10,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3577373e-13 1.0135585e-12 4.2328442e-24 1.7489136e-14 4.0264911e-14
 6.3118275e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:51:10,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5382
[2019-04-04 10:51:10,136] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.79955480443573, 0.0190198850787707, 0.0, 1.0, 43439.39486182722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2098800.0000, 
sim time next is 2099400.0000, 
raw observation next is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.77552735957714, 0.01076023100295946, 0.0, 1.0, 43331.41048358567], 
processed observation next is [1.0, 0.30434782608695654, 0.2742382271468144, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.4812939466314283, 0.5035867436676532, 0.0, 1.0, 0.20634004992183652], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.6302263], dtype=float32), -1.2773912]. 
=============================================
[2019-04-04 10:51:32,826] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [4.6202439e-14 6.4647018e-13 1.0489861e-24 8.0279072e-15 7.4860298e-15
 3.2646301e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:51:32,826] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0727
[2019-04-04 10:51:32,957] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.27123554819115, -0.02785005583528972, 1.0, 1.0, 202391.001403318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2272800.0000, 
sim time next is 2273400.0000, 
raw observation next is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.59915514780625, 0.09175488005604204, 1.0, 1.0, 202977.48468661], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.91, 0.0, 0.0, 0.6666666666666666, 0.46659626231718754, 0.5305849600186807, 1.0, 1.0, 0.966559450888619], 
reward next is 0.0334, 
noisyNet noise sample is [array([-0.20134176], dtype=float32), 0.64453846]. 
=============================================
[2019-04-04 10:51:52,835] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.1891525e-14 8.2562505e-13 1.1946711e-24 1.9546037e-14 3.6540913e-14
 5.9694475e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:51:52,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4714
[2019-04-04 10:51:52,855] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 39.66666666666667, 0.0, 0.0, 26.0, 25.20723954372462, 0.2248363180449495, 0.0, 1.0, 39132.8486542027], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2508600.0000, 
sim time next is 2509200.0000, 
raw observation next is [-1.7, 40.0, 0.0, 0.0, 26.0, 25.14670300291356, 0.2158134236149967, 0.0, 1.0, 39139.22048356795], 
processed observation next is [1.0, 0.043478260869565216, 0.4155124653739613, 0.4, 0.0, 0.0, 0.6666666666666666, 0.5955585835761301, 0.5719378078716656, 0.0, 1.0, 0.18637724039794262], 
reward next is 0.8136, 
noisyNet noise sample is [array([0.46886617], dtype=float32), 0.6027073]. 
=============================================
[2019-04-04 10:51:58,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.58892963e-16 1.39235166e-14 1.00382834e-25 2.83311689e-16
 4.31990509e-16 6.43530759e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 10:51:58,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1685
[2019-04-04 10:51:58,402] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.733333333333333, 51.66666666666667, 241.5, 151.0, 26.0, 25.77486686494223, 0.3909109622776804, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2636400.0000, 
sim time next is 2637000.0000, 
raw observation next is [-1.45, 50.5, 245.0, 147.0, 26.0, 25.71924457396132, 0.2802656686980838, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.422437673130194, 0.505, 0.8166666666666667, 0.16243093922651933, 0.6666666666666666, 0.6432703811634433, 0.5934218895660279, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7138802], dtype=float32), 0.8438464]. 
=============================================
[2019-04-04 10:51:58,415] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[84.41928]
 [84.61439]
 [84.86096]
 [85.1134 ]
 [85.34294]], R is [[84.40895844]
 [84.56487274]
 [84.71922302]
 [84.87203217]
 [85.02331543]].
[2019-04-04 10:52:01,451] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6873017e-15 5.6787606e-14 3.4534800e-25 8.5280733e-16 8.9915110e-16
 2.8189716e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:01,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2058
[2019-04-04 10:52:01,512] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.45, 50.5, 245.0, 147.0, 26.0, 25.71924457396132, 0.2802656686980838, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2637000.0000, 
sim time next is 2637600.0000, 
raw observation next is [-1.166666666666667, 49.33333333333334, 231.5, 157.6666666666667, 26.0, 25.08760560502623, 0.3138641321103695, 1.0, 1.0, 132687.4598448733], 
processed observation next is [1.0, 0.5217391304347826, 0.43028624192059095, 0.4933333333333334, 0.7716666666666666, 0.17421731123388587, 0.6666666666666666, 0.5906338004188525, 0.6046213773701231, 1.0, 1.0, 0.631845046880349], 
reward next is 0.3682, 
noisyNet noise sample is [array([0.82271016], dtype=float32), 0.12270136]. 
=============================================
[2019-04-04 10:52:04,080] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8874868e-14 3.3855031e-13 2.4525875e-24 8.2212001e-15 8.5104636e-15
 8.3655138e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:04,081] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1044
[2019-04-04 10:52:04,101] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 33.66666666666667, 0.0, 0.0, 26.0, 25.22301328189358, 0.2547206939503747, 0.0, 1.0, 40307.01165886742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2499600.0000, 
sim time next is 2500200.0000, 
raw observation next is [-0.8999999999999999, 34.0, 0.0, 0.0, 26.0, 25.20083063640665, 0.2509575226098986, 0.0, 1.0, 40297.89767341728], 
processed observation next is [0.0, 0.9565217391304348, 0.43767313019390586, 0.34, 0.0, 0.0, 0.6666666666666666, 0.6000692197005542, 0.5836525075366329, 0.0, 1.0, 0.19189475082579657], 
reward next is 0.8081, 
noisyNet noise sample is [array([-1.996895], dtype=float32), 0.5945068]. 
=============================================
[2019-04-04 10:52:07,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4224054e-13 9.0721804e-13 2.7595386e-24 3.1065567e-14 4.9066338e-14
 1.2422997e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:07,529] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9267
[2019-04-04 10:52:07,648] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.55311439022909, 0.2481919612132223, 0.0, 1.0, 42659.14484397945], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2950200.0000, 
sim time next is 2950800.0000, 
raw observation next is [-3.0, 84.0, 0.0, 0.0, 26.0, 24.52228707786328, 0.239517921627619, 0.0, 1.0, 42679.31390581978], 
processed observation next is [0.0, 0.13043478260869565, 0.3795013850415513, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5435239231552732, 0.5798393072092064, 0.0, 1.0, 0.20323482812295132], 
reward next is 0.7968, 
noisyNet noise sample is [array([1.0155363], dtype=float32), -0.104589455]. 
=============================================
[2019-04-04 10:52:10,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5384977e-15 1.6322397e-13 5.3724327e-25 1.8211223e-15 2.4149246e-15
 8.6062038e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:10,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0349
[2019-04-04 10:52:10,330] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.833333333333333, 28.33333333333334, 26.99999999999999, 56.0, 26.0, 25.75972350954894, 0.3971785715792159, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [5.666666666666666, 28.66666666666667, 16.0, 51.0, 26.0, 25.82055846524057, 0.3767408953169065, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6195752539242845, 0.28666666666666674, 0.05333333333333334, 0.056353591160221, 0.6666666666666666, 0.6517132054367142, 0.6255802984389688, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.82419413], dtype=float32), 0.9601929]. 
=============================================
[2019-04-04 10:52:24,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1893301e-15 9.3351969e-14 4.1718781e-25 9.0884126e-16 1.5012752e-15
 1.4223374e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:24,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4576
[2019-04-04 10:52:24,126] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 95.33333333333334, 85.5, 0.0, 26.0, 25.43076684893793, 0.3104217401576554, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2892000.0000, 
sim time next is 2892600.0000, 
raw observation next is [1.0, 96.5, 87.0, 0.0, 26.0, 25.39284475769356, 0.3131484124617578, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.4782608695652174, 0.4903047091412743, 0.965, 0.29, 0.0, 0.6666666666666666, 0.6160703964744633, 0.6043828041539193, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([0.2838552], dtype=float32), -1.3536627]. 
=============================================
[2019-04-04 10:52:30,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.6457561e-14 5.4725896e-13 1.0886576e-23 6.2319009e-15 2.7625649e-14
 1.1204382e-15 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:30,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0204
[2019-04-04 10:52:30,052] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 85.0, 0.0, 0.0, 26.0, 25.3847331698079, 0.4872161725607844, 0.0, 1.0, 39295.22201295221], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2926800.0000, 
sim time next is 2927400.0000, 
raw observation next is [-1.0, 83.83333333333334, 0.0, 0.0, 26.0, 25.41389621686639, 0.4858330161022935, 0.0, 1.0, 25620.91099104293], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.8383333333333334, 0.0, 0.0, 0.6666666666666666, 0.6178246847388659, 0.6619443387007645, 0.0, 1.0, 0.12200433805258537], 
reward next is 0.8780, 
noisyNet noise sample is [array([-0.6237265], dtype=float32), 0.87678355]. 
=============================================
[2019-04-04 10:52:44,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.2895873e-14 2.3669356e-13 1.4655288e-25 8.3134776e-15 9.5462587e-15
 2.1873420e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:44,911] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9883
[2019-04-04 10:52:45,073] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 62.33333333333334, 69.33333333333334, 310.8333333333334, 26.0, 23.66078790069373, 0.07859192734328342, 0.0, 1.0, 202451.661015601], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3054000.0000, 
sim time next is 3054600.0000, 
raw observation next is [-6.0, 61.5, 83.0, 359.0, 26.0, 24.05589178567708, 0.2163241350659272, 0.0, 1.0, 201798.4279855502], 
processed observation next is [0.0, 0.34782608695652173, 0.296398891966759, 0.615, 0.27666666666666667, 0.3966850828729282, 0.6666666666666666, 0.5046576488064233, 0.5721080450219758, 0.0, 1.0, 0.9609448951692866], 
reward next is 0.0391, 
noisyNet noise sample is [array([-1.3834246], dtype=float32), 0.17487876]. 
=============================================
[2019-04-04 10:52:50,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5361325e-15 2.5443921e-14 2.3941916e-25 9.8242185e-16 1.3665199e-15
 4.3243522e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:52:50,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4443
[2019-04-04 10:52:50,053] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.60528811520976, 0.6151634927576999, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3195600.0000, 
sim time next is 3196200.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.58491608623974, 0.6035299880582771, 0.0, 1.0, 18738.23346040317], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6320763405199784, 0.7011766626860924, 0.0, 1.0, 0.089229683144777], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.7562623], dtype=float32), -0.64620376]. 
=============================================
[2019-04-04 10:52:57,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.12169185e-14 4.18628321e-13 4.06241623e-25 6.28876181e-15
 1.02330101e-14 1.89041872e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 10:52:57,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8977
[2019-04-04 10:52:57,857] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.333333333333334, 67.0, 0.0, 0.0, 26.0, 25.44262171320674, 0.449686090780377, 0.0, 1.0, 34404.39755344184], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3363600.0000, 
sim time next is 3364200.0000, 
raw observation next is [-4.5, 68.0, 0.0, 0.0, 26.0, 25.40878004174985, 0.4362064425398473, 0.0, 1.0, 53609.0351106581], 
processed observation next is [1.0, 0.9565217391304348, 0.3379501385041552, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6173983368124875, 0.6454021475132824, 0.0, 1.0, 0.2552811195745624], 
reward next is 0.7447, 
noisyNet noise sample is [array([-0.27003857], dtype=float32), -0.6230157]. 
=============================================
[2019-04-04 10:52:58,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.33793928e-14 7.13015598e-14 5.81528143e-25 9.29279952e-16
 5.33022375e-15 1.28668656e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 10:52:58,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0440
[2019-04-04 10:52:58,506] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 95.33333333333334, 0.0, 0.0, 26.0, 25.40295647485065, 0.5980507637520331, 0.0, 1.0, 186166.4554376069], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3192000.0000, 
sim time next is 3192600.0000, 
raw observation next is [2.0, 94.16666666666666, 0.0, 0.0, 26.0, 25.37511262982442, 0.6185558689948861, 0.0, 1.0, 118022.447802358], 
processed observation next is [1.0, 0.9565217391304348, 0.518005540166205, 0.9416666666666665, 0.0, 0.0, 0.6666666666666666, 0.6145927191520352, 0.706185289664962, 0.0, 1.0, 0.5620116562017048], 
reward next is 0.4380, 
noisyNet noise sample is [array([0.7692774], dtype=float32), 0.36815926]. 
=============================================
[2019-04-04 10:52:59,031] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-04 10:52:59,052] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 10:52:59,052] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:52:59,054] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run44
[2019-04-04 10:52:59,088] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 10:52:59,088] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:52:59,090] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 10:52:59,090] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:52:59,091] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run44
[2019-04-04 10:52:59,122] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run44
[2019-04-04 10:56:01,695] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 10:56:34,499] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 10:56:44,318] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 10:56:45,354] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 4300000, evaluation results [4300000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 10:56:49,219] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.4866036e-14 1.5223088e-13 5.8228284e-25 1.0548938e-15 1.2397728e-14
 2.7398078e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:56:49,219] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6825
[2019-04-04 10:56:49,307] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 84.83333333333334, 0.0, 0.0, 26.0, 25.64799717443238, 0.545766317831954, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3448200.0000, 
sim time next is 3448800.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.66927088300581, 0.5360338608149399, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.639105906917151, 0.67867795360498, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.721819], dtype=float32), 0.29505938]. 
=============================================
[2019-04-04 10:56:49,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3080421e-14 2.2326077e-13 1.6899951e-25 4.9710259e-15 4.1869817e-15
 2.0063145e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:56:49,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6533
[2019-04-04 10:56:49,906] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 25.51442090982771, 0.6371267849399952, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3193800.0000, 
sim time next is 3194400.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 25.56634244689427, 0.6325458203339445, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 1.0, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.6305285372411893, 0.7108486067779815, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03747471], dtype=float32), -1.461281]. 
=============================================
[2019-04-04 10:56:54,091] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9323442e-14 1.2388375e-13 7.1732478e-25 1.3786148e-15 2.1680219e-15
 6.0927603e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:56:54,091] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2688
[2019-04-04 10:56:54,108] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 100.0, 0.0, 0.0, 26.0, 25.67056835549376, 0.6506129681696556, 0.0, 1.0, 196217.9094192961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3183600.0000, 
sim time next is 3184200.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.59873587891578, 0.6658584801243972, 0.0, 1.0, 196443.7646087797], 
processed observation next is [1.0, 0.8695652173913043, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6332279899096482, 0.7219528267081324, 0.0, 1.0, 0.9354464981370462], 
reward next is 0.0646, 
noisyNet noise sample is [array([-1.407584], dtype=float32), 1.803566]. 
=============================================
[2019-04-04 10:56:54,547] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.37607778e-14 1.98389387e-13 5.64008479e-25 2.83865728e-15
 1.12210275e-14 2.30892794e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 10:56:54,548] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9368
[2019-04-04 10:56:54,589] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.833333333333333, 90.16666666666667, 0.0, 0.0, 26.0, 25.47357703164096, 0.5392773512662085, 0.0, 1.0, 85564.41175837441], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3275400.0000, 
sim time next is 3276000.0000, 
raw observation next is [-6.0, 92.0, 0.0, 0.0, 26.0, 25.42866328382127, 0.5417853259457495, 0.0, 1.0, 83301.59557899495], 
processed observation next is [1.0, 0.9565217391304348, 0.296398891966759, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6190552736517724, 0.6805951086485832, 0.0, 1.0, 0.3966742646618807], 
reward next is 0.6033, 
noisyNet noise sample is [array([0.83307356], dtype=float32), 0.6233491]. 
=============================================
[2019-04-04 10:56:54,611] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[80.426895]
 [80.20103 ]
 [80.11641 ]
 [80.27131 ]
 [80.35486 ]], R is [[80.50592804]
 [80.29341888]
 [80.19720459]
 [80.30603027]
 [80.50296783]].
[2019-04-04 10:56:55,393] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.8816525e-14 4.0845531e-13 1.5782629e-24 3.6397810e-15 7.5981481e-15
 2.2023795e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:56:55,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7959
[2019-04-04 10:56:55,450] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.75, 77.0, 0.0, 0.0, 26.0, 24.72026030279573, 0.2874264433833935, 0.0, 1.0, 43879.22731328561], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3297000.0000, 
sim time next is 3297600.0000, 
raw observation next is [-8.9, 77.0, 0.0, 0.0, 26.0, 24.76195458024647, 0.2665391113849586, 0.0, 1.0, 44206.4252363448], 
processed observation next is [1.0, 0.17391304347826086, 0.21606648199445982, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5634962150205393, 0.5888463704616529, 0.0, 1.0, 0.21050678683973717], 
reward next is 0.7895, 
noisyNet noise sample is [array([-1.1409221], dtype=float32), -0.38951612]. 
=============================================
[2019-04-04 10:57:03,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5780285e-16 1.8367153e-15 7.8568343e-27 3.7102773e-17 9.7845679e-17
 5.7535592e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:03,774] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9769
[2019-04-04 10:57:03,785] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 113.0, 806.0, 26.0, 26.07264829613202, 0.5982876086554513, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3418200.0000, 
sim time next is 3418800.0000, 
raw observation next is [3.0, 49.0, 111.3333333333333, 800.8333333333334, 26.0, 26.29847878225935, 0.6139510776248985, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5457063711911359, 0.49, 0.371111111111111, 0.8848987108655617, 0.6666666666666666, 0.6915398985216127, 0.7046503592082995, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.22459695], dtype=float32), 0.7587122]. 
=============================================
[2019-04-04 10:57:06,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9308549e-14 1.6374345e-13 1.6607758e-25 1.4861130e-15 6.8522674e-15
 3.0864359e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:06,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9104
[2019-04-04 10:57:06,061] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 26.0, 25.29382155638898, 0.3977736018850613, 0.0, 1.0, 61638.07974194701], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3471600.0000, 
sim time next is 3472200.0000, 
raw observation next is [0.5, 69.5, 0.0, 0.0, 26.0, 25.2684955938171, 0.3958823247713399, 0.0, 1.0, 48435.90254868819], 
processed observation next is [1.0, 0.17391304347826086, 0.4764542936288089, 0.695, 0.0, 0.0, 0.6666666666666666, 0.605707966151425, 0.63196077492378, 0.0, 1.0, 0.23064715499375327], 
reward next is 0.7694, 
noisyNet noise sample is [array([0.4736543], dtype=float32), -0.95248073]. 
=============================================
[2019-04-04 10:57:07,969] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.0764236e-15 3.9544150e-14 5.3935597e-26 7.3147611e-16 6.0873966e-16
 6.0109643e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:07,969] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4759
[2019-04-04 10:57:07,989] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.25222654712269, 0.4386191300512376, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3438600.0000, 
sim time next is 3439200.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.15794340859629, 0.4202834500044692, 0.0, 1.0, 39121.35633113418], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5964952840496908, 0.6400944833348231, 0.0, 1.0, 0.18629217300540085], 
reward next is 0.8137, 
noisyNet noise sample is [array([0.75002867], dtype=float32), -1.4846714]. 
=============================================
[2019-04-04 10:57:09,454] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2142854e-14 2.7935743e-13 1.6856491e-25 1.7033625e-15 9.0152825e-15
 6.6957756e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:09,459] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5916
[2019-04-04 10:57:09,494] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.55508375939783, 0.4575419545566774, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3462000.0000, 
sim time next is 3462600.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 25.56379045140961, 0.4486918080607731, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.6303158709508008, 0.6495639360202577, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.504212], dtype=float32), 0.18217584]. 
=============================================
[2019-04-04 10:57:10,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2925543e-14 1.4148615e-13 4.1469989e-25 4.2916068e-15 2.3060080e-15
 3.9473704e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:10,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2507
[2019-04-04 10:57:10,440] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 76.0, 0.0, 0.0, 26.0, 25.70721875866396, 0.5073779360134476, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3535800.0000, 
sim time next is 3536400.0000, 
raw observation next is [-1.0, 74.0, 0.0, 0.0, 26.0, 25.68242989942167, 0.4713688236872011, 0.0, 1.0, 21402.0364450249], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6402024916184725, 0.657122941229067, 0.0, 1.0, 0.10191445926202333], 
reward next is 0.8981, 
noisyNet noise sample is [array([0.9577283], dtype=float32), -0.42451888]. 
=============================================
[2019-04-04 10:57:12,641] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.9548974e-15 3.0894205e-14 9.3180977e-26 4.0884597e-16 1.2363859e-15
 8.6783373e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:12,643] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0646
[2019-04-04 10:57:12,655] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 43.0, 108.5, 797.0, 26.0, 25.3234273494535, 0.4680431926033817, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3679200.0000, 
sim time next is 3679800.0000, 
raw observation next is [6.0, 43.66666666666667, 107.0, 790.0, 26.0, 25.37336021540277, 0.4694607535598136, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.6288088642659281, 0.4366666666666667, 0.3566666666666667, 0.8729281767955801, 0.6666666666666666, 0.6144466846168974, 0.6564869178532712, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08651886], dtype=float32), 0.353778]. 
=============================================
[2019-04-04 10:57:13,281] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4831391e-14 9.0584990e-14 1.3014765e-24 4.5894570e-15 5.1317199e-15
 6.0956891e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:13,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7638
[2019-04-04 10:57:13,289] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.0, 59.0, 15.0, 165.0, 26.0, 25.30659093338998, 0.3902015901878302, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3691800.0000, 
sim time next is 3692400.0000, 
raw observation next is [4.0, 59.0, 12.5, 137.5, 26.0, 25.26085264557643, 0.375156519071405, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.7391304347826086, 0.5734072022160666, 0.59, 0.041666666666666664, 0.15193370165745856, 0.6666666666666666, 0.6050710537980359, 0.6250521730238017, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9022044], dtype=float32), 0.8855283]. 
=============================================
[2019-04-04 10:57:25,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0423574e-15 5.3896202e-15 1.1820540e-26 1.7766226e-16 1.1548009e-16
 3.1155698e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:25,787] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9181
[2019-04-04 10:57:25,812] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 77.0, 103.6666666666667, 706.3333333333333, 26.0, 26.17058385609787, 0.5053080323227028, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3750600.0000, 
sim time next is 3751200.0000, 
raw observation next is [-3.0, 77.0, 105.5, 722.0, 26.0, 26.22051434788353, 0.5171650147689039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3795013850415513, 0.77, 0.3516666666666667, 0.7977900552486188, 0.6666666666666666, 0.6850428623236274, 0.6723883382563013, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.0691797], dtype=float32), 2.0182912]. 
=============================================
[2019-04-04 10:57:27,976] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.26042063e-16 1.37108164e-14 1.59311816e-25 5.95357753e-16
 5.68881515e-16 2.99187402e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 10:57:27,976] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6401
[2019-04-04 10:57:27,997] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 36.0, 66.0, 536.0, 26.0, 27.05341432342756, 0.7517978244821943, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3947400.0000, 
sim time next is 3948000.0000, 
raw observation next is [-4.666666666666666, 36.66666666666666, 58.16666666666666, 474.8333333333334, 26.0, 26.42483807600421, 0.7042600682467408, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.33333333333333337, 0.3666666666666666, 0.19388888888888886, 0.5246777163904237, 0.6666666666666666, 0.7020698396670175, 0.7347533560822469, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.6166325], dtype=float32), 0.50557876]. 
=============================================
[2019-04-04 10:57:28,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[86.046135]
 [86.34653 ]
 [86.835915]
 [87.05195 ]
 [87.27819 ]], R is [[85.50624084]
 [85.65117645]
 [85.79466248]
 [85.93671417]
 [86.0773468 ]].
[2019-04-04 10:57:31,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6519706e-14 1.2624572e-13 8.3396077e-25 1.1928960e-15 9.7012149e-15
 7.9618410e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:31,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4666
[2019-04-04 10:57:31,422] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.30862239525782, 0.4026604670861376, 0.0, 1.0, 44162.28474514392], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3807000.0000, 
sim time next is 3807600.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.3331287410855, 0.4078558128378999, 0.0, 1.0, 44115.14999956332], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6110940617571249, 0.6359519376126334, 0.0, 1.0, 0.21007214285506343], 
reward next is 0.7899, 
noisyNet noise sample is [array([0.5784227], dtype=float32), -0.6775827]. 
=============================================
[2019-04-04 10:57:33,822] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0867082e-16 1.1018149e-14 3.7514188e-26 3.0107453e-16 8.2090946e-17
 9.8873281e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:33,823] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4871
[2019-04-04 10:57:33,843] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 21.33333333333334, 85.33333333333334, 684.6666666666667, 26.0, 27.15256256693188, 0.783401847490123, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4030800.0000, 
sim time next is 4031400.0000, 
raw observation next is [-1.166666666666667, 21.66666666666667, 81.66666666666667, 657.3333333333334, 26.0, 27.18423004016107, 0.7879877269506745, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.43028624192059095, 0.2166666666666667, 0.27222222222222225, 0.7263351749539595, 0.6666666666666666, 0.7653525033467558, 0.7626625756502249, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.195943], dtype=float32), 0.9779684]. 
=============================================
[2019-04-04 10:57:46,940] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5684829e-15 8.4398105e-14 2.3917999e-25 1.2811828e-15 5.5837227e-16
 1.7722828e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:46,941] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8406
[2019-04-04 10:57:46,944] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4557417e-14 2.5519414e-13 9.6021993e-25 1.3706704e-15 4.4077151e-15
 1.2897530e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:46,944] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8903
[2019-04-04 10:57:46,957] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 38.0, 0.0, 0.0, 26.0, 25.49776095651142, 0.5297698452555641, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4129800.0000, 
sim time next is 4130400.0000, 
raw observation next is [2.333333333333333, 39.0, 0.0, 0.0, 26.0, 25.59498313109998, 0.5251301235006237, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5272391505078486, 0.39, 0.0, 0.0, 0.6666666666666666, 0.6329152609249983, 0.6750433745002079, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.35540733], dtype=float32), 0.34949276]. 
=============================================
[2019-04-04 10:57:46,964] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 37.33333333333334, 0.0, 0.0, 26.0, 25.92050040810451, 0.5908294607137937, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4137600.0000, 
sim time next is 4138200.0000, 
raw observation next is [1.0, 38.0, 0.0, 0.0, 26.0, 25.93830672474444, 0.5846819700331713, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4903047091412743, 0.38, 0.0, 0.0, 0.6666666666666666, 0.6615255603953699, 0.6948939900110571, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.3774821], dtype=float32), -0.93854797]. 
=============================================
[2019-04-04 10:57:47,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6947184e-14 2.0752044e-13 2.1756257e-24 8.6428684e-15 2.0947852e-14
 4.4081481e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:47,800] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1012
[2019-04-04 10:57:47,821] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 41.5, 0.0, 0.0, 26.0, 25.38724420683905, 0.4341621081977253, 0.0, 1.0, 60733.81480205471], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4147800.0000, 
sim time next is 4148400.0000, 
raw observation next is [-1.0, 41.0, 0.0, 0.0, 26.0, 25.37455134778207, 0.4340385280483436, 0.0, 1.0, 54887.67942544416], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.41, 0.0, 0.0, 0.6666666666666666, 0.6145459456485058, 0.6446795093494478, 0.0, 1.0, 0.2613699020259246], 
reward next is 0.7386, 
noisyNet noise sample is [array([0.8825188], dtype=float32), -0.47335124]. 
=============================================
[2019-04-04 10:57:55,998] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [9.8580045e-16 5.3703315e-14 1.1636576e-25 6.8489713e-16 2.2056342e-16
 5.5805231e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:57:56,002] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7338
[2019-04-04 10:57:56,011] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.14977893791432, 0.4331085106042901, 1.0, 1.0, 60575.36979527318], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4558200.0000, 
sim time next is 4558800.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.19610297333207, 0.4327329140220851, 1.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.782608695652174, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5996752477776726, 0.6442443046740284, 1.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.1884643], dtype=float32), -1.0022917]. 
=============================================
[2019-04-04 10:58:03,492] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0302161e-16 1.9059417e-14 5.9395051e-26 4.4419779e-16 1.9838568e-16
 1.0129027e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:03,493] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4870
[2019-04-04 10:58:03,512] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 51.66666666666666, 0.0, 0.0, 26.0, 26.36935321635586, 0.6963164191219818, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4646400.0000, 
sim time next is 4647000.0000, 
raw observation next is [3.166666666666667, 52.33333333333334, 0.0, 0.0, 26.0, 26.34600706413043, 0.6796335708924649, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5503231763619576, 0.5233333333333334, 0.0, 0.0, 0.6666666666666666, 0.6955005886775357, 0.7265445236308216, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2983122], dtype=float32), 1.1206384]. 
=============================================
[2019-04-04 10:58:03,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.76018]
 [85.45475]
 [84.94711]
 [84.44202]
 [84.667  ]], R is [[85.96315765]
 [86.10352325]
 [86.24248505]
 [86.38005829]
 [86.51625824]].
[2019-04-04 10:58:08,151] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4707723e-15 2.0492553e-14 7.8022115e-26 2.8090005e-16 7.9890136e-16
 1.7715181e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:08,153] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9799
[2019-04-04 10:58:08,163] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.1666666666666667, 78.0, 0.0, 0.0, 26.0, 25.46186046092103, 0.4768511406944, 1.0, 1.0, 22939.14917200238], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4731000.0000, 
sim time next is 4731600.0000, 
raw observation next is [-0.3333333333333333, 78.0, 0.0, 0.0, 26.0, 25.46870806872292, 0.4486782821404582, 1.0, 1.0, 25060.77965604894], 
processed observation next is [1.0, 0.782608695652174, 0.4533702677747, 0.78, 0.0, 0.0, 0.6666666666666666, 0.6223923390602432, 0.6495594273801527, 1.0, 1.0, 0.11933704598118543], 
reward next is 0.8807, 
noisyNet noise sample is [array([-0.7220678], dtype=float32), -0.8692476]. 
=============================================
[2019-04-04 10:58:09,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5445411e-16 2.5628212e-15 3.4289830e-26 1.4556180e-16 6.6727760e-17
 2.6988529e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:09,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4521
[2019-04-04 10:58:09,382] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.66666666666666, 123.6666666666667, 49.33333333333334, 26.0, 26.48688364259032, 0.4974069914102581, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4551000.0000, 
sim time next is 4551600.0000, 
raw observation next is [2.0, 49.33333333333333, 116.3333333333333, 58.66666666666667, 26.0, 25.98236559961636, 0.5042486580436539, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.518005540166205, 0.4933333333333333, 0.38777777777777767, 0.06482504604051566, 0.6666666666666666, 0.6651971333013634, 0.6680828860145512, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4908824], dtype=float32), 1.0720403]. 
=============================================
[2019-04-04 10:58:12,631] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7402952e-14 4.2796441e-13 2.3333488e-24 6.6291945e-15 3.3304021e-14
 1.1227952e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:12,631] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1373
[2019-04-04 10:58:12,692] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 75.0, 0.0, 0.0, 26.0, 24.96432186134011, 0.3422328428977399, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4605600.0000, 
sim time next is 4606200.0000, 
raw observation next is [-2.5, 74.0, 0.0, 0.0, 26.0, 25.09691386204256, 0.3641323225708768, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.74, 0.0, 0.0, 0.6666666666666666, 0.5914094885035466, 0.6213774408569589, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.0187907], dtype=float32), 0.7296366]. 
=============================================
[2019-04-04 10:58:18,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3002961e-16 3.5872082e-15 1.8045843e-26 7.0538270e-17 4.8281242e-17
 4.1430584e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:18,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8773
[2019-04-04 10:58:18,772] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 77.33333333333334, 145.1666666666667, 0.9999999999999998, 26.0, 25.09706016410579, 0.5008152022490716, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4714800.0000, 
sim time next is 4715400.0000, 
raw observation next is [1.833333333333333, 75.16666666666667, 155.3333333333333, 2.0, 26.0, 25.73454726040703, 0.5482310851135077, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5133887349953832, 0.7516666666666667, 0.5177777777777777, 0.0022099447513812156, 0.6666666666666666, 0.6445456050339192, 0.6827436950378359, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.95890397], dtype=float32), -0.90712005]. 
=============================================
[2019-04-04 10:58:22,011] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4458628e-14 1.1151505e-12 9.4846482e-25 5.0328470e-15 9.6119961e-15
 1.1413335e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:22,012] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8793
[2019-04-04 10:58:22,041] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 40.0, 0.0, 0.0, 26.0, 25.57052577819092, 0.4668013728358519, 0.0, 1.0, 18742.62383804359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5014200.0000, 
sim time next is 5014800.0000, 
raw observation next is [1.0, 40.0, 0.0, 0.0, 26.0, 25.6125956858565, 0.4561963509634224, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6343829738213751, 0.6520654503211408, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5755744], dtype=float32), 1.5860878]. 
=============================================
[2019-04-04 10:58:23,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4183579e-15 8.1121023e-15 2.2144837e-26 1.2553908e-16 4.3518271e-16
 3.4869688e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:23,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5580
[2019-04-04 10:58:23,786] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.166666666666667, 49.16666666666666, 142.6666666666667, 766.3333333333334, 26.0, 25.30423792503121, 0.4377723932546023, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4791000.0000, 
sim time next is 4791600.0000, 
raw observation next is [-2.0, 46.0, 137.5, 784.5, 26.0, 25.25271581866092, 0.4327443150388743, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.40720221606648205, 0.46, 0.4583333333333333, 0.8668508287292818, 0.6666666666666666, 0.6043929848884101, 0.6442481050129581, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4178744], dtype=float32), 0.6086134]. 
=============================================
[2019-04-04 10:58:27,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:27,525] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:27,530] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run33
[2019-04-04 10:58:29,284] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.00094650e-14 3.16377187e-13 1.85053338e-24 1.76774810e-14
 1.22879756e-14 4.37007425e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 10:58:29,285] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3337
[2019-04-04 10:58:29,319] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 40.0, 0.0, 0.0, 26.0, 25.48721664257493, 0.4610243939052291, 0.0, 1.0, 53717.2212808664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5011800.0000, 
sim time next is 5012400.0000, 
raw observation next is [1.666666666666667, 40.0, 0.0, 0.0, 26.0, 25.47212734314422, 0.4712398477248672, 0.0, 1.0, 50366.66128740685], 
processed observation next is [1.0, 0.0, 0.5087719298245615, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6226772785953516, 0.6570799492416224, 0.0, 1.0, 0.2398412442257469], 
reward next is 0.7602, 
noisyNet noise sample is [array([-1.5151511], dtype=float32), 0.35370398]. 
=============================================
[2019-04-04 10:58:30,196] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:30,197] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:30,205] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run33
[2019-04-04 10:58:31,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:31,233] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:31,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run33
[2019-04-04 10:58:33,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2866041e-16 2.0623507e-15 1.6795561e-27 6.1593609e-17 6.1996234e-17
 2.5788896e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:33,450] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6884
[2019-04-04 10:58:33,490] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6666666666666667, 31.5, 111.0, 745.6666666666667, 26.0, 26.33186565815473, 0.5173229189890091, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4960200.0000, 
sim time next is 4960800.0000, 
raw observation next is [1.0, 30.0, 112.5, 760.0, 26.0, 26.43286376580593, 0.5375894342753825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4903047091412743, 0.3, 0.375, 0.8397790055248618, 0.6666666666666666, 0.7027386471504942, 0.6791964780917942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.34998408], dtype=float32), 0.13330473]. 
=============================================
[2019-04-04 10:58:34,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:34,945] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:34,956] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run33
[2019-04-04 10:58:35,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:35,155] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:35,165] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run33
[2019-04-04 10:58:35,460] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:35,461] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:35,464] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run33
[2019-04-04 10:58:37,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:37,411] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:37,419] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run33
[2019-04-04 10:58:41,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:41,652] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:41,657] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run33
[2019-04-04 10:58:42,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:42,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:42,820] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run33
[2019-04-04 10:58:42,949] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:42,950] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:42,954] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run33
[2019-04-04 10:58:43,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:43,337] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:43,340] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run33
[2019-04-04 10:58:43,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:43,645] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:43,648] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run33
[2019-04-04 10:58:47,359] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:47,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:47,378] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run33
[2019-04-04 10:58:48,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:48,173] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:48,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run33
[2019-04-04 10:58:49,270] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:58:49,271] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:58:49,292] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run33
[2019-04-04 10:58:53,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0737400e-15 3.7838659e-14 8.4429952e-26 3.5185214e-16 1.7335881e-16
 5.9212504e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:53,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0772
[2019-04-04 10:58:53,298] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.799999999999999, 82.0, 189.0, 32.16666666666666, 26.0, 25.3966182823171, 0.2883243135245966, 1.0, 1.0, 42240.28206117477], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 124800.0000, 
sim time next is 125400.0000, 
raw observation next is [-7.8, 84.0, 188.0, 28.33333333333334, 26.0, 25.31844125748749, 0.2955977974339073, 1.0, 1.0, 41962.92698271617], 
processed observation next is [1.0, 0.43478260869565216, 0.24653739612188366, 0.84, 0.6266666666666667, 0.03130755064456722, 0.6666666666666666, 0.6098701047906241, 0.5985325991446357, 1.0, 1.0, 0.19982346182245794], 
reward next is 0.8002, 
noisyNet noise sample is [array([-1.9926776], dtype=float32), 0.08022818]. 
=============================================
[2019-04-04 10:58:55,304] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.4560686e-14 6.9816115e-14 5.3829768e-25 9.1273602e-16 2.2599396e-15
 1.7725329e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:58:55,304] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0478
[2019-04-04 10:58:55,365] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.699999999999999, 93.0, 78.5, 0.0, 26.0, 24.29826064559254, 0.08947310634852557, 0.0, 1.0, 18769.53021224736], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 42000.0000, 
sim time next is 42600.0000, 
raw observation next is [7.7, 93.0, 82.0, 0.0, 26.0, 24.30536076667673, 0.09161281776638376, 0.0, 1.0, 23265.72771261453], 
processed observation next is [0.0, 0.4782608695652174, 0.6759002770083103, 0.93, 0.2733333333333333, 0.0, 0.6666666666666666, 0.5254467305563942, 0.530537605922128, 0.0, 1.0, 0.11078917958387872], 
reward next is 0.8892, 
noisyNet noise sample is [array([0.31659916], dtype=float32), 0.36854365]. 
=============================================
[2019-04-04 10:59:00,512] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 10:59:00,513] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 10:59:00,516] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run33
[2019-04-04 10:59:00,995] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.1589670e-14 4.0892146e-13 3.1803211e-23 9.8135857e-15 2.7543782e-14
 1.5057020e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:00,996] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.5381
[2019-04-04 10:59:01,058] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.3, 69.0, 0.0, 0.0, 26.0, 23.47581943482545, -0.06724060466467074, 0.0, 1.0, 46220.24547086698], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 272400.0000, 
sim time next is 273000.0000, 
raw observation next is [-9.4, 69.5, 0.0, 0.0, 26.0, 23.46750764233409, -0.07780798748475569, 0.0, 1.0, 46359.29128555916], 
processed observation next is [1.0, 0.13043478260869565, 0.20221606648199447, 0.695, 0.0, 0.0, 0.6666666666666666, 0.45562563686117424, 0.4740640041717481, 0.0, 1.0, 0.22075852993123407], 
reward next is 0.7792, 
noisyNet noise sample is [array([0.64510643], dtype=float32), -0.070137575]. 
=============================================
[2019-04-04 10:59:01,097] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[74.03201 ]
 [73.988464]
 [73.95786 ]
 [73.93633 ]
 [73.806984]], R is [[74.11761475]
 [74.15634918]
 [74.19529724]
 [74.23438263]
 [74.27364349]].
[2019-04-04 10:59:05,475] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.8187905e-14 1.0641139e-13 3.6797496e-23 3.3906695e-15 3.3630255e-15
 6.0447004e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:05,476] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8124
[2019-04-04 10:59:05,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 64.0, 44.0, 24.0, 26.0, 25.84766552667926, 0.4427423820368651, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 144000.0000, 
sim time next is 144600.0000, 
raw observation next is [-6.800000000000001, 65.16666666666667, 38.33333333333333, 17.0, 26.0, 25.94940966190902, 0.4466648697379758, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.2742382271468144, 0.6516666666666667, 0.12777777777777777, 0.01878453038674033, 0.6666666666666666, 0.6624508051590849, 0.6488882899126586, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.41504604], dtype=float32), -0.2155131]. 
=============================================
[2019-04-04 10:59:08,236] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2610181e-13 6.6479157e-13 7.7451058e-24 2.3169695e-14 3.3212612e-14
 1.5114165e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:08,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2048
[2019-04-04 10:59:08,271] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.62762680305556, -0.0336945460902464, 0.0, 1.0, 44407.36910292901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 178200.0000, 
sim time next is 178800.0000, 
raw observation next is [-8.9, 74.0, 0.0, 0.0, 26.0, 23.5634557089188, -0.03729942185606019, 0.0, 1.0, 44387.25696204753], 
processed observation next is [1.0, 0.043478260869565216, 0.21606648199445982, 0.74, 0.0, 0.0, 0.6666666666666666, 0.46362130907656657, 0.4875668593813132, 0.0, 1.0, 0.21136789029546443], 
reward next is 0.7886, 
noisyNet noise sample is [array([-0.81583154], dtype=float32), 1.1317103]. 
=============================================
[2019-04-04 10:59:11,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.0674565e-13 3.1724024e-12 5.9533288e-23 3.2903967e-14 1.2989964e-13
 3.4909536e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:11,219] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-04 10:59:11,287] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 67.0, 0.0, 0.0, 26.0, 23.75846298507189, -0.02503167225461504, 0.0, 1.0, 45780.8829456885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 270000.0000, 
sim time next is 270600.0000, 
raw observation next is [-9.0, 67.5, 0.0, 0.0, 26.0, 23.66661994082127, -0.04518219247734239, 0.0, 1.0, 45864.26586220293], 
processed observation next is [1.0, 0.13043478260869565, 0.21329639889196678, 0.675, 0.0, 0.0, 0.6666666666666666, 0.4722183284017725, 0.4849392691742192, 0.0, 1.0, 0.21840126601049012], 
reward next is 0.7816, 
noisyNet noise sample is [array([-0.43167624], dtype=float32), -1.1713729]. 
=============================================
[2019-04-04 10:59:15,463] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.4967805e-15 1.6492671e-13 6.0762820e-24 1.7613593e-15 2.2450831e-15
 5.1175024e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:15,463] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1720
[2019-04-04 10:59:15,554] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.38333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.2424814986382, 0.3430522021490092, 1.0, 1.0, 54637.56965533009], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 328200.0000, 
sim time next is 328800.0000, 
raw observation next is [-12.46666666666667, 65.33333333333334, 0.0, 0.0, 26.0, 25.25073407147145, 0.3357487041749059, 1.0, 1.0, 57003.38167839512], 
processed observation next is [1.0, 0.8260869565217391, 0.11726685133887339, 0.6533333333333334, 0.0, 0.0, 0.6666666666666666, 0.6042278392892874, 0.6119162347249686, 1.0, 1.0, 0.2714446746590244], 
reward next is 0.7286, 
noisyNet noise sample is [array([0.32122868], dtype=float32), 0.23353286]. 
=============================================
[2019-04-04 10:59:17,343] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.9228512e-14 3.2770013e-13 4.1516069e-24 3.0963683e-15 3.5402398e-15
 5.0683887e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:17,344] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0326
[2019-04-04 10:59:17,402] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.916666666666668, 41.66666666666666, 0.0, 0.0, 26.0, 25.10307452681598, 0.2989326077829642, 1.0, 1.0, 56138.62643724662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 417000.0000, 
sim time next is 417600.0000, 
raw observation next is [-10.0, 42.0, 0.0, 0.0, 26.0, 25.12014053890943, 0.2961772965510853, 0.0, 1.0, 48738.35635025954], 
processed observation next is [1.0, 0.8695652173913043, 0.18559556786703602, 0.42, 0.0, 0.0, 0.6666666666666666, 0.593345044909119, 0.5987257655170285, 0.0, 1.0, 0.2320874111917121], 
reward next is 0.7679, 
noisyNet noise sample is [array([-0.70807004], dtype=float32), 0.78972226]. 
=============================================
[2019-04-04 10:59:20,826] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1454784e-16 1.8260763e-14 1.5596851e-24 3.1941078e-16 5.5639487e-16
 1.5401443e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:20,827] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6976
[2019-04-04 10:59:20,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.25, 61.0, 139.0, 484.0, 26.0, 25.79626808572278, 0.4617986743886093, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 135000.0000, 
sim time next is 135600.0000, 
raw observation next is [-7.066666666666666, 61.0, 140.5, 421.0000000000001, 26.0, 25.87615818221982, 0.461753360106995, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.26685133887349954, 0.61, 0.4683333333333333, 0.46519337016574597, 0.6666666666666666, 0.6563465151849851, 0.6539177867023317, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.27870247], dtype=float32), -0.39455366]. 
=============================================
[2019-04-04 10:59:28,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7333641e-15 2.5547794e-14 2.1572531e-25 4.6517625e-16 2.5888389e-16
 1.4557776e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:28,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5217
[2019-04-04 10:59:28,440] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.85, 76.5, 79.0, 0.0, 26.0, 25.40561044885512, 0.2208469208240678, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 207000.0000, 
sim time next is 207600.0000, 
raw observation next is [-7.666666666666666, 76.0, 86.5, 0.0, 26.0, 25.39277262474626, 0.2186697964266428, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.25023084025854114, 0.76, 0.28833333333333333, 0.0, 0.6666666666666666, 0.6160643853955218, 0.5728899321422143, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.4176527], dtype=float32), 0.06504213]. 
=============================================
[2019-04-04 10:59:46,506] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3204569e-14 1.4528187e-13 6.1562823e-25 5.6981571e-15 4.1372232e-15
 2.5752746e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:46,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1656
[2019-04-04 10:59:46,546] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.65, 71.5, 0.0, 0.0, 26.0, 24.39741417185212, 0.09159121289637044, 0.0, 1.0, 40964.45745153773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 693000.0000, 
sim time next is 693600.0000, 
raw observation next is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 26.0, 24.36988705036413, 0.0952106027812938, 0.0, 1.0, 40950.16535211128], 
processed observation next is [1.0, 0.0, 0.3638042474607572, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.5308239208636776, 0.5317368675937646, 0.0, 1.0, 0.1950007873910061], 
reward next is 0.8050, 
noisyNet noise sample is [array([-1.3108157], dtype=float32), 1.688362]. 
=============================================
[2019-04-04 10:59:50,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3445298e-13 5.3415464e-13 1.7975712e-24 6.0089559e-15 1.8235772e-14
 1.1889566e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:50,543] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8968
[2019-04-04 10:59:50,583] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 66.5, 0.0, 0.0, 26.0, 23.75423209237816, -0.005786277350948717, 0.0, 1.0, 44146.15497804353], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 624600.0000, 
sim time next is 625200.0000, 
raw observation next is [-4.5, 66.0, 0.0, 0.0, 26.0, 23.73679739472693, -0.01134866488369858, 0.0, 1.0, 44066.9073705237], 
processed observation next is [0.0, 0.21739130434782608, 0.3379501385041552, 0.66, 0.0, 0.0, 0.6666666666666666, 0.4780664495605776, 0.49621711170543376, 0.0, 1.0, 0.20984241605011286], 
reward next is 0.7902, 
noisyNet noise sample is [array([-0.4622942], dtype=float32), 0.10360816]. 
=============================================
[2019-04-04 10:59:59,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7773368e-14 9.3034501e-14 8.7827474e-25 2.5991015e-15 3.7514111e-15
 2.6350585e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 10:59:59,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4725
[2019-04-04 10:59:59,478] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.87827903437, 0.2013358294776576, 0.0, 1.0, 56899.82649067248], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 669000.0000, 
sim time next is 669600.0000, 
raw observation next is [-1.2, 57.0, 0.0, 0.0, 26.0, 24.86734112436041, 0.2034636400330516, 0.0, 1.0, 55076.37726380645], 
processed observation next is [0.0, 0.782608695652174, 0.42936288088642666, 0.57, 0.0, 0.0, 0.6666666666666666, 0.5722784270300343, 0.5678212133443505, 0.0, 1.0, 0.26226846316098307], 
reward next is 0.7377, 
noisyNet noise sample is [array([-1.3234135], dtype=float32), 1.1397649]. 
=============================================
[2019-04-04 11:00:00,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2322571e-14 9.8116068e-14 2.3604833e-25 1.7181306e-15 4.1905291e-15
 5.7742964e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:00,683] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1898
[2019-04-04 11:00:00,702] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.3203697590196, 0.03489197379376095, 0.0, 1.0, 41789.22018654898], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 713400.0000, 
sim time next is 714000.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.27380084427403, 0.02651684294743556, 0.0, 1.0, 41867.5904029451], 
processed observation next is [1.0, 0.2608695652173913, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5228167370228359, 0.5088389476491452, 0.0, 1.0, 0.19936947810926237], 
reward next is 0.8006, 
noisyNet noise sample is [array([0.00932385], dtype=float32), -0.57004124]. 
=============================================
[2019-04-04 11:00:00,705] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[82.278854]
 [82.32304 ]
 [82.34293 ]
 [82.35365 ]
 [82.36961 ]], R is [[82.18757629]
 [82.16670227]
 [82.14628601]
 [82.1262207 ]
 [82.1064682 ]].
[2019-04-04 11:00:06,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.9427000e-15 7.0868502e-14 2.9407073e-25 9.7009879e-16 1.1176934e-15
 1.0527621e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:06,906] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1798
[2019-04-04 11:00:06,923] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.1, 69.66666666666666, 0.0, 0.0, 26.0, 24.4329517035316, 0.1752826128566064, 0.0, 1.0, 42064.48895375664], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 776400.0000, 
sim time next is 777000.0000, 
raw observation next is [-7.199999999999999, 70.33333333333334, 0.0, 0.0, 26.0, 24.39793993134773, 0.1668662856394149, 0.0, 1.0, 41965.66131990321], 
processed observation next is [1.0, 1.0, 0.26315789473684215, 0.7033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5331616609456441, 0.5556220952131383, 0.0, 1.0, 0.19983648247572955], 
reward next is 0.8002, 
noisyNet noise sample is [array([-1.240041], dtype=float32), -0.74747735]. 
=============================================
[2019-04-04 11:00:06,942] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[81.754654]
 [81.777885]
 [81.74584 ]
 [81.68255 ]
 [81.60418 ]], R is [[81.70999908]
 [81.69258881]
 [81.67480469]
 [81.65655518]
 [81.63777924]].
[2019-04-04 11:00:13,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6961129e-15 6.1966891e-14 1.2353424e-25 2.0288207e-15 1.0553607e-15
 9.9751108e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:13,940] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8665
[2019-04-04 11:00:13,965] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 75.0, 0.0, 0.0, 26.0, 24.27173470705799, 0.05338054274654772, 0.0, 1.0, 41433.56783361636], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 702000.0000, 
sim time next is 702600.0000, 
raw observation next is [-3.3, 75.0, 0.0, 0.0, 26.0, 24.2711637327798, 0.05503330739065138, 0.0, 1.0, 41505.69802623042], 
processed observation next is [1.0, 0.13043478260869565, 0.37119113573407203, 0.75, 0.0, 0.0, 0.6666666666666666, 0.52259697773165, 0.5183444357968838, 0.0, 1.0, 0.1976461810772877], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.23270012], dtype=float32), -0.5067068]. 
=============================================
[2019-04-04 11:00:19,810] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1064601e-15 5.1317389e-15 5.5704981e-28 2.6245351e-17 1.1484048e-16
 1.7330631e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:19,811] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0783
[2019-04-04 11:00:19,821] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.3, 80.0, 0.0, 0.0, 26.0, 25.60644914512243, 0.5862622304403459, 0.0, 1.0, 44611.7739729213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1062000.0000, 
sim time next is 1062600.0000, 
raw observation next is [13.11666666666667, 80.5, 0.0, 0.0, 26.0, 25.62216666231978, 0.5970340078864028, 0.0, 1.0, 24752.07907255277], 
processed observation next is [1.0, 0.30434782608695654, 0.8259464450600187, 0.805, 0.0, 0.0, 0.6666666666666666, 0.6351805551933151, 0.6990113359621343, 0.0, 1.0, 0.11786704320263224], 
reward next is 0.8821, 
noisyNet noise sample is [array([-1.5848951], dtype=float32), -0.27133048]. 
=============================================
[2019-04-04 11:00:21,889] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1667877e-17 9.0157594e-16 4.3349337e-28 8.0666049e-18 8.0226908e-18
 6.0943081e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:21,889] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8912
[2019-04-04 11:00:21,895] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 81.0, 102.3333333333333, 0.0, 26.0, 26.6715021282935, 0.7047458276632216, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1001400.0000, 
sim time next is 1002000.0000, 
raw observation next is [14.4, 81.0, 98.16666666666667, 0.0, 26.0, 26.7421284167335, 0.7140935448881774, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.8614958448753465, 0.81, 0.32722222222222225, 0.0, 0.6666666666666666, 0.7285107013944584, 0.7380311816293924, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.54415655], dtype=float32), -0.50540894]. 
=============================================
[2019-04-04 11:00:21,904] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[94.116974]
 [93.985985]
 [93.89195 ]
 [93.7503  ]
 [93.60998 ]], R is [[94.33306885]
 [94.38973999]
 [94.44584656]
 [94.50138855]
 [94.5563736 ]].
[2019-04-04 11:00:23,287] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6612291e-18 5.3921059e-16 9.3406826e-29 3.7440858e-18 3.9822609e-18
 7.8488047e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:23,290] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0749
[2019-04-04 11:00:23,325] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.28333333333333, 86.0, 125.3333333333333, 0.0, 26.0, 26.73518660766892, 0.7087886452958619, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [12.36666666666667, 86.0, 126.6666666666667, 0.0, 26.0, 26.77678412591492, 0.5768948313239153, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8051708217913206, 0.86, 0.42222222222222233, 0.0, 0.6666666666666666, 0.7313986771595765, 0.6922982771079718, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.4390959], dtype=float32), -0.6617959]. 
=============================================
[2019-04-04 11:00:24,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.45371474e-16 6.68483008e-16 1.07457816e-26 2.11322974e-17
 4.31068434e-17 6.72474522e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 11:00:24,754] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9848
[2019-04-04 11:00:24,765] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 71.0, 0.0, 26.0, 25.85290297407528, 0.5436007408424404, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1348200.0000, 
sim time next is 1348800.0000, 
raw observation next is [1.1, 92.0, 66.5, 0.0, 26.0, 25.8585587312468, 0.5349444379488011, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.49307479224376743, 0.92, 0.22166666666666668, 0.0, 0.6666666666666666, 0.6548798942705666, 0.6783148126496004, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1184844], dtype=float32), 1.6641184]. 
=============================================
[2019-04-04 11:00:29,203] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.5128282e-17 1.1196265e-15 1.8785682e-27 1.0628217e-17 4.1683319e-17
 8.9438742e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:29,209] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3308
[2019-04-04 11:00:29,235] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 59.0, 0.0, 26.0, 26.01572942356153, 0.5191180858962029, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1418400.0000, 
sim time next is 1419000.0000, 
raw observation next is [0.0, 95.0, 63.33333333333334, 0.0, 26.0, 26.02197147757976, 0.5150375394708272, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.95, 0.21111111111111114, 0.0, 0.6666666666666666, 0.6684976231316467, 0.671679179823609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08335425], dtype=float32), 0.9613687]. 
=============================================
[2019-04-04 11:00:29,242] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[90.884895]
 [90.96627 ]
 [90.98905 ]
 [91.053764]
 [91.151764]], R is [[90.98485565]
 [91.07500458]
 [91.16425323]
 [91.25260925]
 [91.34008026]].
[2019-04-04 11:00:30,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1022398e-16 8.1376372e-16 3.9654876e-29 4.6673997e-18 1.2923336e-17
 2.6269550e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:30,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6286
[2019-04-04 11:00:30,119] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 96.66666666666666, 97.0, 0.0, 26.0, 25.05644184616325, 0.4846730999569225, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1252200.0000, 
sim time next is 1252800.0000, 
raw observation next is [14.4, 96.0, 98.0, 0.0, 26.0, 25.06633997677871, 0.4849008704880035, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.8614958448753465, 0.96, 0.32666666666666666, 0.0, 0.6666666666666666, 0.5888616647315592, 0.6616336234960012, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.21404037], dtype=float32), -0.96797764]. 
=============================================
[2019-04-04 11:00:38,367] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2990467e-16 6.2552180e-15 2.5983239e-26 1.1357574e-16 1.7419804e-16
 2.5522027e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:38,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0093
[2019-04-04 11:00:38,415] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 25.53155896722158, 0.4953461041648859, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1450800.0000, 
sim time next is 1451400.0000, 
raw observation next is [1.1, 92.0, 0.0, 0.0, 26.0, 25.44074470147399, 0.4816385913224221, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.49307479224376743, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6200620584561657, 0.6605461971074741, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5497228], dtype=float32), -0.86477625]. 
=============================================
[2019-04-04 11:00:39,721] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2831189e-15 1.5954867e-14 3.3504195e-26 4.2781850e-16 6.6376175e-16
 1.5283912e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:39,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5700
[2019-04-04 11:00:39,733] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.48759701868731, 0.5661307196371765, 0.0, 1.0, 22133.58303279427], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1557000.0000, 
sim time next is 1557600.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.55589103910478, 0.5682060296994981, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6296575865920649, 0.6894020098998327, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.9172655], dtype=float32), -0.26808336]. 
=============================================
[2019-04-04 11:00:45,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5806439e-15 1.4753449e-14 4.5622883e-26 3.2334853e-16 7.1069039e-16
 3.6667837e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:45,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2034
[2019-04-04 11:00:45,147] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.766666666666667, 64.33333333333334, 0.0, 0.0, 26.0, 26.3659032163016, 0.6991324721943787, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1622400.0000, 
sim time next is 1623000.0000, 
raw observation next is [9.583333333333334, 65.16666666666666, 0.0, 0.0, 26.0, 26.24670260780302, 0.6895663290040058, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7280701754385965, 0.6516666666666666, 0.0, 0.0, 0.6666666666666666, 0.6872252173169183, 0.7298554430013353, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.9939293], dtype=float32), -1.7729402]. 
=============================================
[2019-04-04 11:00:45,152] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[85.55778 ]
 [88.64124 ]
 [88.77024 ]
 [88.9437  ]
 [89.028755]], R is [[87.57791138]
 [87.70213318]
 [87.82511139]
 [87.94686127]
 [88.06739044]].
[2019-04-04 11:00:48,149] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.3727500e-16 1.1593573e-14 7.5274525e-26 1.9735913e-16 4.6106878e-16
 2.8457761e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:48,151] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3141
[2019-04-04 11:00:48,217] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 0.0, 0.0, 26.0, 24.87646602702505, 0.4274823799592031, 1.0, 1.0, 111018.7570659336], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1454400.0000, 
sim time next is 1455000.0000, 
raw observation next is [1.183333333333333, 91.5, 0.0, 0.0, 26.0, 24.89628928621424, 0.440354212558292, 0.0, 1.0, 52427.86976269732], 
processed observation next is [1.0, 0.8695652173913043, 0.49538319482917825, 0.915, 0.0, 0.0, 0.6666666666666666, 0.5746907738511867, 0.6467847375194307, 0.0, 1.0, 0.24965652267951105], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.15370491], dtype=float32), 0.33998337]. 
=============================================
[2019-04-04 11:00:48,219] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[86.84074 ]
 [85.396286]
 [82.57796 ]
 [83.761505]
 [86.78232 ]], R is [[84.33021545]
 [83.95825195]
 [83.65228271]
 [83.7193985 ]
 [83.88220215]].
[2019-04-04 11:00:49,734] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.7302136e-15 5.6853929e-14 3.0740017e-26 3.1330452e-16 6.6372125e-16
 2.3232929e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:49,735] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8989
[2019-04-04 11:00:49,768] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.15, 75.0, 0.0, 0.0, 26.0, 26.14492530501775, 0.7149981858788759, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1546200.0000, 
sim time next is 1546800.0000, 
raw observation next is [6.966666666666667, 75.33333333333333, 0.0, 0.0, 26.0, 26.17129154636807, 0.7103570542355199, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.6555863342566944, 0.7533333333333333, 0.0, 0.0, 0.6666666666666666, 0.6809409621973392, 0.7367856847451733, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7528763], dtype=float32), -0.42925578]. 
=============================================
[2019-04-04 11:00:54,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1631819e-14 5.6419025e-14 2.0537279e-25 4.8694855e-16 6.3764227e-16
 9.1636681e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:54,534] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7832
[2019-04-04 11:00:54,566] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.333333333333334, 95.33333333333334, 0.0, 0.0, 26.0, 25.60040838909124, 0.5618897069186216, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1664400.0000, 
sim time next is 1665000.0000, 
raw observation next is [5.25, 94.5, 0.0, 0.0, 26.0, 25.6557764327155, 0.5483634565556433, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.2608695652173913, 0.60803324099723, 0.945, 0.0, 0.0, 0.6666666666666666, 0.6379813693929582, 0.6827878188518811, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9860282], dtype=float32), 0.3523334]. 
=============================================
[2019-04-04 11:00:54,623] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.96694 ]
 [85.16291 ]
 [85.344505]
 [85.422935]
 [85.41431 ]], R is [[84.93965149]
 [85.09025574]
 [85.15008545]
 [85.06987   ]
 [84.84212494]].
[2019-04-04 11:00:56,178] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5104877e-14 7.9877111e-14 9.7949851e-25 1.5493562e-15 2.1075001e-15
 1.7721746e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:00:56,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1801
[2019-04-04 11:00:56,236] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.0, 14.5, 0.0, 26.0, 25.01072465626303, 0.3284866318903856, 0.0, 1.0, 43092.49360591658], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1789200.0000, 
sim time next is 1789800.0000, 
raw observation next is [-3.9, 82.00000000000001, 9.999999999999998, 0.0, 26.0, 25.008350135357, 0.3261464358508387, 0.0, 1.0, 45146.38445239743], 
processed observation next is [0.0, 0.7391304347826086, 0.3545706371191136, 0.8200000000000002, 0.033333333333333326, 0.0, 0.6666666666666666, 0.5840291779464165, 0.6087154786169462, 0.0, 1.0, 0.21498278310665445], 
reward next is 0.7850, 
noisyNet noise sample is [array([-0.29838878], dtype=float32), -0.9215345]. 
=============================================
[2019-04-04 11:01:01,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.64451779e-14 2.89559202e-13 9.47188444e-25 6.50129678e-15
 1.23758036e-14 3.27706989e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:01:01,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1546
[2019-04-04 11:01:01,158] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 27.0, 0.0, 26.0, 23.50548875717431, 0.05706513801021209, 0.0, 1.0, 203468.6302052165], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1845000.0000, 
sim time next is 1845600.0000, 
raw observation next is [-6.700000000000001, 78.0, 47.16666666666666, 15.66666666666666, 26.0, 24.1707305896439, 0.1630856528805252, 0.0, 1.0, 161283.0144685804], 
processed observation next is [0.0, 0.34782608695652173, 0.2770083102493075, 0.78, 0.15722222222222218, 0.017311233885819514, 0.6666666666666666, 0.5142275491369915, 0.5543618842935084, 0.0, 1.0, 0.7680143546122876], 
reward next is 0.2320, 
noisyNet noise sample is [array([0.7840513], dtype=float32), -1.4862679]. 
=============================================
[2019-04-04 11:01:03,116] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9116554e-15 2.6553395e-14 1.9823423e-25 4.7769042e-16 6.7497361e-16
 7.8664372e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:01:03,117] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3709
[2019-04-04 11:01:03,157] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 83.33333333333334, 49.16666666666667, 0.0, 26.0, 25.97078913577122, 0.5606615026904317, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1698000.0000, 
sim time next is 1698600.0000, 
raw observation next is [1.516666666666667, 82.16666666666666, 45.33333333333334, 0.0, 26.0, 26.01127117210967, 0.5624175417662769, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.5046168051708219, 0.8216666666666665, 0.15111111111111114, 0.0, 0.6666666666666666, 0.6676059310091391, 0.6874725139220924, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8775463], dtype=float32), 0.67132807]. 
=============================================
[2019-04-04 11:01:08,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9563372e-15 3.8056665e-14 8.3179081e-25 4.5916438e-16 2.1852760e-15
 2.5376128e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:01:08,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6812
[2019-04-04 11:01:09,018] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.35, 84.5, 30.0, 0.0, 26.0, 24.79973381071785, 0.4167432848643502, 1.0, 1.0, 196399.3894190901], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1701000.0000, 
sim time next is 1701600.0000, 
raw observation next is [1.266666666666667, 85.66666666666667, 25.16666666666667, 0.0, 26.0, 25.09917824114533, 0.4803341435087027, 1.0, 1.0, 6231.073815233195], 
processed observation next is [1.0, 0.6956521739130435, 0.4976915974145891, 0.8566666666666667, 0.0838888888888889, 0.0, 0.6666666666666666, 0.591598186762111, 0.6601113811695676, 1.0, 1.0, 0.029671780072539025], 
reward next is 0.9703, 
noisyNet noise sample is [array([2.4679053], dtype=float32), 0.22818859]. 
=============================================
[2019-04-04 11:01:19,624] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-04 11:01:19,626] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:01:19,626] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:01:19,629] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:01:19,629] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:01:19,631] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run45
[2019-04-04 11:01:19,663] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:01:19,665] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run45
[2019-04-04 11:01:19,705] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:01:19,708] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run45
[2019-04-04 11:02:41,687] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1697944], dtype=float32), -0.29556122]
[2019-04-04 11:02:41,687] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [13.3, 75.16666666666666, 92.0, 238.0, 26.0, 26.57236146057749, 0.704783674750105, 1.0, 1.0, 0.0]
[2019-04-04 11:02:41,687] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:02:41,688] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.1414562e-16 6.0579102e-15 1.6688539e-25 1.1146783e-16 2.7520333e-16
 2.7776808e-18 1.0000000e+00], sampled 0.8051435765866517
[2019-04-04 11:04:24,324] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:04:55,454] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:05:01,229] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:05:02,262] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 4400000, evaluation results [4400000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:05:21,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.36759261e-15 3.45808045e-14 3.16120699e-25 1.04186048e-15
 3.70609398e-15 1.27228815e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:05:21,131] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0738
[2019-04-04 11:05:21,158] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 82.66666666666667, 0.0, 0.0, 26.0, 25.35107609310327, 0.4242061504379578, 0.0, 1.0, 45616.66058840773], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2065800.0000, 
sim time next is 2066400.0000, 
raw observation next is [-3.9, 82.0, 0.0, 0.0, 26.0, 25.34287182101445, 0.4231022141151379, 0.0, 1.0, 43225.42852357546], 
processed observation next is [1.0, 0.9565217391304348, 0.3545706371191136, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6119059850845376, 0.6410340713717126, 0.0, 1.0, 0.20583537392178788], 
reward next is 0.7942, 
noisyNet noise sample is [array([-0.8992647], dtype=float32), -0.22329223]. 
=============================================
[2019-04-04 11:05:25,727] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7217435e-14 2.8507871e-13 1.5709468e-24 2.6849428e-15 1.3408875e-14
 2.2224404e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:25,727] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0920
[2019-04-04 11:05:25,760] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.100000000000001, 86.83333333333334, 0.0, 0.0, 26.0, 24.47069031061058, 0.1740438330499393, 0.0, 1.0, 43217.85449254519], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2085000.0000, 
sim time next is 2085600.0000, 
raw observation next is [-5.2, 87.66666666666667, 0.0, 0.0, 26.0, 24.46721326904312, 0.1702331428976488, 0.0, 1.0, 43302.18611546865], 
processed observation next is [1.0, 0.13043478260869565, 0.31855955678670367, 0.8766666666666667, 0.0, 0.0, 0.6666666666666666, 0.5389344390869267, 0.556744380965883, 0.0, 1.0, 0.20620088626413643], 
reward next is 0.7938, 
noisyNet noise sample is [array([0.06862789], dtype=float32), 0.575822]. 
=============================================
[2019-04-04 11:05:25,959] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.0715223e-15 2.0487276e-14 1.1009557e-25 1.0556501e-16 4.9324703e-16
 2.4958880e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:25,960] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1282
[2019-04-04 11:05:26,013] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 83.33333333333334, 0.0, 0.0, 26.0, 25.70976880488472, 0.41794603542989, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2056800.0000, 
sim time next is 2057400.0000, 
raw observation next is [-3.9, 84.0, 0.0, 0.0, 26.0, 25.52679599056892, 0.3801675393040207, 0.0, 1.0, 9355.898117023791], 
processed observation next is [1.0, 0.8260869565217391, 0.3545706371191136, 0.84, 0.0, 0.0, 0.6666666666666666, 0.6272329992140767, 0.6267225131013402, 0.0, 1.0, 0.04455189579535139], 
reward next is 0.9554, 
noisyNet noise sample is [array([-1.0649627], dtype=float32), 1.00673]. 
=============================================
[2019-04-04 11:05:26,369] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1849269e-14 1.6597535e-13 2.6858012e-24 3.8177041e-15 1.0592291e-14
 4.2831958e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:26,373] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3884
[2019-04-04 11:05:26,399] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.616666666666667, 83.66666666666667, 0.0, 0.0, 26.0, 24.13590745055667, 0.07610600722437852, 0.0, 1.0, 43500.84823573153], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2094600.0000, 
sim time next is 2095200.0000, 
raw observation next is [-6.7, 83.0, 0.0, 0.0, 26.0, 24.06241220357136, 0.07261119835244921, 0.0, 1.0, 43563.55716902344], 
processed observation next is [1.0, 0.2608695652173913, 0.2770083102493075, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5052010169642799, 0.5242037327841497, 0.0, 1.0, 0.20744551032868305], 
reward next is 0.7926, 
noisyNet noise sample is [array([0.0187213], dtype=float32), 1.7008231]. 
=============================================
[2019-04-04 11:05:33,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.4606185e-14 2.6577782e-13 5.3239406e-24 5.4517997e-15 1.1485505e-14
 2.4577003e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:33,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8584
[2019-04-04 11:05:33,159] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 91.0, 0.0, 0.0, 26.0, 23.81030430533653, 0.00882092972217951, 0.0, 1.0, 43421.91337681599], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2264400.0000, 
sim time next is 2265000.0000, 
raw observation next is [-8.9, 91.00000000000001, 0.0, 0.0, 26.0, 23.76246506681927, -0.003680383970683784, 0.0, 1.0, 43366.54977951405], 
processed observation next is [1.0, 0.21739130434782608, 0.21606648199445982, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.4802054222349392, 0.4987732053431054, 0.0, 1.0, 0.20650737990244786], 
reward next is 0.7935, 
noisyNet noise sample is [array([-0.04226112], dtype=float32), 1.6147379]. 
=============================================
[2019-04-04 11:05:33,164] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.177574]
 [78.25075 ]
 [78.319435]
 [78.377594]
 [78.442635]], R is [[78.12818146]
 [78.14012909]
 [78.1517334 ]
 [78.16307068]
 [78.17411041]].
[2019-04-04 11:05:43,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0249374e-15 7.6910270e-15 1.7865027e-25 2.2890390e-16 1.8906928e-16
 7.4283855e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:43,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8208
[2019-04-04 11:05:43,799] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666668, 43.66666666666667, 121.8333333333333, 35.0, 26.0, 25.57042682772497, 0.3951465266145414, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2302800.0000, 
sim time next is 2303400.0000, 
raw observation next is [0.1833333333333333, 43.83333333333334, 105.6666666666667, 28.0, 26.0, 25.76705454118275, 0.41115064529862, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.46768236380424755, 0.4383333333333334, 0.3522222222222223, 0.030939226519337018, 0.6666666666666666, 0.6472545450985624, 0.63705021509954, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26312286], dtype=float32), 0.074099496]. 
=============================================
[2019-04-04 11:05:45,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9783949e-14 6.7172498e-14 5.0062444e-25 1.0284074e-15 4.3987120e-15
 5.1211711e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:45,566] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8192
[2019-04-04 11:05:45,613] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 40.66666666666667, 73.5, 758.3333333333333, 26.0, 25.06391548341242, 0.2316193978038082, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2456400.0000, 
sim time next is 2457000.0000, 
raw observation next is [-3.95, 39.5, 76.0, 777.0, 26.0, 25.00895035065825, 0.228437956698007, 0.0, 1.0, 18744.69522215695], 
processed observation next is [0.0, 0.43478260869565216, 0.3531855955678671, 0.395, 0.25333333333333335, 0.8585635359116022, 0.6666666666666666, 0.5840791958881875, 0.5761459855660024, 0.0, 1.0, 0.08926045343884263], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.7394236], dtype=float32), 0.36994535]. 
=============================================
[2019-04-04 11:05:45,624] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.87422]
 [81.91405]
 [82.08876]
 [82.42717]
 [82.81832]], R is [[82.04062653]
 [82.22022247]
 [82.39801788]
 [82.57403564]
 [82.74829865]].
[2019-04-04 11:05:54,672] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4578131e-13 8.4906593e-13 2.2023379e-24 8.3714604e-15 3.9482726e-14
 2.6264080e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:54,673] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6244
[2019-04-04 11:05:54,698] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.55, 63.5, 0.0, 0.0, 26.0, 24.52150073818959, 0.1860453205060347, 0.0, 1.0, 39862.47935494516], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2345400.0000, 
sim time next is 2346000.0000, 
raw observation next is [-2.633333333333333, 64.0, 0.0, 0.0, 26.0, 24.49227781970483, 0.1783296771472555, 0.0, 1.0, 39977.1636319941], 
processed observation next is [0.0, 0.13043478260869565, 0.38965835641735924, 0.64, 0.0, 0.0, 0.6666666666666666, 0.5410231516420693, 0.5594432257157519, 0.0, 1.0, 0.1903674458666386], 
reward next is 0.8096, 
noisyNet noise sample is [array([-0.67815], dtype=float32), -0.35750353]. 
=============================================
[2019-04-04 11:05:54,733] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.16521 ]
 [80.1886  ]
 [80.20514 ]
 [80.223366]
 [80.23913 ]], R is [[80.135849  ]
 [80.14466858]
 [80.15390778]
 [80.16358185]
 [80.1736908 ]].
[2019-04-04 11:05:58,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [6.8672879e-15 1.0634849e-13 6.1179348e-25 8.0724003e-16 8.3421596e-16
 2.6383576e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:05:58,226] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.2347
[2019-04-04 11:05:58,286] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 63.33333333333333, 0.0, 0.0, 26.0, 25.0022096479458, 0.3740183522085482, 0.0, 1.0, 23329.42446317054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [-1.2, 63.66666666666667, 0.0, 0.0, 26.0, 25.02916427203987, 0.3829862441688923, 0.0, 1.0, 189214.8487388082], 
processed observation next is [1.0, 0.8695652173913043, 0.42936288088642666, 0.6366666666666667, 0.0, 0.0, 0.6666666666666666, 0.5857636893366559, 0.6276620813896308, 0.0, 1.0, 0.90102308923242], 
reward next is 0.0990, 
noisyNet noise sample is [array([-0.4698345], dtype=float32), -0.047693577]. 
=============================================
[2019-04-04 11:06:01,735] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9600966e-13 5.8863803e-13 2.8365336e-24 1.7403217e-14 3.9612997e-14
 5.7160252e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:06:01,738] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7508
[2019-04-04 11:06:01,769] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.6, 43.0, 0.0, 0.0, 26.0, 24.58117727067605, 0.1483217726471891, 0.0, 1.0, 43131.57199094433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2419200.0000, 
sim time next is 2419800.0000, 
raw observation next is [-5.7, 43.83333333333334, 0.0, 0.0, 26.0, 24.55372687203524, 0.1416368753548356, 0.0, 1.0, 43138.21355830986], 
processed observation next is [0.0, 0.0, 0.30470914127423826, 0.4383333333333334, 0.0, 0.0, 0.6666666666666666, 0.5461439060029368, 0.5472122917849452, 0.0, 1.0, 0.20542006456338027], 
reward next is 0.7946, 
noisyNet noise sample is [array([-1.3264713], dtype=float32), 1.4069024]. 
=============================================
[2019-04-04 11:06:45,488] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.4935972e-14 1.4086628e-13 4.6920362e-25 3.9650925e-15 1.1782436e-14
 1.8872907e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:06:45,492] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4689
[2019-04-04 11:06:45,514] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 71.0, 0.0, 0.0, 26.0, 24.42346484431128, 0.1540426015147497, 0.0, 1.0, 38521.25483205608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3031200.0000, 
sim time next is 3031800.0000, 
raw observation next is [-5.166666666666667, 72.0, 0.0, 0.0, 26.0, 24.42007610417912, 0.1485755914165367, 0.0, 1.0, 38641.56955614068], 
processed observation next is [0.0, 0.08695652173913043, 0.31948291782086796, 0.72, 0.0, 0.0, 0.6666666666666666, 0.5350063420149267, 0.5495251971388456, 0.0, 1.0, 0.18400747407686038], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.19115789], dtype=float32), 0.7129372]. 
=============================================
[2019-04-04 11:06:47,108] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.27713815e-14 2.58954425e-13 1.05428144e-24 1.89278660e-15
 1.05443566e-14 5.82334092e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:06:47,111] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1830
[2019-04-04 11:06:47,138] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.93334948045548, 0.3466928504435836, 0.0, 1.0, 43831.6576442455], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3291600.0000, 
sim time next is 3292200.0000, 
raw observation next is [-8.0, 77.0, 0.0, 0.0, 26.0, 24.9567795586364, 0.3461936441523335, 0.0, 1.0, 43832.80598644094], 
processed observation next is [1.0, 0.08695652173913043, 0.24099722991689754, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5797316298863665, 0.6153978813841111, 0.0, 1.0, 0.20872764755448064], 
reward next is 0.7913, 
noisyNet noise sample is [array([-1.4903804], dtype=float32), -1.165166]. 
=============================================
[2019-04-04 11:06:53,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2285432e-16 2.3309042e-15 1.1695624e-26 3.3129804e-17 1.4841286e-17
 1.6312760e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:06:53,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2673
[2019-04-04 11:06:53,895] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 47.66666666666667, 114.0, 797.3333333333333, 26.0, 26.54381868157125, 0.640894424210953, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3410400.0000, 
sim time next is 3411000.0000, 
raw observation next is [3.0, 47.0, 115.0, 804.0, 26.0, 26.61142676651667, 0.4194531454810574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.5457063711911359, 0.47, 0.38333333333333336, 0.8883977900552487, 0.6666666666666666, 0.7176188972097224, 0.6398177151603525, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.5050707], dtype=float32), 0.46753886]. 
=============================================
[2019-04-04 11:06:53,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[89.42543 ]
 [89.662506]
 [89.89017 ]
 [90.076   ]
 [90.26817 ]], R is [[89.10684204]
 [89.21577454]
 [89.32361603]
 [89.43038177]
 [89.53607941]].
[2019-04-04 11:07:01,396] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5983979e-15 9.8198393e-15 2.6753801e-25 2.6559600e-16 4.6228750e-16
 2.0040264e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:01,401] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8954
[2019-04-04 11:07:01,458] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 69.0, 0.0, 0.0, 26.0, 25.87489639686132, 0.5971717201518153, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3267600.0000, 
sim time next is 3268200.0000, 
raw observation next is [-4.0, 70.0, 0.0, 0.0, 26.0, 25.79176387535801, 0.5753833251803645, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.3518005540166205, 0.7, 0.0, 0.0, 0.6666666666666666, 0.6493136562798343, 0.6917944417267882, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.8968906], dtype=float32), -0.15991713]. 
=============================================
[2019-04-04 11:07:04,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8097314e-15 2.0793131e-13 5.6091452e-25 1.0493992e-15 6.0563622e-15
 6.2310829e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:04,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2763
[2019-04-04 11:07:04,377] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.45429022948466, 0.4932090624715694, 0.0, 1.0, 71281.22641698869], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3361800.0000, 
sim time next is 3362400.0000, 
raw observation next is [-4.0, 65.0, 0.0, 0.0, 26.0, 25.41009979984726, 0.5008420114672724, 0.0, 1.0, 77110.78655913717], 
processed observation next is [1.0, 0.9565217391304348, 0.3518005540166205, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6175083166539382, 0.6669473371557575, 0.0, 1.0, 0.36719422171017696], 
reward next is 0.6328, 
noisyNet noise sample is [array([-0.46171203], dtype=float32), 1.3488895]. 
=============================================
[2019-04-04 11:07:07,374] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7284300e-17 4.3902443e-16 2.9667646e-28 2.8668709e-18 5.4367550e-18
 8.1310426e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:07,374] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3602
[2019-04-04 11:07:07,386] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.666666666666667, 48.66666666666666, 110.0, 770.6666666666667, 26.0, 26.56217789389668, 0.6347497231372057, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3408000.0000, 
sim time next is 3408600.0000, 
raw observation next is [2.833333333333333, 48.83333333333334, 111.0, 777.3333333333333, 26.0, 26.61396117165219, 0.6261009028639442, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.541089566020314, 0.48833333333333345, 0.37, 0.8589318600368323, 0.6666666666666666, 0.7178300976376825, 0.7087003009546481, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.14493667], dtype=float32), 2.62614]. 
=============================================
[2019-04-04 11:07:08,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.5012216e-14 5.5539242e-14 1.0584234e-25 1.2304903e-15 8.3976514e-16
 9.6497120e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:08,128] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4088
[2019-04-04 11:07:08,149] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 67.0, 0.0, 0.0, 26.0, 25.51597923177305, 0.4030115921958948, 0.0, 1.0, 82663.0685763576], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3706200.0000, 
sim time next is 3706800.0000, 
raw observation next is [0.6666666666666667, 68.66666666666667, 0.0, 0.0, 26.0, 25.45375880727784, 0.4048932638732558, 0.0, 1.0, 89476.66854211177], 
processed observation next is [0.0, 0.9130434782608695, 0.4810710987996307, 0.6866666666666668, 0.0, 0.0, 0.6666666666666666, 0.6211465672731533, 0.6349644212910853, 0.0, 1.0, 0.4260793740100561], 
reward next is 0.5739, 
noisyNet noise sample is [array([-2.2841136], dtype=float32), 1.7414418]. 
=============================================
[2019-04-04 11:07:10,743] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.5602583e-17 8.6935544e-16 1.2170714e-26 1.1844472e-17 3.8017575e-17
 1.1551957e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:10,744] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0161
[2019-04-04 11:07:10,763] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 115.5, 814.5, 26.0, 26.19217129971147, 0.6377806402552485, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [2.166666666666667, 51.5, 115.3333333333333, 811.6666666666666, 26.0, 26.24545065052456, 0.6549440281957789, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.5226223453370269, 0.515, 0.3844444444444443, 0.8968692449355432, 0.6666666666666666, 0.6871208875437134, 0.7183146760652597, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-3.7290785], dtype=float32), -0.6447978]. 
=============================================
[2019-04-04 11:07:15,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0813975e-16 4.8893296e-15 1.4210346e-26 2.3679243e-17 1.1133269e-16
 4.0294307e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:15,440] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8636
[2019-04-04 11:07:15,450] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 60.33333333333334, 113.0, 796.6666666666667, 26.0, 26.1609878533207, 0.6014188847117291, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3496200.0000, 
sim time next is 3496800.0000, 
raw observation next is [1.333333333333333, 59.66666666666667, 114.0, 803.3333333333333, 26.0, 26.2374795602468, 0.6118321854371301, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4995383194829178, 0.5966666666666667, 0.38, 0.8876611418047882, 0.6666666666666666, 0.6864566300205667, 0.7039440618123768, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7122117], dtype=float32), 1.3029536]. 
=============================================
[2019-04-04 11:07:19,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6373973e-17 3.8160422e-16 1.5129367e-27 2.8321957e-18 8.2189923e-18
 5.3741472e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:19,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4264
[2019-04-04 11:07:19,366] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 60.00000000000001, 115.8333333333333, 814.1666666666666, 26.0, 26.51700421840447, 0.6314048378432074, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3842400.0000, 
sim time next is 3843000.0000, 
raw observation next is [-1.0, 60.0, 117.0, 822.0, 26.0, 26.49444723685937, 0.6394965939769918, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4349030470914128, 0.6, 0.39, 0.9082872928176795, 0.6666666666666666, 0.7078706030716141, 0.713165531325664, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2952205], dtype=float32), 0.37142548]. 
=============================================
[2019-04-04 11:07:19,380] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[91.06577 ]
 [91.23702 ]
 [91.356155]
 [91.43309 ]
 [91.509766]], R is [[90.98590088]
 [91.07604218]
 [91.1652832 ]
 [91.25363159]
 [91.34109497]].
[2019-04-04 11:07:22,114] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6374900e-14 1.7095026e-13 9.2334641e-25 4.7932620e-15 5.2643682e-15
 6.1330082e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:22,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8399
[2019-04-04 11:07:22,177] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.09586133945616, 0.3675262634068896, 0.0, 1.0, 34470.88581350799], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3607200.0000, 
sim time next is 3607800.0000, 
raw observation next is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.13552953166361, 0.3639977295249158, 0.0, 1.0, 18705.04859801172], 
processed observation next is [0.0, 0.782608695652174, 0.4349030470914128, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5946274609719676, 0.6213325765083053, 0.0, 1.0, 0.089071659990532], 
reward next is 0.9109, 
noisyNet noise sample is [array([-1.1624058], dtype=float32), -0.1854436]. 
=============================================
[2019-04-04 11:07:27,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.4080325e-16 9.0513619e-15 1.0394806e-26 2.5130684e-16 7.1640889e-17
 1.5504832e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:27,895] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2432
[2019-04-04 11:07:27,943] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.0, 53.83333333333334, 94.0, 541.3333333333333, 26.0, 25.84275455845108, 0.4847927350080858, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3919800.0000, 
sim time next is 3920400.0000, 
raw observation next is [-8.0, 53.0, 95.5, 579.0, 26.0, 25.90889042083212, 0.4881493706242583, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.24099722991689754, 0.53, 0.31833333333333336, 0.6397790055248619, 0.6666666666666666, 0.65907420173601, 0.6627164568747528, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.8969791], dtype=float32), 1.1140982]. 
=============================================
[2019-04-04 11:07:30,750] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.7166216e-15 7.1486620e-14 4.2856298e-25 7.2566191e-16 4.3653634e-15
 1.2546721e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:30,750] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4510
[2019-04-04 11:07:30,775] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6666666666666666, 76.0, 0.0, 0.0, 26.0, 25.87599483876923, 0.5953808029540478, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [-0.8333333333333334, 77.0, 0.0, 0.0, 26.0, 25.8590077419985, 0.5809542529609356, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.43951985226223456, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6549173118332083, 0.6936514176536451, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1783737], dtype=float32), -0.1744744]. 
=============================================
[2019-04-04 11:07:34,065] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [5.5404209e-15 1.0525574e-13 2.3743741e-24 2.3819602e-15 1.9662654e-15
 1.8158760e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:34,065] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6950
[2019-04-04 11:07:34,075] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 51.0, 0.0, 0.0, 26.0, 25.3778992497437, 0.4720413714756738, 0.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3872400.0000, 
sim time next is 3873000.0000, 
raw observation next is [1.0, 51.0, 0.0, 0.0, 26.0, 25.28829686420379, 0.456379298908846, 0.0, 1.0, 18680.41167023054], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.51, 0.0, 0.0, 0.6666666666666666, 0.6073580720169826, 0.6521264329696154, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([1.5371879], dtype=float32), -0.18898706]. 
=============================================
[2019-04-04 11:07:34,082] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[79.72894]
 [80.67602]
 [83.51714]
 [83.36538]
 [83.40397]], R is [[79.51918793]
 [79.63504791]
 [79.74974823]
 [79.86330414]
 [80.06467438]].
[2019-04-04 11:07:39,248] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0799408e-14 3.8234211e-13 1.6411023e-24 7.6989823e-15 1.0665192e-14
 7.1597994e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:39,248] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9703
[2019-04-04 11:07:39,271] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.5, 48.0, 0.0, 0.0, 26.0, 25.29912453553026, 0.383008597533068, 0.0, 1.0, 39329.89032747964], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4156200.0000, 
sim time next is 4156800.0000, 
raw observation next is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.27821281979057, 0.3768159449600792, 0.0, 1.0, 39372.350749073], 
processed observation next is [0.0, 0.08695652173913043, 0.38873499538319484, 0.4866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6065177349825476, 0.625605314986693, 0.0, 1.0, 0.18748738451939523], 
reward next is 0.8125, 
noisyNet noise sample is [array([1.9048188], dtype=float32), 0.31624734]. 
=============================================
[2019-04-04 11:07:39,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7798582e-15 4.6984836e-14 4.7906783e-25 1.3000287e-15 8.4397214e-16
 2.4908483e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:39,657] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7936
[2019-04-04 11:07:39,714] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.333333333333333, 42.33333333333334, 0.0, 0.0, 26.0, 25.83387297959247, 0.5408514031968326, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3957600.0000, 
sim time next is 3958200.0000, 
raw observation next is [-6.5, 43.0, 0.0, 0.0, 26.0, 25.68977872931113, 0.5296180546107805, 1.0, 1.0, 106751.6727211835], 
processed observation next is [1.0, 0.8260869565217391, 0.28254847645429365, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6408148941092607, 0.6765393515369268, 1.0, 1.0, 0.5083412986723024], 
reward next is 0.4917, 
noisyNet noise sample is [array([0.8029889], dtype=float32), -0.60856056]. 
=============================================
[2019-04-04 11:07:41,002] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0697513e-15 4.3115790e-14 4.7280068e-25 3.0965206e-16 8.1691441e-16
 3.8248848e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:41,003] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6825
[2019-04-04 11:07:41,026] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 35.0, 0.0, 0.0, 26.0, 26.53998194703529, 0.5539523506263906, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4126800.0000, 
sim time next is 4127400.0000, 
raw observation next is [3.0, 35.5, 0.0, 0.0, 26.0, 25.8399790302912, 0.5129828560165571, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5457063711911359, 0.355, 0.0, 0.0, 0.6666666666666666, 0.6533315858576, 0.6709942853388524, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3678984], dtype=float32), -0.09175744]. 
=============================================
[2019-04-04 11:07:50,192] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9635619e-14 2.1068351e-13 2.4398963e-24 9.5318854e-15 3.2873901e-15
 9.2441801e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:50,199] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9225
[2019-04-04 11:07:50,216] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 29.33333333333333, 0.0, 0.0, 26.0, 25.58058165517296, 0.5279482683358331, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4049400.0000, 
sim time next is 4050000.0000, 
raw observation next is [-4.0, 29.0, 0.0, 0.0, 26.0, 25.6804760927208, 0.5256703533002691, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.3518005540166205, 0.29, 0.0, 0.0, 0.6666666666666666, 0.6400396743933999, 0.6752234511000896, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.53629327], dtype=float32), -1.4121172]. 
=============================================
[2019-04-04 11:07:50,231] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[80.722   ]
 [80.86833 ]
 [81.116455]
 [81.3007  ]
 [82.42983 ]], R is [[80.97817993]
 [81.168396  ]
 [81.11037445]
 [80.68366241]
 [80.0061264 ]].
[2019-04-04 11:07:52,696] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.8669392e-14 1.9709196e-13 4.3122953e-25 2.6609303e-15 3.8688111e-15
 2.1469506e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:52,706] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9791
[2019-04-04 11:07:52,719] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 80.0, 0.0, 0.0, 26.0, 25.61494848365923, 0.5138844977000021, 0.0, 1.0, 18735.6487910902], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4431600.0000, 
sim time next is 4432200.0000, 
raw observation next is [2.0, 80.0, 0.0, 0.0, 26.0, 25.58359069485099, 0.5047793785115325, 0.0, 1.0, 30181.95782463135], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.8, 0.0, 0.0, 0.6666666666666666, 0.6319658912375825, 0.6682597928371775, 0.0, 1.0, 0.14372360868872072], 
reward next is 0.8563, 
noisyNet noise sample is [array([0.7100273], dtype=float32), 0.9985537]. 
=============================================
[2019-04-04 11:07:58,059] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3345394e-14 9.4774855e-14 3.8468346e-25 5.4525454e-16 4.2247653e-15
 4.1815324e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:58,059] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5499
[2019-04-04 11:07:58,074] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.19642347960695, 0.3867503536874202, 0.0, 1.0, 40702.75644256443], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4516200.0000, 
sim time next is 4516800.0000, 
raw observation next is [-1.0, 71.0, 0.0, 0.0, 26.0, 25.19355665351097, 0.3927560601369486, 0.0, 1.0, 40624.09660022518], 
processed observation next is [1.0, 0.2608695652173913, 0.4349030470914128, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5994630544592475, 0.6309186867123162, 0.0, 1.0, 0.19344807904869132], 
reward next is 0.8066, 
noisyNet noise sample is [array([-1.1726656], dtype=float32), 0.33973813]. 
=============================================
[2019-04-04 11:07:58,860] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.3093445e-16 5.4085658e-16 1.4373898e-27 3.2180774e-17 4.8658248e-17
 9.5846206e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:07:58,874] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.8616
[2019-04-04 11:07:58,896] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 52.0, 120.1666666666667, 842.8333333333334, 26.0, 25.24076848422609, 0.4117986141407171, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4278000.0000, 
sim time next is 4278600.0000, 
raw observation next is [7.0, 52.0, 120.0, 847.0, 26.0, 25.26391760101993, 0.4155743766300264, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.5217391304347826, 0.6565096952908588, 0.52, 0.4, 0.9359116022099447, 0.6666666666666666, 0.6053264667516608, 0.6385247922100088, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.83968693], dtype=float32), 1.1939223]. 
=============================================
[2019-04-04 11:08:04,215] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.34024028e-16 2.76506219e-15 4.12646437e-27 1.01499574e-16
 4.92096595e-17 4.03181401e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 11:08:04,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6827
[2019-04-04 11:08:04,269] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 36.66666666666666, 94.0, 504.0, 26.0, 25.89728179235071, 0.4577334877616163, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4092000.0000, 
sim time next is 4092600.0000, 
raw observation next is [-3.166666666666667, 37.33333333333334, 96.0, 539.0, 26.0, 26.12780531471229, 0.4570973908542486, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.3748845798707295, 0.3733333333333334, 0.32, 0.5955801104972376, 0.6666666666666666, 0.6773171095593575, 0.6523657969514162, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.343917], dtype=float32), 1.3167696]. 
=============================================
[2019-04-04 11:08:05,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.0041048e-18 3.3193453e-16 8.9746265e-29 2.4596406e-18 2.8477634e-18
 1.7236801e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:05,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2299
[2019-04-04 11:08:05,644] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 28.0, 119.0, 840.5, 26.0, 28.17589291998954, 1.016395572454825, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4363200.0000, 
sim time next is 4363800.0000, 
raw observation next is [14.93333333333333, 28.16666666666667, 118.6666666666667, 844.6666666666667, 26.0, 28.25548361715531, 1.036435469940478, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5217391304347826, 0.8762696214219761, 0.28166666666666673, 0.39555555555555566, 0.9333333333333335, 0.6666666666666666, 0.8546236347629425, 0.8454784899801594, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.86225516], dtype=float32), 0.38152316]. 
=============================================
[2019-04-04 11:08:06,687] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.1279632e-15 1.0332845e-13 1.2261078e-25 7.7709439e-16 3.2949859e-15
 6.1420659e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:06,688] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9531
[2019-04-04 11:08:06,702] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.95, 61.5, 0.0, 0.0, 26.0, 26.19686179526865, 0.6603426127017838, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4401000.0000, 
sim time next is 4401600.0000, 
raw observation next is [8.8, 61.66666666666666, 0.0, 0.0, 26.0, 26.04423717980809, 0.6380758646738804, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.7063711911357342, 0.6166666666666666, 0.0, 0.0, 0.6666666666666666, 0.6703530983173408, 0.7126919548912936, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.72127527], dtype=float32), -0.2324308]. 
=============================================
[2019-04-04 11:08:07,335] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7262418e-16 3.0885589e-15 4.7613977e-26 5.1990432e-17 7.2978393e-17
 1.3743123e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:07,336] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3839
[2019-04-04 11:08:07,424] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 90.0, 117.6666666666667, 0.9999999999999998, 26.0, 25.57214090103567, 0.4742353228757457, 1.0, 1.0, 83733.46803582742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4452000.0000, 
sim time next is 4452600.0000, 
raw observation next is [0.1666666666666666, 91.0, 133.3333333333333, 2.0, 26.0, 24.53630381954985, 0.4570473803887548, 1.0, 1.0, 196296.7294068773], 
processed observation next is [1.0, 0.5217391304347826, 0.4672206832871654, 0.91, 0.4444444444444443, 0.0022099447513812156, 0.6666666666666666, 0.5446919849624875, 0.6523491267962517, 1.0, 1.0, 0.9347463305089395], 
reward next is 0.0653, 
noisyNet noise sample is [array([-1.0277765], dtype=float32), -0.24728252]. 
=============================================
[2019-04-04 11:08:08,285] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.6604285e-16 1.0001226e-14 1.9141535e-25 8.4330236e-17 5.7836687e-17
 7.5939595e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:08,293] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.1347
[2019-04-04 11:08:08,308] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 55.0, 41.33333333333333, 26.0, 26.02367656776397, 0.530022349507092, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4554600.0000, 
sim time next is 4555200.0000, 
raw observation next is [2.0, 52.0, 41.5, 34.66666666666666, 26.0, 26.07325012575709, 0.5075865406684029, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.518005540166205, 0.52, 0.13833333333333334, 0.03830570902394106, 0.6666666666666666, 0.6727708438130909, 0.6691955135561343, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.96122], dtype=float32), 1.9808192]. 
=============================================
[2019-04-04 11:08:14,590] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.1699293e-14 1.3730477e-13 7.9289834e-25 4.8529465e-15 4.4825441e-15
 3.5928606e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:14,591] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9501
[2019-04-04 11:08:14,610] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 68.0, 0.0, 0.0, 26.0, 24.41990923421628, 0.140045316235705, 0.0, 1.0, 39593.18264945376], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4865400.0000, 
sim time next is 4866000.0000, 
raw observation next is [-4.0, 69.0, 23.5, 52.16666666666666, 26.0, 24.39236020533478, 0.1438508383336206, 0.0, 1.0, 39564.89609712193], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.69, 0.07833333333333334, 0.05764272559852669, 0.6666666666666666, 0.5326966837778982, 0.5479502794445402, 0.0, 1.0, 0.18840426712915204], 
reward next is 0.8116, 
noisyNet noise sample is [array([-0.8050454], dtype=float32), -0.65940773]. 
=============================================
[2019-04-04 11:08:14,622] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[81.46866 ]
 [81.5054  ]
 [81.54963 ]
 [81.63523 ]
 [81.738945]], R is [[82.45809174]
 [82.44497681]
 [82.4320755 ]
 [82.41941071]
 [82.40702057]].
[2019-04-04 11:08:16,353] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3610009e-14 6.0355848e-14 4.5131477e-25 2.3823509e-15 3.6675307e-15
 1.9826831e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:16,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8641
[2019-04-04 11:08:16,365] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.42018563747419, 0.398769494746913, 0.0, 1.0, 26283.29589231608], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4831800.0000, 
sim time next is 4832400.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.4021136260202, 0.392895754940746, 0.0, 1.0, 40883.64912220329], 
processed observation next is [0.0, 0.9565217391304348, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.61684280216835, 0.6309652516469154, 0.0, 1.0, 0.1946840434390633], 
reward next is 0.8053, 
noisyNet noise sample is [array([0.78791076], dtype=float32), -0.59407073]. 
=============================================
[2019-04-04 11:08:20,947] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9423801e-15 7.8260464e-14 7.2906032e-25 1.5367420e-15 2.3593160e-15
 1.3839300e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:20,952] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7835
[2019-04-04 11:08:20,978] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 25.5752235081853, 0.544643416315428, 0.0, 1.0, 25842.07194853344], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4651800.0000, 
sim time next is 4652400.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 25.4972833345559, 0.548312644312596, 0.0, 1.0, 196217.9094192961], 
processed observation next is [1.0, 0.8695652173913043, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.6247736112129916, 0.682770881437532, 0.0, 1.0, 0.9343709972347434], 
reward next is 0.0656, 
noisyNet noise sample is [array([1.4169399], dtype=float32), 1.761813]. 
=============================================
[2019-04-04 11:08:21,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4964836e-14 4.3822507e-14 3.4774073e-25 7.6762213e-16 3.8305998e-15
 1.0647736e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:21,299] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2966
[2019-04-04 11:08:21,317] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 57.0, 0.0, 0.0, 26.0, 25.69145984037336, 0.4959008350120493, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4671000.0000, 
sim time next is 4671600.0000, 
raw observation next is [2.0, 58.66666666666666, 0.0, 0.0, 26.0, 25.62326620599688, 0.4820320350479985, 0.0, 1.0, 21528.19719528709], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.5866666666666666, 0.0, 0.0, 0.6666666666666666, 0.6352721838330734, 0.6606773450159995, 0.0, 1.0, 0.10251522473946234], 
reward next is 0.8975, 
noisyNet noise sample is [array([0.03395918], dtype=float32), 0.51117694]. 
=============================================
[2019-04-04 11:08:23,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2846708e-14 2.7674288e-13 3.9488925e-24 4.3291786e-15 7.5109444e-15
 4.7389684e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:23,233] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8016
[2019-04-04 11:08:23,248] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 79.33333333333334, 0.0, 0.0, 26.0, 25.16462895835867, 0.4179323106837782, 0.0, 1.0, 41895.62186974448], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4749600.0000, 
sim time next is 4750200.0000, 
raw observation next is [-3.5, 80.5, 0.0, 0.0, 26.0, 25.13333981593473, 0.4102831689391341, 0.0, 1.0, 41770.82818855055], 
processed observation next is [1.0, 1.0, 0.36565096952908593, 0.805, 0.0, 0.0, 0.6666666666666666, 0.5944449846612274, 0.6367610563130447, 0.0, 1.0, 0.1989087056597645], 
reward next is 0.8011, 
noisyNet noise sample is [array([1.2932225], dtype=float32), 0.052968424]. 
=============================================
[2019-04-04 11:08:28,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:28,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:28,090] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run34
[2019-04-04 11:08:30,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:30,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:30,437] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run34
[2019-04-04 11:08:31,171] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:31,171] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:31,183] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run34
[2019-04-04 11:08:33,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:33,783] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:33,792] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run34
[2019-04-04 11:08:34,918] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:34,918] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:34,936] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run34
[2019-04-04 11:08:35,441] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.10884205e-16 1.04394111e-15 6.32367349e-28 1.56820408e-17
 1.23915155e-17 1.22081428e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 11:08:35,441] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.3250
[2019-04-04 11:08:35,487] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.5, 42.5, 93.0, 560.0, 26.0, 25.23826389180049, 0.329059180771462, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4955400.0000, 
sim time next is 4956000.0000, 
raw observation next is [-1.333333333333333, 41.33333333333334, 95.5, 586.1666666666666, 26.0, 25.43766120046036, 0.3456016635134538, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.42566943674976926, 0.41333333333333344, 0.31833333333333336, 0.6476979742173112, 0.6666666666666666, 0.6198051000383634, 0.6152005545044846, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.50205296], dtype=float32), 0.6767593]. 
=============================================
[2019-04-04 11:08:35,489] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[92.24698 ]
 [91.70207 ]
 [91.10345 ]
 [90.143875]
 [88.77039 ]], R is [[92.05765533]
 [92.13707733]
 [92.21570587]
 [92.29354858]
 [92.3706131 ]].
[2019-04-04 11:08:37,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3168946e-10 1.6609246e-10 3.1088604e-19 1.5272398e-11 1.3207406e-11
 1.1666738e-13 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:37,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9477
[2019-04-04 11:08:37,097] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 96.0, 0.0, 0.0, 26.0, 20.05475781599167, -0.8306253765656183, 0.0, 1.0, 44594.31393830113], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5400.0000, 
sim time next is 6000.0000, 
raw observation next is [7.2, 96.0, 0.0, 0.0, 26.0, 20.14428388293994, -0.8131530050520679, 0.0, 1.0, 44218.62050822999], 
processed observation next is [0.0, 0.043478260869565216, 0.662049861495845, 0.96, 0.0, 0.0, 0.6666666666666666, 0.17869032357832845, 0.22894899831597737, 0.0, 1.0, 0.21056485956299997], 
reward next is 0.7894, 
noisyNet noise sample is [array([0.17878191], dtype=float32), 0.87266713]. 
=============================================
[2019-04-04 11:08:37,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.555786]
 [62.53318 ]
 [58.59775 ]
 [53.863487]
 [48.45039 ]], R is [[68.67617035]
 [68.77705383]
 [68.87468719]
 [68.9677887 ]
 [69.05226898]].
[2019-04-04 11:08:39,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:39,201] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:39,207] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run34
[2019-04-04 11:08:39,719] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.1584012e-16 1.1537366e-14 1.2348571e-25 1.5141865e-16 2.5510081e-16
 2.7547509e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:39,719] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9410
[2019-04-04 11:08:39,737] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.16666666666667, 17.0, 24.0, 194.6666666666667, 26.0, 28.50684658548913, 1.121711318949016, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5075400.0000, 
sim time next is 5076000.0000, 
raw observation next is [11.0, 17.0, 18.0, 146.0, 26.0, 28.56727501970288, 1.110467865862679, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.7673130193905818, 0.17, 0.06, 0.16132596685082873, 0.6666666666666666, 0.8806062516419066, 0.8701559552875597, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3905361], dtype=float32), 0.31530157]. 
=============================================
[2019-04-04 11:08:39,744] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[85.270676]
 [85.68938 ]
 [86.11164 ]
 [86.47648 ]
 [87.037125]], R is [[85.03747559]
 [85.18710327]
 [85.3352356 ]
 [85.48188782]
 [85.62706757]].
[2019-04-04 11:08:41,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:41,664] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:41,674] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run34
[2019-04-04 11:08:42,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:42,773] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:42,776] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run34
[2019-04-04 11:08:43,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:43,396] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:43,404] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run34
[2019-04-04 11:08:43,785] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8890834e-14 2.9767560e-14 9.9631338e-25 1.1542158e-15 3.6824609e-15
 1.8390621e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:08:43,785] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6638
[2019-04-04 11:08:43,807] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 91.0, 0.0, 0.0, 26.0, 24.32998788235721, 0.1452704966806129, 0.0, 1.0, 41215.53058806681], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 91200.0000, 
sim time next is 91800.0000, 
raw observation next is [-1.15, 91.0, 0.0, 0.0, 26.0, 24.35063198306668, 0.1370149550293152, 0.0, 1.0, 41418.3595285769], 
processed observation next is [1.0, 0.043478260869565216, 0.4307479224376732, 0.91, 0.0, 0.0, 0.6666666666666666, 0.5292193319222234, 0.5456716516764384, 0.0, 1.0, 0.1972302834694138], 
reward next is 0.8028, 
noisyNet noise sample is [array([0.68550354], dtype=float32), -1.2588437]. 
=============================================
[2019-04-04 11:08:43,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:43,817] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:43,821] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run34
[2019-04-04 11:08:43,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:43,960] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:43,964] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run34
[2019-04-04 11:08:44,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:44,609] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:44,612] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run34
[2019-04-04 11:08:45,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:45,875] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:45,879] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run34
[2019-04-04 11:08:46,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:46,573] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:46,588] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run34
[2019-04-04 11:08:46,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:08:46,739] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:08:46,751] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run34
[2019-04-04 11:09:04,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:09:04,909] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:04,912] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run34
[2019-04-04 11:09:17,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8927256e-14 2.3680602e-13 4.0978583e-24 7.8664492e-15 9.2847610e-15
 7.7882416e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:09:17,103] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4965
[2019-04-04 11:09:17,148] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.9, 78.0, 0.0, 0.0, 26.0, 22.71178837462841, -0.2542553896173845, 0.0, 1.0, 44953.25353150383], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [-8.900000000000002, 78.0, 0.0, 0.0, 26.0, 22.65610158040396, -0.2598757296748834, 0.0, 1.0, 44957.35091117842], 
processed observation next is [1.0, 0.2608695652173913, 0.2160664819944598, 0.78, 0.0, 0.0, 0.6666666666666666, 0.38800846503366326, 0.4133747567750388, 0.0, 1.0, 0.21408262338656392], 
reward next is 0.7859, 
noisyNet noise sample is [array([-0.6671624], dtype=float32), -0.6554637]. 
=============================================
[2019-04-04 11:09:20,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.30616748e-14 1.85196327e-13 1.09010385e-23 5.32004665e-15
 1.00529021e-14 2.43402170e-16 1.00000000e+00], sum to 1.0000
[2019-04-04 11:09:20,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6393
[2019-04-04 11:09:21,040] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.800000000000001, 67.66666666666667, 0.0, 0.0, 26.0, 24.12653228574858, 0.06556779116206439, 0.0, 1.0, 45023.6349022385], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 263400.0000, 
sim time next is 264000.0000, 
raw observation next is [-6.9, 68.33333333333334, 0.0, 0.0, 26.0, 24.08343087015335, 0.0535814927209981, 0.0, 1.0, 45188.94413755413], 
processed observation next is [1.0, 0.043478260869565216, 0.27146814404432135, 0.6833333333333335, 0.0, 0.0, 0.6666666666666666, 0.5069525725127791, 0.517860497573666, 0.0, 1.0, 0.21518544827406727], 
reward next is 0.7848, 
noisyNet noise sample is [array([-0.51437455], dtype=float32), -1.0042471]. 
=============================================
[2019-04-04 11:09:21,095] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.572266]
 [75.83917 ]
 [76.16646 ]
 [76.4617  ]
 [76.71348 ]], R is [[75.37277222]
 [75.4046402 ]
 [75.43704224]
 [75.46988678]
 [75.50296783]].
[2019-04-04 11:09:24,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3832015e-15 6.2051574e-14 2.0467203e-25 1.4207445e-15 1.3310966e-15
 6.9021811e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:09:24,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8648
[2019-04-04 11:09:24,810] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.63333333333333, 69.0, 0.0, 0.0, 26.0, 23.29202247530414, -0.02481986599102237, 1.0, 1.0, 164685.916841178], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 286800.0000, 
sim time next is 287400.0000, 
raw observation next is [-12.71666666666667, 69.5, 9.999999999999998, 145.3333333333333, 26.0, 23.89287496817208, 0.06762107324930051, 1.0, 1.0, 113684.2988800308], 
processed observation next is [1.0, 0.30434782608695654, 0.1103416435826407, 0.695, 0.033333333333333326, 0.16058931860036826, 0.6666666666666666, 0.49107291401434, 0.5225403577497668, 1.0, 1.0, 0.5413538041906228], 
reward next is 0.4586, 
noisyNet noise sample is [array([0.7035415], dtype=float32), 0.6006697]. 
=============================================
[2019-04-04 11:09:38,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0543749e-15 1.3215928e-14 2.7960840e-25 7.6662422e-16 1.7763516e-16
 1.1136061e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:09:38,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6156
[2019-04-04 11:09:39,040] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3666666666666667, 36.0, 62.33333333333334, 0.0, 26.0, 25.71793706847497, 0.2906890455382483, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 487200.0000, 
sim time next is 487800.0000, 
raw observation next is [0.55, 35.5, 56.0, 0.0, 26.0, 25.73337256856009, 0.2851640153959574, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.4778393351800555, 0.355, 0.18666666666666668, 0.0, 0.6666666666666666, 0.6444477140466741, 0.5950546717986525, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1067979], dtype=float32), 1.2941438]. 
=============================================
[2019-04-04 11:09:54,510] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-04 11:09:54,532] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:09:54,549] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:54,551] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run46
[2019-04-04 11:09:54,538] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:09:54,593] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:54,626] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:09:54,626] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:09:54,628] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run46
[2019-04-04 11:09:54,665] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run46
[2019-04-04 11:12:50,215] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15984598], dtype=float32), -0.30624762]
[2019-04-04 11:12:50,215] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.883333333333334, 77.33333333333333, 0.0, 0.0, 26.0, 25.60766623466373, 0.4972466816954841, 0.0, 1.0, 30587.45308333556]
[2019-04-04 11:12:50,215] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:12:50,217] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [9.4560835e-15 2.8631171e-14 1.5615239e-25 5.6201795e-16 1.7659409e-15
 5.9666672e-18 1.0000000e+00], sampled 0.6880060244119331
[2019-04-04 11:13:07,779] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:13:22,566] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15984598], dtype=float32), -0.30624762]
[2019-04-04 11:13:22,566] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [6.012203333, 68.75936912, 0.0, 0.0, 26.0, 25.75929477682894, 0.5481527394397435, 0.0, 1.0, 0.0]
[2019-04-04 11:13:22,566] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 11:13:22,567] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [6.1999610e-15 2.2543914e-14 9.2868699e-26 4.0412747e-16 1.1924501e-15
 4.0332007e-18 1.0000000e+00], sampled 0.21875203969070123
[2019-04-04 11:13:29,080] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15984598], dtype=float32), -0.30624762]
[2019-04-04 11:13:29,081] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.0, 92.0, 0.0, 0.0, 26.0, 24.31275900916256, 0.188153224704694, 0.0, 1.0, 41334.46556223487]
[2019-04-04 11:13:29,081] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:13:29,082] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [1.9325653e-14 6.2749380e-14 8.2943025e-25 1.4159399e-15 3.4161679e-15
 1.6032937e-17 1.0000000e+00], sampled 0.4613887239834087
[2019-04-04 11:13:29,840] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15984598], dtype=float32), -0.30624762]
[2019-04-04 11:13:29,840] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.786133922, 56.93091737, 121.0466029, 831.9324485, 26.0, 26.53208371532302, 0.6646260479833023, 1.0, 1.0, 0.0]
[2019-04-04 11:13:29,840] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 11:13:29,841] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [8.3547346e-17 5.0427032e-16 4.2325145e-27 1.0775262e-17 1.5341866e-17
 8.7618880e-20 1.0000000e+00], sampled 0.36429773912007335
[2019-04-04 11:13:39,634] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:13:44,738] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:13:45,776] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 4500000, evaluation results [4500000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:13:46,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.67062575e-14 1.25947958e-13 8.58728442e-25 2.85956820e-15
 3.71855595e-15 1.13577415e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:13:46,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7853
[2019-04-04 11:13:46,464] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.816666666666666, 70.66666666666667, 0.0, 0.0, 26.0, 24.68520167067255, 0.1480111871376663, 0.0, 1.0, 41630.89268653015], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 687000.0000, 
sim time next is 687600.0000, 
raw observation next is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.65054447446813, 0.1402517569249488, 0.0, 1.0, 41573.78653294971], 
processed observation next is [0.0, 1.0, 0.3545706371191136, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5542120395390109, 0.5467505856416496, 0.0, 1.0, 0.19797041206166527], 
reward next is 0.8020, 
noisyNet noise sample is [array([1.2504119], dtype=float32), -0.644946]. 
=============================================
[2019-04-04 11:13:49,695] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7358401e-15 2.3634067e-14 1.7900843e-25 7.0789000e-16 7.6211054e-16
 4.4652996e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:13:49,696] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6191
[2019-04-04 11:13:49,801] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 71.0, 0.0, 0.0, 26.0, 24.53252747453214, 0.1198663993339551, 0.0, 1.0, 41378.19103111013], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 689400.0000, 
sim time next is 690000.0000, 
raw observation next is [-3.899999999999999, 71.0, 0.0, 0.0, 26.0, 24.49943217045107, 0.1139857434458505, 0.0, 1.0, 41302.88315288259], 
processed observation next is [0.0, 1.0, 0.35457063711911363, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5416193475375891, 0.5379952478152835, 0.0, 1.0, 0.19668039596610756], 
reward next is 0.8033, 
noisyNet noise sample is [array([-0.84402394], dtype=float32), 0.22247285]. 
=============================================
[2019-04-04 11:13:49,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.072075]
 [81.09169 ]
 [81.11402 ]
 [81.155716]
 [81.197205]], R is [[81.05194092]
 [81.04438782]
 [81.03657532]
 [81.02852631]
 [81.0202713 ]].
[2019-04-04 11:14:27,910] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6960866e-16 5.6510752e-15 2.4894976e-26 5.3860797e-17 2.0389446e-16
 4.9836231e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:14:27,910] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0179
[2019-04-04 11:14:27,955] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 92.16666666666667, 12.0, 0.0, 26.0, 24.51399361889501, 0.310557693130901, 1.0, 1.0, 196475.6107603647], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [5.0, 92.0, 9.0, 0.0, 26.0, 25.00417326812636, 0.3543358689796134, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.6011080332409973, 0.92, 0.03, 0.0, 0.6666666666666666, 0.5836811056771968, 0.6181119563265378, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4671834], dtype=float32), -1.1883141]. 
=============================================
[2019-04-04 11:14:29,771] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.1395103e-18 1.1715331e-16 1.9720064e-28 8.0167212e-19 2.2658148e-18
 1.3249930e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:14:29,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0447
[2019-04-04 11:14:29,785] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 63.5, 0.0, 26.0, 27.53928108468592, 0.973940523660274, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1094400.0000, 
sim time next is 1095000.0000, 
raw observation next is [19.11666666666667, 49.16666666666667, 54.0, 0.0, 26.0, 27.72136614412108, 0.9971248320740161, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.9921514312096033, 0.4916666666666667, 0.18, 0.0, 0.6666666666666666, 0.8101138453434235, 0.832374944024672, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.99778116], dtype=float32), 0.4133059]. 
=============================================
[2019-04-04 11:14:29,813] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[95.363976]
 [95.32286 ]
 [95.37362 ]
 [95.51571 ]
 [95.571655]], R is [[95.40847015]
 [95.45438385]
 [95.49983978]
 [95.54484558]
 [95.58940125]].
[2019-04-04 11:14:33,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5002747e-16 2.3328880e-15 2.0155787e-27 1.6792762e-17 6.1427484e-17
 5.4541155e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:14:33,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9807
[2019-04-04 11:14:33,848] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [8.066666666666666, 83.0, 0.0, 0.0, 26.0, 25.42227444252448, 0.425535941629112, 0.0, 1.0, 69439.4021557638], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 966000.0000, 
sim time next is 966600.0000, 
raw observation next is [8.25, 83.0, 0.0, 0.0, 26.0, 25.35423787129927, 0.4364201088076314, 0.0, 1.0, 79699.27299084108], 
processed observation next is [1.0, 0.17391304347826086, 0.6911357340720222, 0.83, 0.0, 0.0, 0.6666666666666666, 0.6128531559416057, 0.6454733696025438, 0.0, 1.0, 0.37952034757543374], 
reward next is 0.6205, 
noisyNet noise sample is [array([0.83630913], dtype=float32), -1.4033597]. 
=============================================
[2019-04-04 11:14:42,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0125703e-17 5.5746139e-16 2.0585183e-27 7.1901219e-18 1.0830524e-17
 5.0807702e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:14:42,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1438
[2019-04-04 11:14:42,572] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 84.66666666666666, 0.0, 0.0, 26.0, 24.80948289449796, 0.3005154191156528, 1.0, 1.0, 153271.379853048], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 844800.0000, 
sim time next is 845400.0000, 
raw observation next is [-3.9, 85.33333333333334, 0.0, 0.0, 26.0, 24.93457787165721, 0.3115386988878862, 1.0, 1.0, 24845.90572409586], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.8533333333333334, 0.0, 0.0, 0.6666666666666666, 0.5778814893047676, 0.6038462329626287, 1.0, 1.0, 0.11831383678140886], 
reward next is 0.8817, 
noisyNet noise sample is [array([-1.029079], dtype=float32), 0.82538944]. 
=============================================
[2019-04-04 11:14:50,535] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7631868e-17 1.0930439e-15 5.2607227e-27 1.7769810e-17 3.9991757e-17
 8.1756121e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:14:50,537] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7220
[2019-04-04 11:14:50,610] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.183333333333333, 100.0, 12.0, 0.0, 26.0, 25.42195231748854, 0.4350962030989722, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1498200.0000, 
sim time next is 1498800.0000, 
raw observation next is [1.266666666666667, 100.0, 15.0, 0.0, 26.0, 25.39757871601828, 0.4608725881633002, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4976915974145891, 1.0, 0.05, 0.0, 0.6666666666666666, 0.6164648930015234, 0.6536241960544334, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1943433], dtype=float32), 0.47747034]. 
=============================================
[2019-04-04 11:14:55,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.7912904e-16 2.7961195e-15 2.1688781e-26 6.3796550e-17 2.8983006e-16
 6.1242581e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:14:55,378] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3928
[2019-04-04 11:14:55,400] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.26462592852419, 0.5004730076571194, 0.0, 1.0, 40980.64464623587], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1381800.0000, 
sim time next is 1382400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.28457464495722, 0.4981197988760955, 0.0, 1.0, 40775.94494718486], 
processed observation next is [1.0, 0.0, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6070478870797684, 0.6660399329586985, 0.0, 1.0, 0.194171166415166], 
reward next is 0.8058, 
noisyNet noise sample is [array([-0.25847894], dtype=float32), -0.70356333]. 
=============================================
[2019-04-04 11:14:56,605] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [9.0926774e-16 4.5411825e-15 2.6719619e-26 6.0274997e-17 1.8934211e-16
 9.8865361e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:14:56,607] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.4977
[2019-04-04 11:14:56,626] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.516666666666667, 92.0, 0.0, 0.0, 26.0, 25.31672357982757, 0.4875627932713815, 0.0, 1.0, 46622.74766997824], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1464600.0000, 
sim time next is 1465200.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.33615245134857, 0.4938482177562659, 0.0, 1.0, 41248.08623204422], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6113460376123809, 0.664616072585422, 0.0, 1.0, 0.1964194582478296], 
reward next is 0.8036, 
noisyNet noise sample is [array([2.3943298], dtype=float32), 1.1058443]. 
=============================================
[2019-04-04 11:15:00,092] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.39637026e-16 2.13176408e-16 1.10955508e-28 1.34564055e-17
 7.86301760e-18 2.06729096e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 11:15:00,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3012
[2019-04-04 11:15:00,104] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.7, 67.0, 0.0, 0.0, 26.0, 24.68305734964438, 0.3985045341895923, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1192800.0000, 
sim time next is 1193400.0000, 
raw observation next is [17.7, 67.0, 0.0, 0.0, 26.0, 24.65985789859591, 0.394385506145108, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.8260869565217391, 0.9529085872576178, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5549881582163257, 0.6314618353817026, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3654157], dtype=float32), -0.4804894]. 
=============================================
[2019-04-04 11:15:01,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7732965e-16 1.4520409e-16 4.6498690e-29 4.5623763e-18 2.8372146e-17
 4.6635304e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:01,796] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0131
[2019-04-04 11:15:01,801] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.66666666666666, 18.66666666666667, 0.0, 26.0, 23.41746227326142, 0.1319165639956698, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1239000.0000, 
sim time next is 1239600.0000, 
raw observation next is [15.0, 97.33333333333333, 23.33333333333334, 0.0, 26.0, 23.40081039658282, 0.1293995890542536, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.34782608695652173, 0.8781163434903049, 0.9733333333333333, 0.07777777777777779, 0.0, 0.6666666666666666, 0.45006753304856834, 0.5431331963514179, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.901831], dtype=float32), 1.2308016]. 
=============================================
[2019-04-04 11:15:04,052] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.0444968e-17 3.6219118e-16 7.4761350e-28 2.7999791e-18 3.8544718e-18
 4.0792131e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:04,061] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7021
[2019-04-04 11:15:04,092] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.483333333333333, 95.5, 83.0, 472.0000000000001, 26.0, 26.13726231911924, 0.583333948388435, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1509000.0000, 
sim time next is 1509600.0000, 
raw observation next is [3.666666666666667, 95.0, 85.5, 590.0, 26.0, 26.19653379552953, 0.6130434644323003, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.564173591874423, 0.95, 0.285, 0.6519337016574586, 0.6666666666666666, 0.6830444829607941, 0.7043478214774335, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0201149], dtype=float32), -1.0268584]. 
=============================================
[2019-04-04 11:15:08,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7496200e-15 1.4210813e-14 1.1609485e-25 2.5171268e-16 2.6297290e-15
 1.0661102e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:08,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3651
[2019-04-04 11:15:08,046] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 82.0, 0.0, 0.0, 26.0, 25.60266963342723, 0.573493313154922, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1558200.0000, 
sim time next is 1558800.0000, 
raw observation next is [5.0, 82.0, 0.0, 0.0, 26.0, 25.68652467809071, 0.5629039060301487, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.6011080332409973, 0.82, 0.0, 0.0, 0.6666666666666666, 0.6405437231742258, 0.6876346353433829, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.42215836], dtype=float32), 1.3541256]. 
=============================================
[2019-04-04 11:15:09,928] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.1094133e-16 4.0211470e-15 4.9321602e-27 5.0818827e-17 1.9105147e-16
 2.9975503e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:09,929] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.2358
[2019-04-04 11:15:09,938] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.983333333333333, 72.66666666666667, 0.0, 0.0, 26.0, 25.65166767937008, 0.591841409681305, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [7.7, 74.0, 0.0, 0.0, 26.0, 25.57808109625306, 0.5760091901266039, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.6759002770083103, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6315067580210885, 0.6920030633755346, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.819357], dtype=float32), -0.6842361]. 
=============================================
[2019-04-04 11:15:11,028] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0739430e-16 4.4518531e-15 1.4106646e-26 3.8812830e-17 2.0906836e-16
 3.0716801e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:11,034] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5126
[2019-04-04 11:15:11,047] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.2, 84.0, 0.0, 0.0, 26.0, 25.65652957759394, 0.604656001609848, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1639800.0000, 
sim time next is 1640400.0000, 
raw observation next is [7.2, 84.66666666666666, 0.0, 0.0, 26.0, 25.64535006403376, 0.6022782529451448, 0.0, 1.0, 18729.81385330287], 
processed observation next is [1.0, 1.0, 0.662049861495845, 0.8466666666666666, 0.0, 0.0, 0.6666666666666666, 0.6371125053361467, 0.7007594176483817, 0.0, 1.0, 0.08918958977763271], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.04964982], dtype=float32), 0.8469135]. 
=============================================
[2019-04-04 11:15:14,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.8310985e-14 3.7232492e-14 1.9849539e-24 8.4619010e-16 1.5871998e-15
 1.8250986e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:14,166] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.8635
[2019-04-04 11:15:14,221] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 80.33333333333333, 24.33333333333334, 0.0, 26.0, 25.04510848708372, 0.255459612836952, 0.0, 1.0, 32612.02636076955], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1874400.0000, 
sim time next is 1875000.0000, 
raw observation next is [-4.5, 81.66666666666667, 19.66666666666667, 0.0, 26.0, 25.03009228933469, 0.2506010356942625, 0.0, 1.0, 46018.72325876683], 
processed observation next is [0.0, 0.6956521739130435, 0.3379501385041552, 0.8166666666666668, 0.06555555555555558, 0.0, 0.6666666666666666, 0.5858410241112241, 0.5835336785647541, 0.0, 1.0, 0.2191367774226992], 
reward next is 0.7809, 
noisyNet noise sample is [array([-0.01307118], dtype=float32), -0.17418118]. 
=============================================
[2019-04-04 11:15:14,225] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[78.46124]
 [78.47977]
 [78.65007]
 [78.82712]
 [78.90541]], R is [[78.62869263]
 [78.6871109 ]
 [78.77362823]
 [78.76551819]
 [78.72770691]].
[2019-04-04 11:15:18,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3004162e-14 3.5242141e-14 4.3381686e-24 2.6250284e-15 3.0582758e-15
 2.1223669e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:18,412] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1929
[2019-04-04 11:15:18,466] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.8, 85.0, 99.0, 0.0, 26.0, 24.96520802652209, 0.3382100961755444, 0.0, 1.0, 69163.63041404121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1780200.0000, 
sim time next is 1780800.0000, 
raw observation next is [-2.8, 85.66666666666667, 93.5, 0.0, 26.0, 24.95447858253507, 0.3427841864435955, 0.0, 1.0, 60846.71760336556], 
processed observation next is [0.0, 0.6086956521739131, 0.38504155124653744, 0.8566666666666667, 0.31166666666666665, 0.0, 0.6666666666666666, 0.5795398818779226, 0.6142613954811985, 0.0, 1.0, 0.28974627430174077], 
reward next is 0.7103, 
noisyNet noise sample is [array([0.89265615], dtype=float32), 0.5600862]. 
=============================================
[2019-04-04 11:15:24,052] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [2.8115334e-14 3.0635214e-13 7.9993186e-24 5.2559407e-15 2.3842068e-14
 1.5111686e-16 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:24,055] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.1398
[2019-04-04 11:15:24,094] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 81.50000000000001, 0.0, 0.0, 26.0, 24.684988473564, 0.2254813504471681, 0.0, 1.0, 45534.38447824944], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1811400.0000, 
sim time next is 1812000.0000, 
raw observation next is [-5.0, 81.0, 0.0, 0.0, 26.0, 24.65318370241686, 0.2188010437226177, 0.0, 1.0, 45499.18047756444], 
processed observation next is [0.0, 1.0, 0.32409972299168976, 0.81, 0.0, 0.0, 0.6666666666666666, 0.5544319752014051, 0.5729336812408726, 0.0, 1.0, 0.21666276417887828], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.20777586], dtype=float32), -0.14018688]. 
=============================================
[2019-04-04 11:15:24,097] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[77.25575 ]
 [77.28115 ]
 [77.298965]
 [77.30073 ]
 [77.26274 ]], R is [[77.24394989]
 [77.25468445]
 [77.26506805]
 [77.27507019]
 [77.28470612]].
[2019-04-04 11:15:49,596] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0189266e-14 6.1399230e-14 1.6569986e-24 1.6761484e-15 2.0257968e-15
 1.6396440e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:49,597] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3980
[2019-04-04 11:15:49,622] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.79955480443573, 0.0190198850787707, 0.0, 1.0, 43439.39486182722], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2098800.0000, 
sim time next is 2099400.0000, 
raw observation next is [-6.800000000000001, 78.16666666666667, 0.0, 0.0, 26.0, 23.77552735957714, 0.01076023100295946, 0.0, 1.0, 43331.41048358567], 
processed observation next is [1.0, 0.30434782608695654, 0.2742382271468144, 0.7816666666666667, 0.0, 0.0, 0.6666666666666666, 0.4812939466314283, 0.5035867436676532, 0.0, 1.0, 0.20634004992183652], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.34146005], dtype=float32), -0.35092732]. 
=============================================
[2019-04-04 11:15:57,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.8674783e-14 8.4171595e-14 8.4480899e-25 2.6049977e-15 6.5088405e-15
 2.2819498e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:15:57,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4123
[2019-04-04 11:15:57,443] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.00136814149153, 0.0544200177485234, 0.0, 1.0, 41985.49149567245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97494249058295, 0.04286466206617429, 0.0, 1.0, 41960.29159501617], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.4979118742152459, 0.5142882206887248, 0.0, 1.0, 0.19981091235721984], 
reward next is 0.8002, 
noisyNet noise sample is [array([-0.23811725], dtype=float32), -1.8600475]. 
=============================================
[2019-04-04 11:16:02,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.26951880e-15 1.11236914e-13 3.25011580e-25 1.07492441e-15
 4.45348367e-15 1.45584499e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:16:02,204] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8827
[2019-04-04 11:16:02,228] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.70190822272131, 0.2351848978657849, 0.0, 1.0, 38909.75849395445], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2340000.0000, 
sim time next is 2340600.0000, 
raw observation next is [-2.3, 62.0, 0.0, 0.0, 26.0, 24.69102359902441, 0.2397118535487346, 0.0, 1.0, 38992.93789455893], 
processed observation next is [0.0, 0.08695652173913043, 0.3988919667590028, 0.62, 0.0, 0.0, 0.6666666666666666, 0.5575852999187006, 0.5799039511829115, 0.0, 1.0, 0.1856806566407568], 
reward next is 0.8143, 
noisyNet noise sample is [array([0.56927717], dtype=float32), 0.34480578]. 
=============================================
[2019-04-04 11:16:03,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3684194e-14 1.0800057e-13 3.7278703e-24 2.1506577e-15 4.4195346e-15
 9.8384217e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:03,051] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2729
[2019-04-04 11:16:03,072] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.2, 75.0, 0.0, 0.0, 26.0, 24.00136814149153, 0.0544200177485234, 0.0, 1.0, 41985.49149567245], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2178000.0000, 
sim time next is 2178600.0000, 
raw observation next is [-6.2, 75.66666666666667, 0.0, 0.0, 26.0, 23.97494249058295, 0.04286466206617429, 0.0, 1.0, 41960.29159501617], 
processed observation next is [1.0, 0.21739130434782608, 0.2908587257617729, 0.7566666666666667, 0.0, 0.0, 0.6666666666666666, 0.4979118742152459, 0.5142882206887248, 0.0, 1.0, 0.19981091235721984], 
reward next is 0.8002, 
noisyNet noise sample is [array([0.01586313], dtype=float32), 0.10395257]. 
=============================================
[2019-04-04 11:16:15,077] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4872564e-14 2.5236479e-14 3.9019636e-24 1.5359685e-15 8.6442153e-16
 9.6746283e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:15,078] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6072
[2019-04-04 11:16:15,124] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4, 53.33333333333334, 201.1666666666667, 70.66666666666666, 26.0, 24.99199780840915, 0.297906145564304, 0.0, 1.0, 42924.709393446], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2380800.0000, 
sim time next is 2381400.0000, 
raw observation next is [-0.3, 53.0, 191.0, 0.0, 26.0, 24.93904132143366, 0.290751477293717, 0.0, 1.0, 62167.99423955715], 
processed observation next is [0.0, 0.5652173913043478, 0.4542936288088643, 0.53, 0.6366666666666667, 0.0, 0.6666666666666666, 0.5782534434528049, 0.5969171590979057, 0.0, 1.0, 0.296038067807415], 
reward next is 0.7040, 
noisyNet noise sample is [array([-1.2023063], dtype=float32), -0.40376478]. 
=============================================
[2019-04-04 11:16:18,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2442513e-15 3.4549488e-14 5.5340979e-25 1.5941718e-15 1.9649458e-15
 8.4643456e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:18,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5325
[2019-04-04 11:16:18,396] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.833333333333333, 42.0, 0.0, 0.0, 26.0, 24.84119687806032, 0.2029527566834534, 0.0, 1.0, 43015.9288663403], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2414400.0000, 
sim time next is 2415000.0000, 
raw observation next is [-4.916666666666667, 41.5, 0.0, 0.0, 26.0, 24.79086778737992, 0.1936433812443531, 0.0, 1.0, 43038.90527238927], 
processed observation next is [0.0, 0.9565217391304348, 0.32640812557710064, 0.415, 0.0, 0.0, 0.6666666666666666, 0.5659056489483266, 0.5645477937481177, 0.0, 1.0, 0.20494716796375842], 
reward next is 0.7951, 
noisyNet noise sample is [array([-2.0325189], dtype=float32), 1.0890949]. 
=============================================
[2019-04-04 11:16:18,401] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.76986]
 [79.77541]
 [79.83213]
 [79.88202]
 [79.92715]], R is [[79.71780396]
 [79.71578979]
 [79.71391296]
 [79.71213531]
 [79.71034241]].
[2019-04-04 11:16:30,095] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5281349e-14 1.6480627e-13 1.6319815e-24 1.0986833e-15 3.7275459e-15
 1.1333202e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:30,095] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2438
[2019-04-04 11:16:30,108] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.333333333333334, 70.16666666666667, 0.0, 0.0, 26.0, 24.53405557434862, 0.2117973060740423, 0.0, 1.0, 44390.65573560485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2682600.0000, 
sim time next is 2683200.0000, 
raw observation next is [-9.666666666666668, 71.33333333333334, 0.0, 0.0, 26.0, 24.4648809721929, 0.193055691025741, 0.0, 1.0, 44405.43176912737], 
processed observation next is [1.0, 0.043478260869565216, 0.19482917820867957, 0.7133333333333334, 0.0, 0.0, 0.6666666666666666, 0.5387400810160751, 0.5643518970085803, 0.0, 1.0, 0.2114544369958446], 
reward next is 0.7885, 
noisyNet noise sample is [array([1.6258318], dtype=float32), -0.24281415]. 
=============================================
[2019-04-04 11:16:30,331] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5921966e-15 1.1827739e-14 2.5396169e-25 2.3600890e-16 5.8904593e-16
 5.6009545e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:30,331] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5098
[2019-04-04 11:16:30,352] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.716666666666666, 58.5, 0.0, 0.0, 26.0, 25.14740143028855, 0.3207641495639217, 0.0, 1.0, 44455.70435239533], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2587800.0000, 
sim time next is 2588400.0000, 
raw observation next is [-3.9, 59.0, 0.0, 0.0, 26.0, 25.0812523447461, 0.30917607444671, 0.0, 1.0, 42696.59854554223], 
processed observation next is [1.0, 1.0, 0.3545706371191136, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5901043620621751, 0.6030586914822367, 0.0, 1.0, 0.20331713593115347], 
reward next is 0.7967, 
noisyNet noise sample is [array([1.7993549], dtype=float32), 0.58538306]. 
=============================================
[2019-04-04 11:16:39,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.4083565e-14 1.5283543e-13 4.8335432e-24 5.5412662e-15 7.3378778e-15
 6.6498042e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:39,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9025
[2019-04-04 11:16:39,915] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.9166666666666667, 28.33333333333334, 0.0, 0.0, 26.0, 24.92738762874274, 0.2196868400637076, 0.0, 1.0, 18719.20271423861], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2484600.0000, 
sim time next is 2485200.0000, 
raw observation next is [0.7333333333333335, 28.66666666666667, 0.0, 0.0, 26.0, 24.92754968502823, 0.2144898493446322, 0.0, 1.0, 28515.69409870524], 
processed observation next is [0.0, 0.782608695652174, 0.4829178208679595, 0.28666666666666674, 0.0, 0.0, 0.6666666666666666, 0.5772958070856857, 0.5714966164482107, 0.0, 1.0, 0.135789019517644], 
reward next is 0.8642, 
noisyNet noise sample is [array([0.5706246], dtype=float32), 0.28261304]. 
=============================================
[2019-04-04 11:16:44,191] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.2740835e-15 2.4289969e-14 3.2438483e-25 1.3750898e-16 9.3311638e-16
 1.1014223e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:44,191] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7210
[2019-04-04 11:16:44,258] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 93.0, 0.0, 0.0, 26.0, 24.80923560618925, 0.2532786808668571, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2877600.0000, 
sim time next is 2878200.0000, 
raw observation next is [2.0, 93.0, 0.0, 0.0, 26.0, 24.93755501017888, 0.2601049879219444, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.518005540166205, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5781295841815733, 0.5867016626406482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.16706482], dtype=float32), -1.3121928]. 
=============================================
[2019-04-04 11:16:44,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.9590564e-15 4.1919701e-14 5.4793135e-25 6.3211803e-16 2.9619332e-15
 4.7719272e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:44,936] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8613
[2019-04-04 11:16:44,958] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.79190346425975, 0.2460362687841413, 0.0, 1.0, 41940.63978032485], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2770800.0000, 
sim time next is 2771400.0000, 
raw observation next is [-6.0, 59.0, 0.0, 0.0, 26.0, 24.80873738397342, 0.237053059124621, 0.0, 1.0, 41810.85104890822], 
processed observation next is [1.0, 0.043478260869565216, 0.296398891966759, 0.59, 0.0, 0.0, 0.6666666666666666, 0.5673947819977849, 0.5790176863748736, 0.0, 1.0, 0.19909929070908675], 
reward next is 0.8009, 
noisyNet noise sample is [array([-0.28409782], dtype=float32), 1.1779251]. 
=============================================
[2019-04-04 11:16:51,044] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5345350e-15 5.8354133e-14 5.2934401e-25 8.5043903e-16 1.9522816e-15
 6.2552357e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:51,046] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5145
[2019-04-04 11:16:51,067] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 76.66666666666667, 0.0, 0.0, 26.0, 25.13627731932959, 0.3070852976943839, 0.0, 1.0, 54032.04765298268], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2857200.0000, 
sim time next is 2857800.0000, 
raw observation next is [1.0, 77.83333333333334, 0.0, 0.0, 26.0, 25.15195411214892, 0.3102157696109882, 0.0, 1.0, 55492.44893933398], 
processed observation next is [1.0, 0.043478260869565216, 0.4903047091412743, 0.7783333333333334, 0.0, 0.0, 0.6666666666666666, 0.5959961760124101, 0.6034052565369961, 0.0, 1.0, 0.26424975685397134], 
reward next is 0.7358, 
noisyNet noise sample is [array([-0.9098638], dtype=float32), 0.9857869]. 
=============================================
[2019-04-04 11:16:53,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5990805e-16 6.3792237e-15 4.7227791e-25 2.4610511e-16 5.8595320e-16
 6.4152088e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:16:53,129] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7969
[2019-04-04 11:16:53,151] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 96.5, 49.0, 0.0, 26.0, 25.38981470608445, 0.3097593843282179, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2885400.0000, 
sim time next is 2886000.0000, 
raw observation next is [0.3333333333333334, 97.66666666666666, 53.83333333333333, 0.0, 26.0, 25.43062423849855, 0.3127764253049045, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4718374884579871, 0.9766666666666666, 0.17944444444444443, 0.0, 0.6666666666666666, 0.6192186865415458, 0.6042588084349682, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.05308063], dtype=float32), 0.16930874]. 
=============================================
[2019-04-04 11:16:53,161] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.84912]
 [81.24586]
 [81.67305]
 [82.39575]
 [83.39958]], R is [[81.21367645]
 [81.40154266]
 [81.58752441]
 [81.77165222]
 [81.95393372]].
[2019-04-04 11:17:00,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2476314e-16 1.5080949e-14 1.8698914e-24 2.4468410e-16 3.2082853e-16
 2.2188136e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:00,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9608
[2019-04-04 11:17:00,102] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.666666666666666, 57.33333333333333, 0.0, 0.0, 26.0, 25.25574474194425, 0.4008364151675547, 1.0, 1.0, 52968.93920383426], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2745600.0000, 
sim time next is 2746200.0000, 
raw observation next is [-4.833333333333334, 58.16666666666667, 0.0, 0.0, 26.0, 25.17701406784786, 0.403875143630904, 1.0, 1.0, 86123.41721011049], 
processed observation next is [1.0, 0.782608695652174, 0.32871652816251157, 0.5816666666666667, 0.0, 0.0, 0.6666666666666666, 0.5980845056539884, 0.634625047876968, 1.0, 1.0, 0.4101115105243357], 
reward next is 0.5899, 
noisyNet noise sample is [array([2.0547695], dtype=float32), 0.21199305]. 
=============================================
[2019-04-04 11:17:02,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9774532e-15 1.9712908e-14 1.4486649e-25 2.9106879e-16 4.5352420e-16
 5.2365802e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:02,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2954
[2019-04-04 11:17:02,678] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.933333333333333, 100.0, 0.0, 0.0, 26.0, 25.26574266381709, 0.2988020174733481, 0.0, 1.0, 53785.87068932156], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3127800.0000, 
sim time next is 3128400.0000, 
raw observation next is [3.0, 100.0, 0.0, 0.0, 26.0, 25.28959355688119, 0.3264403287357778, 0.0, 1.0, 54132.98238778719], 
processed observation next is [1.0, 0.21739130434782608, 0.5457063711911359, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6074661297400992, 0.608813442911926, 0.0, 1.0, 0.2577761066085104], 
reward next is 0.7422, 
noisyNet noise sample is [array([0.7511564], dtype=float32), -2.0242398]. 
=============================================
[2019-04-04 11:17:06,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0490822e-15 4.2613457e-15 7.8243274e-26 1.5373394e-16 4.8186669e-17
 1.2575801e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:06,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6765
[2019-04-04 11:17:06,800] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 39.0, 91.5, 724.0, 26.0, 25.1194115537939, 0.3653356299190771, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3078000.0000, 
sim time next is 3078600.0000, 
raw observation next is [0.1666666666666667, 39.16666666666666, 89.0, 707.0, 26.0, 25.12496436920787, 0.3648297498295625, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.4672206832871654, 0.39166666666666655, 0.2966666666666667, 0.7812154696132597, 0.6666666666666666, 0.5937470307673225, 0.6216099166098542, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1742246], dtype=float32), -1.2132912]. 
=============================================
[2019-04-04 11:17:11,575] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2155160e-15 2.3630461e-14 1.8307531e-25 2.7528417e-16 1.0216077e-15
 1.0666030e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:11,575] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1235
[2019-04-04 11:17:11,617] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.12866568548637, 0.3480364030657825, 0.0, 1.0, 68935.1501603897], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3094800.0000, 
sim time next is 3095400.0000, 
raw observation next is [-1.0, 92.0, 0.0, 0.0, 26.0, 25.32062574881319, 0.365982979930016, 0.0, 1.0, 50359.08536351011], 
processed observation next is [0.0, 0.8260869565217391, 0.4349030470914128, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6100521457344324, 0.6219943266433386, 0.0, 1.0, 0.2398051683976672], 
reward next is 0.7602, 
noisyNet noise sample is [array([0.77913463], dtype=float32), -0.28436396]. 
=============================================
[2019-04-04 11:17:33,959] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0154832e-16 5.8334409e-15 1.1554346e-25 8.2323013e-17 4.0871497e-16
 4.5166258e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:33,970] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0851
[2019-04-04 11:17:34,020] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.00576152885372, 0.4390311878082396, 0.0, 1.0, 28289.67784980696], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3441600.0000, 
sim time next is 3442200.0000, 
raw observation next is [1.0, 79.00000000000001, 0.0, 0.0, 26.0, 25.03367236499799, 0.4427914997126339, 1.0, 1.0, 18704.54644224883], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.7900000000000001, 0.0, 0.0, 0.6666666666666666, 0.5861393637498326, 0.647597166570878, 1.0, 1.0, 0.08906926877261348], 
reward next is 0.9109, 
noisyNet noise sample is [array([0.53280973], dtype=float32), -0.27586955]. 
=============================================
[2019-04-04 11:17:42,728] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1327379e-15 2.9567966e-15 4.3016115e-27 3.8689841e-17 1.7466783e-16
 4.7993996e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:42,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4867
[2019-04-04 11:17:42,811] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 63.33333333333334, 0.0, 0.0, 26.0, 24.46940794776248, 0.270005523879031, 1.0, 1.0, 202408.5542687548], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3396000.0000, 
sim time next is 3396600.0000, 
raw observation next is [-2.5, 62.5, 2.0, 107.0, 26.0, 24.92115233006141, 0.3300567186641471, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.39335180055401664, 0.625, 0.006666666666666667, 0.11823204419889503, 0.6666666666666666, 0.5767626941717842, 0.6100189062213824, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01673241], dtype=float32), 2.6044588]. 
=============================================
[2019-04-04 11:17:44,007] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2190482e-15 1.4476620e-14 1.4354295e-25 5.4916222e-16 1.2953655e-15
 4.3595156e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:44,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6038
[2019-04-04 11:17:44,033] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.36689788666438, 0.3782259416028793, 0.0, 1.0, 42495.51833689579], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3717000.0000, 
sim time next is 3717600.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.36818814896056, 0.3964003562589438, 0.0, 1.0, 41793.53916371293], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6140156790800466, 0.6321334520863146, 0.0, 1.0, 0.19901685316053774], 
reward next is 0.8010, 
noisyNet noise sample is [array([-1.0610627], dtype=float32), -0.10652044]. 
=============================================
[2019-04-04 11:17:44,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.9612461e-15 3.9475273e-14 4.6613155e-25 7.7412088e-16 3.4192969e-15
 5.2520069e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:44,813] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6711
[2019-04-04 11:17:44,828] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.36818814896056, 0.3964003562589438, 0.0, 1.0, 41793.53916371293], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3717600.0000, 
sim time next is 3718200.0000, 
raw observation next is [-3.0, 71.0, 0.0, 0.0, 26.0, 25.49731616549926, 0.4009323513425718, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.0, 0.3795013850415513, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6247763471249383, 0.6336441171141906, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.6222816], dtype=float32), 0.34661406]. 
=============================================
[2019-04-04 11:17:48,054] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [7.4315147e-15 7.3972241e-14 2.0054274e-24 2.1944650e-15 3.3989185e-15
 3.2580858e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:48,055] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7528
[2019-04-04 11:17:48,070] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.2533645742412, 0.1290197789530338, 0.0, 1.0, 43609.35916900662], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3988200.0000, 
sim time next is 3988800.0000, 
raw observation next is [-12.0, 63.0, 0.0, 0.0, 26.0, 24.18022256450548, 0.1137711891745468, 0.0, 1.0, 43643.89971858619], 
processed observation next is [1.0, 0.17391304347826086, 0.13019390581717452, 0.63, 0.0, 0.0, 0.6666666666666666, 0.5150185470421235, 0.5379237297248489, 0.0, 1.0, 0.20782809389802948], 
reward next is 0.7922, 
noisyNet noise sample is [array([2.3025415], dtype=float32), -0.093032874]. 
=============================================
[2019-04-04 11:17:50,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3918127e-14 6.5215066e-14 9.7922576e-25 1.2015505e-15 2.2207815e-15
 1.0021031e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:17:50,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8077
[2019-04-04 11:17:50,561] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.36898981755879, 0.3379578099860137, 0.0, 1.0, 43903.92484502491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3730800.0000, 
sim time next is 3731400.0000, 
raw observation next is [-3.0, 65.0, 0.0, 0.0, 26.0, 25.40303255625536, 0.3295352875338045, 0.0, 1.0, 25810.01274321187], 
processed observation next is [1.0, 0.17391304347826086, 0.3795013850415513, 0.65, 0.0, 0.0, 0.6666666666666666, 0.6169193796879467, 0.6098450958446014, 0.0, 1.0, 0.1229048225867232], 
reward next is 0.8771, 
noisyNet noise sample is [array([1.0087729], dtype=float32), -1.5887998]. 
=============================================
[2019-04-04 11:17:51,650] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.66201133e-15 1.12486545e-13 1.00458350e-24 8.57825418e-16
 3.77668232e-15 1.97553362e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:17:51,674] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2093
[2019-04-04 11:17:51,706] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 58.33333333333334, 0.0, 0.0, 26.0, 25.71761895734873, 0.5732305329232106, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3878400.0000, 
sim time next is 3879000.0000, 
raw observation next is [-1.0, 57.5, 0.0, 0.0, 26.0, 25.72912805398077, 0.5698116428191674, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4349030470914128, 0.575, 0.0, 0.0, 0.6666666666666666, 0.6440940044983975, 0.6899372142730558, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05033759], dtype=float32), 1.5443779]. 
=============================================
[2019-04-04 11:17:51,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.09212 ]
 [80.26722 ]
 [80.521034]
 [80.550224]
 [80.384056]], R is [[80.12408447]
 [80.32284546]
 [80.51961517]
 [80.62508392]
 [80.40394592]].
[2019-04-04 11:17:59,102] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.42088074e-14 3.12991244e-14 3.30602336e-25 1.13989882e-15
 1.74727227e-15 2.24343707e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:17:59,124] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6482
[2019-04-04 11:17:59,136] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 72.0, 0.0, 0.0, 26.0, 25.4145538759984, 0.4130819472555836, 0.0, 1.0, 56812.84450081544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3708000.0000, 
sim time next is 3708600.0000, 
raw observation next is [-0.5, 70.83333333333333, 0.0, 0.0, 26.0, 25.4244486039148, 0.413240672948575, 0.0, 1.0, 39837.13643324989], 
processed observation next is [0.0, 0.9565217391304348, 0.44875346260387816, 0.7083333333333333, 0.0, 0.0, 0.6666666666666666, 0.6187040503262334, 0.6377468909828584, 0.0, 1.0, 0.18970064968214234], 
reward next is 0.8103, 
noisyNet noise sample is [array([0.0441449], dtype=float32), -0.60836035]. 
=============================================
[2019-04-04 11:18:02,412] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.7197381e-14 2.8439155e-14 3.0109594e-26 4.6330616e-16 2.6962844e-15
 8.1275152e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:18:02,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4409
[2019-04-04 11:18:02,431] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41050594214111, 0.3392473581081427, 0.0, 1.0, 34296.1469278966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4244400.0000, 
sim time next is 4245000.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.41920638981872, 0.338066682289865, 0.0, 1.0, 31983.10151986276], 
processed observation next is [0.0, 0.13043478260869565, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6182671991515599, 0.6126888940966216, 0.0, 1.0, 0.1523004834279179], 
reward next is 0.8477, 
noisyNet noise sample is [array([-0.75794685], dtype=float32), 1.1475973]. 
=============================================
[2019-04-04 11:18:02,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[86.04814 ]
 [86.06896 ]
 [86.10166 ]
 [86.091866]
 [86.065346]], R is [[86.06151581]
 [86.03759003]
 [86.01834869]
 [85.97727966]
 [85.88611603]].
[2019-04-04 11:18:03,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.34399446e-16 1.35521852e-15 1.88477821e-26 1.06267165e-17
 7.86355692e-18 3.61297809e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 11:18:03,556] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0275
[2019-04-04 11:18:03,585] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 27.0, 112.3333333333333, 824.0, 26.0, 26.58744556081609, 0.7036133643341403, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4023600.0000, 
sim time next is 4024200.0000, 
raw observation next is [-3.166666666666667, 26.5, 110.6666666666667, 818.0, 26.0, 26.86703002556503, 0.7363013120690867, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.3748845798707295, 0.265, 0.368888888888889, 0.9038674033149171, 0.6666666666666666, 0.7389191687970857, 0.7454337706896955, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.29603592], dtype=float32), 1.0051302]. 
=============================================
[2019-04-04 11:18:05,123] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8067205e-15 1.4864256e-14 6.2899292e-25 7.5398490e-16 8.4338642e-16
 9.1489286e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:18:05,125] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8288
[2019-04-04 11:18:05,140] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.75, 40.83333333333334, 16.0, 108.6666666666667, 26.0, 25.03810359978095, 0.3248098286477943, 0.0, 1.0, 67954.69356494794], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4211400.0000, 
sim time next is 4212000.0000, 
raw observation next is [1.7, 41.0, 0.0, 0.0, 26.0, 25.02834591687204, 0.3188826164234378, 0.0, 1.0, 47539.81948555692], 
processed observation next is [0.0, 0.782608695652174, 0.5096952908587258, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5856954930726701, 0.6062942054744792, 0.0, 1.0, 0.2263800927883663], 
reward next is 0.7736, 
noisyNet noise sample is [array([0.5667129], dtype=float32), 0.23291035]. 
=============================================
[2019-04-04 11:18:05,146] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.10667]
 [81.79195]
 [81.54856]
 [81.7962 ]
 [82.17931]], R is [[81.1149826 ]
 [80.98023987]
 [80.89630127]
 [80.9983902 ]
 [81.1884079 ]].
[2019-04-04 11:18:10,040] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6182459e-14 5.0569829e-14 1.7958912e-25 1.8984702e-15 4.9595668e-15
 3.5892012e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:18:10,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9947
[2019-04-04 11:18:10,131] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.666666666666667, 48.66666666666666, 0.0, 0.0, 26.0, 25.27821281979057, 0.3768159449600792, 0.0, 1.0, 39372.350749073], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4156800.0000, 
sim time next is 4157400.0000, 
raw observation next is [-2.833333333333333, 49.33333333333334, 0.0, 0.0, 26.0, 25.25514402222802, 0.370372436487191, 0.0, 1.0, 39412.87011698051], 
processed observation next is [0.0, 0.08695652173913043, 0.3841181902123731, 0.4933333333333334, 0.0, 0.0, 0.6666666666666666, 0.6045953351856683, 0.6234574788290637, 0.0, 1.0, 0.1876803338903834], 
reward next is 0.8123, 
noisyNet noise sample is [array([1.2377467], dtype=float32), 1.5313638]. 
=============================================
[2019-04-04 11:18:17,359] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 11:18:17,360] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:18:17,360] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:18:17,364] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:18:17,373] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run47
[2019-04-04 11:18:17,374] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:18:17,375] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:18:17,418] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:18:17,432] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run47
[2019-04-04 11:18:17,473] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run47
[2019-04-04 11:19:08,697] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1510379], dtype=float32), -0.31403702]
[2019-04-04 11:19:08,697] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [7.15, 68.0, 0.0, 0.0, 26.0, 25.36123117162014, 0.3749246583139589, 0.0, 1.0, 39509.84628524524]
[2019-04-04 11:19:08,697] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:19:08,704] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [4.6043097e-15 1.7654770e-14 4.9881129e-26 2.9233955e-16 8.4973858e-16
 2.6467131e-18 1.0000000e+00], sampled 0.49921446327008867
[2019-04-04 11:20:25,991] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.1510379], dtype=float32), -0.31403702]
[2019-04-04 11:20:25,991] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-0.9212298131666667, 37.979626025, 237.9084800666667, 0.0, 26.0, 25.4547367082608, 0.2769742754845042, 1.0, 1.0, 0.0]
[2019-04-04 11:20:25,991] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 11:20:25,993] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [1.3435127e-16 1.4077590e-15 3.9824399e-27 2.8298319e-17 1.9073607e-17
 2.2659866e-19 1.0000000e+00], sampled 0.9873705826923067
[2019-04-04 11:20:33,439] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1510379], dtype=float32), -0.31403702]
[2019-04-04 11:20:33,439] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.18333333333333, 92.16666666666667, 30.99999999999999, 79.0, 26.0, 25.44472861507509, 0.4351507775369142, 1.0, 1.0, 0.0]
[2019-04-04 11:20:33,439] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:20:33,440] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [1.6953428e-16 3.8003385e-16 3.5682958e-28 7.6307850e-18 2.3347105e-17
 3.5616194e-20 1.0000000e+00], sampled 0.30083723118813976
[2019-04-04 11:20:49,988] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.1510379], dtype=float32), -0.31403702]
[2019-04-04 11:20:49,988] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [11.1, 93.0, 36.5, 25.0, 26.0, 25.56229900158129, 0.461259508643506, 0.0, 1.0, 18739.88985927573]
[2019-04-04 11:20:49,988] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:20:49,989] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [8.9351621e-16 2.6725376e-15 3.4336673e-27 4.2528803e-17 1.5149203e-16
 3.3752927e-19 1.0000000e+00], sampled 0.995621615188466
[2019-04-04 11:21:25,649] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:21:56,717] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:22:02,060] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:22:03,098] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 4600000, evaluation results [4600000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:22:05,910] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.8390637e-15 2.3306920e-14 1.5216383e-24 1.0686159e-15 1.6138805e-15
 3.1193752e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:22:05,910] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3241
[2019-04-04 11:22:05,946] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 40.33333333333334, 31.33333333333333, 227.5, 26.0, 25.29012115794143, 0.3576195646300968, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4209600.0000, 
sim time next is 4210200.0000, 
raw observation next is [1.85, 40.5, 24.0, 163.0, 26.0, 25.18321481555864, 0.337642489267477, 0.0, 1.0, 18680.41167023054], 
processed observation next is [0.0, 0.7391304347826086, 0.5138504155124655, 0.405, 0.08, 0.18011049723756906, 0.6666666666666666, 0.5986012346298866, 0.6125474964224923, 0.0, 1.0, 0.0889543412868121], 
reward next is 0.9110, 
noisyNet noise sample is [array([-1.5391856], dtype=float32), 0.6127992]. 
=============================================
[2019-04-04 11:22:13,920] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2617131e-18 4.6101049e-17 1.8176539e-28 1.2123684e-18 1.8968676e-18
 5.2878401e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:22:13,920] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7396
[2019-04-04 11:22:13,999] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.8, 31.0, 120.0, 828.0, 26.0, 27.85674645113373, 0.9431463848533054, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4361400.0000, 
sim time next is 4362000.0000, 
raw observation next is [14.2, 30.0, 119.6666666666667, 832.1666666666666, 26.0, 27.95664068744026, 0.9694612540946957, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.8559556786703602, 0.3, 0.398888888888889, 0.9195211786372007, 0.6666666666666666, 0.8297200572866883, 0.8231537513648987, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.44008395], dtype=float32), 0.33523804]. 
=============================================
[2019-04-04 11:22:14,033] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[95.97499]
 [96.03784]
 [96.13365]
 [96.15682]
 [96.12825]], R is [[95.92964935]
 [95.97035217]
 [96.01065063]
 [96.05054474]
 [96.09004211]].
[2019-04-04 11:22:17,370] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.74445514e-15 4.55254561e-14 1.02046905e-25 3.10146196e-16
 9.89658790e-16 2.54242709e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 11:22:17,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1185
[2019-04-04 11:22:17,414] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.166666666666667, 60.33333333333334, 0.0, 0.0, 26.0, 25.72503580251519, 0.5533496455128698, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4571400.0000, 
sim time next is 4572000.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.73271452606919, 0.5445672071611529, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6443928771724327, 0.681522402387051, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.790207], dtype=float32), 0.40155932]. 
=============================================
[2019-04-04 11:22:17,447] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[83.27855]
 [83.40705]
 [83.52894]
 [83.6823 ]
 [83.853  ]], R is [[83.36795044]
 [83.53427124]
 [83.69892883]
 [83.86193848]
 [84.02332306]].
[2019-04-04 11:22:24,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4216391e-16 3.1896408e-15 1.7710716e-26 1.4319461e-17 5.3358129e-17
 1.5098475e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:22:24,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7593
[2019-04-04 11:22:24,703] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 53.0, 0.0, 0.0, 26.0, 26.25513555105322, 0.65639811591833, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4647600.0000, 
sim time next is 4648200.0000, 
raw observation next is [2.833333333333333, 52.83333333333334, 0.0, 0.0, 26.0, 26.12868891041919, 0.6414891887360449, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.541089566020314, 0.5283333333333334, 0.0, 0.0, 0.6666666666666666, 0.6773907425349325, 0.7138297295786816, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48398492], dtype=float32), 0.3394975]. 
=============================================
[2019-04-04 11:22:33,295] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1089695e-16 1.6396581e-15 4.5469465e-26 9.1591300e-17 5.0664171e-17
 1.6134955e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:22:33,316] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8613
[2019-04-04 11:22:33,407] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 52.0, 0.0, 0.0, 26.0, 24.9835160789254, 0.4085711170836073, 1.0, 1.0, 56537.67331763697], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4563600.0000, 
sim time next is 4564200.0000, 
raw observation next is [2.0, 52.0, 0.0, 0.0, 26.0, 24.95916887256214, 0.4152746666732368, 0.0, 1.0, 52318.2285178832], 
processed observation next is [1.0, 0.8260869565217391, 0.518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5799307393801785, 0.638424888891079, 0.0, 1.0, 0.24913442151372953], 
reward next is 0.7509, 
noisyNet noise sample is [array([1.1408671], dtype=float32), 0.2938705]. 
=============================================
[2019-04-04 11:22:42,318] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.8819537e-15 2.1158143e-14 1.6836890e-25 3.4043800e-16 2.0172757e-15
 7.3590586e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:22:42,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9540
[2019-04-04 11:22:42,339] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 0.0, 0.0, 26.0, 25.42255172835382, 0.4733608044964557, 0.0, 1.0, 71626.06241159666], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4680000.0000, 
sim time next is 4680600.0000, 
raw observation next is [-0.1666666666666667, 93.33333333333334, 0.0, 0.0, 26.0, 25.5626617792319, 0.4781399148857495, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.4579870729455217, 0.9333333333333335, 0.0, 0.0, 0.6666666666666666, 0.6302218149359916, 0.6593799716285832, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1617001], dtype=float32), -0.9612437]. 
=============================================
[2019-04-04 11:22:53,235] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:22:53,236] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:53,264] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run35
[2019-04-04 11:22:55,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:22:55,607] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:55,615] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run35
[2019-04-04 11:22:56,469] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:22:56,470] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:56,477] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run35
[2019-04-04 11:22:58,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:22:58,111] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:58,121] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run35
[2019-04-04 11:22:59,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:22:59,550] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:22:59,554] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run35
[2019-04-04 11:23:06,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:06,152] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:06,158] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run35
[2019-04-04 11:23:07,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:07,331] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:07,368] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run35
[2019-04-04 11:23:07,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.7338795e-15 1.5757630e-13 1.6033981e-24 6.7685842e-16 6.3680394e-15
 1.6218155e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:23:07,934] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8485
[2019-04-04 11:23:07,959] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 84.33333333333334, 0.0, 0.0, 26.0, 24.08479104639365, 0.08299541897174625, 0.0, 1.0, 43209.92480693283], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 98400.0000, 
sim time next is 99000.0000, 
raw observation next is [-3.1, 83.0, 0.0, 0.0, 26.0, 24.0651969360052, 0.0695839793876905, 0.0, 1.0, 43325.48817840763], 
processed observation next is [1.0, 0.13043478260869565, 0.37673130193905824, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5054330780004334, 0.5231946597958969, 0.0, 1.0, 0.20631184846860776], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.14113127], dtype=float32), 1.5056994]. 
=============================================
[2019-04-04 11:23:07,967] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.519394]
 [79.57656 ]
 [79.561646]
 [79.61878 ]
 [79.66872 ]], R is [[79.46053314]
 [79.46016693]
 [79.46038818]
 [79.46133423]
 [79.46302032]].
[2019-04-04 11:23:09,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:09,097] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:09,100] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run35
[2019-04-04 11:23:09,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:09,429] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:09,433] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run35
[2019-04-04 11:23:09,634] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:09,635] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:09,658] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run35
[2019-04-04 11:23:09,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:09,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:09,876] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run35
[2019-04-04 11:23:09,985] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:09,986] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:10,003] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run35
[2019-04-04 11:23:10,176] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:10,179] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:10,183] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run35
[2019-04-04 11:23:10,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.6695737e-15 4.7237219e-15 4.2031692e-26 1.6542643e-16 1.4568965e-15
 2.5569973e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:23:10,742] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9386
[2019-04-04 11:23:10,839] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.616666666666667, 86.0, 81.66666666666666, 0.0, 26.0, 24.42810398181012, 0.1566890980686547, 0.0, 1.0, 30201.51050098607], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 51000.0000, 
sim time next is 51600.0000, 
raw observation next is [7.533333333333333, 86.0, 80.33333333333334, 0.0, 26.0, 24.45655319440051, 0.1588914843269057, 0.0, 1.0, 20857.99212652868], 
processed observation next is [0.0, 0.6086956521739131, 0.6712834718374886, 0.86, 0.26777777777777784, 0.0, 0.6666666666666666, 0.5380460995333758, 0.5529638281089686, 0.0, 1.0, 0.09932377203108896], 
reward next is 0.9007, 
noisyNet noise sample is [array([-0.07907259], dtype=float32), -0.68310314]. 
=============================================
[2019-04-04 11:23:14,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:14,656] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:14,679] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run35
[2019-04-04 11:23:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:14,929] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:14,933] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run35
[2019-04-04 11:23:28,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:23:28,304] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:23:28,323] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run35
[2019-04-04 11:23:39,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3747322e-16 1.7642316e-14 8.3983221e-24 2.0642544e-16 5.5158081e-16
 1.5027061e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:23:39,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7123
[2019-04-04 11:23:39,951] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 42.0, 62.33333333333334, 437.5, 26.0, 26.31216862155506, 0.524594860000917, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 315600.0000, 
sim time next is 316200.0000, 
raw observation next is [-9.5, 42.0, 54.66666666666667, 398.0000000000001, 26.0, 26.37360686428251, 0.3729188676268697, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.1994459833795014, 0.42, 0.18222222222222223, 0.439779005524862, 0.6666666666666666, 0.6978005720235426, 0.6243062892089566, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3668676], dtype=float32), 0.9791785]. 
=============================================
[2019-04-04 11:23:45,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8020041e-15 4.6678187e-14 4.2067025e-24 1.6086068e-15 2.4148602e-15
 1.4332522e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:23:45,758] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6517
[2019-04-04 11:23:45,795] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.866666666666667, 77.00000000000001, 0.0, 0.0, 26.0, 24.12107252646869, 0.0836175830066306, 0.0, 1.0, 44385.65723518514], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 259800.0000, 
sim time next is 260400.0000, 
raw observation next is [-5.233333333333334, 75.0, 0.0, 0.0, 26.0, 24.08031528043955, 0.08287852602678226, 0.0, 1.0, 44431.91033874334], 
processed observation next is [1.0, 0.0, 0.31763619575253926, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5066929400366291, 0.5276261753422608, 0.0, 1.0, 0.21158052542258735], 
reward next is 0.7884, 
noisyNet noise sample is [array([-0.371796], dtype=float32), 1.9986749]. 
=============================================
[2019-04-04 11:23:49,783] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2289577e-15 1.2322718e-13 1.7244929e-23 2.7832217e-15 2.5163119e-15
 5.7627470e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:23:49,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1655
[2019-04-04 11:23:49,832] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 71.16666666666667, 0.0, 0.0, 26.0, 25.1933828926943, 0.3321181105896618, 0.0, 1.0, 58921.90876094742], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 331800.0000, 
sim time next is 332400.0000, 
raw observation next is [-12.8, 72.33333333333334, 0.0, 0.0, 26.0, 25.19447182006785, 0.3113625359440664, 0.0, 1.0, 18775.26655812132], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.5995393183389875, 0.6037875119813555, 0.0, 1.0, 0.08940603122914914], 
reward next is 0.9106, 
noisyNet noise sample is [array([0.2639697], dtype=float32), 0.6486455]. 
=============================================
[2019-04-04 11:24:10,581] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.5078670e-16 7.9243066e-15 4.5189512e-26 7.6574412e-17 1.8664285e-16
 9.2371207e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:10,582] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5385
[2019-04-04 11:24:10,713] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.6000000000000001, 86.0, 0.0, 0.0, 26.0, 24.41755777659319, 0.185242059864534, 1.0, 1.0, 199595.3142240786], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 497400.0000, 
sim time next is 498000.0000, 
raw observation next is [0.7000000000000001, 88.0, 0.0, 0.0, 26.0, 24.64567423507843, 0.211124418576759, 1.0, 1.0, 99170.26201165038], 
processed observation next is [1.0, 0.782608695652174, 0.4819944598337951, 0.88, 0.0, 0.0, 0.6666666666666666, 0.5538061862565359, 0.570374806192253, 1.0, 1.0, 0.4722393429126209], 
reward next is 0.5278, 
noisyNet noise sample is [array([0.02672397], dtype=float32), -0.016776513]. 
=============================================
[2019-04-04 11:24:10,730] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[84.95678 ]
 [84.179054]
 [83.543724]
 [83.469795]
 [83.56719 ]], R is [[84.93082428]
 [84.13106537]
 [83.42237091]
 [83.36222839]
 [83.52861023]].
[2019-04-04 11:24:15,660] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [1.4989862e-15 1.0525514e-13 7.3583344e-24 8.3844313e-16 1.1694262e-15
 1.7568011e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:15,661] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.7944
[2019-04-04 11:24:15,734] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.333333333333333, 48.0, 70.83333333333334, 7.666666666666665, 26.0, 24.69713283323, 0.3357535940435903, 1.0, 1.0, 197075.4339219436], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 750000.0000, 
sim time next is 750600.0000, 
raw observation next is [-1.7, 49.5, 68.0, 3.0, 26.0, 25.22742281717996, 0.3958715926577653, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.4155124653739613, 0.495, 0.22666666666666666, 0.0033149171270718232, 0.6666666666666666, 0.6022852347649966, 0.6319571975525884, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7524673], dtype=float32), 0.53320575]. 
=============================================
[2019-04-04 11:24:17,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4581741e-17 1.4893308e-15 2.0544722e-25 2.2296243e-17 3.9949981e-17
 6.1140807e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:17,351] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2958
[2019-04-04 11:24:17,367] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.3333333333333334, 45.66666666666667, 81.5, 723.8333333333333, 26.0, 25.8369665905087, 0.4438218008294426, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 742800.0000, 
sim time next is 743400.0000, 
raw observation next is [0.25, 46.0, 80.0, 714.0, 26.0, 25.89073588865145, 0.4519577484267781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.46952908587257625, 0.46, 0.26666666666666666, 0.7889502762430939, 0.6666666666666666, 0.6575613240542874, 0.650652582808926, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.4838555], dtype=float32), 0.67699736]. 
=============================================
[2019-04-04 11:24:19,025] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.1919635e-15 6.5771291e-15 3.1374440e-25 3.0897004e-16 1.5535456e-16
 1.1028807e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:19,025] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9089
[2019-04-04 11:24:19,085] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.6, 54.0, 64.33333333333334, 30.33333333333334, 26.0, 24.87638361704145, 0.2204618919391464, 0.0, 1.0, 45858.6972090968], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 661800.0000, 
sim time next is 662400.0000, 
raw observation next is [-0.6, 54.0, 55.0, 26.5, 26.0, 24.87719449451964, 0.222724273438691, 0.0, 1.0, 42015.01762445458], 
processed observation next is [0.0, 0.6956521739130435, 0.44598337950138506, 0.54, 0.18333333333333332, 0.029281767955801105, 0.6666666666666666, 0.5730995412099699, 0.5742414244795636, 0.0, 1.0, 0.20007151249740276], 
reward next is 0.7999, 
noisyNet noise sample is [array([-1.5834848], dtype=float32), 2.397655]. 
=============================================
[2019-04-04 11:24:19,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3377953e-16 3.4022266e-15 1.0701607e-25 4.6301613e-17 8.0038036e-17
 2.5855399e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:19,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2772
[2019-04-04 11:24:19,670] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.15, 67.0, 139.0, 68.0, 26.0, 25.87350514096686, 0.3330410024407361, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 729000.0000, 
sim time next is 729600.0000, 
raw observation next is [-0.9666666666666668, 66.66666666666667, 129.8333333333333, 186.5, 26.0, 25.86998799532421, 0.3441106034221828, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.43582640812557716, 0.6666666666666667, 0.4327777777777776, 0.20607734806629835, 0.6666666666666666, 0.655832332943684, 0.6147035344740609, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.31885087], dtype=float32), -1.0818295]. 
=============================================
[2019-04-04 11:24:28,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8817420e-16 2.9291793e-15 2.0291499e-26 5.3627896e-17 7.7232127e-17
 6.1563481e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:28,700] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2698
[2019-04-04 11:24:28,847] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.08429602448955, 0.08085814667681014, 1.0, 1.0, 202417.3609425817], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 717600.0000, 
sim time next is 718200.0000, 
raw observation next is [-2.3, 76.0, 0.0, 0.0, 26.0, 24.43543098407249, 0.192867377559311, 1.0, 1.0, 185712.6585615965], 
processed observation next is [1.0, 0.30434782608695654, 0.3988919667590028, 0.76, 0.0, 0.0, 0.6666666666666666, 0.5362859153393741, 0.5642891258531036, 1.0, 1.0, 0.8843459931504595], 
reward next is 0.1157, 
noisyNet noise sample is [array([-0.13089691], dtype=float32), 0.8567881]. 
=============================================
[2019-04-04 11:24:40,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7765395e-16 2.4381862e-15 8.0006797e-27 5.8424098e-17 6.1497120e-17
 6.2621495e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:40,307] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2716
[2019-04-04 11:24:40,330] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.96666666666667, 82.0, 0.0, 0.0, 26.0, 25.6230904345656, 0.6105089025052972, 0.0, 1.0, 32283.6665585505], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1146000.0000, 
sim time next is 1146600.0000, 
raw observation next is [12.15, 81.5, 0.0, 0.0, 26.0, 25.62829114679203, 0.6127366466710383, 0.0, 1.0, 25008.24241707429], 
processed observation next is [0.0, 0.2608695652173913, 0.7991689750692522, 0.815, 0.0, 0.0, 0.6666666666666666, 0.6356909288993359, 0.7042455488903462, 0.0, 1.0, 0.11908686865273471], 
reward next is 0.8809, 
noisyNet noise sample is [array([-1.650981], dtype=float32), 0.9895079]. 
=============================================
[2019-04-04 11:24:43,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.98505584e-18 1.39809842e-16 4.96468494e-27 3.92011770e-18
 1.31650625e-17 1.84828765e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 11:24:43,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5193
[2019-04-04 11:24:43,542] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.8, 93.0, 98.66666666666666, 0.0, 26.0, 25.12937993465778, 0.2450991145452246, 1.0, 1.0, 18680.41167023054], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 911400.0000, 
sim time next is 912000.0000, 
raw observation next is [3.8, 93.0, 97.33333333333333, 0.0, 26.0, 25.07515304699709, 0.2471599848050535, 1.0, 1.0, 43331.13372484838], 
processed observation next is [1.0, 0.5652173913043478, 0.5678670360110805, 0.93, 0.3244444444444444, 0.0, 0.6666666666666666, 0.5895960872497575, 0.5823866616016845, 1.0, 1.0, 0.20633873202308753], 
reward next is 0.7937, 
noisyNet noise sample is [array([-0.93476295], dtype=float32), 0.6280945]. 
=============================================
[2019-04-04 11:24:43,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[88.66388]
 [88.68193]
 [88.68672]
 [88.63252]
 [88.60452]], R is [[88.61968231]
 [88.64453888]
 [88.75809479]
 [88.78156281]
 [88.89374542]].
[2019-04-04 11:24:46,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3098144e-16 4.2764737e-15 3.6773306e-26 4.3150115e-17 4.9033146e-17
 1.2507434e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:46,835] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6378
[2019-04-04 11:24:46,880] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.9, 55.5, 0.0, 0.0, 26.0, 25.46739607445529, 0.3921302897568149, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 756600.0000, 
sim time next is 757200.0000, 
raw observation next is [-3.9, 55.0, 0.0, 0.0, 26.0, 25.55570907455701, 0.3637524492780781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.3545706371191136, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6296424228797509, 0.6212508164260261, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7830917], dtype=float32), -0.024492107]. 
=============================================
[2019-04-04 11:24:53,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.93314384e-18 1.13457036e-17 8.58991613e-30 2.03843605e-19
 4.36723446e-20 1.42192216e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 11:24:53,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2474
[2019-04-04 11:24:53,768] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [19.4, 49.0, 116.0, 0.0, 26.0, 27.80621957410134, 1.001892969901353, 1.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1090800.0000, 
sim time next is 1091400.0000, 
raw observation next is [19.4, 49.0, 108.0, 0.0, 26.0, 27.771708576678, 0.9964216242371721, 1.0, 0.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 1.0, 0.49, 0.36, 0.0, 0.6666666666666666, 0.8143090480565002, 0.8321405414123907, 1.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5104288], dtype=float32), 0.81419826]. 
=============================================
[2019-04-04 11:24:57,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8461468e-17 5.5438826e-17 2.9120344e-29 1.6033909e-18 2.6398361e-18
 3.1019967e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 11:24:57,834] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1610
[2019-04-04 11:24:57,838] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.6, 91.33333333333333, 0.0, 0.0, 26.0, 23.94489350018761, 0.2355859088861035, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [15.5, 93.0, 0.0, 0.0, 26.0, 23.92201212604246, 0.2302778929254492, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.13043478260869565, 0.8919667590027703, 0.93, 0.0, 0.0, 0.6666666666666666, 0.4935010105035384, 0.5767592976418164, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.26230264], dtype=float32), -0.9292248]. 
=============================================
[2019-04-04 11:25:05,494] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [2.1303090e-16 1.4123688e-15 3.1512707e-27 1.4310178e-17 5.2184760e-17
 2.4355506e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:05,541] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9502
[2019-04-04 11:25:05,557] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 79.0, 0.0, 0.0, 26.0, 25.43431573142675, 0.5186322872518464, 0.0, 1.0, 127073.1683618299], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1562400.0000, 
sim time next is 1563000.0000, 
raw observation next is [4.9, 80.16666666666667, 0.0, 0.0, 26.0, 25.42491536928962, 0.5362061970372428, 0.0, 1.0, 82056.18959140136], 
processed observation next is [1.0, 0.08695652173913043, 0.5983379501385043, 0.8016666666666667, 0.0, 0.0, 0.6666666666666666, 0.6187429474408018, 0.6787353990124143, 0.0, 1.0, 0.3907437599590541], 
reward next is 0.6093, 
noisyNet noise sample is [array([0.597074], dtype=float32), -1.1224278]. 
=============================================
[2019-04-04 11:25:05,597] A3C_AGENT_WORKER-Thread-5 DEBUG:Value prediction is [[87.74176 ]
 [87.329735]
 [87.10853 ]
 [87.02326 ]
 [86.98768 ]], R is [[87.76593018]
 [87.28316498]
 [87.06726074]
 [87.15200806]
 [87.28048706]].
[2019-04-04 11:25:10,801] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.3733649e-17 1.8715762e-16 9.9335181e-28 4.7315236e-18 1.3405938e-17
 1.9226694e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:10,802] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2170
[2019-04-04 11:25:10,853] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.8, 79.0, 37.0, 35.0, 26.0, 25.82599668127248, 0.5630130517888107, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1585800.0000, 
sim time next is 1586400.0000, 
raw observation next is [6.066666666666666, 78.0, 50.0, 51.83333333333333, 26.0, 26.05918291105849, 0.5836428691953205, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.6306555863342568, 0.78, 0.16666666666666666, 0.057274401473296495, 0.6666666666666666, 0.6715985759215407, 0.6945476230651068, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.11005282], dtype=float32), 0.48993456]. 
=============================================
[2019-04-04 11:25:12,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5729320e-16 1.8160926e-15 9.6974186e-27 8.8123108e-18 8.4047294e-17
 1.5164212e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:12,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9624
[2019-04-04 11:25:12,122] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.40001989716134, 0.493329883948477, 0.0, 1.0, 25659.73258467117], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1466400.0000, 
sim time next is 1467000.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.38690795567233, 0.4886755752241098, 0.0, 1.0, 38180.31502685676], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6155756629726943, 0.6628918584080367, 0.0, 1.0, 0.18181102393741316], 
reward next is 0.8182, 
noisyNet noise sample is [array([0.9712267], dtype=float32), -1.0597421]. 
=============================================
[2019-04-04 11:25:12,131] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[85.634766]
 [85.61596 ]
 [85.56911 ]
 [85.55776 ]
 [85.548775]], R is [[85.55422211]
 [85.57648468]
 [85.55335999]
 [85.50141144]
 [85.42438507]].
[2019-04-04 11:25:22,265] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.8202351e-17 6.4643961e-16 1.1681223e-26 1.6835867e-17 7.5820086e-18
 3.4562977e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:22,265] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2482
[2019-04-04 11:25:22,307] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.2, 54.0, 25.5, 18.5, 26.0, 26.08245426556875, 0.7129832239349164, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1616400.0000, 
sim time next is 1617000.0000, 
raw observation next is [11.91666666666667, 55.16666666666666, 17.33333333333333, 12.33333333333333, 26.0, 26.51950039019786, 0.7305218952740263, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.7927054478301017, 0.5516666666666665, 0.05777777777777776, 0.013627992633517492, 0.6666666666666666, 0.7099583658498215, 0.7435072984246754, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3032098], dtype=float32), -1.1984745]. 
=============================================
[2019-04-04 11:25:22,328] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.62966 ]
 [87.09068 ]
 [87.44519 ]
 [87.746994]
 [87.78899 ]], R is [[86.5681076 ]
 [86.70243073]
 [86.83540344]
 [86.96704865]
 [87.09738159]].
[2019-04-04 11:25:27,332] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2676472e-16 6.5297804e-15 1.1122308e-25 9.0657275e-17 1.7155886e-16
 1.9693026e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:27,345] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9918
[2019-04-04 11:25:27,370] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37778900648344, 0.4685460934494231, 0.0, 1.0, 39655.30938154563], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1731000.0000, 
sim time next is 1731600.0000, 
raw observation next is [0.5, 92.0, 0.0, 0.0, 26.0, 25.37278151432364, 0.4633704071008434, 0.0, 1.0, 43751.60024673781], 
processed observation next is [0.0, 0.043478260869565216, 0.4764542936288089, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6143984595269701, 0.6544568023669478, 0.0, 1.0, 0.20834095355589435], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.13936515], dtype=float32), 0.059895564]. 
=============================================
[2019-04-04 11:25:31,411] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.7569044e-14 9.7320216e-14 3.1155841e-24 1.0707214e-15 1.6265259e-15
 4.0097008e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:31,412] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.0455
[2019-04-04 11:25:31,446] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.700000000000001, 78.0, 0.0, 0.0, 26.0, 23.32053703889784, -0.08742527369345564, 0.0, 1.0, 47192.67374697718], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1840800.0000, 
sim time next is 1841400.0000, 
raw observation next is [-6.7, 78.0, 0.0, 0.0, 26.0, 23.29107496167131, -0.09363154536304692, 0.0, 1.0, 47195.21689731849], 
processed observation next is [0.0, 0.30434782608695654, 0.2770083102493075, 0.78, 0.0, 0.0, 0.6666666666666666, 0.44092291347260915, 0.46878948487898436, 0.0, 1.0, 0.224739128082469], 
reward next is 0.7753, 
noisyNet noise sample is [array([-0.71924514], dtype=float32), 2.4755206]. 
=============================================
[2019-04-04 11:25:38,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6354719e-16 4.7926039e-15 6.5940528e-25 5.7622189e-17 8.9079649e-17
 2.7519991e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:38,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9347
[2019-04-04 11:25:38,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9421572e-15 1.6043302e-14 2.8449194e-25 1.3385811e-16 3.8406205e-16
 9.0474079e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:38,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1343
[2019-04-04 11:25:38,348] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.2, 62.0, 116.1666666666667, 0.0, 26.0, 25.76057828336438, 0.3445323225852374, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1952400.0000, 
sim time next is 1953000.0000, 
raw observation next is [-3.1, 62.0, 112.0, 0.0, 26.0, 25.76909154269032, 0.3420709976176297, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37673130193905824, 0.62, 0.37333333333333335, 0.0, 0.6666666666666666, 0.6474242952241932, 0.6140236658725432, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.16864166], dtype=float32), 1.5305737]. 
=============================================
[2019-04-04 11:25:38,355] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.89247 ]
 [80.83004 ]
 [80.76292 ]
 [80.605774]
 [80.341415]], R is [[81.1219101 ]
 [81.31069183]
 [81.49758911]
 [81.68261719]
 [81.75624847]].
[2019-04-04 11:25:38,366] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.6, 97.0, 0.0, 0.0, 26.0, 25.64712742267326, 0.5679072568420221, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1656600.0000, 
sim time next is 1657200.0000, 
raw observation next is [6.6, 97.0, 0.0, 0.0, 26.0, 25.67670523333569, 0.567836713293819, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.17391304347826086, 0.6454293628808865, 0.97, 0.0, 0.0, 0.6666666666666666, 0.6397254361113074, 0.6892789044312729, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4074501], dtype=float32), 1.4626951]. 
=============================================
[2019-04-04 11:25:40,518] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5332521e-16 1.6941217e-15 3.4724480e-26 6.2960045e-17 8.2018015e-17
 1.6728626e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:40,520] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6077
[2019-04-04 11:25:40,602] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 86.0, 49.0, 0.0, 26.0, 25.69378518040049, 0.3009874606780501, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2019600.0000, 
sim time next is 2020200.0000, 
raw observation next is [-5.933333333333334, 85.5, 55.66666666666667, 0.0, 26.0, 25.63219659690651, 0.2867047831913764, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.29824561403508776, 0.855, 0.18555555555555558, 0.0, 0.6666666666666666, 0.6360163830755425, 0.5955682610637921, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.58985144], dtype=float32), -2.9677038]. 
=============================================
[2019-04-04 11:25:42,878] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.9554067e-16 3.6150536e-15 2.6810399e-25 9.0820653e-17 9.1483396e-17
 2.0774922e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:42,878] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8656
[2019-04-04 11:25:42,935] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.166666666666667, 66.33333333333334, 37.33333333333333, 0.0, 26.0, 25.50084851172112, 0.3160404774809554, 1.0, 1.0, 33520.62074970889], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1959600.0000, 
sim time next is 1960200.0000, 
raw observation next is [-3.35, 68.5, 30.0, 0.0, 26.0, 25.53503542476353, 0.3250331878569039, 1.0, 1.0, 34846.71876804351], 
processed observation next is [1.0, 0.6956521739130435, 0.3698060941828255, 0.685, 0.1, 0.0, 0.6666666666666666, 0.6279196187302943, 0.6083443959523013, 1.0, 1.0, 0.16593675603830244], 
reward next is 0.8341, 
noisyNet noise sample is [array([-0.42666], dtype=float32), -0.09892553]. 
=============================================
[2019-04-04 11:25:48,652] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.6064429e-16 1.2113446e-14 5.2208841e-25 1.8561262e-16 1.5002268e-16
 1.3088641e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:25:48,652] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0654
[2019-04-04 11:25:48,692] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 71.66666666666667, 221.8333333333333, 47.66666666666666, 26.0, 25.71286407272178, 0.3160263770682705, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1941600.0000, 
sim time next is 1942200.0000, 
raw observation next is [-5.3, 70.0, 232.0, 10.0, 26.0, 25.71277014747887, 0.3022091877809206, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.31578947368421056, 0.7, 0.7733333333333333, 0.011049723756906077, 0.6666666666666666, 0.6427308456232392, 0.6007363959269735, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([2.0106163], dtype=float32), 0.9787713]. 
=============================================
[2019-04-04 11:26:06,659] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.49859806e-17 2.54436182e-15 5.91127373e-26 5.23830089e-17
 1.02754804e-16 1.24460366e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 11:26:06,659] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9657
[2019-04-04 11:26:06,720] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 68.0, 14.0, 0.0, 26.0, 26.09740607255883, 0.4572353093997617, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2134800.0000, 
sim time next is 2135400.0000, 
raw observation next is [-4.583333333333333, 68.5, 9.999999999999998, 0.0, 26.0, 26.09553656741864, 0.4501102577367457, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3356417359187443, 0.685, 0.033333333333333326, 0.0, 0.6666666666666666, 0.6746280472848868, 0.6500367525789152, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1754205], dtype=float32), -0.5240813]. 
=============================================
[2019-04-04 11:26:36,540] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.5523539e-14 1.8670144e-14 9.2241703e-25 3.8260269e-16 1.0528033e-15
 3.9324142e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:26:36,540] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8630
[2019-04-04 11:26:36,708] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.1, 78.66666666666667, 0.0, 0.0, 26.0, 23.83768203709759, 0.01046079737752256, 0.0, 1.0, 44623.57049532094], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2616000.0000, 
sim time next is 2616600.0000, 
raw observation next is [-7.199999999999999, 78.83333333333334, 0.0, 0.0, 26.0, 23.78959918561005, 0.01311411878538011, 0.0, 1.0, 44741.34782197177], 
processed observation next is [1.0, 0.2608695652173913, 0.26315789473684215, 0.7883333333333334, 0.0, 0.0, 0.6666666666666666, 0.4824665988008376, 0.50437137292846, 0.0, 1.0, 0.21305403724748462], 
reward next is 0.7869, 
noisyNet noise sample is [array([0.6818614], dtype=float32), -1.8114842]. 
=============================================
[2019-04-04 11:26:51,136] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.8917404e-16 6.7712737e-15 1.7146190e-25 7.4891927e-17 1.1374006e-16
 1.9281874e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:26:51,136] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4672
[2019-04-04 11:26:51,204] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.333333333333334, 32.33333333333334, 0.0, 0.0, 26.0, 24.62605158435519, 0.1976765124467284, 1.0, 1.0, 196217.9094192962], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2830800.0000, 
sim time next is 2831400.0000, 
raw observation next is [4.0, 33.5, 0.0, 0.0, 26.0, 23.53622986955531, 0.1331194530708159, 1.0, 1.0, 196605.2886036068], 
processed observation next is [1.0, 0.782608695652174, 0.5734072022160666, 0.335, 0.0, 0.0, 0.6666666666666666, 0.4613524891296092, 0.5443731510236053, 1.0, 1.0, 0.9362156600171753], 
reward next is 0.0638, 
noisyNet noise sample is [array([-0.5995442], dtype=float32), -1.2442948]. 
=============================================
[2019-04-04 11:27:00,650] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [4.4679622e-16 2.2920110e-15 5.6423444e-25 4.4217224e-17 2.3952895e-16
 2.0671206e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:27:00,650] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.4324
[2019-04-04 11:27:00,697] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 93.0, 0.0, 0.0, 26.0, 25.42314425096293, 0.392462385308785, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2914800.0000, 
sim time next is 2915400.0000, 
raw observation next is [1.166666666666667, 93.0, 0.0, 0.0, 26.0, 24.93144509750729, 0.3857143277759649, 1.0, 1.0, 150609.2410922645], 
processed observation next is [1.0, 0.7391304347826086, 0.49492151431209613, 0.93, 0.0, 0.0, 0.6666666666666666, 0.5776204247922742, 0.6285714425919883, 1.0, 1.0, 0.7171868623441167], 
reward next is 0.2828, 
noisyNet noise sample is [array([0.15190321], dtype=float32), 0.13034187]. 
=============================================
[2019-04-04 11:27:04,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2233210e-15 1.6730359e-13 5.7724289e-24 7.8524142e-16 5.1607740e-15
 2.8982880e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:27:04,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9990
[2019-04-04 11:27:04,660] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 80.33333333333334, 0.0, 0.0, 26.0, 24.98391823548731, 0.4203640499073423, 0.0, 1.0, 171112.8289800145], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2924400.0000, 
sim time next is 2925000.0000, 
raw observation next is [-1.0, 81.5, 0.0, 0.0, 26.0, 25.00089616684421, 0.4417484461020406, 0.0, 1.0, 101817.6854686568], 
processed observation next is [1.0, 0.8695652173913043, 0.4349030470914128, 0.815, 0.0, 0.0, 0.6666666666666666, 0.5834080139036842, 0.6472494820340136, 0.0, 1.0, 0.4848461212793181], 
reward next is 0.5152, 
noisyNet noise sample is [array([0.10634974], dtype=float32), -0.81721526]. 
=============================================
[2019-04-04 11:27:04,734] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.814445]
 [79.35505 ]
 [77.75977 ]
 [80.25506 ]
 [79.67288 ]], R is [[77.22290802]
 [76.63585663]
 [76.77918243]
 [76.7752533 ]
 [76.52223206]].
[2019-04-04 11:27:21,005] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-04 11:27:21,021] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:27:21,021] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:27:21,023] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run48
[2019-04-04 11:27:21,078] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:27:21,078] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:27:21,079] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:27:21,079] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:27:21,082] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run48
[2019-04-04 11:27:21,139] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run48
[2019-04-04 11:30:23,738] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:30:59,269] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:31:03,161] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:31:04,194] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 4700000, evaluation results [4700000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:31:20,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7225849e-17 1.9406369e-17 5.1665507e-28 2.9355606e-19 2.7940893e-18
 5.3862536e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:31:20,174] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8698
[2019-04-04 11:31:20,232] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 96.33333333333333, 604.5, 26.0, 26.37749328642696, 0.5571459455816169, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3144000.0000, 
sim time next is 3144600.0000, 
raw observation next is [7.0, 100.0, 99.0, 647.0, 26.0, 26.42574894519258, 0.573362403467943, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.6565096952908588, 1.0, 0.33, 0.7149171270718232, 0.6666666666666666, 0.702145745432715, 0.691120801155981, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.29564568], dtype=float32), -0.9346649]. 
=============================================
[2019-04-04 11:31:28,389] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.6493612e-17 1.2832266e-16 3.9051314e-26 3.6458341e-18 1.8617486e-17
 8.1816648e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:31:28,390] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.8572
[2019-04-04 11:31:28,418] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.0, 100.0, 98.16666666666667, 749.0, 26.0, 27.63302052376537, 0.9527134640410583, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3163200.0000, 
sim time next is 3163800.0000, 
raw observation next is [7.0, 100.0, 95.33333333333334, 735.0, 26.0, 27.6909242561071, 0.9627956356580049, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.6565096952908588, 1.0, 0.3177777777777778, 0.8121546961325967, 0.6666666666666666, 0.8075770213422583, 0.8209318785526682, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8367565], dtype=float32), -0.22882974]. 
=============================================
[2019-04-04 11:31:34,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9799499e-17 1.1527148e-16 8.8080357e-27 2.8762145e-18 1.1530880e-17
 8.3061651e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:31:34,505] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9776
[2019-04-04 11:31:34,542] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31550079037065, 0.5702048490235775, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3320400.0000, 
sim time next is 3321000.0000, 
raw observation next is [-7.5, 67.0, 111.0, 740.0, 26.0, 26.32773902648768, 0.5766798578429793, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.2548476454293629, 0.67, 0.37, 0.8176795580110497, 0.6666666666666666, 0.6939782522073067, 0.692226619280993, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1820387], dtype=float32), -0.078905426]. 
=============================================
[2019-04-04 11:31:34,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.25139 ]
 [87.439186]
 [87.634254]
 [87.87687 ]
 [88.080345]], R is [[87.19989777]
 [87.32789612]
 [87.45462036]
 [87.58007812]
 [87.70427704]].
[2019-04-04 11:31:47,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0174813e-17 2.4820286e-16 9.6827068e-27 2.8011330e-18 3.6929636e-18
 1.0110217e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:31:47,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9178
[2019-04-04 11:31:47,729] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 49.0, 62.0, 525.0, 26.0, 26.8902782335996, 0.7398299780098004, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3515400.0000, 
sim time next is 3516000.0000, 
raw observation next is [3.0, 49.0, 53.83333333333334, 462.6666666666667, 26.0, 26.44669985766689, 0.6891583532671045, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.5457063711911359, 0.49, 0.1794444444444445, 0.5112338858195212, 0.6666666666666666, 0.7038916548055741, 0.7297194510890348, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2386307], dtype=float32), -0.086584166]. 
=============================================
[2019-04-04 11:31:47,760] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.00637 ]
 [88.09239 ]
 [88.370186]
 [88.48958 ]
 [88.64227 ]], R is [[87.59094238]
 [87.71503448]
 [87.837883  ]
 [87.95950317]
 [88.07991028]].
[2019-04-04 11:31:56,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3947849e-16 1.1589278e-15 4.4742650e-27 1.0913179e-17 1.6683388e-17
 5.8442062e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:31:56,193] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1308
[2019-04-04 11:31:56,226] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 71.0, 89.83333333333334, 444.1666666666666, 26.0, 25.46321279700039, 0.4338616962060126, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3487200.0000, 
sim time next is 3487800.0000, 
raw observation next is [-1.0, 71.0, 91.66666666666666, 489.3333333333334, 26.0, 25.56464750909159, 0.4532993567749493, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4349030470914128, 0.71, 0.3055555555555555, 0.5406998158379375, 0.6666666666666666, 0.6303872924242991, 0.6510997855916497, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1428047], dtype=float32), -1.2379096]. 
=============================================
[2019-04-04 11:31:56,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.4683513e-16 7.1391062e-15 7.9153502e-26 1.1176373e-16 2.7737131e-16
 3.4150813e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:31:56,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7955
[2019-04-04 11:31:56,612] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 70.0, 0.0, 0.0, 26.0, 25.27228074665354, 0.4554576340415566, 0.0, 1.0, 141578.6204448651], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3537600.0000, 
sim time next is 3538200.0000, 
raw observation next is [-1.0, 68.0, 0.0, 0.0, 26.0, 25.20343254415619, 0.4618102159853303, 0.0, 1.0, 83894.40143223676], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.68, 0.0, 0.0, 0.6666666666666666, 0.6002860453463491, 0.6539367386617768, 0.0, 1.0, 0.3994971496773179], 
reward next is 0.6005, 
noisyNet noise sample is [array([-0.08438044], dtype=float32), -0.038317755]. 
=============================================
[2019-04-04 11:32:01,108] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8084801e-16 5.7954240e-15 8.3586814e-26 9.4768746e-17 3.1065415e-16
 1.5654061e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:01,109] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4515
[2019-04-04 11:32:01,130] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 74.0, 0.0, 0.0, 26.0, 25.68242989942167, 0.4713688236872011, 0.0, 1.0, 21402.0364450249], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3536400.0000, 
sim time next is 3537000.0000, 
raw observation next is [-1.0, 72.0, 0.0, 0.0, 26.0, 25.4638917790399, 0.455649773914276, 0.0, 1.0, 143832.2998297152], 
processed observation next is [1.0, 0.9565217391304348, 0.4349030470914128, 0.72, 0.0, 0.0, 0.6666666666666666, 0.6219909815866584, 0.6518832579714253, 0.0, 1.0, 0.6849157134748343], 
reward next is 0.3151, 
noisyNet noise sample is [array([-0.6882781], dtype=float32), 0.5507767]. 
=============================================
[2019-04-04 11:32:01,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[82.60484 ]
 [82.64348 ]
 [82.747475]
 [82.87808 ]
 [82.97587 ]], R is [[82.42116547]
 [82.49504089]
 [82.67008972]
 [82.84339142]
 [83.01496124]].
[2019-04-04 11:32:06,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.0030190e-17 5.7443795e-16 9.1421537e-27 1.0504795e-17 3.1178780e-17
 5.7595953e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:06,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7261
[2019-04-04 11:32:06,097] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 47.0, 95.5, 740.5, 26.0, 25.47949943248313, 0.484377192906331, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3682800.0000, 
sim time next is 3683400.0000, 
raw observation next is [5.833333333333333, 47.5, 92.66666666666666, 728.6666666666667, 26.0, 25.47017154938193, 0.4910581339277718, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.6521739130434783, 0.6241920590951062, 0.475, 0.3088888888888889, 0.8051565377532229, 0.6666666666666666, 0.6225142957818276, 0.6636860446425906, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7326642], dtype=float32), -0.02662817]. 
=============================================
[2019-04-04 11:32:09,491] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6412852e-15 1.0305678e-14 2.2859744e-25 4.0823484e-17 4.3205478e-16
 1.3863387e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:09,492] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.6456
[2019-04-04 11:32:09,504] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 74.0, 0.0, 0.0, 26.0, 25.60529104457443, 0.5238181528440676, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3792600.0000, 
sim time next is 3793200.0000, 
raw observation next is [-3.0, 75.0, 0.0, 0.0, 26.0, 25.55353811799841, 0.5128233472634022, 0.0, 1.0, 44010.22838853925], 
processed observation next is [1.0, 0.9130434782608695, 0.3795013850415513, 0.75, 0.0, 0.0, 0.6666666666666666, 0.6294615098332009, 0.6709411157544674, 0.0, 1.0, 0.2095725161359012], 
reward next is 0.7904, 
noisyNet noise sample is [array([0.05766473], dtype=float32), 1.2953637]. 
=============================================
[2019-04-04 11:32:14,220] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3956836e-15 1.4254955e-14 3.0091814e-25 4.5573713e-16 1.2051723e-15
 4.7437406e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:14,224] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2577
[2019-04-04 11:32:14,244] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.3331287410855, 0.4078558128378999, 0.0, 1.0, 44115.14999956332], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3807600.0000, 
sim time next is 3808200.0000, 
raw observation next is [-4.0, 77.0, 0.0, 0.0, 26.0, 25.39056112803026, 0.4139574111223152, 0.0, 1.0, 34028.41069780409], 
processed observation next is [1.0, 0.043478260869565216, 0.3518005540166205, 0.77, 0.0, 0.0, 0.6666666666666666, 0.6158800940025216, 0.6379858037074384, 0.0, 1.0, 0.16204005094192422], 
reward next is 0.8380, 
noisyNet noise sample is [array([1.425195], dtype=float32), -0.5839484]. 
=============================================
[2019-04-04 11:32:15,765] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5045300e-17 1.0921056e-16 4.6395614e-27 1.6219384e-18 3.7205083e-18
 6.7341852e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:15,765] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9047
[2019-04-04 11:32:15,771] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 107.1666666666667, 789.0, 26.0, 26.68250684877065, 0.7205668722640466, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3853200.0000, 
sim time next is 3853800.0000, 
raw observation next is [2.0, 48.0, 106.0, 782.0, 26.0, 26.79643322113229, 0.7360210715187326, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.518005540166205, 0.48, 0.35333333333333333, 0.8640883977900552, 0.6666666666666666, 0.7330361017610242, 0.7453403571729109, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.18221827], dtype=float32), -1.7604101]. 
=============================================
[2019-04-04 11:32:20,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.4756296e-15 6.8318344e-14 2.5983958e-24 2.2326905e-15 1.9117915e-15
 9.5455882e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:20,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5696
[2019-04-04 11:32:20,133] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-13.0, 66.0, 0.0, 0.0, 26.0, 23.70007963380863, 0.007970069831396526, 0.0, 1.0, 43787.49407207169], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3994200.0000, 
sim time next is 3994800.0000, 
raw observation next is [-13.0, 65.0, 0.0, 0.0, 26.0, 23.623177237459, -0.007283336230190021, 0.0, 1.0, 43757.43976319751], 
processed observation next is [1.0, 0.21739130434782608, 0.10249307479224376, 0.65, 0.0, 0.0, 0.6666666666666666, 0.46859810312158334, 0.49757222125660333, 0.0, 1.0, 0.20836876077713098], 
reward next is 0.7916, 
noisyNet noise sample is [array([-0.5659324], dtype=float32), 1.6014076]. 
=============================================
[2019-04-04 11:32:22,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5100565e-16 4.1828630e-15 1.1539188e-26 4.6056576e-17 2.4351813e-17
 3.3597233e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:22,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1035
[2019-04-04 11:32:22,976] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.33333333333333, 59.66666666666667, 77.5, 370.0, 26.0, 25.61297985375861, 0.4060571333801402, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4004400.0000, 
sim time next is 4005000.0000, 
raw observation next is [-12.0, 58.0, 93.0, 444.0, 26.0, 25.83649952260184, 0.4442952091622723, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.13019390581717452, 0.58, 0.31, 0.49060773480662984, 0.6666666666666666, 0.6530416268834868, 0.6480984030540907, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.6936591], dtype=float32), 1.3155718]. 
=============================================
[2019-04-04 11:32:22,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[90.152596]
 [89.84054 ]
 [89.253334]
 [88.14623 ]
 [86.31847 ]], R is [[90.44563293]
 [90.54117584]
 [90.63576508]
 [90.72940826]
 [90.32389069]].
[2019-04-04 11:32:23,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8373840e-15 3.0404563e-14 3.7332211e-24 1.2224759e-15 1.5247830e-15
 2.8284720e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:23,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4095
[2019-04-04 11:32:23,077] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-10.16666666666667, 58.0, 0.0, 0.0, 26.0, 24.81347924787479, 0.3063339177221197, 0.0, 1.0, 44108.30086904349], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3975000.0000, 
sim time next is 3975600.0000, 
raw observation next is [-10.33333333333333, 58.0, 0.0, 0.0, 26.0, 24.76576336872392, 0.304370766100654, 0.0, 1.0, 44088.59611870675], 
processed observation next is [1.0, 0.0, 0.17636195752539252, 0.58, 0.0, 0.0, 0.6666666666666666, 0.5638136140603267, 0.6014569220335514, 0.0, 1.0, 0.20994569580336547], 
reward next is 0.7901, 
noisyNet noise sample is [array([-0.9771111], dtype=float32), 1.658009]. 
=============================================
[2019-04-04 11:32:25,967] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.7205145e-15 6.7637390e-14 3.5119481e-25 7.3719908e-16 1.2940221e-15
 6.4167262e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:25,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3259
[2019-04-04 11:32:26,000] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 52.66666666666667, 0.0, 0.0, 26.0, 24.95702668827182, 0.2868211983105413, 0.0, 1.0, 39431.12285912866], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4166400.0000, 
sim time next is 4167000.0000, 
raw observation next is [-4.0, 52.0, 0.0, 0.0, 26.0, 24.92328381814866, 0.278139541603662, 0.0, 1.0, 39452.35033191329], 
processed observation next is [0.0, 0.21739130434782608, 0.3518005540166205, 0.52, 0.0, 0.0, 0.6666666666666666, 0.5769403181790551, 0.592713180534554, 0.0, 1.0, 0.1878683349138728], 
reward next is 0.8121, 
noisyNet noise sample is [array([-2.2840185], dtype=float32), 0.13303389]. 
=============================================
[2019-04-04 11:32:26,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.617134]
 [82.65197 ]
 [82.711845]
 [82.78421 ]
 [82.845825]], R is [[82.57440948]
 [82.56089783]
 [82.54753113]
 [82.53426361]
 [82.52107239]].
[2019-04-04 11:32:27,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8407499e-15 5.4792190e-14 2.6150404e-25 6.2829796e-16 7.3349633e-16
 8.2139733e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:27,669] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8402
[2019-04-04 11:32:27,705] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.0, 40.0, 0.0, 0.0, 26.0, 25.00163406006261, 0.2686195909009406, 0.0, 1.0, 40572.11548356387], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4074000.0000, 
sim time next is 4074600.0000, 
raw observation next is [-5.0, 40.5, 0.0, 0.0, 26.0, 24.96676701052392, 0.2644378552075386, 0.0, 1.0, 40536.31624381836], 
processed observation next is [1.0, 0.13043478260869565, 0.32409972299168976, 0.405, 0.0, 0.0, 0.6666666666666666, 0.58056391754366, 0.5881459517358462, 0.0, 1.0, 0.19303007735151598], 
reward next is 0.8070, 
noisyNet noise sample is [array([-0.04525074], dtype=float32), 0.0032036793]. 
=============================================
[2019-04-04 11:32:32,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0498877e-15 6.3812925e-15 2.1163432e-25 1.6549714e-16 1.3071541e-16
 6.6110994e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:32,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1799
[2019-04-04 11:32:32,156] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 0.0, 0.0, 26.0, 25.4148687440496, 0.4352082460343886, 0.0, 1.0, 56389.93696283193], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4147200.0000, 
sim time next is 4147800.0000, 
raw observation next is [-1.0, 41.5, 0.0, 0.0, 26.0, 25.38724420683905, 0.4341621081977253, 0.0, 1.0, 60733.81480205471], 
processed observation next is [0.0, 0.0, 0.4349030470914128, 0.415, 0.0, 0.0, 0.6666666666666666, 0.6156036839032542, 0.6447207027325751, 0.0, 1.0, 0.2892086419145462], 
reward next is 0.7108, 
noisyNet noise sample is [array([-0.5548066], dtype=float32), -1.3495182]. 
=============================================
[2019-04-04 11:32:34,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.2821949e-17 2.1936699e-15 1.2299819e-25 4.6224844e-17 3.8527833e-17
 5.3459535e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:34,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7326
[2019-04-04 11:32:34,065] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.0, 36.5, 39.0, 0.0, 26.0, 28.25508368328948, 1.078321659857876, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4379400.0000, 
sim time next is 4380000.0000, 
raw observation next is [13.0, 37.0, 34.0, 0.0, 26.0, 28.48160046991902, 1.101731800763726, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.37, 0.11333333333333333, 0.0, 0.6666666666666666, 0.8734667058265849, 0.8672439335879086, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2964212], dtype=float32), 0.50616527]. 
=============================================
[2019-04-04 11:32:34,077] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[84.388855]
 [84.656296]
 [85.19127 ]
 [85.684166]
 [85.780624]], R is [[84.81824493]
 [84.97006226]
 [85.12036133]
 [85.26915741]
 [85.41646576]].
[2019-04-04 11:32:45,900] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1333656e-17 3.7719004e-16 3.1918015e-26 8.8013589e-18 5.2330055e-18
 2.4790536e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:45,901] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4695
[2019-04-04 11:32:45,908] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 51.0, 119.5, 0.0, 26.0, 26.24398328761317, 0.5319526279935413, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4534800.0000, 
sim time next is 4535400.0000, 
raw observation next is [2.0, 49.5, 121.0, 0.0, 26.0, 26.18900209559731, 0.5321135998756125, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.495, 0.4033333333333333, 0.0, 0.6666666666666666, 0.6824168412997759, 0.6773711999585376, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2660891], dtype=float32), 0.6476031]. 
=============================================
[2019-04-04 11:32:47,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5997786e-15 1.0269064e-14 1.4871489e-25 9.3622505e-17 5.4753062e-16
 1.9244764e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:47,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0038
[2019-04-04 11:32:47,565] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.2000000000000001, 63.66666666666667, 0.0, 0.0, 26.0, 25.42297883314252, 0.4494053147583457, 0.0, 1.0, 36296.55157329138], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4584000.0000, 
sim time next is 4584600.0000, 
raw observation next is [0.1, 64.0, 0.0, 0.0, 26.0, 25.43833897192, 0.4415889131197385, 0.0, 1.0, 26028.23901997651], 
processed observation next is [1.0, 0.043478260869565216, 0.4653739612188367, 0.64, 0.0, 0.0, 0.6666666666666666, 0.6198615809933333, 0.6471963043732462, 0.0, 1.0, 0.12394399533322148], 
reward next is 0.8761, 
noisyNet noise sample is [array([1.1383996], dtype=float32), -1.08998]. 
=============================================
[2019-04-04 11:32:53,391] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [6.0006805e-17 8.7321092e-16 6.5230685e-27 1.4821823e-17 6.9252318e-18
 6.6668883e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:32:53,391] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.7419
[2019-04-04 11:32:53,435] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 51.33333333333334, 167.0, 16.0, 26.0, 25.256686478145, 0.4441886260079554, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4539000.0000, 
sim time next is 4539600.0000, 
raw observation next is [2.0, 52.0, 187.0, 24.0, 26.0, 25.41242610805704, 0.473907574850384, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.518005540166205, 0.52, 0.6233333333333333, 0.026519337016574586, 0.6666666666666666, 0.61770217567142, 0.6579691916167947, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.05410689], dtype=float32), -1.3989474]. 
=============================================
[2019-04-04 11:33:01,380] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.89351930e-15 1.11070219e-14 1.33798856e-24 6.05328708e-16
 3.74343273e-16 1.11793534e-17 1.00000000e+00], sum to 1.0000
[2019-04-04 11:33:01,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1222
[2019-04-04 11:33:01,419] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.5, 41.5, 0.0, 0.0, 26.0, 25.01396681165377, 0.3457997860858857, 0.0, 1.0, 18996.50026208359], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4818600.0000, 
sim time next is 4819200.0000, 
raw observation next is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.02042626856862, 0.339442212343933, 0.0, 1.0, 19654.07437361286], 
processed observation next is [0.0, 0.782608695652174, 0.4995383194829178, 0.42, 0.0, 0.0, 0.6666666666666666, 0.5850355223807183, 0.6131474041146444, 0.0, 1.0, 0.09359083035053743], 
reward next is 0.9064, 
noisyNet noise sample is [array([0.85176563], dtype=float32), -0.6941844]. 
=============================================
[2019-04-04 11:33:03,258] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6034599e-18 1.3793979e-16 2.1739797e-27 9.6857432e-18 2.8027896e-18
 9.2919614e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:03,261] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4395
[2019-04-04 11:33:03,271] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.9, 49.66666666666666, 201.6666666666667, 520.6666666666666, 26.0, 26.97730287042286, 0.8264752619748582, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [4.95, 49.83333333333334, 200.3333333333333, 442.3333333333334, 26.0, 27.20459239108473, 0.8552530069985483, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.5997229916897507, 0.4983333333333334, 0.6677777777777776, 0.48876611418047894, 0.6666666666666666, 0.7670493659237275, 0.7850843356661827, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3563691], dtype=float32), -0.55058354]. 
=============================================
[2019-04-04 11:33:03,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2017242e-17 5.3151958e-16 2.1905469e-26 2.0542534e-17 1.1691485e-17
 2.2586769e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:03,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5939
[2019-04-04 11:33:03,606] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.800000000000001, 47.0, 52.83333333333333, 145.3333333333333, 26.0, 27.51018954326894, 0.821107692278347, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4641600.0000, 
sim time next is 4642200.0000, 
raw observation next is [4.6, 47.5, 40.0, 145.0, 26.0, 27.31500589531953, 0.77448369262021, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.590027700831025, 0.475, 0.13333333333333333, 0.16022099447513813, 0.6666666666666666, 0.7762504912766275, 0.7581612308734034, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-2.2287793], dtype=float32), 1.071937]. 
=============================================
[2019-04-04 11:33:04,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.7581101e-17 4.7991505e-16 2.8672076e-27 1.3443887e-17 2.4432766e-17
 3.8811143e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:04,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7477
[2019-04-04 11:33:04,490] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 44.83333333333334, 62.00000000000001, 373.3333333333334, 26.0, 25.18660057762263, 0.2710633768556437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4954200.0000, 
sim time next is 4954800.0000, 
raw observation next is [-1.666666666666667, 43.66666666666667, 77.5, 466.6666666666667, 26.0, 25.11934407536977, 0.2968361437051864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.4366666666666667, 0.25833333333333336, 0.5156537753222836, 0.6666666666666666, 0.5932786729474809, 0.5989453812350621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.28721708], dtype=float32), -1.0915335]. 
=============================================
[2019-04-04 11:33:10,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4533000e-15 3.1024709e-14 1.1291907e-24 3.8157948e-16 7.8715783e-16
 5.8048185e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:10,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2730
[2019-04-04 11:33:10,591] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.833333333333333, 82.83333333333333, 0.0, 0.0, 26.0, 25.06726886691895, 0.4034610076018452, 0.0, 1.0, 41616.17290268088], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4751400.0000, 
sim time next is 4752000.0000, 
raw observation next is [-4.0, 84.0, 0.0, 0.0, 26.0, 25.09266322408297, 0.3978784527014491, 0.0, 1.0, 41540.77341939047], 
processed observation next is [0.0, 0.0, 0.3518005540166205, 0.84, 0.0, 0.0, 0.6666666666666666, 0.5910552686735807, 0.632626150900483, 0.0, 1.0, 0.19781320675900224], 
reward next is 0.8022, 
noisyNet noise sample is [array([1.0226256], dtype=float32), -1.2524515]. 
=============================================
[2019-04-04 11:33:10,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.26363 ]
 [80.27756 ]
 [80.278946]
 [80.31659 ]
 [80.33563 ]], R is [[80.26830292]
 [80.26744843]
 [80.26625824]
 [80.26469421]
 [80.26255035]].
[2019-04-04 11:33:13,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:13,581] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:13,618] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run36
[2019-04-04 11:33:14,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:14,232] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:14,245] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run36
[2019-04-04 11:33:15,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:15,825] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:15,832] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run36
[2019-04-04 11:33:17,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5554032e-15 7.0862959e-14 7.4452673e-25 1.5989284e-15 1.3763553e-15
 1.3009005e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:17,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2416
[2019-04-04 11:33:17,857] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.4395408626798, 0.4177878453299554, 0.0, 1.0, 78028.95456035896], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5022000.0000, 
sim time next is 5022600.0000, 
raw observation next is [-1.0, 55.0, 0.0, 0.0, 26.0, 25.49024971915828, 0.4124528778587727, 0.0, 1.0, 27984.44828022132], 
processed observation next is [1.0, 0.13043478260869565, 0.4349030470914128, 0.55, 0.0, 0.0, 0.6666666666666666, 0.6241874765965235, 0.6374842926195909, 0.0, 1.0, 0.13325927752486344], 
reward next is 0.8667, 
noisyNet noise sample is [array([-0.56185424], dtype=float32), 1.3808492]. 
=============================================
[2019-04-04 11:33:18,507] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:18,509] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:18,520] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run36
[2019-04-04 11:33:19,161] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.8537555e-18 2.5467978e-16 2.7153012e-27 6.2847231e-18 3.9155293e-18
 3.3579407e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:19,197] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6704
[2019-04-04 11:33:19,251] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [11.33333333333333, 19.66666666666667, 112.1666666666667, 825.8333333333333, 26.0, 28.10828845635121, 1.027266345899885, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5062800.0000, 
sim time next is 5063400.0000, 
raw observation next is [11.5, 19.5, 111.0, 819.0, 26.0, 28.29822797726312, 1.057243774114189, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.7811634349030472, 0.195, 0.37, 0.9049723756906077, 0.6666666666666666, 0.8581856647719267, 0.8524145913713963, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3817425], dtype=float32), 0.56213003]. 
=============================================
[2019-04-04 11:33:20,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:20,481] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:20,484] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run36
[2019-04-04 11:33:20,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3640194e-16 3.8871528e-16 3.3223289e-27 1.1196937e-17 4.2513070e-17
 7.0651803e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:20,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5126
[2019-04-04 11:33:20,746] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.833333333333333, 44.83333333333334, 62.00000000000001, 373.3333333333334, 26.0, 25.18660057762263, 0.2710633768556437, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4954200.0000, 
sim time next is 4954800.0000, 
raw observation next is [-1.666666666666667, 43.66666666666667, 77.5, 466.6666666666667, 26.0, 25.11934407536977, 0.2968361437051864, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4164358264081256, 0.4366666666666667, 0.25833333333333336, 0.5156537753222836, 0.6666666666666666, 0.5932786729474809, 0.5989453812350621, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6691474], dtype=float32), -2.332333]. 
=============================================
[2019-04-04 11:33:21,845] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:21,846] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:21,851] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run36
[2019-04-04 11:33:22,959] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8349419e-16 1.6311185e-14 1.7306340e-24 4.2715480e-16 5.2756034e-16
 1.2683533e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:22,960] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9083
[2019-04-04 11:33:22,989] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [6.0, 23.0, 0.0, 0.0, 26.0, 26.14454579509912, 0.6073137806944655, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4995600.0000, 
sim time next is 4996200.0000, 
raw observation next is [6.0, 23.0, 0.0, 0.0, 26.0, 26.07761364556133, 0.5992546357202974, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.6288088642659281, 0.23, 0.0, 0.0, 0.6666666666666666, 0.673134470463444, 0.6997515452400992, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.4859539], dtype=float32), 1.3214453]. 
=============================================
[2019-04-04 11:33:23,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:23,237] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:23,241] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run36
[2019-04-04 11:33:27,337] A3C_AGENT_WORKER-Thread-12 INFO:Local step 297500, global step 4757161: loss 0.0679
[2019-04-04 11:33:27,341] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 297500, global step 4757163: learning rate 0.0000
[2019-04-04 11:33:27,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:27,793] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:27,822] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run36
[2019-04-04 11:33:27,853] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:27,855] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:27,873] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run36
[2019-04-04 11:33:27,985] A3C_AGENT_WORKER-Thread-5 INFO:Local step 297500, global step 4757380: loss 0.0684
[2019-04-04 11:33:27,985] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 297500, global step 4757380: learning rate 0.0000
[2019-04-04 11:33:29,087] A3C_AGENT_WORKER-Thread-2 INFO:Local step 297500, global step 4757729: loss 0.0821
[2019-04-04 11:33:29,087] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 297500, global step 4757729: learning rate 0.0000
[2019-04-04 11:33:30,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:30,216] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:30,225] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run36
[2019-04-04 11:33:30,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:30,309] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:30,312] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run36
[2019-04-04 11:33:31,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:31,440] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:31,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:31,442] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:31,448] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run36
[2019-04-04 11:33:31,500] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run36
[2019-04-04 11:33:31,986] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5349133e-16 1.2519870e-14 5.3631567e-25 1.1432426e-16 2.7897787e-16
 5.1902622e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:31,986] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5484
[2019-04-04 11:33:31,993] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [10.66666666666667, 17.66666666666667, 0.0, 0.0, 26.0, 27.86703438743142, 0.9951187151151474, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 5080800.0000, 
sim time next is 5081400.0000, 
raw observation next is [10.5, 18.0, 0.0, 0.0, 26.0, 27.76012808678516, 0.9772095793829733, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.7534626038781165, 0.18, 0.0, 0.0, 0.6666666666666666, 0.8133440072320965, 0.8257365264609912, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.2920194], dtype=float32), 0.18981443]. 
=============================================
[2019-04-04 11:33:32,668] A3C_AGENT_WORKER-Thread-20 INFO:Local step 297500, global step 4758500: loss 0.0791
[2019-04-04 11:33:32,670] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 297500, global step 4758500: learning rate 0.0000
[2019-04-04 11:33:33,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:33,737] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:33,758] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run36
[2019-04-04 11:33:34,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:34,087] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:34,092] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run36
[2019-04-04 11:33:34,432] A3C_AGENT_WORKER-Thread-18 INFO:Local step 297500, global step 4758731: loss 0.1073
[2019-04-04 11:33:34,434] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 297500, global step 4758731: learning rate 0.0000
[2019-04-04 11:33:37,117] A3C_AGENT_WORKER-Thread-6 INFO:Local step 297500, global step 4759131: loss 0.1152
[2019-04-04 11:33:37,118] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 297500, global step 4759131: learning rate 0.0000
[2019-04-04 11:33:39,176] A3C_AGENT_WORKER-Thread-16 INFO:Local step 297500, global step 4759521: loss 0.1317
[2019-04-04 11:33:39,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 297500, global step 4759521: learning rate 0.0000
[2019-04-04 11:33:41,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.8613396e-17 1.3301880e-15 1.6107847e-25 1.2905897e-17 2.1962549e-17
 2.8126956e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:41,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4218
[2019-04-04 11:33:42,009] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.583333333333333, 65.0, 135.6666666666667, 0.0, 26.0, 25.15041350834577, 0.2290822024529025, 1.0, 1.0, 37136.35365911181], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 219000.0000, 
sim time next is 219600.0000, 
raw observation next is [-4.5, 65.0, 139.0, 0.0, 26.0, 25.14385069628581, 0.2362619547458984, 1.0, 1.0, 36050.6008267436], 
processed observation next is [1.0, 0.5652173913043478, 0.3379501385041552, 0.65, 0.4633333333333333, 0.0, 0.6666666666666666, 0.5953208913571508, 0.5787539849152995, 1.0, 1.0, 0.17166952774639807], 
reward next is 0.8283, 
noisyNet noise sample is [array([0.6432766], dtype=float32), -1.0072296]. 
=============================================
[2019-04-04 11:33:42,615] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4742686e-15 3.0091581e-14 2.4714649e-24 4.7180286e-16 1.4734065e-15
 1.2769365e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:33:42,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4649
[2019-04-04 11:33:42,639] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.733333333333334, 73.0, 0.0, 0.0, 26.0, 23.77890500024309, 0.009321873032790676, 0.0, 1.0, 44402.24777808877], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 175200.0000, 
sim time next is 175800.0000, 
raw observation next is [-8.816666666666666, 73.5, 0.0, 0.0, 26.0, 23.73807492215903, 0.002853394037237481, 0.0, 1.0, 44402.62783155789], 
processed observation next is [1.0, 0.0, 0.21837488457987075, 0.735, 0.0, 0.0, 0.6666666666666666, 0.4781729101799191, 0.5009511313457459, 0.0, 1.0, 0.21144108491218042], 
reward next is 0.7886, 
noisyNet noise sample is [array([0.06860875], dtype=float32), -2.0424848]. 
=============================================
[2019-04-04 11:33:42,940] A3C_AGENT_WORKER-Thread-13 INFO:Local step 297500, global step 4760410: loss 0.1471
[2019-04-04 11:33:42,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 297500, global step 4760410: learning rate 0.0000
[2019-04-04 11:33:43,387] A3C_AGENT_WORKER-Thread-4 INFO:Local step 297500, global step 4760555: loss 0.1528
[2019-04-04 11:33:43,387] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 297500, global step 4760555: learning rate 0.0000
[2019-04-04 11:33:46,403] A3C_AGENT_WORKER-Thread-3 INFO:Local step 297500, global step 4761320: loss 0.1717
[2019-04-04 11:33:46,404] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 297500, global step 4761320: learning rate 0.0000
[2019-04-04 11:33:46,560] A3C_AGENT_WORKER-Thread-15 INFO:Local step 297500, global step 4761366: loss 0.1611
[2019-04-04 11:33:46,582] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 297500, global step 4761370: learning rate 0.0000
[2019-04-04 11:33:47,104] A3C_AGENT_WORKER-Thread-10 INFO:Local step 297500, global step 4761538: loss 0.1758
[2019-04-04 11:33:47,104] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 297500, global step 4761538: learning rate 0.0000
[2019-04-04 11:33:47,298] A3C_AGENT_WORKER-Thread-19 INFO:Local step 297500, global step 4761602: loss 0.1746
[2019-04-04 11:33:47,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 297500, global step 4761602: learning rate 0.0000
[2019-04-04 11:33:47,384] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:33:47,385] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:33:47,388] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run36
[2019-04-04 11:33:49,513] A3C_AGENT_WORKER-Thread-11 INFO:Local step 297500, global step 4762184: loss 0.1634
[2019-04-04 11:33:49,513] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 297500, global step 4762184: learning rate 0.0000
[2019-04-04 11:33:49,765] A3C_AGENT_WORKER-Thread-17 INFO:Local step 297500, global step 4762246: loss 0.1567
[2019-04-04 11:33:49,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 297500, global step 4762246: learning rate 0.0000
[2019-04-04 11:33:59,275] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298000, global step 4764536: loss 1.7071
[2019-04-04 11:33:59,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298000, global step 4764536: learning rate 0.0000
[2019-04-04 11:33:59,927] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298000, global step 4764679: loss 1.7541
[2019-04-04 11:33:59,928] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298000, global step 4764679: learning rate 0.0000
[2019-04-04 11:34:01,606] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298000, global step 4765086: loss 1.7179
[2019-04-04 11:34:01,607] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298000, global step 4765086: learning rate 0.0000
[2019-04-04 11:34:02,426] A3C_AGENT_WORKER-Thread-14 INFO:Local step 297500, global step 4765296: loss 0.1628
[2019-04-04 11:34:02,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 297500, global step 4765297: learning rate 0.0000
[2019-04-04 11:34:04,610] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298000, global step 4765802: loss 1.7383
[2019-04-04 11:34:04,611] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298000, global step 4765803: learning rate 0.0000
[2019-04-04 11:34:05,535] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298000, global step 4766067: loss 1.7420
[2019-04-04 11:34:05,535] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298000, global step 4766067: learning rate 0.0000
[2019-04-04 11:34:06,825] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [5.13408862e-15 2.76198013e-14 1.11281275e-24 4.07920585e-16
 4.38027419e-16 4.37667880e-18 1.00000000e+00], sum to 1.0000
[2019-04-04 11:34:06,825] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.0626
[2019-04-04 11:34:06,855] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.316666666666666, 43.83333333333334, 0.0, 0.0, 26.0, 22.50416957045627, -0.3218370096216475, 0.0, 1.0, 46556.55271461963], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 454200.0000, 
sim time next is 454800.0000, 
raw observation next is [-9.133333333333333, 43.66666666666667, 0.0, 0.0, 26.0, 22.50815835960205, -0.3303536651065994, 0.0, 1.0, 46479.65978812299], 
processed observation next is [1.0, 0.2608695652173913, 0.20960295475530935, 0.4366666666666667, 0.0, 0.0, 0.6666666666666666, 0.3756798633001708, 0.3898821116311335, 0.0, 1.0, 0.22133171327677614], 
reward next is 0.7787, 
noisyNet noise sample is [array([1.2777601], dtype=float32), 0.46482307]. 
=============================================
[2019-04-04 11:34:08,808] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298000, global step 4766939: loss 1.7810
[2019-04-04 11:34:08,809] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298000, global step 4766939: learning rate 0.0000
[2019-04-04 11:34:10,960] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298000, global step 4767409: loss 1.7767
[2019-04-04 11:34:10,961] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298000, global step 4767409: learning rate 0.0000
[2019-04-04 11:34:15,006] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298000, global step 4768486: loss 1.8010
[2019-04-04 11:34:15,007] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298000, global step 4768487: learning rate 0.0000
[2019-04-04 11:34:15,339] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298000, global step 4768585: loss 1.7847
[2019-04-04 11:34:15,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298000, global step 4768585: learning rate 0.0000
[2019-04-04 11:34:17,527] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298000, global step 4769181: loss 1.8032
[2019-04-04 11:34:17,552] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298000, global step 4769181: learning rate 0.0000
[2019-04-04 11:34:18,252] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298000, global step 4769354: loss 1.8725
[2019-04-04 11:34:18,253] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298000, global step 4769354: learning rate 0.0000
[2019-04-04 11:34:18,740] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298000, global step 4769466: loss 1.8758
[2019-04-04 11:34:18,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298000, global step 4769466: learning rate 0.0000
[2019-04-04 11:34:19,384] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298000, global step 4769619: loss 1.8786
[2019-04-04 11:34:19,394] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298000, global step 4769619: learning rate 0.0000
[2019-04-04 11:34:20,399] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298000, global step 4769910: loss 1.8174
[2019-04-04 11:34:20,400] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298000, global step 4769910: learning rate 0.0000
[2019-04-04 11:34:20,685] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.3667302e-17 1.6850212e-15 4.7839212e-25 1.6067715e-17 2.0890084e-17
 2.9743137e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:20,688] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-04 11:34:20,738] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.3, 49.33333333333334, 55.5, 890.0, 26.0, 25.66773353306255, 0.4391123930782942, 1.0, 1.0, 67477.09107082966], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 393600.0000, 
sim time next is 394200.0000, 
raw observation next is [-11.1, 48.5, 55.0, 887.0, 26.0, 25.91599278457692, 0.4737028192429443, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.1551246537396122, 0.485, 0.18333333333333332, 0.980110497237569, 0.6666666666666666, 0.6596660653814098, 0.6579009397476481, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.1977342], dtype=float32), 1.272825]. 
=============================================
[2019-04-04 11:34:21,251] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298000, global step 4770173: loss 1.8689
[2019-04-04 11:34:21,251] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298000, global step 4770173: learning rate 0.0000
[2019-04-04 11:34:22,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.8195797e-17 4.0281325e-15 8.2697225e-25 5.0127430e-17 4.3379853e-17
 9.3430112e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:22,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4665
[2019-04-04 11:34:22,256] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.7, 51.0, 56.5, 896.0, 26.0, 25.86460845083914, 0.4067784625772552, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 392400.0000, 
sim time next is 393000.0000, 
raw observation next is [-11.5, 50.16666666666666, 56.0, 893.0, 26.0, 25.52219303993748, 0.4026265010714583, 1.0, 1.0, 96171.36172335374], 
processed observation next is [1.0, 0.5652173913043478, 0.1440443213296399, 0.5016666666666666, 0.18666666666666668, 0.9867403314917127, 0.6666666666666666, 0.62684941999479, 0.6342088336904861, 1.0, 1.0, 0.4579588653493035], 
reward next is 0.5420, 
noisyNet noise sample is [array([0.2452816], dtype=float32), 0.32423314]. 
=============================================
[2019-04-04 11:34:22,270] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.22702 ]
 [79.50061 ]
 [79.92407 ]
 [79.848236]
 [79.65749 ]], R is [[79.23166656]
 [79.43935394]
 [79.6449585 ]
 [79.41384888]
 [78.99358368]].
[2019-04-04 11:34:22,644] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.2136908e-16 7.5126927e-15 2.8886639e-24 8.5203992e-16 3.4079791e-16
 1.1667337e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:22,644] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1850
[2019-04-04 11:34:22,707] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.666666666666668, 40.66666666666667, 0.0, 0.0, 26.0, 25.11212679063894, 0.2897074471181182, 1.0, 1.0, 95333.88043480816], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 415200.0000, 
sim time next is 415800.0000, 
raw observation next is [-9.75, 41.0, 0.0, 0.0, 26.0, 25.08810795553453, 0.2949925818132944, 1.0, 1.0, 87610.09038443935], 
processed observation next is [1.0, 0.8260869565217391, 0.19252077562326872, 0.41, 0.0, 0.0, 0.6666666666666666, 0.5906756629612108, 0.5983308606044314, 1.0, 1.0, 0.4171909065925683], 
reward next is 0.5828, 
noisyNet noise sample is [array([-0.39754066], dtype=float32), 0.87846684]. 
=============================================
[2019-04-04 11:34:28,221] A3C_AGENT_WORKER-Thread-12 INFO:Local step 298500, global step 4772127: loss 0.0081
[2019-04-04 11:34:28,222] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 298500, global step 4772127: learning rate 0.0000
[2019-04-04 11:34:29,060] A3C_AGENT_WORKER-Thread-5 INFO:Local step 298500, global step 4772377: loss 0.0033
[2019-04-04 11:34:29,061] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 298500, global step 4772377: learning rate 0.0000
[2019-04-04 11:34:29,798] A3C_AGENT_WORKER-Thread-2 INFO:Local step 298500, global step 4772611: loss 0.0059
[2019-04-04 11:34:29,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 298500, global step 4772611: learning rate 0.0000
[2019-04-04 11:34:31,987] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298000, global step 4773253: loss 1.8687
[2019-04-04 11:34:31,988] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298000, global step 4773253: learning rate 0.0000
[2019-04-04 11:34:34,208] A3C_AGENT_WORKER-Thread-18 INFO:Local step 298500, global step 4773941: loss 0.0097
[2019-04-04 11:34:34,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 298500, global step 4773943: learning rate 0.0000
[2019-04-04 11:34:34,336] A3C_AGENT_WORKER-Thread-20 INFO:Local step 298500, global step 4773990: loss 0.0079
[2019-04-04 11:34:34,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 298500, global step 4773990: learning rate 0.0000
[2019-04-04 11:34:37,764] A3C_AGENT_WORKER-Thread-6 INFO:Local step 298500, global step 4775000: loss 0.0047
[2019-04-04 11:34:37,766] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 298500, global step 4775000: learning rate 0.0000
[2019-04-04 11:34:38,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0417254e-15 2.8510178e-15 3.8755695e-26 3.3917525e-17 1.0914101e-16
 3.1871530e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:38,435] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1934
[2019-04-04 11:34:38,526] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.2, 75.0, 19.0, 0.0, 26.0, 23.74292305245242, 0.08907865331430442, 0.0, 1.0, 203163.5310088737], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 635400.0000, 
sim time next is 636000.0000, 
raw observation next is [-4.1, 73.66666666666666, 38.33333333333333, 8.499999999999998, 26.0, 24.40412973860231, 0.1910326946519606, 0.0, 1.0, 154797.5509924477], 
processed observation next is [0.0, 0.34782608695652173, 0.3490304709141275, 0.7366666666666666, 0.12777777777777777, 0.009392265193370164, 0.6666666666666666, 0.5336774782168593, 0.5636775648839869, 0.0, 1.0, 0.737131195202132], 
reward next is 0.2629, 
noisyNet noise sample is [array([1.0365869], dtype=float32), 1.3525641]. 
=============================================
[2019-04-04 11:34:38,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.822014]
 [82.97285 ]
 [82.24848 ]
 [82.10871 ]
 [81.91521 ]], R is [[84.30944824]
 [83.498909  ]
 [82.70023346]
 [82.66378021]
 [82.62774658]].
[2019-04-04 11:34:39,499] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6960687e-16 3.4276944e-15 2.5213341e-25 8.0097593e-17 3.6065530e-16
 2.4448417e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:39,499] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0178
[2019-04-04 11:34:39,536] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.9, 83.0, 0.0, 0.0, 26.0, 24.88431210942075, 0.248108071775179, 0.0, 1.0, 42943.01626062771], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 598200.0000, 
sim time next is 598800.0000, 
raw observation next is [-3.0, 83.0, 0.0, 0.0, 26.0, 24.85455687433885, 0.2462084852582791, 0.0, 1.0, 42900.86715484204], 
processed observation next is [0.0, 0.9565217391304348, 0.3795013850415513, 0.83, 0.0, 0.0, 0.6666666666666666, 0.5712130728615709, 0.5820694950860931, 0.0, 1.0, 0.2042898435944859], 
reward next is 0.7957, 
noisyNet noise sample is [array([0.9162052], dtype=float32), -0.77218217]. 
=============================================
[2019-04-04 11:34:39,909] A3C_AGENT_WORKER-Thread-16 INFO:Local step 298500, global step 4775757: loss 0.0096
[2019-04-04 11:34:39,910] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 298500, global step 4775757: learning rate 0.0000
[2019-04-04 11:34:43,340] A3C_AGENT_WORKER-Thread-13 INFO:Local step 298500, global step 4776774: loss 0.0088
[2019-04-04 11:34:43,341] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 298500, global step 4776774: learning rate 0.0000
[2019-04-04 11:34:43,590] A3C_AGENT_WORKER-Thread-4 INFO:Local step 298500, global step 4776839: loss 0.0070
[2019-04-04 11:34:43,592] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 298500, global step 4776839: learning rate 0.0000
[2019-04-04 11:34:44,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2195972e-15 7.1661190e-15 1.6570420e-25 1.3856052e-16 4.0224184e-16
 2.7644357e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:44,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8770
[2019-04-04 11:34:44,872] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.95486878349629, 0.2056499695810395, 0.0, 1.0, 42391.70018764104], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [-3.4, 69.0, 0.0, 0.0, 26.0, 24.93169811083233, 0.1982939963085799, 0.0, 1.0, 42324.179092329], 
processed observation next is [0.0, 0.9130434782608695, 0.368421052631579, 0.69, 0.0, 0.0, 0.6666666666666666, 0.5776415092360274, 0.5660979987695266, 0.0, 1.0, 0.20154370996347143], 
reward next is 0.7985, 
noisyNet noise sample is [array([0.66582334], dtype=float32), -0.8876299]. 
=============================================
[2019-04-04 11:34:44,886] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[81.29075 ]
 [81.34861 ]
 [81.42208 ]
 [81.498314]
 [81.593475]], R is [[81.24002838]
 [81.22576141]
 [81.21134186]
 [81.19692993]
 [81.18144226]].
[2019-04-04 11:34:45,078] A3C_AGENT_WORKER-Thread-15 INFO:Local step 298500, global step 4777356: loss 0.0066
[2019-04-04 11:34:45,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 298500, global step 4777356: learning rate 0.0000
[2019-04-04 11:34:45,258] A3C_AGENT_WORKER-Thread-10 INFO:Local step 298500, global step 4777416: loss 0.0060
[2019-04-04 11:34:45,259] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 298500, global step 4777416: learning rate 0.0000
[2019-04-04 11:34:45,278] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.1859911e-16 1.1503652e-15 2.5926703e-26 8.9980257e-17 2.9351717e-17
 3.9813472e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:45,279] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4162
[2019-04-04 11:34:45,328] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.55, 76.0, 29.0, 0.0, 26.0, 25.11626504081585, 0.2711700017541115, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 894600.0000, 
sim time next is 895200.0000, 
raw observation next is [0.7333333333333334, 77.33333333333333, 32.16666666666666, 0.0, 26.0, 25.30551150048129, 0.2885176088876454, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.4829178208679595, 0.7733333333333333, 0.10722222222222219, 0.0, 0.6666666666666666, 0.6087926250401076, 0.5961725362958817, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0096391], dtype=float32), -0.036577974]. 
=============================================
[2019-04-04 11:34:45,832] A3C_AGENT_WORKER-Thread-3 INFO:Local step 298500, global step 4777614: loss 0.0086
[2019-04-04 11:34:45,833] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 298500, global step 4777614: learning rate 0.0000
[2019-04-04 11:34:46,945] A3C_AGENT_WORKER-Thread-19 INFO:Local step 298500, global step 4778014: loss 0.0045
[2019-04-04 11:34:46,946] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 298500, global step 4778014: learning rate 0.0000
[2019-04-04 11:34:47,925] A3C_AGENT_WORKER-Thread-11 INFO:Local step 298500, global step 4778343: loss 0.0068
[2019-04-04 11:34:47,947] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 298500, global step 4778347: learning rate 0.0000
[2019-04-04 11:34:48,091] A3C_AGENT_WORKER-Thread-17 INFO:Local step 298500, global step 4778391: loss 0.0047
[2019-04-04 11:34:48,092] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 298500, global step 4778391: learning rate 0.0000
[2019-04-04 11:34:48,529] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1760030e-16 2.9341336e-15 1.1655546e-25 7.6495585e-17 3.6812790e-16
 1.2341645e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:48,533] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6699
[2019-04-04 11:34:48,544] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.566666666666666, 71.66666666666667, 0.0, 0.0, 26.0, 24.36988705036413, 0.0952106027812938, 0.0, 1.0, 40950.16535211128], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 693600.0000, 
sim time next is 694200.0000, 
raw observation next is [-3.483333333333333, 71.83333333333333, 0.0, 0.0, 26.0, 24.40855126251162, 0.09266879816121947, 0.0, 1.0, 40936.36623151281], 
processed observation next is [1.0, 0.0, 0.3661126500461681, 0.7183333333333333, 0.0, 0.0, 0.6666666666666666, 0.5340459385426349, 0.5308895993870731, 0.0, 1.0, 0.19493507729291815], 
reward next is 0.8051, 
noisyNet noise sample is [array([0.05958047], dtype=float32), 0.39285585]. 
=============================================
[2019-04-04 11:34:48,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7347722e-16 1.1036484e-15 1.7809233e-25 2.2983739e-17 1.6222742e-16
 4.7962150e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:48,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8061
[2019-04-04 11:34:48,986] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 80.0, 134.3333333333333, 552.3333333333333, 26.0, 24.99726839148161, 0.3501328964923981, 0.0, 1.0, 18725.75373743529], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 568200.0000, 
sim time next is 568800.0000, 
raw observation next is [-1.2, 80.0, 132.5, 531.0, 26.0, 25.00245289473588, 0.349198396560372, 0.0, 1.0, 18723.87818829856], 
processed observation next is [0.0, 0.6086956521739131, 0.42936288088642666, 0.8, 0.44166666666666665, 0.5867403314917127, 0.6666666666666666, 0.5835377412279902, 0.616399465520124, 0.0, 1.0, 0.08916132470618363], 
reward next is 0.9108, 
noisyNet noise sample is [array([-0.8676766], dtype=float32), 0.21078923]. 
=============================================
[2019-04-04 11:34:50,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2333040e-17 9.2961284e-17 1.0913082e-27 9.6115730e-19 1.4980863e-18
 2.9226314e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:50,777] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4330
[2019-04-04 11:34:50,830] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.7, 92.5, 27.0, 0.0, 26.0, 25.71156784323063, 0.5083116900686168, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 981000.0000, 
sim time next is 981600.0000, 
raw observation next is [9.8, 92.33333333333333, 32.5, 0.0, 26.0, 25.92291067003763, 0.5361060202239345, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.7340720221606649, 0.9233333333333333, 0.10833333333333334, 0.0, 0.6666666666666666, 0.6602425558364692, 0.6787020067413115, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.7865462], dtype=float32), 1.3369722]. 
=============================================
[2019-04-04 11:34:50,997] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299000, global step 4779452: loss 0.0085
[2019-04-04 11:34:50,998] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299000, global step 4779452: learning rate 0.0000
[2019-04-04 11:34:51,551] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8513343e-16 3.2042630e-15 8.5770326e-26 1.3441944e-16 3.9761624e-16
 2.6884644e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:51,552] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3371
[2019-04-04 11:34:51,566] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.533333333333333, 66.0, 0.0, 0.0, 26.0, 24.62260968390205, 0.2208489905353493, 0.0, 1.0, 42931.40748044642], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 772800.0000, 
sim time next is 773400.0000, 
raw observation next is [-6.616666666666667, 66.5, 0.0, 0.0, 26.0, 24.59085504386332, 0.2137159059543678, 0.0, 1.0, 42783.71985238007], 
processed observation next is [1.0, 0.9565217391304348, 0.2793167128347184, 0.665, 0.0, 0.0, 0.6666666666666666, 0.5492379203219434, 0.5712386353181226, 0.0, 1.0, 0.20373199929704794], 
reward next is 0.7963, 
noisyNet noise sample is [array([0.8611289], dtype=float32), -0.18987346]. 
=============================================
[2019-04-04 11:34:52,662] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299000, global step 4780134: loss 0.0117
[2019-04-04 11:34:52,662] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299000, global step 4780134: learning rate 0.0000
[2019-04-04 11:34:53,022] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299000, global step 4780278: loss 0.0111
[2019-04-04 11:34:53,039] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299000, global step 4780278: learning rate 0.0000
[2019-04-04 11:34:54,436] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.7990662e-17 1.9129067e-16 1.2839496e-26 3.2927318e-18 2.5511319e-18
 5.1287906e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:54,463] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.4327
[2019-04-04 11:34:54,515] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 83.66666666666666, 57.33333333333334, 0.0, 26.0, 26.28186620210179, 0.4503129875735648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 830400.0000, 
sim time next is 831000.0000, 
raw observation next is [-3.9, 84.83333333333334, 55.66666666666666, 0.0, 26.0, 26.26077279109061, 0.4445927523343778, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.3545706371191136, 0.8483333333333334, 0.18555555555555553, 0.0, 0.6666666666666666, 0.6883977325908841, 0.6481975841114592, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37539437], dtype=float32), 1.4012257]. 
=============================================
[2019-04-04 11:34:54,527] A3C_AGENT_WORKER-Thread-4 DEBUG:Value prediction is [[85.34976 ]
 [85.375015]
 [85.52257 ]
 [85.61514 ]
 [85.72618 ]], R is [[85.4931488 ]
 [85.63822174]
 [85.78183746]
 [85.92401886]
 [86.06478119]].
[2019-04-04 11:34:56,493] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9827646e-17 1.3557717e-15 2.3346554e-26 7.3072234e-18 1.4651771e-17
 2.7404845e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:56,493] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0028
[2019-04-04 11:34:56,531] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.899999999999999, 83.33333333333334, 45.66666666666667, 0.0, 26.0, 25.76830698133907, 0.4094998772813001, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 834000.0000, 
sim time next is 834600.0000, 
raw observation next is [-3.9, 82.66666666666667, 42.33333333333334, 0.0, 26.0, 25.9545985033458, 0.4248330232141264, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.3545706371191136, 0.8266666666666667, 0.14111111111111113, 0.0, 0.6666666666666666, 0.66288320861215, 0.6416110077380421, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6826416], dtype=float32), -0.65491945]. 
=============================================
[2019-04-04 11:34:57,049] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299000, global step 4781791: loss 0.0103
[2019-04-04 11:34:57,050] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299000, global step 4781791: learning rate 0.0000
[2019-04-04 11:34:57,450] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299000, global step 4781964: loss 0.0070
[2019-04-04 11:34:57,451] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299000, global step 4781964: learning rate 0.0000
[2019-04-04 11:34:58,025] A3C_AGENT_WORKER-Thread-14 INFO:Local step 298500, global step 4782206: loss 0.0002
[2019-04-04 11:34:58,025] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 298500, global step 4782206: learning rate 0.0000
[2019-04-04 11:34:58,100] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6397570e-17 1.2239077e-16 6.7825298e-27 2.7608315e-18 7.9516988e-18
 5.5261591e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:34:58,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1993
[2019-04-04 11:34:58,113] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.26666666666667, 84.33333333333334, 120.5, 0.0, 26.0, 26.18727228012763, 0.6260741240635478, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 998400.0000, 
sim time next is 999000.0000, 
raw observation next is [13.55, 83.5, 119.0, 0.0, 26.0, 26.28732943942103, 0.6472030167620462, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.5652173913043478, 0.8379501385041552, 0.835, 0.39666666666666667, 0.0, 0.6666666666666666, 0.6906107866184191, 0.7157343389206821, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.1244114], dtype=float32), -0.45156687]. 
=============================================
[2019-04-04 11:34:58,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[87.59303 ]
 [87.56273 ]
 [87.56838 ]
 [87.563515]
 [87.73876 ]], R is [[87.87223816]
 [87.99351501]
 [88.1135788 ]
 [88.23244476]
 [88.35012054]].
[2019-04-04 11:34:59,489] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299000, global step 4782886: loss 0.0102
[2019-04-04 11:34:59,490] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299000, global step 4782886: learning rate 0.0000
[2019-04-04 11:35:00,006] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.3032989e-18 2.1709990e-17 4.8010073e-28 2.4591420e-19 1.5116744e-18
 6.3006525e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:35:00,009] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3627
[2019-04-04 11:35:00,031] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [12.45, 86.0, 128.0, 0.0, 26.0, 25.73372131643142, 0.5533700844303221, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 995400.0000, 
sim time next is 996000.0000, 
raw observation next is [12.53333333333333, 86.0, 126.5, 0.0, 26.0, 24.72091038412076, 0.5005779768195365, 1.0, 1.0, 5604.123501069162], 
processed observation next is [1.0, 0.5217391304347826, 0.8097876269621421, 0.86, 0.4216666666666667, 0.0, 0.6666666666666666, 0.5600758653433967, 0.6668593256065122, 1.0, 1.0, 0.026686302386043626], 
reward next is 0.9733, 
noisyNet noise sample is [array([0.14153074], dtype=float32), -1.9032165]. 
=============================================
[2019-04-04 11:35:00,053] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[90.005196]
 [90.37069 ]
 [90.55735 ]
 [90.64633 ]
 [90.758965]], R is [[89.59809113]
 [89.70211029]
 [89.80509186]
 [89.90704346]
 [90.00797272]].
[2019-04-04 11:35:00,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.0520325e-18 1.3737470e-17 1.0136353e-28 6.3778241e-19 9.5677117e-19
 2.1469790e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:35:00,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6006
[2019-04-04 11:35:00,958] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.3, 65.0, 138.3333333333333, 0.0, 26.0, 25.05266998707916, 0.4980474766512137, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1174200.0000, 
sim time next is 1174800.0000, 
raw observation next is [18.3, 65.0, 133.1666666666667, 0.0, 26.0, 25.04767801826771, 0.5005875134826833, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.6086956521739131, 0.9695290858725764, 0.65, 0.44388888888888905, 0.0, 0.6666666666666666, 0.5873065015223092, 0.6668625044942278, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.3543638], dtype=float32), -0.23888949]. 
=============================================
[2019-04-04 11:35:01,555] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1680382e-17 4.6655173e-16 3.6857831e-27 3.1729494e-18 1.6488517e-17
 3.0266648e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:35:01,556] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6660
[2019-04-04 11:35:01,569] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.1, 79.66666666666667, 0.0, 0.0, 26.0, 24.78651452917165, 0.2196883615292922, 0.0, 1.0, 39436.89078638557], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 868800.0000, 
sim time next is 869400.0000, 
raw observation next is [-2.0, 79.5, 0.0, 0.0, 26.0, 24.75598200917459, 0.2241161367041825, 0.0, 1.0, 39419.24702649705], 
processed observation next is [1.0, 0.043478260869565216, 0.40720221606648205, 0.795, 0.0, 0.0, 0.6666666666666666, 0.5629985007645493, 0.5747053789013942, 0.0, 1.0, 0.1877107001261764], 
reward next is 0.8123, 
noisyNet noise sample is [array([-0.9324402], dtype=float32), -0.43597516]. 
=============================================
[2019-04-04 11:35:01,842] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299000, global step 4784157: loss 0.0122
[2019-04-04 11:35:01,844] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299000, global step 4784157: learning rate 0.0000
[2019-04-04 11:35:03,804] A3C_AGENT_WORKER-Thread-12 INFO:Local step 299500, global step 4785147: loss 0.0651
[2019-04-04 11:35:03,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 299500, global step 4785148: learning rate 0.0000
[2019-04-04 11:35:04,632] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299000, global step 4785600: loss 0.0192
[2019-04-04 11:35:04,633] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299000, global step 4785600: learning rate 0.0000
[2019-04-04 11:35:04,840] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299000, global step 4785690: loss 0.0168
[2019-04-04 11:35:04,877] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299000, global step 4785690: learning rate 0.0000
[2019-04-04 11:35:05,138] A3C_AGENT_WORKER-Thread-2 INFO:Local step 299500, global step 4785839: loss 0.0695
[2019-04-04 11:35:05,140] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 299500, global step 4785840: learning rate 0.0000
[2019-04-04 11:35:05,283] A3C_AGENT_WORKER-Thread-5 INFO:Local step 299500, global step 4785927: loss 0.0788
[2019-04-04 11:35:05,284] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 299500, global step 4785927: learning rate 0.0000
[2019-04-04 11:35:06,072] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299000, global step 4786317: loss 0.0175
[2019-04-04 11:35:06,075] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299000, global step 4786317: learning rate 0.0000
[2019-04-04 11:35:06,408] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3569028e-17 3.5941784e-16 2.0349006e-27 2.5996033e-18 1.7876417e-17
 5.3226924e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:35:06,411] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5867
[2019-04-04 11:35:06,424] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.0, 96.0, 0.0, 0.0, 26.0, 25.23977803303761, 0.4092096379802662, 0.0, 1.0, 38549.2671142858], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 948600.0000, 
sim time next is 949200.0000, 
raw observation next is [5.0, 96.0, 0.0, 0.0, 26.0, 25.24021052798343, 0.4145045783637066, 0.0, 1.0, 38454.95948723696], 
processed observation next is [1.0, 1.0, 0.6011080332409973, 0.96, 0.0, 0.0, 0.6666666666666666, 0.6033508773319524, 0.6381681927879023, 0.0, 1.0, 0.1831188547011284], 
reward next is 0.8169, 
noisyNet noise sample is [array([-0.49723354], dtype=float32), 0.7975212]. 
=============================================
[2019-04-04 11:35:06,589] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299000, global step 4786607: loss 0.0218
[2019-04-04 11:35:06,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299000, global step 4786608: learning rate 0.0000
[2019-04-04 11:35:07,186] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299000, global step 4786989: loss 0.0172
[2019-04-04 11:35:07,188] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299000, global step 4786989: learning rate 0.0000
[2019-04-04 11:35:07,911] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299000, global step 4787373: loss 0.0210
[2019-04-04 11:35:07,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299000, global step 4787374: learning rate 0.0000
[2019-04-04 11:35:08,197] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299000, global step 4787515: loss 0.0221
[2019-04-04 11:35:08,200] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299000, global step 4787517: learning rate 0.0000
[2019-04-04 11:35:08,264] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299000, global step 4787554: loss 0.0256
[2019-04-04 11:35:08,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299000, global step 4787555: learning rate 0.0000
[2019-04-04 11:35:09,244] A3C_AGENT_WORKER-Thread-18 INFO:Local step 299500, global step 4788154: loss 0.0603
[2019-04-04 11:35:09,250] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 299500, global step 4788158: learning rate 0.0000
[2019-04-04 11:35:09,856] A3C_AGENT_WORKER-Thread-20 INFO:Local step 299500, global step 4788521: loss 0.0621
[2019-04-04 11:35:09,857] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 299500, global step 4788522: learning rate 0.0000
[2019-04-04 11:35:11,479] A3C_AGENT_WORKER-Thread-6 INFO:Local step 299500, global step 4789536: loss 0.1104
[2019-04-04 11:35:11,482] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 299500, global step 4789537: learning rate 0.0000
[2019-04-04 11:35:15,487] A3C_AGENT_WORKER-Thread-16 INFO:Local step 299500, global step 4790837: loss 0.0925
[2019-04-04 11:35:15,490] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 299500, global step 4790837: learning rate 0.0000
[2019-04-04 11:35:22,346] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299000, global step 4792523: loss 0.0330
[2019-04-04 11:35:22,346] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299000, global step 4792523: learning rate 0.0000
[2019-04-04 11:35:23,039] A3C_AGENT_WORKER-Thread-13 INFO:Local step 299500, global step 4792740: loss 0.0745
[2019-04-04 11:35:23,064] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 299500, global step 4792740: learning rate 0.0000
[2019-04-04 11:35:23,923] A3C_AGENT_WORKER-Thread-4 INFO:Local step 299500, global step 4792997: loss 0.0860
[2019-04-04 11:35:23,974] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 299500, global step 4792997: learning rate 0.0000
[2019-04-04 11:35:27,174] A3C_AGENT_WORKER-Thread-10 INFO:Local step 299500, global step 4793890: loss 0.0923
[2019-04-04 11:35:27,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 299500, global step 4793892: learning rate 0.0000
[2019-04-04 11:35:27,629] A3C_AGENT_WORKER-Thread-15 INFO:Local step 299500, global step 4794002: loss 0.1035
[2019-04-04 11:35:27,631] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 299500, global step 4794002: learning rate 0.0000
[2019-04-04 11:35:28,928] A3C_AGENT_WORKER-Thread-3 INFO:Local step 299500, global step 4794317: loss 0.1030
[2019-04-04 11:35:28,929] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 299500, global step 4794317: learning rate 0.0000
[2019-04-04 11:35:29,654] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300000, global step 4794499: loss 5.0942
[2019-04-04 11:35:29,654] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300000, global step 4794499: learning rate 0.0000
[2019-04-04 11:35:29,749] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300000, global step 4794522: loss 5.0902
[2019-04-04 11:35:29,750] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300000, global step 4794522: learning rate 0.0000
[2019-04-04 11:35:30,041] A3C_AGENT_WORKER-Thread-19 INFO:Local step 299500, global step 4794609: loss 0.1058
[2019-04-04 11:35:30,067] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 299500, global step 4794609: learning rate 0.0000
[2019-04-04 11:35:30,286] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300000, global step 4794674: loss 5.0555
[2019-04-04 11:35:30,301] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300000, global step 4794674: learning rate 0.0000
[2019-04-04 11:35:31,230] A3C_AGENT_WORKER-Thread-17 INFO:Local step 299500, global step 4794950: loss 0.1182
[2019-04-04 11:35:31,231] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 299500, global step 4794950: learning rate 0.0000
[2019-04-04 11:35:31,283] A3C_AGENT_WORKER-Thread-11 INFO:Local step 299500, global step 4794964: loss 0.1139
[2019-04-04 11:35:31,284] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 299500, global step 4794964: learning rate 0.0000
[2019-04-04 11:35:32,521] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6095022e-16 2.0559175e-15 9.2684262e-27 1.5370396e-17 1.1565909e-16
 1.6765670e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:35:32,522] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9176
[2019-04-04 11:35:32,560] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.9, 92.0, 0.0, 0.0, 26.0, 25.47704199794899, 0.5539567211572425, 0.0, 1.0, 18756.04120420961], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1312200.0000, 
sim time next is 1312800.0000, 
raw observation next is [1.8, 92.0, 0.0, 0.0, 26.0, 25.49982154747223, 0.5419040528677315, 0.0, 1.0, 18753.29272727015], 
processed observation next is [1.0, 0.17391304347826086, 0.5124653739612189, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6249851289560192, 0.6806346842892438, 0.0, 1.0, 0.08930139393938166], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.12918408], dtype=float32), 1.1060807]. 
=============================================
[2019-04-04 11:35:37,936] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300000, global step 4796937: loss 5.4631
[2019-04-04 11:35:37,938] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300000, global step 4796937: learning rate 0.0000
[2019-04-04 11:35:39,551] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300000, global step 4797371: loss 5.5186
[2019-04-04 11:35:39,552] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300000, global step 4797371: learning rate 0.0000
[2019-04-04 11:35:40,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9446830e-17 2.1532765e-15 4.1903756e-27 1.4702159e-17 1.0345174e-16
 4.4190851e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:35:40,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0880
[2019-04-04 11:35:40,339] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 95.0, 0.0, 0.0, 26.0, 25.22611638676946, 0.4658672886692772, 0.0, 1.0, 39697.07950340564], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1390800.0000, 
sim time next is 1391400.0000, 
raw observation next is [0.0, 95.0, 0.0, 0.0, 26.0, 25.21355004619539, 0.4706732029652509, 0.0, 1.0, 39635.23691453414], 
processed observation next is [1.0, 0.08695652173913043, 0.46260387811634357, 0.95, 0.0, 0.0, 0.6666666666666666, 0.6011291705162826, 0.6568910676550836, 0.0, 1.0, 0.18873922340254354], 
reward next is 0.8113, 
noisyNet noise sample is [array([0.7069819], dtype=float32), -0.14585805]. 
=============================================
[2019-04-04 11:35:40,673] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300000, global step 4797712: loss 5.5648
[2019-04-04 11:35:40,673] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300000, global step 4797712: learning rate 0.0000
[2019-04-04 11:35:45,541] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300000, global step 4799042: loss 5.8384
[2019-04-04 11:35:45,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300000, global step 4799042: learning rate 0.0000
[2019-04-04 11:35:46,568] A3C_AGENT_WORKER-Thread-14 INFO:Local step 299500, global step 4799317: loss 0.1802
[2019-04-04 11:35:46,569] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 299500, global step 4799317: learning rate 0.0000
[2019-04-04 11:35:49,030] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-04 11:35:49,039] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:35:49,039] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:35:49,045] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:35:49,045] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:35:49,047] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run49
[2019-04-04 11:35:49,100] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:35:49,102] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:35:49,106] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run49
[2019-04-04 11:35:49,155] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run49
[2019-04-04 11:36:24,531] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15329415], dtype=float32), -0.32758558]
[2019-04-04 11:36:24,531] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-1.45, 26.5, 129.0, 0.0, 26.0, 25.16601169557941, 0.154144857398555, 1.0, 1.0, 19878.01638510948]
[2019-04-04 11:36:24,531] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:36:24,532] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [8.1691404e-17 6.6354909e-16 1.2446793e-26 2.1779169e-17 9.8561523e-18
 1.7124999e-19 1.0000000e+00], sampled 0.2249109286173533
[2019-04-04 11:37:47,084] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15329415], dtype=float32), -0.32758558]
[2019-04-04 11:37:47,084] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-6.416666666666667, 76.33333333333333, 152.3333333333333, 46.33333333333333, 26.0, 25.66692616119619, 0.3367504042133649, 1.0, 1.0, 18727.76364756783]
[2019-04-04 11:37:47,085] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:37:47,085] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [4.7510594e-17 2.4399343e-16 4.1409243e-27 6.5835954e-18 6.4180485e-18
 6.4461112e-20 1.0000000e+00], sampled 0.962281625768147
[2019-04-04 11:38:02,676] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15329415], dtype=float32), -0.32758558]
[2019-04-04 11:38:02,676] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [2.082943639666667, 29.81060076666667, 188.8671980666666, 227.5912600833334, 26.0, 25.37982903910629, 0.30919794815499, 1.0, 1.0, 0.0]
[2019-04-04 11:38:02,676] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 11:38:02,679] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.21661688e-17 2.15509971e-16 1.86528025e-26 8.10407954e-18
 5.42589826e-18 1.05395444e-19 1.00000000e+00], sampled 0.025198251833728924
[2019-04-04 11:38:49,241] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15329415], dtype=float32), -0.32758558]
[2019-04-04 11:38:49,241] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [22.0, 38.0, 0.0, 0.0, 26.0, 29.75715562620458, 1.644100036084178, 1.0, 0.0, 0.0]
[2019-04-04 11:38:49,241] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:38:49,242] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.7446392e-18 1.1609709e-17 4.1680518e-29 2.1632425e-19 3.5627491e-19
 6.0427482e-22 1.0000000e+00], sampled 0.4334176585934255
[2019-04-04 11:38:57,363] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:39:26,786] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:39:31,534] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:39:32,580] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 4800000, evaluation results [4800000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:39:33,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.0831674e-16 2.0603613e-15 8.5833823e-26 6.8896628e-17 3.6964769e-16
 1.0105518e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:39:33,392] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3592
[2019-04-04 11:39:33,416] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.6, 92.0, 0.0, 0.0, 26.0, 25.37952916535713, 0.4955869625587179, 0.0, 1.0, 35145.83102403407], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1465800.0000, 
sim time next is 1466400.0000, 
raw observation next is [1.6, 92.0, 0.0, 0.0, 26.0, 25.40001989716134, 0.493329883948477, 0.0, 1.0, 25659.73258467117], 
processed observation next is [1.0, 1.0, 0.5069252077562327, 0.92, 0.0, 0.0, 0.6666666666666666, 0.6166683247634449, 0.6644432946494924, 0.0, 1.0, 0.12218920278414842], 
reward next is 0.8778, 
noisyNet noise sample is [array([2.1946936], dtype=float32), 0.40222657]. 
=============================================
[2019-04-04 11:39:34,180] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300000, global step 4800454: loss 6.1689
[2019-04-04 11:39:34,186] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300000, global step 4800455: learning rate 0.0000
[2019-04-04 11:39:34,585] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300000, global step 4800566: loss 6.1565
[2019-04-04 11:39:34,586] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300000, global step 4800566: learning rate 0.0000
[2019-04-04 11:39:38,078] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9455358e-16 4.3262564e-15 4.1405337e-25 3.1056870e-17 1.8009624e-16
 5.5449980e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:39:38,079] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1923
[2019-04-04 11:39:38,090] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300000, global step 4801786: loss 6.4047
[2019-04-04 11:39:38,091] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300000, global step 4801786: learning rate 0.0000
[2019-04-04 11:39:38,091] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.933333333333334, 69.0, 0.0, 0.0, 26.0, 25.70716641642262, 0.5793050635582521, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1539600.0000, 
sim time next is 1540200.0000, 
raw observation next is [7.566666666666666, 71.0, 0.0, 0.0, 26.0, 25.60369170532055, 0.5626294949109124, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.672206832871653, 0.71, 0.0, 0.0, 0.6666666666666666, 0.6336409754433792, 0.6875431649703042, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.6481878], dtype=float32), -0.037133053]. 
=============================================
[2019-04-04 11:39:38,993] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300000, global step 4802126: loss 6.2489
[2019-04-04 11:39:39,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300000, global step 4802130: learning rate 0.0000
[2019-04-04 11:39:39,259] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300000, global step 4802233: loss 6.3418
[2019-04-04 11:39:39,259] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300000, global step 4802233: learning rate 0.0000
[2019-04-04 11:39:40,698] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300000, global step 4802779: loss 6.5075
[2019-04-04 11:39:40,700] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300000, global step 4802781: learning rate 0.0000
[2019-04-04 11:39:40,819] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300000, global step 4802823: loss 6.5190
[2019-04-04 11:39:40,821] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300000, global step 4802824: learning rate 0.0000
[2019-04-04 11:39:41,450] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300000, global step 4803020: loss 6.4986
[2019-04-04 11:39:41,452] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300000, global step 4803020: learning rate 0.0000
[2019-04-04 11:39:43,278] A3C_AGENT_WORKER-Thread-2 INFO:Local step 300500, global step 4803626: loss 0.0321
[2019-04-04 11:39:43,279] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 300500, global step 4803626: learning rate 0.0000
[2019-04-04 11:39:43,797] A3C_AGENT_WORKER-Thread-5 INFO:Local step 300500, global step 4803836: loss 0.0318
[2019-04-04 11:39:43,799] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 300500, global step 4803837: learning rate 0.0000
[2019-04-04 11:39:44,001] A3C_AGENT_WORKER-Thread-12 INFO:Local step 300500, global step 4803924: loss 0.0321
[2019-04-04 11:39:44,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 300500, global step 4803924: learning rate 0.0000
[2019-04-04 11:39:46,977] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9160668e-16 3.4792696e-15 2.0925571e-25 1.8942653e-17 4.5814260e-16
 1.8069328e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:39:46,977] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9581
[2019-04-04 11:39:46,999] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.25, 93.5, 0.0, 0.0, 26.0, 25.43541144103114, 0.4840968147140132, 0.0, 1.0, 59715.88290312618], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1722600.0000, 
sim time next is 1723200.0000, 
raw observation next is [0.1666666666666667, 94.0, 0.0, 0.0, 26.0, 25.36615097963132, 0.4880868407521859, 0.0, 1.0, 83856.70398825551], 
processed observation next is [1.0, 0.9565217391304348, 0.4672206832871654, 0.94, 0.0, 0.0, 0.6666666666666666, 0.6138459149692768, 0.6626956135840619, 0.0, 1.0, 0.399317638039312], 
reward next is 0.6007, 
noisyNet noise sample is [array([-0.5129386], dtype=float32), 0.24952903]. 
=============================================
[2019-04-04 11:39:50,144] A3C_AGENT_WORKER-Thread-18 INFO:Local step 300500, global step 4805966: loss 0.0373
[2019-04-04 11:39:50,144] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 300500, global step 4805966: learning rate 0.0000
[2019-04-04 11:39:50,835] A3C_AGENT_WORKER-Thread-20 INFO:Local step 300500, global step 4806128: loss 0.0384
[2019-04-04 11:39:50,837] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 300500, global step 4806128: learning rate 0.0000
[2019-04-04 11:39:51,547] A3C_AGENT_WORKER-Thread-6 INFO:Local step 300500, global step 4806361: loss 0.0391
[2019-04-04 11:39:51,547] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 300500, global step 4806361: learning rate 0.0000
[2019-04-04 11:39:51,695] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300000, global step 4806413: loss 7.1451
[2019-04-04 11:39:51,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300000, global step 4806413: learning rate 0.0000
[2019-04-04 11:39:53,989] A3C_AGENT_WORKER-Thread-16 INFO:Local step 300500, global step 4807217: loss 0.0382
[2019-04-04 11:39:53,990] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 300500, global step 4807217: learning rate 0.0000
[2019-04-04 11:39:54,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3990267e-16 1.0636826e-15 5.8636389e-26 1.2679953e-17 2.4340112e-17
 1.4923760e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:39:54,521] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2321
[2019-04-04 11:39:54,588] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.166666666666667, 86.83333333333333, 19.66666666666667, 0.0, 26.0, 25.28947086336251, 0.2501139423251139, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2016600.0000, 
sim time next is 2017200.0000, 
raw observation next is [-6.133333333333334, 86.66666666666667, 24.33333333333334, 0.0, 26.0, 25.22414346677312, 0.2847992808064334, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.2927054478301016, 0.8666666666666667, 0.08111111111111113, 0.0, 0.6666666666666666, 0.6020119555644268, 0.5949330936021445, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04045365], dtype=float32), 0.8942767]. 
=============================================
[2019-04-04 11:39:57,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3060980e-15 6.7311235e-15 1.3878584e-24 3.1400925e-16 3.1905396e-16
 7.2366806e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:39:57,146] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2156
[2019-04-04 11:39:57,223] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.5, 79.0, 167.0, 70.0, 26.0, 25.02572517597086, 0.2847397046276891, 0.0, 1.0, 43144.55794133589], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1867200.0000, 
sim time next is 1867800.0000, 
raw observation next is [-4.5, 81.0, 148.0, 56.00000000000001, 26.0, 25.03170838759691, 0.2859354279134825, 0.0, 1.0, 39539.85861198878], 
processed observation next is [0.0, 0.6086956521739131, 0.3379501385041552, 0.81, 0.49333333333333335, 0.06187845303867404, 0.6666666666666666, 0.5859756989664092, 0.5953118093044941, 0.0, 1.0, 0.18828504100947036], 
reward next is 0.8117, 
noisyNet noise sample is [array([1.0504975], dtype=float32), 0.9221034]. 
=============================================
[2019-04-04 11:39:58,868] A3C_AGENT_WORKER-Thread-4 INFO:Local step 300500, global step 4808515: loss 0.0396
[2019-04-04 11:39:58,869] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 300500, global step 4808515: learning rate 0.0000
[2019-04-04 11:39:59,197] A3C_AGENT_WORKER-Thread-13 INFO:Local step 300500, global step 4808600: loss 0.0369
[2019-04-04 11:39:59,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 300500, global step 4808600: learning rate 0.0000
[2019-04-04 11:40:01,994] A3C_AGENT_WORKER-Thread-10 INFO:Local step 300500, global step 4809508: loss 0.0392
[2019-04-04 11:40:01,995] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 300500, global step 4809508: learning rate 0.0000
[2019-04-04 11:40:02,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8028032e-16 8.1011205e-16 2.9183999e-26 4.6835433e-17 9.1107625e-17
 2.3943816e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:02,720] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4237
[2019-04-04 11:40:02,794] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 12.33333333333333, 7.666666666666665, 26.0, 24.79274058743896, 0.1526231350193654, 1.0, 1.0, 70895.38934126002], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1929000.0000, 
sim time next is 1929600.0000, 
raw observation next is [-9.5, 91.0, 17.5, 11.0, 26.0, 24.98534946820245, 0.18130498475133, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.1994459833795014, 0.91, 0.058333333333333334, 0.012154696132596685, 0.6666666666666666, 0.5821124556835375, 0.56043499491711, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0365742], dtype=float32), -1.3388114]. 
=============================================
[2019-04-04 11:40:04,024] A3C_AGENT_WORKER-Thread-15 INFO:Local step 300500, global step 4809999: loss 0.0354
[2019-04-04 11:40:04,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 300500, global step 4809999: learning rate 0.0000
[2019-04-04 11:40:04,528] A3C_AGENT_WORKER-Thread-3 INFO:Local step 300500, global step 4810130: loss 0.0356
[2019-04-04 11:40:04,529] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 300500, global step 4810130: learning rate 0.0000
[2019-04-04 11:40:05,280] A3C_AGENT_WORKER-Thread-11 INFO:Local step 300500, global step 4810313: loss 0.0361
[2019-04-04 11:40:05,280] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 300500, global step 4810313: learning rate 0.0000
[2019-04-04 11:40:05,549] A3C_AGENT_WORKER-Thread-19 INFO:Local step 300500, global step 4810374: loss 0.0362
[2019-04-04 11:40:05,569] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 300500, global step 4810374: learning rate 0.0000
[2019-04-04 11:40:07,059] A3C_AGENT_WORKER-Thread-17 INFO:Local step 300500, global step 4810747: loss 0.0336
[2019-04-04 11:40:07,061] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 300500, global step 4810748: learning rate 0.0000
[2019-04-04 11:40:09,854] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301000, global step 4811658: loss 0.0217
[2019-04-04 11:40:09,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301000, global step 4811658: learning rate 0.0000
[2019-04-04 11:40:10,429] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301000, global step 4811824: loss 0.0274
[2019-04-04 11:40:10,431] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301000, global step 4811824: learning rate 0.0000
[2019-04-04 11:40:11,277] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301000, global step 4812047: loss 0.0243
[2019-04-04 11:40:11,278] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301000, global step 4812047: learning rate 0.0000
[2019-04-04 11:40:11,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3584268e-16 3.3331394e-15 3.3042834e-25 3.8430073e-17 4.0172642e-17
 7.3364908e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:11,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2252
[2019-04-04 11:40:12,038] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.35, 68.5, 30.0, 0.0, 26.0, 25.53503542476353, 0.3250331878569039, 1.0, 1.0, 34846.71876804351], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1960200.0000, 
sim time next is 1960800.0000, 
raw observation next is [-3.533333333333333, 70.66666666666667, 25.83333333333333, 0.3333333333333333, 26.0, 25.58066244577827, 0.2343629715244648, 1.0, 1.0, 36214.3142843807], 
processed observation next is [1.0, 0.6956521739130435, 0.36472760849492153, 0.7066666666666667, 0.0861111111111111, 0.00036832412523020257, 0.6666666666666666, 0.6317218704815225, 0.5781209905081549, 1.0, 1.0, 0.17244911563990808], 
reward next is 0.8276, 
noisyNet noise sample is [array([2.4391935], dtype=float32), -0.43229944]. 
=============================================
[2019-04-04 11:40:16,973] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301000, global step 4813516: loss 0.0178
[2019-04-04 11:40:16,974] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301000, global step 4813516: learning rate 0.0000
[2019-04-04 11:40:17,570] A3C_AGENT_WORKER-Thread-14 INFO:Local step 300500, global step 4813693: loss 0.0299
[2019-04-04 11:40:17,587] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 300500, global step 4813693: learning rate 0.0000
[2019-04-04 11:40:17,794] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2923497e-15 5.1559137e-15 2.9536814e-25 1.1123080e-16 2.2359282e-16
 8.2128452e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:17,795] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8333
[2019-04-04 11:40:17,827] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.299999999999999, 79.5, 0.0, 0.0, 26.0, 24.46262983067114, 0.1903912408624214, 0.0, 1.0, 42453.99146300081], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2163000.0000, 
sim time next is 2163600.0000, 
raw observation next is [-7.3, 79.0, 0.0, 0.0, 26.0, 24.46131348602582, 0.1797361216175922, 0.0, 1.0, 42484.89424388644], 
processed observation next is [1.0, 0.043478260869565216, 0.26038781163434904, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5384427905021516, 0.5599120405391974, 0.0, 1.0, 0.20230902020898306], 
reward next is 0.7977, 
noisyNet noise sample is [array([-0.317239], dtype=float32), 1.5449207]. 
=============================================
[2019-04-04 11:40:18,737] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301000, global step 4814088: loss 0.0290
[2019-04-04 11:40:18,739] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301000, global step 4814088: learning rate 0.0000
[2019-04-04 11:40:19,017] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301000, global step 4814165: loss 0.0279
[2019-04-04 11:40:19,017] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301000, global step 4814165: learning rate 0.0000
[2019-04-04 11:40:21,771] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301000, global step 4814822: loss 0.0216
[2019-04-04 11:40:21,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301000, global step 4814822: learning rate 0.0000
[2019-04-04 11:40:26,241] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301000, global step 4816150: loss 0.0349
[2019-04-04 11:40:26,242] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301000, global step 4816151: learning rate 0.0000
[2019-04-04 11:40:26,737] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301000, global step 4816319: loss 0.0326
[2019-04-04 11:40:26,738] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301000, global step 4816319: learning rate 0.0000
[2019-04-04 11:40:29,756] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301000, global step 4817101: loss 0.0177
[2019-04-04 11:40:29,757] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301000, global step 4817101: learning rate 0.0000
[2019-04-04 11:40:32,869] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301000, global step 4817969: loss 0.0210
[2019-04-04 11:40:32,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301000, global step 4817969: learning rate 0.0000
[2019-04-04 11:40:33,075] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301000, global step 4818049: loss 0.0252
[2019-04-04 11:40:33,077] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301000, global step 4818049: learning rate 0.0000
[2019-04-04 11:40:33,295] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301000, global step 4818123: loss 0.0235
[2019-04-04 11:40:33,296] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301000, global step 4818123: learning rate 0.0000
[2019-04-04 11:40:33,776] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301000, global step 4818311: loss 0.0244
[2019-04-04 11:40:33,787] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301000, global step 4818313: learning rate 0.0000
[2019-04-04 11:40:34,785] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301000, global step 4818650: loss 0.0318
[2019-04-04 11:40:34,786] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301000, global step 4818650: learning rate 0.0000
[2019-04-04 11:40:38,168] A3C_AGENT_WORKER-Thread-2 INFO:Local step 301500, global step 4819543: loss 0.9694
[2019-04-04 11:40:38,168] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 301500, global step 4819543: learning rate 0.0000
[2019-04-04 11:40:38,442] A3C_AGENT_WORKER-Thread-5 INFO:Local step 301500, global step 4819604: loss 0.9736
[2019-04-04 11:40:38,443] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 301500, global step 4819604: learning rate 0.0000
[2019-04-04 11:40:39,281] A3C_AGENT_WORKER-Thread-12 INFO:Local step 301500, global step 4819835: loss 0.9475
[2019-04-04 11:40:39,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 301500, global step 4819835: learning rate 0.0000
[2019-04-04 11:40:40,217] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7653077e-15 1.2033997e-14 3.4482540e-25 1.5025520e-16 2.8330846e-16
 1.7411151e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:40,217] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7041
[2019-04-04 11:40:40,264] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 42.0, 0.0, 0.0, 26.0, 24.9602912527448, 0.1852125825410776, 0.0, 1.0, 38660.13633837409], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2515200.0000, 
sim time next is 2515800.0000, 
raw observation next is [-1.7, 43.0, 0.0, 0.0, 26.0, 24.94857546907565, 0.178931114052672, 0.0, 1.0, 38639.71611160116], 
processed observation next is [1.0, 0.08695652173913043, 0.4155124653739613, 0.43, 0.0, 0.0, 0.6666666666666666, 0.5790479557563041, 0.559643704684224, 0.0, 1.0, 0.18399864815048173], 
reward next is 0.8160, 
noisyNet noise sample is [array([-0.6291506], dtype=float32), 0.4731688]. 
=============================================
[2019-04-04 11:40:41,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.2417742e-15 3.3013018e-15 2.5681027e-25 9.6780661e-17 3.0894529e-16
 2.0622757e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:41,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2879
[2019-04-04 11:40:41,775] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.016666666666667, 35.16666666666666, 82.66666666666667, 811.6666666666667, 26.0, 24.98855079778854, 0.2499331062512562, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2459400.0000, 
sim time next is 2460000.0000, 
raw observation next is [-1.733333333333333, 34.33333333333334, 84.33333333333334, 820.3333333333334, 26.0, 24.98140870641038, 0.2498453329389426, 0.0, 1.0, 18734.07632168736], 
processed observation next is [0.0, 0.4782608695652174, 0.41458910433979695, 0.34333333333333343, 0.28111111111111114, 0.9064456721915286, 0.6666666666666666, 0.5817840588675317, 0.5832817776463142, 0.0, 1.0, 0.08920988724613028], 
reward next is 0.9108, 
noisyNet noise sample is [array([0.49218494], dtype=float32), 2.1755261]. 
=============================================
[2019-04-04 11:40:41,796] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.61288 ]
 [82.71419 ]
 [82.687706]
 [82.67759 ]
 [82.555504]], R is [[82.5942688 ]
 [82.76832581]
 [82.85139465]
 [82.83115387]
 [82.80643463]].
[2019-04-04 11:40:42,295] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.1767304e-15 2.5885552e-14 1.1829168e-24 1.0749736e-15 1.0495754e-15
 4.1679960e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:42,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0834
[2019-04-04 11:40:42,323] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 66.33333333333334, 0.0, 0.0, 26.0, 24.21362894256016, 0.1088185025260333, 0.0, 1.0, 41173.27378551433], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2355600.0000, 
sim time next is 2356200.0000, 
raw observation next is [-3.1, 67.0, 0.0, 0.0, 26.0, 24.19917590433491, 0.09932681345474502, 0.0, 1.0, 41201.83459730625], 
processed observation next is [0.0, 0.2608695652173913, 0.37673130193905824, 0.67, 0.0, 0.0, 0.6666666666666666, 0.5165979920279092, 0.5331089378182483, 0.0, 1.0, 0.19619921236812501], 
reward next is 0.8038, 
noisyNet noise sample is [array([1.1169298], dtype=float32), 0.666721]. 
=============================================
[2019-04-04 11:40:43,960] A3C_AGENT_WORKER-Thread-18 INFO:Local step 301500, global step 4821425: loss 0.8485
[2019-04-04 11:40:43,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 301500, global step 4821425: learning rate 0.0000
[2019-04-04 11:40:45,285] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301000, global step 4821856: loss 0.0285
[2019-04-04 11:40:45,286] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301000, global step 4821856: learning rate 0.0000
[2019-04-04 11:40:45,919] A3C_AGENT_WORKER-Thread-6 INFO:Local step 301500, global step 4822054: loss 0.8243
[2019-04-04 11:40:45,940] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 301500, global step 4822054: learning rate 0.0000
[2019-04-04 11:40:46,510] A3C_AGENT_WORKER-Thread-20 INFO:Local step 301500, global step 4822225: loss 0.8290
[2019-04-04 11:40:46,512] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 301500, global step 4822225: learning rate 0.0000
[2019-04-04 11:40:47,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.8931116e-16 1.2152324e-14 4.0777071e-26 9.0124881e-17 1.9221525e-16
 1.2086557e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:47,321] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7371
[2019-04-04 11:40:47,336] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.7, 55.0, 0.0, 0.0, 26.0, 25.43301926057366, 0.4261490673601802, 0.0, 1.0, 32336.74252528141], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2323800.0000, 
sim time next is 2324400.0000, 
raw observation next is [-1.7, 55.33333333333333, 0.0, 0.0, 26.0, 25.40218378438302, 0.4230242712954465, 0.0, 1.0, 51810.36194285156], 
processed observation next is [1.0, 0.9130434782608695, 0.4155124653739613, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.616848648698585, 0.6410080904318155, 0.0, 1.0, 0.24671600925167408], 
reward next is 0.7533, 
noisyNet noise sample is [array([-0.89782315], dtype=float32), 0.5142613]. 
=============================================
[2019-04-04 11:40:48,790] A3C_AGENT_WORKER-Thread-16 INFO:Local step 301500, global step 4823146: loss 0.8079
[2019-04-04 11:40:48,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 301500, global step 4823146: learning rate 0.0000
[2019-04-04 11:40:52,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0686521e-16 7.0653602e-15 1.8076856e-25 6.9796038e-17 7.4814834e-17
 6.8201776e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:52,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5228
[2019-04-04 11:40:52,524] A3C_AGENT_WORKER-Thread-4 INFO:Local step 301500, global step 4824315: loss 0.8027
[2019-04-04 11:40:52,526] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 301500, global step 4824316: learning rate 0.0000
[2019-04-04 11:40:52,563] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.4166666666666667, 35.83333333333333, 0.0, 0.0, 26.0, 25.01278699963433, 0.3195993129907079, 1.0, 1.0, 82167.01313448175], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2573400.0000, 
sim time next is 2574000.0000, 
raw observation next is [-0.6, 36.0, 0.0, 0.0, 26.0, 24.97583633636908, 0.3397244153446112, 1.0, 1.0, 81781.82283234474], 
processed observation next is [1.0, 0.8260869565217391, 0.44598337950138506, 0.36, 0.0, 0.0, 0.6666666666666666, 0.5813196946974234, 0.613241471781537, 1.0, 1.0, 0.389437251582594], 
reward next is 0.6106, 
noisyNet noise sample is [array([0.82774496], dtype=float32), -1.0936704]. 
=============================================
[2019-04-04 11:40:52,586] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[82.10908 ]
 [81.883965]
 [81.86162 ]
 [82.00889 ]
 [82.33047 ]], R is [[82.21374512]
 [82.00033569]
 [81.92162323]
 [82.10240936]
 [82.28138733]].
[2019-04-04 11:40:52,896] A3C_AGENT_WORKER-Thread-13 INFO:Local step 301500, global step 4824430: loss 0.7936
[2019-04-04 11:40:52,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 301500, global step 4824430: learning rate 0.0000
[2019-04-04 11:40:54,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.9961648e-15 5.3077551e-14 8.0613469e-25 1.7828968e-15 1.5693367e-15
 1.1292723e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:54,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9554
[2019-04-04 11:40:54,053] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.3, 42.0, 0.0, 0.0, 26.0, 24.6505243525592, 0.1707716268380425, 0.0, 1.0, 43107.65729545213], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2417400.0000, 
sim time next is 2418000.0000, 
raw observation next is [-5.4, 42.33333333333333, 0.0, 0.0, 26.0, 24.65011148072754, 0.1620680664688781, 0.0, 1.0, 43105.64281369696], 
processed observation next is [0.0, 1.0, 0.31301939058171746, 0.4233333333333333, 0.0, 0.0, 0.6666666666666666, 0.5541759567272949, 0.5540226888229594, 0.0, 1.0, 0.20526496577950934], 
reward next is 0.7947, 
noisyNet noise sample is [array([0.1360518], dtype=float32), 0.28841493]. 
=============================================
[2019-04-04 11:40:54,082] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.295815]
 [79.33009 ]
 [79.395836]
 [79.47282 ]
 [79.53538 ]], R is [[79.27866364]
 [79.2806015 ]
 [79.28251648]
 [79.28450775]
 [79.28659821]].
[2019-04-04 11:40:56,312] A3C_AGENT_WORKER-Thread-10 INFO:Local step 301500, global step 4825645: loss 0.7649
[2019-04-04 11:40:56,316] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 301500, global step 4825647: learning rate 0.0000
[2019-04-04 11:40:58,982] A3C_AGENT_WORKER-Thread-11 INFO:Local step 301500, global step 4826512: loss 0.6873
[2019-04-04 11:40:58,983] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 301500, global step 4826512: learning rate 0.0000
[2019-04-04 11:40:59,337] A3C_AGENT_WORKER-Thread-19 INFO:Local step 301500, global step 4826630: loss 0.6794
[2019-04-04 11:40:59,337] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 301500, global step 4826630: learning rate 0.0000
[2019-04-04 11:40:59,358] A3C_AGENT_WORKER-Thread-3 INFO:Local step 301500, global step 4826639: loss 0.6842
[2019-04-04 11:40:59,358] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 301500, global step 4826639: learning rate 0.0000
[2019-04-04 11:40:59,963] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [6.7889907e-15 2.2433891e-14 3.3485702e-24 4.4622225e-16 2.6166504e-15
 9.7104932e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:40:59,965] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.6082
[2019-04-04 11:40:59,983] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.333333333333334, 71.5, 0.0, 0.0, 26.0, 24.66755185192774, 0.2510504924373266, 0.0, 1.0, 44425.42122929738], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2679000.0000, 
sim time next is 2679600.0000, 
raw observation next is [-7.666666666666667, 71.0, 0.0, 0.0, 26.0, 24.60527887881784, 0.2497021991480936, 0.0, 1.0, 44430.08555026496], 
processed observation next is [1.0, 0.0, 0.2502308402585411, 0.71, 0.0, 0.0, 0.6666666666666666, 0.5504399065681532, 0.5832340663826979, 0.0, 1.0, 0.21157183595364265], 
reward next is 0.7884, 
noisyNet noise sample is [array([0.71473855], dtype=float32), 0.75909305]. 
=============================================
[2019-04-04 11:41:00,105] A3C_AGENT_WORKER-Thread-15 INFO:Local step 301500, global step 4826900: loss 0.6739
[2019-04-04 11:41:00,106] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 301500, global step 4826901: learning rate 0.0000
[2019-04-04 11:41:00,531] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302000, global step 4827078: loss 0.0935
[2019-04-04 11:41:00,548] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302000, global step 4827078: learning rate 0.0000
[2019-04-04 11:41:01,166] A3C_AGENT_WORKER-Thread-17 INFO:Local step 301500, global step 4827357: loss 0.6854
[2019-04-04 11:41:01,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 301500, global step 4827357: learning rate 0.0000
[2019-04-04 11:41:01,432] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302000, global step 4827467: loss 0.0774
[2019-04-04 11:41:01,434] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302000, global step 4827469: learning rate 0.0000
[2019-04-04 11:41:02,180] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302000, global step 4827727: loss 0.0757
[2019-04-04 11:41:02,205] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302000, global step 4827727: learning rate 0.0000
[2019-04-04 11:41:06,467] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302000, global step 4829201: loss 0.0746
[2019-04-04 11:41:06,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302000, global step 4829201: learning rate 0.0000
[2019-04-04 11:41:07,738] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302000, global step 4829744: loss 0.0622
[2019-04-04 11:41:07,739] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302000, global step 4829744: learning rate 0.0000
[2019-04-04 11:41:09,333] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302000, global step 4830243: loss 0.0714
[2019-04-04 11:41:09,333] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302000, global step 4830243: learning rate 0.0000
[2019-04-04 11:41:10,759] A3C_AGENT_WORKER-Thread-14 INFO:Local step 301500, global step 4830705: loss 0.7649
[2019-04-04 11:41:10,759] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 301500, global step 4830705: learning rate 0.0000
[2019-04-04 11:41:11,467] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302000, global step 4830949: loss 0.0673
[2019-04-04 11:41:11,467] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302000, global step 4830949: learning rate 0.0000
[2019-04-04 11:41:14,084] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.6625643e-16 1.9003479e-14 5.9429763e-25 9.4091173e-17 5.3351440e-16
 2.8734402e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:14,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7659
[2019-04-04 11:41:14,129] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.2, 65.0, 0.0, 0.0, 26.0, 25.36209804119012, 0.4587974201818167, 0.0, 1.0, 46979.20167103971], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2667600.0000, 
sim time next is 2668200.0000, 
raw observation next is [-1.516666666666667, 65.66666666666667, 0.0, 0.0, 26.0, 25.4635123139604, 0.4606349123137094, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.9130434782608695, 0.4205909510618652, 0.6566666666666667, 0.0, 0.0, 0.6666666666666666, 0.6219593594967, 0.6535449707712365, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.22928223], dtype=float32), 1.9598433]. 
=============================================
[2019-04-04 11:41:14,521] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302000, global step 4832081: loss 0.0647
[2019-04-04 11:41:14,522] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302000, global step 4832081: learning rate 0.0000
[2019-04-04 11:41:15,074] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302000, global step 4832293: loss 0.0652
[2019-04-04 11:41:15,074] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302000, global step 4832293: learning rate 0.0000
[2019-04-04 11:41:17,353] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0360896e-17 3.8026297e-16 8.2937246e-27 5.2263223e-18 6.6102703e-18
 2.9044929e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:17,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4859
[2019-04-04 11:41:17,450] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-14.33333333333333, 88.33333333333334, 82.83333333333334, 377.0, 26.0, 25.91862088338155, 0.4092565918935945, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2709600.0000, 
sim time next is 2710200.0000, 
raw observation next is [-14.16666666666667, 89.66666666666667, 85.66666666666667, 424.0, 26.0, 26.02374888291107, 0.4276603084892431, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.07017543859649114, 0.8966666666666667, 0.28555555555555556, 0.4685082872928177, 0.6666666666666666, 0.6686457402425893, 0.642553436163081, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5541627], dtype=float32), 0.9968491]. 
=============================================
[2019-04-04 11:41:18,797] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302000, global step 4833437: loss 0.0713
[2019-04-04 11:41:18,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302000, global step 4833438: learning rate 0.0000
[2019-04-04 11:41:19,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7485038e-15 4.0922398e-14 5.3102898e-24 6.8214110e-16 5.8644542e-15
 3.7945564e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:19,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3157
[2019-04-04 11:41:19,841] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.333333333333334, 73.0, 0.0, 0.0, 26.0, 24.39569780390377, 0.1408009645623995, 0.0, 1.0, 38752.02420802842], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3032400.0000, 
sim time next is 3033000.0000, 
raw observation next is [-5.5, 74.0, 0.0, 0.0, 26.0, 24.36361217295942, 0.1326868515724793, 0.0, 1.0, 38846.10037409302], 
processed observation next is [0.0, 0.08695652173913043, 0.3102493074792244, 0.74, 0.0, 0.0, 0.6666666666666666, 0.530301014413285, 0.5442289505241598, 0.0, 1.0, 0.18498143035282388], 
reward next is 0.8150, 
noisyNet noise sample is [array([-0.42093512], dtype=float32), -1.2622147]. 
=============================================
[2019-04-04 11:41:19,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.62245]
 [77.67159]
 [77.67473]
 [77.68875]
 [77.73091]], R is [[77.61301422]
 [77.65235138]
 [77.69182587]
 [77.73147583]
 [77.77143097]].
[2019-04-04 11:41:21,122] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302000, global step 4834268: loss 0.0735
[2019-04-04 11:41:21,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302000, global step 4834269: learning rate 0.0000
[2019-04-04 11:41:21,212] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302000, global step 4834308: loss 0.0708
[2019-04-04 11:41:21,215] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302000, global step 4834309: learning rate 0.0000
[2019-04-04 11:41:21,460] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302000, global step 4834402: loss 0.0598
[2019-04-04 11:41:21,463] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302000, global step 4834403: learning rate 0.0000
[2019-04-04 11:41:23,339] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302000, global step 4835010: loss 0.0709
[2019-04-04 11:41:23,340] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302000, global step 4835010: learning rate 0.0000
[2019-04-04 11:41:23,923] A3C_AGENT_WORKER-Thread-2 INFO:Local step 302500, global step 4835205: loss 0.0467
[2019-04-04 11:41:23,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 302500, global step 4835205: learning rate 0.0000
[2019-04-04 11:41:24,034] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302000, global step 4835243: loss 0.0736
[2019-04-04 11:41:24,035] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302000, global step 4835243: learning rate 0.0000
[2019-04-04 11:41:25,076] A3C_AGENT_WORKER-Thread-5 INFO:Local step 302500, global step 4835602: loss 0.0500
[2019-04-04 11:41:25,078] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 302500, global step 4835602: learning rate 0.0000
[2019-04-04 11:41:25,343] A3C_AGENT_WORKER-Thread-12 INFO:Local step 302500, global step 4835696: loss 0.0500
[2019-04-04 11:41:25,344] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 302500, global step 4835696: learning rate 0.0000
[2019-04-04 11:41:25,376] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4099410e-15 1.2850936e-15 4.0059082e-25 5.5553357e-17 2.3456833e-16
 2.1237821e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:25,382] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3477
[2019-04-04 11:41:25,453] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 181.0, 691.0, 26.0, 25.04818707354564, 0.3923796391365241, 0.0, 1.0, 28298.04132218865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2982600.0000, 
sim time next is 2983200.0000, 
raw observation next is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 26.0, 25.05054027155332, 0.3961856520499302, 0.0, 1.0, 30595.95339748564], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.563888888888889, 0.783609576427256, 0.6666666666666666, 0.5875450226294433, 0.6320618840166434, 0.0, 1.0, 0.14569501617850303], 
reward next is 0.8543, 
noisyNet noise sample is [array([0.12058985], dtype=float32), 0.2346145]. 
=============================================
[2019-04-04 11:41:28,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9723857e-17 6.9724221e-16 3.9969767e-26 1.4377097e-17 2.2398541e-17
 2.1394269e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:28,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4502
[2019-04-04 11:41:28,647] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 93.0, 95.66666666666666, 130.0, 26.0, 25.27218574100274, 0.3249576294998061, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2882400.0000, 
sim time next is 2883000.0000, 
raw observation next is [1.166666666666667, 93.0, 86.33333333333334, 104.0, 26.0, 25.38223872261973, 0.3167644610154963, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.34782608695652173, 0.49492151431209613, 0.93, 0.2877777777777778, 0.11491712707182321, 0.6666666666666666, 0.6151865602183108, 0.605588153671832, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.03456172], dtype=float32), 1.7246375]. 
=============================================
[2019-04-04 11:41:28,674] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[84.98415]
 [86.17922]
 [86.17932]
 [86.02528]
 [85.94625]], R is [[83.86103821]
 [84.02243042]
 [84.1822052 ]
 [83.91742706]
 [83.98912048]].
[2019-04-04 11:41:29,066] A3C_AGENT_WORKER-Thread-18 INFO:Local step 302500, global step 4837054: loss 0.0571
[2019-04-04 11:41:29,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 302500, global step 4837054: learning rate 0.0000
[2019-04-04 11:41:29,449] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9268416e-15 2.5231666e-14 1.7073490e-24 6.6185280e-16 2.1246373e-15
 3.6923862e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:29,449] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2473
[2019-04-04 11:41:29,471] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 75.83333333333334, 0.0, 0.0, 26.0, 23.88464341453021, 0.02359580570904125, 0.0, 1.0, 40180.42283557937], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3042600.0000, 
sim time next is 3043200.0000, 
raw observation next is [-6.0, 74.66666666666667, 0.0, 0.0, 26.0, 23.85988515543255, 0.02146623851106124, 0.0, 1.0, 40163.87637566785], 
processed observation next is [0.0, 0.21739130434782608, 0.296398891966759, 0.7466666666666667, 0.0, 0.0, 0.6666666666666666, 0.48832376295271257, 0.5071554128370205, 0.0, 1.0, 0.1912565541698469], 
reward next is 0.8087, 
noisyNet noise sample is [array([-2.0145533], dtype=float32), 0.2748259]. 
=============================================
[2019-04-04 11:41:31,334] A3C_AGENT_WORKER-Thread-6 INFO:Local step 302500, global step 4837939: loss 0.0681
[2019-04-04 11:41:31,335] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 302500, global step 4837939: learning rate 0.0000
[2019-04-04 11:41:32,575] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302000, global step 4838407: loss 0.0734
[2019-04-04 11:41:32,576] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302000, global step 4838407: learning rate 0.0000
[2019-04-04 11:41:32,945] A3C_AGENT_WORKER-Thread-20 INFO:Local step 302500, global step 4838550: loss 0.0804
[2019-04-04 11:41:32,947] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 302500, global step 4838551: learning rate 0.0000
[2019-04-04 11:41:34,215] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.9456690e-17 3.4064714e-16 1.0631417e-25 1.9072588e-17 6.6447584e-17
 3.5414484e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:34,218] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0972
[2019-04-04 11:41:34,262] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 65.0, 181.0, 691.0, 26.0, 25.04818707354564, 0.3923796391365241, 0.0, 1.0, 28298.04132218865], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2982600.0000, 
sim time next is 2983200.0000, 
raw observation next is [-3.0, 65.0, 169.1666666666667, 709.1666666666667, 26.0, 25.05054027155332, 0.3961856520499302, 0.0, 1.0, 30595.95339748564], 
processed observation next is [0.0, 0.5217391304347826, 0.3795013850415513, 0.65, 0.563888888888889, 0.783609576427256, 0.6666666666666666, 0.5875450226294433, 0.6320618840166434, 0.0, 1.0, 0.14569501617850303], 
reward next is 0.8543, 
noisyNet noise sample is [array([1.2755207], dtype=float32), -0.49647045]. 
=============================================
[2019-04-04 11:41:34,357] A3C_AGENT_WORKER-Thread-16 INFO:Local step 302500, global step 4839108: loss 0.0864
[2019-04-04 11:41:34,358] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 302500, global step 4839109: learning rate 0.0000
[2019-04-04 11:41:37,758] A3C_AGENT_WORKER-Thread-13 INFO:Local step 302500, global step 4840378: loss 0.1266
[2019-04-04 11:41:37,759] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 302500, global step 4840378: learning rate 0.0000
[2019-04-04 11:41:38,045] A3C_AGENT_WORKER-Thread-4 INFO:Local step 302500, global step 4840493: loss 0.1334
[2019-04-04 11:41:38,046] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 302500, global step 4840493: learning rate 0.0000
[2019-04-04 11:41:41,647] A3C_AGENT_WORKER-Thread-10 INFO:Local step 302500, global step 4841919: loss 0.1821
[2019-04-04 11:41:41,648] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 302500, global step 4841919: learning rate 0.0000
[2019-04-04 11:41:43,258] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303000, global step 4842559: loss 0.0002
[2019-04-04 11:41:43,258] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303000, global step 4842559: learning rate 0.0000
[2019-04-04 11:41:43,564] A3C_AGENT_WORKER-Thread-3 INFO:Local step 302500, global step 4842704: loss 0.1864
[2019-04-04 11:41:43,565] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 302500, global step 4842704: learning rate 0.0000
[2019-04-04 11:41:43,763] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303000, global step 4842790: loss 0.0003
[2019-04-04 11:41:43,769] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303000, global step 4842790: learning rate 0.0000
[2019-04-04 11:41:43,982] A3C_AGENT_WORKER-Thread-11 INFO:Local step 302500, global step 4842878: loss 0.2089
[2019-04-04 11:41:43,989] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 302500, global step 4842879: learning rate 0.0000
[2019-04-04 11:41:44,447] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303000, global step 4843051: loss 0.0006
[2019-04-04 11:41:44,449] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303000, global step 4843051: learning rate 0.0000
[2019-04-04 11:41:44,503] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1289439e-16 2.7357688e-15 1.4083265e-25 4.9599212e-17 3.1080468e-16
 3.0033646e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:44,504] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1750
[2019-04-04 11:41:44,527] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86367884426795, 0.3370372271014519, 0.0, 1.0, 43350.77912403444], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2939400.0000, 
sim time next is 2940000.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86028124116183, 0.3312413121638043, 0.0, 1.0, 43336.77042553438], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5716901034301524, 0.6104137707212681, 0.0, 1.0, 0.20636557345492562], 
reward next is 0.7936, 
noisyNet noise sample is [array([2.6270523], dtype=float32), -0.12206618]. 
=============================================
[2019-04-04 11:41:44,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[81.699875]
 [81.7457  ]
 [81.751816]
 [81.79528 ]
 [81.90342 ]], R is [[81.67783356]
 [81.65462494]
 [81.63162231]
 [81.60886383]
 [81.58636475]].
[2019-04-04 11:41:44,563] A3C_AGENT_WORKER-Thread-19 INFO:Local step 302500, global step 4843103: loss 0.2118
[2019-04-04 11:41:44,564] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 302500, global step 4843103: learning rate 0.0000
[2019-04-04 11:41:45,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 302500, global step 4843590: loss 0.2270
[2019-04-04 11:41:45,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 302500, global step 4843590: learning rate 0.0000
[2019-04-04 11:41:46,421] A3C_AGENT_WORKER-Thread-17 INFO:Local step 302500, global step 4843846: loss 0.2302
[2019-04-04 11:41:46,424] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 302500, global step 4843846: learning rate 0.0000
[2019-04-04 11:41:47,841] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303000, global step 4844471: loss 0.0001
[2019-04-04 11:41:47,842] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303000, global step 4844471: learning rate 0.0000
[2019-04-04 11:41:49,407] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2125350e-16 8.4299446e-15 1.8725611e-24 6.1313241e-17 2.2418384e-16
 2.7310621e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:49,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8079
[2019-04-04 11:41:49,419] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.333333333333333, 100.0, 0.0, 0.0, 26.0, 26.05818267898679, 0.7036096481318066, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3181200.0000, 
sim time next is 3181800.0000, 
raw observation next is [3.166666666666667, 100.0, 0.0, 0.0, 26.0, 25.93355090874136, 0.6830744519518244, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.5503231763619576, 1.0, 0.0, 0.0, 0.6666666666666666, 0.6611292423951133, 0.7276914839839415, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5611], dtype=float32), -2.0364451]. 
=============================================
[2019-04-04 11:41:49,872] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303000, global step 4845374: loss 0.0001
[2019-04-04 11:41:49,875] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303000, global step 4845375: learning rate 0.0000
[2019-04-04 11:41:51,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.8005517e-18 1.9224303e-17 3.7174411e-27 2.0112638e-18 1.3138015e-18
 8.0224645e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:51,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9588
[2019-04-04 11:41:51,352] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3333333333333334, 63.66666666666667, 100.6666666666667, 686.6666666666667, 26.0, 25.85387625279866, 0.5189949064705648, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3490800.0000, 
sim time next is 3491400.0000, 
raw observation next is [-0.1666666666666666, 61.83333333333333, 102.3333333333333, 703.3333333333334, 26.0, 26.01452750198677, 0.5421911438660643, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.391304347826087, 0.4579870729455217, 0.6183333333333333, 0.341111111111111, 0.7771639042357275, 0.6666666666666666, 0.6678772918322308, 0.6807303812886881, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.5119856], dtype=float32), 1.0025325]. 
=============================================
[2019-04-04 11:41:51,548] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303000, global step 4846153: loss 0.0006
[2019-04-04 11:41:51,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303000, global step 4846154: learning rate 0.0000
[2019-04-04 11:41:52,495] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303000, global step 4846569: loss 0.0003
[2019-04-04 11:41:52,497] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303000, global step 4846569: learning rate 0.0000
[2019-04-04 11:41:55,180] A3C_AGENT_WORKER-Thread-14 INFO:Local step 302500, global step 4847719: loss 0.2615
[2019-04-04 11:41:55,180] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 302500, global step 4847719: learning rate 0.0000
[2019-04-04 11:41:55,556] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303000, global step 4847880: loss 0.0016
[2019-04-04 11:41:55,556] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303000, global step 4847880: learning rate 0.0000
[2019-04-04 11:41:56,325] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303000, global step 4848189: loss 0.0013
[2019-04-04 11:41:56,327] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303000, global step 4848189: learning rate 0.0000
[2019-04-04 11:41:56,776] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4979290e-16 7.4713055e-16 3.9908063e-26 1.7533465e-17 5.8776172e-17
 2.9551780e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:56,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8716
[2019-04-04 11:41:56,810] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.0, 60.0, 0.0, 0.0, 26.0, 24.76576953807101, 0.224006998512049, 0.0, 1.0, 42771.63906723134], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3391200.0000, 
sim time next is 3391800.0000, 
raw observation next is [-3.0, 60.83333333333333, 0.0, 0.0, 26.0, 24.7220160433251, 0.2177831447910835, 0.0, 1.0, 42843.31372042868], 
processed observation next is [1.0, 0.2608695652173913, 0.3795013850415513, 0.6083333333333333, 0.0, 0.0, 0.6666666666666666, 0.5601680036104252, 0.5725943815970278, 0.0, 1.0, 0.20401577962108897], 
reward next is 0.7960, 
noisyNet noise sample is [array([0.52180433], dtype=float32), -0.33851856]. 
=============================================
[2019-04-04 11:41:56,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2477037e-16 1.2147544e-15 1.8270973e-25 2.7264105e-17 2.9623045e-16
 2.1137517e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:41:56,861] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1142
[2019-04-04 11:41:56,875] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 86.0, 0.0, 0.0, 26.0, 25.62603199400968, 0.4647106498214083, 0.0, 1.0, 26646.07216546434], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3449400.0000, 
sim time next is 3450000.0000, 
raw observation next is [1.0, 86.0, 0.0, 0.0, 26.0, 25.52007861668139, 0.4558429605449961, 0.0, 1.0, 88732.61916906358], 
processed observation next is [1.0, 0.9565217391304348, 0.4903047091412743, 0.86, 0.0, 0.0, 0.6666666666666666, 0.6266732180567823, 0.6519476535149987, 0.0, 1.0, 0.4225362817574456], 
reward next is 0.5775, 
noisyNet noise sample is [array([1.2721202], dtype=float32), 0.72247773]. 
=============================================
[2019-04-04 11:41:56,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[83.04305 ]
 [82.96934 ]
 [82.897766]
 [82.79427 ]
 [82.82631 ]], R is [[83.06077576]
 [83.10328674]
 [83.27225494]
 [83.43952942]
 [83.60513306]].
[2019-04-04 11:42:00,171] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303000, global step 4849886: loss 0.0045
[2019-04-04 11:42:00,174] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303000, global step 4849887: learning rate 0.0000
[2019-04-04 11:42:00,827] A3C_AGENT_WORKER-Thread-2 INFO:Local step 303500, global step 4850196: loss 0.2456
[2019-04-04 11:42:00,828] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 303500, global step 4850196: learning rate 0.0000
[2019-04-04 11:42:01,421] A3C_AGENT_WORKER-Thread-5 INFO:Local step 303500, global step 4850489: loss 0.2442
[2019-04-04 11:42:01,423] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 303500, global step 4850489: learning rate 0.0000
[2019-04-04 11:42:01,672] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303000, global step 4850599: loss 0.0051
[2019-04-04 11:42:01,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303000, global step 4850600: learning rate 0.0000
[2019-04-04 11:42:01,984] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303000, global step 4850750: loss 0.0047
[2019-04-04 11:42:01,989] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303000, global step 4850753: learning rate 0.0000
[2019-04-04 11:42:02,227] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303000, global step 4850872: loss 0.0050
[2019-04-04 11:42:02,227] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303000, global step 4850872: learning rate 0.0000
[2019-04-04 11:42:02,274] A3C_AGENT_WORKER-Thread-12 INFO:Local step 303500, global step 4850898: loss 0.2363
[2019-04-04 11:42:02,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 303500, global step 4850898: learning rate 0.0000
[2019-04-04 11:42:03,859] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303000, global step 4851601: loss 0.0078
[2019-04-04 11:42:03,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303000, global step 4851601: learning rate 0.0000
[2019-04-04 11:42:04,791] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303000, global step 4852030: loss 0.0079
[2019-04-04 11:42:04,791] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303000, global step 4852030: learning rate 0.0000
[2019-04-04 11:42:05,132] A3C_AGENT_WORKER-Thread-18 INFO:Local step 303500, global step 4852204: loss 0.2342
[2019-04-04 11:42:05,133] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 303500, global step 4852204: learning rate 0.0000
[2019-04-04 11:42:07,292] A3C_AGENT_WORKER-Thread-6 INFO:Local step 303500, global step 4853208: loss 0.2274
[2019-04-04 11:42:07,295] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 303500, global step 4853208: learning rate 0.0000
[2019-04-04 11:42:09,137] A3C_AGENT_WORKER-Thread-20 INFO:Local step 303500, global step 4854072: loss 0.2395
[2019-04-04 11:42:09,157] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 303500, global step 4854074: learning rate 0.0000
[2019-04-04 11:42:10,264] A3C_AGENT_WORKER-Thread-16 INFO:Local step 303500, global step 4854611: loss 0.2380
[2019-04-04 11:42:10,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 303500, global step 4854612: learning rate 0.0000
[2019-04-04 11:42:10,405] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.1255933e-16 7.5961195e-15 8.6725433e-26 1.4573347e-16 2.8693903e-16
 2.3735664e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:10,407] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7537
[2019-04-04 11:42:10,442] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.5, 67.5, 0.0, 0.0, 26.0, 24.74656920981397, 0.291798522973665, 0.0, 1.0, 40879.51138366049], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3562200.0000, 
sim time next is 3562800.0000, 
raw observation next is [-5.666666666666667, 68.33333333333333, 0.0, 0.0, 26.0, 24.75281485937699, 0.284219640077176, 0.0, 1.0, 40860.18113321579], 
processed observation next is [0.0, 0.21739130434782608, 0.30563250230840255, 0.6833333333333332, 0.0, 0.0, 0.6666666666666666, 0.5627345716147492, 0.5947398800257253, 0.0, 1.0, 0.19457229111055138], 
reward next is 0.8054, 
noisyNet noise sample is [array([-0.56682026], dtype=float32), -0.25404727]. 
=============================================
[2019-04-04 11:42:12,804] A3C_AGENT_WORKER-Thread-4 INFO:Local step 303500, global step 4855883: loss 0.2677
[2019-04-04 11:42:12,808] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 303500, global step 4855884: learning rate 0.0000
[2019-04-04 11:42:12,952] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303000, global step 4855954: loss 0.0016
[2019-04-04 11:42:12,962] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303000, global step 4855957: learning rate 0.0000
[2019-04-04 11:42:13,072] A3C_AGENT_WORKER-Thread-13 INFO:Local step 303500, global step 4856006: loss 0.2501
[2019-04-04 11:42:13,090] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 303500, global step 4856006: learning rate 0.0000
[2019-04-04 11:42:17,161] A3C_AGENT_WORKER-Thread-10 INFO:Local step 303500, global step 4857831: loss 0.2129
[2019-04-04 11:42:17,162] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 303500, global step 4857831: learning rate 0.0000
[2019-04-04 11:42:18,279] A3C_AGENT_WORKER-Thread-3 INFO:Local step 303500, global step 4858372: loss 0.2239
[2019-04-04 11:42:18,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 303500, global step 4858372: learning rate 0.0000
[2019-04-04 11:42:18,544] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304000, global step 4858511: loss 0.0101
[2019-04-04 11:42:18,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304000, global step 4858512: learning rate 0.0000
[2019-04-04 11:42:18,733] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304000, global step 4858611: loss 0.0191
[2019-04-04 11:42:18,735] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304000, global step 4858611: learning rate 0.0000
[2019-04-04 11:42:19,131] A3C_AGENT_WORKER-Thread-19 INFO:Local step 303500, global step 4858824: loss 0.2136
[2019-04-04 11:42:19,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 303500, global step 4858824: learning rate 0.0000
[2019-04-04 11:42:19,208] A3C_AGENT_WORKER-Thread-11 INFO:Local step 303500, global step 4858862: loss 0.2038
[2019-04-04 11:42:19,209] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 303500, global step 4858862: learning rate 0.0000
[2019-04-04 11:42:19,604] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304000, global step 4859059: loss 0.0186
[2019-04-04 11:42:19,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304000, global step 4859059: learning rate 0.0000
[2019-04-04 11:42:20,789] A3C_AGENT_WORKER-Thread-15 INFO:Local step 303500, global step 4859586: loss 0.2304
[2019-04-04 11:42:20,790] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 303500, global step 4859586: learning rate 0.0000
[2019-04-04 11:42:21,140] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6055706e-17 2.6348889e-16 8.6725057e-27 1.9199741e-18 9.7725889e-18
 2.9125811e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:21,141] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7470
[2019-04-04 11:42:21,152] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 67.0, 115.6666666666667, 823.1666666666666, 26.0, 26.46200068963748, 0.5943524528362522, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3757200.0000, 
sim time next is 3757800.0000, 
raw observation next is [-2.166666666666667, 66.0, 116.3333333333333, 824.3333333333334, 26.0, 26.49326282704165, 0.5943016467144335, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.4025854108956602, 0.66, 0.38777777777777767, 0.910865561694291, 0.6666666666666666, 0.7077719022534709, 0.6981005489048112, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3600297], dtype=float32), -0.9726811]. 
=============================================
[2019-04-04 11:42:21,913] A3C_AGENT_WORKER-Thread-17 INFO:Local step 303500, global step 4860089: loss 0.2162
[2019-04-04 11:42:21,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 303500, global step 4860089: learning rate 0.0000
[2019-04-04 11:42:22,344] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304000, global step 4860301: loss 0.0364
[2019-04-04 11:42:22,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304000, global step 4860302: learning rate 0.0000
[2019-04-04 11:42:22,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4840315e-18 2.6494114e-17 1.8923346e-27 4.7437019e-19 1.0928504e-18
 1.9896127e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:22,931] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0624
[2019-04-04 11:42:22,948] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 20.0, 96.5, 753.0, 26.0, 26.72075497273509, 0.7203018289818702, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4028400.0000, 
sim time next is 4029000.0000, 
raw observation next is [-1.833333333333333, 20.33333333333334, 94.0, 739.3333333333334, 26.0, 26.88919388687148, 0.7483360577144825, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6521739130434783, 0.41181902123730385, 0.2033333333333334, 0.31333333333333335, 0.8169429097605894, 0.6666666666666666, 0.7407661572392902, 0.7494453525714942, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.14103267], dtype=float32), 0.9628439]. 
=============================================
[2019-04-04 11:42:22,953] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[88.5558  ]
 [88.59068 ]
 [88.83099 ]
 [89.22479 ]
 [89.452545]], R is [[88.59080505]
 [88.70489502]
 [88.81784821]
 [88.92967224]
 [89.04037476]].
[2019-04-04 11:42:24,551] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304000, global step 4861308: loss 0.0190
[2019-04-04 11:42:24,557] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304000, global step 4861310: learning rate 0.0000
[2019-04-04 11:42:25,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1638698e-17 4.6383667e-16 1.1243582e-26 3.1264993e-18 8.3715749e-18
 3.5745502e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:25,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4815
[2019-04-04 11:42:25,750] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.0, 42.0, 106.0, 796.0, 26.0, 25.19040879022298, 0.4514794822520629, 0.0, 1.0, 18695.61459197597], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3593400.0000, 
sim time next is 3594000.0000, 
raw observation next is [-1.0, 42.0, 104.0, 792.0, 26.0, 25.19219606046752, 0.4526946226029807, 0.0, 1.0, 18696.13212588566], 
processed observation next is [0.0, 0.6086956521739131, 0.4349030470914128, 0.42, 0.3466666666666667, 0.8751381215469614, 0.6666666666666666, 0.5993496717056267, 0.6508982075343269, 0.0, 1.0, 0.08902920059945552], 
reward next is 0.9110, 
noisyNet noise sample is [array([-0.42972615], dtype=float32), -0.5022408]. 
=============================================
[2019-04-04 11:42:25,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[85.58296]
 [85.64847]
 [85.63895]
 [85.64805]
 [85.69562]], R is [[85.58180237]
 [85.63695526]
 [85.69155884]
 [85.74562073]
 [85.88816833]].
[2019-04-04 11:42:27,082] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304000, global step 4862313: loss 0.0299
[2019-04-04 11:42:27,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304000, global step 4862313: learning rate 0.0000
[2019-04-04 11:42:27,936] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304000, global step 4862710: loss 0.0328
[2019-04-04 11:42:27,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304000, global step 4862710: learning rate 0.0000
[2019-04-04 11:42:30,158] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304000, global step 4863709: loss 0.0451
[2019-04-04 11:42:30,167] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304000, global step 4863715: learning rate 0.0000
[2019-04-04 11:42:30,301] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304000, global step 4863774: loss 0.0427
[2019-04-04 11:42:30,306] A3C_AGENT_WORKER-Thread-14 INFO:Local step 303500, global step 4863775: loss 0.2514
[2019-04-04 11:42:30,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304000, global step 4863775: learning rate 0.0000
[2019-04-04 11:42:30,308] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 303500, global step 4863775: learning rate 0.0000
[2019-04-04 11:42:35,081] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304000, global step 4865845: loss 0.0152
[2019-04-04 11:42:35,083] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4198431e-16 2.1821936e-15 8.1375649e-26 4.1888625e-17 9.5296937e-17
 2.4472573e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:35,088] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.9529
[2019-04-04 11:42:35,090] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304000, global step 4865846: learning rate 0.0000
[2019-04-04 11:42:35,118] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [3.0, 45.0, 0.0, 0.0, 26.0, 25.4132140626375, 0.3219714102994023, 0.0, 1.0, 54281.35811103373], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4249200.0000, 
sim time next is 4249800.0000, 
raw observation next is [3.0, 45.0, 0.0, 0.0, 26.0, 25.36043118715667, 0.3203615385194852, 0.0, 1.0, 69382.40589490208], 
processed observation next is [0.0, 0.17391304347826086, 0.5457063711911359, 0.45, 0.0, 0.0, 0.6666666666666666, 0.6133692655963893, 0.6067871795064951, 0.0, 1.0, 0.33039240902334327], 
reward next is 0.6696, 
noisyNet noise sample is [array([0.9377426], dtype=float32), 0.31757447]. 
=============================================
[2019-04-04 11:42:35,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3636090e-15 1.7950586e-14 6.8214799e-26 1.2660723e-16 2.0557282e-16
 1.4933734e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:35,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4374
[2019-04-04 11:42:35,443] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.333333333333333, 47.0, 0.0, 0.0, 26.0, 25.38952141633659, 0.3462614696184534, 0.0, 1.0, 46967.06205429121], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4238400.0000, 
sim time next is 4239000.0000, 
raw observation next is [2.5, 46.5, 0.0, 0.0, 26.0, 25.3994941135691, 0.3476176788052936, 0.0, 1.0, 36757.02658301219], 
processed observation next is [0.0, 0.043478260869565216, 0.5318559556786704, 0.465, 0.0, 0.0, 0.6666666666666666, 0.6166245094640918, 0.6158725596017646, 0.0, 1.0, 0.17503345991910568], 
reward next is 0.8250, 
noisyNet noise sample is [array([-1.6950158], dtype=float32), -1.7831502]. 
=============================================
[2019-04-04 11:42:35,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.39371 ]
 [85.334114]
 [85.24312 ]
 [85.24431 ]
 [85.24769 ]], R is [[85.39884186]
 [85.32119751]
 [85.19683838]
 [85.1580658 ]
 [85.15248871]].
[2019-04-04 11:42:36,197] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304000, global step 4866327: loss 0.0185
[2019-04-04 11:42:36,197] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304000, global step 4866327: learning rate 0.0000
[2019-04-04 11:42:36,828] A3C_AGENT_WORKER-Thread-2 INFO:Local step 304500, global step 4866618: loss 1.0781
[2019-04-04 11:42:36,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 304500, global step 4866618: learning rate 0.0000
[2019-04-04 11:42:36,841] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304000, global step 4866623: loss 0.0175
[2019-04-04 11:42:36,841] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304000, global step 4866623: learning rate 0.0000
[2019-04-04 11:42:37,109] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304000, global step 4866729: loss 0.0249
[2019-04-04 11:42:37,109] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304000, global step 4866729: learning rate 0.0000
[2019-04-04 11:42:37,138] A3C_AGENT_WORKER-Thread-5 INFO:Local step 304500, global step 4866745: loss 1.0622
[2019-04-04 11:42:37,151] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 304500, global step 4866746: learning rate 0.0000
[2019-04-04 11:42:38,086] A3C_AGENT_WORKER-Thread-12 INFO:Local step 304500, global step 4867173: loss 1.1362
[2019-04-04 11:42:38,095] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 304500, global step 4867173: learning rate 0.0000
[2019-04-04 11:42:38,275] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304000, global step 4867244: loss 0.0154
[2019-04-04 11:42:38,279] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304000, global step 4867247: learning rate 0.0000
[2019-04-04 11:42:39,735] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304000, global step 4867901: loss 0.0262
[2019-04-04 11:42:39,736] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304000, global step 4867901: learning rate 0.0000
[2019-04-04 11:42:40,172] A3C_AGENT_WORKER-Thread-18 INFO:Local step 304500, global step 4868103: loss 1.1437
[2019-04-04 11:42:40,173] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 304500, global step 4868103: learning rate 0.0000
[2019-04-04 11:42:40,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5604672e-16 1.6805468e-15 1.0779083e-25 4.1934192e-17 3.3535676e-17
 9.2228260e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:40,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1812
[2019-04-04 11:42:40,644] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 41.33333333333334, 191.5, 185.6666666666666, 26.0, 25.16148979747748, 0.3665817718249081, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4198800.0000, 
sim time next is 4199400.0000, 
raw observation next is [2.0, 42.0, 187.0, 89.0, 26.0, 25.09487207614589, 0.3502308564863266, 0.0, 1.0, 34114.9352457493], 
processed observation next is [0.0, 0.6086956521739131, 0.518005540166205, 0.42, 0.6233333333333333, 0.09834254143646409, 0.6666666666666666, 0.5912393396788241, 0.6167436188287755, 0.0, 1.0, 0.1624520725988062], 
reward next is 0.8375, 
noisyNet noise sample is [array([-0.7192552], dtype=float32), -0.22652881]. 
=============================================
[2019-04-04 11:42:42,918] A3C_AGENT_WORKER-Thread-6 INFO:Local step 304500, global step 4869427: loss 1.1791
[2019-04-04 11:42:42,922] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 304500, global step 4869427: learning rate 0.0000
[2019-04-04 11:42:45,193] A3C_AGENT_WORKER-Thread-20 INFO:Local step 304500, global step 4870523: loss 1.1547
[2019-04-04 11:42:45,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 304500, global step 4870523: learning rate 0.0000
[2019-04-04 11:42:45,694] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [3.2394733e-16 2.2959664e-15 1.1364234e-26 6.9434613e-17 9.3721482e-17
 3.5592716e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:45,694] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.9823
[2019-04-04 11:42:45,708] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42819013031823, 0.353650259967738, 0.0, 1.0, 27776.53517172558], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4236000.0000, 
sim time next is 4236600.0000, 
raw observation next is [2.0, 48.0, 0.0, 0.0, 26.0, 25.42894863510603, 0.3497558857824769, 0.0, 1.0, 32339.73601528629], 
processed observation next is [0.0, 0.0, 0.518005540166205, 0.48, 0.0, 0.0, 0.6666666666666666, 0.6190790529255024, 0.6165852952608256, 0.0, 1.0, 0.1539987429299347], 
reward next is 0.8460, 
noisyNet noise sample is [array([-0.6912513], dtype=float32), 1.0091542]. 
=============================================
[2019-04-04 11:42:46,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.0291785e-17 1.2140688e-15 4.7135613e-26 2.4845281e-17 1.9876660e-17
 1.8813800e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:46,341] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2114
[2019-04-04 11:42:46,349] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 42.0, 0.0, 0.0, 26.0, 25.53008891434226, 0.4933166659938813, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4132200.0000, 
sim time next is 4132800.0000, 
raw observation next is [1.0, 43.0, 0.0, 0.0, 26.0, 25.47821166648921, 0.4801128622134125, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8695652173913043, 0.4903047091412743, 0.43, 0.0, 0.0, 0.6666666666666666, 0.6231843055407674, 0.6600376207378041, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.9495272], dtype=float32), -0.51415694]. 
=============================================
[2019-04-04 11:42:46,369] A3C_AGENT_WORKER-Thread-16 INFO:Local step 304500, global step 4871088: loss 1.1935
[2019-04-04 11:42:46,372] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 304500, global step 4871088: learning rate 0.0000
[2019-04-04 11:42:47,570] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304000, global step 4871680: loss 0.0048
[2019-04-04 11:42:47,572] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304000, global step 4871680: learning rate 0.0000
[2019-04-04 11:42:48,214] A3C_AGENT_WORKER-Thread-13 INFO:Local step 304500, global step 4872029: loss 1.1549
[2019-04-04 11:42:48,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 304500, global step 4872030: learning rate 0.0000
[2019-04-04 11:42:48,264] A3C_AGENT_WORKER-Thread-4 INFO:Local step 304500, global step 4872062: loss 1.1407
[2019-04-04 11:42:48,265] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 304500, global step 4872062: learning rate 0.0000
[2019-04-04 11:42:49,239] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.8287065e-16 1.7749639e-15 3.1999452e-26 3.8215612e-17 1.8897338e-16
 3.5782763e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:49,245] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5865
[2019-04-04 11:42:49,258] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [5.05, 73.5, 0.0, 0.0, 26.0, 25.74505424664915, 0.4370453933015619, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4309800.0000, 
sim time next is 4310400.0000, 
raw observation next is [5.0, 74.0, 0.0, 0.0, 26.0, 25.69303509996219, 0.4251844215632814, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.6011080332409973, 0.74, 0.0, 0.0, 0.6666666666666666, 0.6410862583301826, 0.6417281405210938, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7240398], dtype=float32), -0.22837758]. 
=============================================
[2019-04-04 11:42:50,676] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.0321992e-17 3.8023251e-16 1.5378293e-27 8.2755268e-18 2.2120707e-17
 6.3507994e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:50,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0423
[2019-04-04 11:42:50,717] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.333333333333333, 41.66666666666667, 105.3333333333333, 631.3333333333333, 26.0, 25.52410676820093, 0.422973496585226, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4180800.0000, 
sim time next is 4181400.0000, 
raw observation next is [-3.0, 40.0, 108.0, 660.0, 26.0, 25.53769251314317, 0.4222099034391429, 0.0, 1.0, 0.0], 
processed observation next is [0.0, 0.391304347826087, 0.3795013850415513, 0.4, 0.36, 0.7292817679558011, 0.6666666666666666, 0.6281410427619308, 0.6407366344797143, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.6319883], dtype=float32), 0.027295494]. 
=============================================
[2019-04-04 11:42:51,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1397857e-15 2.4788411e-15 2.2297173e-25 8.2358813e-17 7.5882624e-17
 2.3344511e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:51,550] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6280
[2019-04-04 11:42:51,625] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.9666666666666667, 71.33333333333334, 0.0, 0.0, 26.0, 25.19723456878289, 0.3835606140144486, 0.0, 1.0, 40429.94648901503], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [-0.9333333333333333, 71.66666666666667, 0.0, 0.0, 26.0, 25.21055773401685, 0.4304753313418897, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4367497691597415, 0.7166666666666667, 0.0, 0.0, 0.6666666666666666, 0.6008798111680708, 0.6434917771139632, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.0761411], dtype=float32), -0.7349191]. 
=============================================
[2019-04-04 11:42:51,928] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305000, global step 4873931: loss 0.0028
[2019-04-04 11:42:51,931] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305000, global step 4873932: learning rate 0.0000
[2019-04-04 11:42:52,105] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305000, global step 4874031: loss 0.0071
[2019-04-04 11:42:52,108] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305000, global step 4874031: learning rate 0.0000
[2019-04-04 11:42:52,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.0465276e-18 3.9327933e-16 1.7548439e-26 3.8704881e-18 4.3200984e-18
 3.6049519e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:52,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3267
[2019-04-04 11:42:52,658] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.15, 34.5, 81.66666666666667, 0.0, 26.0, 28.80457531143676, 1.144815157405964, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4377000.0000, 
sim time next is 4377600.0000, 
raw observation next is [13.0, 35.0, 71.0, 0.0, 26.0, 28.5015611102056, 0.9442803419856131, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.8227146814404434, 0.35, 0.23666666666666666, 0.0, 0.6666666666666666, 0.8751300925171334, 0.8147601139952044, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.2628436], dtype=float32), -0.41712156]. 
=============================================
[2019-04-04 11:42:52,817] A3C_AGENT_WORKER-Thread-10 INFO:Local step 304500, global step 4874406: loss 1.1803
[2019-04-04 11:42:52,831] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 304500, global step 4874412: learning rate 0.0000
[2019-04-04 11:42:53,330] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305000, global step 4874693: loss 0.0055
[2019-04-04 11:42:53,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305000, global step 4874693: learning rate 0.0000
[2019-04-04 11:42:54,029] A3C_AGENT_WORKER-Thread-3 INFO:Local step 304500, global step 4875036: loss 1.1420
[2019-04-04 11:42:54,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 304500, global step 4875037: learning rate 0.0000
[2019-04-04 11:42:54,503] A3C_AGENT_WORKER-Thread-11 INFO:Local step 304500, global step 4875285: loss 1.1990
[2019-04-04 11:42:54,504] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 304500, global step 4875286: learning rate 0.0000
[2019-04-04 11:42:54,587] A3C_AGENT_WORKER-Thread-19 INFO:Local step 304500, global step 4875332: loss 1.2347
[2019-04-04 11:42:54,589] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 304500, global step 4875332: learning rate 0.0000
[2019-04-04 11:42:55,200] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305000, global step 4875628: loss 0.0108
[2019-04-04 11:42:55,201] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305000, global step 4875628: learning rate 0.0000
[2019-04-04 11:42:56,159] A3C_AGENT_WORKER-Thread-15 INFO:Local step 304500, global step 4876174: loss 1.2044
[2019-04-04 11:42:56,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 304500, global step 4876174: learning rate 0.0000
[2019-04-04 11:42:57,191] A3C_AGENT_WORKER-Thread-17 INFO:Local step 304500, global step 4876708: loss 1.2372
[2019-04-04 11:42:57,191] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 304500, global step 4876708: learning rate 0.0000
[2019-04-04 11:42:57,894] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305000, global step 4877051: loss 0.0195
[2019-04-04 11:42:57,898] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305000, global step 4877052: learning rate 0.0000
[2019-04-04 11:42:59,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6070955e-16 5.6326094e-15 3.4729133e-25 7.0071078e-17 3.0144111e-16
 2.3718467e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:42:59,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0424
[2019-04-04 11:42:59,615] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.43262460177545, 0.4711925099586247, 0.0, 1.0, 18761.68373759752], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4578000.0000, 
sim time next is 4578600.0000, 
raw observation next is [1.0, 61.0, 0.0, 0.0, 26.0, 25.44882421776278, 0.4764550070083706, 0.0, 1.0, 18759.14213959462], 
processed observation next is [1.0, 1.0, 0.4903047091412743, 0.61, 0.0, 0.0, 0.6666666666666666, 0.6207353514802317, 0.6588183356694569, 0.0, 1.0, 0.0893292482837839], 
reward next is 0.9107, 
noisyNet noise sample is [array([0.17578085], dtype=float32), -0.5635792]. 
=============================================
[2019-04-04 11:42:59,891] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305000, global step 4878140: loss 0.0235
[2019-04-04 11:42:59,894] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305000, global step 4878140: learning rate 0.0000
[2019-04-04 11:43:00,830] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305000, global step 4878613: loss 0.0177
[2019-04-04 11:43:00,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305000, global step 4878614: learning rate 0.0000
[2019-04-04 11:43:01,253] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0987795e-18 3.0435154e-17 1.8311983e-27 2.1664973e-18 1.7091071e-18
 2.1598277e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:01,258] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1277
[2019-04-04 11:43:01,329] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.0, 92.0, 208.0, 6.0, 26.0, 26.44882807218339, 0.5934658210484952, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4703400.0000, 
sim time next is 4704000.0000, 
raw observation next is [0.0, 92.0, 208.8333333333333, 6.0, 26.0, 26.46950636015861, 0.5966741291643781, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.46260387811634357, 0.92, 0.6961111111111109, 0.0066298342541436465, 0.6666666666666666, 0.7057921966798842, 0.698891376388126, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.08069181], dtype=float32), -0.17744367]. 
=============================================
[2019-04-04 11:43:01,363] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[89.83674 ]
 [89.867744]
 [89.896355]
 [89.84583 ]
 [89.66359 ]], R is [[89.64728546]
 [89.75081635]
 [89.85330963]
 [89.95478058]
 [90.05523682]].
[2019-04-04 11:43:02,871] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305000, global step 4879677: loss 0.0286
[2019-04-04 11:43:02,872] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305000, global step 4879677: learning rate 0.0000
[2019-04-04 11:43:03,137] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305000, global step 4879809: loss 0.0274
[2019-04-04 11:43:03,148] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305000, global step 4879810: learning rate 0.0000
[2019-04-04 11:43:04,607] A3C_AGENT_WORKER-Thread-14 INFO:Local step 304500, global step 4880527: loss 1.2847
[2019-04-04 11:43:04,609] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 304500, global step 4880527: learning rate 0.0000
[2019-04-04 11:43:05,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3301806e-18 1.8418002e-17 2.1617560e-28 3.3992488e-19 3.0114412e-19
 2.0321917e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:05,147] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6752
[2019-04-04 11:43:05,162] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.666666666666667, 53.33333333333334, 126.6666666666667, 789.0, 26.0, 26.71748024104239, 0.6982243478392177, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4618200.0000, 
sim time next is 4618800.0000, 
raw observation next is [2.0, 52.0, 125.5, 800.0, 26.0, 26.74981740876595, 0.4754606231419446, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.4782608695652174, 0.518005540166205, 0.52, 0.41833333333333333, 0.8839779005524862, 0.6666666666666666, 0.7291514507304958, 0.6584868743806482, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.15718324], dtype=float32), 1.9733007]. 
=============================================
[2019-04-04 11:43:05,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.1238972e-16 7.2311448e-15 1.1101368e-25 5.9874185e-17 8.9227933e-17
 1.4313774e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:05,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5853
[2019-04-04 11:43:05,514] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.0, 58.66666666666666, 0.0, 0.0, 26.0, 25.62326620599688, 0.4820320350479985, 0.0, 1.0, 21528.19719528709], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4671600.0000, 
sim time next is 4672200.0000, 
raw observation next is [2.0, 60.33333333333334, 0.0, 0.0, 26.0, 25.5609890260627, 0.4720204234668229, 0.0, 1.0, 57599.84395202609], 
processed observation next is [1.0, 0.043478260869565216, 0.518005540166205, 0.6033333333333334, 0.0, 0.0, 0.6666666666666666, 0.6300824188385583, 0.6573401411556076, 0.0, 1.0, 0.27428497120012424], 
reward next is 0.7257, 
noisyNet noise sample is [array([0.32789004], dtype=float32), -1.1734935]. 
=============================================
[2019-04-04 11:43:07,642] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305000, global step 4882053: loss 0.0163
[2019-04-04 11:43:07,647] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305000, global step 4882055: learning rate 0.0000
[2019-04-04 11:43:08,386] A3C_AGENT_WORKER-Thread-2 INFO:Local step 305500, global step 4882401: loss 0.0879
[2019-04-04 11:43:08,388] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 305500, global step 4882402: learning rate 0.0000
[2019-04-04 11:43:08,906] A3C_AGENT_WORKER-Thread-5 INFO:Local step 305500, global step 4882633: loss 0.0711
[2019-04-04 11:43:08,908] A3C_AGENT_WORKER-Thread-5 DEBUG:Local step 305500, global step 4882634: learning rate 0.0000
[2019-04-04 11:43:08,949] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305000, global step 4882647: loss 0.0097
[2019-04-04 11:43:08,950] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305000, global step 4882647: learning rate 0.0000
[2019-04-04 11:43:08,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2577542e-16 2.4673693e-15 2.6542157e-25 5.9027152e-17 1.5422322e-16
 1.8536973e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:08,966] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5845
[2019-04-04 11:43:08,981] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 61.0, 0.0, 0.0, 26.0, 25.51153291632533, 0.468982368761643, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4579200.0000, 
sim time next is 4579800.0000, 
raw observation next is [0.9, 61.33333333333334, 0.0, 0.0, 26.0, 25.49057330924003, 0.4553080373708892, 0.0, 1.0, 18752.10902630351], 
processed observation next is [1.0, 0.0, 0.48753462603878117, 0.6133333333333334, 0.0, 0.0, 0.6666666666666666, 0.6242144424366692, 0.6517693457902963, 0.0, 1.0, 0.08929575726811195], 
reward next is 0.9107, 
noisyNet noise sample is [array([1.5199598], dtype=float32), -0.3277026]. 
=============================================
[2019-04-04 11:43:09,007] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.7270435e-17 8.5600467e-16 2.8176026e-27 3.6781188e-17 2.8271885e-17
 2.1362871e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:09,008] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8696
[2019-04-04 11:43:09,102] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 69.0, 117.5, 260.8333333333334, 26.0, 24.40235324840997, 0.2654942244284128, 0.0, 1.0, 202487.9849510309], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4868400.0000, 
sim time next is 4869000.0000, 
raw observation next is [-3.5, 68.0, 141.0, 313.0, 26.0, 24.80509418951284, 0.3384438834282639, 0.0, 1.0, 8346.298220997533], 
processed observation next is [0.0, 0.34782608695652173, 0.36565096952908593, 0.68, 0.47, 0.34585635359116024, 0.6666666666666666, 0.5670911824594033, 0.6128146278094213, 0.0, 1.0, 0.03974427724284539], 
reward next is 0.9603, 
noisyNet noise sample is [array([-0.5217785], dtype=float32), 0.9974783]. 
=============================================
[2019-04-04 11:43:09,110] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[86.83961 ]
 [85.84298 ]
 [85.059135]
 [84.01191 ]
 [82.743324]], R is [[87.66890717]
 [86.8279953 ]
 [86.77362823]
 [86.71875   ]
 [86.66363525]].
[2019-04-04 11:43:09,183] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305000, global step 4882756: loss 0.0092
[2019-04-04 11:43:09,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305000, global step 4882756: learning rate 0.0000
[2019-04-04 11:43:09,500] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305000, global step 4882916: loss 0.0112
[2019-04-04 11:43:09,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305000, global step 4882917: learning rate 0.0000
[2019-04-04 11:43:10,429] A3C_AGENT_WORKER-Thread-12 INFO:Local step 305500, global step 4883401: loss 0.0578
[2019-04-04 11:43:10,432] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 305500, global step 4883403: learning rate 0.0000
[2019-04-04 11:43:11,043] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305000, global step 4883691: loss 0.0122
[2019-04-04 11:43:11,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305000, global step 4883691: learning rate 0.0000
[2019-04-04 11:43:11,299] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2887957e-15 7.3304916e-15 1.6509534e-25 1.7422729e-16 5.8557108e-16
 2.6711372e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:11,301] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5550
[2019-04-04 11:43:11,321] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.066666666666666, 92.33333333333333, 20.66666666666666, 69.83333333333331, 26.0, 23.82486478741811, 0.09276322967466975, 0.0, 1.0, 41895.04283221092], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4779600.0000, 
sim time next is 4780200.0000, 
raw observation next is [-6.033333333333333, 92.16666666666667, 41.33333333333332, 139.6666666666666, 26.0, 23.80477625513047, 0.09882538547008764, 0.0, 1.0, 41792.64482487012], 
processed observation next is [0.0, 0.30434782608695654, 0.29547553093259465, 0.9216666666666667, 0.13777777777777775, 0.1543278084714548, 0.6666666666666666, 0.48373135459420585, 0.5329417951566958, 0.0, 1.0, 0.19901259440414343], 
reward next is 0.8010, 
noisyNet noise sample is [array([-0.64463496], dtype=float32), -1.2264194]. 
=============================================
[2019-04-04 11:43:11,843] A3C_AGENT_WORKER-Thread-18 INFO:Local step 305500, global step 4884084: loss 0.0734
[2019-04-04 11:43:11,843] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 305500, global step 4884084: learning rate 0.0000
[2019-04-04 11:43:12,242] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305000, global step 4884287: loss 0.0100
[2019-04-04 11:43:12,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305000, global step 4884287: learning rate 0.0000
[2019-04-04 11:43:14,776] A3C_AGENT_WORKER-Thread-6 INFO:Local step 305500, global step 4885549: loss 0.0661
[2019-04-04 11:43:14,777] A3C_AGENT_WORKER-Thread-6 DEBUG:Local step 305500, global step 4885550: learning rate 0.0000
[2019-04-04 11:43:16,574] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0524248e-17 9.5800293e-17 2.0649909e-27 6.6843642e-18 5.1898466e-18
 4.1267311e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:16,576] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7343
[2019-04-04 11:43:16,586] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.333333333333333, 59.66666666666667, 204.6666666666667, 15.0, 26.0, 26.17621292804095, 0.5427284043271279, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 4530000.0000, 
sim time next is 4530600.0000, 
raw observation next is [1.5, 59.0, 221.0, 18.0, 26.0, 26.2050105790231, 0.5507811084114114, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.5041551246537397, 0.59, 0.7366666666666667, 0.019889502762430938, 0.6666666666666666, 0.6837508815852583, 0.6835937028038037, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.3037923], dtype=float32), 0.3492643]. 
=============================================
[2019-04-04 11:43:16,769] A3C_AGENT_WORKER-Thread-20 INFO:Local step 305500, global step 4886588: loss 0.0557
[2019-04-04 11:43:16,772] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 305500, global step 4886588: learning rate 0.0000
[2019-04-04 11:43:17,428] A3C_AGENT_WORKER-Thread-16 INFO:Local step 305500, global step 4886938: loss 0.0650
[2019-04-04 11:43:17,431] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 305500, global step 4886939: learning rate 0.0000
[2019-04-04 11:43:18,942] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305000, global step 4887743: loss 0.0099
[2019-04-04 11:43:18,943] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305000, global step 4887744: learning rate 0.0000
[2019-04-04 11:43:19,260] A3C_AGENT_WORKER-Thread-4 INFO:Local step 305500, global step 4887915: loss 0.0385
[2019-04-04 11:43:19,277] A3C_AGENT_WORKER-Thread-4 DEBUG:Local step 305500, global step 4887915: learning rate 0.0000
[2019-04-04 11:43:19,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:19,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:19,756] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res2/Eplus-env-sub_run37
[2019-04-04 11:43:19,910] A3C_AGENT_WORKER-Thread-13 INFO:Local step 305500, global step 4888220: loss 0.0377
[2019-04-04 11:43:19,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 305500, global step 4888220: learning rate 0.0000
[2019-04-04 11:43:20,314] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:20,315] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:20,329] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-5_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res5/Eplus-env-sub_run37
[2019-04-04 11:43:21,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:21,253] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:21,294] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res9/Eplus-env-sub_run37
[2019-04-04 11:43:24,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:24,060] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:24,064] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res15/Eplus-env-sub_run37
[2019-04-04 11:43:25,201] A3C_AGENT_WORKER-Thread-10 INFO:Local step 305500, global step 4889790: loss 0.0265
[2019-04-04 11:43:25,202] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 305500, global step 4889790: learning rate 0.0000
[2019-04-04 11:43:27,337] A3C_AGENT_WORKER-Thread-3 INFO:Local step 305500, global step 4890332: loss 0.0197
[2019-04-04 11:43:27,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 305500, global step 4890332: learning rate 0.0000
[2019-04-04 11:43:27,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:27,598] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:27,673] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-6_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res6/Eplus-env-sub_run37
[2019-04-04 11:43:27,810] A3C_AGENT_WORKER-Thread-11 INFO:Local step 305500, global step 4890455: loss 0.0246
[2019-04-04 11:43:27,811] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 305500, global step 4890455: learning rate 0.0000
[2019-04-04 11:43:28,144] A3C_AGENT_WORKER-Thread-19 INFO:Local step 305500, global step 4890549: loss 0.0195
[2019-04-04 11:43:28,146] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 305500, global step 4890549: learning rate 0.0000
[2019-04-04 11:43:29,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 305500, global step 4891000: loss 0.0209
[2019-04-04 11:43:29,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 305500, global step 4891000: learning rate 0.0000
[2019-04-04 11:43:31,785] A3C_AGENT_WORKER-Thread-17 INFO:Local step 305500, global step 4891472: loss 0.0158
[2019-04-04 11:43:31,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 305500, global step 4891472: learning rate 0.0000
[2019-04-04 11:43:32,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:32,029] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:32,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res17/Eplus-env-sub_run37
[2019-04-04 11:43:32,757] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:32,758] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:32,761] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res13/Eplus-env-sub_run37
[2019-04-04 11:43:36,370] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:36,371] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:36,399] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-4_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res4/Eplus-env-sub_run37
[2019-04-04 11:43:37,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:37,397] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:37,400] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res10/Eplus-env-sub_run37
[2019-04-04 11:43:44,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:44,217] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:44,263] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res7/Eplus-env-sub_run37
[2019-04-04 11:43:45,472] A3C_AGENT_WORKER-Thread-14 INFO:Local step 305500, global step 4894564: loss 0.0202
[2019-04-04 11:43:45,495] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 305500, global step 4894564: learning rate 0.0000
[2019-04-04 11:43:46,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:46,112] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:46,116] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res3/Eplus-env-sub_run37
[2019-04-04 11:43:46,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:46,577] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:46,580] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res16/Eplus-env-sub_run37
[2019-04-04 11:43:46,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:46,745] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:46,748] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res8/Eplus-env-sub_run37
[2019-04-04 11:43:48,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0008396e-15 2.5084053e-15 2.5281901e-26 4.6113890e-17 7.1533838e-17
 8.1069506e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:48,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0967
[2019-04-04 11:43:48,172] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [7.7, 93.0, 35.16666666666666, 0.0, 26.0, 23.09922377279745, -0.167245102879706, 0.0, 1.0, 58869.7088830855], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 33600.0000, 
sim time next is 34200.0000, 
raw observation next is [7.7, 93.0, 38.0, 0.0, 26.0, 23.19193762870782, -0.144888404966822, 0.0, 1.0, 58605.64452457494], 
processed observation next is [0.0, 0.391304347826087, 0.6759002770083103, 0.93, 0.12666666666666668, 0.0, 0.6666666666666666, 0.43266146905898495, 0.45170386501105936, 0.0, 1.0, 0.2790744977360711], 
reward next is 0.7209, 
noisyNet noise sample is [array([0.4668511], dtype=float32), 0.2765999]. 
=============================================
[2019-04-04 11:43:49,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:49,717] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:49,720] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res12/Eplus-env-sub_run37
[2019-04-04 11:43:50,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2613025e-16 5.9068660e-15 1.1113831e-24 6.9445737e-17 4.1647260e-16
 1.6522130e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:43:50,897] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0883
[2019-04-04 11:43:50,960] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.25, 89.0, 0.0, 0.0, 26.0, 24.23082506461778, 0.1113484315665553, 0.0, 1.0, 42428.88192590918], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 95400.0000, 
sim time next is 96000.0000, 
raw observation next is [-2.433333333333333, 88.33333333333334, 0.0, 0.0, 26.0, 24.19240022771296, 0.09729269250240746, 0.0, 1.0, 42594.51462153073], 
processed observation next is [1.0, 0.08695652173913043, 0.3951985226223454, 0.8833333333333334, 0.0, 0.0, 0.6666666666666666, 0.5160333523094133, 0.5324308975008024, 0.0, 1.0, 0.2028310220072892], 
reward next is 0.7972, 
noisyNet noise sample is [array([0.7438934], dtype=float32), -0.87468284]. 
=============================================
[2019-04-04 11:43:51,056] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[77.60375 ]
 [77.68925 ]
 [77.78675 ]
 [77.899994]
 [78.03049 ]], R is [[77.5322113 ]
 [77.55484772]
 [77.57794189]
 [77.60157776]
 [77.62569427]].
[2019-04-04 11:43:53,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:43:53,139] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:43:53,190] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res14/Eplus-env-sub_run37
[2019-04-04 11:44:06,847] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-04 11:44:06,848] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:44:06,896] EPLUS_ENV_Part4-Light-Pit-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res11/Eplus-env-sub_run37
[2019-04-04 11:44:11,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4689105e-16 4.0214541e-15 4.7030566e-25 3.8928489e-17 3.7548435e-17
 4.9895199e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:44:11,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0870
[2019-04-04 11:44:11,267] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.3, 61.0, 0.0, 0.0, 26.0, 25.26696233427857, 0.3631959783602204, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 151200.0000, 
sim time next is 151800.0000, 
raw observation next is [-7.383333333333333, 61.5, 0.0, 0.0, 26.0, 25.39275058883189, 0.3596560141248426, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.25807940904893817, 0.615, 0.0, 0.0, 0.6666666666666666, 0.6160625490693242, 0.6198853380416142, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.6307123], dtype=float32), -0.96416855]. 
=============================================
[2019-04-04 11:44:14,708] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-04 11:44:14,714] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:44:14,714] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:44:14,716] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run50
[2019-04-04 11:44:14,794] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:44:14,796] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:44:14,797] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:44:14,799] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:44:14,804] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run50
[2019-04-04 11:44:14,850] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run50
[2019-04-04 11:46:37,901] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15534602], dtype=float32), -0.33030653]
[2019-04-04 11:46:37,901] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [-12.96773092666667, 92.72521393000001, 66.09389015, 525.4708496000001, 26.0, 25.7550766833947, 0.3643567556770078, 1.0, 1.0, 0.0]
[2019-04-04 11:46:37,901] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 11:46:37,902] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.70399832e-17 7.87448812e-17 1.04747755e-26 4.00787908e-18
 3.58564308e-18 3.17225639e-20 1.00000000e+00], sampled 0.8361890836493852
[2019-04-04 11:47:15,494] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15534602], dtype=float32), -0.33030653]
[2019-04-04 11:47:15,494] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-3.0, 65.0, 0.0, 0.0, 26.0, 25.39774539797779, 0.3339450714021434, 0.0, 1.0, 35423.14202899583]
[2019-04-04 11:47:15,494] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:47:15,495] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [2.0176911e-15 6.5672010e-15 5.9515064e-25 1.5241774e-16 3.3294652e-16
 2.0350262e-18 1.0000000e+00], sampled 0.9505176400629164
[2019-04-04 11:47:28,474] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:48:01,477] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:48:05,219] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:48:06,258] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 4900000, evaluation results [4900000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
[2019-04-04 11:48:08,834] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3063122e-15 1.6300983e-14 2.3674824e-24 1.9516997e-16 5.4694818e-16
 3.8234844e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:08,835] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3995
[2019-04-04 11:48:08,890] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.900000000000002, 75.33333333333334, 0.0, 0.0, 26.0, 23.45459818424639, -0.06099688281454933, 0.0, 1.0, 44194.13633280256], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 181200.0000, 
sim time next is 181800.0000, 
raw observation next is [-8.9, 76.0, 0.0, 0.0, 26.0, 23.49439674432379, -0.05578624164162118, 0.0, 1.0, 44127.71623885525], 
processed observation next is [1.0, 0.08695652173913043, 0.21606648199445982, 0.76, 0.0, 0.0, 0.6666666666666666, 0.45786639536031587, 0.4814045861194596, 0.0, 1.0, 0.21013198208978692], 
reward next is 0.7899, 
noisyNet noise sample is [array([-0.7318928], dtype=float32), 0.75255054]. 
=============================================
[2019-04-04 11:48:09,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Policy network output: [3.4714327e-16 2.6880993e-15 2.0610829e-24 1.5367413e-16 5.2723215e-17
 1.6767215e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:09,032] A3C_AGENT_WORKER-Thread-5 DEBUG:Softmax action selection sampled number: 0.6065
[2019-04-04 11:48:09,105] A3C_AGENT_WORKER-Thread-5 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.56189653091911, 0.3671376187320992, 1.0, 1.0, 34523.80684485524], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 411000.0000, 
sim time next is 411600.0000, 
raw observation next is [-9.5, 40.0, 0.0, 0.0, 26.0, 25.60628907505891, 0.3549069539082217, 1.0, 1.0, 41764.94630336553], 
processed observation next is [1.0, 0.782608695652174, 0.1994459833795014, 0.4, 0.0, 0.0, 0.6666666666666666, 0.6338574229215759, 0.6183023179694073, 1.0, 1.0, 0.19888069668269298], 
reward next is 0.8011, 
noisyNet noise sample is [array([0.38549742], dtype=float32), 1.8495895]. 
=============================================
[2019-04-04 11:48:09,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8414125e-16 2.6169161e-14 6.8242464e-24 8.4310616e-17 4.7038858e-16
 4.8079964e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:09,589] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6191
[2019-04-04 11:48:09,644] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.99985502818841, 0.2804744929156408, 0.0, 1.0, 35184.41583394648], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 160200.0000, 
sim time next is 160800.0000, 
raw observation next is [-8.4, 68.0, 0.0, 0.0, 26.0, 24.89836812354203, 0.2591702579782298, 0.0, 1.0, 42252.91766367253], 
processed observation next is [1.0, 0.8695652173913043, 0.2299168975069252, 0.68, 0.0, 0.0, 0.6666666666666666, 0.5748640102951693, 0.5863900859927432, 0.0, 1.0, 0.20120436982701206], 
reward next is 0.7988, 
noisyNet noise sample is [array([0.5215266], dtype=float32), 0.5541693]. 
=============================================
[2019-04-04 11:48:26,611] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0818035e-17 1.4797602e-15 2.5011022e-25 3.2911134e-17 4.6761523e-17
 2.6066028e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:26,611] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9636
[2019-04-04 11:48:26,741] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-0.3, 36.0, 94.0, 0.0, 26.0, 24.44096525628457, 0.1806223074810993, 1.0, 1.0, 198395.930798331], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 484200.0000, 
sim time next is 484800.0000, 
raw observation next is [-0.2, 36.33333333333333, 87.66666666666667, 0.0, 26.0, 24.9395545108863, 0.2284579148952023, 1.0, 1.0, 23914.7471645275], 
processed observation next is [1.0, 0.6086956521739131, 0.4570637119113574, 0.3633333333333333, 0.2922222222222222, 0.0, 0.6666666666666666, 0.5782962092405249, 0.5761526382984008, 1.0, 1.0, 0.1138797484025119], 
reward next is 0.8861, 
noisyNet noise sample is [array([-0.7803425], dtype=float32), -2.0969923]. 
=============================================
[2019-04-04 11:48:31,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3650671e-17 4.1663649e-15 1.2537204e-24 1.1355841e-16 3.3115653e-17
 3.7522641e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:31,980] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5954
[2019-04-04 11:48:32,094] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.3, 63.0, 0.0, 0.0, 26.0, 25.21531750975478, 0.3431045700788252, 1.0, 1.0, 72530.2479771495], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 327600.0000, 
sim time next is 328200.0000, 
raw observation next is [-12.38333333333333, 64.16666666666667, 0.0, 0.0, 26.0, 25.24244907925018, 0.3430424166914944, 1.0, 1.0, 54601.20244930372], 
processed observation next is [1.0, 0.8260869565217391, 0.1195752539242845, 0.6416666666666667, 0.0, 0.0, 0.6666666666666666, 0.6035374232708483, 0.6143474722304981, 1.0, 1.0, 0.2600057259490653], 
reward next is 0.7400, 
noisyNet noise sample is [array([-0.1644037], dtype=float32), -0.51627976]. 
=============================================
[2019-04-04 11:48:32,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.3731473e-15 2.4542376e-14 1.5435796e-24 4.7005135e-16 5.5397628e-16
 1.6752829e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:32,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6550
[2019-04-04 11:48:32,308] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-11.1, 52.0, 0.0, 0.0, 26.0, 22.81093187715102, -0.2551587828956076, 0.0, 1.0, 46530.19152018274], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 447000.0000, 
sim time next is 447600.0000, 
raw observation next is [-11.0, 52.0, 0.0, 0.0, 26.0, 22.73666249978902, -0.2622796460544719, 0.0, 1.0, 46628.68699150426], 
processed observation next is [1.0, 0.17391304347826086, 0.15789473684210528, 0.52, 0.0, 0.0, 0.6666666666666666, 0.39472187498241834, 0.41257345131517603, 0.0, 1.0, 0.22204136662621077], 
reward next is 0.7780, 
noisyNet noise sample is [array([-0.66534734], dtype=float32), -1.346935]. 
=============================================
[2019-04-04 11:48:35,652] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.9934642e-16 4.1773002e-14 2.5847826e-24 1.3910548e-16 3.7491346e-16
 2.4372445e-17 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:35,652] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2051
[2019-04-04 11:48:35,750] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-12.8, 74.66666666666666, 0.0, 0.0, 26.0, 25.0014235894116, 0.259432396414797, 0.0, 1.0, 38311.62525118916], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 333600.0000, 
sim time next is 334200.0000, 
raw observation next is [-12.8, 75.83333333333334, 0.0, 0.0, 26.0, 24.87957393386549, 0.2348964661590384, 0.0, 1.0, 44520.35259172615], 
processed observation next is [1.0, 0.8695652173913043, 0.1080332409972299, 0.7583333333333334, 0.0, 0.0, 0.6666666666666666, 0.5732978278221242, 0.5782988220530129, 0.0, 1.0, 0.21200167900821976], 
reward next is 0.7880, 
noisyNet noise sample is [array([1.7071403], dtype=float32), 0.63354087]. 
=============================================
[2019-04-04 11:48:54,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0488857e-16 1.8986949e-15 6.0943825e-26 2.2480288e-17 3.4180807e-17
 6.2247606e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:48:54,936] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1804
[2019-04-04 11:48:55,072] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [0.4166666666666667, 91.83333333333334, 23.0, 71.00000000000001, 26.0, 24.33096940491359, 0.1449987863523344, 0.0, 1.0, 40966.92989815716], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 547800.0000, 
sim time next is 548400.0000, 
raw observation next is [0.3333333333333334, 91.66666666666667, 28.5, 87.5, 26.0, 24.30754467152769, 0.2209473390545356, 0.0, 1.0, 202446.3460995434], 
processed observation next is [0.0, 0.34782608695652173, 0.4718374884579871, 0.9166666666666667, 0.095, 0.09668508287292818, 0.6666666666666666, 0.5256287226273075, 0.5736491130181786, 0.0, 1.0, 0.9640302195216353], 
reward next is 0.0360, 
noisyNet noise sample is [array([-0.8620128], dtype=float32), -0.58730716]. 
=============================================
[2019-04-04 11:49:05,838] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.6451519e-16 3.0981880e-15 1.1555895e-24 2.9422454e-17 3.6148706e-17
 1.6298090e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:05,838] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.5530
[2019-04-04 11:49:05,914] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.35, 55.0, 0.0, 0.0, 26.0, 25.62676680033684, 0.3080593722070693, 1.0, 1.0, 74821.89715983628], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 754200.0000, 
sim time next is 754800.0000, 
raw observation next is [-3.533333333333333, 55.33333333333333, 0.0, 0.0, 26.0, 24.92629477322842, 0.2794418936363932, 1.0, 1.0, 197842.2627738118], 
processed observation next is [1.0, 0.7391304347826086, 0.36472760849492153, 0.5533333333333332, 0.0, 0.0, 0.6666666666666666, 0.5771912311023684, 0.5931472978787977, 1.0, 1.0, 0.9421060132086275], 
reward next is 0.0579, 
noisyNet noise sample is [array([-2.0730615], dtype=float32), 0.28114438]. 
=============================================
[2019-04-04 11:49:08,169] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3325656e-16 1.0455233e-15 6.5602036e-26 3.9632281e-17 5.0952767e-17
 4.2964138e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:08,169] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4960
[2019-04-04 11:49:08,226] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.566666666666666, 65.0, 98.16666666666667, 6.333333333333332, 26.0, 24.82906791209721, 0.2069649271944035, 0.0, 1.0, 51425.15721552544], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 643200.0000, 
sim time next is 643800.0000, 
raw observation next is [-3.483333333333333, 65.0, 96.33333333333333, 12.66666666666666, 26.0, 24.87334566296288, 0.2107160721077112, 0.0, 1.0, 23411.84616064343], 
processed observation next is [0.0, 0.43478260869565216, 0.3661126500461681, 0.65, 0.32111111111111107, 0.013996316758747691, 0.6666666666666666, 0.5727788052469066, 0.5702386907025704, 0.0, 1.0, 0.11148498171734966], 
reward next is 0.8885, 
noisyNet noise sample is [array([0.35802165], dtype=float32), -0.88539475]. 
=============================================
[2019-04-04 11:49:18,523] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2442625e-17 1.3488482e-16 9.8441155e-28 4.9444628e-19 1.0521760e-17
 1.6492770e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:18,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6050
[2019-04-04 11:49:18,534] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.0, 77.0, 0.0, 0.0, 26.0, 25.9156054019178, 0.591101125226258, 0.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1042800.0000, 
sim time next is 1043400.0000, 
raw observation next is [13.9, 77.5, 0.0, 0.0, 26.0, 25.81389063268284, 0.579897273662448, 0.0, 1.0, 0.0], 
processed observation next is [1.0, 0.043478260869565216, 0.847645429362881, 0.775, 0.0, 0.0, 0.6666666666666666, 0.6511575527235699, 0.693299091220816, 0.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.7850882], dtype=float32), -2.1996164]. 
=============================================
[2019-04-04 11:49:19,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1662644e-16 3.9380798e-15 1.8024799e-25 9.8924310e-17 4.9854917e-17
 4.8672863e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:19,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3788
[2019-04-04 11:49:19,206] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-7.8, 74.0, 0.0, 0.0, 26.0, 23.87076405202682, 0.04655277786813255, 0.0, 1.0, 41332.08207109112], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 788400.0000, 
sim time next is 789000.0000, 
raw observation next is [-7.716666666666667, 74.16666666666667, 0.0, 0.0, 26.0, 23.92628655232409, 0.04628881364258416, 0.0, 1.0, 41256.88528060348], 
processed observation next is [1.0, 0.13043478260869565, 0.24884579870729456, 0.7416666666666667, 0.0, 0.0, 0.6666666666666666, 0.49385721269367416, 0.5154296045475281, 0.0, 1.0, 0.19646135847906418], 
reward next is 0.8035, 
noisyNet noise sample is [array([-1.3685805], dtype=float32), -1.0537544]. 
=============================================
[2019-04-04 11:49:19,218] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[83.60302 ]
 [83.51222 ]
 [83.461464]
 [83.41553 ]
 [83.40033 ]], R is [[83.70173645]
 [83.66790009]
 [83.63413239]
 [83.60047913]
 [83.56697845]].
[2019-04-04 11:49:19,900] A3C_AGENT_WORKER-Thread-4 DEBUG:Policy network output: [1.5677906e-18 7.7808771e-17 2.0087395e-27 7.0778898e-19 2.0847086e-18
 7.0635061e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:19,905] A3C_AGENT_WORKER-Thread-4 DEBUG:Softmax action selection sampled number: 0.0908
[2019-04-04 11:49:19,977] A3C_AGENT_WORKER-Thread-4 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [4.4, 96.0, 0.0, 0.0, 26.0, 25.13118846047306, 0.3063244988459518, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 928800.0000, 
sim time next is 929400.0000, 
raw observation next is [4.4, 96.66666666666666, 0.0, 0.0, 26.0, 25.07084299046677, 0.2688755455604734, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.782608695652174, 0.5844875346260389, 0.9666666666666666, 0.0, 0.0, 0.6666666666666666, 0.5892369158722307, 0.5896251818534911, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.1633625], dtype=float32), -0.65420336]. 
=============================================
[2019-04-04 11:49:22,969] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5818307e-17 1.9241039e-16 8.5473021e-28 1.3176461e-18 5.3253176e-18
 2.2532883e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:22,969] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0256
[2019-04-04 11:49:22,985] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [9.9, 84.66666666666667, 0.0, 0.0, 26.0, 25.43887766905723, 0.4590517533259002, 0.0, 1.0, 49411.88880657491], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 976200.0000, 
sim time next is 976800.0000, 
raw observation next is [9.8, 86.33333333333334, 0.0, 0.0, 26.0, 25.48746379996748, 0.4579219987709814, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.7340720221606649, 0.8633333333333334, 0.0, 0.0, 0.6666666666666666, 0.6239553166639565, 0.6526406662569938, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.48160467], dtype=float32), -1.0275013]. 
=============================================
[2019-04-04 11:49:25,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9191755e-18 2.5932671e-16 3.3908816e-27 4.5530549e-19 5.0995519e-18
 8.6515499e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:25,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1855
[2019-04-04 11:49:25,197] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [14.4, 78.33333333333333, 0.0, 0.0, 26.0, 25.21788801298528, 0.440858667253848, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1021200.0000, 
sim time next is 1021800.0000, 
raw observation next is [14.4, 77.66666666666667, 0.0, 0.0, 26.0, 25.12155699967074, 0.4267865185489064, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.8260869565217391, 0.8614958448753465, 0.7766666666666667, 0.0, 0.0, 0.6666666666666666, 0.593463083305895, 0.6422621728496355, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.7314164], dtype=float32), 1.2240871]. 
=============================================
[2019-04-04 11:49:26,310] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7940437e-16 1.8331455e-15 3.0318208e-26 9.6024163e-18 2.6882710e-17
 2.4226968e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:26,315] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2288
[2019-04-04 11:49:26,328] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-1.616666666666667, 78.50000000000001, 0.0, 0.0, 26.0, 24.58480514745298, 0.1803158189320113, 0.0, 1.0, 39304.19919942499], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 875400.0000, 
sim time next is 876000.0000, 
raw observation next is [-1.533333333333333, 78.0, 0.0, 0.0, 26.0, 24.57566320659661, 0.1843557792699911, 0.0, 1.0, 39268.00197170453], 
processed observation next is [1.0, 0.13043478260869565, 0.42012927054478305, 0.78, 0.0, 0.0, 0.6666666666666666, 0.5479719338830508, 0.5614519264233303, 0.0, 1.0, 0.18699048557954537], 
reward next is 0.8130, 
noisyNet noise sample is [array([0.06940566], dtype=float32), -0.4576064]. 
=============================================
[2019-04-04 11:49:26,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[86.52807 ]
 [86.503784]
 [86.50005 ]
 [86.53254 ]
 [86.54755 ]], R is [[86.46992493]
 [86.4180603 ]
 [86.36655426]
 [86.31541443]
 [86.26476288]].
[2019-04-04 11:49:26,679] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [2.0410602e-17 1.1552102e-17 8.3447463e-29 1.3099880e-18 1.3789862e-18
 5.8189516e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:26,680] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.3382
[2019-04-04 11:49:26,687] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [18.38333333333334, 64.66666666666667, 150.0, 0.0, 26.0, 25.20668647485015, 0.5111148184841271, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1163400.0000, 
sim time next is 1164000.0000, 
raw observation next is [18.46666666666667, 64.33333333333334, 155.0, 0.0, 26.0, 25.17723454235248, 0.5083517091195883, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.4782608695652174, 0.9741458910433982, 0.6433333333333334, 0.5166666666666667, 0.0, 0.6666666666666666, 0.5981028785293733, 0.6694505697065294, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.645013], dtype=float32), -1.1774447]. 
=============================================
[2019-04-04 11:49:26,692] A3C_AGENT_WORKER-Thread-6 DEBUG:Value prediction is [[94.13754 ]
 [93.11346 ]
 [92.80131 ]
 [92.020775]
 [91.634415]], R is [[94.81195068]
 [94.86383057]
 [94.91519165]
 [94.96604156]
 [95.01638031]].
[2019-04-04 11:49:32,646] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3883176e-17 2.1674905e-17 3.2265023e-29 4.9169265e-19 1.4218972e-18
 1.3270511e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:32,653] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9875
[2019-04-04 11:49:32,685] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [15.0, 96.0, 0.0, 0.0, 26.0, 23.45096544400141, 0.1378830058280725, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1236600.0000, 
sim time next is 1237200.0000, 
raw observation next is [15.0, 96.0, 0.0, 0.0, 26.0, 23.43141006263212, 0.1402564947286637, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.30434782608695654, 0.8781163434903049, 0.96, 0.0, 0.0, 0.6666666666666666, 0.45261750521934346, 0.5467521649095546, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.01936967], dtype=float32), 0.44844094]. 
=============================================
[2019-04-04 11:49:32,804] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [5.9009371e-17 1.7013939e-16 2.3090837e-27 1.6210786e-18 2.6813155e-18
 4.1043122e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:32,805] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2557
[2019-04-04 11:49:32,823] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [2.8, 92.0, 0.0, 0.0, 26.0, 25.53004911675011, 0.5688243860354517, 0.0, 1.0, 18749.45340735314], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1306200.0000, 
sim time next is 1306800.0000, 
raw observation next is [2.7, 92.0, 0.0, 0.0, 26.0, 25.52847677621851, 0.5648522309285848, 0.0, 1.0, 18746.34736803384], 
processed observation next is [1.0, 0.13043478260869565, 0.5373961218836566, 0.92, 0.0, 0.0, 0.6666666666666666, 0.627373064684876, 0.688284076976195, 0.0, 1.0, 0.08926832080016114], 
reward next is 0.9107, 
noisyNet noise sample is [array([-1.1554058], dtype=float32), -0.737921]. 
=============================================
[2019-04-04 11:49:36,245] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7239588e-18 2.3758050e-17 2.8556236e-29 1.9726925e-19 3.3118148e-19
 1.7218326e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:36,246] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3570
[2019-04-04 11:49:36,267] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [13.55, 61.0, 0.0, 0.0, 26.0, 25.62754878858968, 0.6721080609533719, 0.0, 1.0, 148597.2146069741], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1110600.0000, 
sim time next is 1111200.0000, 
raw observation next is [13.46666666666667, 61.33333333333333, 0.0, 0.0, 26.0, 25.63273079385275, 0.6896844791921866, 0.0, 1.0, 70090.4454993592], 
processed observation next is [1.0, 0.8695652173913043, 0.8356417359187445, 0.6133333333333333, 0.0, 0.0, 0.6666666666666666, 0.6360608994877293, 0.7298948263973956, 0.0, 1.0, 0.33376402618742473], 
reward next is 0.6662, 
noisyNet noise sample is [array([0.9788498], dtype=float32), 1.5932046]. 
=============================================
[2019-04-04 11:49:39,960] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.20162724e-17 6.12954358e-18 1.87302074e-29 2.67907515e-19
 2.78270641e-19 2.47065682e-22 1.00000000e+00], sum to 1.0000
[2019-04-04 11:49:39,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1407
[2019-04-04 11:49:39,966] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.6, 75.0, 0.0, 0.0, 26.0, 24.38783406561935, 0.3296011041979662, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1204200.0000, 
sim time next is 1204800.0000, 
raw observation next is [16.6, 75.0, 0.0, 0.0, 26.0, 24.35590633610353, 0.3284045748231311, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9565217391304348, 0.922437673130194, 0.75, 0.0, 0.0, 0.6666666666666666, 0.5296588613419608, 0.6094681916077104, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.8632079], dtype=float32), -0.26003766]. 
=============================================
[2019-04-04 11:49:41,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6926795e-17 9.5948732e-18 3.7124349e-29 8.8449027e-20 3.2278976e-19
 6.0298502e-23 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:41,676] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2995
[2019-04-04 11:49:41,679] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [16.1, 81.0, 0.0, 0.0, 26.0, 24.07148002383564, 0.2632064915061604, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1214400.0000, 
sim time next is 1215000.0000, 
raw observation next is [16.1, 81.5, 0.0, 0.0, 26.0, 24.04427414845465, 0.2587719871047598, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.043478260869565216, 0.9085872576177286, 0.815, 0.0, 0.0, 0.6666666666666666, 0.503689512371221, 0.5862573290349199, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5261944], dtype=float32), 0.5116206]. 
=============================================
[2019-04-04 11:49:41,695] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[98.558464]
 [98.555664]
 [98.53359 ]
 [98.52926 ]
 [98.51977 ]], R is [[98.60391998]
 [98.61788177]
 [98.63170624]
 [98.64539337]
 [98.65894318]].
[2019-04-04 11:49:42,110] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4445696e-18 6.0015705e-18 1.6139428e-29 6.3074137e-20 3.8595901e-19
 3.5923718e-22 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:42,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9667
[2019-04-04 11:49:42,116] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [17.15, 71.0, 0.0, 0.0, 26.0, 24.49777400354039, 0.3628261785481878, 0.0, 0.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1200600.0000, 
sim time next is 1201200.0000, 
raw observation next is [16.96666666666667, 72.33333333333334, 0.0, 0.0, 26.0, 24.53009955798245, 0.3597404996148073, 0.0, 0.0, 0.0], 
processed observation next is [0.0, 0.9130434782608695, 0.9325946445060022, 0.7233333333333334, 0.0, 0.0, 0.6666666666666666, 0.5441749631652041, 0.6199134998716024, 0.0, 0.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.2299979], dtype=float32), -2.7925692]. 
=============================================
[2019-04-04 11:49:42,543] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [1.5112947e-17 1.4714791e-16 1.6880990e-27 1.0216763e-18 4.4141025e-18
 3.2643135e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:42,548] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.2796
[2019-04-04 11:49:42,567] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.433333333333333, 90.0, 0.0, 0.0, 26.0, 25.10493229092007, 0.5231709894785996, 0.0, 1.0, 71120.4492216831], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1456800.0000, 
sim time next is 1457400.0000, 
raw observation next is [1.516666666666667, 89.5, 0.0, 0.0, 26.0, 25.31118574858411, 0.5472547409963874, 0.0, 1.0, 50075.17429047314], 
processed observation next is [1.0, 0.8695652173913043, 0.5046168051708219, 0.895, 0.0, 0.0, 0.6666666666666666, 0.6092654790486757, 0.6824182469987958, 0.0, 1.0, 0.23845321090701496], 
reward next is 0.7615, 
noisyNet noise sample is [array([-0.3038189], dtype=float32), -1.9608788]. 
=============================================
[2019-04-04 11:49:54,584] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4274821e-18 1.2990563e-16 6.3507282e-27 4.8091495e-19 2.5119349e-18
 3.2179497e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:49:54,586] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7945
[2019-04-04 11:49:54,626] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.1, 92.0, 22.66666666666666, 0.0, 26.0, 25.59986680016079, 0.4871786981100868, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1441200.0000, 
sim time next is 1441800.0000, 
raw observation next is [1.1, 92.0, 18.0, 0.0, 26.0, 25.80606929791018, 0.4981458413407355, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6956521739130435, 0.49307479224376743, 0.92, 0.06, 0.0, 0.6666666666666666, 0.6505057748258484, 0.6660486137802452, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.5335689], dtype=float32), 1.3459057]. 
=============================================
[2019-04-04 11:50:12,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8667153e-16 5.8391851e-15 9.4997115e-25 5.2203676e-17 1.5028215e-16
 2.8729470e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:50:12,734] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9708
[2019-04-04 11:50:12,756] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-9.5, 91.0, 0.0, 0.0, 26.0, 23.11923672893798, -0.1933502383145946, 0.0, 1.0, 44508.9306684652], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1926000.0000, 
sim time next is 1926600.0000, 
raw observation next is [-9.5, 91.00000000000001, 0.0, 0.0, 26.0, 23.10268740586633, -0.1985076971403363, 0.0, 1.0, 44502.11013177613], 
processed observation next is [1.0, 0.30434782608695654, 0.1994459833795014, 0.9100000000000001, 0.0, 0.0, 0.6666666666666666, 0.42522395048886086, 0.43383076761988787, 0.0, 1.0, 0.2119148101513149], 
reward next is 0.7881, 
noisyNet noise sample is [array([0.87081164], dtype=float32), -0.540184]. 
=============================================
[2019-04-04 11:50:14,337] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2861256e-15 3.8018049e-15 2.7387113e-24 2.1580285e-16 3.1720087e-16
 1.3433371e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:50:14,338] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5924
[2019-04-04 11:50:14,387] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.483333333333333, 86.16666666666667, 40.66666666666666, 0.0, 26.0, 24.97855466178236, 0.3304793867083899, 0.0, 1.0, 64707.09898018058], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1786200.0000, 
sim time next is 1786800.0000, 
raw observation next is [-3.566666666666667, 85.33333333333334, 34.33333333333334, 0.0, 26.0, 24.97640432872877, 0.3331087779962963, 0.0, 1.0, 56937.79337887621], 
processed observation next is [0.0, 0.6956521739130435, 0.3638042474607572, 0.8533333333333334, 0.11444444444444447, 0.0, 0.6666666666666666, 0.5813670273940641, 0.6110362593320987, 0.0, 1.0, 0.27113234942322006], 
reward next is 0.7289, 
noisyNet noise sample is [array([0.6086417], dtype=float32), 1.2246089]. 
=============================================
[2019-04-04 11:50:26,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.8679461e-16 2.4550043e-15 7.4988250e-25 7.7243026e-17 1.7762567e-16
 1.1577020e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:50:26,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0820
[2019-04-04 11:50:26,866] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.7, 81.66666666666667, 0.0, 0.0, 26.0, 24.83541036807099, 0.1952910660612594, 0.0, 1.0, 44728.42424400686], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 1890600.0000, 
sim time next is 1891200.0000, 
raw observation next is [-5.8, 80.33333333333334, 0.0, 0.0, 26.0, 24.79831792480922, 0.1877902010773478, 0.0, 1.0, 44737.15796440637], 
processed observation next is [0.0, 0.9130434782608695, 0.30193905817174516, 0.8033333333333335, 0.0, 0.0, 0.6666666666666666, 0.5665264937341016, 0.5625967336924492, 0.0, 1.0, 0.21303408554479222], 
reward next is 0.7870, 
noisyNet noise sample is [array([1.3831844], dtype=float32), -0.80509216]. 
=============================================
[2019-04-04 11:50:42,943] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3891725e-16 3.7619012e-15 6.0338018e-25 4.6128316e-17 1.6899060e-16
 5.3959044e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:50:42,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4295
[2019-04-04 11:50:42,972] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-5.4, 89.33333333333334, 0.0, 0.0, 26.0, 24.38834446244927, 0.1460217016253325, 0.0, 1.0, 43453.19759809693], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2086800.0000, 
sim time next is 2087400.0000, 
raw observation next is [-5.5, 90.16666666666667, 0.0, 0.0, 26.0, 24.33002214882667, 0.1566965362283043, 0.0, 1.0, 43830.69728576837], 
processed observation next is [1.0, 0.13043478260869565, 0.3102493074792244, 0.9016666666666667, 0.0, 0.0, 0.6666666666666666, 0.5275018457355559, 0.5522321787427681, 0.0, 1.0, 0.20871760612270654], 
reward next is 0.7913, 
noisyNet noise sample is [array([-2.1787357], dtype=float32), -1.7887981]. 
=============================================
[2019-04-04 11:50:48,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.3679080e-17 1.4071200e-15 1.6280198e-25 1.9121469e-17 5.0931777e-17
 1.1629694e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:50:48,162] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0337
[2019-04-04 11:50:48,203] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.583333333333333, 68.5, 9.999999999999998, 0.0, 26.0, 26.09553656741864, 0.4501102577367457, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2135400.0000, 
sim time next is 2136000.0000, 
raw observation next is [-4.666666666666667, 69.0, 5.999999999999999, 0.0, 26.0, 26.07787685412749, 0.2658997879117886, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.7391304347826086, 0.3333333333333333, 0.69, 0.019999999999999997, 0.0, 0.6666666666666666, 0.6731564045106241, 0.5886332626372629, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-0.46921426], dtype=float32), -0.098073915]. 
=============================================
[2019-04-04 11:50:48,206] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[82.28962 ]
 [82.36225 ]
 [82.32232 ]
 [82.229805]
 [82.203705]], R is [[82.26133728]
 [82.43872833]
 [82.61434174]
 [82.78820038]
 [82.96031952]].
[2019-04-04 11:51:23,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1296988e-17 7.9661245e-17 8.2150365e-27 2.2684525e-18 2.3663791e-18
 2.1651485e-20 1.0000000e+00], sum to 1.0000
[2019-04-04 11:51:23,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3063
[2019-04-04 11:51:23,850] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.816666666666666, 64.5, 132.3333333333333, 213.6666666666667, 26.0, 25.81087476728168, 0.3637348747227365, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2628600.0000, 
sim time next is 2629200.0000, 
raw observation next is [-4.633333333333334, 64.0, 142.1666666666667, 244.3333333333333, 26.0, 25.78877569277127, 0.3677149945195923, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.3342566943674977, 0.64, 0.473888888888889, 0.26998158379373843, 0.6666666666666666, 0.6490646410642725, 0.6225716648398641, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.37036327], dtype=float32), -2.043393]. 
=============================================
[2019-04-04 11:51:39,667] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4534448e-15 6.5044719e-15 1.0221059e-24 9.7356829e-17 6.1085417e-16
 6.3222589e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:51:39,667] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1695
[2019-04-04 11:51:39,697] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-4.0, 77.0, 14.0, 15.66666666666666, 26.0, 23.55692550020545, 0.03588894036118623, 0.0, 1.0, 60784.7028087997], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2965200.0000, 
sim time next is 2965800.0000, 
raw observation next is [-4.0, 77.0, 27.99999999999999, 22.33333333333333, 26.0, 23.55670173226881, 0.03318139203032623, 0.0, 1.0, 60770.28938373447], 
processed observation next is [0.0, 0.30434782608695654, 0.3518005540166205, 0.77, 0.0933333333333333, 0.024677716390423567, 0.6666666666666666, 0.46305847768906744, 0.5110604640101087, 0.0, 1.0, 0.28938233039873557], 
reward next is 0.7106, 
noisyNet noise sample is [array([1.9815233], dtype=float32), 0.39065975]. 
=============================================
[2019-04-04 11:51:53,030] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8122949e-16 2.4023460e-15 2.8732654e-25 3.0826586e-17 8.8739473e-17
 4.3239708e-19 1.0000000e+00], sum to 1.0000
[2019-04-04 11:51:53,030] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3958
[2019-04-04 11:51:53,045] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.90423157677807, 0.3398234318940764, 0.0, 1.0, 43355.75156073116], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2938800.0000, 
sim time next is 2939400.0000, 
raw observation next is [-2.0, 85.0, 0.0, 0.0, 26.0, 24.86367884426795, 0.3370372271014519, 0.0, 1.0, 43350.77912403444], 
processed observation next is [0.0, 0.0, 0.40720221606648205, 0.85, 0.0, 0.0, 0.6666666666666666, 0.5719732370223293, 0.6123457423671507, 0.0, 1.0, 0.20643228154302112], 
reward next is 0.7936, 
noisyNet noise sample is [array([-1.2227275], dtype=float32), -0.27494138]. 
=============================================
[2019-04-04 11:51:59,215] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.39304504e-18 1.75459857e-16 1.43044845e-27 9.66698685e-19
 1.05288605e-18 8.41818882e-21 1.00000000e+00], sum to 1.0000
[2019-04-04 11:51:59,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1162
[2019-04-04 11:51:59,235] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.0, 52.5, 174.0, 508.0, 26.0, 26.01832948085698, 0.4521229902171777, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 2802600.0000, 
sim time next is 2803200.0000, 
raw observation next is [-1.666666666666667, 51.66666666666666, 165.8333333333333, 550.5, 26.0, 26.02863145029203, 0.45587645292657, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.43478260869565216, 0.4164358264081256, 0.5166666666666666, 0.5527777777777776, 0.6082872928176796, 0.6666666666666666, 0.669052620857669, 0.65195881764219, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([-1.5636405], dtype=float32), 0.08058587]. 
=============================================
[2019-04-04 11:52:12,059] A3C_AGENT_WORKER-Thread-6 DEBUG:Policy network output: [3.93822609e-17 5.24912564e-16 2.55882118e-26 5.05749311e-18
 1.28829749e-17 1.06646354e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 11:52:12,060] A3C_AGENT_WORKER-Thread-6 DEBUG:Softmax action selection sampled number: 0.9359
[2019-04-04 11:52:12,104] A3C_AGENT_WORKER-Thread-6 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.0, 79.0, 0.0, 0.0, 26.0, 25.05325187706555, 0.4154175641174782, 1.0, 1.0, 86742.98464506885], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3439800.0000, 
sim time next is 3440400.0000, 
raw observation next is [1.0, 79.0, 0.0, 0.0, 26.0, 24.98559883959826, 0.4260191682615519, 0.0, 1.0, 94808.19138469317], 
processed observation next is [1.0, 0.8260869565217391, 0.4903047091412743, 0.79, 0.0, 0.0, 0.6666666666666666, 0.5821332366331884, 0.6420063894205174, 0.0, 1.0, 0.45146757802234844], 
reward next is 0.5485, 
noisyNet noise sample is [array([-0.52698344], dtype=float32), 1.4444542]. 
=============================================
[2019-04-04 11:52:14,148] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0898446e-18 1.8758137e-17 5.9544523e-27 1.9358808e-19 7.8259372e-19
 5.9321161e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:52:14,149] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6866
[2019-04-04 11:52:14,171] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-3.666666666666667, 95.16666666666667, 104.3333333333333, 783.3333333333334, 26.0, 26.75671888981066, 0.8124937177063464, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3247800.0000, 
sim time next is 3248400.0000, 
raw observation next is [-3.333333333333333, 90.33333333333334, 102.6666666666667, 776.1666666666667, 26.0, 26.82653675238075, 0.8238859196633371, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.6086956521739131, 0.37026777469990774, 0.9033333333333334, 0.3422222222222223, 0.8576427255985268, 0.6666666666666666, 0.7355447293650625, 0.7746286398877791, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([0.04793613], dtype=float32), 1.6663618]. 
=============================================
[2019-04-04 11:52:20,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.59249252e-16 7.96151096e-16 4.04884807e-25 3.78537601e-17
 1.04481706e-16 3.55577028e-19 1.00000000e+00], sum to 1.0000
[2019-04-04 11:52:20,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0721
[2019-04-04 11:52:20,795] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.02234619193356, 0.3636279260487965, 0.0, 1.0, 41525.74974122704], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3369600.0000, 
sim time next is 3370200.0000, 
raw observation next is [-6.0, 77.0, 0.0, 0.0, 26.0, 25.01171949150443, 0.3557869456313673, 0.0, 1.0, 41486.66349902084], 
processed observation next is [1.0, 0.0, 0.296398891966759, 0.77, 0.0, 0.0, 0.6666666666666666, 0.5843099576253691, 0.6185956485437891, 0.0, 1.0, 0.19755554047152782], 
reward next is 0.8024, 
noisyNet noise sample is [array([0.09795276], dtype=float32), 0.78297716]. 
=============================================
[2019-04-04 11:52:23,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0035338e-17 5.6795749e-17 5.5197740e-28 7.7033433e-19 1.7695054e-18
 3.1018681e-21 1.0000000e+00], sum to 1.0000
[2019-04-04 11:52:23,657] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1091
[2019-04-04 11:52:23,700] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [-2.333333333333333, 61.66666666666667, 16.16666666666666, 159.5, 26.0, 25.41409869212297, 0.3732327566930405, 1.0, 1.0, 0.0], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [-2.166666666666667, 60.83333333333333, 30.33333333333333, 212.0, 26.0, 25.57698328643263, 0.3974027186347329, 1.0, 1.0, 0.0], 
processed observation next is [1.0, 0.30434782608695654, 0.4025854108956602, 0.6083333333333333, 0.1011111111111111, 0.23425414364640884, 0.6666666666666666, 0.6314152738693858, 0.6324675728782443, 1.0, 1.0, 0.0], 
reward next is 1.0000, 
noisyNet noise sample is [array([1.3960284], dtype=float32), -0.39032635]. 
=============================================
[2019-04-04 11:52:52,483] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.7436312e-17 1.1156487e-15 2.1489902e-25 1.2892168e-17 4.3742105e-17
 1.0592188e-18 1.0000000e+00], sum to 1.0000
[2019-04-04 11:52:52,483] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6160
[2019-04-04 11:52:52,562] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 6, 
current raw observation is [1.833333333333333, 48.5, 0.0, 0.0, 26.0, 25.61920228760544, 0.5403690646692779, 1.0, 1.0, 73963.42234868222], 
current ob forecast is [], 
actual action is [26.0], 
sim time this is 3867000.0000, 
sim time next is 3867600.0000, 
raw observation next is [1.666666666666667, 49.0, 0.0, 0.0, 26.0, 24.97219156296409, 0.4801047369321038, 1.0, 1.0, 184952.3626066606], 
processed observation next is [1.0, 0.782608695652174, 0.5087719298245615, 0.49, 0.0, 0.0, 0.6666666666666666, 0.5810159635803407, 0.6600349123107013, 1.0, 1.0, 0.8807255362221934], 
reward next is 0.1193, 
noisyNet noise sample is [array([0.6500137], dtype=float32), -0.6292686]. 
=============================================
[2019-04-04 11:52:56,027] A3C_AGENT_WORKER-Thread-4 INFO:Evaluating...
[2019-04-04 11:52:56,032] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation job starts!
[2019-04-04 11:52:56,033] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation job starts!
[2019-04-04 11:52:56,034] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation job starts!
[2019-04-04 11:52:56,038] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:52:56,042] EPLUS_ENV_Part4-Light-Pit-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v2-res1/Eplus-env-sub_run51
[2019-04-04 11:52:56,038] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:52:56,038] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-04 11:52:56,124] EPLUS_ENV_Part4-Light-Pit-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Test-v1-res1/Eplus-env-sub_run51
[2019-04-04 11:52:56,212] EPLUS_ENV_Part4-Light-Pit-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part4_pit_light/4/Eplus-env-Part4-Light-Pit-Train-v1-res1/Eplus-env-sub_run51
[2019-04-04 11:54:48,538] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.16233768], dtype=float32), -0.33745903]
[2019-04-04 11:54:48,538] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation this: [2.2, 67.0, 0.0, 0.0, 26.0, 24.95395936422239, 0.3884901830658674, 0.0, 1.0, 37382.96744646758]
[2019-04-04 11:54:48,538] A3C_EVAL-Part4-Light-Pit-Test-v1 DEBUG:Observation forecast: []
[2019-04-04 11:54:48,540] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Softmax [2.9199293e-16 2.0353943e-15 2.3343269e-25 3.8621126e-17 6.9280360e-17
 9.3338916e-19 1.0000000e+00], sampled 0.687682756876924
[2019-04-04 11:55:37,447] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16233768], dtype=float32), -0.33745903]
[2019-04-04 11:55:37,447] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation this: [-7.666666666666667, 68.0, 109.8333333333333, 719.1666666666667, 26.0, 26.31550079037065, 0.5702048490235775, 1.0, 1.0, 0.0]
[2019-04-04 11:55:37,447] A3C_EVAL-Part4-Light-Pit-Train-v1 DEBUG:Observation forecast: []
[2019-04-04 11:55:37,448] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Softmax [6.9506655e-18 3.7733088e-17 2.1970146e-27 7.7113701e-19 1.3052492e-18
 9.4665579e-21 1.0000000e+00], sampled 0.9981499787798369
[2019-04-04 11:56:01,722] A3C_EVAL-Part4-Light-Pit-Test-v1 INFO:Evaluation: average rewards by now are 7353.7145 239879953.7401 1605.0915
[2019-04-04 11:56:10,206] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16233768], dtype=float32), -0.33745903]
[2019-04-04 11:56:10,206] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation this: [5.234303540666667, 21.31088530166667, 102.3037735733333, 842.4696166333333, 26.0, 26.38112810764398, 0.6132787098339951, 1.0, 1.0, 0.0]
[2019-04-04 11:56:10,206] A3C_EVAL-Part4-Light-Pit-Test-v2 DEBUG:Observation forecast: []
[2019-04-04 11:56:10,207] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Softmax [3.9794667e-18 3.7571361e-17 2.4754525e-27 7.1492569e-19 7.3349801e-19
 7.1705541e-21 1.0000000e+00], sampled 0.14594061929770785
[2019-04-04 11:56:35,134] A3C_EVAL-Part4-Light-Pit-Train-v1 INFO:Evaluation: average rewards by now are 7241.2807 263491058.6319 1557.0220
[2019-04-04 11:56:38,273] A3C_EVAL-Part4-Light-Pit-Test-v2 INFO:Evaluation: average rewards by now are 7182.5014 275834701.7674 1233.3127
[2019-04-04 11:56:39,304] A3C_AGENT_WORKER-Thread-4 INFO:Global step: 5000000, evaluation results [5000000.0, 7241.280673181259, 263491058.63193664, 1557.0219705148743, 7353.714505999617, 239879953.74008468, 1605.0915103508335, 7182.501420155226, 275834701.76740354, 1233.3126919883946]
